[[release-notes-ocp-3-5-release-notes]]
= {product-title} 3.5 Release Notes
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

Red Hat {product-title} is a Platform as a Service (PaaS) that provides
developers and IT organizations with a cloud application platform for deploying
new applications on secure, scalable resources with minimal configuration and
management overhead. {product-title} supports a wide selection of
programming languages and frameworks, such as Java, Ruby, and PHP.

Built on Red Hat Enterprise Linux and Google Kubernetes, {product-title}
provides a secure and scalable multi-tenant operating system for today’s
enterprise-class applications, while providing integrated application runtimes
and libraries. {product-title} brings the OpenShift PaaS platform to customer
data centers, enabling organizations to implement a private PaaS that meets
security, privacy, compliance, and governance requirements.

[[ocp-35-about-this-release]]
== About This Release

Red Hat {product-title} version 3.5
(link:https://access.redhat.com/errata/RHBA-2017:0884[RHBA-2017:0884]) is now
available. This release is based on
link:https://github.com/openshift/origin/releases/tag/v1.5.0-rc.0[OpenShift
Origin 1.5]. New features, changes, bug fixes, and known issues that pertain to
{product-title} 3.5 are included in this topic.

{product-title} 3.5 is supported on RHEL 7.2 and 7.3 with the latest packages
from Extras, including Docker 1.12.

[IMPORTANT]
====
{product-title} 3.5 is not certified with older versions of Docker and RHEL 7.1
or earlier.
====

For initial installations, see the
xref:../install_config/install/planning.adoc#install-config-install-planning[Installing
a Cluster] topics in the
xref:../install_config/index.adoc#install-config-index[Installation and
Configuration] documentation.

To upgrade to this release from a previous version, see the
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[Upgrading
a Cluster] topics in the
xref:../install_config/index.adoc#install-config-index[Installation and
Configuration] documentation.

[[ocp-35-new-features-and-enhancements]]
== New Features and Enhancements

This release adds improvements related to the following components and concepts.

[[ocp-35-container-orchestration]]
=== Container Orchestration

[[ocp-35-shared-memory-for-containers-within-the-same-pod]]
==== Shared Memory for Containers within the Same Pod

When running more than one container in the same pod, those containers can now
share memory segments because *_/dev/shm_* is shared across containers in a pod.
*_/dev/shm_* is limited to 64 MB and charged under the `memcg cgroup`.

[[ocp-35-ability-to-refer-to-external-endpoints-by-name]]
==== Ability to Refer to External Endpoints by Name

It is very common to add an external service to your application. This is a
service that does not live on the {product-title}. Before this feature, you had
to add that endpoint to your application via IP address. These services often
had pools of IPs or required access via fully qualified domain name (FQDN),
which is similar to SNI in {product-title}'s load balancer. Now, {product-title}
has a new `ExternalName` type that will accept an FQDN.

[[ocp-35-setting-a-nodeselector-and-whitelisting-labels]]
==== Setting a NodeSelector and Whitelisting Labels

{product-title} 3.5 now supports setting a `nodeSelector` on a namespace, with the
ability to whitelist labels during the selection. In
`cpodNodeSelectorPluginConfig`, there are four added levels of logic to allow
for various control of the selection process:

. `clusterDefaultNodeSelector: region=west` - The default global (cluster level) list of labels (L1).

. `ns1: os=centos,region=west` and `ns2: os=rhel` - The whitelist of labels per namespace (L2).

. `scheduler.alpha.kubernetes.io/node-selector` - The default list of labels per namespace (L3).

. `podSpec.nodeSelector` - The list of labels per pod (L4).

[[ocp-35-kubelet-directory-should-not-share-selinux-labels-with-its-containers]]
==== Kubelet Directory Should Not Share SELinux Labels with its Containers

Previously, the kubelet directory had to be labeled `svirt_sandbox_file_t`. This
widens the attack surface available to containers escaping namespace isolation.
Now, the kubelet directory has an SELinux context and the kubelet cannot share
SELinux labels that are known, preventing the intruder from doing more damage.
With {product-title} 3.5, the use of `svirt_sandbox_file_t` is removed on the
kublet directory and *_openshift.local.volumes_* directory. The need to carry a
matching SElinux MAC is removed through the use of a `:Z` bind mount flag for
container directories.

[[ocp-35-HPA-ignores-pod-startup-spikes]]
==== Horizontal Pod Autoscaler Ignores Pod Startup Spikes

The Horizontal Pod Autoscaler (HPA) now ignores the CPU usage for pods that are
not marked as `ready`, so that any extra CPU usage caused by initialization is
not the cause for a scale-up. Pods in the `unready` state will have zero CPU
usage when scaling up, and the HPA ignores them when scaling down. Pods without
known metrics will have a 0% CPU usage when scaling up, and 100% CPU when
scaling down. This allows for more stability during the HPA decision.

[[ocp-35-control-over-multiple-secret-file-permissions-in-a-volume]]
==== Control Over Multiple Secret File Permissions in a Volume

Many applications require that permissions on the mounted file (that hold their
configuration) be only owner readable. Users can now specify the mode for the
mount point in the downward API annotation or the `configMap`.

[[ocp-35-kubelet-removes-memory-backed-volumes-upon-pod-termination]]
==== Kubelet Removes Memory-backed Volumes Upon Pod Termination

Previously, when a pod terminated, volumes used by `emptyDir`, `configMaps`, and
secrets on the node were kept. Now, the kubelet deletes memory-backed volumes
when the associated pods enter into terminated mode. This makes the vectors less
likely to be used as an attack vector.

[[ocp-35-rate-limiting-controller-retries-to-improve-cluster-stability]]
==== Rate Limiting Controller Retries to Improve Cluster Stability

Controllers on the master can become unstable or cause resource issues when they
continuously retry on unresponsive endpoints in the cluster. {product-title} 3.5
introduces logic to the controllers that controls how many times they retry
calls, and intelligently backs them off so that they do not hang.

These controllers now contain the logic:

* replication controller
* replica set
* daemonset
* certificates
* deployments
* endpoints
* pod disruption budget
* jobs

[[ocp-35-kubelet-collection-of-node-attriutes-for-scheduling-considerations]]
==== Kubelet Collection of Node Attributes for Scheduling Considerations (Technology Preview)

The kubelet is now able to collect any attribute on the node for scheduling
considerations. This feature is currently in xref:ocp-35-technology-preview[Technology
Preview].

The cluster operator must advertise a per-node opaque resource on one or more
nodes. Users must request the opaque resource in pods. To advertise a new opaque
integer resource, the cluster operator should submit a PATCH HTTP request to the
API server to specify the available quantity in the `status.capacity` for a node
in the cluster. After this operation, the node's `status.capacity` will include
a new resource. The `status.allocatable` field is updated automatically with the
new resource asynchronously by the kubelet.

See xref:../dev_guide/compute_resources.adoc#opaque-integer-resources[Opaque
Integer Resources] for more information.

[[ocp-35-statefulsets]]
==== StatefulSets (Technology Preview)

`StatefulSets` (currently in xref:ocp-35-technology-preview[Technology Preview]
and formerly known as `PetSets`) offer more control over scale, network naming,
handling of PVs, and deployment sequencing.

This new controller allows for the deployment of application types that require
changes to their configuration or deployment count (instances) to be done in a
specific and ordered manner.

Supported:

- Declaration of the Ordinal Index.
- Stable network ID nomenclature.
- Controlled or manual handling of PVs.
- Sequence control at deployment time.
- Ordered control during scale up or scale down, based on instance status.

Not Supported:

- Slow to iterate through the Ordinal Index and, therefore, slow on scale up and
scale down.
- No deployment or pod specification post deployment verification of what is
deployed versus what is configured in the JSON file.
- Locality awareness of zones or regions when dealing with scale up or scale down
ordinality changes or mounted PVs.

[IMPORTANT]
====
If you have any existing `PetSets` in your cluster, you must remove them before
upgrading to {product-title} 3.5. Automatically migrating `PetSets` to
`StatefulSets` in {product-title} 3.5 is not supported. Follow the instructions
for
xref:../install_config/upgrading/manual_upgrades.adoc#install-config-upgrading-manual-upgrades[manually
migrating `PetSets` to `StatefulSets`].
====

See more information about xref:ocp-35-web-console-statefulsets[web console
enhancements] related to this feature for {product-title} 3.5.

[[ocp-35-registry]]
=== Registry

{product-title} now allows control of whether or not an image is cached locally
in the internal OpenShift Container Registry via the `oc tag` command with the
`--reference-policy=local` and `--scheduled=true` options.

The storage of the manifest is moved to the OpenShift Container Registry,
instead of storing it in etcd. There are two processes that will clean up
existing images' metadata from etcd:

* `push` and `prune` will gradually migrate all etcd images to not have the manifest attached.
* Use a provided script manually to do them all at once.

Create an image stream from a Docker image and tell it to store locally in the
internal {product-title} registry:

----
$ oc tag --reference-policy=local --source=docker docker.io/image:tag
myimagestream:tag
----

Schedule the image stream to track new image changes in the external registry:

----
$ oc tag --scheduled=true --source=docker docker.io/image:tag myimagestream:tag
----

See
xref:../install_config/registry/extended_registry_configuration.adoc#install-config-registry-extended-configuration[Extended
Registry Configuration] for more information.

[[ocp-35-platform-management]]
=== Platform Management

[[ocp-35-application-service-cert-regeneration]]
==== Application Service Certificate Regeneration (Technology Preview)

Application service certificate regeneration is currently in
xref:ocp-35-technology-preview[Technology Preview].

The controller will now look over the expiry of application certificates that have used
the `service.alpha.openshift.io/serving-cert-secret-name` API and regenerate them.

Set the `service.alpha.openshift.io/serving-cert-secret-name` to the name you
want to use for your secret. Then, your `PodSpec` can mount that secret. When it
is available, your pod will run. The certificate will be good for the internal
service DNS name, `<service.name>.<service.namespace>.svc`. The certificate and
key are in PEM format, stored in *_tls.crt_* and *_tls.key_*, respectively.

----
$ oc get secret ssl-key -o yaml

kind: Secret
metadata:
  annotations:
    service.alpha.openshift.io/expiry: 2017-03-19T08:07:07Z
----

When the regenerator finds a certificate that does not have the expiry
annotation, it will regenerate as well. However, the existing secret is not
invalidated. Therefore, no manual intervention is required to get the
regeneration behavior.

See xref:../dev_guide/secrets.adoc#service-serving-certificate-secrets[Service Serving Certificate Secrets] for more information.

[[ocp-35-configurable-expiry-range-for-framework-certs]]
==== Configurable Expiry Range for Framework Certificates

By default, the certificates used to govern the etcd, master, and kubelet expire
after two to five years. There is now an `oc` command to change this expiry to
be end-user configurable. This has not been implemented in the Ansible installer
yet.

Use the `oc adm ca` command, specifying a validity period greater than two years:

----
# oc adm ca create-master-certs --hostnames=example.org --signer-expire-days=$[365*2+1]`
----

See
xref:../install_config/master_node_configuration.adoc#creating-new-configuration-files[Creating
New Configuration Files] for more information.

[[ocp-35-can-i-command-and-scc-review-command]]
==== can-i Command and scc-review Command Options

The `can-i` and `scc-review` command options allow users to better understand
their permissions and
xref:../architecture/additional_concepts/authorization.adoc#security-context-constraints[security
context constraints (SCC)] setting in their projects. Users see a list of the
commands they are allowed to execute.

The `can-i` command  option tests scopes in terms of the user and role. The
`scc-review` command option checks which `ServiceAccount` can create a pod.

`scc-subject-review` can check whether a user or a `ServiceAccount` can create a
pod.

List which permissions a particular user or group has in the project by project
administrator:

----
$ oc policy can-i --list --user=**
$ oc policy can-i --list --groups=**
----

List which permissions a particular user or group has in the project by system
administrator role:

----
$ oc policy can-i --list --user=** -n <project>
$ oc policy can-i --list --groups=**  -n <project>
----

Determine if users can have all the combination of verbs and resources from `oc
policy can-i --list [--user|--groups]`

----
$ oc policy can-i <verb> <resource> --[--user|--groups]
----

Test the SCCs with scopes: `oc policy can-i [--user|--groups]`

----
$ oc policy can-i <verb> <resource> [--user|--groups] --scopes=user:info
$ oc policy can-i <verb> <resource> [--user|--groups] --scopes=user:info,role:admin:<namespace>
$ oc policy can-i <verb> <resource> [--user|--groups] --scopes=role:view:*
$ oc policy can-i <verb> <resource> [--user|--groups] --scopes=role:edit:*
$ oc policy can-i <verb> <resource> [--user|--groups] --scopes=role:admin:*
$ oc policy can-i <verb> <resource> [--user|--groups] --scopes=role:admin:*:!
----

Test with the `ignore-scopes` flag in the `oc policy can-i [--user|--groups]` command:

----
$ oc policy can-i <verb> <resource> [--user|--groups] --ignore-scopes=true
----

The lower-level user cannot list project administrator or system administrator
roles:

----
$ oc policy can-i --list --user project admin
$ oc policy can-i --list --user system:admin
----

Check whether a user or a `ServiceAccount` can create a pod:

----
$ oc policy scc-subject-review -f examples/hello-openshift/hello-pod.json
RESOURCE ALLOWED BY
Pod/hello-openshift restricted
----

See
xref:../dev_guide/authorization.adoc#dev-guide-authorization-determining-what-you-can-do[Authorization]
for more information.

[[ocp-35-github-identity-provider-can-optionally-require-a-team]]
==== GitHub Identity Provider Can Optionally Require a Team

Users can now test for GitHub team membership at log in.

There is now a list of one or more GitHub teams to which a user must have
membership in order to authenticate. If specified, only GitHub users that are
members of at least one of the listed teams will be allowed to log in. If this
is not specified, then any person with a valid GitHub account can log in.

See
xref:../admin_solutions/authentication.adoc#admin-solutions-authentication[Authentication]
for more information.

[[ocp-35-storage]]
=== Storage

[[ocp-35-qualification-of-external-dynamic-provisioner-interface-and-third-party-pv]]
====  Qualification of External Dynamic Provisioner Interface and Third-party PV

In {product-title} 3.5, there is now the qualification of the Kubernetes
interface for an external dynamic provisioner so that Red Hat can support a
customer using a third-party storage solution such as
xref:../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#available-dynamically-provisioned-plug-ins[NetApp
Trident].

There is a concept of _in-tree_ and _out-of-tree_ with
Kubernetes storage. Out-of-Tree means that it is not in the Kubernetes source tree
and does not ship in Kubernetes or {product-title}. The ability is provided post-installation.
Many of the third-party storage vendors gravitate towards out-of-tree because it
allows them to ship on their own schedule and own the distribution of their
code.

See
xref:../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#available-dynamically-provisioned-plug-ins[Available
Dynamically Provisioned Plug-ins] for more information.

[[ocp-35-dynamic-provisioner-for-azure-block-storage]]
==== Dynamic Provisioner for Azure Block Storage

Dynamic provisioning is now available for Azure block storage. Just like AWS and
GCE, you declare the Azure cloud provider in the *_cloud-config_* file, and then
create `StorageClasses` with the Azure block storage options and connection
information.

.Configure the Cloud Provider for Azure
----
kubernetesMasterConfig:
  ...
  apiServerArguments:
    cloud-provider:
      - "azure"
    cloud-config:
      - "/etc/azure/azure.conf"
  controllerArguments:
    cloud-provider:
      - "azure"
    cloud-config:
      - "/etc/azure/azure.conf"
----

.Example StorageClass
----
kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
  name: slow
provisioner: kubernetes.io/azure-disk
parameters:
  skuName: Standard_LRS
  location: eastus
  storageAccount: azure_storage_account_name
----

See
xref:../install_config/persistent_storage/persistent_storage_azure.adoc#install-config-persistent-storage-persistent-storage-azure[Dynamic
Provisioning and Creating Storage Classes] for more information.

[[ocp-35-scale]]
=== Scale

[[ocp-35-scalability-enhancements]]
==== Scalability Enhancements for Metrics

With {product-title} 3.5, the default value of the `METRICS_RESOLUTION`
parameter is now `30` (seconds). This change was introduced to better match the
cAdvisor housekeeping interval of 30 seconds
link:https://bugzilla.redhat.com/show_bug.cgi?id=1421834[(BZ#1421834)].

Increasing the `METRICS_RESOLUTION` interval helped achieve better results in
relation to how many pods can be monitored by one set of metrics pods. In
{product-title} 3.5, tests showed that OpenShift metrics collection was stable for
test cases up to 25,000 monitored pods in a {product-title} cluster.

See
xref:../scaling_performance/scaling_cluster_metrics.adoc#scaling-performance-cluster-metrics[Scaling
Cluster Metrics] for more information.

Currently, up to 100 container native storage (CNS) volumes on one trusted
storage pool (TSP) is supported. For more information, see the
xref:../install_config/persistent_storage/persistent_storage_glusterfs.adoc#container-native-storage-recommendations[Persistent
Storage Using GlusterFS].

[[ocp-35-networking]]
=== Networking

[[ocp-35-multicast-support]]
==== Multicast Support

{product-title} 3.5 introduces multicast support. Pods can now send or receive
traffic with other pods subscribed to the same multicast group.

This requires the *ovs-multitenant* plug-in and only works with annotated
namespaces:

----
netnamespace.network.openshift.io/multicast-enabled: "true"
----

Pods in different tenants can subscribe to same multicast group, but cannot see
each other's traffic. Administrator tenant (default project) multicast traffic
does not appear in other projects. Overlay (OVS and tenants) and underlay
(virtual machine and a physical server) multicast traffic never mix.

[NOTE]
====
Multicast is best used for low bandwidth coordination or service discovery and
not a high-bandwidth solution.
====

See
xref:../admin_guide/managing_networking.adoc#admin-guide-networking-multicast[Managing
Networking] for more information.

[[ocp-35-cli-understands-wildcard-routes]]
==== CLI Understand Wildcard Routes

In {product-title} 3.5, there is the added ability to see the subdomain wildcard
routes added in {product-title} 3.4, create them, and edit them using the CLI.

Add the wildcard support. Enable this on the router. The default is `off`:

----
$ oc env dc/router ROUTER_ALLOW_WILDCARD_ROUTES=true
----

Create an application or service, then create the wildcard route:

----
$ oc expose svc service-unsecure --wildcard-policy=Subdomain --name=app --hostname=app.example.com
----

Create an edge, passthrough, or reencrypt route, for example:

----
$ oc create route edge edgeroute --service=service-secure --wildcard-policy=Subdomain --hostname=edge.edgeroute.com
----

Test the route:

----
$ curl --resolve edge2.edgeroute.com:443:$router_ip https://edge2.edgeroute.com -k
----

Support was also added to the xref:../install_config/web_console_customization.adoc#web-console-enable-wildcard-routes[web console].

[[ocp-35-allow-host-claims-to-be-disabled-in-the-router]]
==== Allow Host Claims to be Disabled in the Router

This new feature provides the ability to create claims from different namespaces
on the first directory of the path. The goal is to be able to split an
application into different pods running in different namespaces.

This works by providing a way to disable the host claims is sufficient
(initially). The administrator handles the routes and forbids projects from
manipulating them.

For example:

Create a route in namespace 1 with:

* host name `foo.com`
* path= `/bar`

Create a route in namespace 2 with:

* host name `foo.com`
* path= `/foo`

----
namespace 2 →/bar      <1>
namespace 2 →/         <2>
namespace 2 →/bar/test <3>
----
<1> Should be rejected.
<2> Should be admitted.
<3> Should be admitted.

[WARNING]
====
This is for controlled environments only. If users can create routes, and they
are untrusted, then there is a security concern.
====

[[ocp-35-network-policy]]
==== Network Policy Plug-in (Technology Preview)

Network Policy (currently in xref:ocp-35-technology-preview[Technology Preview])
is an optional plug-in specification of how selections of pods are allowed to
communicate with each other and other network endpoints.

Network Policy works by way of namespace isolation at the network layer using
defined labels. You can also limit connections to specific ports (e.g., only TCP
ports 80 and 443).

----
kind: NetworkPolicy
apiVersion: extensions/v1beta1
metadata:
  name: allow-http-and-https
spec:
podSelector:
ingress:
- ports:
  - protocol: TCP
    - port: 80
    - port: 443
----

After installing the Network Policy plug-in, an annotation must first be set on
the namespace, which flips the namespace from `allow all traffic` to `deny all
traffic`. At that point, you can create `NetworkPolicies` that define what
traffic to allow. The annotation is as follows:

----
$ oc annotate namespace ${ns}
'net.beta.kubernetes.io/network-policy={"ingress":{"isolation":"DefaultDeny"}}'
----

With Network Policy in Technology Preview, not all features are available.
Multi-tenant isolation is not available by default. Currently, it must be
configured by creating default isolation policies for each namespace, and there
is currently no clean path to upgrade or migrate from the multi-tenant plug-in.

See
xref:../admin_guide/managing_networking.adoc#admin-guide-manage-networking[Managing
Networking] for more information.

[[ocp-35-ingress-object-support]]
==== Ingress Object Support (Technology Preview)

In {product-title} 3.5, there is added support for the K8s Ingress object, a set
of rules that allow inbound connections to reach cluster services.

Ingress is disabled in the router, by default. When enabled, Ingress
objects are handled equivalently to routes. The precedence rules apply to both
if they claim the same host name.

[NOTE]
====
To use Ingress, the router must be given permission to read all cluster
secrets.
====

*Example Testing Ingress Object with TLS*

.test-secret.yaml
----
apiVersion: v1
kind: Secret
metadata:
  name: test-secret
data:
  tls.crt: `base64 -w 0 /some/path/tls.crt`
  tls.key: `base64 -w 0 /some/path/tls.key`
----

.test-ingress.yaml
----
$ cat ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  tls:
  - secretName: test-secret
  backend:
    serviceName: test-service
    servicePort: 8080
----

See
link:https://kubernetes.io/docs/concepts/services-networking/ingress/[Ingress
Resources] for more information.

[[ocp-35-installation]]
=== Installation

{product-title} and OpenShift Online operations are now using the same Ansible
upgrade playbooks.

Lots of work around idempotency resulted in an increase in installer and upgrade
stability.

Main features include:

* pre- and post- hooks for master upgrades. Integration points are now added so
that users can perform
xref:../install_config/upgrading/automated_upgrades.adoc#upgrade-hooks[custom
tasks], such as cycling hosts in and out of load balancers during the upgrade
process.
* Open vSwitch (OVS) and etcd version increases.
* Rolling updates of certificates.
* More customization possible during upgrade steps to meet local needs.
* Code refactoring for idempotency.
* Deployment of router shards during installation is now possible. This allows
 administrators to establish swim lanes to specific route shards for labeled
 routes.

////
In 2018, {product-title}'s RPM-based installation process is being deprecated.
Instead, {product-title} will follow a containerized installation process.
link:https://access.redhat.com/articles/2993761[See the Customer FAQ].
////

[[ocp-35-metrics-and-logging]]
=== Metrics and Logging

{product-title} 3.5 includes enhanced Ansible playbooks to better handle
deployments and upgrades. This deprecates the deployer deployment procedure and
replaces it with Ansible in a manner that is more consistent with the
installation of the rest of the product.

Administrators can declare variables in the inventory file to cause playbooks
*_openshift_metrics.yml_* and *_openshift_logging.yml_* to behave differently.
The metrics and EFK stacks can be deployed without requiring Java to be
installed on the master node.

Ansible handles:

* Metrics stack for {product-title} 3.5.
* Fresh deployment of metrics and logging.
* Upgrading of metrics from {product-title} version 3.3 to 3.5 and {product-title}
version 3.4 to 3.5.
* Upgrading of logging from {product-title} version 3.3 to 3.4.
* Re-installation of metrics and logging (`cleanup` and `install`).
* Scaling metrics and logging.

See
xref:../install_config/cluster_metrics.adoc#install-config-cluster-metrics[Enabling
Cluster Metrics] and
xref:../install_config/aggregate_logging.adoc#install-config-aggregate-logging[Aggregating
Container Logs] for more information.

[[ocp-35-developer-experience]]
=== Developer Experience

[[ocp-35-pulling-artifacts-from-remote-resources]]
==== Pulling Artifacts from Remote Resources

Previously, `oc start-build` only allowed a local file to be specified, but did
not allow a URL to a remote resource. Now, users can pull in artifacts
via `oc start-build --from-file=<some URL>`.

This feature only works against GET-based endpoints that do not require
authentication and use either no transport layer security (TLS), or TLS with a
certificate trusted by the client. This feature does not reinvent `curl`. The
file is downloaded by the CLI, then uploaded to the binary build endpoint.

[[ocp-35-setting-env-vars-when-creating-an-app-from-template]]
==== Setting Environment Variables When Creating an Application from a Template

Users now also have the ability to set environment variable when creating an
object (for example, an application) from a template. Previously, this was a
separate step following template creation.

[[ocp-35-support-for-p-parameter-values]]
==== Support for -p Parameter Values

Both `oc new-app` and `oc process` now support `-p` for parameter values. The
`-v` flag is deprecated.

[[ocp-35-ci-cd-pipeline]]
==== CI/CD Pipeline

In {product-title} 3.5, enablement materials regarding use of CI/CD pipelines
with {product-title} are improved. The complexity and number of pipeline samples
provided is increased.

Support is added to `oc new-app` and `oc new-build` so that the commands are
pipeline aware.

.Pipelines Page
image::ocp35-pipelines_page.png[Pipelines Page]

See
xref:../dev_guide/application_lifecycle/promoting_applications.adoc#dev-guide-promoting-applications[Promoting
Applications Across Environments] and
xref:../dev_guide/application_lifecycle/new_app.adoc#dev-guide-new-app[Creating New Applications] for
more information.

[[ocp-35-web-console]]
=== Web Console

[[acp-35-run-and-deploy-on-ocp]]
==== Run and Deploy on {product-title}

In {product-title} 3.5, there is now a "Run on OpenShift" experience that allows
you to provide external links in the web console to deploy templates.

image::ocp35-run-on-ocp-buttons.png[Run on OpenShift Buttons]

Use the URL pattern to select a template or image. You can customize it to have
it come from separate project. The end-user is prompted for the project.

See xref:../dev_guide/create_from_url.adoc#dev-guide-create-from-url[Create From
URL] for more information.

[[ocp-35-web-console-added-service-details]]
==== Added Service Details

There are now added service details on configuration, traffic, routes, and pods.

There is a new section highlighting routes, service and target ports, host name,
and TLS. There is also a section iterating pods and their status.

.Service Details View
image::ocp35-service-details.png[Service Details]

[[ocp-35-web-console-configmap-create-list-detail]]
==== ConfigMap: Create, List, Detail

In {product-title} 3.5, there is now the ability to
easily work with configuration data decoupled from the container image. You can:

* Create new `ConfigMap`
* List out existing `ConfigMaps`
* Work with the configuration details.
* Easily consume them from various other pages.

.Create a ConfigMap
image::ocp-35-create-configmap.png[Create a ConfigMap]

.Add Config Files
image::ocp-35-add-config-files.png[Add Config Files]


[[ocp-35-web-console-show-build-failures]]
==== Show Build Failures

Users no longer have to search logs to gain a better understanding of why build
failed. Individual build status messages are now updated with details that are
available via the web console and the CLI.

.Build Failures as Seen in the Web Console
image::ocp35-build-failure.png[Build Failures in the Web Console]

.Build Failures as Seen in the CLI
image::ocp35-build-failure-cli.png[Build Failures in the CLI]

[[ocp-35-web-console-statefulsets]]
==== StatefulSets (Technology Preview)

Custom resource listing and details pages for `StatefulSets` (formerly known as
`PetSets`) is now available. Users can get details of all `StatefulSets`,
including deployments and replica sets.

See the
xref:../architecture/infrastructure_components/web_console.adoc#web-console-statefulsets[Web
Console] documentation for more information.


[[ocp-35-notable-technical-changes]]
== Notable Technical Changes

{product-title} 3.5 introduces the following notable technical changes.

[discrete]
[[ocp-35-updated-infrastructure-components]]
=== Updated Infrastructure Components

* {product-title} 3.5 is supported on RHEL 7.2 and 7.3 with the latest packages
from Extras, including Docker 1.12.

* {product-title} 3.5 is _not_ certified with older versions of Docker and RHEL 7.1
or earlier.

* Kubernetes has been updated to v1.5.

* etcd has been updated to 3.1.

* Open vSwitch (OVS) was upgraded to 2.6 and the package is now provided via the
Red Hat Enterprise Linux Fast Datapath channel.

[discrete]
[[ocp-35-miscellaneous-changes]]
=== Miscellaneous Changes

* `activeDeadlineSeconds` is now configurable for deployer pods via the deployment
configuration API.

* In {product-title} 3.5, `ScheduledJob` is renamed `CronJob`. If you want to keep
your scheduled jobs, you need to export them from the 3.4 cluster (using `oc
export` or `oc get -o yaml`) and create them again, after the upgrade, on the
3.5 cluster. The storage prefix has changed, along with the name, and newly
created clusters do not know where to look for `ScheduledJob`. Cluster version
3.5 operates on `CronJob`, but it also understands `ScheduledJob` submitted to
it. It performs rapid conversion, saving your newly created object as a
`CronJob`, resulting in all subsequent read operations returning `CronJob`
instead. See xref:../dev_guide/cron_jobs.adoc#dev-guide-cron-jobs[Cron Jobs] for
more information.

* The default value for `ingressIPNetworkCIDR` was previously a non-private range
(`172.46.0.0/16`) and has been changed to a private range (`172.29.0.0/16`).
Clusters configured with the non-private range run the risk of routing issues,
and updating to a private range is advised.
+
[WARNING]
====
When `ingressIPNetworkCIDR` changes, any external IPs allocated from the
previous range will be reallocated from the new range.
====

* The `groups` field in the user object is now deprecated. Instead, create Group
API objects containing the names of the users that are members of the group.

* `oc whoami --token` was deprecated in {product-title} 3.4 in favor of `oc whoami
-t`. Also, `oc whoami --context` is deprecated in favor of `oc whoami -c`. The
`--token` and `--context` options now behave consistently with all other `oc`
commands, indicating the specified token or context should be used.

* `extensions/v1beta1.Job` is deprecated in favor of using `batch/v1.Job`. The
storage should be updated to keep the Jobs readable in future versions of the
cluster. See
xref:../install_config/upgrading/manual_upgrades.adoc#install-config-upgrading-manual-upgrades[Manual
Upgrades] for more information.

* {product-title} 3.5 requires that the `rhel-7-fast-datapath` repository be
enabled.

* Template instantiation now respects namespaces defined in the template objects
(meaning it will create the object in specified namespace) if and only if the
namespace definition uses a parameter reference. Previously, it never respected
the namespace defined in the object.

[[ocp-35-bug-fixes]]
== Bug Fixes

This release fixes bugs for the following components:

*Authentication*

* There was a bug in how policies were listed internally when used to build role
bindings. Filtering of role bindings based on selectors did not work correctly.
With this bug fix, the internal listing of policies was updated to the correct
behavior. As a result, the filtering of role bindings based on selectors now
works as expected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1423215[*BZ#1423215*])

*Builds*

* Source-to-Image builds expect image commits to take no longer than two minutes.
Commits that take longer than two minutes result in a timeout and a failed
build. With this bug fix, the timeout is removed so that image commits can take
indeterminate lengths of time. As a result, commits that take an excessive
amount of time will not result in a failed build.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1391665[*BZ#1391665*])

* The build failure reason was not getting set or saved correctly. Therefore, the
build failure reason was not shown in command output. The code is now updated to
correctly save the build failure reason and the build failure reason now shows
correctly in command output.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1415946[*BZ#1415946*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1419810[*BZ#1419810*])

* Previously, running a custom build with an image containing a Docker binary that
was a different version than the Docker container running on the
{product-title}  node would result in an error. The build would fail with a
message about mismatched Docker API version. Now, you can  set the
`DOCKER_API_VERSION` environment variable in the `BuildConfig` to match the API
version running on the node. For example:
+
----
$ oc set env bc/buildcfg DOCKER_API_VERSION=1.22
----
+
Note that this will only work if the version of the Docker binary on the custom
builder image is newer than the version running on the {product-title}  node.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1422798[*BZ#1422798*])

* The build duration was not being consistently calculated. Therefore, the build
duration displayed in the web console and on the command line was inaccurate.
With this bug fix, the duration of completed builds is now consistently
calculated and a consistent build duration value is reported for builds under
all circumstances.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1318403[*BZ#1318403*])

* Previously, the `oc new-app` command would try to interpret its argument as a
path and would exit with an error when a component of this path existed, but was
not a directory. Running `oc new-app X/Y’ with a file named `X` in the current
directory would cause an exit with an error, even though `X/Y` denotes a valid
Docker image. When `oc new-app` tries to interpret the input component as a
directory and object with that name exists on a file system but is not a
directory, try another possible interpretation instead of exiting with an error.
As a result, running `oc new-app X/Y` creates a new application based on Docker
image X/Y, even in the case when file X exists in the current directory.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1347512[*BZ#1347512*])

* There were different code paths for retrieving and setting the commit
information. Therefore, the `OPENSHIFT_BUILD_COMMIT` environment variable was
only set in the output image when the build was triggered by a webhook. To fix
this issue, use a common code path for retrieving and setting the commit
information so it is always available to be added to the image. As a result of
this bug fix, the `OPENSHIFT_BUILD_COMMIT` environment variable is always
present in the output image.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1408879[*BZ#1408879*])

* Previously, a race condition could cause builds with short-running post-commit
 hooks to hang. This bug fix resolves the issue and builds no longer hang.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1425824[*BZ#1425824*])

* Master returned an internal server error HTTP code when the Docker image lookup
failed due to unreachable registry. This happened for every image lookup in
disconnected {product-title} environments. Therefore, `oc new-app` reported the
internal server error as a warning to the user, which can make the user think
there is something wrong with their {product-title} deployment. Change the
wording of the error `oc new-app` prints to not include the string "internal
server error".  As a result, the warning that is printed does not sound more
severe than it is.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1398330[*BZ#1398330*])

*Command Line Interface*

* The latest version of Docker for Mac/Windows uses the Community Edition
versioning scheme. This causes `oc cluster up` to halt with an error because
the new version cannot be parsed by the `semver` library. This bug fix
changes the behavior to display a warning instead of exiting with an
Error. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1428978[*BZ#1428978*])

* The race condition is seen when updating a batch of nodes in the cluster using
`oc adm manage-node` to be schedulable or unschedulable. Therefore,  several nodes
could not be updated with the "object has been modified" error. Use a patch on
the `unschedulable` field of the node object instead of a full update. With this
bug fix, all nodes can be properly updated as schedulable or unschedulable.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1279303[*BZ#1279303*])

* Previously, the `--overwrite` option for `oc volume` was confusing. This bug fix
improves the `oc set volume --override` flag description so that users
understand that they are not replacing the current volume that is being used.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1319964[*BZ#1319964*])

* Previously, a confusing error message was generated when `oc set probe` was run
without  providing a port with a get-url. With this bug fix, the error is now
formatted to be much more readable to the user.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1332871[*BZ#1332871*])

* The `oc get` command would return the message "No resources found", even in
cases where resources did exist, but could not be retrieved due to a connection
error. The command `oc get` was updated to only show the message "No resources
found" in cases when resources truly did not exist in the server. As a result of
this bug fix, `oc get` no longer displays "No resources found" in cases when
there is an error retrieving resources from the server.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1393289[*BZ#1393289*])

* The new responsive terminal would wrap long lines in the output of CLI commands.
The `oc adm diagnostics` indentation did not work well, and no longer had color in
its output. This bug fix bypasses the responsive terminal in `oc adm diagnostics`
(currently only being used in CLI help output). As a result, `oc adm diagnostics`
now has proper indentation and colorized output.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1397995[*BZ#1397995*])

* Output from the `oc idle` command was confusing to end users. A user could not
easily tell what was being done by the `oc idle` command. With this bug fix, the
output of the `oc idle` command was updated to clarify what the command had done
and is now easier to understand.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1402356[*BZ#1402356*])

* Previously, `oc status` tried to generate a status for the "default" cluster
namespace if a user had not yet created a project after logging in. The user
would see a forbidden status error "cannot get projects in project" when their
context was still in the cluster's "default" namespace after logging in, and did
not have permissions to "LIST" in this namespace. With this bug fix, `oc status`
now checks to see if a user cannot list projects in the default namespace. As a
result, the user no longer sees the error message "cannot get projects in
project <default cluster namespace>" when they execute `oc status` and have no
projects in their current namespace. They instead see a message prompting them
to create a new project, or to contact their administrator to have one created
for them.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1405636[*BZ#1405636*])

* After  running ` oc adm drain -h`, the user would try to open the provided link
`\http://kubernetes.io/images/docs/kubectl _drain.svg`, but would receive a “404
page not found” error. This bug fix corrects an extra space in the link path and
the link now works as expected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1415985[*BZ#1415985*])

* Although a `MasterConfig` load error is stored globally, it is only printed the
first time that it is encountered during a diagnostics check. This bug fix
ensures that, even if the error has already been encountered once, its message
gets printed in subsequent diagnostic checks.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1419472[*BZ#1419472*])

* Deleting an access token using the *OAuthAccessTokens* client would fail for
 users that had logged in using a *serviceaccount* token. A failure from the
 access token client would prevent the token from being deleted from the local
 configuration, causing a user to be unable to log out. With this bug fix, the
 failure is now logged, ensuring that an attempt to remove the token from the
 user's local configuration always takes place. A user is now able to log out
 after logging in with a *serviceaccount* token.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1422252[*BZ#1422252*])

* Tags were not sorted according to `\http://semver.org/` and, therefore, the
"highest" tags were not imported when the image import limit was cutting down
the amount of imported images. With this bug fix, tags are now sorted according
to semantic versioning rules. The "highest" tags are now properly imported, even
when only a limited number of tags is allowed to be imported.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1339754[*BZ#1339754*])

* Previously, the *_.kubeconfig_* file was being generated with a server URL that
did not include a port number. Although the port number was safely assumed to be
`443` with an HTTPS protocol, it prevented the certificate from being
successfully verified during the login sequence (an exact match including the
port was required). Therefore, the user was prompted with the warning "The
server uses a certificate signed by an unknown authority" every time they
attempted to log in using an {product-title} installation completed through
`openshift-ansible`. With this bug fix, the command `oc adm create-kubeconfig`
(used by the `openshift-ansible` playbook) was patched to normalize the server
URL so that it included the port with the server URL in the generated
*_.kubeconfig_* file every time. As a result, the user no longer sees the
message "The server uses a certificate signed by an unknown authority" when
logging in using a *_.kubeconfig_* file generated by an *openshift-ansible*
installation.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1393943[*BZ#1393943*])

* There was a duplicated resource "quota" in the `oc describe` list of valid
resources. Therefore, "quota" was printed twice. This bug fix removes one entry
on "quota" in the `oc describe` list of valid resources. Now, each resource type
is only printed once.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1396397[*BZ#1396397*])

* Multi-line output for a template description did not display all lines with
 correct indentation under `oc new-app`. Therefore, the output for template
 descriptions was hard to read. This bug fix added a new helper function
 `formatString`, which indents all lines for a multi-line template description.
 Template descriptions for `oc new-app <my_template>` are now easier to read.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1370104[*BZ#1370104*])

 * The `.spec.dockerImageMetadata` field was unnecessarily used when patching an
image stream tag. As a consequence, the `oc edit` command could not succeed.
This bug fix modifies the patch mechanism used in `oc edit` to always replace
the contents of the `.spec.dockerImageMetadata` field. As a result, users should
be able to invoke `oc edit` on any image stream tag.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1403134[*BZ#1403134*])

* There was previously no information about the `--generator` parameter explaining
its use in the help output of the `oc expose` command. This bug fix adds an
explanation that gives example usage
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1420165[*BZ#1420165*])

*Containers*

* This enhancement updates the Jenkins examples to remove the need for a slave,
which makes configuration simpler.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1374249[*BZ#1374249*])

*Deployments*

* The rolling updater was not ignoring pods marked for deletion and was counting
them as ready. This bug fix updates the rolling updater to ignore such pods.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1307004[*BZ#1307004*])

*Image*

* This enhancement allows Maven and Node.js slave image paths to be specified
explicitly. Disconnected environments were unable to pull the images from the
hardcoded paths, so `MAVEN_SLAVE_IMAGE` and `NODEJS_SLAVE_IMAGE` environment
variables can now be used to control where to pull the images from, overriding
the hardcoded defaults.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1397260[*BZ#1397260*])

*Image Registry*

* The OpenShift Container Registry (OCR) was not able to handle forwarded headers
provided by an HAProxy in front of it, making it unusable when exposed on
insecure port 80. Pushes failed because the registry generated incorrect URLs.
An upstream fix has been backported to the OCR. As a result, the OCR now handles
forwarded headers and it is usable again when exposed on an insecure port.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1383439[*BZ#1383439*])

* The master API previously investigated the incorrect object when determining the
docker image reference of a new image stream mapping when the referenced image
already existed. This created image stream tags containing misleading
information about an image's location, pointing to the original image stream.
This bug fix updates the master API to now properly determine docker image
references for new image stream mappings. As a result, image stream tags now
show proper docker image references pointing to managed images.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1408993[*BZ#1408993*])

* The OpenShift Container Registry (OCR) did not consider insecure import policies
of image stream tags when deciding whether to fall back to insecure transport
when serving blobs from external registries. This meant images imported from
external insecure (no HTTPS or a bad certificate) with an `--insecure` flag
applied could not be pulled through the OCR. With this bug fix, the OCR now
considers the insecure import policy of image stream tags where the requested
image is tagged. As a result, the OCR allows serving images from insecure
external registries if they are tagged with an insecure import policy.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1421954[*BZ#1421954*])

*Kubernetes*

* Using `hostPath` for storage could lead to running out of disk space, and the
root disk could become full and unusable. This bug fix adds support for pod
eviction based on disk space. If a pod using `hostPath` uses too much space, it
may be evicted from the node.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1349311[*BZ#1349311*])

* Horizontal pod autoscalers (HPAs) would fail to scale when it could not retrieve
metrics for pods matching its target selector. Therefore, dead pods and newly
created pods would cause HPAs to skip scaling. This bug fix adds logic to the
HPA controller which assumes conservative metric values, depending on the state
of the pod and the direction of the scale, when metrics are missing or pods are
marked as unready or not active. As a result, newly created or dead pods will no
longer block scaling.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1382855[*BZ#1382855*])

* Previously, pod evictions due to disk pressure did not resolve until the pod was
deleted from the API server. This bug fix causes local storage to be freed on
pod termination (i.e., eviction) rather than pod deletion.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1390963[*BZ#1390963*])

* Previously, I/O could be saturated on a node due to the collection of
per-container disk stats from a thin pool with a large amount of metadata. This
bug fix disables the collection of these statistics until such time that an
efficiently way to collect them can be found.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1405347[*BZ#1405347*])

* Previously, docker could refuse to start new containers due to reaching
`dm.min_free_space` (default 10%), but the devicemapper thin pool usage did not
exceed `image-gc-high-threshold` (default 90%), so the image reclaim occurred
and the node was stuck. This bug fix changes the default
`image-gc-high-threshold` to 85%, which causes image reclaim to occur before the
default `dm.min_free_space` is reached.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1408309[*BZ#1408309*])

* The kubelet previously had a fixed constant for how long it would tolerate the
docker daemon being down before reporting the node as `NotReady`. That was
previously set to 5 minutes, which meant that it could take up to 5 minutes for
the kubelet to report it was no longer ready. This bug fix introduces new
behavior so that the kubelet will wait 30 seconds for the container runtime to
be down before reporting the node as `NotReady`. As a result, the node now
reports `NotReady` faster when the docker daemon is down.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1418461[*BZ#1418461*])

* The `oc adm drain --force` command was ignoring any pods that indicated they
were managed by a daemonset even if the managing daemonset was missing. This bug
fix updates the command to detect when a daemonset pod is orphaned and warn
about the missing daemonset rather than generating an error. As a result, the
command removes orphaned daemonset pods.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1424678[*BZ#1424678*])

* When attempting to connect to etcd to acquire a leader lease, the master
controllers process only tried to reach a single etcd cluster member even if
multiple were specified. If the selected etcd cluster member was unavailable,
the master controllers process was unable to acquire the leader lease and would
not start up and run properly. This bug fix updates this process to attempt to
connect to all of the specified etcd cluster members until a successful
connection is made. As a result, the master controllers process can acquire the
leader lease and start up properly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1426733[*BZ#1426733*])

* Excessive logging to the journal caused masters to take longer to restart. This
bug fix reduces the amount of logging that occurs when initial list/watch
actions happen against etcd. As a result, the journal is no longer pegged with a
lot of messages that cause logging messages to be rate limited and dropped.
Server restart time should be improved on clusters with larger data sets.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1427532[*BZ#1427532*])

* OpenShift Container Platform nodes configured with OpenStack as the cloud
provider could previously move into `NotReady` state if contact with the
OpenStack API was lost. With this bug fix, nodes now remain in `Ready` state
even if the OpenStack API is not responding. Note that a new node process
configured to use OpenStack cloud integration cannot start without the OpenStack
API being responsive.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1400574[*BZ#1400574*])

* The admission plug-in `LimitPodHardAntiAffinityTopology` has been disabled by
default. Enabling it by default previously caused conflict with one of the end
to end tests.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1413748[*BZ#1413748*])

*Logging*

* The Diagnostic Tool (`oc adm diagnostics`) now correctly reports the presence of
the `logging-curator-ops` pod. The `logging-curator-ops` was not in the list of
pods to investigate, resulting in an error that indicated the pod was missing.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1394716[*BZ#1394716*])

* Switching between indices in the Kibana UI now displays the appropriate log
entries. Because default field mappings were being applied in Elasticsearch, the
user might receive the `Apply these filters?` error message.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1426061[*BZ#1426061*])

*Web Console*

* The *Browse* tab now shows the local host name of a service.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1395821[*BZ#1395821*])

* On a project’s *Settings* tab, the Quota terminating scope descriptions are not
clear. The spinning icon on the *Browse* tab that indicates a pod is running no longer
appears jittery. In some browser/operating system combinations, font and
line-height issues could make a spinning icon wobble. Those issues have been
corrected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1365301[*BZ#1365301*])

* A link to documentation on using persistent volumes was added to the *Create
 Storage* page.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1367718[*BZ#1367718*])

* If the web console encounters an error updating Hawkular Metrics charts, the
console will automatically attempt to update again. If the error(s) persist, the
web console will show an alert at the top of the page with a *Retry* link.
Previously, the user would need to reload the browser if an update error
occurred.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1388493[*BZ#1388493*])

* On the web console *About* page, the user can copy the CLI code to log into
{product-title} using the current session token. The token is now permanently hidden
and the web console now appends the user token if the user copies the CLI
example using the *Copy to Clipboard* button.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1388770[*BZ#1388770*])

* The web console now displays any Kubernetes `StatefulSet` objects (formerly
called `PetSets`) in a project with the same level of detail as other resources.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1393202[*BZ#1393202*])

* On the *Create Secret* page, if the user uploads a file that is not a properly
formed file, the *Create* button will now be disabled. Previously, the *Create*
button was enabled if an improper file was uploaded.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1400775[*BZ#1400775*])

* The screen to edit a JSON-formatted template in YAML format now displays the
entire template file in YAML. Previously, because of space restrictions, some of
the JSON formatting would not be converted to YAML.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1402260[*BZ#1402260*])

* When a build is in the *Pending* state, the *Duration* time will not be
calculated. The duration time starts when the build changes to *Running*. This
change was made to prevent negative duration times that could arise from
differences in the browser clock time and the server clock time.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1404417[*BZ#1404417*])

* Previously, under specific circumstances, a single build could appear twice in
the *Overview* page of the web console. The web console now correctly lists each
specific build one time on the *Overview* page.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1410662[*BZ#1410662*])

* In the JVM console, for Apache Camel diagrams, the *Breakout suspended at*
slideout window can be closed and appears only when a breakout is suspended.
Previously, the window could not be closed, which could prevent the user from
selecting Camel route elements.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1411296[*BZ#1411296*])

* The web console now validates deployment controller and replication controller
memory limits that are specified in kB. Previously, validation of memory units
in kB would incorrectly fail as being too small for the limit range. This
happened only for kB, and not other memory units.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1413516[*BZ#1413516*])

* The links to the documentation in the web console now point to the correct
product. Previously, the links led to the OpenShift Origin documentation.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1426061[*BZ#1426061*])

* When editing a deployment configuration (DC) through the web console, the memory
unit is properly retained. Previously, the requested memory was not retained.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1413842[*BZ#1413842*])

* Project display names that contain less than (<) and greater than (>) characters
always display in the *Choose Existing Project* list. Previously, if a display
names contained these characters in a way that mimicked HTML (such as:
`<displayname>`) would result in the display name not appearing or not appearing
correctly in the list.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1414195[*BZ#1414195*])

* Client-side validation for persistent volume claim limit ranges has been added
to the "Create Storage" page in the web console allowing the user to specify
minimum and maximum values for capacity.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1414229[*BZ#1414229*])

* When using self-defined stage names for a pipeline, the `stage` parameter must
include a block argument, for example: `stage('build is the greatest stage') {}`
in the Jenkinsfile.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1414661[*BZ#1414661*])

* Name validation in the web console is now consistent with the CLI. Periods are
now allowed in the names, and the maximum length has been increased to 253
characters. Previously, the validation in the web console was more strict than
in the CLI. Validation has been relaxed for the following forms in the web
console to match the command line:
** Add Autoscaler
** Add Storage
** Create Config Map
** Create Route
** Create Secret
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1414691[*BZ#1414691*])

* In the JVM Console, the *Preference* button in the User page of the JVM Console
 has been added back to the interface. Previously, the *Preferences* button was
 missing.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1415463[*BZ#1415463*])

* In the web console, when deploying an application based on an image, the *Next
Step* page correctly appears. Previously, the web console would incorrectly
redirect to the *Overview* page.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1415602[*BZ#1415602*])

* The web console now displays an error message when a user with an unauthorized
role tries to grant the `serviceaccount:builder` role to a user. Previously, the
web console did not display an error message.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1420247[*BZ#1420247*])

* If you accessed the *Build Configuration* edit page using the page URL, the
 *Create New Secret* button correctly appears. Previously, if you accessed the
 edit page using the URL, the *Create New Secret* button would not appear.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1421097[*BZ#1421097*])

* Logs in the web console for a pod with multiple containers have been fixed to
address a situation where it was possible for log output from more than one
container to appear.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1427289[*BZ#1427289*])

* The pod metrics graph for CPU in the web console would not render if there is
zero CPU activity. Previously, the graph line would not connect to the zero
baseline.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1427360[*BZ#1427360*])

* On the *Application Deployment* page, the annotations associated with the
deployment might appear truncated in the *Show Annotations* list, if the
annotation is too long. Click the *See All* button to display the full
annotation or *Collapse* to hide the truncated section of the annotation.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1233511[*BZ#1233511*])

* In the web console, environment variables in the build file are no longer
truncated after the `=` character. Previously, the environment variable values
that contained an `=` character were being truncated.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1357107[*BZ#1357107*])

* In the pod metrics page, the donut chart for current usage now appears to the
right of the metrics sparkline. The new position allows you to see more metric
data on the screen. Previously the donut chart was above the sparkline.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1387286[*BZ#1387286*])

* Previously, some changes to a health check command or deployment hook command in
the web console would not be saved. This happened when editing an existing
command and adding or removing a single argument. The web console has been fixed
to correctly save all edits to health check and deployment hook commands.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1411258[*BZ#1411258*])

* Previously, you had to enter weights between `0` and `256` as integer values.
When creating or editing routes that send traffic to two services in the web
console, you can now specify the service weights as percentages using a slider
control. You can still enter integer weights if desired.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1416882[*BZ#1416882*])

* On the *Add to Project* page of the web console, if you entered an invalid
setting for some advanced options, then hide the advanced options, the form
would be submitted with invalid values, causing errors when creating some
resources like horizontal pod autoscalers. The web console has been changed to
correctly validate these fields so that you cannot submit the form with invalid
values. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1419887[*BZ#1419887*])

* Previously, the link to download the OpenShift CLI linked to the incorrect
version (of OpenShift Origin). The link has been updated, and the link downloads
the correct version.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1421949[*BZ#1421949*])

* Previously, the *Create a Secret* and *Add Config Files* buttons when creating
using the web console mistakenly linked to the other page. The buttons have been
corrected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1425728[*BZ#1425728*])

* Dates now use the word form over the number form to avoid ambiguity (For example,
May 4, 2016 instead of 05/04/2017).
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1333101[*BZ#1333101*])

* Previously, dropdown menus on the web console overlay the navigation menu
dropdowns, blocking the view and usability of the navigation menu dropdowns. The
navigation menu dropdown’s z-index has been set to a value greater than that of
page content dropdowns, resulting in navigation menu dropdowns to always appear
on top of page content dropdowns.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1366090[*BZ#1366090*])

* A DOM element under the label filter component was being removed during certain
navigation situations, preventing the *Clear Filters* link from appearing until
the browser was refreshed. The correct element is now removed under these
navigation situations meaning the *Clear Filters* link will always appear when
any label filters are active.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1375862[*BZ#1375862*])

* When using the *Deploy Image* tab from the *Add to Project* page, changing the
name input value no longer causes the displayed image name to change. The
correct image name is now displayed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1403097[*BZ#1403097*])

*Metrics*

* Previously, the Heapster image and pod did not specify the user it should be run
under and defaulted to using the root user. If the user is running with the
`MustRunAsNonRoot` SCC, then it would fail since it its not allowed to be run as
a root user. This bug fix ensured it would specify a default user for the
Heapster image meaning users can run with the `MustRunAsNonRoot` SCC without
issues. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1393103[*BZ#1393103*])

* The Hawkular Metrics log data was missing the date in its timestamps. This bug
fix enables the timestamps in the logs.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1423014[*BZ#1423014*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1427666[*BZ#1427666*])

* Previously, JDK and Cassandra could not determine the filesize for extremely
large filesystems, such as EFS, because Cassandra tries to and read the
filesystem size when it configures itself, but notices the invalid size and fail
to start properly. Cassandra has been patched to work around the failure
encountered and will be able to start on systems that are using extremely large
filesystems.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1418748[*BZ#1418748*])

*Networking*

* Previously, wildcard route support was not exposed in the CLI. This fix enables
 support, meaning you can now create wildcard routes in the CLI.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1391786[*BZ#1391786*])

* Previously, unidling connections could time out if the pod took longer than 30s
to start, because clients had connections closed with no data. The timeout has
been increased to 120 seconds so that slow pods do not break clients.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1416037[*BZ#1416037*])

* To be consistent with edge routes, this bug fix makes is possible to configure
insecure termination for all types of routes from the CLI.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1403155[*BZ#1403155*])

* This bug feature adds an environment variable to configure haproxy router
logging facility, so that the syslog facility can be set. Now, users can
separate log traffic as desired.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1419127[*BZ#1419127*])

* Previously, the CIDR for multicast addresses was incorrect. Leading to
 addresses that were in the mis-claimed portion being treated incorrectly, as
 multicast would not work. This fix allows the range to be the IETF assigned one
 (per RFC 5771), meaning that addresses that were in the wrong portion of the
 range now work.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1420032[*BZ#1420032*])

*REST API*

* Previously, there was a code difference with the code used to build the root
etcd prefix between etcdv2 and etcdv3. This resulted in, when migrating from
etcdv2 to etcdv3, the cluster not being able to find any data if a root etcd
prefix was used that did not start with a "/" (which is the default case for
OpenShift). Now, the same code is used to build the root etcd prefix for both
etcdv2 and etcdv3, meaning that after a migration, the cluster is able to find
migrated data as expected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1393744[*BZ#1393744*])

*Routing*

* The max connection was too low, causing the pod to restart. With this fix,
the default value of the connection was increased. As a result, the pod does not
restart.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1405440[*BZ#1405440*])

* Previously, if you created two ipfailover instances and had them run on the same
node, it would fail because both would to use hostPort 1985. This was corrected
by using the ServicePort as a mechanism to prevent multiple pods for same
configuration from starting on the same node.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1411501[*BZ#1411501*])

* Previously, as routers were removed, the route status was not regularly cleared.
This fix added a script to clean out the defunct route status, and documented
expectations of operators. As a result, route statuses are clear and correct.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1356819[*BZ#1356819*])

* Previously, permissions would reset to preset values on a periodic basis causing
the scripts to lose execute permissions. This fix set the correct preset value
in the RC.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1408172[*BZ#1408172*])

* Previously, default host name generation did not take into account that routes
could have the "." character. Therefore when a generated host name was used for
a route that included a "." in the name, and had allowed `wildcardpolicy`, there
would be an extra subdomain. This fix changed the host name generator to replace "."
in a route's name to "-" in the generated host name. As a result, generated host
names cannot create additional subdomains.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1414956[*BZ#1414956*])

* To match user expectations, this feature makes the default for routes with
multiple active services be round-robin. Without this feature, users needed to
set an annotation on a route as well as weights to make it behave correctly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1416869[*BZ#1416869*])

* Previously, re-encryption routes were not correctly supporting redirect access
from HTTP to HTTPS. As a result, it was not possible to set a re-encrypt routes
insecure termination policy to redirect. The HAproxy template file was edited to
correctly implement redirect as a valid insecure termination policy for redirect
routes. Now re-encrypt routes can be configured to redirect HTTP to HTTPS
traffic.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1317159[*BZ#1317159*])

*Storage*

* Previously, the Azure provisioner was not enabled, causing a failure to
provision Azure disks. This fix enabled the Azure provisioner. As a result, it
is now able to provision Azure disks.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1415466[*BZ#1415466*])

* Previously, {product-title} used the wrong `InstanceID` for checking that
volumes were attached to nodes, causing it to think that a volume was detached
while it is still attached. This resulted in volumes remaining attached when
they were not needed, and unable to be deleted according to their reclaim
policy. With this fix, {product-title} now uses the right `InstanceID` for all
attach, detach, and check operations. And as a result, volumes are detached and
deleted when they are not needed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1420645[*BZ#1420645*])

* Previously, *ceph-common* packages were not installed in the infra container,
causing failure to provision Ceph RBD volumes. With this fix, *ceph-common*
packages are installed in the infra container. As a result, Ceph RBD volumes now
provision correctly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1420698[*BZ#1420698*])

* Previously, the AWS device IDs were incorrect. This caused failure to attach EBS
volume due to `InvalidParameterValue` for the parameter device. This fix updated
the AWD device IDs, and as a result, the EBS volume is successfully attached.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1422457[*BZ#1422457*])

* Previously, {product-title} contained a race condition in NFS recycler handling.
This caused some pods to fail to start, and failed to recycle the corresponding
NFS share when recycler pods for multiple NFS shares were started at the same
time. With this fix, the race condition was corrected. As a result, all
scheduled NFS recycler pods are started and NFS shares are recycled.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1392338[*BZ#1392338*])

* Previously, the device name provided by Cinder was being used for volume
mounting into a pod, however, the device name provided by Cinder is unreliable
for the actual mounting. This caused some Cinder volumes to fail to be mounted
into a pod, and resulted in an inconclusive message to appear in the logs. This
fix enables a detection to be performed using the Cinder ID. As a result, Cinder
volumes are reliably being mounted into appropriate pods.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1408867[*BZ#1408867*])

* Previously, the same iSCSI device could not successfully be used by multiple
pods on same node. When one pod would shut down, the iSCSI device for the other
pod would be unavailable. The code was changed with this fix. As a result, the
iscsi device are successfully run.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1426778[*BZ#1426778*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1426775[*BZ#1426775*])

* Previously, if a mount was in progress and pod was deleted, the pod would fail
to be cleaned up properly. This meant the pod was left with volumes attached to
the node. This fix makes  sure that the pending operation is completed before
volume is unmounted from node. As a result, the pod gets cleaned up properly
even if mount was in flight when deletion request is received.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1432949[*BZ#1432949*])

[[ocp-35-technology-preview]]
== Technology Preview Features

Some features in this release are currently in Technology Preview. These
experimental features are not intended for production use. Note the
following scope of support on the Red Hat Customer Portal for these features:

https://access.redhat.com/support/offerings/techpreview[Technology Preview
Features Support Scope]

The following new features are now available in Technology Preview:

- xref:ocp-35-kubelet-collection-of-node-attriutes-for-scheduling-considerations[Kubelet Collection of Node Attributes for Scheduling Considerations]
- xref:ocp-35-statefulsets[`StatefulSets`]
- xref:ocp-35-application-service-cert-regeneration[Application Service Certificate Regeneration]
- xref:ocp-35-network-policy[Network Policy Plug-in]
- xref:ocp-35-kubelet-collection-of-node-attriutes-for-scheduling-considerations[Kubelet Collection of Node Attributes for Scheduling Considerations]
- xref:ocp-35-ingress-object-support[Ingress Object Support]
- xref:../architecture/core_concepts/containers_and_images.adoc#init-containers[Init
containers]

The following features that were formerly in Technology Preview from a previous
{product-title} release remain in Technology Preview:

- xref:../dev_guide/deployments/kubernetes_deployments.adoc#dev-guide-kubernetes-deployments-support[Kubernetes
Deployments Support]
 -xref:../admin_guide/managing_pods.adoc#managing-pods-poddisruptionbudget[Pod Distribution Budgets]
- xref:../dev_guide/cron_jobs.adoc#dev-guide-cron-jobs[Cron Jobs (formerly called Scheduled Jobs)]

See more details on xref:ocp-35-miscellaneous-changes[technical changes related
to Cron Jobs] in {product-title} 3.5.

[[ocp-35-known-issues]]
== Known Issues

- In {product-title} 3.4, the master connected to the etcd cluster using the host
name of the etcd endpoints. In {product-title} 3.5, the master now connects to
etcd via IP address. When configuring a cluster to use proxy settings, this
change causes the master-to-etcd connection to be proxied as well, rather than
being excluded by host name in each host's `NO_PROXY` setting.
+
Workarounds for setting the IP addresses manually in each host's `NO_PROXY`
setting are documented in the installation and upgrade steps. The installer will
be updated in a future release to handle this scenario automatically during
installation and upgrades. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1466783[*BZ#1466783*])

[[ocp-35-asynchronous-errata-updates]]
== Asynchronous Errata Updates

Security, bug fix, and enhancement updates for {product-title} 3.5 are released
as asynchronous errata through the Red Hat Network. All {product-title} 3.5
errata is https://access.redhat.com/downloads/content/290/[available on the Red
Hat Customer Portal]. See the
https://access.redhat.com/support/policy/updates/openshift[{product-title}
Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account
settings for Red Hat Subscription Management (RHSM). When errata notifications
are enabled, users are notified via email whenever new errata relevant to their
registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming
{product-title} entitlements for {product-title} errata notification
emails to generate.
====

This section will continue to be updated over time to provide notes on
enhancements and bug fixes for future asynchronous errata releases of
{product-title} 3.5. Versioned asynchronous releases, for example with the form
{product-title} 3.5.z, will be detailed in subsections. In addition, releases in
which the errata text cannot fit in the space provided by the advisory will be
detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[upgrading your cluster] properly.
====

[[ocp-3-5-rhba-2017-0903]]
=== RHBA-2017:0903 - atomic-openshift-utils Bug Fix and Enhancement Update

Issued: 2017-04-12

{product-title} bug fix and enhancement advisory
link:https://access.redhat.com/errata/RHBA-2017:0903[RHBA-2017:0903], providing
updated *atomic-openshift-utils*, *ansible*, and *openshift-ansible* packages
that fix several bugs and add enhancements, is now available.

Space precluded documenting all of the bug fixes and enhancements for this
release in the advisory. See the following sections for notes on upgrading and
details on the bug fixes and enhancements included in this release.

[[ocp-3-5-rhba-2017-0903-upgrading]]
==== Upgrading

To apply this update, run the following on all hosts where you intend to
initiate Ansible-based installation or upgrade procedures:

----
# yum update atomic-openshift-utils
----

[[ocp-3-5-rhba-2017-0903-bug-fixes]]
==== Bug Fixes

* When CloudFront was enabled, the installer did not use the private key for the registry, and the registry failed to deploy successfully. This bug fix adds new steps to ensure the private key creates a secret and attaches to the CloudFront registry. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1395168[*BZ#1395168*])

* Previously, the facts generation procedures may have incorrectly determined major release versions prior to package installation. Because the playbooks are now version specific, this defaulting has been eliminated, ensuring that OpenShift Container Platform 3.5 playbooks receive 3.5 content in all scenarios. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1395637[*BZ#1395637*])

* OpenShift Container Platform 3.4 and 3.3 introduced a requirement on the `conntrack` executable, but this dependency was not enforced at install time. This made it possible for service proxy management to fail post installation. This bug fix updates the installer to ensure that `conntrack` is installed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1420182[*BZ#1420182*])

* An Ansible release introduced a regression that caused datastructures to fail to serialize when writing them out to a YAML document. Users would trigger the regression during the pre-run fact fetching, causing their installation to crash. Ansible introduced a new YAML serializing system in an update. The old serializing system was replaced with the new one, `AnsibleDumper`. As a result, the quick installer can run the "Gathering information from hosts" actions now without triggering the error during serializing. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1420970[*BZ#1420970*])

* Previously, if `ansible_user` was a Windows domain user in the format of `dom\user`, the installation playbooks would fail. This bug fix escapes this user name properly, ensuring playbooks run successfully. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1426703[*BZ#1426703*])

* In containerized environments, the CNI data directory located at *_/var/lib/cni_* was not properly configured to persist on the node host. This bug fix updates the installer to ensure that pod IP allocation data is persisted when restarting containerized nodes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1427789[*BZ#1427789*])

* The command line option that flags unattended mode was not being checked when the scaleup routine was ran, and users would be prompted to enter host information. This bug fix ensures the unattended mode flag is checked during the scaleup routine. As a result, users are kicked out and given instructions on how to continue if the unattended mode flag is set during a scaleup run. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1390135[*BZ#1390135*])

* A `when` clause was present on the *firewalld* service installation task, causing the installation to be skipped for *firewalld* when running a containerized install. This bug fix removes the `when` clause from the *firewalld* installation task, and as a result *firewalld* is installed properly when running a containerized install. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1413447[*BZ#1413447*])

* A custom systemd unit file was used for the *docker* service specifying `Requires=iptables`. This resulted in the *docker* service being restarted when *iptables* was restarted. This bug fix updates the custom systemd unit file to specify `Wants=iptables`. This will still ensure that *iptables* is started before *docker*, but will not cause *docker* to restart when *iptables* is restarted. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1416156[*BZ#1416156*])

* Re-running the installation playbook with `openshift_hosted_logging_deploy=true` would redeploy the logging deployer pod using the install mode and would fail because logging was already installed. The Ansible playbook fails due to waiting on the deployer pod to complete successfully. In {product-title} 3.5, the logging deployer pod is no longer used to install logging, but rather the `openshift_logging` role. As a result, it is able to handle previously installed logging, and the playbook now completes successfully. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1417525[*BZ#1417525*])

* The fact `etcd_is_atomic` was detected incorrectly due to the role ordering of some fact setting operations. Atomic Host systems do not support yum, repoquery, and rpm commands; Atomic Host systems would attempt to run commands specific to managing and inspecting repositories and packages when they should not. This bug fix changes the ordering of role calls and fact updates and wrapped in a meta-role to ensure they stay in the correct order. As a result, Atomic Host systems will no longer attempt to run the problematic commands, because the `etcd_is_atomic` fact is now correctly detected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1427067[*BZ#1427067*])

* Previously, *atomic-openshift-docker-excluder* was disabled before the *docker* installation, and *docker* could be installed with newer version that is not compatible. This bug fix enables *atomic-openshift-docker-excluder* during the *docker* installation so that *docker* is installed with a version that is compatible. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1430612[*BZ#1430612*])

* Previously, *atomic-openshift-excluder* was not enabled after installation, meaning OpenShift Container Platform components were not protected from accidental package updates. This bug fix enables *atomic-openshift-excluder* correctly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1430613[*BZ#1430613*])

* The example inventories were incorrect for the logging public master URL, and `loggingPublicURL` was not being set as expected. This bug fix updates the example inventories to be accurate for the new logging role. As a result, `loggingPublicURL` is correctly set as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1430628[*BZ#1430628*])

* Previously, the *atomic-openshift-excluder* and *atomic-openshift-docker-excluder* packages were not acknowledged during node or master scale-up. This meant that these excluder packages were not installed on new nodes or masters. This bug fix ensures that the excluder packages are installed on new nodes and masters as well. As a result, the excluder packages are installed on new nodes and masters when scaling up. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1432868[*BZ#1432868*])

* The quick installer used a system for counting the number of plays in a run that was not 100% accurate due to conditional play includes. The reported number of plays could be bigger or smaller than the original estimate. With this bug fix, at the end of the playbook run, the installer now explains why the actual play count may be different than the estimate. Users still have an idea of about how far along their install is and if the number of tasks do not match the original estimate they understand why. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1388739[*BZ#1388739*])

* A bug was fixed in the *openvswitch* upgrade to v2.6 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1420636[*BZ#1420636*])

* Automatic migration is not possible from `PetSets` to `StatefulSets`. An additional validation step was added to the pre-upgrade validation playbook. `PetSets` are searched for in the cluster and if any existing `PetSets` are detected, the installation errors and quits. The user is given an information message (including documentation references) describing what went wrong, why, and what the users choices are for continuing the upgrade without migrating `PetSets`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1428229[*BZ#1428229*])

* In some situations, node upgrade could terminate a running pod that was upgrading the router or registry, causing the router or registry to fail to upgrade. The router and registry upgrade is now re-ordered to follow node upgrade when performing a full cluster in-place upgrade. As a result, nodes are no longer taken offline for upgrade while the router or registry is still running. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1395081[*BZ#1395081*])

* Previously, an error in the upgrade playbooks prevented ansible from detecting when a host had successfully been rebooted. This error has been corrected and upgrades that use `openshift_rolling_restart_mode=system` now work properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1421002[*BZ#1421002*])

* The *atomic-openshift-excluder* and *atomic-openshift-docker-excluder* packages are now updated to the latest available packages when upgrading OpenShift Container Platform. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1426070[*BZ#1426070*])

* The *atomic-openshift-docker-excluder* package was not updated during containerized cluster upgrades. If this package was not up to date, the cluster was not protected from accidentally upgrading to an unsupported *docker* version. This bug fix ensures the package is now updated during containerized upgrade. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1430700[*BZ#1430700*])

* Previously, *tar* was not listed as a dependency for the installer. On systems where *tar* was not part of the base system, the installer could fail. This bug fix adds *tar* as a dependency, and as a result the installer is now able to use *tar* during installations and upgrades. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1388445[*BZ#1388445*])

* The upgrade plays were updating the excluder packages on Atomic Host systems. This caused the plays to fail as the excluders are not supported on Atomic Host. This bug fix skips excluders on Atomic Host systems, and as a result these plays no longer fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1431077[*BZ#1431077*])

* The default *docker* log driver has been changed to *journald* in order to provider higher performance and lower logging latency. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1388191[*BZ#1388191*])

* Previously, the file specified in `openshift_master_ca_certificate` was not deployed when performing a master scaleup. The scaleup playbooks have been updated to ensure that this certificate is deployed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1427003[*BZ#1427003*], link:https://bugzilla.redhat.com/show_bug.cgi?id=1426677[*BZ#1426677*])

* When using `grep` to find `pluginOrderOverride` within the *_/etc/origin/master/master-config.yaml_* file, if the string was not found the task failed, causing the playbook to fail. This bug fix updates the task to no longer fail if the RC != 0 (the string was not found). As a result, if the string is missing, the playbook no longer fails and instead continues to run as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1425400[*BZ#1425400*])

* Previously, the registry certificate was not properly updated when running the certificate re-deploy playbooks, which may have prevented pushing or pulling images. This bug fix updates the playbooks to ensure that the registry certificate is correctly updated. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1418191[*BZ#1418191*])

* Previously, the installer may have failed to add *iptables* rules if other *iptables* rules were being updated at the same time. This bug fix updates the installer to wait to obtain a lock when updating *iptables* rules, ensuring that rules are properly created. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1415800[*BZ#1415800*])

* The certificate re-deploy playbooks have been updated to ensure all internal certificates have been updated and, when possible, the update is done in a rolling manner, preventing downtime. See xref:../install_config/redeploying_certificates.adoc#install-config-redeploying-certificates[Redeploying Certificates] for the latest information on updating certificates. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1397958[*BZ#1397958*])

* The registry console deployment now allows both the prefix and version to be specified if the user needs to reference an alternate registry or specific version of the registry console. These values may be configured by setting, for example, `openshift_cockpit_deployer_prefix='registry.example.com/openshift'` and `openshift_cockpit_deployer_version='3.5.0'`, which would result in use of `registry.example.com/openshift/registry-console:3.5.0`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1383275[*BZ#1383275*])

[[ocp-3-5-rhba-2017-0903-enhancements]]
==== Enhancements

* This enhancement enables the `gitNoProxy` default value for builds to be set explicitly via the installer. Previously, the value was inherited from the `no_proxy` settings for the installer, which was insufficiently configurable. The build default `gitNoProxy` value can now be directly controlled by the `openshift_builddefaults_git_no_proxy` setting. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1384753[*BZ#1384753*])

* This enhancement adds the new option `openshift_docker_selinux_enabled`. This allows users to override the default installation *docker* options setting of `--selinux-enabled`. Placing `openshift_docker_selinux_enabled=false` in user inventory file will remove the `--selinux-enabled` docker option. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1392742[*BZ#1392742*])


[[ocp-3-5-5-8]]
=== RHBA-2017:1129 - {product-title} 3.5.5.8 Bug Fix and Enhancement Update

Issued: 2017-04-26

{product-title} release 3.5.5.8 is now available. The list of packages, bug
fixes, and enhancements included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:1129[RHBA-2017:1129] advisory.
The list of container images included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:1130[RHBA-2017:1130] advisory.

[[ocp-3-5-5-8-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.4 or 3.5 cluster to this latest release, use the
automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-5-5-15]]
=== RHBA-2017:1235 - {product-title} 3.5.5.15 Bug Fix Update

Issued: 2017-05-18

{product-title} release 3.5.5.15 is now available. The list of packages and bug
fixes included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:1235[RHBA-2017:1235] advisory.
The list of container images included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:1236[RHBA-2017:1236] advisory.

[[ocp-3-5-5-15-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.4 or 3.5 cluster to this latest release, use the
automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-5-5-24]]
=== RHBA-2017:1425 - {product-title} 3.5.5.24 Bug Fix Update

Issued: 2017-06-15

{product-title} release 3.5.5.24 is now available. The packages and bug fixes
included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:1425[RHBA-2017:1425] advisory.
The container images included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2017:1426[RHBA-2017:1426] advisory
and listed in xref:ocp-3-5-rhba-2017-1425-images[Images].

Space precluded documenting all of the bug fixes and images for this release in
the advisory. See the following sections for notes on upgrading and details on
the bug fixes and images included in this release.

[[ocp-3-5-5-24-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.4 or 3.5 cluster to this latest release, use the
automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-5-rhba-2017-1425-bug-fixes]]
==== Bug Fixes

* Containers were being force killed by the build process. Some container resources were not freed when they were force killed, resulting in resource exhaustion and the inability to run new builds. Containers are now gracefully killed, allowing them to clean up their resources. Resource exhaustion no longer occurs and builds continue to run normally. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1437121[*BZ#1437121*])

* The OpenShift Container Registry (OCR) takes the `dockerImageReference` from an image object. The `dockerImageReference` is shared across image streams and the same image is fetched using the same `dockerImageReference` for all image streams. Now, the `dockerImageReference` is taken from an image stream. OCR fetches the image from different places for different image streams. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1433232[*BZ#1433232*])

* This bug fix adjusted conditionals to allow audit configuration for non-HA environments. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1447019[*BZ#1447019*])

* Missing rules in the Network Policy SDN plug-in did not allow proper off-cluster access. Off-cluster resources were not reachable. The rules are corrected and resources accessible. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1445500[*BZ#1445500*])

* Multiple node IP addresses were reported in random order by node status. Consequently, the SDN controller picked up a random one each time. This bug fix maintains the stickiness of the IP once it is chosen until valid, and IP addresses are no longer switched unexpectedly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1451830[*BZ#1451830*])

* The ARP cache size tuning parameters were not set when performing an installation on bare metal hosts. The bare metal profiles are now updated to ensure that the ARP cache is set correctly on bare metal hosts. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1452235[*BZ#1452235*])

* If the pod was using a persistent volume and had been deleted while the controller was down, the volume was never detached from the node. The restarted controller was not able to find the attached volume and never tried to detach it. Now, the fixed controller examines the node volumes on startup, determines which ones need to be detached, then detaches them properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1377486[*BZ#1377486*])

* Volumes attached to non-running AWS instances were being incorrectly marked as detached by a periodic routine that verified if volumes were attached because non-running AWS instances were not considered nodes by the routine. Volumes that were incorrectly marked detached were never detached if or when they needed to be later. By considering non-running AWS instances to be nodes in the routine, the issue is fixed. Volumes attached to non-running AWS instances are correctly tracked as attached and will be detached when they need to be later. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1455675[*BZ#1455675*])

[[ocp-3-5-rhba-2017-1425-images]]
==== Images

This release updates the Red Hat Container Registry
(`registry.access.redhat.com`) with the following images:

----
openshift3/ose-pod:v3.5.5.24-2
rhel7/pod-infrastructure:v3.5.5.24-2
openshift3/ose:v3.5.5.24-2
openshift3/ose-docker-registry:v3.5.5.24-2
openshift3/ose-egress-router:v3.5.5.24-2
openshift3/ose-keepalived-ipfailover:v3.5.5.24-2
openshift3/ose-f5-router:v3.5.5.24-2
openshift3/ose-deployer:v3.5.5.24-2
openshift3/ose-haproxy-router:v3.5.5.24-2
openshift3/node:v3.5.5.24-2
openshift3/ose-recycler:v3.5.5.24-2
openshift3/ose-sti-builder:v3.5.5.24-2
openshift3/ose-docker-builder:v3.5.5.24-2
openshift3/logging-deployer:v3.5.5.24-2
openshift3/metrics-deployer:v3.5.5.24-2
openshift3/openvswitch:v3.5.5.24-2
openshift3/logging-auth-proxy:3.5.0-15
openshift3/logging-curator:3.5.0-13
openshift3/logging-elasticsearch:3.5.0-23
openshift3/logging-fluentd:3.5.0-15
openshift3/logging-kibana:3.5.0-14
openshift3/metrics-cassandra:3.5.0-18
openshift3/metrics-hawkular-metrics:3.5.0-21
openshift3/metrics-hawkular-openshift-agent:3.5.0-15
openshift3/metrics-heapster:3.5.0-15
openshift3/registry-console:3.5-13
----

[[ocp-3-5-5-26]]
=== RHBA-2017:1492 - {product-title} 3.5.5.26 Bug Fix Update

Issued: 2017-06-20

{product-title} release 3.5.5.26 is now available. The list of packages and bug
fixes included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:1492[RHBA-2017:1492] advisory.
The list of container images included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:1493[RHBA-2017:1493] advisory.

[[ocp-3-5-5-26-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.4 or 3.5 cluster to this latest release, use the
automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-5-rhba-2017-1666]]
=== RHBA-2017:1666 - atomic-openshift-utils Bug Fix and Enhancement Update

Issued: 2017-06-29

{product-title} bug fix and enhancement advisory
link:https://access.redhat.com/errata/RHBA-2017:1666[RHBA-2017:1666], providing
updated *atomic-openshift-utils* and *openshift-ansible* packages that fix
several bugs and add an enhancement, is now available.

Space precluded documenting all of the bug fixes and enhancements for this
release in the advisory. See the following sections for notes on upgrading and
details on the bug fixes and enhancements included in this release.

[[ocp-3-5-rhba-2017-1666-upgrading]]
==== Upgrading

To apply this update, run the following on all hosts where you intend to
initiate Ansible-based installation or upgrade procedures:

----
# yum update atomic-openshift-utils
----

[[ocp-3-5-rhba-2017-1666-bug-fixes]]
==== Bug Fixes

* During control plan upgrades, a subset of pre-check and verification tasks for upgrades is run. However, the tasks were run over non-control plane nodes as well. Some of the tasks need **-excluder* RPMs to be disabled in order to work properly. Given the excluders are disabled on control plane hosts only, the tasks run over the remaining nodes caused a failure. With this bug fix, all pre-check and verification tasks are run over control plane nodes only. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1440167[*BZ#1440167*])

* Starting with {product-title} 3.4, OpenShift's SDN plug-ins no longer reconfigure the `docker` bridge MTU; instead, pods are configured properly on creation. Because of this change, non-OpenShift containers may have an MTU configured that is too large to allow access to hosts on the SDN. This bug fix updates the installer to align the MTU setting for the `docker` bridge with the MTU used inside the cluster, thus avoiding the problem. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1460235[*BZ#1460235*])

* The OpenShift CA redeployment playbook (*_playbooks/byo/openshift-cluster/redeploy-openshift-ca.yml_*) would fail to restart services if certificates were previously expired. This bug fix ensures that service restarts are now skipped within the OpenShift CA redeployment playbook when expired certificates are detected. Expired cluster certificates may be replaced with the certificate redeployment playbook (*_playbooks/byo/openshift-cluster/redeploy-certificates.yml_*) after the OpenShift CA certificate has been replaced via the OpenShift CA redeployment playbook. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1460969[*BZ#1460969*])

* Previously, installation would fail in multi-master environments in which the load balanced API was listening on a different port than that of the {product-title} API and web console. This bug fix accounts for this difference and ensures the master loopback client configuration is configured to interact with the local master. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1462276[*BZ#1462276*])

* The use of `oc patch` to update router images was setting additional configuration items to defaults, even if they were configured differently in the environment. This bug fix converts those tasks to use Ansible modules, which are much more precise and change only the provided parameter. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1462721[*BZ#1462721*])

* When using the `openshift_upgrade_nodes_label` variable during upgrades, if the label did not match any hosts, the upgrade would silently proceed with upgrading all nodes given. This bug fix verifies the provided label matches a set of hosts prior to upgrading, and the upgrade fails if no nodes match. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1462995[*BZ#1462995*])

* During certificate expiration checking or redeployment, certificiates with large serial numbers could not be parsed using the existing manual parser workaround on hosts that were missing the OpenSSL python library. This bug fix updates the manual parser to account for the format of certificates with large serial numbers. As a result, these certificates can now be parsed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1464543[*BZ#1464543*])

[[ocp-3-5-rhba-2017-1666-enhancements]]
==== Enhancements

* Previously, it was only possible to redeploy the etcd CA certificate by also redeploying the OpenShift CA certificate, which was unnecessary maintenance. With this enhancement, the etcd CA certificate may now be replaced independent of the OpenShift CA certificate using the etcd CA certificate redeployment playbook (*_playbooks/byo/openshift-cluster/redeploy-etcd-ca.yml_*). Note that the OpenShift CA redeployment playbook (*_playbooks/byo/openshift-cluster/redeploy-openshift-ca.yml_*) now only replaces the OpenShift CA certificate. Similarly, the etcd CA redeployment playbook only redeploys the etcd CA certificate. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1463772[*BZ#1463772*])

[[ocp-3-5-5-31]]
=== RHBA-2017:1640 - {product-title} 3.5.5.31 Bug Fix Update

Issued: 2017-07-11

{product-title} release 3.5.5.31 is now available. The packages and bug fixes
included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:1640[RHBA-2017:1640] advisory.
The container images included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:1646[RHBA-2017:1646] advisory.

Space precluded documenting all of the bug fixes for this release in the
advisory. See the following sections for notes on upgrading and details on the
bug fixes included in this release.

[[ocp-3-5-5-31-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.4 or 3.5 cluster to this latest release, use the
automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-5-5-3-bug-fixes]]
==== Bug Fixes

* When doing an incremental build, the S2I builder pulls its builder image before calling the *_save-artifacts_* script and does not ensure that the builder image is still there when it calls *_assemble_*. This leaves a gap of time between the start of the build and the calling of the *_assemble_* script in which the image can be removed. If the image is removed, the build fails. This bug fix adds a call to ensure that the builder image exists right before calling the *_assemble_* script. As a result, the chance of the *_assemble_* script running and not finding an available builder image is greatly reduced. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1464537[*BZ#1464537*])

* The Elasticsearch (ES) default value for sharing storage between ES instances was wrong. The incorrect default value allowed an ES pod starting up (when another ES pod was shutting down, e.g., during deployment configuration redeployments) to create a new location on the persistent volume (PV) for managing the storage volume. This duplicated data, and in some instances, potentially caused data loss. With this bug fix, all ES pods now run with `node.max_local_storage_nodes` set to `1`. As a result, the ES pods starting up or shutting down will no longer share the same storage, preventing data duplication and data loss. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1463046[*BZ#1463046*])

* The version of Netty that is part of Cassandra 3.0.9 had a memory leak. This bug fix updates Cassandra to 3.0.13, which has a version of Netty that has a fix for the memory leak. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1457501[*BZ#1457501*])

* VNID allow rules were being incorrectly removed before they were actually no longer in use. When containers had startup errors, it could cause the tracking to get out of sync. The rules that allowed communication for a namespace were removed too early, so that if there were still pods in that namespace on the node, they could not communicate with one another. This bug fix changes the way that the tracking is done to avoid edge cases around pod creation and deletion failures. As a result, the VNID tracking no longer fails, allowing traffic to flow. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1462338[*BZ#1462338*])

* When an IP address was re-used, it would be generated with a random MAC address that would be different from the previous one. Any node with an ARP cache that still held the old entry for the IP would not be able to communicate with the node. This bug fix generates the MAC address deterministically from the IP address. As a result, a re-used IP address will always have the same MAC address, so the ARP cache no longer gets out of sync, allowing traffic to flow. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1462955[*BZ#1462955*])

* Due to a coding error, `Pop()` operations could panic and cause the router to stop. This bug fix updates this logic and as a result panics no longer occur. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1464563[*BZ#1464563*])

* When master controller routines watch for persistent volume Recycle success events, they may never receive one, but still keep trying indefinitely. This caused the potential for high CPU usage by the master controller as it leaks routines. This bug fix updates the routines to stop watching for these Recycle success events when they will never be received. As a result, the chance of high CPU usage by the master controller is reduced. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1460289[*BZ#1460289*])

[[ocp-3-5-rhba-2017-1828]]
=== RHBA-2017:1828 - {product-title} 3.5.5.31 Bug Fix Update

Issued: 2017-08-31

{product-title} release 3.5.5.31 is now available. The packages and bug fixes
included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:1828[RHBA-2017:1828] advisory.
The container images included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2017:1829[RHBA-2017:1829] advisory
and listed in xref:ocp-3-5-rhba-2017-1828-images[Images].

Space precluded documenting all of the images for this release in the advisory.
See the following sections for notes on upgrading and details on the images
included in this release.

[[ocp-3-5-rhba-2017-1828-images]]
==== Images

This release updates the Red Hat Container Registry
(`registry.access.redhat.com`) with the following images:

----
openshift3/ose-pod:v3.5.5.31.19-2
rhel7/pod-infrastructure:v3.5.5.31.19-2
openshift3/ose-ansible:v3.5.5.31.19-2
openshift3/ose:v3.5.5.31.19-2
openshift3/ose-docker-registry:v3.5.5.31.19-2
openshift3/ose-egress-router:v3.5.5.31.19-2
openshift3/ose-keepalived-ipfailover:v3.5.5.31.19-2
openshift3/ose-f5-router:v3.5.5.31.19-2
openshift3/ose-deployer:v3.5.5.31.19-2
openshift3/ose-haproxy-router:v3.5.5.31.19-2
openshift3/node:v3.5.5.31.19-2
openshift3/ose-recycler:v3.5.5.31.19-2
openshift3/ose-sti-builder:v3.5.5.31.19-2
openshift3/ose-docker-builder:v3.5.5.31.19-2
openshift3/logging-deployer:v3.5.5.31.19-2
openshift3/logging-curator:v3.5.5.31.23-2
openshift3/metrics-deployer:v3.5.5.31.19-2
openshift3/openvswitch:v3.5.5.31.19-2
openshift3/logging-auth-proxy:3.5.0-28
openshift3/logging-elasticsearch:3.5.0-37
openshift3/logging-fluentd:3.5.0-26
openshift3/logging-kibana:3.5.0-30
openshift3/metrics-cassandra:3.5.0-33
openshift3/metrics-hawkular-metrics:3.5.0-36
openshift3/metrics-hawkular-openshift-agent:3.5.0-26
openshift3/metrics-heapster:3.5.0-26
openshift3/registry-console:3.5-26
----

[[ocp-3-5-rhba-2017-1828-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.4 or 3.5 cluster to this latest
release, use the automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-5-rhba-2017-2670]]
=== RHBA-2017:2670 - {product-title} 3.5.5.31.24 Bug Fix Update

Issued: 2017-09-07

{product-title} release 3.5.5.31.24 is now available. The packages and bug fixes
included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:2670[RHBA-2017:2670] advisory.
The container images included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2017:2643[RHBA-2017:2643] advisory.

[[ocp-3-5-rhba-2017-2670-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.4 or 3.5 cluster to this latest
release, use the automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-5-rhba-2017-3049]]
=== RHBA-2017:3049 - {product-title} 3.5.5.31.36 Bug Fix Update

Issued: 2017-10-25

{product-title} release 3.5.5.31.36 is now available. The list of packages
included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:3049[RHBA-2017:3049] advisory.
The container images included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2017:3050[RHBA-2017:3050] advisory.

Space precluded documenting all of the bug fixes and images for this release in
the advisories. See the following sections for notes on upgrading and details on
the bug fixes and images included in this release.

[[ocp-3-5-rhba-2017-3049-bug-fixes]]
==== Bug Fixes

[discrete]
===== Image Registry

* The size of cached layers was previously uncounted, causing an image's layer size for cached layers to be zero. This bug fix ensures cached layers are now properly counted, and as a result images now have the proper layer sizes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1457043[*BZ#1457043*])

* The OpenShift Container Registry used to append the forwarded target port to redirected location URLs. The registry client would get confused by the received location containing a superfluous port, and could not match it against the original host. This happened when exposed with TLS-termination other than passthrough. The client's new request to the target location lacked credentials, and as a consequence, the image push failed due to authorization error. This bug fix rebases the registry to a newer version, which fixes forwarding processing logic. As a result, the registry no longer confuses its clients; clients can push images successfully to the exposed registry using arbitrary TLS-termination. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1489039[*BZ#1489039*])

* Images younger than the threshold were not added to the dependency graph. Blobs used by a young image and by a prunable image were deleted because they had no references in the graph. This bug fix adds young images to the graph and marks them as non-prunable. As a result, blobs now have references and are not deleted. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1498123[*BZ#1498123*])

* Neither documentation nor CLI help talked about insecure connections to the secured registry. Errors used to be hard to decipher when users attempted to prune the secured registry with a bad CA certificate. This bug fix ensures that errors are now printed with hints, CLI help has been updated, and new flags have been provided to allow for insecure fallback. As a result, users can now easily enforce both secure and insecure connections and understand any HTTPS errors and how to resolve them. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1474446[*BZ#1474446*])

[discrete]
===== Logging

* Messages were previously read into Fluentd's memory buffer and were lost if the pod was restarted. Because Fluentd considers them read even though they have not been pushed to storage, any message not stored but already read by Fluentd was lost. This bug fix replaces the memory buffer with a file-based buffer. As a result, file-buffered messages are pushed to storage once Fluentd restarts. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1477515[*BZ#1477515*])

* The pattern for container logs in the journal field `CONTAINER_NAME` changed. The pattern was not matching for logs from pods in the `default`, `openshift`, or `openshift-infra` namespaces. Logs from these namespaces were being stored in indices matching `project.default.++*++`, for example rather than `.operations.++*++`. This bug fix updates the pattern matcher to match the correct pattern. As a result, logs from pods in the affected namespaces are correctly written to the `.operations.++*++` indices. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1494310[*BZ#1494310*])

* Memory was not being set properly for the Kibana container. This bug fix uses underscores instead of dashes, and memory settings are now honored by the Node.js runtime. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1469711[*BZ#1469711*])

* When moving to use the journald log driver instead of the json-file log driver, the code that parses the journald Kubernetes records was not preserving the `message` field created by the Kubernetes meta filter plug-in. This caused the raw JSON message to be put back into the `message` field. This bug fix preserves the `message` field created when the Kubernetes meta filter parses the JSON blob. As a result, the parsed `message` field is preserved. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1439504[*BZ#1439504*])

[discrete]
===== Web Console

* Previously, the web console Overview page would not finish loading if a resource name contained only digits, for example a deployment configuration "54321". This bug fix updates the web console, and the Overview page now works as expected for any valid resource name. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1485892[*BZ#1485892*])

[discrete]
===== Master

* In some failure cases, the etcd client used by OpenShift will not rotate through all the available etcd cluster members. The client will end up repeatedly trying the same server. If that server is down, then requests will fail for an extended time until the client finds the server invalid. If the etcd leader goes away when it is attempted to be contacted for something like authentication, then the authentication fails and the etcd client is stuck trying to communicate with the etcd member that does not exist. User authentication would fail for an extended period of time. With this bug fix, the etcd client now rotates to other cluster members even on failure. If the etcd leader goes away, the worst that should happen is a failure of that one authentication attempt. The next attempt will succeed because a different etcd member will be used. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1490428[*BZ#1490428*])

* The upstream discovery was not set to prefer version v1. It preferred the API group version instead. The old `oc` client was able to get discovery information when talking to the newer server and prefer the grouped API version of the resource. However, the version was not recognized. With this bug fix, the upstream directory is now set to prefer version v1. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1463576[*BZ#1463576*])

[discrete]
===== Networking

* Conntrack entries for UDP traffic were not cleared when an endpoint was added for a service that previously had no endpoints. The system could end up incorrectly caching a rule that would cause traffic to that service to be dropped rather than being sent to the new endpoint. With this bug fix, the relevant conntrack entries are now deleted at the right time and UDP services work correctly when endpoints are added and removed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1497768[*BZ#1497768*])

[discrete]
===== Pod

* When describing multiple instances on AWS, each node is supplied as a filter. This fails to work if the cluster is large enough because AWS only allows up to 200 filters to a request. As a result, `DescribeInstances` calls fail, resulting in broken load balancer and storage functionality in AWS. This bug fix implements batching of describeinstance calls to get over the filtering limit. `DescribeInstances` calls now also work for larger clusters. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1460388[*BZ#1460388*])

* Disabling the use of the proxy via `--disable-proxy` triggers a panic because the service stores have nil values. When disabling the proxy, the node will never start leaving the system in an indeterminate state. With this bug fix, the logic was reworked to ensure that the service stores are populated with non-nil values when the proxy is disabled. Using `--disable=proxy` no longer causes a panic and overall node start failure. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1484272[*BZ#1484272*])

[discrete]
===== Storage

* When the `atomic-openshift-node` service was restarted, all processes in its control group were terminated, including the glusterfs mounted points. Each glusterfs volume in OpenShift corresponds to one mounted point. If all mounting points are lost, so are all of the volumes. This bug fix sets the control group mode to terminate only the main process and leave the remaining glusterfs mounting points untouched. When the `atomic-openshift-node` service is restarted, no glusterfs mounting point is terminated. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1472370[*BZ#1472370*])

[[ocp-3-5-rhba-2017-3049-images]]
==== Images

This release updates the Red Hat Container Registry
(`registry.access.redhat.com`) with the following images:

----
openshift3/logging-curator:v3.5.5.31.36-4
openshift3/logging-elasticsearch:3.5.0-46
openshift3/logging-kibana:3.5.0-43
openshift3/metrics-deployer:v3.5.5.31.36-5
openshift3/metrics-heapster:3.5.0-33
openshift3/ose-deployer:v3.5.5.31.36-4
openshift3/ose:v3.5.5.31.36-4
openshift3/ose-egress-router:v3.5.5.31.36-4
openshift3/ose-haproxy-router:v3.5.5.31.36-4
openshift3/openvswitch:v3.5.5.31.36-5
openshift3/ose-recycler:v3.5.5.31.36-4
openshift3/ose-sti-builder:v3.5.5.31.36-4
openshift3/registry-console:3.5.0-33
openshift3/ose-f5-router:v3.5.5.31.36-4
openshift3/logging-auth-proxy:3.5.0-37
openshift3/logging-deployer:v3.5.5.31.36-5
openshift3/logging-fluentd:3.5.0-38
openshift3/metrics-cassandra:3.5.0-41
openshift3/metrics-hawkular-metrics:3.5.0-49
openshift3/metrics-hawkular-openshift-agent:3.5.0-33
openshift3/ose-docker-builder:v3.5.5.31.36-4
openshift3/ose-docker-registry:v3.5.5.31.36-4
openshift3/ose-keepalived-ipfailover:v3.5.5.31.36-4
openshift3/node:v3.5.5.31.36-5
openshift3/ose-pod:v3.5.5.31.36-4
----

[[ocp-3-5-rhba-2017-3049-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.4 or 3.5 cluster to this latest
release, use the automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-5-rhsa-2017-3389]]
=== RHSA-2017:3389 - Moderate: {product-title} 3.5.5.31.47 Security, Bug Fix, and Enhancement Update

Issued: 2017-12-06

{product-title} release 3.5.5.31.47 is now available. The list of packages
included in the update are documented in the
link:https://access.redhat.com/errata/RHSA-2017:3389[RHSA-2017:3389] advisory.
The container images included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2017:3390[RHBA-2017:3390] advisory.

Space precluded documenting all of the bug fixes, enhancements, and images for
this release in the advisories. See the following sections for notes on
upgrading and details on the bug fixes and images included in this release.


[[ocp-3-5-rhsa-2017-3389-images]]
==== Images

This release updates the Red Hat Container Registry
(`registry.access.redhat.com`) with the following images:

----
openshift3/logging-curator:v3.5.5.31.47-10
openshift3/logging-deployer:v3.5.5.31.47-10
openshift3/metrics-deployer:v3.5.5.31.47-10
openshift3/node:v3.5.5.31.47-10
openshift3/openvswitch:v3.5.5.31.47-10
openshift3/ose-ansible:v3.5.5.31.47-10
openshift3/ose-base:v3.5.5.31.47-10
openshift3/ose-deployer:v3.5.5.31.47-10
openshift3/ose-docker-builder:v3.5.5.31.47-10
openshift3/ose-docker-registry:v3.5.5.31.47-10
openshift3/ose-egress-router:v3.5.5.31.47-10
openshift3/ose-f5-router:v3.5.5.31.47-10
openshift3/ose-haproxy-router:v3.5.5.31.47-10
openshift3/ose-keepalived-ipfailover:v3.5.5.31.47-10
openshift3/ose-pod:v3.5.5.31.47-10
openshift3/ose-recycler:v3.5.5.31.47-10
openshift3/ose-sti-builder:v3.5.5.31.47-10
openshift3/ose:v3.5.5.31.47-10
----

[[ocp-3-6-rhsa-2017-3389-bug-fixes]]
==== Bug Fixes

[discrete]
===== Authentication

* During upgrades, reconciliation happens only for cluster roles automatically,
but this role needs to be adjusted in 3.6 due to enablement of API groups in
this release. The Ansible upgrade code has been changed to address this role
upgrade.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1493213[*BZ#1493213*])

[discrete]
===== Image Registry

* The size of a cached layer did not get counted. Therefore, the layer size for
cached layers was zero. Counting the size for cached layers now allows images to
have proper layer sizes.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1457042[*BZ#1457042*])

[discrete]
===== Logging

* `openshift-elasticsearch-plugin` was creating ACL roles based on the provided
name, which could include slashes and commas. This caused the dependent library
to not properly evaluate roles. With this bug fix, hash the name when creating
ACL roles so they no longer contain the invalid characters.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1494239[*BZ#1494239*])

* If the logging system is under a heavy load, it may take longer than the
five-second timeout for Elasticsearch to respond, or it may respond with an
error indicating that Fluentd needs to back off. In the former case, Fluentd
will retry to send the records again, which can lead to having duplicate
records. In the latter case, if Fluentd is unable to retry, it will drop
records, leading to data loss. For the former case, the fix is to set the
`request_timeout` to 10 minutes, so that Fluentd will wait up to 10 minutes for
the reply from Elasticsearch before retrying the request. In the latter case,
Fluentd will block attempting to read more input, until the output queues and
buffers have enough room to write more data. This bug fix greatly reduces
chances of duplicate data (though it is not entirely eliminated). Also, there is
no data loss due to back pressure.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1497836[*BZ#1497836*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1501948[*BZ#1501948*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1506854[*BZ#1506854*])

[discrete]
===== Management Console

* The management console was defaulting to the legacy API group `extensions` for
jobs. As a result, the legacy API group appeared in the UI in places such as
*Edit YAML*. With this bug fix, the console snow uses the new `batch` API group
as the default for job resources. The API group and version on a job resource
now appear as `batch/v1` wherever it is visible in the console.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1506233[*BZ#1506233*])

[discrete]
===== Metrics

* Extra, unnecessary queries were being performed on each request. The GET
`/hawkular/metrics/metrics` endpoint could fail with timeouts. With this bug
fix, the extra queries are only performed when explicitly requested. By default,
do not execute the extra queries that provide optional data. The endpoint is now
more stable and not as susceptible to timeouts.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1458186[*BZ#1458186*])

* When either a certificate within the chain at `serviceaccount/ca.crt` or any of
the certificates within the provided truststore file contained a white space
after the `BEGIN CERTIFICATE` declaration, the Java keytool rejected the
certificate with an error, causing Origin Metrics to fail to start. As a
workaround, Origin Metrics will now attempt to remove the spaces before feeding
the certificate to the Keytool, but administrators should ensure their
certificates do not contain such spaces.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1471251[*BZ#1471251*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1500464[*BZ#1500464*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1500471[*BZ#1500471*])

[discrete]
===== Networking

* A slow image pull made the network diagnostics fail. With this bug fix, the
 timeout for the image pull was increased. The diagnostics now run in slow
 environments.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1481550[*BZ#1481550*])

* The OpenShift node proxy previously did not support using a specified IP
address. This could prevent correct operation on hosts with multiple network
interface cards. The OpenShift node process already accepts a
`--bind-address=<ip address>:<port>` command-line flag and `bindAddress:`
configuration file option for the multiple network interface card case. The
proxy functionality has been fixed to respect these options. When
`--bind-address` or `bindAddress` are used, the OpenShift node proxy should work
correctly when the OpenShift node host has multiple network interface cards.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1489023[*BZ#1489023*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1489024[*BZ#1489024*])

* Iptables called too often and unnecessarily. Therefore, time-outs would wait for
iptables operations to finish. This bug fix changes the code so that it skips
reloads when the iptables rules are unchanged. There are now fewer calls to
iptables and, therefore, less time-outs.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1501517[*BZ#1501517*])

[discrete]
===== Pod

* There was a symbolic link error for the log file of every pod started when the
docker log driver was journald. Log symlink creation that fails when using
journald logging driver was skipped. This bug fix resolves the issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1434942[*BZ#1434942*])

* Currently, pod anti-affinity is respected across projects. Pod A from Project 1
will not land on node where Pod B from Project 2 is running, if pod
anti-affinity is enabled when scheduling Pod A. While scheduling Pod A, check
for pod anti-affinity only within the project of Pod A. Pod anti-affinity will
not be respected across projects.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1492194[*BZ#1492194*])

[discrete]
===== Storage

* The volumePath that included the datastore name was parsed incorrectly. The same
applies to volumePath that included datacluster and datastore names. It is not
possible to attach persistent volumes that have the above described volumePath
values. volumePath is now parsed correctly. Persistent volumes that have the
above described volumePath values are attached
correctly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1497042[*BZ#1497042*])

[discrete]
===== Security

* An attacker with knowledge of the given name used to authenticate and access
Elasticsearch can later access it without the token, bypassing authentication.
This attack also requires that the Elasticsearch be configured with an external
route, and the data accessed is limited to the indices.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1501986[*BZ#1501986*])

[[ocp-3-5-rhsa-2017-3389-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.4 or 3.5 cluster to this latest
release, use the automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-5-rhba-2017-3438]]
=== RHBA-2017:3438 - {product-title} 3.5.5.31.48 Bug Fix and Enhancement Update

Issued: 2017-12-14

{product-title} release 3.5.5.31.48 is now available. The list of packages
included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:3438[RHBA-2017:3438] advisory.
The container images included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2017:3439[RHBA-2017:3439] advisory.

Space precluded documenting all of the bug fixes, enhancements, and images for
this release in the advisories. See the following sections for notes on
upgrading and details on the bug fixes and images included in this release.

[[ocp-3-5-rhba-2017-3438-images]]
==== Images

This release updates the Red Hat Container Registry
(`registry.access.redhat.com`) with the following images:

----
openshift3/logging-curator:v3.5.5.31.48-2
openshift3/metrics-deployer:v3.5.5.31.48-3
openshift3/node:v3.5.5.31.48-2
openshift3/openvswitch:v3.5.5.31.48-2
openshift3/ose-base:v3.5.5.31.48-2
openshift3/ose-deployer:v3.5.5.31.48-2
openshift3/ose-docker-builder:v3.5.5.31.48-2
openshift3/ose-docker-registry:v3.5.5.31.48-2
openshift3/ose-egress-router:v3.5.5.31.48-2
openshift3/ose-f5-router:v3.5.5.31.48-2
openshift3/ose-haproxy-router:v3.5.5.31.48-3
openshift3/ose-keepalived-ipfailover:v3.5.5.31.48-2
openshift3/ose-pod:v3.5.5.31.48-2
openshift3/ose-sti-builder:v3.5.5.31.48-2
openshift3/ose:v3.5.5.31.48-2
----

[[ocp-3-5-rhba-2017-3438-bug-fixes]]
==== Bug Fixes

* Custom *openshift-ansible* modules were not being loaded due to a missing dependency configuration. The issue was first identified when using Ansible 2.4.1, which included a change in how tasks and roles were dynamically included at runtime.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=1516469[*BZ#1516469*])

* Previously, the registry CA, certificate, and key were not updated when running the certificate redeploy playbooks. This bug fix updates the playbooks to ensure that these are all updated whenever new certificates are deployed, ensuring that pushes to the registry work as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1383965[*BZ#1383965*])

* When using `openshift_metrics_heapster_standalone=true` in an inventory file, the CA certificate was not generated, causing playbooks to fail. This bug fix allows the CA certificate to be generated when `openshift_metrics_heapster_standalone=true` is set as well. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1443741[*BZ#1443741*])

* Previously, replacement of router certificates through use of the certificate redeployment playbook (*_redeploy-certificates.yml_*) or the router certificate redeployment playbook (*_redeploy-router-certificates.yml_*) would fail when a custom router certificate was provided. With this bug fix, custom router certificates set by `openshift_hosted_router_certificate` in inventory files now be redeployed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1446737[*BZ#1446737*])

* In some mixed-node environments, it was possible that host facts were not collected for containerized hosts, causing a conditional to fail. This bug fix adds a conditional to allow the check to complete correctly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1462517[*BZ#1462517*])

* Invalid PEM data could be left in the route configuration file during extended validation, causing the router to crash. This bug fix sanitizes PEM data from route configuration, and as a result extended validations now properly catch malformed certificates. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1511732[*BZ#1511732*])

* The configuration management for `BuildDefaults` attempted to remove environment variables that were previously defined, but have since been removed from the configuration. In situations where no environment variables have been configured, this was failing because the `env` key did not exist. This bug fix updates the process to skip the cleanup when the `env` key does not exist. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1515459[*BZ#1515459*])

[[ocp-3-5-rhba-2017-3438-enhancements]]
==== Enhancements

* When installing logging, the master configuration is now updated to add the `loggingPublicURL` parameter. Without `loggingPublicURL` being set, a user would not see a link to view historical logs from the web console. With this enhancement, the `loggingPublicURL` and its value is now added to the master configuration when installing logging and restarted so that when logging into the web console users can see the link to view historical logs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1414706[*BZ#1414706*])

[[ocp-3-5-rhba-2017-3438-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.4 or 3.5 cluster to this latest
release, use the automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-5-rhba-2018-0076]]
=== RHBA-2018:0076 - {product-title} 3.5.5.31.48-10 Images Update

Issued: 2018-01-10

{product-title} release 3.5.5.31.48-10 is now available. The list of container
images included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2018:0076[RHBA-2018:0076] advisory.

The container images in this release have been updated using the latest base
images.

[[ocp-3-5-rhba-2018-0076-images]]
==== Images

This release updates the Red Hat Container Registry
(`registry.access.redhat.com`) with the following images:

----
openshift3/logging-kibana:3.5.0-54
openshift3/node:v3.5.5.31.48-10
openshift3/openvswitch:v3.5.5.31.48-11
----

[[ocp-3-5-rhba-2018-0076-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.4 or 3.5 cluster to this latest release, use the
automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.
