[[release-notes-v2-vs-v3]]
= Comparing with OpenShift Enterprise 2
{product-author}
{product-version}
:icons: font
:experimental:
:toc: macro
:toc-title:
:prewrap!:
:description: This topic is a list of the differences between OpenShift v2 and OpenShift v3.

toc::[]

// tag::release_notes_v2_vs_v3[]

== Overview

{product-title} 3 is based on the OpenShift version 3 (v3) architecture, which
is very different product than OpenShift version 2 (v2). Many of the same terms
from OpenShift v2 are used in v3, and the same functions are performed, but the
terminology can be different, and behind the scenes things may be happening very
differently. Still, OpenShift remains an application platform.

This topic discusses these differences in detail, in an effort to help OpenShift
users in the transition from OpenShift v2 to OpenShift v3.

ifdef::openshift-dedicated[]
[NOTE]
====
{product-title} 3 follows the numbering of the product's major version, and uses
the same code base as OpenShift Container Platform 3.
====
endif::[]

== Architecture Changes

*Gears Versus Containers*

Gears were a core component of OpenShift v2. Technologies such as kernel
namespaces, cGroups, and SELinux helped deliver a highly-scalable, secure,
containerized application platform to OpenShift users. Gears themselves were a
form of container technology.

OpenShift v3 takes the gears idea to the next level. It uses Docker as the next
evolution of the v2 container technology. This container architecture is at the
core of OpenShift v3.

*Kubernetes*

As applications in OpenShift v2 typically used multiple gears, applications on
OpenShift v3 will expectedly use multiple containers. In OpenShift v2, gear
orchestration, scheduling, and placement was handled by the OpenShift broker
host. OpenShift v3 integrates Kubernetes into the master host to drive container
orchestration.

== Applications

Applications are still the focal point of OpenShift. In OpenShift v2, an
application was a single unit, consisting of one web framework of no more than
one cartridge type. For example, an application could have one PHP and one
MySQL, but it could not have one Ruby, one PHP, and two MySQLs. It also could
not be a database cartridge, such as MySQL, by itself.

This limited scoping for applications meant that OpenShift performed seamless
linking for all components within an application using environment variables.
For example, every web framework knew how to connect to MySQL using the
`OPENSHIFT_MYSQL_DB_HOST` and `OPENSHIFT_MYSQL_DB_PORT` variables. However, this
linking was limited to within an application, and only worked within cartridges
designed to work together. There was nothing to help link across application
components, such as sharing a MySQL instance across two applications.

While most other PaaSes limit themselves to web frameworks and rely on external
services for other types of components, OpenShift v3 makes even more application
topologies possible and manageable.

OpenShift v3 uses the term "application" as a concept that links services
together. You can have as many components as you desire, contained and flexibly
linked within a
xref:../architecture/core_concepts/projects_and_users.adoc#projects[project],
and, optionally, labeled to provide grouping or structure. This updated model
allows for a standalone MySQL instance, or one shared between JBoss components.

Flexible linking means you can link any two arbitrary components together. As
long as one component can export environment variables and the second component
can consume values from those environment variables, and with potential variable
name transformation, you can link together any two components without having to
change the images they are based on. So, the best containerized implementation
of your desired database and web framework can be consumed directly rather than
you having to fork them both and rework them to be compatible.

This means you can build anything on OpenShift. And that is OpenShift's primary
aim: to be a container-based platform that lets you build entire applications in
a repeatable lifecycle.

== Cartridges Versus Images

In OpenShift v3, an
xref:../architecture/core_concepts/containers_and_images.adoc#docker-images[image]
has replaced OpenShift v2's concept of a cartridge.

Cartridges in OpenShift v2 were the focal point for building applications. Each
cartridge provided the required libraries, source code, build mechanisms,
connection logic, and routing logic along with a preconfigured environment to
run the components of your applications.

However, cartridges came with disadvantages. With cartridges, there was no clear
distinction between the developer content and the cartridge content, and you did
not have ownership of the home directory on each gear of your application. Also,
cartridges were not the best distribution mechanism for large binaries. While
you could use external dependencies from within cartridges, doing so would lose
the benefits of encapsulation.

From a packaging perspective, an image performs more tasks than a cartridge, and
provides better encapsulation and flexibility. However, cartridges also included
logic for building, deploying, and routing, which do not exist in images. In
OpenShift v3, these additional needs are met by
xref:../architecture/core_concepts/builds_and_image_streams.adoc#source-build[Source-to-Image
(S2I)] and xref:../dev_guide/templates.adoc#dev-guide-templates[configuring the
template].

*Dependencies*

In OpenShift v2, cartridge dependencies were defined with `*Configure-Order*` or
`*Requires*` in a cartridge manifest. OpenShift v3 uses a declarative model
where xref:../architecture/core_concepts/pods_and_services.adoc#pods[pods] bring
themselves in line with a predefined state. Explicit dependencies that are
applied are done at runtime rather than just install time ordering.

For example, you might require another service to be available before you start.
Such a dependency check is always applicable and not just when you create the
two components. Thus, pushing dependency checks into runtime enables the system
to stay healthy over time.

*Collection*

Whereas cartridges in OpenShift v2 were colocated within gears,
xref:../architecture/core_concepts/containers_and_images.adoc#docker-images[images]
in OpenShift v3 are mapped 1:1 with
xref:../architecture/core_concepts/containers_and_images.adoc#containers[containers],
which use xref:../architecture/core_concepts/pods_and_services.adoc#pods[pods]
as their colocation mechanism.

*Source Code*

In OpenShift v2, applications were required to have at least one web framework
with one Git repository. In OpenShift v3, you can choose which images are built
from source and that source can be located outside of OpenShift itself. Because
the source is disconnected from the images, the choice of image and source are
distinct operations with source being optional.

*Build*

In OpenShift v2, builds occurred in application gears. This meant downtime for
non-scaled applications due to resource constraints. In v3,
xref:../architecture/core_concepts/builds_and_image_streams.adoc#builds[builds]
happen in separate containers. Also, OpenShift v2 build results used rsync to
synchronize gears. In v3, build results are first committed as an immutable
image and published to an internal registry. That image is then available to
launch on any of the nodes in the cluster, or available to rollback to at a
future date.

*Routing*

In OpenShift v2, you had to choose up front as to whether your application was
scalable, and whether the routing layer for your application was enabled for
high availability (HA). In OpenShift v3,
xref:../architecture/networking/routes.adoc#architecture-core-concepts-routes[routes] are first-class objects
that are HA-capable simply by scaling up your application component to two or
more replicas. There is never a need to recreate your application or change its
DNS entry.

The routes themselves are disconnected from images. Previously, cartridges
defined a default set of routes and you could add additional aliases to your
applications. With OpenShift v3, you can use templates to set up any number of
routes for an image. These routes let you modify the scheme, host, and paths
exposed as desired, with no distinction between system routes and user aliases.

== Broker Versus Master

A
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[master]
in OpenShift v3 is similar to a broker host in OpenShift v2. However, the
MongoDB and ActiveMQ layers used by the broker in OpenShift v2 are no longer
necessary, because *etcd* is typically installed with each master host.

== Domain Versus Project

A xref:../architecture/core_concepts/projects_and_users.adoc#projects[project]
is essentially a v2 domain.

// end::release_notes_v2_vs_v3[]

////
== Routing and Scaling



== DNS
////
