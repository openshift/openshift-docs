[id="ocp-4-4-release-notes"]
= {product-title} {product-version} release notes
include::modules/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid
cloud application platform for deploying both new and existing applications on
secure, scalable resources with minimal configuration and management overhead.
{product-title} supports a wide selection of programming languages and
frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on Red Hat Enterprise Linux and Kubernetes, {product-title}
provides a more secure and scalable multi-tenant operating system for todayâ€™s
enterprise-class applications, while delivering integrated application runtimes
and libraries. {product-title} enables organizations to meet security, privacy,
compliance, and governance requirements.

[id="ocp-4-4-about-this-release"]
== About this release

Red Hat {product-title}
(link:https://access.redhat.com/errata/RHBA-2020:0581[RHBA-2020:0581]) is now
available. This release uses link:https://v1-17.docs.kubernetes.io/docs/setup/release/notes/[Kubernetes 1.17] with CRI-O runtime. New features, changes, and known issues that pertain to
{product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at
https://cloud.redhat.com/openshift. The {cloud-redhat-com}
application for {product-title} allows you to deploy OpenShift clusters to
either on-premise or cloud environments.

{product-title} {product-version} is supported on Red Hat Enterprise Linux 7.6 or
later, as well as {op-system-first} 4.4.

You must use {op-system} for the control plane, which are also known as master machines, and
can use either {op-system} or Red Hat Enterprise Linux 7.6 or later for
compute machines, which are also known as worker machines.

[IMPORTANT]
====
Because only Red Hat Enterprise Linux version 7.6 or later is supported for compute
machines, you must not upgrade the Red Hat Enterprise Linux compute machines to
version 8.
====

[id="ocp-4-4-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-4-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-4-installing-cluster-on-azure-upi"]
==== Installing a cluster on Microsoft Azure using user-provisioned infrastructure

{product-title} 4.4 introduces support for installing a cluster on Azure using
user-provisioned infrastructure. Running user-provisioned infrastructure on
Azure lets you use customizations your environment might require, like
regulatory, security, and operational control.

You can incorporate example Azure Resource Manager (ARM) templates provided by
Red Hat to assist in the deployment process, or create your own. You are also
free to create the required resources through other methods; the ARM templates
are just an example.

See xref:../installing/installing_azure/installing-azure-user-infra.adoc#installing-azure-user-infra[Installing a cluster on Azure using ARM templates]
for details.

[id="ocp-4-4-installing-cluster-on-rhv-ipi"]
==== Installing a cluster on Red Hat Virtualization using installer-provisioned infrastructure

{product-title} 4.4 introduces support for installing a cluster in a Red Hat
Virtualization (RHV) environment using installer-provisioned infrastructure.

//For more information, see Installing a cluster on RHV (link when published).

[id="ocp-4-4-installing-cluster-on-openstack-upi"]
==== Installing a cluster on OpenStack using user-provisioned infrastructure

{product-title} 4.4 introduces support for installing a cluster on
{rh-openstack-first} that runs on infrastructure that you provide.
Using your own infrastructure allows you to integrate your cluster with existing
infrastructure and modifications. For example, you must create all
{rh-openstack} resources, like Nova servers, Neutron ports, and security groups.
Red Hat provides Ansible playbooks to help you with the deployment process.

You can also install a cluster on {rh-openstack} with Kuryr using your own
infrastructure.

// Incorporate links when available:
//../installing/installing_openstack/installing-openstack-user.adoc#installing-openstack-user[Installing a cluster on OpenStack on your own infrastructure]
//../installing/installing_openstack/installing-openstack-user-kuryr.adoc#installing-openstack-user-kuryr[Installing a cluster on OpenStack with Kuryr on your own infrastructure]

[id="ocp-4-4-installing-cluster-on-openstack-no-longer-requires-swift-storage-service"]
==== Installing a cluster on OpenStack no longer requires the Swift object storage service

Beginning with version 4.4, {product-title} no longer requires that the Swift
object storage service be present on the {rh-openstack} cloud where it is
installed. If Swift is not available for the {product-title} installation, the
installer uses the Cinder block storage and Glance image registry services in
its place.

//For more information, see Installing a cluster on OpenStack using your own infrastructure (link when published).

[id="ocp-4-4-clusters-installed-on-openstack-support-self-signed-certs"]
==== Clusters installed on OpenStack support self-signed certificates

{product-title} 4.4 can now be installed on {rh-openstack} clouds that use
self-signed certificates for authorization.

//For more information, see Installing a cluster on OpenStack using your own infrastructure (link when published).

[id="ocp-4-4-openstack-validates-rhcos-images-checksum"]
==== OpenStack validates {op-system} images by checking sha256 checksum

On {rh-openstack}, the installer now performs automatic checksum validation of
{op-system-first} images.

[id="ocp-4-4-support-for-east-west-traffic-with-ovn-on-openstack-kuryr"]
==== Support for east-west traffic with OVN load balancing on OpenStack with Kuryr

{product-title} installations that use Kuryr on {rh-openstack} 16 can now use
the OVN load-balancing provider driver instead of the Amphora driver. If OVN and
the OVN Octavia driver are present in the environment, OVN is used
automatically. As a result, load balancer performance and resource utilization
are improved. The need for a load balancer VM for each service is also removed.

[id="ocp-4-4-using-upgrade-channels"]
==== Using upgrade channels for 4.4 release

Upgrades from the latest {product-title} 4.3.z release to 4.4 will be available
at GA for clusters that have switched to the fast-4.4 channel. Telemetry data
from early adopters in the fast-4.4 channel will be monitored to inform when the
upgrade is promoted into the stable-4.4 channel. This monitoring is above and
beyond our extensive enterprise-grade testing and may take several weeks.

[id="ocp-4-4-security"]
=== Security

[id="ocp-4-4-bound-service-account-tokens"]
==== Support for bound service account tokens

{product-title} 4.4 provides support for bound service account tokens, which improves the ability to integrate with cloud provider identity access management (IAM) services, such as AWS IAM.

For more information, see xref:../authentication/bound-service-account-tokens.adoc#bound-service-account-tokens[Using bound service account tokens].

[id="ocp-4-4-oauth-proxy-imagestream"]
==== The `oauth-proxy` imagestream is now available

{product-title} 4.4 introduces the `oauth-proxy` imagestream for third party authentication integration. You should no longer use the `oauth-proxy` image from the Red Hat Registry. You should instead use the `openshift/oauth-proxy:v4.4` imagestream if you target {product-title} 4.4 clusters and newer. This guarantees backwards compatibility and allows you to add imagestream triggers to get critical fixes. The `v4.4` tag will be available for at least the next three {product-title} minor releases without breaking changes. Each minor release will also introduce its own tag.

[id="ocp-4-4-kube-apiserver-check-certs-before-tokens"]
==== kube-apiserver checks client certificates before tokens

In previous versions of {product-title}, the kube-apiserver checked tokens
before client certificates for authentication. Now kube-apiserver checks client
certificates before tokens.

For example, if you had a `system:admin` kubeconfig and ran the `oc --token=foo get pod`
command in previous versions of {product-title}, it would authenticate as a user
with token `foo`. Now it authenticates as `system:admin`. The recommendation for
past releases was to impersonate a user with the parameter `--as` in such cases
instead of overriding the token when using a client certificate; this is no
longer necessary.

[id="ocp-4-4-nodes"]
=== Nodes

[id="ocp-4-4-evicting-pods-using-descheduler-tp"]
==== Evicting Pods using the descheduler (Technology Preview)

The descheduler provides the ability to evict a running Pod so that the Pod can
be rescheduled onto a more suitable node.

You can benefit from descheduling Pods in situations such as the following:

* Nodes are underutilized or overutilized.
* Pod and node affinity requirements, such as taints or labels, have changed and
the original scheduling decisions are no longer appropriate for certain nodes.
* Node failure requires Pods to be moved.
* New nodes are added to clusters.

See xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler[Evicting Pods using the descheduler]
for more information.

[id="ocp-4-4-controlling-overcommit-and-managing-container-density-on-nodes"]
==== Controlling overcommit and managing container density on nodes

{product-title} administrators can now control the level of overcommit and
manage container density on nodes. You can configure cluster-level overcommit
using the Cluster Resource Override Operator to override the ratio between
requests and limits set on developer containers.

//For more information, see nodes/clusters/nodes-cluster-overcommit.adoc (link when published) https://github.com/openshift/openshift-docs/pull/21319

[id="ocp-4-4-cluster-monitoring"]
=== Cluster monitoring

[id="ocp-4-4-monitoring-dashboards-in-web-console"]
==== Monitoring Dashboards in web console

The Dashboards view is now available from the Monitoring section in the web
console. This lets you view metrics that bring transparency to the
{product-title} cluster and its dependent components.

[id="ocp-4-4-hwmon-collector-disabled"]
==== hwmon collector disabled in node-exporter

The `hwmon` collector has been disabled in the node-exporter monitoring
component because it is no longer used to collect cluster metrics.

[id="ocp-4-4-cluster-reader-can-read-node-metrics"]
==== cluster-reader can read node metrics

The `cluster-reader` role can now read node metrics by default.

[id="ocp-4-4-new-cluster-alert"]
==== Cluster alert for when multiple containers are killed

You are notified with a `MultipleContainersOOMKilled` alert when multiple
containers are killed within 15 minutes due to memory outages.

[id="ocp-4-4-new-api-server-alerts"]
==== New API server alerts

There are two new API server alerts available for {product-title} 4.4:

* `ErrorBudgetBurn`: fires when the API server issues 5xx request responses.
* `AggregatedAPIErrors`: fires when the number of errors have increased for the
aggregated API servers.

[id="ocp-4-4-permission-updates-for-po"]
==== Permission updates for Prometheus Operator

The custom resource definitions managed by the Prometheus Operator now have more
restrictive permissions.

The custom resources the Prometheus Operator manages include:

* `Prometheus`
* `ServiceMonitor`
* `PodMonitor`
* `Alertmanager`
* `PrometheusRule`

[id="ocp-4-4-cluster-monitoring-version-updates"]
==== Cluster monitoring component version updates

The following monitoring components have been upgraded:

* Prometheus: version upgrade from 2.14.0 to 2.15.2
* Alertmanager: version upgrade from 0.19.0 to 0.20.0
* Prometheus Operator: version upgrade from 0.34.0 to 0.35.1
* kube-state-metrics: version upgrade from 1.8.0 to 1.9.5
* Grafana: version upgrade from 6.4.3 to 6.5.3

[id="ocp-4-4-logging"]
=== Logging

[id="ocp-4-4-web-console"]
=== Web console

[id="ocp-4-4-openshift-pipelines-tp"]
==== OpenShift Pipelines (Technology Preview)

You can use the Developer perspective of the {product-title} web console to
create CI/CD Pipelines for your software delivery process while creating an
application on {product-title}.

// For more information, see Working with OpenShift Pipelines. (link when available) https://github.com/openshift/openshift-docs/pull/20989

[id="ocp-4-4-marketplace-integration-in-operator-hub"]
==== IBM Marketplace integration in OperatorHub

IBM Marketplace is now integrated with the OperatorHub, which is located in the
{product-title} web console. This integration allows you to install and manage
Operators hosted on the IBM Marketplace from within the OperatorHub interface.

[id="ocp-4-4-edit-apps-in-topology-view"]
==== Edit applications in the Topology view

You can now edit applications from the Developer perspective by using the
Topology view.

[id="ocp-4-4-create-helm-releases"]
==== Create Helm releases

You can now create Helm releases from the Helm charts that are provided in the
Developer Catalog.

[id="ocp-4-4-networking"]
=== Networking

[id="ocp-4-4-sctp-on-ocp"]
==== Stream Control Transmission Protocol (SCTP) on {product-title}

SCTP is a reliable message based protocol that runs on top of an IP network.
When enabled, you can use SCTP as a protocol with both Pods and Services.
For more information, see xref:../networking/using-sctp.adoc#using-sctp[Using SCTP].

[id="ocp-4-4-using-dns-forwarding"]
==== Using DNS forwarding

You can use DNS forwarding to override the forwarding default configuration on a
per-zone basis by specifying which name server should be used for a given zone.

For more information, see
xref:../networking/dns-operator.adoc#dns-operator[Using DNS forwarding].

[id="ocp-4-4-haproxy-upgraded-to-version-2"]
==== HAProxy upgraded to version 2.0

The HAProxy used for Ingress has been upgraded from version 1.8.17 to 2.0.13.
This upgrade introduces no new APIs or supported user-facing capabilities to
{product-title}. The upgrade does provide significant performance improvements
and many bug fixes. HAProxy 2.0 also adds native Prometheus metrics and provides
full IPv6 support when other {product-title} components are configured to
support it.

[id="ocp-4-4-ingress-enhancements"]
==== Ingress Enhancements

There are two noteworthy Ingress enhancements introduced in {product-title} 4.4:

* xref:../networking/routes/route-configuration.adoc#route-configuration[Ingress gains a Route admission policy API]:
allows you to run applications in multiple namespaces with the same domain name.
* xref:../networking/ingress-operator.adoc#configuring-ingress[Ingress can be exposed by a NodePort service]:
facilitates integration of your existing load balancer so that you can have
granular control over your load balancing solution.

[id="ocp-4-4-storage"]
=== Storage

[id="ocp-4-4-persistent-storage-csi-snapshots"]
==== Persistent storage using CSI snapshots (Technology Preview)

You can now use the Container Storage Interface (CSI) to create, restore, and delete a volume snapshot. This feature is enabled by default in Technology Preview.

[id="ocp-4-4-persistent-storage-csi-cloning"]
==== Persistent storage using CSI cloning (Technology Preview)

You can now use the Container Storage Interface (CSI) to clone storage volumes after they have already been created. This feature is enabled by default in Technology Preview.

[id="ocp-4-4-scale"]
=== Scale

[id="ocp-4-4-scale-cluster-maximums"]
==== Cluster maximums

Updated guidance around
xref:../scalability_and_performance/planning-your-environment-according-to-object-maximums.adoc#planning-your-environment-according-to-object-maximums[Cluster
maximums] for {product-title} {product-version} is now available.

The {product-version} tested maximum for the number of Pods per node is 500.

Use the link:https://access.redhat.com/labs/ocplimitscalculator/[{product-title}
Limit Calculator] to estimate cluster limits for your environment.

[id="ocp-4-4-developer-experience"]
=== Developer experience

[id="ocp-4-4-automatic-image-pruning"]
==== Automatic image pruning

You can now enable automatic image pruning. This is not enabled by default;
you will be notified of this option after installing or upgrading to
{product-title} 4.4. This automation is managed by the Image Registry Operator, which
creates a CronJob to run periodic image pruning.

[id="ocp-4-4-build-objects-report-conditions-in-status"]
==== Build objects report conditions in status

Build conditions have been added for each existing {product-title} build phase.
These conditions contain information about the build during its build lifecycle.
You can also use commands like `oc wait` to wait for a specific build phase to
be reached.

[id="ocp-4-4-recreate-rollouts-for-image-registry"]
==== Recreate rollouts for image registry

You can now use the `Recreate` rollout strategy when deploying the image
registry. This lets you use `ReadWriteOnce` persistent volumes, such as AWS
Elastic Block Store. When using these storage types, you must use the `Recreate`
rollout strategy to successfully upgrade an {product-title} cluster.

[id="ocp-4-4-odo"]
==== odo enhancements

`odo` has several enhancements and improvements that focus on the user experience:

*  An `odo debug info` command is now available.
*  The `odo url` command now has a `--secure` flag to specify HTTPS URLs.
* The `odo create`, `odo url`, and `odo config` commands now have a `--now` flag to apply changes on the cluster immediately.
* The `odo debug port-forward` command now selects a port automatically if the default port is occupied.
* The output of the `odo storage` and `odo push` commands is restructured for better readability.
* Experimental mode is now available, in which you may use Technology Preview features, such as creating applications using devfiles.
* Technology Preview feature - support of devfiles is now available. To learn more, see the xref:../cli_reference/openshift_developer_cli/odo-release-notes.adoc#odo-notable-improvements_odo-release-notes[odo Release Notes].

[id="ocp-4-4-helm-3-ga-support"]
==== Helm 3 GA support

Helm is a package manager for Kubernetes and {product-title} applications. It
uses a packaging format called Helm charts to simplify defining, installing, and
upgrading of applications and Services.

Helm CLI is built and shipped with {product-title} and is available to download
from the web console's CLI menu.

[id="ocp-4-4-operators"]
=== Operators

[id="ocp-4-4-etcd-cluster-operator"]
==== etcd cluster Operator

{product-title} 4.4 introduces the etcd cluster Operator, which handles the scaling of etcd and provisioning etcd dependencies such as TLS certificates. The etcd cluster Operator simplifies the disaster recovery procedure to restore to a previous cluster state, automates the addition of etcd members, provides more accurate etcd member health reporting, and reports events to assist with debugging the etcd cluster.

With this update, the names of the following disaster recovery scripts were changed:

* `etcd-snapshot-backup.sh` is now `cluster-backup.sh`.
* `etcd-snapshot-restore.sh` is now `cluster-restore.sh`.

For more information, see xref:../backup_and_restore/disaster_recovery/about-disaster-recovery.adoc#about-dr[About disaster recovery].

[id="ocp-4-4-insights-operator-anonymized-csr"]
==== Insights Operator now collects anonymized CSRs

With this enhancement, the Insights Operator periodically collects anonymized
certificate signing requests (CSR) to identify CSRs that are not verified in
Kubernetes or have not been approved. Additionally, the Insights Operator collects
data if certificates are valid. As a result, this helps improve the {product-title}
customer support experience.

[id="ocp-4-4-remove-samples-operator-when-unable-to-connect"]
==== Remove Samples Operator if it cannot connect to registry.redhat.io

Sample imagestreams are not created if the Samples Operator cannot connect to
`registry.redhat.io` during installation. This ensures that sample content
installation does not fail {product-title} cluster installation.

You can xref:../openshift_images/samples-operator-alt-registry.adoc#installation-restricted-network-samples_samples-operator-alt-registry[configure alternate or mirrored registries]
to bypass this issue if it arises during your cluster installation.

[id="ocp-4-4-notable-technical-changes"]
== Notable technical changes

{product-title} 4.4 introduces the following notable technical changes.

[discrete]
[id="ocp-4-4-sending-cluster-logs-using-fluent-syslog-plug-in"]
==== Sending cluster logs using the Fluentd syslog plug-in (RFC 3164)

Due to changes introduced with the Log Forwarding feature in {product-title}
4.3, you could no longer use the Fluentd syslog plug-in to forward logs to an
external syslog server. In {product-title} 4.4, this functionality is restored
and you can use the syslog plug-in. The procedure to configure the plug-in is
different in {product-title} version 4.4 than it was in version 4.2. For more
information, see
xref:../logging/config/cluster-logging-external.adoc#cluster-logging-collector-syslog_cluster-logging-external[Sending logs using the Fluentd syslog plug-in (RFC 3164)].

[discrete]
[id="ocp-4-4-osdk-v0-15-0"]
==== Operator SDK v0.15.0

{product-title} 4.4 supports Operator SDK v0.15.0, which introduces the
following notable technical changes:

* The `olm-catalog gen-csv` subcommand is now moved to the `generate csv`
subcommand.
* The `up local` subcommand is now moved to the `run --local` subcommand.

[id="ocp-4-4-unsupported-features"]
=== Unsupported features

[id="ocp-4-4-oc-secrets-subcommands"]
==== OpenShift CLI secrets subcommands

The following `oc secrets` subcommands that were deprecated in {product-title}
3.9 are no longer available:

* `new`
* `new-basicauth`
* `new-dockercfg`
* `new-sshauth`

You must use the `oc create secret` command instead.

[id="ocp-4-4-oc-build-logs-command"]
==== OpenShift CLI build-logs command

The `oc build-logs` command was deprecated in {product-title} 3.11 and has been
removed. You must use `oc logs` instead.

[id="ocp-4-4-deprecated-kubernetes-metrics-removed"]
==== Deprecated upstream Kubernetes metrics have been removed

All deprecated upstream Kubernetes metrics have been removed. The complete list
of removed metrics is next.

[discrete]
[id="ocp-4-4-kubelet-metrics"]
===== Kubelet metrics

* `kubelet_pod_worker_latency_microseconds`
* `kubelet_pod_start_latency_microseconds`
* `kubelet_cgroup_manager_latency_microseconds`
* `kubelet_pod_worker_start_latency_microseconds`
* `kubelet_pleg_relist_latency_microseconds`
* `kubelet_pleg_relist_interval_microseconds`
* `kubelet_runtime_operations`
* `kubelet_runtime_operations_latency_microseconds`
* `kubelet_runtime_operations_errors`
* `kubelet_eviction_stats_age_microseconds`
* `kubelet_device_plugin_registration_count`
* `kubelet_device_plugin_alloc_latency_microseconds`
* `kubelet_network_plugin_operations_latency_microseconds`

[discrete]
[id="ocp-4-4-schedular-metrics"]
===== Schedular metrics

* `scheduler_e2e_scheduling_latency_microseconds`
* `scheduler_scheduling_algorithm_predicate_evaluation`
* `scheduler_scheduling_algorithm_priority_evaluation`
* `scheduler_scheduling_algorithm_preemption_evaluation`
* `scheduler_scheduling_algorithm_latency_microseconds`
* `scheduler_binding_latency_microseconds`
* `scheduler_scheduling_latency_seconds`

[discrete]
[id="ocp-4-4-api-server-metrics"]
===== API server metrics

* `apiserver_request_count`
* `apiserver_request_latencies`
* `apiserver_request_latencies_summary`
* `apiserver_dropped_requests`
* `apiserver_storage_data_key_generation_latencies_microseconds`
* `apiserver_storage_transformation_failures_total`
* `apiserver_storage_transformation_latencies_microseconds`
* `apiserver_proxy_tunnel_sync_latency_secs`

[discrete]
[id="ocp-4-4-docker-metrics"]
===== Docker metrics

* `kubelet_docker_operations`
* `kubelet_docker_operations_latency_microseconds`
* `kubelet_docker_operations_errors`
* `kubelet_docker_operations_timeout`

[discrete]
[id="ocp-4-4-reflector-metrics"]
===== Reflector metrics

* `reflector_items_per_list`
* `reflector_items_per_watch`
* `reflector_list_duration_seconds`
* `reflector_lists_total`
* `reflector_short_watches_total`
* `reflector_watch_duration_seconds`
* `reflector_watches_total`

[discrete]
[id="ocp-4-4-etcd-metrics"]
===== etcd metrics

* `etcd_helper_cache_hit_count`
* `etcd_helper_cache_miss_count`
* `etcd_helper_cache_entry_count`
* `etcd_request_cache_get_latencies_summary`
* `etcd_request_cache_add_latencies_summary`
* `etcd_request_latencies_summary`

[discrete]
[id="ocp-4-4-transformation-metrics"]
===== Transformation metrics

* `transformation_latencies_microseconds`
* `transformation_failures_total`

[discrete]
[id="ocp-4-4-other-metrics"]
===== Other metrics

* `admission_quota_controller_adds`
* `crd_autoregistration_controller_work_duration`
* `APIServiceOpenAPIAggregationControllerQueue1_adds`
* `AvailableConditionController_retries`
* `crd_openapi_controller_unfinished_work_seconds`
* `APIServiceRegistrationController_retries`
* `admission_quota_controller_longest_running_processor_microseconds`
* `crdEstablishing_longest_running_processor_microseconds`
* `crdEstablishing_unfinished_work_seconds`
* `crd_openapi_controller_adds`
* `crd_autoregistration_controller_retries`
* `crd_finalizer_queue_latency`
* `AvailableConditionController_work_duration`
* `non_structural_schema_condition_controller_depth`
* `crd_autoregistration_controller_unfinished_work_seconds`
* `AvailableConditionController_adds`
* `DiscoveryController_longest_running_processor_microseconds`
* `autoregister_queue_latency`
* `crd_autoregistration_controller_adds`
* `non_structural_schema_condition_controller_work_duration`
* `APIServiceRegistrationController_adds`
* `crd_finalizer_work_duration`
* `crd_naming_condition_controller_unfinished_work_seconds`
* `crd_openapi_controller_longest_running_processor_microseconds`
* `DiscoveryController_adds`
* `crd_autoregistration_controller_longest_running_processor_microseconds`
* `autoregister_unfinished_work_seconds`
* `crd_naming_condition_controller_queue_latency`
* `crd_naming_condition_controller_retries`
* `non_structural_schema_condition_controller_queue_latency`
* `crd_naming_condition_controller_depth`
* `AvailableConditionController_longest_running_processor_microseconds`
* `crdEstablishing_depth`
* `crd_finalizer_longest_running_processor_microseconds`
* `crd_naming_condition_controller_adds`
* `APIServiceOpenAPIAggregationControllerQueue1_longest_running_processor_microseconds`
* `DiscoveryController_queue_latency`
* `DiscoveryController_unfinished_work_seconds`
* `crd_openapi_controller_depth`
* `APIServiceOpenAPIAggregationControllerQueue1_queue_latency`
* `APIServiceOpenAPIAggregationControllerQueue1_unfinished_work_seconds`
* `DiscoveryController_work_duration`
* `autoregister_adds`
* `crd_autoregistration_controller_queue_latency`
* `crd_finalizer_retries`
* `AvailableConditionController_unfinished_work_seconds`
* `autoregister_longest_running_processor_microseconds`
* `non_structural_schema_condition_controller_unfinished_work_seconds`
* `APIServiceOpenAPIAggregationControllerQueue1_depth`
* `AvailableConditionController_depth`
* `DiscoveryController_retries`
* `admission_quota_controller_depth`
* `crdEstablishing_adds`
* `APIServiceOpenAPIAggregationControllerQueue1_retries`
* `crdEstablishing_queue_latency`
* `non_structural_schema_condition_controller_longest_running_processor_microseconds`
* `autoregister_work_duration`
* `crd_openapi_controller_retries`
* `APIServiceRegistrationController_work_duration`
* `crdEstablishing_work_duration`
* `crd_finalizer_adds`
* `crd_finalizer_depth`
* `crd_openapi_controller_queue_latency`
* `APIServiceOpenAPIAggregationControllerQueue1_work_duration`
* `APIServiceRegistrationController_queue_latency`
* `crd_autoregistration_controller_depth`
* `AvailableConditionController_queue_latency`
* `admission_quota_controller_queue_latency`
* `crd_naming_condition_controller_work_duration`
* `crd_openapi_controller_work_duration`
* `DiscoveryController_depth`
* `crd_naming_condition_controller_longest_running_processor_microseconds`
* `APIServiceRegistrationController_depth`
* `APIServiceRegistrationController_longest_running_processor_microseconds`
* `crd_finalizer_unfinished_work_seconds`
* `crdEstablishing_retries`
* `admission_quota_controller_unfinished_work_seconds`
* `non_structural_schema_condition_controller_adds`
* `APIServiceRegistrationController_unfinished_work_seconds`
* `admission_quota_controller_work_duration`
* `autoregister_depth`
* `autoregister_retries`
* `kubeproxy_sync_proxy_rules_latency_microseconds`
* `rest_client_request_latency_seconds`
* `non_structural_schema_condition_controller_retries`

[id="ocp-4-4-high-granularity-request-duration-buckets"]
==== High granularity request duration buckets in Prometheus

High granularity request duration buckets were dropped in Prometheus, which
were tracked with the `apiserver_request_duration_seconds_bucket` metric. This
leaves enough buckets for meaningful alerting from other monitoring components,
but drastically reduces cardinality.

[id="ocp-4-4-deprecated-features"]
=== Deprecated features

[id="ocp-4-4-oc-config-flag"]
==== OpenShift CLI config flag

The `--config` flag used with `oc` is deprecated. You should start using the
`--kubeconfig` flag instead.

[id="ocp-4-4-oc-timeout-flag"]
==== OpenShift CLI timeout flag

The `--timeout` flag used with `oc rsh` is deprecated. You should start using
the `--request-timeout` flag instead.

[id="ocp-4-4-oc-editor"]
==== OpenShift editor

The `OS_EDITOR` is deprecated. Users should start using `KUBE_EDITOR` or
`EDITOR` instead.

[id="ocp-4-4-machinecidr-network-param"]
==== machineCIDR network parameter

The `machineCIDR` network parameter used in the `install-config.yaml` file is
now deprecated. You should use `machineNetwork.cidr` instead.

[id="ocp-4-4-service-catalog-deprecated"]
==== Service Catalog, Template Service Broker, Ansible Service Broker, and their Operators

Service Catalog, Template Service Broker, Ansible Service Broker, and their
associated Operators were deprecated in {product-title} 4.2.

Ansible Service Broker, the Ansible Service Broker Operator, and the following
APBs have now been removed starting in {product-title} 4.4:

* APB base image
* APB tools container
* PostgreSQL APB
* MySQL APB
* MariaDB APB

The following related APIs have also been removed:

* `.automationbroker.io/v1alpha1`
* `.osb.openshift.io/v1`

Service Catalog and Template Service Broker will be removed in a future
{product-title} release. If they are enabled in 4.4, the web console warns the
user that these features are still enabled.

The following alerts can be viewed from the *Monitoring* -> *Alerting* page and
have a *Warning* severity:

* `ServiceCatalogAPIServerEnabled`
* `ServiceCatalogControllerManagerEnabled`
* `TemplateServiceBrokerEnabled`

The following related API will be removed in a future release:

* `.servicecatalog.k8s.io/v1beta1`

[id="ocp-4-4-deprecation-of-operatorsources"]
==== Deprecation of OperatorSources, CatalogSourceConfigs, and packaging format

[id="ocp-4-4-marketplace-apis-deprecated"]
OperatorSources and CatalogSourceConfigs are deprecated from OperatorHub. The
following related APIs will be removed in a future release:

* `operatorsources.operators.coreos.com/v1`
* `catalogsourceconfigs.operators.coreos.com/v2`
* `catalogsourceconfigs.operators.coreos.com/v1`

The Operator Framework's current packaging format is also being deprecated in a
future release, to be replaced by a new bundle format. As a result, the
following command will also be deprecated at that time:

* `oc adm catalog build`

For more information on the upcoming new Operator bundle format and Operator
Package Manager CLI (`opm`), see the link:https://docs.okd.io/latest/operators/understanding_olm/olm-understanding-olm.html#olm-new-bundle-opm_olm-understanding-olm[upstream OKD documentation].

[id="ocp-4-4-bug-fixes"]
== Bug fixes

*Web Console*

* Previously, when two consecutive rollouts failed, the *Topology* view showed failed pods instead of displaying the last active revision. With this bug fix, when a rollout fails, the last active revision is displayed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1760828[*BZ#1760828*])

* Previously, existing imagestreams in a namespace were not detected when creating an application. This occurred when users with limited cluster-wide permissions used the *Container Image* -> *Image name from the internal registry* options in the *Add* page. Now, the imagestream fetching logic has been moved from the cluster level to the namespace level, which enables a user with permission to a namespace to see imagestreams in that namespace.(link:https://bugzilla.redhat.com/show_bug.cgi?id=1784264[*BZ#1784264*])

[id="ocp-4-4-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These
experimental features are not intended for production use. Note the
following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview
Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *DEP*: _Deprecated_
* *-*: _Not Available_

.Technology Preview Tracker
[cols="4",options="header"]
|====
|Feature |OCP 4.2 |OCP 4.3 |OCP 4.4

|Prometheus Cluster Monitoring
|GA
|GA
|GA

|Precision Time Protocol (PTP)
|-
|TP
|TP

|CRI-O for runtime Pods
|GA
|GA
|GA

|`oc` CLI Plug-ins
|TP
|TP
|TP

|Service Catalog
|DEP
|DEP
|DEP

|Template Service Broker
|DEP
|DEP
|DEP

|OpenShift Ansible Service Broker
|DEP
|DEP
|-

|Network Policy
|GA
|GA
|GA

|Multus
|GA
|GA
|GA

|New Add Project Flow
|GA
|GA
|GA

|Search Catalog
|GA
|GA
|GA

|Cron Jobs
|GA
|GA
|GA

|Kubernetes Deployments
|GA
|GA
|GA

|StatefulSets
|GA
|GA
|GA

|Explicit Quota
|GA
|GA
|GA

|Mount Options
|GA
|GA
|GA

|System Containers for Docker, CRI-O
|-
|-
|-

|Hawkular Agent
|-
|-
|-

|Pod PreSets
|-
|-
|-

|experimental-qos-reserved
|TP
|TP
|TP

|Pod sysctls
|GA
|GA
|GA

|Central Audit
|-
|-
|-

|Static IPs for External Project Traffic
|GA
|GA
|GA

|Template Completion Detection
|GA
|GA
|GA

|`replicaSet`
|GA
|GA
|GA

|Clustered MongoDB Template
|-
|-
|-

|Clustered MySQL Template
|-
|-
|-

|ImageStreams with Kubernetes Resources
|GA
|GA
|GA

|Device Manager
|GA
|GA
|GA

|Persistent Volume Resize
|GA
|GA
|GA

|Huge Pages
|GA
|GA
|GA

|CPU Pinning
|GA
|GA
|GA

|Admission Webhooks
|GA
|GA
|GA

|External provisioner for AWS EFS
|TP
|TP
|TP

|Pod Unidler
|TP
|TP
|TP

|Node Problem Detector
|TP
|TP
|TP

|Ephemeral Storage Limit/Requests
|TP
|TP
|TP

|Descheduler
|-
|-
|TP

|CephFS Provisioner
|-
|-
|-

|Podman
|TP
|TP
|TP

|Kuryr CNI Plug-in
|TP
|GA
|GA

|Sharing Control of the PID Namespace
|TP
|TP
|TP

|Manila Provisioner
|-
|-
|-

|Cluster Administrator console
|GA
|GA
|GA

|Cluster Autoscaling
|GA
|GA
|GA

|Container Storage Interface (CSI)
|GA
|GA
|GA

|Operator Lifecycle Manager
|GA
|GA
|GA

|Red Hat OpenShift Service Mesh
|GA
|GA
|GA

|"Fully Automatic" Egress IPs
|GA
|GA
|GA

|Pod Priority and Preemption
|GA
|GA
|GA

|Multi-stage builds in Dockerfiles
|GA
|GA
|GA

|OVN-Kubernetes Pod network provider
|TP
|TP
|TP

|HPA custom metrics adapter based on Prometheus
|TP
|TP
|TP

|Machine health checks
|TP
|GA
|GA

|Persistent Storage with iSCSI
|TP
|GA
|GA

|Raw Block with iSCSI
|TP
|GA
|GA

|Raw Block with Cinder
|-
|TP
|TP

|OperatorHub
|GA
|GA
|GA

|Three-node bare metal deployments
|TP
|TP
|TP

|SR-IOV Network Operator
|TP
|GA
|GA

|Helm CLI
|-
|TP
|GA

|Service Binding
|-
|TP
|TP

|Log forwarding
|-
|TP
|TP

|User workload monitoring
|-
|TP
|TP

|OpenShift Serverless
|TP
|TP
|GA

|Compute Node Topology Manager
|-
|TP
|TP

|CSI volume snapshots
|-
|-
|TP

|CSI volume cloning
|-
|-
|TP

|OpenShift Pipelines
|-
|TP
|TP

|====

[id="ocp-4-4-known-issues"]
== Known issues

* There is an issue with the Machine Config Operator (MCO) supporting Day 2
proxy support, which describes when an existing non-proxied
cluster is reconfigured to use a proxy. The MCO should apply newly configured
proxy CA certificates in a ConfigMap to the {op-system} trust bundle; this is
not working. As a workaround, you must manually add the proxy CA certificate to
your trust bundle and then update the trust bundle:
+
----
$ cp /opt/registry/certs/<my_root_ca>.crt /etc/pki/ca-trust/source/anchors/
$ update-ca-trust extract
$ oc adm drain <node>
$ systemctl reboot
----
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1784201[*BZ#1784201*])

* When using a self-signed {rh-openstack-first} 16 cluster, you cannot pull from
or push to an internal image registry. As a workaround, you must set
`spec.disableRedirects = true` in the `configs.imageregistry/cluster` resource.
This allows the client to pull the image layers from the image registry rather
than from links directly from Swift.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1810461[*BZ#1810461*])

* The cluster proxy configuration `HTTP_PROXY` is only available for
{product-title} components, not user applications. As a workaround, you must run
the following command to enable cluster proxy configuration for user
applications:
+
----
$ oc set env dc/jenkins \
    http_proxy=$(oc get proxy cluster -o jsonpath='{.status.httpProxy}') \
    https_proxy=$(oc get proxy cluster -o jsonpath='{.status.httpsProxy}') \
    no_proxy=$(oc get proxy cluster -o jsonpath='{.status.noProxy}')
----
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1780125[*BZ#1780125*])

* All `git clone` operations that go through an HTTPS proxy fail. HTTP proxies
can be used successfully.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1750650[*BZ#1750650*])

* All `git clone` operations fail in builds running behind a proxy if the source
URIs use the `git://` or `ssh://` scheme.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1751738[*BZ#1751738*])

* When using a mirror to build images, the build fails when the pull secret for
the mirror registry only links to the builder service account. The pull secret
must also link to the build config object.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1810904[*BZ#1810904*])

* In {rh-openstack-first} 13 with Kuryr, if FIPS is disabled, you cannot enable
Service Catalog. Pods for Service Catalog's controller manager and API server
components show a status of `CrashLoopBackOff`. This is due to the
`\https://etcd.openshift-etcd.svc.cluster.local:2379` URL not always resolving.
There is a new technique for getting the etcd cluster URL in {product-title} 4.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821589[*BZ#1821589*])

* Installing {rh-openstack} 16 with Kuryr does not work due to the
`ovn_controller` crashing after initial setup.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1812009[*BZ#1812009*])

* The Red Hat Virtualization (RHV) machine `instance-state` annotation and the
`providerStatus.instanceState` status do not always match. This mismatch causes
the client to fail or incorrectly patch the RHV machine status.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1815394[*BZ#1815394*])

* When scaling up a MachineSet on RHV, the new machine cannot exit the
`Provisioned` phase. This causes the machine to never run.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1815435[*BZ#1815435*], link:https://bugzilla.redhat.com/show_bug.cgi?id=1817853[*BZ#1817853*])

* {product-title} cluster autoscaling on RHV fails due to cluster resource
computation errors.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1822118[*BZ#1822118*])

* When using the Firefox browser to select a node or a group of nodes in the *Topology* view, the backgrounds of all the associated labels and nodes become transparent. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1822337[*BZ#1822337*])

* In the *Topology view*, when a user selects a node or workload, and then  clicks *Monitoring* -> *View monitoring dashboard* on the side panel, the user sees the monitoring dashboard for that specific workload. This filtered workload dashboard view is not clearly named, which causes confusion with the generic dashboard that displays metrics for all the workloads. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1822331[*BZ#1822331*])

* When invalid characters such as a period (.) are entered in the serverless traffic distribution tag from the *Developer* perspective, the traffic distribution feature stops working. However, it displays no error messages to prevent invalid characters from being entered in the tag. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1822344[*BZ#1822344*])
