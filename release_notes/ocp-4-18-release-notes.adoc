:_mod-docs-content-type: ASSEMBLY
[id="ocp-4-18-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red{nbsp}Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-18-about-this-release_{context}"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-2024:xxxx[RHSA-2024:xxxx]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md[Kubernetes 1.31] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. From the {hybrid-console}, you can deploy {product-title} clusters to either on-premises or cloud environments.

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.8 and a later version of {op-system-base} 8 that is released before End of Life of {product-title} {product-version}. {product-title} {product-version} is also supported on {op-system-first}. To understand {op-system-base} versions used by {op-system}, see link:https://access.redhat.com/articles/6907891[{op-system-base} Versions Utilized by {op-system-first} and {product-title}] (Knowledgebase article).

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines. {op-system-base} machines are deprecated in {product-title} 4.16 and will be removed in a future release.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//Even-numbered release lifecycle verbiage (Comment in for even-numbered releases)
Starting from {product-title} 4.14, the Extended Update Support (EUS) phase for even-numbered releases increases the total available lifecycle to 24 months on all supported architectures, including `x86_64`, 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures. Beyond this, Red{nbsp}Hat also offers a 12-month additional EUS add-on, denoted as _Additional EUS Term 2_, that extends the total available lifecycle from 24 months to 36 months. The Additional EUS Term 2 is available on all architecture variants of {product-title}. For more information about support for all versions, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

//Odd-numbered release lifecycle verbiage (Comment in for odd-numbered releases)
////
The support lifecycle for odd-numbered releases, such as {product-title} {product-version}, on all supported architectures, including `x86_64`, 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures is 18 months. For more information about support for all versions, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].
////

Commencing with the {product-title} 4.14 release, Red{nbsp}Hat is simplifying the administration and management of Red{nbsp}Hat shipped cluster Operators with the introduction of three new life cycle classifications; Platform Aligned, Platform Agnostic, and Rolling Stream. These life cycle classifications provide additional ease and transparency for cluster administrators to understand the life cycle policies of each Operator and form cluster maintenance and upgrade plans with predictable support boundaries. For more information, see link:https://access.redhat.com/webassets/avalon/j/includes/session/scribe/?redirectTo=https%3A%2F%2Faccess.redhat.com%2Fsupport%2Fpolicy%2Fupdates%2Fopenshift_operators[OpenShift Operator Life Cycles].

// Added in 4.14. Language came directly from Kirsten Newcomer.
{product-title} is designed for FIPS. When running {op-system-base-full} or {op-system-first} booted in FIPS mode, {product-title} core components use the {op-system-base} cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the `x86_64`, `ppc64le`, and `s390x` architectures.

For more information about the NIST validation program, see link:https://csrc.nist.gov/Projects/cryptographic-module-validation-program/validated-modules[Cryptographic Module Validation Program]. For the latest NIST status for the individual versions of {op-system-base} cryptographic libraries that have been submitted for validation, see link:https://access.redhat.com/articles/2918071#fips-140-2-and-fips-140-3-2[Compliance Activities and Government Standards].

[id="ocp-4-18-add-on-support-status_{context}"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-18-new-features-and-enhancements_{context}"]
== New features and enhancements

This release adds improvements related to the following components and concepts:

[id="ocp-release-notes-auth_{context}"]
=== Authentication and authorization

[id="ocp-release-notes-auth-ccoctl-rotation_{context}"]
==== Rotating OIDC bound service account signer keys

With this release, you can use the Cloud Credential Operator (CCO) utility (`ccoctl`) to rotate the OpenID Connect (OIDC) bound service account signer key for clusters installed on the following cloud providers:

* xref:../post_installation_configuration/changing-cloud-credentials-configuration.adoc#rotating-bound-service-keys_key-rotation-aws[{aws-first} with {sts-first}]
* xref:../post_installation_configuration/changing-cloud-credentials-configuration.adoc#rotating-bound-service-keys_key-rotation-gcp[{gcp-first} with {gcp-wid-short}]
* xref:../post_installation_configuration/changing-cloud-credentials-configuration.adoc#rotating-bound-service-keys_key-rotation-azure[{azure-first} with {entra-short}]

[id="ocp-release-notes-backup-restore_{context}"]
=== Backup and restore

[id="ocp-4-18-hibernating_{context}"]
==== Hibernating a cluster for up to 90 days

With this release, you can now hibernate your {product-title} cluster for up to 90 days and expect the cluster to recover successfully. Before this release, you could only hibernate for up to 30 days.

For more information, see xref:../backup_and_restore/hibernating-cluster.adoc#hibernating-cluster[Hibernating an {product-title} cluster].

[id="ocp-release-notes-builds_{context}"]
=== Builds

[id="ocp-release-notes-cro_{context}"]
=== Cluster Resource Override Admission Operator

[id="ocp-release-notes-extensions_{context}"]
=== Extensions ({olmv1})

[id="ocp-release-notes-edge-computing_{context}"]
=== Edge computing

[id="ocp-4-18-edge-computing-sno-shutdown_{context}"]
==== Shutting down and restarting {sno} clusters up to 1 year after cluster installation

With this release, you can shut down and restart {sno} clusters up to 1 year after cluster installation. If certificates expired while the cluster was shut down, you must approve certificate signing requests (CSRs) upon restarting the cluster.

Before this update, you could shut down and restart {sno} clusters for only 120 days after cluster installation.

[IMPORTANT]
====
Evacuate all workload pods from the {sno} cluster before you shut it down.
====

For more information, see xref:../backup_and_restore/graceful-cluster-shutdown.adoc#graceful-shutdown-cluster[Shutting down the cluster gracefully].

[id="ocp-4-18-deprecation-of-siteconfig-v1_{context}"]
==== Deprecation of SiteConfig v1

SiteConfig v1 is deprecated starting with {product-title} 4.18. Equivalent and improved functionality is now available through the SiteConfig Operator using the `ClusterInstance` custom resource. For more information, see the Red Hat Knowledge Base solution link:https://access.redhat.com/articles/7105238[Procedure to transition from SiteConfig CRs to the ClusterInstance API].

For more information about the SiteConfig Operator, see link:https://docs.redhat.com/en/documentation/red_hat_advanced_cluster_management_for_kubernetes/2.12/html-single/multicluster_engine_operator_with_red_hat_advanced_cluster_management/index#siteconfig-intro[SiteConfig].

[id="ocp-release-notes-hcp_{context}"]
=== Hosted control planes

[id="ocp-release-notes-ibm-z_{context}"]
=== {ibm-z-title} and {ibm-linuxone-title}

With this release, {ibm-z-name} and {ibm-linuxone-name} are now compatible with {product-title} {product-version}. You can perform the installation with z/VM, LPAR, or {op-system-base-full} Kernel-based Virtual Machine (KVM). For installation instructions, see
xref:../installing/installing_ibm_z/preparing-to-install-on-ibm-z.adoc#preparing-to-install-on-ibm-z[Installation methods].

[IMPORTANT]
====
Compute nodes must run {op-system-first}.
====

[discrete]
[id="ocp-4-18-ibm-z-enhancements_{context}"]
==== {ibm-z-title} and {ibm-linuxone-title} notable enhancements

The {ibm-z-name} and {ibm-linuxone-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components and concepts.

This release introduces support for the following features on {ibm-z-name} and {ibm-linuxone-name}:

* Adding compute nodes to on-premise clusters using {oc-first}

[id="ocp-release-notes-ibm-power_{context}"]
=== {ibm-power-title}

The {ibm-power-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on {ibm-power-title}:

* Added four new data centers to PowerVS Installer Provisioned Infrastructure deployments
* Adding compute nodes to on-premise clusters using {oc-first}

[discrete]
[id="ocp-release-notes-ibm-z-power-support-matrix_{context}"]
=== {ibm-power-title}, {ibm-z-title}, and {ibm-linuxone-title} support matrix

Starting in {product-title} 4.14, Extended Update Support (EUS) is extended to the {ibm-power-name} and the {ibm-z-name} platform. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

.{product-title} features
[cols="3,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Adding compute nodes to on-premise clusters using {oc-first}
|Supported
|Supported

|Alternate authentication providers
|Supported
|Supported

|Agent-based Installer
|Supported
|Supported

|Assisted Installer
|Supported
|Supported

|Automatic Device Discovery with Local Storage Operator
|Unsupported
|Supported

|Automatic repair of damaged machines with machine health checking
|Unsupported
|Unsupported

|Cloud controller manager for {ibm-cloud-name}
|Supported
|Unsupported

|Controlling overcommit and managing container density on nodes
|Unsupported
|Unsupported

|CPU manager
|Supported
|Supported

|Cron jobs
|Supported
|Supported

|Descheduler
|Supported
|Supported

|Egress IP
|Supported
|Supported

|Encrypting data stored in etcd
|Supported
|Supported

|FIPS cryptography
|Supported
|Supported

|Helm
|Supported
|Supported

|Horizontal pod autoscaling
|Supported
|Supported

|Hosted control planes
|Supported
|Supported

|IBM Secure Execution
|Unsupported
|Supported

|Installer-provisioned Infrastructure Enablement for {ibm-power-server-name}
|Supported
|Unsupported

|Installing on a single node
|Supported
|Supported

|IPv6
|Supported
|Supported

|Monitoring for user-defined projects
|Supported
|Supported

|Multi-architecture compute nodes
|Supported
|Supported

|Multi-architecture control plane
|Supported
|Supported

|Multipathing
|Supported
|Supported

|Network-Bound Disk Encryption - External Tang Server
|Supported
|Supported

|Non-volatile memory express drives (NVMe)
|Supported
|Unsupported

|nx-gzip for Power10 (Hardware Acceleration)
|Supported
|Unsupported

|oc-mirror plugin
|Supported
|Supported

|OpenShift CLI (`oc`) plugins
|Supported
|Supported

|Operator API
|Supported
|Supported

|OpenShift Virtualization
|Unsupported
|Supported

|OVN-Kubernetes, including IPsec encryption
|Supported
|Supported

|PodDisruptionBudget
|Supported
|Supported

|Precision Time Protocol (PTP) hardware
|Unsupported
|Unsupported

|{openshift-local-productname}
|Unsupported
|Unsupported

|Scheduler profiles
|Supported
|Supported

|Secure Boot
|Unsupported
|Supported

|Stream Control Transmission Protocol (SCTP)
|Supported
|Supported

|Support for multiple network interfaces
|Supported
|Supported

|The `openshift-install` utility to support various SMT levels on {ibm-power-name} (Hardware Acceleration)
|Supported
|Supported

|Three-node cluster support
|Supported
|Supported

|Topology Manager
|Supported
|Unsupported

|z/VM Emulated FBA devices on SCSI disks
|Unsupported
|Supported

|4K FCP block device
|Supported
|Supported
|====

.Persistent storage options
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}
|Persistent storage using iSCSI
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using local volumes (LSO)
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using hostPath
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Fibre Channel
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Raw Block
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using EDEV/FBA
|Supported ^[1]^
|Supported ^[1]^,^[2]^
|====
[.small]
--
1. Persistent shared storage must be provisioned by using either {rh-storage-first} or other supported storage protocols.
2. Persistent non-shared storage must be provisioned by using local storage, such as iSCSI, FC, or by using LSO with DASD, FCP, or EDEV/FBA.
--

.Operators
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|{cert-manager-operator}
|Supported
|Supported

|Cluster Logging Operator
|Supported
|Supported

|Cluster Resource Override Operator
|Supported
|Supported

|Compliance Operator
|Supported
|Supported

|Cost Management Metrics Operator
|Supported
|Supported

|File Integrity Operator
|Supported
|Supported

|HyperShift Operator
|Supported
|Supported

|{ibm-power-server-name} Block CSI Driver Operator
|Supported
|Unsupported

|Ingress Node Firewall Operator
|Supported
|Supported

|Local Storage Operator
|Supported
|Supported

|MetalLB Operator
|Supported
|Supported

|Network Observability Operator
|Supported
|Supported

|NFD Operator
|Supported
|Supported

|NMState Operator
|Supported
|Supported

|OpenShift Elasticsearch Operator
|Supported
|Supported

|Vertical Pod Autoscaler Operator
|Supported
|Supported
|====

.Multus CNI plugins
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Bridge
|Supported
|Supported

|Host-device
|Supported
|Supported

|IPAM
|Supported
|Supported

|IPVLAN
|Supported
|Supported
|====

.CSI Volumes
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Cloning
|Supported
|Supported

|Expansion
|Supported
|Supported

|Snapshot
|Supported
|Supported
|====

[id="ocp-release-notes-insights-operator-enhancements_{context}"]
=== Insights Operator
[id="ocp-release-notes-insights-operator-runtime-extractor_{context}"]
==== Insights Runtime Extractor (Technology Preview)

In this release, the Insights Operator introduces the workload data collection _Insights Runtime Extractor_ feature to help Red{nbsp}Hat better understand the workload of your containers.
Available as a Technology Preview, the Insights Runtime Extractor feature gathers runtime workload data and sends it to Red{nbsp}Hat.
Red{nbsp}Hat uses the collected runtime workload data to gain insights that can help you make investment decisions that will drive and optimize how you use your {product-title} containers.
For more information, see xref:../nodes/clusters/nodes-cluster-enabling-features.adoc#nodes-cluster-enabling[_Enabling features using feature gates_].

[id="ocp-release-notes-installation-and-update_{context}"]
=== Installation and update

[id="ocp-4-18-installation-and-update-ovn-kubernetes-join-subnet_{context}"]
==== Configuring the `ovn-kubernetes` join subnet during cluster installation
With this release, you can configure the IPv4 join subnet that is used internally by `ovn-kubernetes` when installing a cluster. You can set the `internalJoinSubnet` parameter in the `install-config.yaml` file and deploy the cluster into an existing Virtual Private Cloud (VPC).

For more information, see xref:../installing/installing_aws/installation-config-parameters-aws.adoc#installation-configuration-parameters-network_installation-config-parameters-aws[Network configuration parameters].

[id="ocp-4-18-updating-oc-adm-upgrade-recommend_{context}"]
==== Introducing the oc adm upgrade recommend command (Technology Preview)
When updating your cluster, the `oc adm upgrade` command returns a list of the next available versions. As long as you are using 4.18 `oc` client binary, you can use the `oc adm upgrade recommend` command to narrow down your suggestions and recommend a new target release before you launch your update. This feature is available for {product-title} version 4.16 and newer clusters that are connected to an update service.

For more information, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#update-upgrading-cli_updating-cluster-cli[Updating a cluster by using the CLI]

[id="ocp-4-18-installation-and-update-nc2-aws-support_{context}"]
==== Support for Nutanix Cloud Clusters (NC2) on {aws-first} and NC2 on {azure-first}
With this release, you can install {product-title} on Nutanix Cloud Clusters (NC2) on {aws-short} or NC2 on {azure-short}.

For more information, see xref:../installing/installing_nutanix/preparing-to-install-on-nutanix.adoc#installation-nutanix-installer-infrastructure-reqs_preparing-to-install-on-nutanix[Infrastructure requirements].

[id="ocp-4-18-installation-and-update-gcp-c4-c4a_{context}"]
==== Installing a cluster on {gcp-full} using the C4 and C4A machine series

With this release, you can deploy a cluster on {gcp-short} using the C4 and C4A machine series for compute or control plane machines. The supported disk type of these machines is `hyperdisk-balanced`. If you use an instance type that requires Hyperdisk storage, all of the nodes in your cluster must support Hyperdisk storage, and you must change the default storage class to use Hyperdisk storage.

For more information about configuring machine types, see xref:../installing/installing_gcp/installation-config-parameters-gcp.adoc#installation-configuration-parameters-gcp[Installation configuration parameters for GCP], link:https://cloud.google.com/compute/docs/general-purpose-machines#c4_series[C4 machine series] (Compute Engine docs), and link:https://cloud.google.com/compute/docs/general-purpose-machines#c4a_series[C4A machine series] (Compute Engine docs).

[id="ocp-4-18-installation-and-update-gcp-byo-vpc-phz_{context}"]
==== Provide your own private hosted zone when installing a cluster on {gcp-full}
With this release, you can provide your own private hosted zone when installing a cluster on {gcp-short} into a shared VPC. If you do, the requirements for the bring your own (BYO) zone are that the zone must use a DNS name such as `<cluster name>.<base domain>.` and that you bind the zone to the VPC network of the cluster.

For more information, see xref:../installing/installing_gcp/installing-gcp-shared-vpc.adoc#installation-gcp-shared-vpc-prerequisites_installing-gcp-shared-vpc[Prerequisites for installing a cluster on GCP into a shared VPC] and xref:../installing/installing_gcp/installing-gcp-user-infra-vpc.adoc#prerequisites[Prerequisites for installing a cluster into a shared VPC on GCP using Deployment Manager templates].

[id="ocp-4-18-installation-and-update-nutanix-preloaded-image-support_{context}"]
==== Installing a cluster on Nutanix by using a preloaded {op-system} image object
With this release, you can install a cluster on Nutanix by using the named, preloaded {op-system} image object from the private cloud or the public cloud. Rather than creating and uploading a {op-system} image object for each {product-title} cluster, you can use the `preloadedOSImageName` parameter in the `install-config.yaml` file.

For more information, see xref:../installing/installing_nutanix/installation-config-parameters-nutanix.adoc#installation-configuration-parameters-additional-nutanix_installation-config-parameters-nutanix[Additional Nutanix configuration parameters].

[id="ocp-4-18-installation-and-update-nutanix-multiple-nics_{context}"]
==== Installing a cluster on Nutanix with up to 32 subnets
With this release, Nutanix supports more than one subnet for the Prism Element where you deployed an {product-title} cluster to. A maximum of 32 subnets for each Prism Element is supported.

For more information, see xref:../installing/installing_nutanix/installing-nutanix-installer-provisioned.adoc#installation-configuring-nutanix-failure-domains_installing-nutanix-installer-provisioned[Configuring failure domains] and xref:../installing/installing_nutanix/installation-config-parameters-nutanix.adoc#installation-configuration-parameters-additional-nutanix_installation-config-parameters-nutanix[Additional Nutanix configuration parameters].

For an existing Nutanix cluster, you can add multiple subnets by using machine sets. For more information, see xref:../installing/installing_nutanix/nutanix-failure-domains.adoc#post-installation-configuring-nutanix-failure-domains_nutanix-failure-domains[Adding failure domains to the Infrastructure CR].

[id="ocp-release-notes-agent-5-node-control-plane_{context}"]
==== Configuring 4 and 5 node control planes with the Agent-based Installer
With this release, if you are using the Agent-based Installer, you can now configure your cluster to be installed with either 4 or 5 nodes in the control plane. This feature is enabled by setting the `controlPlane.replicas` parameter to either `4` or `5` in the `install-config.yaml` file.

For more information, see xref:../installing/installing_with_agent_based_installer/installation-config-parameters-agent.adoc#installation-configuration-parameters-optional_installation-config-parameters-agent[Optional configuration parameters] for the Agent-based Installer.

[id="ocp-release-notes-agent-minimal-iso_{context}"]
==== Minimal ISO image support for the Agent-based Installer
With this release, the Agent-based Installer supports creating a minimal ISO image on all supported platforms. Previously, minimal ISO images were supported only on the `external` platform.

This feature is enabled using the `minimalISO` parameter in the `agent-config.yaml` file.

For more information, see xref:../installing/installing_with_agent_based_installer/installation-config-parameters-agent.adoc#agent-configuration-parameters-optional_installation-config-parameters-agent[Optional configuration parameters] for the Agent-based Installer.

[id="ocp-release-notes-agent-iscsi_{context}"]
==== Internet Small Computer System Interface (iSCSI) boot support for the Agent-based Installer
With this release, the Agent-based Installer supports creating assets that can be used to boot an {product-title} cluster from an iSCSI target.

For more information, see xref:../installing/installing_with_agent_based_installer/installing-using-iscsi.adoc#installing-using-iscsi[Preparing installation assets for iSCSI booting].

[id="ocp-release-notes-olm_{context}"]
=== Operator lifecycle

[id="ocp-release-notes-osdk_{context}"]
=== Operator development

[id="ocp-release-notes-openshift-cli_{context}"]
=== OpenShift CLI (oc)

[id="ocp-release-notes-oci_{context}"]
=== {oci-first}

[id="ocp-release-notes-oci-bare-metal_{context}"]
==== Bare-metal support on {oci-first}
{product-title} cluster installations on {oci-first} are now supported for bare-metal machines. You can install bare-metal clusters on {oci} by using either the Assisted Installer or the Agent-based Installer. To install a bare-metal cluster on {oci}, choose one of the following installation options:

* xref:../installing/installing_oci/installing-oci-assisted-installer.adoc#installing-oci-assisted-installer[Installing a cluster on {oci-first-no-rt} by using the {ai-full}]
* xref:../installing/installing_oci/installing-oci-agent-based-installer.adoc#installing-oci-agent-based-installer[Installing a cluster on {oci-first-no-rt} by using the Agent-based Installer]

[id="ocp-release-notes-machine-config-operator_{context}"]
=== Machine Config Operator

[id="ocp-release-notes-machine-config-operator-aws-boot-ga_{context}"]
==== Updated boot images for AWS clusters promoted to GA
Updated boot images has been promoted to GA for Amazon Web Services (AWS) clusters. For more information, see xref:../machine_configuration/mco-update-boot-images.adoc#mco-update-boot-images[Updated boot images].

[id="ocp-release-notes-machine-config-operator-imageconfignodes_{context}"]
==== Expanded image config nodes information (Technology Preview)
The image config nodes custom resource, which allows you to monitor the progress of machine configuration updates to nodes, now presents more information on the update. The output of the `oc get machineconfignodes` command now reports on the following and other conditions. You can use these statuses to follow the update, or troubleshoot the node if it experiences an error during the update:

* If each node was cordoned and uncordoned
* If each node was drained
* If each node was rebooted
* If a node had a CRI-O reload
* If a node had the operating system and node files updated

[id="ocp-release-notes-machine-config-operator-ocl_{context}"]
==== On-cluster layering changes (Technology Preview)

There are several important changes to the on-cluster layering feature:

* You can now install extensions onto an on-cluster customer layered image by using a `MachineConfig` object.
* Updating the Containerfile in a `MachineOSConfig` object now triggers a build to be performed.
* You can now revert an on-cluster custom layered image back to the base image by removing a label from the `MachineOSConfig` object.
* The `must-gather` for the Machine Config Operator now includes data on the `MachineOSConfig` and `MachineOSBuild` objects.

For more information about on-cluster layering, see xref:../machine_configuration/mco-coreos-layering.html#coreos-layering-configuring-on_mco-coreos-layering[Using on-cluster layering to apply a custom layered image].

[id="ocp-release-notes-machine-management_{context}"]
=== Machine management

[id="ocp-4-18-capi-tp-azure_{context}"]
==== Managing machines with the Cluster API for {azure-full} (Technology Preview)

This release introduces the ability to manage machines by using the upstream Cluster API, integrated into {product-title}, as a Technology Preview for {azure-full} clusters.
This capability is in addition or an alternative to managing machines with the Machine API.
For more information, see xref:../machine_management/cluster_api_machine_management/cluster-api-about.adoc#cluster-api-about[About the Cluster API].

[id="ocp-release-notes-monitoring_{context}"]
=== Monitoring

The in-cluster monitoring stack for this release includes the following new and modified features:

[id="ocp-4-18-monitoring-updates-to-monitoring-stack-components-and-dependencies"]
==== Updates to monitoring stack components and dependencies

This release includes the following version updates for in-cluster monitoring stack components and dependencies:

* Metrics Server to 0.7.2
* Prometheus to 2.55.1
* Prometheus Operator to 0.78.1
* Thanos to 0.36.1

// Note: no alerting rule changes for this release

// [id="ocp-4-18-monitoring-changes-to-alerting-rules"]
// ==== Changes to alerting rules

// [NOTE]
// ====
// Red{nbsp}Hat does not guarantee backward compatibility for recording rules or alerting rules.
// ====

[id="ocp-4-18-monitoring-added-scrape-and-evaluation-intervals-for-uwm-prometheus"]
==== Added scrape and evaluation intervals for user workload monitoring Prometheus

With this update, you can configure the intervals between consecutive scrapes and between rule evaluations for Prometheus for user workload monitoring.

[id="ocp-4-18-monitoring-early-validation-for-configurations-in-monitoring-config-maps"]
==== Added early validation for the monitoring configurations in monitoring config maps

This update introduces early validation for changes to monitoring configurations in `cluster-monitoring-config` and `user-workload-monitoring-config` config maps to provide shorter feedback loops and enhance user experience.

[id="ocp-4-18-monitoring-configure-cross-project-alerting-rules-for-uwm"]
==== Added the proxy environment variables to Alertmanager containers

With this update, Alertmanager uses the proxy environment variables. Therefore, if you configured an HTTP cluster-wide proxy, you can enable proxying by setting the `proxy_from_environment` parameter to `true` in your alert receivers or at the global config level in Alertmanager.

[id="ocp-4-18-monitoring-cross-project-alerting-rules-uwm"]
==== Added cross-project user workload alerting and recording rules

With this update, you can create user workload alerting and recording rules that query multiple projects at the same time.

[id="ocp-release-notes-networking_{context}"]
=== Networking

[id="ocp-4-18-holdover-in-grandmaster-clock_{context}"]
==== Holdover in a grandmaster clock with GNSS as the source

With this release, you can configure the holdover behavior in a grandmaster (T-GM) clock with Global Navigation Satellite System (GNSS) as the source. Holdover allows the T-GM clock to maintain synchronization performance when the GNSS source is unavailable. During this period, the T-GM clock relies on its internal oscillator and holdover parameters to reduce timing disruptions.

You can define the holdover behavior by configuring the following holdover parameters in the `PTPConfig` custom resource (CR):

* `MaxInSpecOffset`
* `LocalHoldoverTimeout`
* `LocalMaxHoldoverOffSet`

For more information, see xref:../networking/ptp/configuring-ptp.adoc#holdover-in-a-grandmaster-clock_configuring-ptp[Holdover in a grandmaster clock with GNSS as the source].

[id="ocp-4-18-support-for-IPVLAN_{context}"]
==== Support for configuring a multi-network policy for IPVLAN and Bond CNI

With this release, you can configure a multi-network policy for the following network types:

* IP Virtual Local Area Network (IPVLAN)
* Bond Container Network Interface (CNI) over SR-IOV

For more information, see xref:../networking/multiple_networks/secondary_networks/configuring-multi-network-policy.adoc[Configuring multi-network policy]

[id="ocp-release-notes-networking-whitelist-blacklist-annotation-updated-allowlist-denylist_{context}"]
==== Updated terminology for whitelist and blacklist annotations
The terminology for the `ip_whitelist` and `ip_blacklist` annotations have been updated to `ip_allowlist` and `ip_denylist`, respectively. Currently, {product-title} still supports the `ip_whitelist` and `ip_blacklist` annotations. However, these annotations are planned for removal in a future release.

[id="ocp-release-notes-networking-ovn-kubernetes-observability_{context}"]
==== Checking OVN-Kubernetes network traffic with OVS sampling using the CLI

OVN-Kubernetes network traffic can be viewed with OVS sampling via the CLI for the following network APIs:

* `NetworkPolicy`
* `AdminNetworkPolicy`
* `BaselineNetworkPolicy`
* `UserDefinesdNetwork` isolation
* `EgressFirewall`
* Multicast ACLs.

Checking OVN-Kubernetes network traffic with OVS sampling using the CLI is intended to help with packet tracing. It can also be used while the Network Observability Operator is installed.

For more information, see xref:../networking/ovn_kubernetes_network_provider/ovn-kubernetes-troubleshooting-sources.adoc#nw-ovn-kubernetes-observability_ovn-kubernetes-sources-of-troubleshooting-information[Checking OVN-Kubernetes network traffic with OVS sampling using the CLI].

[id="ocp-release-notes-networking-dynamic-config-manager_{context}"]
==== The dynamic configuration manager is enabled by default (Technology Preview)

You can reduce your memory footprint by using the dynamic configuration manager on Ingress Controllers. The dynamic configuration manager propagates endpoint changes through a dynamic API. This process enables the underlying routers to adapt to changes (scale ups and scale downs) without reloads.

To use the dynamic configuration manager, enable the `TechPreviewNoUpgrade` feature set by running the following command:

[source,terminal]
----
$ oc patch featuregates cluster -p '{"spec": {"featureSet": "TechPreviewNoUpgrade"}}' --type=merge
----

[id="ocp-release-notes-networking-network-matrix-environments_{context}"]
==== Additional environments for the network flow matrix

With this release, you can view network information for ingress flows to {product-title} services in the following environments:

* {product-title} on bare metal
* {sno-caps} on bare metal
* {product-title} on {aws-first}
* {sno-caps} on {aws-short}

For more information, see  xref:../installing/install_config/configuring-firewall.adoc#network-flow-matrix_configuring-firewall[{product-title} network flow matrix].

[id="ocp-release-notes-networking-metallb-dynamic-asn_{context}"]
==== MetalLB updates for Border Gateway Protocol

With this release, MetalLB includes a new field for the Border Gateway Protocol (BGP) peer custom resource.
You can use the `dynamicASN` field to detect the Autonomous System Number (ASN) to use for the remote end of a BGP session.
This is an alternative to explicitly setting an ASN in the `spec.peerASN` field.

[id="ocp-release-notes-networking-sr-iov-rdma-cni_{context}"]
==== Configuring an RDMA subsytem for SR-IOV

With this release, you can configure a Remote Direct Memory Access (RDMA) Container Network Interface (CNI) on Single Root I/O Virtualization (SR-IOV) to enable high-performance, low-latency communication between containers.
When you combine RDMA with SR-IOV, you provide a mechanism to expose hardware counters of Mellanox Ethernet devices to be used inside Data Plane Development Kit (DPDK) applications.

[id="ocp-release-notes-networking-sr-iov-mlx-secure-boot_{context}"]
==== Support configuring the SR-IOV Network Operator on a Secure-Boot-enabled environment for Mellanox cards

With this release, you can configure the Single Root I/O Virtualization (SR-IOV) Network Operator when the system has secure boot enabled. The SR-IOV Operator is configured after you first manually configure the firmware for Mellanox devices. With secure boot enabled, the resilience of your system is enhanced, and a crucial layer of defense for the overall security of your computer is provided.

For more information, see  xref:../networking/hardware_networks/configuring-sriov-device.adoc#nw-sriov-nic-mlx-secure-boot_configuring-sriov-device[Configuring the SR-IOV Network Operator on Mellanox cards when Secure Boot is enabled].

[id="ocp-release-notes-nodes_{context}"]
=== Nodes

[id="ocp-release-notes-nodes-crun-default_{context}"]
==== crun is now the default container runtime
crun is now the default container runtime for new containers created in {product-title}. The runC runtime is still supported and you can change the default runtime to runC, if needed. For more information on crun, see xref:../nodes/containers/nodes-containers-using.adoc#nodes-containers-runtimes[About the container engine and container runtime]. For information on changing the default to runC, see xref:../machine_configuration/machine-configs-custom.adoc#create-a-containerruntimeconfig_machine-configs-custom[Creating a ContainerRuntimeConfig CR to edit CRI-O parameters].

After updating from {product-title} 4.17.z to {product-title} {product-version}, the container runtime configured as the default is respected in {product-version}.

[id="ocp-release-notes-nodes-crun-sigstore_{context}"]
==== sigstore support (Technology Preview)

Available as a Technology Preview, you can use the sigstore project with {product-title} to improve supply chain security. You can create signature policies at the cluster-wide level or for a specific namespace. For more information, see xref:../nodes/nodes-sigstore-using.adoc#nodes-sigstore-using[Manage secure signatures with sigstore].

[id="ocp-release-notes-nodes-adding-enhancements_{context}"]
==== Enhancements to process for adding nodes
Enhancements have been added to the process for xref:../nodes/nodes/nodes-nodes-adding-node-iso.adoc#adding-node-iso[adding worker nodes to an on-premise cluster] that was introduced in {product-title} 4.17.
With this release, you can now generate Preboot Execution Environment (PXE) assets instead of an ISO image file, and you can configure reports to be generated regardless of whether the node creation process fails or not.

[id="ocp-release-notes-postinstallation-configuration_{context}"]
=== Postinstallation configuration

[id="ocp-release-notes-registry_{context}"]
=== Registry

[discrete]
[id="ocp-4-release-notes-read-only-registry-enhancements_{context}"]
==== Read-only registry enhancements

In previous versions of {product-title}, storage mounted as read-only returned no specific metrics or information about storage errors. This could result in silent failures of a registry when the storage backend was read-only. With this release, the following alerts have been added to return storage information when the backend is set to read-only:

[cols="2", options="header"]
|===
| Alert Name | Message

| `ImageRegistryStorageReadOnly`
| The image registry storage is read-only and no images will be committed to storage.

| `ImageRegistryStorageFull`
| The image registry storage disk is full and no images will be committed to storage.

|===

[id="ocp-release-notes-rhcos_{context}"]
=== {op-system-first}

[id="ocp-release-notes-rhcos-rhel-9.4-packages_{context}"]
==== {op-system} uses {op-system-base} 9.4

{op-system} uses {op-system-base-full} 9.4 packages in {product-title} {product-version}. These packages ensure that your {product-title} instances receive the latest fixes, features, enhancements, hardware support, and driver updates.

[id="ocp-release-notes-scalability-and-performance_{context}"]
=== Scalability and performance

[id="ocp-release-notes-scalability-and-performance-cluster-compare_{context}"]
==== Cluster validation with the cluster-compare plugin

The `cluster-compare` plugin is an OpenShift CLI (`oc`) plugin that compares a cluster configuration with a target configuration. The plugin reports configuration differences while suppressing expected variations by using configurable validation rules and templates.

For example, the plugin can highlight unexpected differences, such as mismatched field values, missing resources, or version discrepancies, while ignoring expected differences, such as optional components or hardware-specific fields. This focused comparison makes it easier to assess cluster compliance with the target configuration.

You can use the `cluster-compare` plugin in development, production, and support scenarios.

For more information about the `cluster-compare` plugin, see xref:../scalability_and_performance/cluster-compare/understanding-the-cluster-compare-plugin.adoc#cluster-compare-overview_understanding-cluster-compare[Overview of the cluster-compare plugin].

[id="ocp-release-notes-node-tuning-operator-deferred-updates_{context}"]
==== Node Tuning Operator: Deferred Tuning Updates

In this release, the Node Tuning Operator introduces support for deferring tuning updates. Administrators can schedule updates to be applied during a maintenance window with this feature.

For more information, see xref:../scalability_and_performance/using-node-tuning-operator.adoc#defer-application-of-tuning-changes_node-tuning-operator[Deferring application of tuning changes].

[id="ocp-release-notes-scalability-and-performance-nrop-policy_{context}"]
==== NUMA Resources Operator now uses default SELinux policy

With this release, the NUMA Resources Operator no longer creates a custom SELinux policy to enable the installation of Operator components on a target node. Instead, the Operator uses a built-in container SELinux policy. This change removes the additional node reboot that was previously required when applying a custom SELinux policy during an installation.

[IMPORTANT]
====
In clusters with an existing NUMA-aware scheduler configuration, upgrading to {product-title} 4.18 might result in an additional reboot for each configured node. For further information about how to manage an upgrade in this scenario and limit disruption, see the Red Hat Knowledgebase article link:https://access.redhat.com/articles/7107603[Managing an upgrade to {product-title} 4.18 or later for a cluster with an existing NUMA-aware scheduler configuration]
====

[id="ocp-release-notes-scalability-and-performance-nto-platforms_{context}"]
==== Node Tuning Operator platform detection

With this release, when you apply a performance profile, the Node Tuning Operator detects the platform and configures kernel arguments and other platform-specific options accordingly. This release adds support for detecting the following platforms:

* AMD64
* AArch64
* Intel 64

[id="ocp-4-18-support-for-worker-nodes-on-amd-cpus_{context}"]
==== Support for worker nodes with AMD EPYC Zen 4 CPUs

With this release, you can use the `PerformanceProfile` custom resource (CR) to configure worker nodes on machines equipped with AMD EPYC Zen 4 CPUs (Genoa and Bergamo). These CPUs are fully supported.

[IMPORTANT]
====
The per pod power management feature is not functional on AMD EPYC Zen 4 CPUs.
====

[id="ocp-release-notes-etcd-certificates_{context}"]
=== Security

[id="ocp-release-notes-storage_{context}"]
=== Storage

[id="ocp-4-18-storage-editing-overprovisioning-ratio_{context}"]
==== Over-provisioning ratio update after LVMCluster custom resource creation

Previously, the `thinPoolConfig.overprovisionRatio` field in the `LVMCluster` custom resource (CR) could be configured only during the creation of the `LVMCluster` CR. With this release, you can now update the `thinPoolConfig.overprovisionRatio` field even after creating the `LVMCluster` CR.

[id="ocp-release-notes-web-console_{context}"]
=== Web console

[id="ocp-4-18-administrator-perspective_{context}"]
==== Administrator perspective

This release introduces the following updates to the *Administrator* perspective of the web console:

* A new setting for hiding the *Getting started resources* card on the *Overview* page allowing for maximum use of the dashboard.
* A *Start Job* option was added to the CronJob *List* and *Details* pages, so you can start individual CronJobs manually directly in the web console without having to use the `oc` CLI.
* The *Import YAML* button in the masthead is now a *Quick Create* button that you can use for the rapid deployment of workloads by imprting from YAML, Git, or using container images.
* You can build your own generative-AI chat bot with a chat bot sample. The generative-AI chat bot sample is deployed with Helm and includes a full CI/CD pipeline. You can also run this sample on your cluster with no CPUs.
* You can import YAML into the console using {ols}.

[id="ocp-4-18-administrator-perspective_content-security-policy{context}"]
===== Content Security Policy (CSP)

With this release, the console Content Security Policy (CSP) is deployed in report-only mode. CSP violations will be logged in the browser console, but the associated CSP directives will not be enforced. Dynamic plugin creators can add their own policies.

Additionally, you can report any plugins that break security policies. Administrators have the ability to disable any plugin breaking those policies. CSP violations will be logged in the browser console, but the associated CSP directives will not be enforced. This feature is behind a `feature-gate`, so you will need to manually enable it.

For more information, see xref:../web_console/dynamic-plugin/content-security-policy.adoc#content-security-policy[Content Security Policy (CSP)]and xref:../nodes/clusters/nodes-cluster-enabling-features.adoc#nodes-cluster-enabling-features-console_nodes-cluster-enabling[Enabling feature sets using the web console].

[id="ocp-4-18-developer-perspective_{context}"]
==== Developer Perspective

This release introduces the following updates to the *Developer* perspective of the web console:

* Added a {product-title} toolkit, Quarkus tools and JBoss EAP, and a Language Server Protocol Plugin for Visual Studio Code and IntelliJ.
* Previously, when moving from light mode to dark mode in the Monaco editor, the console remained in dark mode. With this update, the Monaco code editor will match the selected theme.

[id="ocp-4-18-notable-technical-changes_{context}"]
== Notable technical changes

[discrete]
[id="ocp-4-18-notable-technical-changes-uninstall-sr-iov-operator_{context}"]
=== Uninstalling the SR-IOV Network Operator changed

From {product-title} {product-version}, to successfully uninstall the SR-IOV Network Operator, you need to delete the `sriovoperatorconfigs` custom resource and custom resource definition too.

For more information, see xref:../networking/networking_operators/sr-iov-operator/uninstalling-sriov-operator.adoc#nw-sriov-operator-uninstall_uninstalling-sr-iov-operator[Uninstalling the SR-IOV Network Operator].

[id="ocp-4-18-rhcos-iscsi-initiator_{context}"]
=== Changes to the iSCSI initiator name and service

Previously, the `/etc/iscsi/initiatorname.iscsi` file was present by default on {op-system} images. With this release, the `initiatorname.iscsi` file is no longer present by default. Instead, it is created at run time when the `iscsi.service` and subsequent `iscsi-init.service` services start. This service is not enabled by default and might affect any CSI drivers that rely on reading the contents of the `initiatorname.iscsi` file prior to starting the service.

[id="ocp-4-18-deprecated-removed-features_{context}"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* _Not Available_
* _Technology Preview_
* _General Availability_
* _Deprecated_
* _Removed_

[discrete]
[id="ocp-release-note-bare-metal-dep-rem_{context}"]
=== Bare metal monitoring deprecated and removed features

.Bare Metal Event Relay Operator tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|Bare Metal Event Relay Operator
|Deprecated
|Removed
|Removed
|====

[discrete]
[id="ocp-release-note-images-dep-rem_{context}"]
=== Images deprecated and removed features

.Images deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
[id="ocp-release-note-install-dep-rem_{context}"]
=== Installation deprecated and removed features

.Installation deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|`--cloud` parameter for `oc adm release extract`
|Deprecated
|Deprecated
|Deprecated

|CoreDNS wildcard queries for the `cluster.local` domain
|Deprecated
|Deprecated
|Deprecated

|`compute.platform.openstack.rootVolume.type` for {rh-openstack}
|Deprecated
|Deprecated
|Deprecated

|`controlPlane.platform.openstack.rootVolume.type` for {rh-openstack}
|Deprecated
|Deprecated
|Deprecated

|`ingressVIP` and `apiVIP` settings in the `install-config.yaml` file for installer-provisioned infrastructure clusters
|Deprecated
|Deprecated
|Deprecated

|Package-based {op-system-base} compute machines
|Deprecated
|Deprecated
|Deprecated

|Managing machines with the Cluster API for {azure-full}
|Not Available
|Not Available
|Technology Preview

|`platform.aws.preserveBootstrapIgnition` parameter for {aws-first}
|Deprecated
|Deprecated
|Deprecated

|Installing a cluster on {aws-short} with compute nodes in {aws-short} Outposts
|Deprecated
|Deprecated
|Deprecated
|====


[discrete]
=== Machine management deprecated and removed features

.Machine management deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|Managing machine with Machine API for {alibaba}
|Removed
|Removed
|Removed

|Cloud controller manager for {alibaba}
|Removed
|Removed
|Removed

|====

////
[discrete]
[id="ocp-release-note-monitoring-dep-rem_{context}"]
=== Monitoring deprecated and removed features

.Monitoring deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18
|====
////


[discrete]
[id="ocp-release-note-networking-dep-rem_{context}"]
=== Networking deprecated and removed features

.Networking deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|OpenShift SDN network plugin
|Deprecated
|Removed
|Removed

|iptables
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
[id="ocp-release-note-node-dep-rem_{context}"]
=== Node deprecated and removed features

.Node deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|`ImageContentSourcePolicy` (ICSP) objects
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/zone`
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/region`
|Deprecated
|Deprecated
|Deprecated

|cgroup v1
|Deprecated
|Deprecated
|Deprecated
|====

////
[discrete]
[id="ocp-release-note-cli-dep-rem_{context}"]
=== OpenShift CLI (oc) deprecated and removed features

.OpenShift CLI (oc) deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|====
////

[discrete]
[id="ocp-release-note-operators-dep-rem_{context}"]
=== Operator lifecycle and development deprecated and removed features

// "Operator lifecycle" refers to OLMv0 and "development" refers to Operator SDK

.Operator lifecycle and development deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|Operator SDK
|Deprecated
|Deprecated
|Deprecated

|Scaffolding tools for Ansible-based Operator projects
|Deprecated
|Deprecated
|Deprecated

|Scaffolding tools for Helm-based Operator projects
|Deprecated
|Deprecated
|Deprecated

|Scaffolding tools for Go-based Operator projects
|Deprecated
|Deprecated
|Deprecated

|Scaffolding tools for Hybrid Helm-based Operator projects
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Java-based Operator projects
|Deprecated
|Deprecated
|Removed

// Do not remove the SQLite database... entry until otherwise directed by the Operator Framework PM
|SQLite database format for Operator catalogs
|Deprecated
|Deprecated
|Deprecated
|====

////
[discrete]
[id="ocp-4-18-hardware-an-driver-dep-rem_{context}"]
=== Specialized hardware and driver enablement deprecated and removed features

.Specialized hardware and driver enablement deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18
|====

[discrete]
=== Storage deprecated and removed features

.Storage deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|====


[discrete]
[id="ocp-4-18-clusters-dep-rem_{context}"]
=== Updating clusters deprecated and removed features

.Updating clusters deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18
|====
////

[discrete]
[id="ocp-release-note-web-console-dep-rem_{context}"]
=== Web console deprecated and removed features

.Web console deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|Patternfly 4
|Deprecated
|Deprecated
|Deprecated

|React Router 5
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
[id="ocp-release-note-workloads-dep-rem_{context}"]
=== Workloads deprecated and removed features

.Workloads deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|`DeploymentConfig` objects
|Deprecated
|Deprecated
|Deprecated
|====

[id="ocp-4-18-deprecated-features_{context}"]
=== Deprecated features

[id="ocp-4-18-removed-features_{context}"]
=== Removed features

[id="ocp-4-18-future-deprecation_{context}"]
=== Notice of future deprecation

[id="ocp-4-18-future-removals"]
=== Future Kubernetes API removals

// Kubernetes 1.32 isn't released yet, but it will be by the time OCP 4.18 comes out
The next minor release of {product-title} is expected to use Kubernetes 1.32. Kubernetes 1.32 removed a deprecated API.

See the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-32[Deprecated API Migration Guide] in the upstream Kubernetes documentation for the list of planned Kubernetes API removals.

See link:https://access.redhat.com/articles/6955985[Navigating Kubernetes API deprecations and removals] for information about how to check your cluster for Kubernetes APIs that are planned for removal.

[id="ocp-4-18-bug-fixes_{context}"]
== Bug fixes
//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / TALO
//Telco Edge / ZTP

[discrete]
[id="ocp-release-note-api-auth-bug-fixes_{context}"]
==== API Server and Authentication

[discrete]
[id="ocp-release-note-bare-metal-hardware-bug-fixes_{context}"]
==== Bare Metal Hardware Provisioning

[discrete]
[id="ocp-release-note-builds-bug-fixes_{context}"]
==== Builds

[discrete]
[id="ocp-release-note-cloud-compute-bug-fixes_{context}"]
==== Cloud Compute

[discrete]
[id="ocp-release-note-cloud-cred-operator-bug-fixes_{context}"]
==== Cloud Credential Operator

[discrete]
[id="ocp-release-note-cluster-version-operator-bug-fixes_{context}"]
==== Cluster Version Operator

[discrete]
[id="ocp-release-note-dev-console-bug-fixes_{context}"]
==== Developer Console

[discrete]
[id="ocp-release-note-driver-toolkit-bug-fixes_{context}"]
==== Driver ToolKit (DTK)

[discrete]
[id="ocp-release-note-cloud-etcd-operator-bug-fixes_{context}"]
==== etcd Cluster Operator

[discrete]
[id="ocp-release-note-hosted-control-planes-bug-fixes_{context}"]
==== Hosted control planes

[discrete]
[id="ocp-release-note-image-registry-bug-fixes_{context}"]
==== Image Registry

[discrete]
[id="ocp-release-note-installer-bug-fixes_{context}"]
==== Installer

[discrete]
[id="ocp-release-note-insights-operator-bug-fixes_{context}"]
==== Insights Operator

[discrete]
[id="ocp-release-note-kube-controller-bug-fixes_{context}"]
==== Kubernetes Controller Manager

[discrete]
[id="ocp-release-note-kube-scheduler-bug-fixes_{context}"]
==== Kubernetes Scheduler

[discrete]
[id="ocp-release-note-machine-config-operator-bug-fixes_{context}"]
==== Machine Config Operator

[discrete]
[id="ocp-release-note-management-console-bug-fixes_{context}"]
==== Management Console

[discrete]
[id="ocp-release-note-monitoring-bug-fixes_{context}"]
==== Monitoring

[discrete]
[id="ocp-release-note-networking-bug-fixes_{context}"]
==== Networking

* Previously, enabling encapsulated security payload (ESP) offload hardware when using IPSec on Open vSwitch attached interfaces would break connectivity in your cluster. To resolve this issue, {product-title} by default disables ESP offload hardware on Open vSwitch attached interfaces. This fixes the issue. (link:https://issues.redhat.com/browse/OCPBUGS-42987[*OCPBUGS-42987*])

* Previously, if you deleted the default `sriovOperatorConfig` custom resource (CR), you could not recreate the default `sriovOperatorConfig` CR, because the `ValidatingWebhookConfiguration` was not initially deleted. With this release, the Single Root I/O Virtualization (SR-IOV) Network Operator removes validating webhooks when you delete the `sriovOperatorConfig` CR, so that you can create a new `sriovOperatorConfig` CR. (link:https://issues.redhat.com/browse/OCPBUGS-41897[*OCPBUGS-41897*])

* Previously, if you set custom annotations in a custom resource (CR), the SR-IOV Operator would override all the default annotations in the `SriovNetwork` CR. With this release, when you define custom annotations in a CR, the SR-IOV Operator does not override the default annotations. (link:https://issues.redhat.com/browse/OCPBUGS-41352[*OCPBUGS-41352*])

* Previously, bonds that were configured in `active-backup` mode would have IPsec Encapsulating Security Payload (ESP) offload active even if underlying links did not support ESP offload. This caused IPsec associations to fail. With this release, ESP offload is disabled for bonds so that IPsec associations pass. (link:https://issues.redhat.com/browse/OCPBUGS-39438[*OCPBUGS-39438*])

* When you start the limited live migration method and an issue exists with network overlap, the Cluster Network Operator (CNO) can now expose network overlap metrics for the issue. This is possible because the `openshift_network_operator_live_migration_blocked` metric now includes the new `NetworkOverlap` label. (link:https://issues.redhat.com/browse/OCPBUGS-39096[*OCPBUGS-39096*])

* Previously, the Machine Config Operator (MCO)'s vSphere `resolve-prepender` script used `systemd` directives that were incompatible with old bootimage versions used in {product-title} 4. With this release, nodes can scale using newer bootimage versions {product-version} 4.13 and above, through manual intervention, or by upgrading to a release that includes this fix. (link:https://issues.redhat.com/browse/OCPBUGS-38012[*OCPBUGS-38012*])

* Previously, the Ingress Controller status incorrectly displayed as `Degraded=False` because of a migration time issue with the `CanaryRepetitiveFailures` condition. With this release, the Ingress Controller status is correctly marked as `Degraded=True` for the appropriate length of time that the `CanaryRepetitiveFailures` condition exists. (link:https://issues.redhat.com/browse/OCPBUGS-37491[*OCPBUGS-37491*])

* Previously, when a pod was running on a node on which egress IPv6 is assigned, the pod was not able to communicate with the Kubernetes service in a dual stack cluster. This resulted in the traffic with the IP family, that the egressIP is not applicable to, being dropped. With this release, only the source network address translation (SNAT) for the IP family that the egress IPs applied to is deleted, eliminating the risk of traffic being dropped. (link:https://issues.redhat.com/browse/OCPBUGS-37193[*OCPBUGS-37193*])

* Previously, the Single-Root I/O Virtualization (SR-IOV) Operator did not expire the acquired lease during the Operator's shutdown operation. This impacted a new instance of the Operator, because the new instance had to wait for the lease to expire before the new instance was operational. With this release, an update to the Operator shutdown logic ensures that the Operator expires the lease when the Operator is shutting down. (link:https://issues.redhat.com/browse/OCPBUGS-23795[*OCPBUGS-23795*])

* The current PTP grandmaster clock (T-GM) implementation has a single National Marine Electronics Association (NMEA) sentence generator sourced from the GNSS without a backup NMEA sentence generator. If NMEA sentences are lost before reaching the e810 NIC, the T-GM cannot synchronize the devices in the network synchronization chain and the PTP Operator reports an error. A proposed fix is to report a `FREERUN` event when the NMEA string is lost. Until this limitation is addressed, T-GM does not support PTP clock holdover state. (link:https://issues.redhat.com/browse/OCPBUGS-19838[*OCPBUGS-19838*])

* Previously, for an Ingress resource with an `IngressWithoutClassName` alert, the Ingress Controller did not delete the alert along with deletion of the resource. The alert continued to show on the {product-title} web console. With this release, the Ingress Controller resets the `openshift_ingress_to_route_controller_ingress_without_class_name` metric to `0` before the controller deletes the Ingress resource, so that the alert is deleted and no longer shows on the web console. (link:https://issues.redhat.com/browse/OCPBUGS-13181[*OCPBUGS-13181*])

* Previously, when the live migration to OVN-Kubernetes was employed on clusters using the subnet 100.88.0.0/16 for either the `clusterNetwork` or `serviceNetwork` IP address pools, the `ovnkube-node` pod would crash after migration. With this update, the issue is fixed. (link:https://issues.redhat.com/browse/OCPBUGS-43740[*OCPBUGS-43740*])

* Previously, a change in OVN-Kubernetes that standardized the `appProtocol` value `h2c` to `kubernetes.io/h2c` was not recognized by OpenShift router. Consequently, specifying `appProtocol: kubernetes.io/h2c` on a service did not cause OpenShift router to use clear-text HTTP/2 to connect to the service endpoints. With this release, OpenShift router was changed to handle `appProtocol: kubernetes.io/h2c` the same way as it handles `appProtocol: h2c` resolving the issue. (link:https://issues.redhat.com/browse/OCPBUGS-42972[*OCPBUGS-42972*])

* Previously, instructions that guided the user after changing the `LoadBalancer` parameter from `External` to `Internal` were missing for {ibm-power-server-title}, {alibaba}, and {rh-openstack-first}. This caused the Ingress Controller to be put in a permanent `Progressing` state. With this release the message `The IngressController scope was changed from Internal to External` is followed by `To effectuate this change, you must delete the service` resolving the permanent `Progressing` state. (link:https://issues.redhat.com/browse/OCPBUGS-39151[*OCPBUGS-39151*])

* Previously, there was no event logged when an error occurred from failed conversion from ingress to route conversion. With this update, this error appear in the event logs. (link:https://issues.redhat.com/browse/OCPBUGS-29354[*OCPBUGS-29354*])

[discrete]
[id="ocp-release-note-node-bug-fixes_{context}"]
==== Node

[discrete]
[id="ocp-release-note-node-tuning-operator-bug-fixes_{context}"]
==== Node Tuning Operator (NTO)

* Previously, if you specified a long string of isolated CPUs in a performance profile, such as `0,1,2,...,512`, the `tuned`, Machine Config Operator, and `rpm-ostree` components failed to process the string as expected. As a consequence, after you applied the performance profile, the expected kernel arguments were missing. The system failed silently with no reported errors. With this release, the string for isolated CPUs in a performance profile is converted to sequential ranges, such as `0-512`. As a result, the kernel arguments are applied as expected in most scenarios. (link:https://issues.redhat.com/browse/OCPBUGS-45264[*OCPBUGS-45264*])
+
[NOTE]
====
The issue might still occur with some combinations of input for isolated CPUs in a performance profile, such as a long list of odd numbers `1,3,5,...,511`.
====

* Previously, CPU masks for interrupt and network handling CPU affinity were computed incorrectly on machines with more than 256 CPUs. This issue prevented proper CPU isolation and caused `systemd` unit failures during internal node configuration. This fix ensures accurate CPU affinity calculations, enabling correct CPU isolation on machines with more than 256 CPUs. (link:https://issues.redhat.com/browse/OCPBUGS-36431[*OCPBUGS-36431*])

* Previously, entering an invalid value in any `cpuset` field under `spec.cpu` in the `PerformanceProfile` resource caused the webhook validation to crash. With this release, improved error handling for the `PerformanceProfile` validation webhook ensures that invalid values for these fields return an informative error. (link:https://issues.redhat.com/browse/OCPBUGS-45616[*OCPBUGS-45616*])

[discrete]
[id="ocp-release-note-observability-bug-fixes_{context}"]
==== Observability

[discrete]
[id="ocp-release-note-openshift-cli-bug-fixes_{context}"]
==== OpenShift CLI (oc)

[discrete]
[id="ocp-release-note-olm-bug-fixes_{context}"]
==== Operator Lifecycle Manager (OLM)

[discrete]
[id="ocp-release-note-openshift-api-server-bug-fixes_{context}"]
==== OpenShift API server

[discrete]
[id="ocp-release-note-rhcos-bug-fixes_{context}"]
==== {op-system-first}

[discrete]
[id="ocp-release-note-scalability-and-performance-bug-fixes_{context}"]
==== Scalability and performance

* Previously, you could configure the NUMA Resources Operator to map a `nodeGroup` to more than one `MachineConfigPool`. This implementation is contrary to the intended design of the Operator, which assumed a one-to-one mapping between a `nodeGroup` and a `MachineConfigPool`. With this release, if a `nodeGroup` maps to more than one `MachineConfigPool`, the Operator accepts the configuration, but the Operator state moves to `Degraded`. To retain the previous behavior, you can apply the `config.node.openshift-kni.io/multiple-pools-per-tree: enabled` annotation to the NUMA Resources Operator. However, the ability to assign a `nodeGroup` to more than one `MachineConfigPool` will be removed in a future release. (link:https://issues.redhat.com/browse/OCPBUGS-42523[*OCPBUGS-42523*])

[discrete]
[id="ocp-release-note-storage-bug-fixes_{context}"]
==== Storage

[discrete]
[id="ocp-release-note-windows-containers-bug-fixes_{context}"]
==== Windows containers

[id="ocp-4-18-technology-preview-tables_{context}"]
== Technology Preview features status

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red{nbsp}Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the following tables, features are marked with the following statuses:

* _Not Available_
* _Technology Preview_
* _General Availability_
* _Deprecated_
* _Removed_


[discrete]
[id="ocp-release-notes-auth-tech-preview_{context}"]
=== Authentication and authorization Technology Preview features

.Authentication and authorization Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|Pod security admission restricted enforcement
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
[id="ocp-release-notesedge-computing-tp-features_{context}"]
=== Edge computing Technology Preview features

.Edge computing Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|Accelerated provisioning of {ztp}
|Technology Preview
|Technology Preview
|Technology Preview

|Enabling disk encryption with TPM and PCR protection
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
[id="ocp-release-notes-hcp-tech-preview_{context}"]
=== Hosted control planes Technology Preview features

.Hosted control planes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|{hcp-capital} for {product-title} using non-bare metal agent machines
|Technology Preview
|Technology Preview
|Technology Preview

|{hcp-capital} for an ARM64 {product-title} cluster on {aws-full}
|Technology Preview
|General Availability
|General Availability

|{hcp-capital} for {product-title} on {ibm-power-title}
|Technology Preview
|General Availability
|General Availability

|{hcp-capital} for {product-title} on {ibm-z-title}
|Technology Preview
|General Availability
|General Availability

|{hcp-capital} for {product-title} on {rh-openstack}
|Not Available
|Not Available
|Developer Preview

|====

[discrete]
[id="ocp-release-notes-installing-tech-preview_{context}"]
=== Installation Technology Preview features

.Installation Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

// All GA in 4.17 notes for oci-first
|Adding kernel modules to nodes with kvc
|Technology Preview
|Technology Preview
|Technology Preview

|Enabling NIC partitioning for SR-IOV devices
|Technology Preview
|General Availability
|General Availability

|User-defined labels and tags for {gcp-first}
|Technology Preview
|General Availability
|General Availability

|Installing a cluster on Alibaba Cloud by using Assisted Installer
|Technology Preview
|Technology Preview
|Technology Preview

|Mount shared entitlements in BuildConfigs in RHEL
|Technology Preview
|Technology Preview
|Technology Preview

|{product-title} on {oci-first}
|General Availability
|General Availability
|General Availability

|Selectable Cluster Inventory
|Technology Preview
|Technology Preview
|Technology Preview

|Installing a cluster on {gcp-short} using the Cluster API implementation
|Technology Preview
|General Availability
|General Availability

|{product-title} on Oracle Compute Cloud@Customer (C3)
|Not Available
|Not Available
|General Availability

|{product-title} on Oracle Private Cloud Appliance (PCA)
|Not Available
|Not Available
|General Availability
|====

[discrete]
[id="ocp-release-notes-mco-tech-preview_{context}"]
=== Machine Config Operator Technology Preview features

.Machine Config Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|Improved MCO state reporting (`oc get machineconfigpool`)
|Technology Preview
|Technology Preview
|Technology Preview

|On-cluster RHCOS image layering
|Technology Preview
|Technology Preview
|Technology Preview

|Node disruption policies
|Technology Preview
|General Availability
|General Availability

|Updating boot images for GCP clusters
|Technology Preview
|General Availability
|General Availability

|Updating boot images for AWS clusters
|Technology Preview
|Technology Preview
|General Availability

|====

[discrete]
[id="ocp-release-notes-machine-management-tech-preview_{context}"]
=== Machine management Technology Preview features

.Machine management Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|Managing machines with the Cluster API for {aws-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {gcp-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {vmw-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for {ibm-power-server-name}
|Technology Preview
|Technology Preview
|Technology Preview

|Defining a {vmw-short} failure domain for a control plane machine set
|General Availability
|General Availability
|General Availability

|Cloud controller manager for {alibaba}
|Removed
|Removed
|Removed

|====

[discrete]
[id="ocp-release-notes-monitoring-tech-preview_{context}"]
=== Monitoring Technology Preview features

.Monitoring Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|Metrics Collection Profiles
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
[id="ocp-release-notes-web-console-tech-preview_{context}"]
=== Web console Technology Preview features

.Web console Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|{ols-official} in the {product-title} web console
| Technology Preview
| Technology Preview
| Technology Peview

|====

[discrete]
[id="ocp-release-notes-multi-arch-tech-preview_{context}"]
=== Multi-Architecture Technology Preview features

.Multi-Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|`kdump` on `arm64` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `s390x` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `ppc64le` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|Multiarch Tuning Operator
|General Availability
|General Availability
|General Availability

|====

[discrete]
[id="ocp-release-notes-networking-tech-preview_{context}"]
=== Networking Technology Preview features

.Networking Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|Advertise using L2 mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Technology Preview
|Technology Preview
|Technology Preview

|Updating the interface-specific safe sysctls list
|Technology Preview
|Technology Preview
|Technology Preview

|Egress service custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|VRF specification in `BGPPeer` custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|VRF specification in `NodeNetworkConfigurationPolicy` custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|Host network settings for SR-IOV VFs
|Technology Preview
|General Availability
|General Availability

|Integration of MetalLB and FRR-K8s
|Technology Preview
|General Availability
|General Availability

|Automatic leap seconds handling for PTP grandmaster clocks
|Not Available
|General Availability
|General Availability

|PTP events REST API v2
|Not Available
|General Availability
|General Availability

|Customized `br-ex` bridge needed by OVN-Kuberenetes to use NMState
|Technology Preview
|Technology Preview
|General Availability

| Live migration to OVN-Kubernetes from OpenShift SDN
| Not Available
| General Availability
| Not Available

|User defined network segmentation
|Not Available
|Not Available
|Technology Preview

|Dynamic configuration manager
|Not Available
|Not Available
|Technology Preview

|SR-IOV Network Operator support for Intel C741 Emmitsburg Chipset
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
[id="ocp-release-notes-nodes-tech-preview_{context}"]
=== Node Technology Preview features

.Nodes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|`MaxUnavailableStatefulSet` featureset
|Technology Preview
|Technology Preview
|Technology Preview

|sigstore support
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
[id="ocp-release-notes-oc-cli-tech-preview_{context}"]
=== OpenShift CLI (oc) Technology Preview features

.OpenShift CLI (`oc`) Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|oc-mirror plugin v2
|Technology Preview
|Technology Preview
|Technology Preview

|oc-mirror plugin v2 enclave support
|Technology Preview
|Technology Preview
|Technology Preview

|oc-mirror plugin v2 delete functionality
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
[id="ocp-release-notes-extensions-tech-preview_{context}"]
=== Extensions Technology Preview features

// "Extensions" refers to OLMv1

.Extensions Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|{olmv1-first}
|Technology Preview
|Technology Preview
|General Availability

|====

[discrete]
[id="ocp-release-notes-operator-lifecycle-tech-preview_{context}"]
=== Operator lifecycle and development Technology Preview features

// "Operator lifecycle" refers to OLMv0 and "development" refers to Operator SDK

.Operator lifecycle and development Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|{olmv1-first}
|Technology Preview
|Technology Preview
|General Availability

|Scaffolding tools for Hybrid Helm-based Operator projects
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Java-based Operator projects
|Deprecated
|Deprecated
|Removed

|====

[discrete]
[id="ocp-release-notes-rhcos-tech-preview_{context}"]
=== {rh-openstack-first} Technology Preview features

.{rh-openstack} Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|{rh-openstack} integration into the {cluster-capi-operator}
|Technology Preview
|Technology Preview
|Technology Preview

|Control Plane with `rootVolumes` and `etcd` on local disk
|Technology Preview
|General Availability
|General Availability

|====

[discrete]
[id="ocp-release-notes-scalability-tech-preview_{context}"]
=== Scalability and performance Technology Preview features

.Scalability and performance Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|{factory-prestaging-tool}
|Technology Preview
|Technology Preview
|Technology Preview

|Hyperthreading-aware CPU manager policy
|Technology Preview
|Technology Preview
|Technology Preview

|Mount namespace encapsulation
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Increasing the etcd database size
|Technology Preview
|Technology Preview
|Technology Preview

|Using {rh-rhacm} `PolicyGenerator` resources to manage {ztp} cluster policies
|Technology Preview
|Technology Preview
|Technology Preview

|Pinned Image Sets
|Technology Preview
|Technology Preview
|Technology Preview

|====

////
[discrete]
[id="ocp-release-notes-special-hardware-tech-preview_{context}"]
=== Specialized hardware and driver enablement Technology Preview features

.Specialized hardware and driver enablement Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|====
////

[discrete]
[id="ocp-release-notes-storage-tech-preview_{context}"]
=== Storage Technology Preview features

.Storage Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.16 |4.17 |4.18

|AWS EFS storage CSI usage metrics
|Not Available
|General Availability
|General Availability

|Automatic device discovery and provisioning with Local Storage Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Azure File CSI snapshot support
|Not Available
|Technology Preview
|Technology Preview

|Shared Resources CSI Driver in OpenShift Builds
|Technology Preview
|Technology Preview
|Technology Preview

|{secrets-store-operator}
|Technology Preview
|Technology Preview
|Technology Preview

|CIFS/SMB CSI Driver Operator
|Technology Preview
|Technology Preview
|Technology Preview

|VMWare vSphere multiple vCenter support
|Not Available
|Technology Preview
|Technology Preview

|Disabling/enabling storage on vSphere
|Not Available
|Technology Preview
|Technology Preview

|RWX/RWO SELinux Mount
|Not Available
|Developer Preview
|Developer Preview

|Migrating CNS Volumes Between Datastores
|Not Available
|Developer Preview
|Developer Preview

|====

[id="ocp-4-18-known-issues_{context}"]
== Known issues

* A regression in the behaviour of `libreswan` caused some nodes with IPsec enabled to lose communication with pods on other nodes in the same cluster. To resolve this issue, consider disabling IPsec for your cluster. (link:https://issues.redhat.com/browse/OCPBUGS-43713[*OCPBUGS-43713*])

* There is a known issue with {op-base-system} 8 worker nodes that  use `cgroupv1` Linux Control Groups (cgroup). The following is an example of the error message displayed for impacted nodes: `UDN are not supported on the node ip-10-0-51-120.us-east-2.compute.internal as it uses cgroup v1.` As a workaround, users should migrate worker nodes from `cgroupv1` to `cgroupv2`. (link:https://issues.redhat.com/browse/OCPBUGS-49933[*OCPBUGS-49933*])

* The current PTP grandmaster clock (T-GM) implementation has a single National Marine Electronics Association (NMEA) sentence generator sourced from the GNSS without a backup NMEA sentence generator. If NMEA sentences are lost before reaching the e810 NIC, the T-GM cannot synchronize the devices in the network synchronization chain and the PTP Operator reports an error. A proposed fix is to report a `FREERUN` event when the NMEA string is lost. Until this limitation is addressed, T-GM does not support PTP clock holdover state. (link:https://issues.redhat.com/browse/OCPBUGS-19838[*OCPBUGS-19838*])

* There is a known issue with a Layer 2 network topology on clusters running on Google Cloud Platform (GCP). At this time, the egress IP addresses being used in the Layer 2 network that is created by a `UserDefinedNetwork` (UDN) resource are using the wrong source IP address. Consequentially, UDN is not supported on Layer 2 on GCP. Currently, there is no fix for this issue. (link:https://issues.redhat.com/browse/OCPBUGS-48301[*OCPBUGS-48301*])

* There is a known issue with user-defined networks (UDN) that causes OVN-Kubernetes to delete any routing table ID equal or higher to 1000 that it does not manage. Consequently, any Virtual Routing and Forwarding (VRF) instance created outside OVN-Kubernetes is deleted. This issue impacts users who have created user-defined VRFs with a table ID greater than or equal to 1000. As a workaround, users must change their VRFs to a table ID lower than 1000 as these are reserved for {product-title}. (link:https://issues.redhat.com/browse/OCPBUGS-50855[*OCPBUGS-50855*])

* If you attempted to log in to a {product-title} 4.17 server by using the {oc-first} that you installed as part of the {product-title} {product-version}, you would see the following warning message in your terminal:
+
[source,terminal]
----
Warning: unknown field "metadata"
You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>
----
+
This warning message is a known issue but does not indicate any functionality issues with {product-title}. You can safely ignore the warning message and continue to use {product-title} as intended. (link:https://issues.redhat.com/browse/OCPBUGS-44833[*OCPBUGS-44833*])

[id="ocp-telco-ran-4-18-known-issues_{context}"]

* When you run Cloud-native Network Functions (CNF) latency tests on an {product-title} cluster, the test can sometimes return results greater than the latency threshold for the test; for example, 20 microseconds for `cyclictest` testing. This results in a test failure.
(link:https://issues.redhat.com/browse/OCPBUGS-42328[*OCPBUGS-42328*])

[id="ocp-telco-core-4-18-known-issues_{context}"]

* Due to an issue with Kubernetes, the CPU Manager is unable to return CPU resources from the last pod admitted to a node to the pool of available CPU resources. These resources are allocatable if a subsequent pod is admitted to the node. However, this pod then becomes the last pod, and again, the CPU manager cannot return this pod's resources to the available pool.
+
This issue affects CPU load-balancing features, which depend on the CPU Manager releasing CPUs to the available pool. Consequently, non-guaranteed pods might run with a reduced number of CPUs. As a workaround, schedule a pod with a `best-effort` CPU Manager policy on the affected node. This pod will be the last admitted pod and this ensures the resources will be correctly released to the available pool. (link:https://issues.redhat.com/browse/OCPBUGS-46428[*OCPBUGS-46428*])

* When a pod uses the CNI plugin for DHCP address assignment in conjunction with other CNI plugins, the network interface for the pod might be unexpectedly deleted. As a result, when the DHCP lease for the pod expires, the DHCP proxy enters a loop when trying to re-create a new lease, leading to the node becoming unresponsive. There is currently no workaround. (link:https://issues.redhat.com/browse/OCPBUGS-45272[*OCPBUGS-45272*])

[id="ocp-nodes-4-18-known-issues_{context}"]

* When using PXE boot to xref:../nodes/nodes/nodes-nodes-adding-node-iso.adoc#adding-node-iso[add a worker node to an on-premise cluster], sometimes the host fails to reboot from the disk properly, preventing the installation from completing.
As a workaround, you must manually reboot the failed host from the disk. (link:https://issues.redhat.com/browse/OCPBUGS-45116[*OCPBUGS-45116*])

[id="ocp-storage-core-4-18-known-issues_{context}"]

[id="ocp-hosted-control-planes-4-18-known-issues_{context}"]

[id="ocp-4-18-asynchronous-errata-updates_{context}"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red{nbsp}Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red{nbsp}Hat Customer Portal users can enable errata notifications in the account settings for Red{nbsp}Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red{nbsp}Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating_a_cluster/updating-cluster-web-console.adoc#updating-cluster-web-console[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-18-0-ga_{context}"]
=== RHSA-2024:XXXX - {product-title} {product-version}.0 image release, bug fix, and security update advisory

Issued: TBD

{product-title} release {product-version}.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:XXXX[RHSA-2024:XXXX] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2024:3722[RHSA-2024:XXXX] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.18.0 --pullspecs
----

[id="ocp-4-18-0-updating_{context}"]
==== Updating
To update an {product-title} 4.17 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

//replace 4.y.z for the correct values for the release. You do not need to update oc to run this command.
