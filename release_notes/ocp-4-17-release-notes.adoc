:_mod-docs-content-type: ASSEMBLY
[id="ocp-4-17-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red{nbsp}Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-17-about-this-release_{context}"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-2024:XXXX[RHSA-2024:XXXX]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.30.md[Kubernetes 1.30] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. With the {cluster-manager-first} application for {product-title}, you can deploy {product-title} clusters to either on-premises or cloud environments.

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.8-8.10, and on {op-system-first} 9.4.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines. {op-system-base} machines are deprecated in {product-title} 4.16 and will be removed in a future release.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//Even-numbered release lifecycle verbiage (Comment in for even-numbered releases)
//Starting from {product-title} 4.14, the Extended Update Support (EUS) phase for even-numbered releases increases the total available lifecycle to 24 months on all supported architectures, including `x86_64`, 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures.
//Beyond this, Red{nbsp}Hat also offers a 12-month additional EUS add-on, denoted as _Additional EUS Term 2_, that extends the total available lifecycle from 24 months to 36 months. The Additional EUS Term 2 is available on all architecture variants of {product-title}.

//Odd-numbered release lifecycle verbiage (Comment in for odd-numbered releases)
The support lifecycle for odd-numbered releases, such as {product-title} {product-version}, on all supported architectures, including `x86_64`, 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures is 18 months.
For more information about support for all versions, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

Commencing with the {product-title} 4.14 release, Red{nbsp}Hat is simplifying the administration and management of Red{nbsp}Hat shipped cluster Operators with the introduction of three new life cycle classifications; Platform Aligned, Platform Agnostic, and Rolling Stream. These life cycle classifications provide additional ease and transparency for cluster administrators to understand the life cycle policies of each Operator and form cluster maintenance and upgrade plans with predictable support boundaries. For more information, see link:https://access.redhat.com/webassets/avalon/j/includes/session/scribe/?redirectTo=https%3A%2F%2Faccess.redhat.com%2Fsupport%2Fpolicy%2Fupdates%2Fopenshift_operators[OpenShift Operator Life Cycles].

// Added in 4.14. Language came directly from Kirsten Newcomer.
{product-title} is designed for FIPS. When running {op-system-base-full} or {op-system-first} booted in FIPS mode, {product-title} core components use the {op-system-base} cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the `x86_64`, `ppc64le`, and `s390x` architectures.

For more information about the NIST validation program, see link:https://csrc.nist.gov/Projects/cryptographic-module-validation-program/validated-modules[Cryptographic Module Validation Program]. For the latest NIST status for the individual versions of {op-system-base} cryptographic libraries that have been submitted for validation, see link:https://access.redhat.com/articles/2918071#fips-140-2-and-fips-140-3-2[Compliance Activities and Government Standards].

[id="ocp-4-17-add-on-support-status_{context}"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-17-new-features-and-enhancements_{context}"]
== New features and enhancements

This release adds improvements related to the following components and concepts:

[id="ocp-4-17-rhcos_{context}"]
=== {op-system-first}

[id="ocp-4-17-rhcos-rhel-9-4-packages_{context}"]
==== {op-system} uses {op-system-base} 9.4
{op-system} uses {op-system-base-full} 9.4 packages in {product-title} {product-version}. These packages ensure that your {product-title} instance receives the latest fixes, features, enhancements, hardware support, and driver updates.

[id="ocp-4-17-rhcos-dnf_{context}"]
==== Support for the DNF package manager
With this release, you can now use DNF to install additional packages to your customized {op-system-first} builds. For more information, see xref:../machine_configuration/mco-coreos-layering.adoc[{op-system-first} layering].

[id="ocp-4-17-installation-and-update_{context}"]
=== Installation and update

[id="ocp-4-17-installation-gcp-user-defined-label-tags_{context}"]
==== User-defined labels and tags for GCP

With this update, the user-defined labels and tags for {gcp-first} is Generally Available.

For more information, see xref:../installing/installing_gcp/installing-gcp-customizations.adoc#installing-gcp-user-defined-labels-and-tags_installing-gcp-customizations[Managing user-defined labels and tags for GCP].

[id="ocp-4-17-installation-and-update-azure-spain_{context}"]
==== Installing a cluster on {azure-short} in the Central Spain region

You can deploy {product-title} {product-version} in {azure-first} in the Spain, Central (`spaincentral`) region. For more information, see xref:../installing/installing_azure/installing-azure-account.adoc#installation-azure-regions_installing-azure-account[Supported Azure regions].

[id="ocp-4-17-installation-and-update-azure-capi_{context}"]
==== Cluster API replaces Terraform for {azure-first} installations
In {product-title} 4.17, the installation program uses Cluster API instead of Terraform to provision cluster infrastructure during installations on {azure-short}.

[NOTE]
====
With the replacement of Terraform, the following permissions are required if you use a service principal with limited privileges:

* `Microsoft.Network/loadBalancers/inboundNatRules/read`
* `Microsoft.Network/loadBalancers/inboundNatRules/write`
* `Microsoft.Network/loadBalancers/inboundNatRules/join/action`
* `Microsoft.Network/loadBalancers/inboundNatRules/delete`
* `Microsoft.Network/routeTables/read`
* `Microsoft.Network/routeTables/write`
* `Microsoft.Network/routeTables/join/action`

For more information on required permissions, see xref:../installing/installing_azure/installing-azure-account.adoc#minimum-required-permissions-ipi-azure_installing-azure-account[Required Azure permissions for installer-provisioned infrastructure].
====

[id="ocp-4-17-installation-and-update-gcp-service-account_{context}"]
==== Installing a cluster on {gcp-full} by using an existing service account

With this update, you can install a cluster on {gcp-short} by using an existing service account, allowing you to minimize the permissions that you grant to the service account the installation program uses. You can specify this service account in the `compute.platform.gcp.serviceAccount` and `controlPlane.platform.gcp.serviceAccount` parameters in the `install-config.yaml` file. For more information, see xref:../installing/installing_gcp/installation-config-parameters-gcp.adoc#installation-configuration-parameters_installation-config-parameters-gcp[Available installation configuration parameters for GCP].

[id="ocp-4-17-installation-and-update-aws-iam_{context}"]
==== Installing a cluster on {aws-short} by using an existing IAM profile

With this release, you can install {product-title} on {aws-first} by using an existing identity and access management (IAM) instance profile. For more information, see xref:../installing/installing_aws/installation-config-parameters-aws.adoc#installation-configuration-parameters-optional-aws_installation-config-parameters-aws[Optional {aws-short} configuration parameters].

[id="ocp-4-17-installation-and-update-gcp-n4_{context}"]
==== Installing a cluster on {gcp-short} using the N4 machine series

With this release, you can deploy a cluster on {gcp-short} using the link:https://cloud.google.com/compute/docs/general-purpose-machines#n4_series[N4 machine series] for compute or control plane machines. The supported disk type of N4 machine series is `hyperdisk-balanced`. For more information, see xref:../installing/installing_gcp/installation-config-parameters-gcp.adoc#installation-configuration-parameters-gcp[Installation configuration parameters for GCP].

[id="ocp-4-17-installation-and-update-gcp-capi_{context}"]
==== Cluster API replaces Terraform for {gcp-first} installations

With this release, the installation program uses Cluster API instead of Terraform to provision cluster infrastructure during installations on {gcp-short}.

[id="installation-and-update-shiftstack-compact_{context}"]
==== Three-node cluster support for {rh-openstack}

Deploying a three-node cluster on installer-provisioned infrastructure is now supported on {rh-openstack-first}.

For more information, see xref:../installing/installing_openstack/installing-openstack-three-node.adoc#installing-openstack-three-node[Installing a three-node cluster on OpenStack].

[id="ocp-4-17-deploying-osp-with-root-volume-etcd-on-local-disk"]
==== Deploying {rh-openstack-first} with root volume and etcd on local disk (Generally Available)

You can now move etcd from a root volume (Cinder) to a dedicated ephemeral local disk as a Day 2 deployment with this generally available feature.

For more information, see xref:../installing/installing_openstack/deploying-openstack-with-rootVolume-etcd-on-local-disk.adoc#deploying-openstack-with-rootvolume-etcd-on-local-disk[Deploying on OpenStack with rootVolume and etcd on local disk].

[id="ocp-4-17-installation-aws-outposts-deprecated_{context}"]
==== Announcement of the deprecation of extending compute nodes into AWS Outposts for clusters deployed on AWS Public Cloud

With this release, extending compute nodes into AWS Outposts for clusters deployed on AWS Public Cloud is deprecated. The ability to deploy compute nodes into AWS Outposts after installation, as an extension of an existing {product-title} cluster operating in a public AWS region, will be removed with the release of {product-title} version 4.20.

For more information, see xref:../installing/installing_aws/ipi/installing-aws-outposts.html#installing-aws-outposts[Extending an AWS VPC cluster into an AWS Outpost].

[id="ocp-4-17-postinstallation-configuration_{context}"]
=== Postinstallation configuration

[id="ocp-4-17-web-console_{context}"]
=== Web console

[id="ocp-4-17-administrator-perspective_{context}"]
==== Administrator perspective

This release introduces the following updates to the *Administrator* perspective of the web console:

[id="ocp-4-17-developer-perspective_{context}"]
==== Developer Perspective

This release introduces the following updates to the *Developer* perspective of the web console:

[id="ocp-4-17-extensions_{context}"]
=== Extensions ({olmv1})

[id="ocp-4-17-extensions-new-guide_{context}"]
==== {olmv1-first} documentation moved to new Extensions guide (Technology Preview)

The documentation for {olmv1}, which has been in Technology Preview starting in {product-title} 4.14, is now moved and reworked as a separate guide called xref:../extensions/index.adoc#olmv1-about[Extensions]. Previously, {olmv1} documentation was a subsection of the existing xref:../operators/index.adoc#operators-overview[Operators] guide, which otherwise documents the {olmv0} feature set.

The updated location and guide name reflect a more focused documentation experience and aims to differentiate between {olmv1} and {olmv0}.

[id="ocp-4-17-openshift-cli_{context}"]
=== OpenShift CLI (oc)

[id="ocp-4-17-openshift-cli-oc-mirror-kubevirt_{context}"]
==== oc-mirror to include the HyperShift KubeVirt CoreOS container

With this release, oc-mirror now includes the {op-system-first} image for the HyperShift KubeVirt provider when mirroring the {product-title} release payload.

The `kubeVirtContainer` flag, which is set to false by default, must be set to `true` in the `imageSetConfig.yaml` file to extract the KubeVirt Container {op-system}. This ensures support for disconnected environments by including the required image for KubeVirt virtual machines acting as nodes for hosted clusters.

[id="ocp-4-17-ibm-z_{context}"]
=== {ibm-z-title} and {ibm-linuxone-title}
//double check closer to release
With this release, {ibm-z-name} and {ibm-linuxone-name} are now compatible with {product-title} {product-version}. You can perform the installation with z/VM, LPAR, or {op-system-base-full} Kernel-based Virtual Machine (KVM). For installation instructions, see
xref:../installing/installing_ibm_z/preparing-to-install-on-ibm-z.adoc#preparing-to-install-on-ibm-z[Preparing to install on {ibm-z-title} and {ibm-linuxone-title}].

[IMPORTANT]
====
Compute nodes must run {op-system-first}.
====

[discrete]
[id="ocp-4-17-ibm-z-enhancements_{context}"]
==== {ibm-z-title} and {ibm-linuxone-title} notable enhancements

The {ibm-z-name} and {ibm-linuxone-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components and concepts.

This release introduces support for the following features on {ibm-z-name} and {ibm-linuxone-name}:

* CPU manager
* Non-volatile memory express (NVMe) support for LPAR
* Tuning etcd latency tolerances

[id="ocp-4-17-ibm-power_{context}"]
=== {ibm-power-title}

{ibm-power-name} is now compatible with {product-title} {product-version}. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_power/installing-ibm-power.adoc#installing-ibm-power_installing-ibm-power[Installing a cluster on {ibm-power-name}]
* xref:../installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc#installing-restricted-networks-ibm-power_installing-restricted-networks-ibm-power[Installing a cluster on {ibm-power-name} in a restricted network]

[IMPORTANT]
====
Compute nodes must run {op-system-first}.
====

[discrete]
[id="ocp-4-17-ibm-power-enhancements_{context}"]
==== {ibm-power-title} notable enhancements

The {ibm-power-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on {ibm-power-title}:

* Tuning etcd latency tolerances
* Installer Provisioned Infrastructure for IBM PowerVS - move to CAPI

[discrete]
[id="ocp-4-17-ibm-z-power-support-matrix_{context}"]
=== {ibm-power-title}, {ibm-z-title}, and {ibm-linuxone-title} support matrix

Starting in {product-title} 4.14, Extended Update Support (EUS) is extended to the {ibm-power-name} and the {ibm-z-name} platform. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

.{product-title} features
[cols="3,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Alternate authentication providers
|Supported
|Supported

|Agent-based Installer
|Supported
|Supported

|Assisted Installer
|Supported
|Supported

|Automatic Device Discovery with Local Storage Operator
|Unsupported
|Supported

|Automatic repair of damaged machines with machine health checking
|Unsupported
|Unsupported

|Cloud controller manager for {ibm-cloud-name}
|Supported
|Unsupported

|Controlling overcommit and managing container density on nodes
|Unsupported
|Unsupported

|CPU manager
|Supported
|Supported

|Cron jobs
|Supported
|Supported

|Descheduler
|Supported
|Supported

|Egress IP
|Supported
|Supported

|Encrypting data stored in etcd
|Supported
|Supported

|FIPS cryptography
|Supported
|Supported

|Helm
|Supported
|Supported

|Horizontal pod autoscaling
|Supported
|Supported

|Hosted control planes (Technology Preview)
|Supported
|Supported

|IBM Secure Execution
|Unsupported
|Supported

|Installer-provisioned Infrastructure Enablement for {ibm-power-server-name}
|Supported
|Unsupported

|Installing on a single node
|Supported
|Supported

|IPv6
|Supported
|Supported

|Monitoring for user-defined projects
|Supported
|Supported

|Multi-architecture compute nodes
|Supported
|Supported

|Multi-architecture control plane
|Supported
|Supported

|Multipathing
|Supported
|Supported

|Network-Bound Disk Encryption - External Tang Server
|Supported
|Supported

|Non-volatile memory express drives (NVMe)
|Supported
|Unsupported

|nx-gzip for Power10 (Hardware Acceleration)
|Supported
|Unsupported

|oc-mirror plugin
|Supported
|Supported

|OpenShift CLI (`oc`) plugins
|Supported
|Supported

|Operator API
|Supported
|Supported

|OpenShift Virtualization
|Unsupported
|Unsupported

|OVN-Kubernetes, including IPsec encryption
|Supported
|Supported

|PodDisruptionBudget
|Supported
|Supported

|Precision Time Protocol (PTP) hardware
|Unsupported
|Unsupported

|{openshift-local-productname}
|Unsupported
|Unsupported

|Scheduler profiles
|Supported
|Supported

|Secure Boot
|Unsupported
|Supported

|Stream Control Transmission Protocol (SCTP)
|Supported
|Supported

|Support for multiple network interfaces
|Supported
|Supported

|The `openshift-install` utility to support various SMT levels on {ibm-power-name} (Hardware Acceleration)
|Supported
|Supported

|Three-node cluster support
|Supported
|Supported

|Topology Manager
|Supported
|Unsupported

|z/VM Emulated FBA devices on SCSI disks
|Unsupported
|Supported

|4K FCP block device
|Supported
|Supported
|====

.Persistent storage options
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}
|Persistent storage using iSCSI
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using local volumes (LSO)
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using hostPath
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Fibre Channel
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Raw Block
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using EDEV/FBA
|Supported ^[1]^
|Supported ^[1]^,^[2]^
|====
[.small]
--
1. Persistent shared storage must be provisioned by using either {rh-storage-first} or other supported storage protocols.
2. Persistent non-shared storage must be provisioned by using local storage, such as iSCSI, FC, or by using LSO with DASD, FCP, or EDEV/FBA.
--

.Operators
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|{cert-manager-operator}
|Supported
|Supported

|Cluster Logging Operator
|Supported
|Supported

|Cluster Resource Override Operator
|Supported
|Supported

|Compliance Operator
|Supported
|Supported

|Cost Management Metrics Operator
|Supported
|Supported

|File Integrity Operator
|Supported
|Supported

|HyperShift Operator
|Technology Preview
|Technology Preview

|{ibm-power-server-name} Block CSI Driver Operator
|Supported
|Unsupported

|Ingress Node Firewall Operator
|Supported
|Supported

|Local Storage Operator
|Supported
|Supported

|MetalLB Operator
|Supported
|Supported

|Network Observability Operator
|Supported
|Supported

|NFD Operator
|Supported
|Supported

|NMState Operator
|Supported
|Supported

|OpenShift Elasticsearch Operator
|Supported
|Supported

|Vertical Pod Autoscaler Operator
|Supported
|Supported
|====

.Multus CNI plugins
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Bridge
|Supported
|Supported

|Host-device
|Supported
|Supported

|IPAM
|Supported
|Supported

|IPVLAN
|Supported
|Supported
|====

.CSI Volumes
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Cloning
|Supported
|Supported

|Expansion
|Supported
|Supported

|Snapshot
|Supported
|Supported
|====

[id="ocp-4-17-auth_{context}"]
=== Authentication and authorization

[id="ocp-4-17-networking_{context}"]
=== Networking

[id="ocp-4-17-networking-azure-support-nmstate-operator_{context}"]
==== Microsoft Azure for the Kubernetes NMState Operator

Red{nbsp}Hat support exists for using the Kubernetes NMState Operator on {azure-first} but in a limited capacity. Support is limited to configuring DNS servers on your system as a postinstallation task.

For more information, see xref:../networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.adoc#k8s-nmstate-about-the-k8s-nmstate-operator_k8s-nmstate-operator[About the Kubernetes NMState Operator].

[id="ocp-4-17-networking-nmstate-operator-metrics_{context}"]
==== View metrics collected by the Kubernetes NMState Operator

The Kubernetes NMState Operator, `kubernetes-nmstate-operator`, can collect metrics from the `kubernetes_nmstate_features_applied` component and expose them as ready-to-use metrics. You can view these metrics by using the *Administrator* and *Developer* perspectives.

For more information, see xref:../networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.adoc#viewing-stats-collected-kubernetes-nmtate-op_k8s-nmstate-operator[Viewing metrics collected by the Kubernetes NMState Operator].

[id="ocp-ptp-fast-events-rest-api-v2-available_{context}"]
==== New PTP fast events REST API version 2 available

A new PTP fast events O-RAN Release 3 compliant REST API version 2 is available.
Now, you can develop PTP event consumer applications that receive host hardware PTP events directly from the PTP Operator-managed pod.
The PTP fast events REST API v1 will be deprecated in a future release.

[NOTE]
====
In link:https://orandownloadsweb.azurewebsites.net/download?id=344[O-RAN O-Cloud Notification API Specification for Event Consumers 3.0], the resource is defined as a hierarchical path for the subsystem that produces the notifications.
The PTP events REST API v2 does not have a global subscription for all lower hierarchy resources contained in the resource path.
You subscribe consumer applications to the various available event types separately.
====

For more information, see xref:../networking/ptp/ptp-cloud-events-consumer-dev-reference-v2.adoc#ptp-cloud-events-consumer-dev-reference-v2[Developing PTP event consumer applications with the REST API v2].

[id="ptp-automatic-leap-seconds-handling-for-ptp-clocks_{context}"]
==== Automatic leap seconds handling for PTP grandmaster clocks

The PTP Operator now automatically updates the leap second file by using Global Positioning System (GPS) announcements.

Leap second information is stored in an automatically generated `ConfigMap` resource named `leap-configmap` in the `openshift-ptp` namespace.

For more information, see xref:../networking/ptp/configuring-ptp.adoc#ptp-configuring-dynamic-leap-seconds-handling-for-tgm_configuring-ptp[Configuring dynamic leap seconds handling for PTP grandmaster clocks].

[id="ocp-4-17-nw-sriov-day1-nics"]
==== NIC partitioning for SR-IOV devices (Generally Available)

With this update, the ability to enable NIC partitioning for Single Root I/O Virtualization (SR-IOV) devices at install time is Generally Available.

For more information, see xref:../installing/installing_bare_metal/preparing-to-install-on-bare-metal.adoc#nw-sriov-dual-nic-con_preparing-to-install-on-bare-metal[NIC partitioning for SR-IOV devices].

[id="ocp-4-17-nw-sriov-vfs-day2"]
==== Host network settings for SR-IOV VFs (Generally Available)

With this update, the ability to update host network settings for Single Root I/O Virtualization (SR-IOV) network virtual functions in an existing cluster is Generally Available.

For more information, see xref:../networking/k8s_nmstate/k8s-nmstate-updating-node-network-config.adoc#virt-example-vf-host-services_k8s_nmstate-updating-node-network-config[Node network configuration policy for virtual functions].

[id="ocp-4-17-nw-coredns-1-11-3"]
==== CoreDNS update to version 1.11.3

{product-title} {product-version} now includes CoreDNS version 1.11.3.

[id="ocp-4-17-nw-metallb"]
==== Changes to MetalLB

With this update, MetalLB uses `FRR-K8s` as the default backend.
Previously, this was an optional feature available in Technology Preview.
For more information, see xref:../networking/metallb/metallb-frr-k8s.adoc#metallb-configure-frr-k8s[Configuring the integration of MetalLB and FRR-K8s].

MetalLB also includes a new field for the Border Gateway Protocol (BGP) peer custom resource, `connectTime`.
You can use this field to specify how long BGP waits between connection attempts to a neighbor.
For more information, see xref:../networking/metallb/metallb-configure-bgp-peers.html#nw-metallb-bgppeer-cr_configure-metallb-bgp-peers[About the BGP peer custom resource].



[id="ocp-4-17-registry"]
=== Registry

[id="ocp-4-17-chunksizemib-configuration-parameter"]
==== New chunkSizeMiB configuration parameter for S3 registry storage

A new, optional configuration parameter, `chunkSizeMiB`, is now available for deployments using S3 API-compatible backend storage. When configured, it determines the size of the multipart upload chunks for the S3 API. The default value is `10` MiB, with a minimum of `5` MiB.

For more information, see xref:../registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.adoc#registry-operator-configuration-resource-overview-aws-s3_configuring-registry-storage-aws-user-infrastructure[Image Registry Operator configuration parameters for AWS S3].

[id="ocp-4-17-storage_{context}"]
=== Storage

[id="ocp-4-17-storage-efs-vol-metrics"]
==== AWS EFS CSI storage usage metrics is generally available
Amazon Web Services (AWS) Elastic File Service (EFS) usage metrics allow you to monitor how much space is used by EFS volumes. This feature is generally available.

[IMPORTANT]
====
Turning on these metrics can lead to performance degradation because the CSI driver walks through the whole volume. Therefore, this option is disabled by default. Administrators must explicitly enable this feature.
====

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-aws-efs.adoc#aws-efs-storage-csi-usage-metrics[Amazon Elastic File CSI Storage usage metrics].

[id="ocp-4-17-storage-control-vol-mode-conversion"]
==== Preventing unauthorized volume mode conversion is generally available
Previously, there was no validation of whether the mode of an original volume (filesystem or raw block), whose snapshot was taken, matches the mode of a newly created volume. This presented a security gap that could allow malicious users to potentially exploit an as-yet-unknown vulnerability in the host operating system.

Nevertheless, some users have a legitimate need to perform such conversions. This feature allows cluster administrators to provide these rights (ability to perform update or patch operations on `VolumeSnapshotContents objects`) only to trusted users or applications, such as backup vendors.

To convert a volume mode, an authorized user needs to change `snapshot.storage.kubernetes.io/allow-volume-mode-change: "true"` for VolumeSnapshotContent of the snapshot source.

This feature is supported as generally available.

[id="ocp-4-17-storage-auto-del-resources-gcp-filestore"]
==== Automatic deletion of resources for GCP Filestore is generally available
In earlier versions of {product-title}, when destroying a cluster, Google Compute Platform (GCP) Filestore Storage did not delete all of the cloud resources belonging to that cluster. This required manually deleting all of the persistent volume claims (PVCs) that used the Filestore storage class before destroying the cluster.

With {product-title} 4.17, when destroying a cluster the {product-title} installer should generally delete all of the cloud resources that belong to that cluster, and therefore manual deletion of PVCs should not be required. However, due to the special nature of the Google Compute Platform (GCP) Filestore resources, the automated cleanup process might not remove all of the resources in some rare cases. This feature is supported as generally available.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-google-cloud-file.adoc#persistent-storage-csi-google-cloud-file-delete-instances_persistent-storage-csi-google-cloud-file[Destroying clusters and GCP Filestore].

[id="ocp-4-17-storage-azure-file-snapshot"]
==== Azure File CSI supports snapshots (Technology Preview)
{product-title} 4.17 introduces volume snapshot support for the Microsoft Azure File Container Storage Interface (CSI) Driver Operator. This capability is supported as a Technology Preview feature.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi.adoc#csi-drivers-supported_persistent-storage-csi[CSI drivers supported by {product-title}] and xref:../storage/container_storage_interface/persistent-storage-csi-snapshots.adoc[CSI volume snapshots].

[id="ocp-4-17-storage-multi-vcenter-support"]
==== Multiple vCenter support for vSphere CSI (Technology Preview)
{product-title} v4.17 introduces the ability to deploy {product-title} across multiple vSphere clusters (vCenters). This feature is supported with Technology Preview status.

Multiple vCenters can only be configured during installation. The maximum number of supported vCenter clusters is three.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere-multi-vcenter-support-overview_persistent-storage-csi-vsphere[Multiple vCenter support for vSphere CSI] and xref:../installing/installing_vsphere/installation-config-parameters-vsphere.adoc[Installation configuration parameters for vSphere].

[id="ocp-4-17-storage-disabling-storage-vsphere"]
==== Disabling and enabling storage on vSphere (Technology Preview)
Cluster administrators might want to disable the VMWare vSphere Container Storage Interface (CSI) Driver as a Day 2 operation, so the vSphere CSI Driver does not interface with your vSphere setup. This feature is supported at the Technology Preview level.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere-disable-storage-overview_persistent-storage-csi-vsphere[Disabling and enabling storage on vSphere].

[id="ocp-4-17-storage-selinuxmount-dev-preview"]
==== RWX/RWO SELinux Mount (Developer Preview)
Pods might take a very long time to start when the volume contains a large number of files. To avoid SELinux labeling issues while keeping SELinux confining, you can enable the ReadWriteMany/ReadWriteOnce (RWX/RWO) SELinux Mount feature. Be advised that the RWX/RWO SELinux Mount feature is a Developer Preview feature. It is not supported by Red Hat, and you should not enable this feature set on production or clusters that you plan to maintain over time.

:FeatureName: RWX/RWO SELinux Mount
include::snippets/developer-preview.adoc[]

For more information about the RWX/RWO SELinux Mount feature, including how to enable it, see link:https://access.redhat.com/articles/7087028[RWX/RWO SELinux Mount feature Knowledge Centered Service article].

[id="ocp-4-17-storage-migrating-cns-vols-between-datastores-dev-preview"]
==== Migrating CNS volumes between datastores with cns-migration (Developer Preview)
In {product-title} 4.17, if you are running out of space in your current datastore, or want to move to a more performant datastore, you can migrate volumes between datastores. Be advised that this feature is a Developer Preview feature. It is not supported by Red Hat.

:FeatureName: Migrating CNS Volumes Between Datastores
include::snippets/developer-preview.adoc[]

For more information about cns-migration, see link:https://github.com/openshift/vmware-vsphere-csi-driver-operator/blob/master/docs/cns-migration.md[Moving CNS volumes between datastores].

[id="ocp-4-17-oci"]
=== {oci-first}

[id="ocp-4-17-oci_{context}"]
=== Oracle(R) Cloud Infrastructure

[id="ocp-4-17-olm_{context}"]
=== Operator lifecycle ({olmv0})

[id="ocp-4-17-olm-extensions-guide_{context}"]
==== {olmv1-first} documentation moved to new Extensions guide (Technology Preview)

For more information, see the new features and enhancements section for xref:../release_notes/ocp-4-17-release-notes.adoc#ocp-4-17-extensions_release-notes[Extensions].

[id="ocp-4-17-osdk_{context}"]
=== Operator development ({olmv0})

[id="ocp-4-17-builds_{context}"]
=== Builds

[id="ocp-4-17-machine-config-operator_{context}"]
=== Machine Config Operator

[id="ocp-4-17-machine-config-operator-global-tls_{context}"]
==== Control plane TLS security profiles supported by the MCO

The Machine Config Operator (MCO) and Machine Config Server now use the TLS security profile that is configured for the control plane components. For more information, see xref:../security/tls-security-profiles.adoc#tls-profiles-kubernetes-configuring_tls-security-profiles[Configuring the TLS security profile for the control plane].

[id="ocp-4-17-machine-config-operator-boot-image-aws_{context}"]
==== Updated boot images for AWS now supported (Technology Preview)

Updated boot images are now supported as a Technology Preview feature for Amazon Web Services (AWS) clusters. This feature allows you configure your cluster to update the node boot image whenever you update your cluster. By default, the boot image in your cluster is not updated along with your cluster. For more information, see xref:../machine_configuration/mco-update-boot-images.adoc#mco-update-boot-images[Updated boot images].

[id="ocp-4-17-machine-config-operator-boot-image-gcp_{context}"]
==== Updated boot images for GCP clusters promoted to GA

Updated boot images has been promoted to GA for Google Cloud Platform (GCP) clusters. For more information, see xref:../machine_configuration/mco-update-boot-images.adoc#mco-update-boot-images[Updated boot images].

[id="ocp-4-17-machine-config-operator-node-disrupt-ga_{context}"]
==== Node disruption policies promoted to GA

Node disruption policies for Google Cloud Platform (GCP) clusters has been promoted to GA. A node disruption policy allows you to define a set of Ignition config objects changes that would require little or no disruption to your workloads. For more information, see xref:../machine_configuration/machine-config-node-disruption.adoc#machine-config-node-disruption[Using node disruption policies to minimize disruption from machine config changes].

[id="ocp-4-17-machine-management_{context}"]
=== Machine management

[id="ocp-4-17-enhancements-azure-res-cap_{context}"]
==== Configuring Capacity Reservation by using machine sets

{product-title} release {product-version} introduces support for on-demand Capacity Reservation with Capacity Reservation groups on {azure-full} clusters.
For more information, see _Configuring Capacity Reservation by using machine sets_ for xref:../machine_management/creating_machinesets/creating-machineset-azure.adoc#machineset-azure-capacity-reservation_creating-machineset-azure[compute] or xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-azure.adoc#machineset-azure-capacity-reservation_cpmso-config-options-azure[control plane] machine sets.

[id="ocp-4-17-nodes_{context}"]
=== Nodes

[id="ocp-4-17-enhancements-nodes-crio-wipe_{context}"]
==== New CRIO command behavior
Beginning in {product-title} 4.17, when a node is rebooted, the `crio wipe` command checks that the CRI-O binary exited cleanly. Those images that did not exit cleanly are targeted as corrupted and removed. This behavior prevents CRI-O from failing to start due to half-pulled images or other unsynced files. In {product-title} 4.15 and 4.16, the `crio wipe` command removed all images when a node was rebooted. The `crio wipe` commandâ€™s new behavior increases efficiency while still reducing the risk of image corruption when a node is rebooted.

[id="ocp-4-17-must-gather-new-flags_{context}"]
==== New flags added for must-gather command

{product-title} release {product-version} adds two new flags for use with the `oc adm must-gather` command to limit the timespan of the information gathered. Only one of the following flags can be used at a time. Plugins are encouraged but not required to support these flags.

* `--since`: Only return logs newer than a relative duration, such as 5s, 2m, or 3h. Defaults to all logs.
* `--since-time`: Only return logs after a specific date, expressed in the RFC3339 format. Defaults to all logs.

For a full list of flags to use with the `oc adm must-gather command`, see xref:../support/gathering-cluster-data.adoc#must-gather-flags_gathering-cluster-data[Must-gather flags].

[id="ocp-4-17-nodes-userns_{context}"]
==== Linux user namespaces now supported for pods (Technology Preview)

{product-title} release {product-version} adds support for deploying pods and containers into Linux user namespaces. Running pods and containers in individual user namespaces can mitigate several vulnerabilities that a compromised container can pose to other pods and the node itself. For more information, see xref:../nodes/pods/nodes-pods-user-namespaces.adoc#nodes-pods-user-namespaces[Running pods in Linux user namespaces].

[id="ocp-4-17-nodes-crio-port_{context}"]
==== CRI-O metrics port now uses TLS

{product-title} monitoring now uses a TLS-backed endpoint to fetch CRI-O container runtime metrics. These certificates are managed by the system and not the user. {product-title} monitoring queries have been updated to the new port. For information on the certificates used by monitoring, see xref:../security/certificate_types_descriptions/monitoring-and-cluster-logging-operator-component-certificates.adoc#cert-types-monitoring-and-cluster-logging-operator-component-certificates[Monitoring and OpenShift Logging Operator component certificates].

[id="ocp-4-17-cro_{context}"]
=== Cluster Resource Override Admission Operator

[id="ocp-4-17-cro-infra_{context}"]
==== Moving the Cluster Resource Override Operator

By default, the installation process creates a Cluster Resource Override Operator pod on a worker node and two Cluster Resource Override pods on control plane nodes. You can move these pods to other nodes, such as an infrastructure node, as needed. For more information, see xref:../nodes/clusters/nodes-cluster-overcommit.adoc#nodes-cluster-resource-override-move-infra_nodes-cluster-overcommit[Moving the Cluster Resource Override Operator pods].

[id="ocp-4-17-cro-deploy_{context}"]
=== Cluster Resource Override Operator pod is owned by a deployment object

The Cluster Resource Override Operator pod is now owned by a deployment object. Previously, the Operator was owned by a daemon set object. Using a deployment for the Operator addresses a number of issues, including additional security and add the ability to run the pods on worker nodes.

[id="ocp-4-17-monitoring_{context}"]
=== Monitoring

The in-cluster monitoring stack for this release includes the following new and modified features.
//change the line if no features

[id="ocp-4-17-monitoring-updates-to-monitoring-stack-components-and-dependencies"]
==== Updates to monitoring stack components and dependencies

This release includes the following version updates for in-cluster monitoring stack components and dependencies:

* Alertmanager to 0.27.0
* Prometheus Operator to 0.75.2
* Prometheus to 2.53.1
* kube-state-metrics to 2.13.0
* node-exporter to 1.8.2
* Thanos to 0.35.1

[id="ocp-4-17-monitoring-changes-to-alerting-rules"]
==== Changes to alerting rules

[NOTE]
====
Red{nbsp}Hat does not guarantee backward compatibility for recording rules or alerting rules.
====

* Added the `PrometheusKubernetesListWatchFailures` alert to warn users about Prometheus and Kubernetes API failures, such as unreachable API and permissions issues, which can lead into silent service discovery failures.

[id="ocp-4-17-monitoring-prometheus-updated-to-tolerate-jitters-at-scrape-time-uwm"]
==== Updated Prometheus to tolerate jitters at scrape time for user-defined projects
With this update, the Prometheus configuration for monitoring for user-defined projects now tolerates jitters at scrape time. This update optimizes data compression for monitoring deployments that show sub-optimal chunk compression for data storage, which reduces the disk space used by the time series database in these deployments.

[id="ocp-4-17-network-observability-1-6_{context}"]
==== Network Observability Operator
The Network Observability Operator releases updates independently from the {product-title} minor version release stream. Updates are available through a single, Rolling Stream which is supported on all currently supported versions of {product-title} 4. Information regarding new features, enhancements, and bug fixes for the Network Observability Operator is found in the xref:../observability/network_observability/network-observability-operator-release-notes.adoc#network-observability-rn[Network Observability release notes].

[id="ocp-4-17-scalability-and-performance_{context}"]
=== Scalability and performance

[id="ocp-4-17-etcd-4-5-nodes_{context}"]
==== Node scaling for etcd

In this release, if your cluster is installed on a bare metal platform, you can scale a cluster to up to 5 nodes as a post-installation task. The etcd Operator scales accordingly to account for the additional node. For more information, see xref:../scalability_and_performance/recommended-performance-scale-practices/recommended-etcd-practices.adoc#etcd-node-scaling_recommended-etcd-practices[Node scaling for etcd].

[id="ocp-4-17-edge-computing_{context}"]
=== Edge computing

[id="ocp-4-17-edge-computing-managing-firmware-with-ztp_{context}"]
==== Managing host firmware settings with {ztp}

You can now configure host firmware settings for managed clusters that you deploy with {ztp}.
You save host profile YAML files alongside `SiteConfig` custom resources (CRs) that you use to deploy the managed clusters.
{ztp} uses the host profiles to configure firmware settings in the managed cluster hosts during deployment.
On the hub cluster, you can use `FirmwareSchema` CRs to discover managed cluster host firmware schema, and `HostFirmwareSettings` CRs and retrieve managed clusters firmware settings.

For more information, see xref:../edge_computing/ztp-deploying-far-edge-sites.adoc#ztp-configuring-host-firmware-with-gitops-ztp_ztp-deploying-far-edge-sites[Managing host firmware settings with {ztp}].

[id="ocp-4-17-image-based-upgrade-enhancements_{context}"]
==== Image-based upgrade enhancements

With this release, the image-based upgrade introduces the following enhancements:

* Simplifies the upgrade process for a large group of managed clusters by adding the `ImageBasedGroupUpgrade` API on the hub
* Labels the managed clusters for action completion when using the `ImageBasedGroupUpgrade` API
* Improves seed cluster validation before the seed image generation
* Automatically cleans up the container storage disk if usage reaches a certain threshold on the managed clusters
* Adds comprehensive event history in the new `status.history` field of the `ImageBasedUpgrade` CR

For more information about the `ImageBasedGroupUpgrade` API, see xref:../edge_computing/image_based_upgrade/ztp-image-based-upgrade.adoc#ztp-image-based-upgrade-concept_ztp-gitops[Managing the image-based upgrade at scale using the ImageBasedGroupUpgrade CR on the hub].

[id="ocp-4-17-hcp_{context}"]
=== Hosted control planes

[id="ocp-4-17-insights-operator-enhancements_{context}"]
=== Insights Operator enhancements

* The Insights Operator now collects the `haproxy_exporter_server_threshold` metric. (link:https://issues.redhat.com/browse/OCPBUGS-36687[*OCPBUGS-36687*])

* Previously, the Insights Operator gathered information about all Ingress Controller certificates, including their `NotBefore` and `NotAfter` dates. This data is now compiled into a `JSON` file located at `aggregated/ingress_controllers_certs.json` for easier monitoring of certificate validity across the cluster.
(link:https://issues.redhat.com/browse/OCPBUGS-35727[*OCPBUGS-35727*])

[id="ocp-4-17-etcd-certificates_{context}"]
=== Security

[id="ocp-17-auto-rotate-etcd-ca_{context}"]
==== Automatic rotation of signer certificates

With this release, all `etcd` certificates originate from a new namespace: `openshift-etcd`. When a new signer certificate is close to its expiration date, the following actions occur:

. An automatic rotation of the signer certificate activates.
. The certificate bundle updates.
. All certificates regenerate with the new signers.

Manual rotation of signer certificates is still supported by deleting the specific secret and waiting for the status pod rollout to complete.

[id="ocp-17-sigstore-verification_{context}"]
==== Sigstore signature image verification

With this release, Technology Preview clusters use Sigstore signatures to verify images that were retrieved using a pull spec that references the `quay.io/openshift-release-dev/ocp-release`repository.

Currently, if you are mirroring images, you must also mirror `quay.io/openshift-release-dev/ocp-release:<release_image_digest_with_dash>.sig` Sigstore signatures in order for the image verification to succeed.

[id="ocp-4-17-notable-technical-changes_{context}"]
== Notable technical changes

{product-title} {product-version} introduces the following notable technical changes:

// Note: use [discrete] for these sub-headings.
// example sub-heading below:
//[discrete]
//[id="ocp-4-17-cluster-cloud-controller-manager-operator"]
//=== Cloud controller managers for additional cloud providers
//

[discrete]
[id="ocp-4-17-operator-sdk-1-36-1_{context}"]
=== Operator SDK 1.36.1

{product-title} {product-version} supports Operator SDK 1.36.1. See xref:../cli_reference/osdk/cli-osdk-install.adoc#cli-osdk-install[Installing the Operator SDK CLI] to install or update to this latest version.

[NOTE]
====
Operator SDK 1.36.1 now supports Kubernetes 1.29 and uses a {op-system-base-full} 9 base image.
====

If you have Operator projects that were previously created or maintained with Operator SDK 1.31.0, update your projects to keep compatibility with Operator SDK 1.36.1.

* xref:../operators/operator_sdk/golang/osdk-golang-updating-projects.adoc#osdk-upgrading-projects_osdk-golang-updating-projects[Updating Go-based Operator projects]

* xref:../operators/operator_sdk/ansible/osdk-ansible-updating-projects.adoc#osdk-upgrading-projects_osdk-ansible-updating-projects[Updating Ansible-based Operator projects]

* xref:../operators/operator_sdk/helm/osdk-helm-updating-projects.adoc#osdk-upgrading-projects_osdk-helm-updating-projects[Updating Helm-based Operator projects]

* xref:../operators/operator_sdk/helm/osdk-hybrid-helm-updating-projects.adoc#osdk-upgrading-projects_osdk-hybrid-helm-updating-projects[Updating Hybrid Helm-based Operator projects]

* xref:../operators/operator_sdk/java/osdk-java-updating-projects.adoc#osdk-upgrading-projects_osdk-java-updating-projects[Updating Java-based Operator projects]

[id="ocp-4-17-deprecated-removed-features_{context}"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* _Not Available_
* _Technology Preview_
* _General Availability_
* _Deprecated_
* _Removed_

[discrete]
[id="ocp-4-17-operators-dep-rem_{context}"]
=== Operator lifecycle and development deprecated and removed features

.Operator lifecycle and development deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Operator SDK
|General Availability
|Deprecated
|Deprecated

|Scaffolding tools for Ansible-based Operator projects
|General Availability
|Deprecated
|Deprecated

|Scaffolding tools for Helm-based Operator projects
|General Availability
|Deprecated
|Deprecated

|Scaffolding tools for Go-based Operator projects
|General Availability
|Deprecated
|Deprecated

|Scaffolding tools for Hybrid Helm-based Operator projects
|Technology Preview
|Deprecated
|Deprecated

|Scaffolding tools for Java-based Operator projects
|Technology Preview
|Deprecated
|Deprecated

|Platform Operators
|Technology Preview
|Removed
|Removed

|Plain bundles
|Technology Preview
|Removed
|Removed

// Do not remove the SQLite database... entry until otherwise directed by the Operator Framework PM
|SQLite database format for Operator catalogs
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
[id="ocp-4-17-images-dep-rem_{context}"]
=== Images deprecated and removed features

.Cluster Samples Operator deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Cluster Samples Operator
|General Availability
|Deprecated
|Deprecated

|====

[discrete]
[id="ocp-4-17-monitoring-dep-rem_{context}"]
=== Monitoring deprecated and removed features

.Monitoring deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|`dedicatedServiceMonitors` setting that enables dedicated service monitors for core platform monitoring
|Deprecated
|Removed
|Removed

|`prometheus-adapter` component that queries resource metrics from Prometheus and exposes them in the metrics API
|Deprecated
|Removed
|Removed

|====

[discrete]
[id="ocp-4-17-install-dep-rem_{context}"]
=== Installation deprecated and removed features

.Installation deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|`--cloud` parameter for `oc adm release extract`
|Deprecated
|Deprecated
|Deprecated

|CoreDNS wildcard queries for the `cluster.local` domain
|Deprecated
|Deprecated
|Deprecated

|`compute.platform.openstack.rootVolume.type` for {rh-openstack}
|Deprecated
|Deprecated
|Deprecated

|`controlPlane.platform.openstack.rootVolume.type` for {rh-openstack}
|Deprecated
|Deprecated
|Deprecated

|`ingressVIP` and `apiVIP` settings in the `install-config.yaml` file for installer-provisioned infrastructure clusters
|Deprecated
|Deprecated
|Deprecated

|Package-based {op-system-base} compute machines
|General Availability
|Deprecated
|Deprecated

|`platform.aws.preserveBootstrapIgnition` parameter for {aws-first}
|General Availability
|Deprecated
|Deprecated

|Terraform infrastructure provider for {aws-first}, {vmw-first} and Nutanix
|General Availability
|Removed
|Removed

|Installing a cluster on {alibaba} with installer-provisioned infrastructure
|Technology Preview
|Removed
|Removed

|Installing a cluster on {aws-short} with compute nodes in {aws-short} Outposts
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
[id="ocp-4-17-clusters-dep-rem_{context}"]
=== Updating clusters deprecated and removed features

.Updating clusters deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|====

[discrete]
=== Machine management deprecated and removed features

.Machine management deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Managing machine with Machine API for {alibaba}
|Technology Preview
|Removed
|Removed

|Cloud controller manager for {alibaba}
|Technology Preview
|Removed
|Removed

|====

[discrete]
=== Storage deprecated and removed features

.Storage deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|AliCloud Disk CSI Driver Operator
|General Availability
|Removed
|Removed

|====

[discrete]
[id="ocp-4-17-hardware-an-driver-dep-rem_{context}"]
=== Specialized hardware and driver enablement deprecated and removed features
.Specialized hardware and driver enablement deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17
|====

[discrete]
[id="ocp-4-17-networking-dep-rem_{context}"]
=== Networking deprecated and removed features

.Networking deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|OpenShift SDN network plugin
|Deprecated
|Deprecated
|Removed

|iptables
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
[id="ocp-4-17-web-console-dep-rem_{context}"]
=== Web console deprecated and removed features

.Web console deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Patternfly 4
|Deprecated
|Deprecated
|Deprecated

|React Router 5
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
[id="ocp-4-17-node-dep-rem_{context}"]
=== Node deprecated and removed features

.Node deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|`ImageContentSourcePolicy` (ICSP) objects
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/zone`
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/region`
|Deprecated
|Deprecated
|Deprecated

|cgroup v1
|General Availability
|Deprecated
|Deprecated

|====

[discrete]
[id="ocp-4-17-cli-dep-rem_{context}"]
=== OpenShift CLI (oc) deprecated and removed features
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|====

[discrete]
[id="ocp-4-17-workloads-dep-rem_{context}"]
=== Workloads deprecated and removed features

.Workloads deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|`DeploymentConfig` objects
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
[id="ocp-4-17-bare-metal-dep-rem_{context}"]
=== Bare metal monitoring deprecated and removed features

.Bare Metal Event Relay Operator tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Bare Metal Event Relay Operator
|Deprecated
|Deprecated
|Removed

|====

[id="ocp-4-17-deprecated-features_{context}"]
=== Deprecated features

[id="ocp-4-17-preserveBootstrapIgnition-deprecated_{context}"]
==== The `preserveBootstrapIgnition` parameter for {aws-short}

The `preserveBootstrapIgnition` parameter for {aws-short} in the `install-config.yaml` file has been deprecated. You can use the `bestEffortDeleteIgnition` parameter instead.
(link:https://issues.redhat.com/browse/OCPBUGS-33661[*OCPBUGS-33661*])

* In {product-title} {product-version}, `kube-apiserver` no longer gets a valid cloud configuration object. As a result, the `PersistentVolumeLabel` admission plugin rejects in-tree Google Compute Engine (GCE) PD PVs, or persistent disk persistent volumes, that do not have the correct topology. (link:https://issues.redhat.com/browse/OCPBUGS-34544[*OCPBUGS-34544*])

[id="ocp-4-17-removed-features_{context}"]
=== Removed features

[id="ocp-4-17-bare-metal-operator_{context}"]
==== Bare Metal Event Relay Operator (BMER)

BMER was deprecated in {product-title} version 4.15 and 4.16. With this release, BMER is no longer supported and the related BMER content is removed from the documentation.

[id="ocp-4-17-sdn-plugin_{context}"]
==== OpenShift SDN network plugin (Removed)
OpenShift SDN network plugin was deprecated in 4.15 and 4.16. With this release, the SDN network plugin is no longer supported and the content has been removed from the documentation.

[id="ocp-4-17-rukpak_{context}"]
==== Removal of RukPak (Technology Preview)

RukPak was introduced as a Technology Preview feature in {product-title} 4.12. Starting in {product-title} 4.14, it was used as a component in the Technology Preview of {olmv1-first}.

Starting in {product-title} 4.17, RukPak is now removed and relevant functionality relied upon by {olmv1} has been moved to other components.

[id="ocp-4-17-future-deprecation"]
=== Notice of future deprecation

[id="ocp-4-17-bug-fixes_{context}"]
== Bug fixes
//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / TALO
//Telco Edge / ZTP

[discrete]
[id="ocp-4-17-api-auth-bug-fixes_{context}"]
==== API Server and Authentication

[discrete]
[id="ocp-4-17-bare-metal-hardware-bug-fixes_{context}"]
==== Bare Metal Hardware Provisioning

[discrete]
[id="ocp-4-17-builds-bug-fixes_{context}"]
==== Builds

* Previously, builds could not set the `GIT_LFS_SKIP_SMUDGE` environment variable and use its value when cloning the source code. This caused builds to fail for some Git repositories with LFS files. With this release, the build is able to set this environment variable and use it during the `git clone` step of the build, which resolves the issue. (link:https://issues.redhat.com/browse/OCPBUGS-33215[*OCPBUGS-33215*])

[discrete]
[id="ocp-4-17-cloud-compute-bug-fixes_{context}"]
==== Cloud Compute

* Previously, a machine controller failed to save the {vmw-full} task ID of an instance template clone operation. This caused the machine to go into the `Provisioning` state and to power off. With this release, the {vmw-full} machine controller can detect and recover from this state. (link:https://issues.redhat.com/browse/OCPBUGS-1735[*OCPBUGS-1735*])

* Previously, the `machine-api` Operator reacted when it deleted a server that was in an `ERROR` state. This happened because the server did not pass a port list. With this release, deleting a machine stuck in an `ERROR` state does not cause an Operator reaction. (link:https://issues.redhat.com/browse/OCPBUGS-34155[*OCPBUGS-34155*])

* Previously, you could not configure capacity reservation on a {azure-first} Workload Identity cluster because of missing permissions. With this release, the `Microsoft.Compute/capacityReservationGroups/deploy/action` permission is added as a default credential request in the `<infra-name>-openshift-machine-api-azure-cloud-credentials` custom role, so that you can now configure capacity reservation as expected. (link:https://issues.redhat.com/browse/OCPBUGS-37154[*OCPBUGS-37154*])

* Previously, an optional internal function of the cluster autoscaler caused repeated log entries when it was not implemented. The issue is resolved in this release. (link:https://issues.redhat.com/browse/OCPBUGS-33932[*OCPBUGS-33932*])

* Previously, a node associated with a restarting machine briefly having a status of `Ready=Unknown` triggered the `UnavailableReplicas` condition in the Control Plane Machine Set Operator. This condition caused the Operator to enter the `Available=False` state and trigger alerts because that state indicates a nonfunctional component that requires immediate administrator intervention. This alert should not have been triggered for the brief and expected unavailabilty during a restart. With this release, a grace period for node unreadiness is added to avoid triggering unnecessary alerts. (link:https://issues.redhat.com/browse/OCPBUGS-20061[*OCPBUGS-20061*])

* Previously, when an {product-title} cluster was installed with no capabilities and later enabled the Build capability, the related Build cluster configuration custom resource definition (CRD) was not created. With this release, the Build cluster configuration CRD and its default instance are created. This allows the Build capability to be fully configured and customized. (link:https://issues.redhat.com/browse/OCPBUGS-34395[*OCPBUGS-34395*])

* Previously, role bindings related to the Image Registry, Build, and `DeploymentConfig` capabilities were created in every namespace, even if the the capabilities were disabled. With this release, role bindings is only created if the capability is enabled on the cluster. (link:https://issues.redhat.com/browse/OCPBUGS-34077[*OCPBUGS-34077*])

[discrete]
[id="ocp-4-17-cloud-cred-operator-bug-fixes_{context}"]
==== Cloud Credential Operator

* Previously, secrets in the cluster were fetched in a single call. When there was a large number of secrets, the API timed out. With this release, the Cloud Credential Operator fetches secrets in batches limited to 100 secrets. This change prevents timeouts when there is a large number of secrets in the cluster. (link:https://issues.redhat.com/browse/OCPBUGS-41233[*OCPBUGS-41233*])

* Previously, the Cloud Credential Operator reported an error when the `awsSTSIAMRoleARN` role was not present on a cluster that used manual mode with AWS Security Token Service. With this release, the Cloud Credential Operator no longer reports this as an error. (link:https://issues.redhat.com/browse/OCPBUGS-33566[*OCPBUGS-33566*])

* Previously, when checking whether passthrough permissions are sufficient, the Cloud Credential Operator sometimes received a response from the Google Cloud Platform API that a permission is invalid for a project. This response caused the Operator to become degraded and installation to fail. With this release, the Operator is updated to handle this error gracefully. (link:https://issues.redhat.com/browse/OCPBUGS-36140[*OCPBUGS-36140*])

[discrete]
[id="ocp-4-17-cluster-version-operator-bug-fixes_{context}"]
==== Cluster Version Operator

* Previously, a rarely occurring race condition between Go routines caused the Cluster Version Operator (CVO) to panic after the CVO started. With this release, the Go routines synchronization is improved and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-32678[*OCPBUGS-32678*])

[discrete]
[id="ocp-4-17-dev-console-bug-fixes_{context}"]
==== Developer Console

* Previously, on some browsers, some icons in the samples catalog were stretched, making it hard to read. With this update, the icons were resized correctly, and now the icons are no longer stretched and easier to read. (link:https://issues.redhat.com/browse/OCPBUGS-34516[*OCPBUGS-34516*])

* Previously, s2i build strategy was not explicitly mentioned in the `func.yml`. Therefore you could not create {ServerlessProductName} functions with the repository. Additionally, error messages were not available if s2i is not mentioned or if `func.yml`. As a result, identifying the reason of failures was not apparent. With this update, if the s2i build strategy is not mentioned, users can still create a function. If it is not s2i, users cannot create a function. The error messages are now different for both the cases. (link:https://issues.redhat.com/browse/OCPBUGS-33733[*OCPBUGS-33733*])

* Previously, when using a Quick Start guided tour in the {product-title} web console, it took multiple clicks of the *Next* button to skip to the next step if the `check your work` dialog was ignored. With this update, it only takes one click, regardless of the state of the `check your work` box. (link:https://issues.redhat.com/browse/OCPBUGS-25929[*OCPBUGS-25929*])

[discrete]
[id="ocp-4-17-driver-toolkit-bug-fixes_{context}"]
==== Driver ToolKit (DTK)

* Previously, DTK incorrectly included the same values for the `KERNEL_VERSION` and `RT_KERNEL_VERSION` environment variables. With this update, the `RT_KERNEL_VERSION` is displayed correctly. (link:https://issues.redhat.com/browse/OCPBUGS-33699[*OCPBUGS-33699*])

[discrete]
[id="ocp-4-17-cloud-etcd-operator-bug-fixes_{context}"]
==== etcd Cluster Operator

* Previous versions of the etcd Operator checked the health of etcd members in serial with an all-member timeout that matched the single-member timeout. As a result, one slow member check could consume the entire timeout and cause later member checks to fail, regardless of the health of that later member. In this release, the etcd Operator checks the health of members in parallel, so the health and speed of one member's check does not affect the other members' checks. (link:https://issues.redhat.com/browse/OCPBUGS-36301[*OCPBUGS-36301*])

[discrete]
[id="ocp-hosted-control-planes-bug-fixes_{context}"]
==== Hosted control planes

* Previously, when a hosted cluster proxy was configured and it used an identity provider (IDP) that had an HTTP or HTTPS endpoint, the hostname of the IDP was unresolved before sending it through the proxy. Consequently, hostnames that could only be resolved by the data plane failed to resolve for IDPs. With this update, a DNS lookup is performed before sending IPD traffic through the `konnectivity` tunnel. As a result, IDPs with hostnames that can only be resolved by the data plane can be verified by the Control Plane Operator. (link:https://issues.redhat.com/browse/OCPBUGS-41371[*OCPBUGS-41371*])

* Previously, when the hosted cluster `controllerAvailabilityPolicy` was set to `SingleReplica`, `podAntiAffinity` on networking components blocked the availability of the components. With this release, the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-39313[*OCPBUGS-39313*])

* Previously, the `AdditionalTrustedCA` that was specified in the hosted cluster image configuration was not reconciled into the `openshift-config` namespace, as expected by the `image-registry-operator`, and the component did not become available. With this release, the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-39225[*OCPBUGS-39225*])

* Previously, Red{nbsp}Hat HyperShift periodic conformance jobs failed because of changes to the core operating system. These failed jobs caused the OpenShift API deployment to fail. With this release, an update recursively copies individual trusted certificate authority (CA) certificates instead of copying a single file, so that the periodic conformance jobs succeed and the OpenShift API runs as expected. (link:https://issues.redhat.com/browse/OCPBUGS-38941[*OCPBUGS-38941*])

* Previously, the Konnectivity proxy agent in a hosted cluster always sent all TCP traffic through an HTTP/S proxy. It also ignored host names in the `NO_PROXY` configuration because it only received resolved IP addresses in its traffic. As a consequence, traffic that was not meant to be proxied, such as LDAP traffic, was proxied regardless of configuration. With this release, proxying is completed at the source (control plane) and the Konnectivity agent proxying configuration is removed. As a result, traffic that is not meant to be proxied, such as LDAP traffic, is not proxied anymore. The `NO_PROXY` configuration that includes host names is honored. (link:https://issues.redhat.com/browse/OCPBUGS-38637[*OCPBUGS-38637*])

* Previously, the `azure-disk-csi-driver-controller` image was not getting appropriate override values when using `registryOverride`. This was intentional so as to avoid propagating the values to the `azure-disk-csi-driver` data plane images. With this update, the issue is resolved by adding a separate image override value. As a result, the `azure-disk-csi-driver-controller` can be used with `registryOverride` and no longer affects `azure-disk-csi-driver` data plane images. (link:https://issues.redhat.com/browse/OCPBUGS-38183[*OCPBUGS*])

* Previously, the AWS cloud controller manager within a hosted control plane that was running on a proxied management cluster would not use the proxy for cloud API communication. With this release, the issue is fixed. (link:https://issues.redhat.com/browse/OCPBUGS-37832[*OCPBUGS-37832*])

* Previously, proxying for Operators that run in the control plane of a hosted cluster was performed through proxy settings on the Konnectivity agent pod that runs in the data plane. It was not possible to distinguish if proxying was needed based on application protocol.
+
For parity with {product-title}, IDP communication via HTTPS or HTTP should be proxied, but LDAP communication should not be proxied. This type of proxying also ignores `NO_PROXY` entries that rely on host names because by the time traffic reaches the Konnectivity agent, only the destination IP address is available.
+
With this release, in hosted clusters, proxy is invoked in the control plane via `konnectivity-https-proxy` and `konnectivity-socks5-proxy`, and proxying traffic is stopped from the Konnectivity agent. As a result, traffic that is destined for LDAP servers is no longer proxied. Other HTTPS or HTTPS traffic is proxied correctly. The `NO_PROXY` setting is honored when you specify hostnames. (link:https://issues.redhat.com/browse/OCPBUGS-37052[*OCPBUGS-37052*])

* Previously, proxying for IDP communication occurred in the Konnectivity agent. By the time traffic reached Konnectivity, its protocol and hostname were no longer available. As a consequence, proxying was not done correctly for the OAUTH server pod. It did not distinguish between protocols that require proxying (http/s) and protocols that do not (ldap://). In addition, it did not honor the `no_proxy` variable that is configured in the `HostedCluster.spec.configuration.proxy` spec.
+
With this release, you can configure the proxy on the Konnectivity sidecar of the OAUTH server so that traffic is routed appropriately, honoring your `no_proxy` settings. As a result, the OAUTH server can communicate properly with identity providers when a proxy is configured for the hosted cluster. (link:https://issues.redhat.com/browse/OCPBUGS-36932[*OCPBUGS-36932*])

* Previously, the HostedClusterConfig Operator (HCCO) did not delete the `ImageDigestMirrorSet` CR (IDMS) after you removed the `ImageContentSources` field from the `HostedCluster` object. As a consequence, the IDMS persisted in the `HostedCluster` object when it should not. With this release, the HCCO manages the deletion of IDMS resources from the `HostedCluster` object. (link:https://issues.redhat.com/browse/OCPBUGS-34820[*OCPBUGS-34820*])

* Previously, deploying a `hostedCluster` in a disconnected environment required setting the `hypershift.openshift.io/control-plane-operator-image` annotation. With this update, the annotation is no longer needed. Additionally, the metadata inspector works as expected during the hosted Operator reconciliation, and `OverrideImages` is populated as expected. (link:https://issues.redhat.com/browse/OCPBUGS-34734[*OCPBUGS-34734*])

* Previously, hosted clusters on AWS leveraged their VPC's primary CIDR range to generate security group rules on the data plane. As a consequence, if you installed a hosted cluster into an AWS VPC with multiple CIDR ranges, the generated security group rules could be insufficient. With this update, security group rules are generated based on the provided machine CIDR range to resolve this issue. (link:https://issues.redhat.com/browse/OCPBUGS-34274[*OCPBUGS-34274*])

* Previously, the OpenShift Cluster Manager container did not have the right TLS certificates. As a consequence, you could not use image streams in disconnected deployments. With this release, the TLS certificates are added as projected volumes to resolve this issue. (link:https://issues.redhat.com/browse/OCPBUGS-31446[*OCPBUGS-31446*])

[discrete]
[id="ocp-4-17-image-registry-bug-fixes_{context}"]
==== Image Registry

* In {product-title} 4.14, installing a cluster with {entra-first} was made generally available. With this feature, administrators can configure a Microsoft Azure cluster to use {entra-short}. With {entra-short}, cluster components use temporary security credentials that are managed outside of the cluster.
+
Previously, when {product-title} was deployed on Azure clusters with {entra-short}, storage accounts created for the cluster and the image registry had *Storage Account Key Access* enabled by default, which could pose security risks to the deployment.
+
With this update, shared access keys are disabled by default on new installations that use {entra-short}, enhancing security by preventing the use of shared access keys.
+
[IMPORTANT]
====
Shared access keys should only be disabled if the cluster is configured to use {entra-short}. Disabling shared access keys on a cluster not configured with {entra-first} can cause the Image Registry Operator to become degraded.
====
+
For existing storage accounts created before this update, shared access keys are not automatically disabled. Administrators must manually disable shared access key support on these storage accounts to prevent the use of shared keys. For more information about disabling shared access keys, see link:https://learn.microsoft.com/en-us/azure/storage/common/shared-key-authorization-prevent?tabs=portal[Prevent Shared Key authorization for an Azure Storage account].
+
link:https://issues.redhat.com/browse/OCPBUGS-39428[(*OCPBUGS-39428*)]

[discrete]
[id="ocp-4-17-installer-bug-fixes_{context}"]
==== Installer

* Previously, when installing a cluster on {ibm-cloud-name} into an existing VPC, the installation program retrieved an unsupported VPC region. Attempting to install into a supported VPC region that follows the unsupported VPC region alphabetically caused the installation program to crash. With this release, the installation program is updated to ignore any VPC regions that are not fully available during resource lookups. (link:https://issues.redhat.com/browse/OCPBUGS-14963[*OCPBUGS-14963*])

* Previously, the installation program attempted to download the OVA on {vmw-first} whether the template field was defined or not. With this update, the issue is resolved. The installation program verifies if the template field is defined. If the template field is not defined, the OVA is downloaded. If the template field is defined, the OVA is not downloaded. (link:https://issues.redhat.com/browse/OCPBUGS-39240[*OCPBUGS-39240*])

* Previously, enabling custom feature gates sometimes caused installation on an AWS cluster to fail if the feature gate `ClusterAPIInstallAWS=true` was not enabled. With this release, the `ClusterAPIInstallAWS=true` feature gate is not required.
(link:https://issues.redhat.com/browse/OCPBUGS-34708[*OCPBUGS-34708*])

* Previously, some processes could be left running if the installation program exited due to infrastructure provisioning failures. With this update, all installation-related processes are terminated when the installation program terminates. (link:https://issues.redhat.com/browse/OCPBUGS-36378[*OCPBUGS-36378*])

* Previously, the installation program required permission to create and delete IAM roles when installing a cluster on {aws-short} even when an existing IAM role was provided. With this update, the installation program only requires these permissions when it is creating IAM roles. (link:https://issues.redhat.com/browse/OCPBUGS-36390[*OCPBUGS-36390*])

* Previously, long cluster names were trimmed without warning the user. With this update, the installation program warns the user when trimming long cluster names. (link:https://issues.redhat.com/browse/OCPBUGS-33840[*OCPBUGS-33840*])

* Previously, the `openshift-install` CLI sometimes failed to connect to the bootstrap node when collecting bootstrap gather logs. The installation program reported an error message such as `The bootstrap machine did not execute the release-image.service systemd unit`. With this release and after the bootstrap gather logs issue occurs, the installation program now reports `Invalid log bundle or the bootstrap machine could not be reached and bootstrap logs were not collected`, which is a more accurate error message. (link:https://issues.redhat.com/browse/OCPBUGS-34953[*OCPBUGS-34953*])

* Previously, when installing a cluster on {aws-short}, subnets that the installation program created were incorrectly tagged with the `kubernetes.io/cluster/<clusterID>: shared` tag. With this update, these subnets are correctly tagged with the `kubernetes.io/cluster/<clusterID>: owned` tag. (link:https://issues.redhat.com/browse/OCPBUGS-36904[*OCPBUGS-36904*])

* Previously, the local etcd data store that is saved during installation was not deleted if the installation failed, consuming extra space on the installation host. With this update, the data store is deleted if infrastructure provisioning failures prevent a successful installation. (link:https://issues.redhat.com/browse/OCPBUGS-36284[*OCPBUGS-36284*])

* Previously, when a folder was undefined and the data center was located in a data center folder, an wrong folder structure was created starting from the root of the vCenter server. By using the Govmomi `DatacenterFolders.VmFolder`, it used the a wrong path. With this release, the folder structure uses the data center inventory path and joins it with the virtual machine (VM) and cluster ID value, and the issue is resolved.(link:https://issues.redhat.com/browse/OCPBUGS-38616[*OCPBUGS-38616*])

* Previously, when templates are defined for each failure domain, the installation program required an external connection to download the OVA in {vmw-full}. With this release, the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-39239[*OCPBUGS-39239*])

* Previously, installing a cluster with a DHCP network on Nutanix caused a failure. With this release, this issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-38934[*OCPBUGS-38934*])

* Previously, due to an EFI Secure Boot failure in the SCOS, when the FCOS pivoted to the SCOS the virtual machine (VM) failed to boot. With this release, the Secure Boot is disabled only when the Secure Boot is enabled in the `coreos.ovf` configuration file, and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-37736[*OCPBUGS-37736*])

* Previously, if you specified an unsupported architecture in the `install-config.yaml` file the installation program would fail with a `connection refused` message. With this update, the installation program correctly validates the cluster architecture parameter, leading to successful installations. (link:https://issues.redhat.com/browse/OCPBUGS-38841[*OCPBUGS-38841*])

* Previously, a rare condition om {vmw-full} Cluster API machines caused the vCenter session management to time out unexpectedly. With this release, the Keep AliveÂ support is disabled in the current and later versions of CAPV, and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-38677[*OCPBUGS-38677*])

* Previously, the installation program on {aws-first} used multiple IPv4 public IP addresses that Amazon has started charging for. With this release, support is provided for bring your own (BYO) public IPv4 pools in {product-title} so that users have control of IP addresses that are used by their services. Where the BYO public IPv4 pools feature is enabled, two new permissions, `ec2:DescribePublicIpv4Pools` and `ec2:DisassociateAddress`, are required, and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-35504[*OCPBUGS-35504*])

* Previously, when users provided public subnets while using existing subnets and creating a private cluster, the installation program occasionally exposed on the public internet the load balancers that were created in public subnets. This invalidated the reason for a private cluster. With this release, the issue is resolved by displaying a warning during a private installation that providing public subnets might break the private clusters and, to prevent this, users must fix their inputs. (link:https://issues.redhat.com/browse/OCPBUGS-38963[*OCPBUGS-38963*])

* Previously, during installation the `oc adm node-image create` command used the kube-system/cluster-config-v1 resource to determine the platform type. With this release, the installation program uses the infrastructure resource, which provides more accurate information about the platform type. (link:https://issues.redhat.com/browse/OCPBUGS-39092[*OCPBUGS-39092*])

* Previously, the `oc adm node-image create` command failed when run against a cluster in a restricted environment with a proxy because the command ignored the cluster-wide proxy setting. With this release, when the command is run it checks the cluster proxy resource settings, where available, to ensure the command is run successfully and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-39090[*OCPBUGS-39090*])

* Previously, when installing a cluster with the Agent-based installer, the assisted-installer process could timeout when attempting to add control plane nodes to the cluster. With this update, the assisted-installer process loads fresh data from the assisted-service process, preventing the timeout. (link:https://issues.redhat.com/browse/OCPBUGS-36779[*OCPBUGS-36779*])

* Previously, when the {vmw-full} vCenter cluster contained an ESXi host that did not have a standard port group defined and the installation program tried to select that host to import the OVA, the import failed and the error â€œInvalid Configuration for device '0'â€ was presented.
With this release, the installation program verifies whether a standard port group for an ESXi host is defined and, if not, continues until it locates an ESXi host with a defined standard port group or presents an error message if it fails to locate one, resolving the issue.
 (link:https://issues.redhat.com/browse/OCPBUGS-38560[*OCPBUGS-38560*])

* Previously, extracting the IP address from the Cluster API Machine object only returned a single IP address. On {vmw-first}, the returned address would always be an IPv6 address and this caused issues with the `must-gather` implementation if the address was non-routable. With this release, the Cluster API Machine object returns all IP addresses, including IPv4, so that the `must-gather` issue no longer occurs on {vmw-full}. (link:https://issues.redhat.com/browse/OCPBUGS-37607[*OCPBUGS-37607*])

* Previously, when installing a cluster on {aws-short}, EKS messages could appear in the installation logs even when EKS was meant to be disabled. With this update, EKS log messages have been disabled. (link:https://issues.redhat.com/browse/OCPBUGS-35752[*OCPBUGS-35752*])

* Previously, unexpected output would appear in the terminal when creating an installer-provisioned infrastructure cluster. With this release, the issue has been resolved and the unexpected output no longer shows. (link:https://issues.redhat.com/browse/OCPBUGS-35547[*OCPBUGS-35547*])

* Previously, when installing a cluster on {aws-short} after deleting a cluster with the `./openshift-install destroy cluster` command, the installation would fail with an error stating that there might already be a running cluster. With this update, all leftover artifacts are removed when the cluster is destroyed, resulting in successful installations afterwards. (link:https://issues.redhat.com/browse/OCPBUGS-35542[*OCPBUGS-35542*])

* Previously, when installing a cluster on {aws-short}, load balancer ingress rules were continuously revoked and re-authorized, causing unnecessary API calls and delays in cluster provisioning. With this update, load balancer ingress rules are no longer revoked during installation, reducing API traffic and installation delays. (link:https://issues.redhat.com/browse/OCPBUGS-35440[*OCPBUGS-35440*])

* Previously, when setting `platform.openstack.controlPlanePort.network` without a `fixedIPs` value, the installation program would output a misleading error message about the network missing subnets. With this release, the installation program validates that the `install-config` field `controlPlanePort` has a valid subnet filter set because it is a required value. (link:https://issues.redhat.com/browse/OCPBUGS-37104[*OCPBUGS-37104*])

* Previously, adding IPv6 support for user-provisioned installation platforms caused an issue with naming {rh-openstack-first} resources, especially when you run two user-provisioned installation clusters on the same {rh-openstack-first} platform. This happened because the two clusters share the same names for network, subnets, and router resources. With this release, all the resources names for a cluster remain unique for that cluster so no interfere occurs. (link:https://issues.redhat.com/browse/OCPBUGS-33973[*OCPBUGS-33973*])

* Previously, when installing a cluster on {ibm-power-server-name} with installer-provisioned infrastructure, the installation could fail due to load balancer timeouts. With this update, the installation program waits for the load balancer to be available instead of timing out. (link:https://issues.redhat.com/browse/OCPBUGS-34869[*OCPBUGS-34869*])

* Previously, when using the Assisted Installer, using a password that contained the colon character (`:`) resulted in a failed installation. With this update, pull secrets containing a colon in the password do not cause the Assisted Installer to fail. (link:https://issues.redhat.com/browse/OCPBUGS-31727[*OCPBUGS-31727*])

* Previously, solid state drives (SSD) that used SATA hardware were identified as removable. The Assisted Installer for {product-title} reported that no eligible disks were found and the installation stopped. With this release, removable disks are eligible for installation. (link:https://issues.redhat.com/browse/OCPBUGS-33404[*OCPBUGS-33404*])

* Previously, when installing a cluster on bare metal using installer provisioned infrastructure, the installation could time out if the network to the bootstrap virtual machine is slow. With this update, the timeout duration has been increased to cover a wider range of network performance scenarios. (link:https://issues.redhat.com/browse/OCPBUGS-41500[*OCPBUGS-41500*])

* Previously, when installing a cluster on {ibm-power-server-name}, the installation program did not list the `e980` system type in the `madrid` region. With this update, the installation program correctly lists this region. (link:https://issues.redhat.com/browse/OCPBUGS-38439[*OCPBUGS-38439*])

* Previously, after installing a {sno} cluster, the monitoring system could produce an alert that applied to clusters with multiple nodes. With this update, {sno} clusters only produce monitoring alerts that apply to {sno} clusters. (link:https://issues.redhat.com/browse/OCPBUGS-35833[*OCPBUGS-35833*])

* Previously, when installing a cluster on {ibm-power-server-name}, the installation could fail due to a DHCP server network collision. With this update, the installation program selects a random number to generate the DHCP network to avoid collision. (link:https://issues.redhat.com/browse/OCPBUGS-33912[*OCPBUGS-33912*])

* Previously, the installation program used the Neutron API endpoint to tag security groups. This API does not support special characters, so some {rh-openstack-first} clusters failed to install on {rh-openstack}. With this release, the installation program uses an alternative endpoint to tag security groups so that the issue no longer persists. (link:https://issues.redhat.com/browse/OCPBUGS-36913[*OCPBUGS-36913*])

* Previously, setting an invalid Universally Unique Identifier (UUID) for the `additionalNetworkIDs` parameter of a machine pool in your `install-config` configuration file could result in the installation program exiting from installing the cluster. With this release, the installation program checks the validity of the `additionalNetworkIDs` parameter before the program continuing with installing the cluster so that this issue no longer persists. (link:https://issues.redhat.com/browse/OCPBUGS-35420[*OCPBUGS-35420*])

* Previously, for {ibm-power-server-name} installer-provisioned infrastructure clusters, if no network name existed for a Dynamic Host Configuration Protocol (DHCP), the destroy code would skip deleting the DHCP resource.  With this release, a test now checks if a DHCP is in an `ERROR` state, so that the DHCP resource is deleted. (link:https://issues.redhat.com/browse/OCPBUGS-35039[*OCPBUGS-35039*])

[discrete]
[id="ocp-4-17-insights-operator-bug-fixes_{context}"]
==== Insights Operator

* Previously, in some Hypershift hosted clusters, the IO archive contained the hostname even with network obfuscation enabled. This issue has been resolved, and IO archives no longer contain hostnames when they are obfuscated. (link:https://issues.redhat.com/browse/OCPBUGS-33082[*OCPBUGS-33082*])

[discrete]
[id="ocp-4-17-kube-controller-bug-fixes_{context}"]
==== Kubernetes Controller Manager


[discrete]
[id="ocp-4-17-kube-scheduler-bug-fixes_{context}"]
==== Kubernetes Scheduler

[discrete]
[id="ocp-4-17-machine-config-operator-bug-fixes_{context}"]
==== Machine Config Operator

* Previously, in a cluster that runs {product-title} 4.16 with the Telco RAN DU reference configuration, long duration `cyclictest` or `timerlat` tests could fail with maximum latencies detected above `20`us. This issue occured because the `psi` kernel command line argument was being set to `1` by default when cgroup v2 is enabled. With this release, the issue is fixed by setting `psi=0` in the kernel arguments when enabling cgroup v2. The `cyclictest` latency issue reported in link:https://issues.redhat.com/browse/OCPBUGS-34022[*OCPBUGS-34022*] is now also fixed. (link:https://issues.redhat.com/browse/OCPBUGS-37271[*OCPBUGS-37271*])

* Previously, if a cluster admin creates a new `MachineOSConfig` object that references a legacy pull secret, the canonicalized version of this secret that gets created is not updated whenever the original pull secret changes. With this release, the issue is resolved.
(link:https://issues.redhat.com/browse/OCPBUGS-34079[*OCPBUGS-34079*])

* Previously, the `/etc/mco/internal-registry-pull-secret.json` secret was being managed by the Machine Config Operator (MCO). Due to a recent change, this secret rotates on an hourly basis. Whenever the MCO detected a change to this secret, it rolled the secret out to each node in the cluster, which resulted in disruptions. With this fix, a different internal mechanism processes changes to the internal registry pull secret to avoid rolling out repeated MachineConfig updates.
(link:https://issues.redhat.com/browse/OCPBUGS-33913[*OCPBUGS-33913*])

* Previously, if you created more than one `MachineOSConfig` object that required a canonicalized secret, only the first object would build. With this fix, the build controller handles multiple `MachineOSBuilds` that use the same canonicalized secret. (link:https://issues.redhat.com/browse/OCPBUGS-33671[*OCPBUGS-33671*])

* Previously, if machine config pools (MCP) had a higher `maxUnavailable` value than the cluster's number of unavailable nodes, cordoned nodes were able to be erroneously selected as an update candidate. This fix adds a node readiness check in the node controller so that cordoned nodes are queued for an update. (link:https://issues.redhat.com/browse/OCPBUGS-33397[*OCPBUGS-33397*])

* Previously, nodes could be drained twice if the node was queued multiple times in the drain controller. This behaviour might have been due to increased activity on the node object by on-cluster layering functionality. With this fix, a node queued for drain only drains once. (link:https://issues.redhat.com/browse/OCPBUGS-33134[*OCPBUGS-33134*])

* Previously, a potential panic was seen in Machine Config Controller and Machine Build Controller objects if a de-reference accidentally deleted `MachineOSConfig/MachineOSBuild` to read the build status. The panic is controlled with additional error conditions to warn for allowed MachineOSConfig deletions. (link:https://issues.redhat.com/browse/OCPBUGS-33129[*OCPBUGS-33129*])

* Previously, after upgrading from {product-title} 4.1 or 4.2 to version 4.15, some machines could get stuck during provisioning and never became available. This was because the `machine-config-daemon-firstboot` service was failing due to an incompatible `machine-config-daemon` binary on those nodes. With this release, the correct `machine-config-daemon` binary is copied to nodes before booting. (link:https://issues.redhat.com/browse/OCPBUGS-28974[*OCPBUGS-28974*])

* Previously, if you attempted to configure on-cluster {op-system-first} image layering on a non-{op-system} node, the node became degraded. With this fix, in this situation, an error message is produced in the node logs, but the node is not degraded. (link:https://issues.redhat.com/browse/OCPBUGS-19537[*OCPBUGS-197537*])

[discrete]
[id="ocp-4-17-management-console-bug-fixes_{context}"]
==== Management Console

* Previously, a warning was not provided when you were on a {gcp-first} cluster that supports {gcp-wid-short} and that the Operator supports it. With this release, logic was added to support {gcp-wid-short} and Federated Identity Operator installs, so now you are alerted when you are on a {gcp-short} cluster. (link:https://issues.redhat.com/browse/OCPBUGS-38591[*OCPBUGS-38591*])

* Previously, the version number text in the *Updates* graph on the *Cluster Settings* page appeared as black text on a dark background when using Firefox in dark mode. With this update, the text appears as white text. (link:https://issues.redhat.com/browse/OCPBUGS-38427[*OCPBUGS-38427*])

* Previously, dynamic plugins using PatternFly 4 referenced variables that are not available in {product-title} 4.15 and later. This was causing contrast issues for {rh-rhacm-first} in dark mode. With this update, older chart styles are now available to support PatternFly 4 charts used by dynamic plugins. (link:https://issues.redhat.com/browse/OCPBUGS-36816[*OCPBUGS-36816*])

* Previously, when the `Display Admission Webhook` warning implementation presented issues with some incorrect code. With this update, the unnecessary warning message has been removed. (link:https://issues.redhat.com/browse/OCPBUGS-35940[*OCPBUGS-35940*])

* Previously, the global sync lock that applied to all HTTP servers spawned goroutines with a sync lock that is specific to each of the refresh tokens. With this release, the global refresh sync lock on a cluster with an external OIDC environment was replaced with a sync that refreshes for each token. As a result, refresh token performance is improved by 30% to 50%. (link:https://issues.redhat.com/browse/OCPBUGS-35080[*OCPBUGS-35080*])

* Previously, a warning was not displayed for the `minAvailable` warning in `PodDisruptionBudget` create and edit form. With this update, code logic for displaying the `minAvailable` warning was added, and the `minAvailable` warning is displayed if violated. (link:https://issues.redhat.com/browse/OCPBUGS-34937[*OCPBUGS-34937*])

* Previously, the *OperandDetails* page displayed information for the first CRD that matched by name. After this fix, the *OperandDetails* page displays information for the CRD that matches by name and the version of the operand. (link:https://issues.redhat.com/browse/OCPBUGS-34901[*OCPBUGS-34901*])

* Previously, one inactive or idle browser tab caused session expiration for all other tabs. With this change, activity in any tab will prevent session expiration even if there is one inactive or idle browser tab. (link:https://issues.redhat.com/browse/OCPBUGS-34387[*OCPBUGS-34387*])

* Previously, the `Display Admission Webhook` warning implementation presented issues with some incorrect code. With this release, the unnecessary warning message has been removed. (link:https://issues.redhat.com/browse/OCPBUGS-35940[*OCPBUGS-34316*])

* Previously, text areas were not resizable. With this update, you are now able to resize text areas. (link:https://issues.redhat.com/browse/OCPBUGS-34200[*OCPBUGS-34200*])

* Previously, the Console Operator was not able to tolerate the absence of the ingress capability. With this update, the Console Operator configuration API has been enhanced with the possibility to add alternative ingress for the environments where the ingress cluster capability is disabled. (link:https://issues.redhat.com/browse/OCPBUGS-33787[OCPBUGS-33787*])

* Previously, the *Debug container* link was not displayed for pods with a `Completed` status. With this change, the link now appears. (link:https://issues.redhat.com/browse/OCPBUGS-33631[*OCPBUGS-33631*])

* Previously, the {product-title} web console did not show `filesystem` metrics on the *Nodes list* page due to incorrect Prometheus query. With this update, `filesystem` metrics are correctly displayed. (link:https://issues.redhat.com/browse/OCPBUGS-33136[*OCPBUGS-33136*])

* Previously, pseudolocalization was not working due to a configuration issue. After this fix, pseudolocalization works again. (link:https://issues.redhat.com/browse/OCPBUGS-30218[*OCPBUGS-30218*])

* Previously, console pods would crash loop if the `--user-auth` flag was set to `disabled`. With this update, the console backend properly handles this value. (link:https://issues.redhat.com/browse/OCPBUGS-29510[*OCPBUGS-29510*])

* Previously, utilization cards displayed a `limit` that incorrectly implied a relationship between capacity and limits. With this update, the position of `limit` was changed and the wording updated. (link:https://issues.redhat.com/browse/OCPBUGS-23332[*OCPBUGS-23332*])

* Previously, in some edge cases, the wrong resource could be fetched when using websockets to watch a namespaced resource without providing a namespace. With this update, a validation to the resource watch logic was added to prevent the websocket request and log an error under this condition. (link:https://issues.redhat.com/browse/OCPBUGS-19855[*OCPBUGS-19855*])

* Previously, perspective switching was not properly handled. With this update, perspectives that are passed with URL search parameters or plugin route page extensions now correctly switch the perspective and retain the correct URL path. (link:https://issues.redhat.com/browse/OCPBUGS-19048[*OCPBUGS-19048*])

[discrete]
[id="ocp-4-17-monitoring-bug-fixes_{context}"]
==== Monitoring

* Previously, the `config-reloader` of Prometheus for user-defined projects would fail if unset environment variables were used in the `ServiceMonitor` configuration, which resulted in Prometheus pod failure. With this release, the reloader no longer fails when an unset environment variable is encountered. Instead, unset environment variables are left as they are, while set environment variables are expanded as usual. Any expansion errors, suppressed or otherwise, can be tracked through the `reloader_config_environment_variable_expansion_errors` variable. (link:https://issues.redhat.com/browse/OCPBUGS-23252[*OCPBUGS-23252*])

[discrete]
[id="ocp-4-17-networking-bug-fixes_{context}"]
==== Networking

* Previously, the SR-IOV Network Operator was listing the `SriovNetworkNodePolicies` resources in random order. This caused the `sriov-device-plugin` pod to enter a continuous restart loop. With this release, the SR-IOV Network Operator lists policies in a deterministic order so that the `sriov-device-plugin` pod does not enter a continuous restart loop. (link:https://issues.redhat.com/browse/OCPBUGS-36243[*OCPBUGS-36243*])

* Previously, an interface created inside a new pod would remain inactive and the Gratuitous Address Resolution Protocol (GARP) notification would be generated. The notification did not reach the cluster and this prevented ARP tables of other pods inside the cluster from updating the MAC address of the new pod. This situation caused cluster traffic to stall until ARP table entries expired. With this release, a GARP notification is now sent after the interface inside a pod is active so that the GARP notification reaches the cluster. As a result, surrounding pods can identify the new pod earlier than they could with the previous behavior. (link:https://issues.redhat.com/browse/OCPBUGS-30549[*OCPBUGS-30549*])

* Previously, enabling FIPS for a cluster caused SR-IOV device plugin pods to fail. With this release, SR-IOV device plugin pods have FIPS enabled so that when you enable FIPS for the cluster, the pods do not fail. (link:https://issues.redhat.com/browse/OCPBUGS-41131[*OCPBUGS-41131*])

* Previously, a race condition was generated after rebooting an {product-title} node that used a performance profile with a small number of reserved CPUs. This occurred because Single Root I/O Virtualization (SR-IOV) virtual functions (VFs) shared the same MAC address and any pods that used the VFs would experience communication issues. With this release, an update to the SR-IOV Network Operator config daemon ensures that the Operator checks that no duplicate MAC addresses do not exist on VFs. (link:https://issues.redhat.com/browse/OCPBUGS-33137[*OCPBUGS-33137*])

* Previously, if you deleted the `sriovOperatorConfig` custom resource (CR), you could not create a new `sriovOperatorConfig` CR. With this release, the Single Root I/O Virtualization (SR-IOV) Network Operator removes validating webhooks when you delete the `sriovOperatorConfig` CR, so that you can create a new `sriovOperatorConfig` CR.   (link:https://issues.redhat.com/browse/OCPBUGS-37567[*OCPBUGS-37567*])

* Previously, when you switched your cluster to use a different load balancer, the Ingress Operator did not remove the values from the `classicLoadBalancer` and `networkLoadBalancer` parameters in the `IngressController` custom resource (CR) status. This situation caused the status of the CR to report wrong information from the `classicLoadBalancer` and `networkLoadBalancer` parameters. With this release, after you switch your cluster to use a different load balancer, the Ingress Operator removes values from these parameters so that the CR reports a more accurate and less confusing message status. (link:https://issues.redhat.com/browse/OCPBUGS-38646[*OCPBUGS-38646*])

* Previously, no multicast packets reached their intended target nodes when a multicast sender and a multicast receiver existed on the same node. This happened because of an OVN-Kubernetes RPM package update. With this release, this regression is fixed in the OVN-Kubernetes RPM package, so that the issue no longer persists. (link:https://issues.redhat.com/browse/OCPBUGS-34778[*OCPBUGS-34778*])

* Previously, when you created a `LoadBalancer` service for the Ingress Operator, a log message was generated that stated the change was not effective. This log message should only trigger for a change to an `Infra` custom resource. With this release, this log message is no longer generated when you create a `LoadBalancer` service for the Ingress Operator. (link:https://issues.redhat.com/browse/OCPBUGS-34413[*OCPBUGS-34413*])

* Previously, the `DNSNameResolver` controller sent DNS requests to CoreDNS pods for DNS names that had IP addresses with expired time-to-live (TTL) values. This caused a continuous generation of DNS requests and memory leak issues for those pods. With this release, the `DNSNameResolver` controller waits until it receives the updated list of IP addresses and TTL values for a DNS name before sending any more requests to the DNS name. As a result, the controller no longer generates erroneous requests and sends them to pods. CoreDNS pods can now respond to DNS requests in a timely manner and update the `DNSNameResolver` objects with the latest IP addresses and TTLs. (link:https://issues.redhat.com/browse/OCPBUGS-33750[*OCPBUGS-33750*])

* Previously, when you used the `must-gather` tool, a Multus Container Network Interface (CNI) log file, `multus.log`, was stored in a node's file system. This situation caused the tool to generate unnecessary debug pods in a node. With this release, the Multus CNI no longer creates a `multus.log` file, and instead uses a CNI plugin pattern to inspect any logs for Multus DaemonSet pods in the `openshift-multus` namespace. (link:https://issues.redhat.com/browse/OCPBUGS-33959[*OCPBUGS-33959*])

* Previously, an alert for `OVNKubernetesNorthdInactive` would not fire in circumstances where it should fire. With this release, the issue is fixed so that the alert for `OVNKubernetesNorthdInactive` fires as expected. (link:https://issues.redhat.com/browse/OCPBUGS-33758[*OCPBUGS-33758*])

* Previously, for all pods where the default route has been customized, a missing route for the Kubernetes-OVN masquerade address caused each pod to be unable to connect to itself through a service for which it acts as a backend. With this release, the missing route for Kubernetes-OVN masquerade address is added to pods so that the issue no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-36865[*OCPBUGS-36865*])

* Previously, the `iptables-alerter` pod did not handle errors from the `crictl` command-line interface, which could cause the pod to incorrectly log events from `host-network` pods or cause pod restarts. With this release, the errors are handled correctly so that these issues no longer persist. (link:https://issues.redhat.com/browse/OCPBUGS-37713[*OCPBUGS-37713*])

* Previously, if you created a hosted cluster by using a proxy for the purposes of making the cluster reach a control plane from a compute node, the compute node would be unavailable to the cluster. With this release, the proxy settings are updated for the node so that the node can use a proxy to successfully communicate with the control plane. (link:https://issues.redhat.com/browse/OCPBUGS-37786[*OCPBUGS-37786*])

* Previously, when a cluster failed to install on an on-premise platform with a configured load balancer, the `LoadBalancer` service's `LoadBalancerReady` condition received the `SyncLoadBalancerFailed` status. The status generated the following message:
+
[source,text]
----
The kube-controller-manager logs might contain more details.
----
+
This message is wrong because the logs are stored in the `cloud-controller-manager` namespace of a project. With this release,  the `SyncLoadBalancerFailed` status now communicates the correct message: +
+
[source, text]
----
The cloud-controller-manager logs may contain more details.
----
+
(link:https://issues.redhat.com/browse/OCPBUGS-31664[*OCPBUGS-31664*])

* Previously, you could not control log levels for the internal component that selects IP addresses for cluster nodes. With this release, you can now enable debug log levels so that you can either increase or decrease log levels on-demand. To adjust log levels, you must create a config map manifest file with a configuration similar to the following:
+
[source,yaml]
----
apiVersion: v1
data:
  enable-nodeip-debug: "true"
kind: ConfigMap
metadata:
  name: logging
  namespace: openshift-vsphere-infra
# ...
----
+
(link:https://issues.redhat.com/browse/OCPBUGS-32348[*OCPBUGS-32348*])

* Previously, the Ingress Operator could not successfully update the canary route because the Operator did not have permission to update `spec.host` or `spec.subdomain` fields on an existing route. With this release, the required permission is added to the cluster role for the Operatorâ€™s service account and the Ingress Operator can update the canary route. (link:https://issues.redhat.com/browse/OCPBUGS-36465[*OCPBUGS-36465*])

* Previously, administrator privileges were required to run some networking containers, such as Keepalived, on supported on-premise platforms. With this release, these containers no longer require administrator privileges to run them on supported on-premise platforms. (link:https://issues.redhat.com/browse/OCPBUGS-36175[*OCPBUGS-36175*])

* Previously, if your `NodeNetworkConfigurationPolicy` (NNCP) custom resource (CR) is set to use the default spanning tree protocol (STP) implementation, the CR configuration file would show `stp.enabled: true`, but the {product-title} web console cleared the STP checkbox. With this release, the web console only clears the STEP checkbox after you define `stp.enabled: false` in the NNCP CR YAML file. (link:https://issues.redhat.com/browse/OCPBUGS-36238[*OCPBUGS-36238*])

* Previously, the Ingress Controller status was incorrectly displayed as `Degraded=False` because of a migration time issue with the `CanaryRepetitiveFailures` condition. With this release, the Ingress Controller status is correctly marked as `Degraded=True` for the appropriate length of time that the `CanaryRepetitiveFailures` condition exists. (link:https://issues.redhat.com/browse/OCPBUGS-39220[*OCPBUGS-39220*])

[discrete]
[id="ocp-4-17-node-bug-fixes_{context}"]
==== Node

* Previously, the Container Runtime Config controller did not detect whether a mirror configuration was in use before adding the scope from a `ClusterImagePolicy` CR to the `/etc/containers/registries.d/sigstore-registries.yaml` file. As a consequence, image verification failed with a `Not looking for sigstore attachments` message. With this fix, images are pulled from the mirror registry as expected. (link:https://issues.redhat.com/browse/OCPBUGS-36344[*OCPBUGS-36344*])

* Previously, a group ID was not added to the `/etc/group` directory within a container when the `spec.securityContext.runAsGroup` attribute was set in the pod specification. With this release, this issue is fixed. (link:https://issues.redhat.com/browse/OCPBUGS-39478[*OCPBUGS-39478*])

* Previously, because of a critical regression on RHEL 9.4 kernels earlier than `5.14.0-427.26.1.el9_4`, the `mglru` feature had memory management disabled. In this release, the regression issue is fixed so that the `mglru` feature is now enabled in {product-title} {product-version}. (link:https://issues.redhat.com/browse/OCPBUGS-35436[*OCPBUGS-35436*])

[discrete]
[id="ocp-4-17-node-tuning-operator-bug-fixes_{context}"]
==== Node Tuning Operator (NTO)

* Previously, due to an internal bug, the Node Tuning Operator incorrectly computed CPU masks for interrupt and network-handling CPU affinity if a machine had more than 256 CPUs. This prevented proper CPU isolation on those machines and resulted in `systemd` unit failures. With this release, the Node Tuning Operator computes the masks correctly. (link:https://issues.redhat.com/browse/OCPBUGS-39164[*OCPBUGS-39164*])

* Previously, the Open vSwitch (OVS) pinning procedure set the CPU affinity of the main thread, but other CPU threads did not pick up this affinity if they had already been created. As a consequence, some OVS threads did not run on the correct CPU set, which might interfere with the performance of pods with a Quality of Service (QoS) class of `Guaranteed`. With this update, the OVS pinning procedure updates the affinity of all the OVS threads, ensuring that all OVS threads run on the correct CPU set. (link:https://issues.redhat.com/browse/OCPBUGS-35347[*OCPBUGS-35347*])

[discrete]
[id="ocp-4-17-observability-bug-fixes_{context}"]
==== Observability

* Previously, when you log on under the *Administrator* perspective on the {product-title} web console and use the *Observe* -> *Alerting* function,  an `S is not a function` displayed on alert metrics graph. This issue happened because of a missing function validation check. With this release, the function validation check is added so the alert metric chart displays collected metrics. (link:https://issues.redhat.com/browse/OCPBUGS-37291[*OCPBUGS-37291*])

[discrete]
[id="ocp-4-17-openshift-cli-bug-fixes_{context}"]
==== OpenShift CLI (oc)

[discrete]
[id="ocp-4-17-olm-bug-fixes_{context}"]
==== Operator Lifecycle Manager (OLM)

* Previously, clusters with many custom resources (CRs) experienced timeouts from the API server and stranded updates where the only workaround was to uninstall and then reinstall the stranded Operators. This occurred because OLM evaluated potential updates by using a dynamic client lister. With this fix, OLM uses a paging lister for custom resource definitions (CRDs) to avoid timeouts and stranded updates. (link:https://issues.redhat.com/browse/OCPBUGS-41549[*OCPBUGS-47549*])

* Previously, catalog source pods could not recover from a cluster node failure when the `registryPoll` parameter was unset. With this fix, OLM updates its logic for checking for dead pods. As a result, catalog source pods now recover from node failures as expected. (link:https://issues.redhat.com/browse/OCPBUGS-39574[*OCPBUGS-39574*])

* Previously, if you tried to install a previously-deleted Operator after an {product-title} update, the installation might fail. This occurred because OLM could not find previously created bundle unpack jobs. With this fix, OLM correctly installs previously installed Operators. (link:https://issues.redhat.com/browse/OCPBUGS-32439[*OCPBUGS-32439*])

* Previously, when a new version of a custom resource definition (CRD) specified a new conversion strategy, this conversion strategy was expected to successfully convert resources. However, OLM cannot run the new conversion strategies for CRD validation without actually performing the update operation. With this release, OLM generates a warning message during the update process when CRD validations fail with the existing conversion strategy, and the new conversion strategy is specified in the new version of the CRD. (link:https://issues.redhat.com/browse/OCPBUGS-31522[*OCPBUGS-31522*])

* Previously, if the `spec.grpcPodConfig.securityContextConfig` field in `CatalogSource` objects was unset within namespaces with a `PodSecurityAdmission` (PSA) level value of `restricted`, the catalog pod would not pass PSA validation. With this release, the OLM Catalog Operator now configures the catalog pod with the `securityContexts` necessary to pass PSA validation.(link:https://issues.redhat.com/browse/OCPBUGS-29729[*OCPBUGS-29729*])

* Previously, the `catalogd-controller-manager` pod might not have been deployed to a node despite being in the scheduling queue, and the OLM Operator would fail to install. With this fix, CPU requests are reduced for the related resources, and the issue no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-29705[*OCPBUGS-29705*])

* Previously, the Catalog Operator sometimes attempted to connect to deleted catalog sources that were stored in the cache. With this fix, the Catalog Operator queries a client to list the catalog sources on a cluster. (link:https://issues.redhat.com/browse/OCPBUGS-8659[*OCPBUGS-8659*])

[discrete]
[id="ocp-4-17-openshift-api-server-bug-fixes_{context}"]
==== OpenShift API server

[discrete]
[id="ocp-4-17-rhcos-bug-fixes_{context}"]
==== {op-system-first}

[discrete]
[id="ocp-4-17-scalability-and-performance-bug-fixes_{context}"]
==== Scalability and performance

[discrete]
[id="ocp-4-17-storage-bug-fixes_{context}"]
==== Storage

* Previously, the Secrets Store Container Storage Interface (CSI) Driver on {hcp} clusters failed to mount secrets because of an issue when using the {hcp} command-line interface, `hcp`, to create OpenID Connect (OIDC) infrastructure on {aws-full}. With this release, the issue has been fixed so that the driver can now mount volumes. (link:https://issues.redhat.com/browse/OCPBUGS-18711[*OCPBUGS-18711*])

[discrete]
[id="ocp-4-17-windows-containers-bug-fixes_{context}"]
==== Windows containers

[id="ocp-4-17-technology-preview-tables_{context}"]
== Technology Preview features status

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red{nbsp}Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the following tables, features are marked with the following statuses:

* _Not Available_
* _Technology Preview_
* _General Availability_
* _Deprecated_
* _Removed_

[discrete]
=== Networking Technology Preview features

.Networking Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Advertise using L2 mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Technology Preview
|Technology Preview
|Technology Preview

|Multi-network policies for SR-IOV networks
|General Availability
|General Availability
|General Availability

|Updating the interface-specific safe sysctls list
|Technology Preview
|Technology Preview
|Technology Preview

|Egress service custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|VRF specification in `BGPPeer` custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|VRF specification in `NodeNetworkConfigurationPolicy` custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|Admin Network Policy (`AdminNetworkPolicy`)
|Technology Preview
|General Availability
|General Availability

|IPsec external traffic (north-south)
|General Availability
|General Availability
|General Availability

|Host network settings for SR-IOV VFs
|Technology Preview
|Technology Preview
|General Availability

|Integration of MetalLB and FRR-K8s
|Not Available
|Technology Preview
|General Availability

|Dual-NIC Intel E810 PTP boundary clock with highly available system clock
|Not Available
|General Availability
|General Availability

|Intel E810 Westport Channel NIC as PTP grandmaster clock
|Technology Preview
|General Availability
|General Availability

|Dual-NIC Intel E810 Westport Channel as PTP grandmaster clock
|Technology Preview
|General Availability
|General Availability

|Automatic leap seconds handling for PTP grandmaster clocks
|Not Available
|Not Available
|General Availability

|PTP events REST API v2
|Not Available
|Not Available
|General Availability

|Configure the `br-ex` bridge needed by OVN-Kuberenetes using NMState
|Not Available
|Technology Preview
|Technology Preview

| Live migration to OVN-Kubernetes from OpenShift SDN
| Not Available
| General Availability
| Not Available

| Overlapping IP configuration for multi-tenant networks with Whereabouts
| Not Available
| General Availability
| General Availability

|====

[discrete]
=== Storage Technology Preview features

.Storage Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|AWS EFS storage CSI usage metrics
|Not Available
|Not Available
|General Availability

|Automatic device discovery and provisioning with Local Storage Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Azure File CSI snapshot support
|Not Available
|Not Available
|Technology Preview

|{ibm-power-server-name} Block CSI Driver Operator
|General Availability
|General Availability
|General Availability

|Read Write Once Pod access mode
|Technology Preview
|General Availability
|General Availability

|Shared Resources CSI Driver in OpenShift Builds
|Technology Preview
|Technology Preview
|Technology Preview

|{secrets-store-operator}
|Technology Preview
|Technology Preview
|Technology Preview

|CIFS/SMB CSI Driver Operator
|Not Available
|Technology Preview
|Technology Preview

|VMWare vSphere multiple vCenter support
|Not Available
|Not Available
|Technology Preview

|Disabling/enabling storage on vSphere
|Not Available
|Not Available
|Technology Preview

|RWX/RWO SELinux Mount
|Not Available
|Not Available
|Developer Preview

|Migrating CNS Volumes Between Datastores
|Not Available
|Not Available
|Developer Preview

|====

[discrete]
=== Installation Technology Preview features

.Installation Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Installing {product-title} on {oci-first} with VMs
|Technology Preview
|Technology Preview
|Technology Preview

|Installing {product-title} on {oci-first} on bare metal
|Developer Preview
|Developer Preview
|Developer Preview

|Adding kernel modules to nodes with kvc
|Technology Preview
|Technology Preview
|Technology Preview

|Enabling NIC partitioning for SR-IOV devices
|Technology Preview
|Technology Preview
|General Availability

|User-defined labels and tags for {gcp-first}
|Technology Preview
|Technology Preview
|General Availability

|Installing a cluster on {alibaba} by using installer-provisioned infrastructure
|Technology Preview
|Not Available
|Not Available

|Installing a cluster on Alibaba Cloud by using Assisted Installer
|Not Available
|Technology Preview
|Technology Preview

|Mount shared entitlements in BuildConfigs in RHEL
|Technology Preview
|Technology Preview
|Technology Preview

|{product-title} on {oci-first}
|Technology Preview
|Technology Preview
|Technology Preview

|Selectable Cluster Inventory
|Technology Preview
|Technology Preview
|Technology Preview

|Static IP addresses with {vmw-first} (IPI only)
|Technology Preview
|General Availability
|General Availability

|Support for iSCSI devices in {op-system}
|Technology Preview
|General Availability
|General Availability

|Installing a cluster on {gcp-short} using the Cluster API implementation
|Not Available
|Technology Preview
|General Availability

|Support for Intel(R) VROC-enabled RAID devices in {op-system}
|Technology Preview
|General Availability
|General Availability

|====

[discrete]
=== Node Technology Preview features

.Nodes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|`MaxUnavailableStatefulSet` featureset
|Technology Preview
|Technology Preview
|Technology Preview

|Linux user namespace support
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Multi-Architecture Technology Preview features

.Multi-Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|{ibm-power-server-name} using installer-provisioned infrastructure
|General Availability
|General Availability
|General Availability

|`kdump` on `arm64` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `s390x` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `ppc64le` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|Multiarch Tuning Operator
|Not available
|Technology Preview
|Technology Preview

|====

//[discrete]
//=== Specialized hardware and driver enablement Technology Preview features
//
//.Specialized hardware and driver enablement Technology Preview tracker
//[cols="4,1,1,1",options="header"]
// |====
// |Feature |4.15 |4.16 |4.17
// |===
// No Tech Preview or going-to-ga features from driver enablement - info from bthurber 24-Sep-24

[discrete]
[id="ocp-4-17-scalability-tech-preview_{context}"]
=== Scalability and performance Technology Preview features

.Scalability and performance Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|{factory-prestaging-tool}
|Technology Preview
|Technology Preview
|Technology Preview

|Hyperthreading-aware CPU manager policy
|Technology Preview
|Technology Preview
|Technology Preview

|HTTP transport replaces AMQP for PTP and bare-metal events
|Technology Preview
|General Availability
|General Availability

|Mount namespace encapsulation
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Tuning etcd latency tolerances
|Technology Preview
|General Availability
|General Availability

|Increasing the etcd database size
|Not Available
|Technology Preview
|Technology Preview

|Using {rh-rhacm} `PolicyGenerator` resources to manage {ztp} cluster policies
|Not Available
|Technology Preview
|Technology Preview

|Pinned Image Sets
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
[id="ocp-4-17-operators-tech-preview_{context}"]
=== Operator lifecycle and development Technology Preview features

.Operator lifecycle and development Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Operator Lifecycle Manager (OLM) v1
|Technology Preview
|Technology Preview
|Technology Preview

|RukPak
|Technology Preview
|Technology Preview
|Removed

|Platform Operators
|Technology Preview
|Removed
|Removed

|Scaffolding tools for Hybrid Helm-based Operator projects
|Technology Preview
|Deprecated
|Deprecated

|Scaffolding tools for Java-based Operator projects
|Technology Preview
|Deprecated
|Deprecated

|====

[discrete]
=== OpenShift CLI (oc) Technology Preview features

.OpenShift CLI (`oc`) Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|oc-mirror plugin v2
|Not Available
|Technology Preview
|Technology Preview

|Enclave support
|Not Available
|Technology Preview
|Technology Preview

|Delete functionality
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Monitoring Technology Preview features

.Monitoring Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Metrics Collection Profiles
|Technology Preview
|Technology Preview
|Technology Preview

|Metrics Server
|Technology Preview
|General Availability
|General Availability

|====


[discrete]
=== {rh-openstack-first} Technology Preview features

.{rh-openstack} Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Dual-stack networking with installer-provisioned infrastructure
|General Availability
|General Availability
|General Availability

|Dual-stack networking with user-provisioned infrastructure
|General Availability
|General Availability
|General Availability

|{rh-openstack} integration into the {cluster-capi-operator}
|Technology Preview
|Technology Preview
|Technology Preview

|Control Plane with `rootVolumes` and `etcd` on local disk
|Technology Preview
|Technology Preview
|General Availability

|Hosted control planes
|Not Available
|Not Available
|Developer Preview

|====

[discrete]
=== Hosted control planes Technology Preview features

.Hosted control planes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Hosted control planes for {product-title} on {aws-first}
|Technology Preview
|Technology Preview
|Technology Preview

|Hosted control planes for {product-title} using non-bare metal agent machines
|Technology Preview
|Technology Preview
|Technology Preview

|Hosted control planes for an ARM64 {product-title} cluster on {aws-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Hosted control planes for {product-title} on {ibm-power-title}
|Technology Preview
|Technology Preview
|Technology Preview

|Hosted control planes for {product-title} on {ibm-z-title}
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Machine management Technology Preview features

.Machine management Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Managing machines with the Cluster API for {aws-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {gcp-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {vmw-full}
|Not Available
|Technology Preview
|Technology Preview

|Defining a {vmw-short} failure domain for a control plane machine set
|Technology Preview
|General Availability
|General Availability

|Cloud controller manager for {alibaba}
|Technology Preview
|Removed
|Removed

|Cloud controller manager for {gcp-full}
|General Availability
|General Availability
|General Availability

|Cloud controller manager for {ibm-power-server-name}
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Authentication and authorization Technology Preview features

.Authentication and authorization Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Pod security admission restricted enforcement
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Machine Config Operator Technology Preview features

.Machine Config Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Improved MCO state reporting
|Technology Preview
|Technology Preview
|Technology Preview

|On-cluster RHCOS image layering
|Not Available
|Technology Preview
|Technology Preview

|Node disruption policies
|Not Available
|Technology Preview
|General Availability

|Updating boot images for GCP clusters
|Not Available
|Technology Preview
|General Availability

|Updating boot images for AWS clusters
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
[id="edge-computing-tp-features_{context}"]
=== Edge computing Technology Preview features

.Edge computing Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.15 |4.16 |4.17

|Accelerated provisioning of {ztp}
|Not Available
|Technology Preview
|Technology Preview

|Deploying IPsec encryption to managed clusters with {ztp} and {rh-rhacm}
|Not Available
|Technology Preview
|Technology Preview

|====

[id="ocp-4-17-known-issues_{context}"]
== Known issues

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* A known issue exists when deleting a `NetworkAttachmentDefinition` (NAD) resource created by a `UserDefinedNetwork` resource. You must check to see if a pod is referencing the NAD before deleting the NAD. The pod should be deleted before the NAD. Failure to do so can leave pods in an unexpected state. (link:https://issues.redhat.com/browse/OCPBUGS-39185[*OCPBUGS-39185*])

* The DNF package manager included in {op-system-first} images cannot be used at runtime, because DNF relies on additional packages to access entitled nodes in a cluster that are under a Red Hat subscription. As a workaround, use the `rpm-ostree` command instead. (link:https://issues.redhat.com/browse/OCPBUGS-35247[*OCPBUGS-35247*])

* When installing a cluster on {azure-full}, the installation will fail if no `install-config.yaml` file is provided. If an `install-config.yaml` file is provided, and `controlPlane.platform` is present but `controlPlane.platform.azure` is not provided, the installation will fail. (link:https://issues.redhat.com/browse/OCPBUGS-42296[*OCPBUGS-42296*])
+
See xref:../installing/installing_azure/ipi/installing-azure-customizations.adoc#installation-azure-config-yaml_installing-azure-customizations[Sample customized install-config.yaml file for Azure] for a sample configuration file, or set a non-null parameter as in the following example:
+
[source,yaml]
----
controlPlane:
  platform:
    azure: {}
----

* When installing multiple clusters on {azure-full}, running multiple installations simultaneously from the same installation host will result in only one of the clusters installing successfully. If you run the installations sequentially rather than simultaneously, you can install multiple clusters on {azure-short} from the same installation host. (link:https://issues.redhat.com/browse/OCPBUGS-42296[*OCPBUGS-42296*])

* When installing a cluster on {azure-full}, specifying the `Standard_M8-4ms` instance type results in an error due to that instance type specifying its memory in decimal format instead of integer format. (link:https://issues.redhat.com/browse/OCPBUGS-42241[*OCPBUGS-42241*])

* Due to a change in storage account naming in {product-title} {product-version}, the Azure File Container Storage Interface (CSI) driver now alphabetically matches storage account of the Image Registry Operator. With this change, there is a known issue where the Azure File CSI driver fails to mount all volumes when the image registry is configured as private. The mount failures occur because the CSI driver tries to use the storage account of the Image Registry Operator, which is not configured to allow connections from worker subnets.
+
As a temporary workaround, the image registry should not be configured as private when using the Azure File CSI driver. This is a known issue and will be fixed in a future version of {product-title}. (link:https://issues.redhat.com/browse/OCPBUGS-42308[*OCPBUGS-42308*])

* When installing a cluster on {azure-short}, the installation fails if a customer-managed encryption key is specified. (link:https://issues.redhat.com/browse/OCPBUGS-42349[*OCPBUGS-42349*])

* When an error occurs during mirroring Operators and additional images, the log message "Generating Catalog Source" might still appear, even if no files are generated. (link:https://issues.redhat.com/browse/OCPBUGS-42503[*OCPBUGS-42503*])

[id="ocp-telco-ran-4-17-known-issues_{context}"]

* When you run Cloud-native Network Functions (CNF) latency tests on an {product-title} cluster, the test can sometimes return results greater than the latency threshold for the test; for example, 20 microseconds for `cyclictest` testing. This results in a test failure.
(link:https://issues.redhat.com/browse/OCPBUGS-42328[*OCPBUGS-42328*])

[id="ocp-telco-core-4-17-known-issues_{context}"]

[id="ocp-storage-core-4-17-known-issues_{context}"]

* If the controller pod terminates while cloning, or taking or restoring a volume snapshot, is in progress, the Microsoft Azure File clone or snapshot persistent volume claims (PVCs) remain in the Pending state. To resolve this issue, delete any affected clone or snapshot PVCs, and then recreate those PVCs. (link:https://issues.redhat.com/browse/OCPBUGS-35977[*OCPBUGS-35977*])

[id="ocp-hosted-control-planes-4-17-known-issues_{context}"]

* Deploying a self-managed private hosted cluster on AWS fails because the `bootstrap-kubeconfig` file uses an incorrect KAS port. As a result, the AWS instances are provisioned, but cannot join the hosted cluster as nodes. (link:https://issues.redhat.com/browse/OCPBUGS-31840[*OCPBUGS-31840*])

[id="ocp-4-17-asynchronous-errata-updates_{context}"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red{nbsp}Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red{nbsp}Hat Customer Portal users can enable errata notifications in the account settings for Red{nbsp}Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red{nbsp}Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating_a_cluster/updating-cluster-web-console.adoc#updating-cluster-web-console[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-17-0-ga_{context}"]
=== RHSA-2024:XXXX - {product-title} {product-version}.0 image release, bug fix, and security update advisory

Issued: TBD

{product-title} release {product-version}.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:XXXX[RHSA-2024:XXXX] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2024:XXXX[RHSA-2024:XXXX] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.17.0 --pullspecs
----
//replace 4.y.z for the correct values for the release. You do not need to update oc to run this command.
