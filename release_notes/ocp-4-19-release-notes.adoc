:_mod-docs-content-type: ASSEMBLY
[id="ocp-4-19-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red{nbsp}Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-19-about-this-release_{context}"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-2024:11038[RHSA-2024:11038]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.32.md[Kubernetes 1.32] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. From the {hybrid-console}, you can deploy {product-title} clusters to either on-premises or cloud environments.

You must use {op-system} machines for the control plane and for the compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517
//Removed paragraph about the RHEL package because mode workers are removed from 4.19, per Scott Dodson
//Even-numbered release lifecycle verbiage (Comment in for even-numbered releases)
////
Starting from {product-title} 4.14, the Extended Update Support (EUS) phase for even-numbered releases increases the total available lifecycle to 24 months on all supported architectures, including `x86_64`, 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures. Beyond this, Red{nbsp}Hat also offers a 12-month additional EUS add-on, denoted as _Additional EUS Term 2_, that extends the total available lifecycle from 24 months to 36 months. The Additional EUS Term 2 is available on all architecture variants of {product-title}. For more information about support for all versions, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].
////

//Odd-numbered release lifecycle verbiage (Comment in for odd-numbered releases)

The support lifecycle for odd-numbered releases, such as {product-title} {product-version}, on all supported architectures, including `x86_64`, 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures is 18 months. For more information about support for all versions, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

Commencing with the {product-title} 4.14 release, Red{nbsp}Hat is simplifying the administration and management of Red{nbsp}Hat shipped cluster Operators with the introduction of three new life cycle classifications; Platform Aligned, Platform Agnostic, and Rolling Stream. These life cycle classifications provide additional ease and transparency for cluster administrators to understand the life cycle policies of each Operator and form cluster maintenance and upgrade plans with predictable support boundaries. For more information, see link:https://access.redhat.com/webassets/avalon/j/includes/session/scribe/?redirectTo=https%3A%2F%2Faccess.redhat.com%2Fsupport%2Fpolicy%2Fupdates%2Fopenshift_operators[OpenShift Operator Life Cycles].

// Added in 4.14. Language came directly from Kirsten Newcomer.
{product-title} is designed for FIPS. When running {op-system-base-full} or {op-system-first} booted in FIPS mode, {product-title} core components use the {op-system-base} cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the `x86_64`, `ppc64le`, and `s390x` architectures.

For more information about the NIST validation program, see link:https://csrc.nist.gov/Projects/cryptographic-module-validation-program/validated-modules[Cryptographic Module Validation Program]. For the latest NIST status for the individual versions of {op-system-base} cryptographic libraries that have been submitted for validation, see link:https://access.redhat.com/articles/2918071#fips-140-2-and-fips-140-3-2[Compliance Activities and Government Standards].

[id="ocp-4-19-add-on-support-status_{context}"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-19-new-features-and-enhancements_{context}"]
== New features and enhancements

The following new features are supported on {ibm-power-title} with {product-title} {product-version}:

* Support for {ibm-power-name}11

This release adds improvements related to the following components and concepts:

[id="ocp-release-notes-auth_{context}"]
=== Authentication and authorization

[id="ocp-release-notes-auth-direct_{context}"]
==== Enabling direct authentication with an external OIDC identity provider (Technology Preview)

With this release, you can enable direct integration with an external OpenID Connect (OIDC) identity provider to issue tokens for authentication. This bypasses the built-in OAuth server and uses the external identity provider directly.

By integrating directly with an external OIDC provider, you can leverage the advanced capabilities of your preferred OIDC provider instead of being limited by the capabilities of the built-in OAuth server. Your organization can manage users and groups from a single interface, while also streamlining authentication across multiple clusters and in hybrid environments. You can also integrate with existing tools and solutions.

Direct authentication is available as a Technology Preview feature.

For more information, see xref:../authentication/external-auth.adoc#external-auth[Enabling direct authentication with an external OIDC identity provider].

[id="ocp-4-19-auth-ServiceAccountTokenNodeBinding_{context}"]
==== Enable ServiceAccountTokenNodeBinding Kubernetes feature by default

In {product-title} {product-version}, the `ServiceAccountTokenNodeBinding` feature is now enabled by default, aligning with upstream Kubernetes behavior. This feature allows service account tokens to be bound directly to node objects in addition to the existing binding options. Benefits of this change include enhanced security through automatic token invalidation when bound nodes are deleted and better protection against token replay attacks across different nodes.

[id="ocp-release-notes-documentation_{context}"]
=== Documentation

[id="ocp-release-notes-documentation-etcd_{context}"]
==== Consolidated etcd documentation

This release includes an _etcd_ section, which consolidates all of the existing documentation about etcd for {product-title}. For more information, see xref:../etcd/etcd-overview.adoc#etc-overview[Overview of etcd].

[id="ocp-release-notes-documentation-tutorials_{context}"]
==== Tutorials guide

{product-title} 4.19 now includes a _Tutorials_ guide, which takes the place of the _Getting started_ guide in previous releases. The existing tutorials were refreshed and the guide now focuses solely on hands-on tutorial content. It also provides a jumping off point to other recommended hands-on learning resources for {product-title} across Red{nbsp}Hat.

For more information, see xref:../tutorials/index.adoc#tutorials-overview[Tutorials].

[id="ocp-release-notes-edge-computing_{context}"]
=== Edge computing

[id="ocp-4-19-edge-computing_pg-ztp-rhacm_{context}"]
==== Using {rh-rhacm} PolicyGenerator resources to manage {ztp} cluster policies (General Availability)

You can now use `PolicyGenerator` resources and {rh-rhacm-first} to deploy polices for managed clusters with {ztp}.
The `PolicyGenerator` API is part of the link:https://open-cluster-management.io/[Open Cluster Management] standard and provides a generic way of patching resources, which is not possible with the `PolicyGenTemplate` API.
Using `PolicyGenTemplate` resources to manage and deploy polices will be deprecated in an upcoming {product-title} release.

For more information, see xref:../edge_computing/policygenerator_for_ztp/ztp-configuring-managed-clusters-policygenerator.adoc#ztp-configuring-managed-clusters-policygenerator[Configuring managed cluster policies by using PolicyGenerator resources].

[id="ocp-4-19-edge-computing-arbiter-node_{context}"]
==== Configuring a local arbiter node (Technology Preview)

You can configure an {product-title} cluster with two control plane nodes and one local arbiter node so to retain high availability (HA) while reducing infrastructure costs for your cluster. This configuration is only supported for bare-metal installations.

A local arbiter node is a lower-cost, co-located machine that participates in control plane quorum decisions. Unlike a standard control plane node, the arbiter node does not run the full set of control plane services. You can use this configuration to maintain HA in your cluster with only two fully provisioned control plane nodes instead of three.

To enable this feature, you must define the arbiter machine pool in the `install-config.yaml` file and enable the `TechPreviewNoUpgrade` feature set.

Configuring a local arbiter node is available as a Technology Preview feature. For more information, see xref:../installing/installing_bare_metal/ipi/ipi-install-installation-workflow.adoc#ipi-install-config-local-arbiter-node_ipi-install-installation-workflow[Configuring a local arbiter node].

[id="ocp-4-19-edge-computing-coordinating-reboots_{context}"]
==== Coordinating reboots for configuration changes

This release adds reboot policies to ZTP reference that can be applied by {cgu-operator-full} (TALM) to coordinate reboots across a fleet of spoke clusters when configuration changes require a reboot, such as deferred tuning changes. {cgu-operator} reboots all nodes in the targeted `MachineConfigPool` object on the selected clusters when the reboot policy is applied.

Instead of rebooting nodes after each individual change, you can apply all configuration updates through policies and then trigger a single, coordinated reboot.

For more information, see xref:../edge_computing/policygenerator_for_ztp/ztp-configuring-managed-clusters-policygenerator.adoc#ztp-coordinating-reboots-for-config-changes_ztp-configuring-managed-clusters-policygenerator[Coordinating reboots for configuration changes].

[id="ocp-release-notes-extensions_{context}"]
=== Extensions ({olmv1})

[id="ocp-release-notes-olmv1-preflight-permissions-check_{context}"]
==== Preflight permissions check for cluster extensions (Technology Preview)

With this release, the Operator Controller performs a dry run of the installation process when you try to install an extension. This dry run verifies that the specified service account has the required role-based access control (RBAC) rules for the roles and bindings defined by the bundle.

If the service account is missing any required RBAC rules, the preflight check fails before the actual installation proceeds and generates a report.

For more information, see xref:../extensions/ce/managing-ce.adoc#olmv1-troubleshooting-rbac-errors-with-preflight-check_managing-ce[Preflight permissions check for cluster extensions (Technology Preview)]

[id="ocp-release-notes-olmv1-deploying-a-cluster-extension-to-a-specific-namespace_{context}"]
==== Deploying a cluster extension in a specific namespace (Technology Preview)

With this release, you can deploy an extension in a specific namespace by using the `OwnNamespace` or `SingleNamespace` install modes as a Technology Preview feature for `registry+v1` Operator bundles.

For more information, see xref:../extensions/ce/managing-ce.adoc#olmv1-deploying-a-ce-in-a-specific-namespace_managing-ce[Deploying a cluster extension in a specific namespace (Technology Preview)]

[id="ocp-release-notes-hardware-accelerators_{context}"]
=== Hardware accelerators

[id="ocp-4-19-dynamic-accelerator-slicer-operator-tp_{context}"]
==== Dynamic Accelerator Slicer Operator (Technology Preview)

With this release, you can use the Dynamic Accelerator Slicer (DAS) Operator to dynamically slice GPU accelerators in {product-title}, instead of relying on statically sliced GPUs defined when the node is booted. This allows you to dynamically slice GPUs based on specific workload demands, ensuring efficient resource utilization.

For more information, see xref:../hardware_accelerators/das-about-dynamic-accelerator-slicer-operator.adoc#das-about-dynamic-accelerator-slicer-operator[Dynamic Accelerator Slicer (DAS) Operator].

[id="ocp-release-notes-hcp_{context}"]
=== Hosted control planes

Because {hcp} releases asynchronously from {product-title}, it has its own release notes. For more information, see xref:../hosted_control_planes/hosted-control-planes-release-notes.adoc#hosted-control-planes-release-notes[{hcp-capital} release notes].

[id="ocp-release-notes-hcp-openstack-tp_{context}"]
==== Hosted control planes on {rh-openstack-first} 17.1 (Technology Preview)

{hcp-capital} on {rh-openstack} 17.1 are now supported as a Technology Preview.

For more information, see xref:../hosted_control_planes/hcp-deploy/hcp-deploy-openstack.adoc#hosted-clusters-openstack-prerequisites_hcp-deploy-openstack[Deploying hosted control planes on OpenStack].

[id="ocp-release-notes-ibm-power_{context}"]
=== {ibm-power-title}

The {ibm-power-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on {ibm-power-title}:

* Expand Compliance Operator support with profiles for Defense Information Systems Agency Security Technical Implementation Guide (DISA STIG)

[id="ocp-release-notes-ibm-z_{context}"]
=== {ibm-z-title} and {ibm-linuxone-title}

The {ibm-z-name} and {ibm-linuxone-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on {ibm-z-name} and {ibm-linuxone-name}:

* Support for {ibm-name} z17 and {ibm-linuxone-name} 5
* Boot volume Linux Unified Key Setup (LUKS) encryption via {ibm-name} Crypto Express (CEX)


[id="ocp-release-notes-ibm-z-power-support-matrix_{context}"]
=== {ibm-power-title}, {ibm-z-title}, and {ibm-linuxone-title} support matrix

Starting in {product-title} 4.14, Extended Update Support (EUS) is extended to the {ibm-power-name} and the {ibm-z-name} platform. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

.CSI Volumes
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Cloning
|Supported
|Supported

|Expansion
|Supported
|Supported

|Snapshot
|Supported
|Supported
|====

.Multus CNI plugins
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Bridge
|Supported
|Supported

|Host-device
|Supported
|Supported

|IPAM
|Supported
|Supported

|IPVLAN
|Supported
|Supported
|====

.{product-title} features
[cols="3,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Adding compute nodes to on-premise clusters using {oc-first}
|Supported
|Supported

|Alternate authentication providers
|Supported
|Supported

|Agent-based Installer
|Supported
|Supported

|Assisted Installer
|Supported
|Supported

|Automatic Device Discovery with Local Storage Operator
|Unsupported
|Supported

|Automatic repair of damaged machines with machine health checking
|Unsupported
|Unsupported

|Cloud controller manager for {ibm-cloud-name}
|Supported
|Unsupported

|Controlling overcommit and managing container density on nodes
|Unsupported
|Unsupported

|CPU manager
|Supported
|Supported

|Cron jobs
|Supported
|Supported

|Descheduler
|Supported
|Supported

|Egress IP
|Supported
|Supported

|Encrypting data stored in etcd
|Supported
|Supported

|FIPS cryptography
|Supported
|Supported

|Helm
|Supported
|Supported

|Horizontal pod autoscaling
|Supported
|Supported

|Hosted control planes
|Supported
|Supported

|IBM Secure Execution
|Unsupported
|Supported

|Installer-provisioned Infrastructure Enablement for {ibm-power-server-name}
|Supported
|Unsupported

|Installing on a single node
|Supported
|Supported

|IPv6
|Supported
|Supported

|Monitoring for user-defined projects
|Supported
|Supported

|Multi-architecture compute nodes
|Supported
|Supported

|Multi-architecture control plane
|Supported
|Supported

|Multipathing
|Supported
|Supported

|Network-Bound Disk Encryption - External Tang Server
|Supported
|Supported

|Non-volatile memory express drives (NVMe)
|Supported
|Unsupported

|nx-gzip for Power10 (Hardware Acceleration)
|Supported
|Unsupported

|oc-mirror plugin
|Supported
|Supported

|OpenShift CLI (`oc`) plugins
|Supported
|Supported

|Operator API
|Supported
|Supported

|OpenShift Virtualization
|Unsupported
|Supported

|OVN-Kubernetes, including IPsec encryption
|Supported
|Supported

|PodDisruptionBudget
|Supported
|Supported

|Precision Time Protocol (PTP) hardware
|Unsupported
|Unsupported

|{openshift-local-productname}
|Unsupported
|Unsupported

|Scheduler profiles
|Supported
|Supported

|Secure Boot
|Unsupported
|Supported

|Stream Control Transmission Protocol (SCTP)
|Supported
|Supported

|Support for multiple network interfaces
|Supported
|Supported

|The `openshift-install` utility to support various SMT levels on {ibm-power-name} (Hardware Acceleration)
|Supported
|Unsupported

|Three-node cluster support
|Supported
|Supported

|Topology Manager
|Supported
|Unsupported

|z/VM Emulated FBA devices on SCSI disks
|Unsupported
|Supported

|4K FCP block device
|Supported
|Supported
|====

.Operators
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|{cert-manager-operator}
|Supported
|Supported

|Cluster Logging Operator
|Supported
|Supported

|Cluster Resource Override Operator
|Supported
|Supported

|Compliance Operator
|Supported
|Supported

|Cost Management Metrics Operator
|Supported
|Supported

|File Integrity Operator
|Supported
|Supported

|HyperShift Operator
|Supported
|Supported

|{ibm-power-server-name} Block CSI Driver Operator
|Supported
|Unsupported

|Ingress Node Firewall Operator
|Supported
|Supported

|Local Storage Operator
|Supported
|Supported

|MetalLB Operator
|Supported
|Supported

|Network Observability Operator
|Supported
|Supported

|NFD Operator
|Supported
|Supported

|NMState Operator
|Supported
|Supported

|OpenShift Elasticsearch Operator
|Supported
|Supported

|Vertical Pod Autoscaler Operator
|Supported
|Supported
|====

.Persistent storage options
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}
|Persistent storage using iSCSI
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using local volumes (LSO)
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using hostPath
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Fibre Channel
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Raw Block
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using EDEV/FBA
|Supported ^[1]^
|Supported ^[1]^,^[2]^
|====
[.small]
--
1. Persistent shared storage must be provisioned by using either {rh-storage-first} or other supported storage protocols.
2. Persistent non-shared storage must be provisioned by using local storage, such as iSCSI, FC, or by using LSO with DASD, FCP, or EDEV/FBA.
--

[id="ocp-release-notes-insights-operator-enhancements_{context}"]
=== Insights Operator


[id="ocp-release-notes-insights-operator-runtime-extractor_{context}"]
==== Insights Runtime Extractor is generally available

In {product-title} 4.18, the Insights Operator introduced the _Insights Runtime Extractor_ workload data collection feature as a Technology Preview feature to help Red{nbsp}Hat better understand the workload of your containers.
Now, in version 4.19, the feature is generally available.
The Insights Runtime Extractor feature gathers runtime workload data and sends it to Red{nbsp}Hat.

[id="ocp-release-notes-installation-and-update_{context}"]
=== Installation and update

[id="ocp-4-19-installation-and-update-remove-terraform-ibm-cloud_{context}"]
==== Cluster API replaces Terraform on {ibm-cloud-title} installations

In {product-title} {product-version}, the installation program uses the Cluster API instead of Terraform to provision cluster infrastructure during installations on {ibm-cloud-title}.

[id="ocp-release-notes-installation-azure-encrypted-vnet_{context}"]
==== Installing a cluster on {azure-full} with virtual network encryption

With this release, you can install a cluster on {azure-short} using encrypted virtual networks. You are required to use {azure-short} virtual machines that have the `premiumIO` parameter set to `true`. See Microsoft's documentation about link:https://learn.microsoft.com/en-us/azure/virtual-network/how-to-create-encryption?tabs=portal[Creating a virtual network with encryption] and link:https://learn.microsoft.com/en-us/azure/virtual-network/virtual-network-encryption-overview#requirements[Requirements and Limitations] for more information.

[id="ocp-4-19-installation-and-update-aws-malaysia-thailand_{context}"]
==== Installing a cluster on {aws-short} in the Malaysia and Thailand regions

You can now install an {product-title} cluster on {aws-first} in the Malaysia (`ap-southeast-5`) and Thailand (`ap-southeast-7`) regions.

For more information, see xref:../installing/installing_aws/installing-aws-account.adoc#installation-aws-regions_installing-aws-account[Supported {aws-first} regions].

[id="ocp-4-19-installation-and-update-remove-terraform-azure-stack-hub_{context}"]
==== Cluster API replaces Terraform on {azure-first} Stack Hub installations

In {product-title} {product-version}, the installation program uses the Cluster API instead of Terraform to provision clusters during installer-provisioned infrastructure installations on {azure-first} Stack Hub.

[id="ocp-4-19-installation-and-update-support-azure-instance-types_{context}"]
==== Support added for additional {azure-first} instance types

Additional {azure-first} instance types for machine types based on 64-bit x86 architecture have been tested with {product-title} {product-version}.

For the Dxv6 machine series, the following instance types have been tested:

* `StandardDdsv6Family`
* `StandardDldsv6Family`
* `StandardDlsv6Family`
* `StandardDsv6Family`

For the Lsv4 and Lasv4 machine series, the following instance types have been tested:

* `standardLasv4Family`
* `standardLsv4Family`

For the ND and NV machine series, the following instance types have been tested:

* `StandardNVadsV710v5Family`
* `Standard NDASv4_A100 Family`

For more information, see xref:../installing/installing_azure/ipi/installing-azure-network-customizations.adoc#installation-azure-tested-machine-types_installing-azure-network-customizations[Tested instance types for {azure-short}] and link:https://learn.microsoft.com/en-us/azure/?product=popular[{azure-short} documentation] (Microsoft documentation).

[id="ocp-4-19-installation-and-update-azure-outbound-access-vms_{context}"]
==== Outbound access for VMs in {azure-first} will be retired

On 30 September 2025, the default outbound access connectivity for all new virtual machines (VMs) in {azure-first} will be retired. To enhance security, {azure-short} is moving towards a secure-by-default model where default outbound access to the internet will be turned off. However, configuration changes to {product-title} are not required. By default, the installation program creates an outbound rule for the load balancer.

For more information, see link:https://azure.microsoft.com/en-us/updates?id=default-outbound-access-for-vms-in-azure-will-be-retired-updates-and-more-information[Azure Updates] (Microsoft documentation), link:https://learn.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections#scenarios[Azure's outbound connectivity methods] (Microsoft documentation), and xref:../installing/installing_azure/upi/installing-azure-preparing-upi.adoc#installing-azure-preparing-upi[Preparing to install a cluster on {azure-short}].

[id="ocp-4-19-installation-and-update-gcp-confidential-computing-expansion_{context}"]
==== Additional Confidential Computing platforms for {gcp-short}
With this release, you can use additional Confidential Computing platforms on {gcp-short}. The new supported platforms, which can be enabled in the `install-config.yaml` file prior to installation, or configured after installation by using machine sets and control plane machine sets, are as follows:

* `AMDEncryptedVirtualization`, which enables Confidential Computing with AMD Secure Encrypted Virtualization (AMD SEV)
* `AMDEncryptedVirtualizationNestedPaging`, which enables Confidential Computing with AMD Secure Encrypted Virtualization Secure Nested Paging (AMD SEV-SNP)
* `IntelTrustedDomainExtensions`, which enables Confidential Computing with Intel Trusted Domain Extensions (Intel TDX)

For more information, see xref:../installing/installing_gcp/installation-config-parameters-gcp.adoc#installation-config-parameters-additional-gcp_installation-config-parameters-gcp[Installation configuration parameters for {gcp-full}], xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-gcp.adoc#machineset-gcp-confidential-vm_cpmso-config-options-gcp[Configuring Confidential VM by using machine sets (control plane)], and xref:../machine_management/creating_machinesets/creating-machineset-gcp.html#machineset-gcp-confidential-vm_creating-machineset-gcp[Configuring Confidential VM by using machine sets (compute)].

[id="ocp-4-19-GCP-custom-dns_{context}"]
==== Installing a cluster on {gcp-first} with a user-provisioned DNS (Technology Preview)

With this release, you can enable a user-provisioned domain name server (DNS) instead of the default cluster-provisioned DNS solution. For example, your organization's security policies might not allow the use of public DNS services such as Google Cloud DNS. You can manage your DNS only for the IP addresses of the API and Ingress servers. If you use this feature, you must provide your own DNS solution that includes records for `api.<cluster_name>.<base_domain>.` and `*.apps.<cluster_name>.<base_domain>.`. Enabling a user-provisioned DNS is available as a Technology Preview feature.

For more information, see xref:../installing/installing_gcp/installing-gcp-customizations.adoc#installation-gcp-enabling-user-managed-DNS_installing-gcp-customizations[Enabling user-managed DNS].

[id="ocp-4-19-installation-and-update-vsphere-multidisk_{context}"]
==== Installing a cluster on {vmw-first} with multiple disks (Technology Preview)
With this release, you can install a cluster on {vmw-first} with multiple storage disks as a Technology Preview feature. You can assign these additional disks to special functions within the cluster, such as etcd storage.

For more information, see xref:../installing/installing_vsphere/installation-config-parameters-vsphere.adoc#installation-configuration-parameters-optional-vsphere_installation-config-parameters-vsphere[Optional vSphere configuration parameters].

[id="ocp-4-19-installation-and-update-azure-boot-diagnostics_{context}"]
==== Enabling boot diagnostics collection during installation on {azure-first}

With this release, you can enable boot diagnostics collection when you install a cluster on {azure-first}. Boot diagnostics is a debugging feature for {azure-short} virtual machines (VMs) to identify VM boot failures. You can set the `bootDiagnostics` parameter in the `install-config.yaml` file for compute machines, for control plane machines, or for all machines.

For more information, see xref:../installing/installing_azure/installation-config-parameters-azure.adoc#installation-configuration-parameters-additional-azure_installation-config-parameters-azure[Additional {azure-short} configuration parameters].

[id="ocp-4-19-admin-ack-updating_{context}"]
==== Required administrator acknowledgment when updating from {product-title} 4.18 to 4.19

{product-title} 4.19 uses Kubernetes 1.32, which removed several xref:../release_notes/ocp-4-19-release-notes.adoc#ocp-4-19-removed-kube-1-32-apis_{context}[deprecated APIs].

A cluster administrator must provide manual acknowledgment before the cluster can be updated from {product-title} 4.18 to 4.19. This is to help prevent issues after updating to {product-title} 4.19, where APIs that have been removed are still in use by workloads, tools, or other components running on or interacting with the cluster. Administrators must evaluate their cluster for any APIs in use that will be removed and migrate the affected components to use the appropriate new API version. After this is done, the administrator can provide the administrator acknowledgment.

All {product-title} 4.18 clusters require this administrator acknowledgment before they can be updated to {product-title} 4.19.

For more information, see xref:../updating/preparing_for_updates/updating-cluster-prepare.adoc#updating-cluster-prepare[Preparing to update to {product-title} 4.19].

[id="ocp-4-19-vsphere-host-groups_{context}"]
==== OpenShift zones support for vSphere host groups (Technology Preview)

With this release, you can map {product-title} failure domains to {vmw-full} host groups. This enables you to make use of the high availability offered by a {vmw-short} stretched cluster configuration. This feature is available as a Technology Preview in {product-title} {product-version}.

For information on configuring host groups at installation, see xref:../installing/installing_vsphere/ipi/installing-vsphere-installer-provisioned-customizations.adoc#installation-vsphere-regions-zones-host-groups_installing-vsphere-installer-provisioned-customizations[VMware vSphere host group enablement].

For information on configuring host groups for existing clusters, see xref:../installing/installing_vsphere/post-install-vsphere-zones-regions-configuration.adoc#specifying-host-groups-vsphere_post-install-vsphere-zones-regions-configuration[Specifying multiple host groups for your cluster on vSphere].

[id="ocp-release-notes-agent-nutanix_{context}"]
==== Nutanix support for the Agent-based Installer
With this release, you can now use the Agent-based Installer to install a cluster on Nutanix.
Installing a cluster on Nutanix with the Agent-based Installer is enabled by setting the `platform` parameter to `nutanix` in the `install-config.yaml` file.

For more information, see xref:../installing/installing_with_agent_based_installer/installation-config-parameters-agent.adoc#installation-configuration-parameters-required_installation-config-parameters-agent[Required configuration parameters] in the Agent-based Installer documentation.

[id="ocp-release-notes-vcf-9-support_{context}"]
==== Support for {vmw-full} Foundation 9 and VMware Cloud Foundation 9

You can now install {product-title} on {vmw-full} Foundation (VVF) 9 and VMware Cloud Foundation (VCF) 9.

[NOTE]
====
The following additional VCF and VVF components are outside the scope of Red Hat support:

* Management: VCF Operations, VCF Automation, VCF Fleet Management, and VCF Identity Broker.
* Networking: VMware NSX Container Plugin (NCP).
* Migration: VMware HCX.
====

[id="ocp-release-notes-machine-config-operator_{context}"]
=== Machine Config Operator

[id="ocp-release-notes-machine-config-operator-naming_{context}"]
==== New naming for features

_{op-system-first} image layering_ is now called _image mode for OpenShift_. As a part of this change, _on-cluster layering_ is now  called _on-cluster image mode_ and _out-of-cluster layering_ is now _out-of-cluster image mode_.

The _updated boot images_ feature is now called _boot image management_.

[id="ocp-release-notes-machine-config-operator-ocl-ga_{context}"]
==== Image mode for OpenShift is now generally available

Image mode for OpenShift, formerly called on-cluster layering, is now Generally Available (GA). The following changes have been introduced with the promotion to GA:

* The API version is now `machineconfiguration.openshift.io/v1`. The new version includes the following changes:
** The `baseImagePullSecret` parameter is now optional. If not specified, the default `global-pull-secret-copy` is used.
** The `buildInputs` parameter is no longer required. All parameters previously under the `buildInputs` parameter are promoted one level.
** The `containerfileArch` parameter now supports multiple architectures. Previously, only `noarch` was supported.
** The required `imageBuilderType` is now `Job`. Previously, the required builder was `PodImageBuilder`.
** The `renderedImagePushspec` parameter is now `renderedImagePushSpec`.
** The `buildOutputs` and `currentImagePullSecret` parameters are no longer required.

* The output of the `oc describe MachineOSConfig` and `oc describe MachineOSBuild` commands have multiple differences.

* The `global-pull-secret-copy` is automatically added to the `openshift-machine-config-operator` namespace.

* You can now revert an on-cluster custom layered image back to the base image by removing a label from the `MachineOSConfig` object

* You can now automatically delete an on-cluster custom layered image by deleting the associated `MachineOSBuild` object.

* The `must-gather` for the Machine Config Operator now includes data on the `MachineOSConfig` and `MachineOSBuild` objects.

* On-cluster layering is now supported in disconnected environments.

* On-cluster layering is now supported in single node OpenShift (SNO) clusters.

[id="ocp-release-notes-machine-config-operator-boot-image_{context}"]
==== Boot image management is now default for {gcp-first} and {aws-first}

The boot image management feature, previously called updated boot images, is now the default behavior in {gcp-first} and {aws-first} clusters. As such, after updating to {product-title} {product-version}, the boot images in your cluster are automatically updated to version {product-version}. With subsequent updates, the Machine Config Operator (MCO) again updates the boot images in your cluster. A boot images is associated with a machine set and is used when scaling new nodes. Any new nodes you create after updating are based on the new version. Current nodes are not affected by this feature.

Before upgrading to {product-version}, you must opt-out of this default behavior or acknowledge this change before proceeding. For more information, see xref:../machine_configuration/mco-update-boot-images.adoc#mco-update-boot-images-disable_machine-configs-configure[Disabling boot image management].

[NOTE]
====
The managed boot images feature is available for only {gcp-short} and {aws-short} clusters. For all other platforms, the MCO does not update the boot image with each cluster update.
====

[id="ocp-release-notes-machine-config-operator-cert-changes_{context}"]
==== Changes to the Machine Config Operator certificates
The Machine Config Server (MCS) CA bundle created by the installation program is now stored in the `machine-config-server-ca` config map in the `openshift-machine-config-operator` namespace. The bundle was previously stored in the `root-ca` configmap in the `kube-system namespace`. The `root-ca` configmap is no longer used in a cluster that cluster that is updated to {product-title} {product-version}. This change was made to make it clear that this CA bundle is managed by the Machine Config Operator (MCO).

The MCS signing key is stored in the `machine-config-server-ca` secret in the `openshift-machine-config-operator` namespace.

The MCS CA and MCS cert are valid for 10 years and are automatically rotated by the MCO at approximately 8 years. Upon update to {product-title} {product-version}, the CA signing key is not present. As a result, the CA bundle is immediately considered expired when the MCO certificate controller comes up. This expiration causes an immediate certificate rotation, even if the cluster is not 10 years old. After that point, the next rotation takes place at the standard 8 year period.

[NOTE]
====
This automatic certificate rotation applies only to clusters that use machine sets. For clusters that do not use machine sets, such as vSphere user-provisioned infrastructure clusters, you are required to manually rotate these certificates. For more information on manual certificate rotation, see the Red{nbsp}Hat Knowledgebase article link:https://access.redhat.com/articles/regenerating_cluster_certificates#regenerating-ca-certificates-for-the-machine-config-server-5[Regenerating CA certificates for the Machine Config Server].
====

For more information about the MCO certificates, see xref:../security/certificate_types_descriptions/machine-config-operator-certificates.adoc#cert-types-machine-config-operator-certificates[Machine Config Operator certificates].

[id="ocp-release-notes-machine-management_{context}"]
=== Machine management

[id="ocp-4-19-capi-mapi-migration_{context}"]
==== Migrating resources between the Cluster API and the Machine API (Technology Preview)

With this release, you can migrate some resources between the Cluster API and the Machine API on {aws-first} as a Technology Preview feature.
For more information, see xref:../machine_management/cluster_api_machine_management/cluster-api-getting-started.adoc#capi-mapi-migration-overview_cluster-api-getting-started[Migrating Machine API resources to Cluster API resources].

To support this capability, the {product-title} Cluster API documentation now includes additional configuration details for xref:../machine_management/cluster_api_machine_management/cluster_api_provider_configurations/cluster-api-config-options-aws.html#cluster-api-supported-features-aws_cluster-api-config-options-aws[{aws-short}] clusters.

[id="ocp-4-19-cpms-prefix_{context}"]
==== Custom prefixes for control plane machine names

With this release, you can customize the prefix of machine names for machines created by the control plane machine set.
This feature is enabled by modifying the `spec.machineNamePrefix` parameter of the `ControlPlaneMachineSet` custom resource.

For more information, see xref:../machine_management/control_plane_machine_management/cpmso-configuration.adoc#cpmso-config-prefix_cpmso-configuration[Adding a custom prefix to control plane machine names].

[id="ocp-4-19-aws-capacity-reservations_{context}"]
==== Configuring Capacity Reservations on {aws-full} clusters

With this release, you can deploy machines that use Capacity Reservations, including On-Demand Capacity Reservations and Capacity Blocks for ML, on {aws-full} clusters.

You can configure these features with xref:../machine_management/creating_machinesets/creating-machineset-aws.adoc#machineset-capacity-reservation_creating-machineset-aws[compute] and xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-aws.adoc#machineset-capacity-reservation_cpmso-config-options-aws[control plane] machine sets.

[id="ocp-4-19-vmw-multi-disk_{context}"]
==== Support for multiple {vmw-full} data disks (Technology Preview)

With this release, you can add up to 29 disks to the virtual machine (VM) controller for your {vmw-short} cluster as a Technology Preview feature.
This capability is available for xref:../machine_management/creating_machinesets/creating-machineset-vsphere.adoc#machineset-vsphere-data-disks_creating-machineset-vsphere[compute] and xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-vsphere.adoc#machineset-vsphere-data-disks_cpmso-config-options-vsphere[control plane] machine sets.

[id="ocp-release-notes-monitoring_{context}"]
=== Monitoring

The in-cluster monitoring stack for this release includes the following new and modified features:

[id="ocp-4-19-monitoring-updates-to-monitoring-stack-components-and-dependencies"]
==== Updates to monitoring stack components and dependencies

This release includes the following version updates for in-cluster monitoring stack components and dependencies:

* Alertmanager to 0.28.1
* Prometheus to 3.2.1
* Prometheus Operator to 0.81.0
* Thanos to 0.37.2
* kube-state-metrics to 2.15.0
* node-exporter to 1.9.1

[id="ocp-4-19-monitoring-changes-to-alerting-rules"]
==== Changes to alerting rules

[NOTE]
====
Red{nbsp}Hat does not guarantee backward compatibility for recording rules or alerting rules.
====

* Added the `PrometheusPossibleNarrowSelectors` alert to warn users when PromQL queries or metric relabel configurations use selectors that could be too restrictive and might not take into account that values on the `le` label of classic histograms or the `quantile` label of summaries are floats in Prometheus v3. For more information, see the "Prometheus v3 upgrade" section.

[id="ocp-4-19-monitoring-prometheus-v3-upgrade"]
==== Prometheus v3 upgrade

This release introduces a major update to the Prometheus component, transitioning from v2 to v3. The monitoring stack and other core components include all of the necessary adjustments to ensure a smooth upgrade. However, some user-managed configurations might require modifications. The key changes include the following items:

* The values of the `le` label for classic histograms and the `quantile` label for summaries are normalized during ingestion. For example, the `example_bucket{le="10"}` metric selector is ingested as `example_bucket{le="10.0"}`. As a result, alerts, recording rules, dashboards, and relabeling configurations that reference label values as whole numbers, for example, `le="10"`, might no longer work as intended.
+
To mitigate the issue, update your selectors:

** If your queries need to cover data from both before and after the Prometheus upgrade, ensure both values are considered, for example, use a regular expression, `example_bucket{le=~"10(.0)?"}`.

** For queries that only cover data after the upgrade, use float values, for example, `le="10.0"`.

* Configurations that send alerts to additional Alertmanager instances through `additionalAlertmanagerConfigs` by using the Alertmanager v1 API are no longer supported.
+
To mitigate the issue, upgrade any affected Alertmanager instances to support the v2 API, which is supported since Alertmanager `v0.16.0`, and update your monitoring configuration to use the v2 scheme.

For more information about the changes between Prometheus v2 and v3, see link:https://prometheus.io/docs/prometheus/latest/migration/[Prometheus 3.0 migration guide].

[id="ocp-4-19-monitoring-metrics-collection-profiles-ga"]
==== Metrics collection profiles is generally available

{product-title} 4.13 introduced the ability to set a metrics collection profile for default platform monitoring to collect either the default amount of metrics data or a minimal amount of metrics data. In {product-title} {product-version}, metrics collection profiles are now generally available.

For more information, see link:https://docs.redhat.com/en/documentation/monitoring_stack_for_red_hat_openshift/4.19/html/monitoring_key_concepts/key-concepts#configuring-metrics-collection-profiles_key-concepts[About metrics collection profiles] and link:https://docs.redhat.com/en/documentation/monitoring_stack_for_red_hat_openshift/4.19/html/configuring_core_platform_monitoring/configuring-performance-and-scalability#choosing-a-metrics-collection-profile_configuring-performance-and-scalability[Choosing a metrics collection profile].

[id="ocp-4-19-monitoring-added-cluster-proxy-support-for-external-alertmanager-instances"]
==== Added cluster proxy support for external Alertmanager instances

With this release, external Alertmanager instances now use the cluster-wide HTTP proxy settings for communication. The {cmo-first} reads the cluster-wide proxy settings and configures the appropriate proxy URL for the Alertmanager endpoints.

[id="ocp-4-19-monitoring-strict-validation-for-cmo-is-improved"]
==== Strict validation for the {cmo-full} is improved

With this release, the strict validation introduced in {product-title} 4.18 is improved. Error messages now clearly identify the affected field, and validation is case-sensitive to ensure more accurate and consistent configuration.

For more information, see (link:https://issues.redhat.com/browse/OCPBUGS-42671[OCPBUGS-42671]) and (link:https://issues.redhat.com/browse/OCPBUGS-54516[OCPBUGS-54516]).

[id="ocp-release-notes-networking_{context}"]
=== Networking

[id="ocp-4-19-support-for-route-advertisements-cudns-with-bgp_{context}"]
==== Support for route advertisements for cluster user-defined networks (CUDNs) with Border Gateway Protocol (BGP)

With route advertisements enabled, the OVN-Kubernetes network plugin supports the direct advertisement of routes for pods and services associated with cluster user-defined networks (CUDNs) to the provider network. This feature enables some of the following benefits:

- Learns routes to pods dynamically
- Advertises routes dynamically
- Enables layer 3 notifications of EgressIP failovers in addition to the layer 2 ones based on gratuitous ARPs.
- Supports external route reflectors, which reduces the number of BGP connections required in large networks

For more information, see xref:../networking/advanced_networking/route_advertisements/about-route-advertisements.adoc#about-route-advertisements[About route advertisements].

[id="ocp-4-19-networking-support-load-secrets_{context}"]
==== Creating a route with externally managed certificate (General Availability)

With this release, {product-title} routes can be configured with third-party certificate management solutions, utilizing the `.spec.tls.externalCertificate` field in the route API. This allows you to reference externally managed TLS certificates through secrets, streamlining the process by eliminating manual certificate management. By using externally managed certificates, you reduce errors, ensure a smoother certificate update process, and enable the OpenShift router to promptly serve renewed certificates. For more information, see xref:../networking/ingress_load_balancing/routes/creating-advanced-routes.adoc#nw-ingress-route-secret-load-external-cert_creating-advanced-routes[Creating a route with externally managed certificate].

[id="ocp-4-19-support-for-bgp-routing-protocol_{context}"]
==== Support for the BGP routing protocol

The Cluster Network Operator (CNO) now supports enabling Border Gateway Protocol (BGP) routing. With BGP, you can import and export routes to the underlying provider network and use multi-homing, link redundancy, and fast convergence. BGP configuration is managed with the `FRRConfiguration` custom resource (CR).

When upgrading from an earlier version of {product-title} in which you installed the MetalLB Operator, you must manually migrate your custom frr-k8s configurations from the `metallb-system` namespace to the `openshift-frr-k8s` namespace. To move these CRs, enter the following commands:

. To create the `openshift-frr-k8s` namespace, enter the following command:
+
[source,terminal]
----
$ oc create namespace openshift-frr-k8s
----

. To automate the migration, create a `migrate.sh` file with the following content:
+
[source,bash]
----
#!/bin/bash
OLD_NAMESPACE="metallb-system"
NEW_NAMESPACE="openshift-frr-k8s"
FILTER_OUT="metallb-"
oc get frrconfigurations.frrk8s.metallb.io -n "${OLD_NAMESPACE}" -o json |\
  jq -r '.items[] | select(.metadata.name | test("'"${FILTER_OUT}"'") | not)' |\
  jq -r '.metadata.namespace = "'"${NEW_NAMESPACE}"'"' |\
  oc create -f -
----

. To run the migration script, enter the following command:
+
[source,terminal]
----
$ bash migrate.sh
----

. To verify that the migration succeeded, enter the following command:
+
[source,terminal]
----
$ oc get frrconfigurations.frrk8s.metallb.io -n openshift-frr-k8s
----

After the migration is complete, you can remove the `FRR-K8s` custom resources from the `metallb-system` namespace.

For more information, see xref:../networking/advanced_networking/bgp_routing/about-bgp-routing.adoc#about-bgp-routing[About BGP routing].

[id="ocp-4-19-networking-gateway-api-controller_{context}"]
==== Support for using Gateway API to configure cluster ingress traffic (General Availability)
With this release, support for managing ingress cluster traffic using Gateway API resources is Generally Available. Gateway API provides a robust networking solution within the transport layer, L4, and the application layer, L7, for {product-title} clusters using a standardized open source ecosystem.

For more information, see xref:../networking/ingress_load_balancing/configuring_ingress_cluster_traffic/ingress-gateway-api.adoc#ingress-gateway-api[Gateway API with {product-title} networking].

[IMPORTANT]
====
Gateway API resources must conform to the supported {product-title} API surface. This means you cannot use another vendor-specific resource, such as Istio's VirtualService, with {product-title}'s implementation of Gateway API. For more information, see xref:../networking/ingress_load_balancing/configuring_ingress_cluster_traffic/ingress-gateway-api.adoc#nw-ingress-gateway-api-implementation[Gateway API implementation for {product-title}].
====

[id="ocp-4-19-networking-gateway-api-crd-lifecycle_{context}"]
==== Support for managing Gateway API custom resource definition (CRD) lifecycle
With this release, {product-title} manages the lifecycle of Gateway API CRDs. This means that the Ingress Operator handles the required versioning and management of resources. Any Gateway API resources created in a previous {product-title} version must be re-created and redeployed so that it conforms to the specifications required by the Ingress Operator.

For more information, see xref:../updating/preparing_for_updates/updating-cluster-prepare.adoc#nw-ingress-gateway-api-manage-succession[Preparing for Gateway API management succession by the Ingress Operator].

[id="ocp-4-19-networking-gateway-api-ossm-version-bump_{context}"]
==== Updates to Gateway API custom resource definitions (CRDs)
{product-title} {product-version} updates {SMProductName} to version 3.0.2, and Gateway API to version 1.2.1. See the link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_mesh/3.0/html/release_notes/ossm-release-notes[{SMProductShortName} 3.0.0 release notes] and the link:https://github.com/kubernetes-sigs/gateway-api/blob/main/CHANGELOG/1.2-CHANGELOG.md#v121[Gateway API 1.2.1 changelog] for more information.

[id="ocp-4-19-networking-allocate-load-balancers-to-specific-subnets_{context}"]
==== Allocate API and ingress load balancers to specific subnets
With this release, you can now allocate load balancers to customize deployments when installing an {product-title} cluster on AWS. This feature ensures optimal traffic distribution, high application availability, uninterrupted service, and network segmentation.

For more information, see xref:../installing/installing_aws/installation-config-parameters-aws.adoc#installation-configuration-parameters-network_installation-config-parameters-aws[Installation configuration parameters on AWS] and xref:../networking/ingress_load_balancing/configuring_ingress_cluster_traffic/allocating-load-balancers.adoc#allocating-load-balancers[Allocating load balancers to specific subnets].

[id="ocp-4-19-networking-ptp-dual-oc_{context}"]
==== Dual-port NICs for improved redundancy in PTP ordinary clocks (Technology Preview)
With this release, you can use a dual-port network interface controller (NIC) to improve redundancy for Precision Time Protocol (PTP) ordinary clocks.
Available as a Technology Preview, in a dual-port NIC configuration for an ordinary clock, if one port fails, the standby port takes over, maintaining PTP timing synchronization.

[NOTE]
====
You can configure PTP ordinary clocks with added redundancy on `x86` architecture nodes with dual-port NICs only.
====

For more information, see xref:../networking/advanced_networking/ptp/about-ptp.adoc#ptp-dual-ports-oc_about-ptp[Using dual-port NICs to improve redundancy for PTP ordinary clocks].

[id="ocp-4-19-networking-conditional-webhook_{context}"]
==== Support for conditional webhook matching in the SR-IOV Network Operator

You can now enable the `featureGates.resourceInjectorMatchCondition` feature in the `SriovOperatorConfig` object to limit the scope of the Network Resources Injector webhook. If this feature is enabled, the webhook applies only to pods with the secondary network annotation `k8s.v1.cni.cncf.io/networks`.

If this feature is disabled, the webhooks `failurePolicy` is set to `Ignore` by default. This configuration can cause pods requesting SR-IOV networks to be deployed without the required resource injection if the webhook is unavailable. If this feature is enabled and the webhook is unavailable, pods without the annotation are still deployed, preventing unnecessary disruptions to other workloads.

For more information, see xref:../networking/networking_operators/sr-iov-operator/configuring-sriov-operator.adoc#about-network-resource-injector_configuring-sriov-operator[About the Network Resources Injector]

[id="ocp-4-19-dpu-device-management-with-dpu-operator_{context}"]
==== Enabling DPU device management with the DPU Operator
With this release, {product-title} introduces the Data Processing Unit (DPU) Operator and using the Operator to manage DPU devices. The DPU Operator manages components on compute nodes that have configured DPUs, such as enabling the offloading of data networking, storage, and security workloads. Enabling DPU device management leads to improved cluster performance, reduced latency, and enhanced security, that overall contribute to a more efficient cluster infrastructure. For more information, see xref:../networking/networking_operators/dpu-operator/about-dpu.adoc#dpu-operator[About DPU and the DPU Operator].

[id="ocp-4-19-cluster-user-defined-networks-localnet_{context}"]
==== Localnet topology for user-defined networks (Generally Available)

Administrators can now use the `ClusterUserDefinedNetwork` custom resource to deploy secondary networks on a `Localnet` topology. This feature allows pods and virtual machines connected to the localnet network to egress to the physical network. For more information, see xref:../networking/multiple_networks/primary_networks/about-user-defined-networks.adoc#nw-cudn-localnet[Creating a ClusterUserDefinedNetwork CR for a Localnet topology].

[id="ocp-4-19-port-isolation-linux-bridge_{context}"]
==== Enable port isolation for a Linux bridge NAD (Generally Available)

You can enable port isolation for a Linux bridge network attachment definition (NAD) so that virtual machines (VMs) or pods that run on the same virtual LAN (VLAN) can operate in isolation from one another. For more information, see xref:../virt/vm_networking/virt-connecting-vm-to-linux-bridge.adoc#virt-linux-bridge-nad-port-isolation_virt-connecting-vm-to-linux-bridge[Enabling port isolation for a Linux bridge NAD].

[id="ocp-4-19-whereabouts-ipam_{context}"]
==== Fast IPAM configuration for the Whereabouts IPAM CNI plugin (Technology Preview)

To improve the performance of Whereabouts, especially if nodes in your cluster run a high amount of pods, you can now enable the Fast IP Address Management (IPAM) feature. The Fast IPAM feature uses `nodeslicepools`, which are managed by the Whereabouts Controller, to optimize IP address allocation for nodes. For more information, see xref:../networking/multiple_networks/secondary_networks/configuring-ip-secondary-nwt.adoc#nw-multus-whereabouts-fast-ipam_configuring-additional-network[Fast IPAM configuration for the Whereabouts IPAM CNI plugin].
[id="ocp-4-19-metallb-unnumbered-bgp-peering_{context}"]

[id="ocp-4-19-unnumbered-bgp-peering_{context}"]
==== Unnumbered BGP peering (Technology Preview)

With this release, {product-title} introduces unnumbered BGP peering.
Available as a Technology Preview feature, you can use the `spec.interface` field of the BGP peer custom resource to configure unnumbered BGP peering.

[id="ocp-4-19-custom-dns-host-name-disconnected_{context}"]
==== Create a custom DNS host name to resolve DNS connectivity issues

In a disconnected environment where the external DNS server cannot be reached, you can resolve Kubernetes NMState Operator health probe issues by specifying a custom DNS host name in the `NMState` custom resource definition (CRD). For more information, see xref:../networking/k8s_nmstate/k8s-nmstate-troubleshooting-node-network.adoc#k8s-nmstate-troubleshooting-dns-disconnected-env-resolv_k8s-nmstate-troubleshooting-node-network[Creating a custom DNS host name to resolve DNS connectivity issues].

[id="ocp-4-19-ptp-fast-events-rest-api-v2_{context}"]
==== Removal of PTP events REST API v1 and events consumer application sidecar
With this release, the PTP events REST API v1 and events consumer application sidecar support are removed.

You must use the O-RAN compliant PTP events REST API v2 instead.

For more information, see xref:../networking/advanced_networking/ptp/ptp-cloud-events-consumer-dev-reference-v2.adoc#ptp-cloud-events-consumer-dev-reference-v2[Developing PTP event consumer applications with the REST API v2].

[id="ocp-4-19-route-external-cert-feature-gate_{context}"]
==== Re-add a previously deleted secret with `RouteExternalCertificate` feature gate enabled

If you enabled the `RouteExternalCertificate` feature gate for your cluster, you can now re-add a previously deleted secret. (link:https://issues.redhat.com/browse/OCPBUGS-33958[OCPBUGS-33958])

[id="ocp-release-notes-openshift-cli_{context}"]
=== OpenShift CLI (oc)

[id="ocp-release-notes-openshift-cli-sign-mirroring_{context}"]
==== Mirroring and verifying image signatures in oc-mirror plugin v2

Starting with {product-title} 4.19, the oc-mirror plugin v2 supports mirroring and verifying cosign tag-based signatures for container images.

[id="ocp-release-notes-osdk_{context}"]
=== Operator development

[id="ocp-release-notes-osdk-base-images_{context}"]
==== Supported Operator base images

The following base images for Operator projects are updated for compatibility with {product-title} {product-version}. The runtime functionality and configuration APIs for these base images are supported for bug fixes and for addressing CVEs.

* The base image for Ansible-based Operator projects
* The base image for Helm-based Operator projects

For more information, see link:https://access.redhat.com/node/7123119[Updating the base image for existing Ansible- or Helm-based Operator projects for {product-title} 4.19 and later] (Red{nbsp}Hat Knowledgebase).

// NOTE: The KCS article link will be published on the GA date. It is a draft right now.

[id="ocp-release-notes-postinstallation-configuration_{context}"]
=== Postinstallation configuration

[id=ocp-release-notes-postinstallation-configuration-using-bmaas_{context}]
==== Using bare metal as a service (Technology Preview)

In {product-title} {product-version}, you can deploy non-{product-title} nodes by using bare metal as a service (BMaaS). BMaaS nodes can run workloads that might not be suitable for containerization or virtualization. For example, workloads such as applications that require direct hardware access, conduct high-performance computing tasks or are legacy applications and operate independently of the cluster are suitable for deployment by using BMaaS.

For more information, see xref:../installing/installing_bare_metal/bare-metal-using-bare-metal-as-a-service.html#bmo-configuring-userdata_bare-metal-using-bmaas[Using bare metal as a service].

[id="ocp-release-notes-rhcos_{context}"]
=== {op-system-first}

[id="ocp-4-19-rhcos-rhel-9-6_{context}"]
==== {op-system} uses {op-system-base} 9.6
{op-system} uses {op-system-base-full} 9.6 packages in {product-title} {product-version}. These packages ensure that your {product-title} instances receive the latest fixes, features, enhancements, hardware support, and driver updates.

[id="ocp-release-notes-scalability-and-performance_{context}"]
=== Scalability and performance

[id="ocp-release-notes-scalability-and-performance_kernelpagesize_{context}"]
==== Performance profile kernel page size configuration

With this update, you can specify larger kernel page sizes to improve performance for memory-intensive, high-performance workloads on ARM infrastructure nodes with the realtime kernel disabled. For more information, see xref:../scalability_and_performance/cnf-tuning-low-latency-nodes-with-perf-profile.adoc#cnf-configuring-kernal-page-size_cnf-low-latency-perf-profile[Configuring kernel page sizes].

[id="ocp-release-notes-scalability-and-performance-cluster-compare-enhancements_{context}"]
==== Updates to the cluster-compare plugin

This release includes the following usability and functional updates to the `cluster-compare` plugin:

* Match capture groups more effectively: You can now match across and between templates more accurately with improved capture group handling.
* Generate JUnit output: You can use the `-o junit` flag to output results in `junit` format, making it easier to integrate with testing or CI/CD systems.
* `sprig` function support: The `cluster-compare` plugin supports all `sprig` library functions, except for the `env` and `expandenv` functions. For the full list of sprig library functions, see link:https://masterminds.github.io/sprig/[Sprig Function Documentation].

For a complete list of available template functions, see xref:../scalability_and_performance/cluster-compare/creating-a-reference-configuration.adoc#cluster-compare-templating-reference_creating-a-reference-configuration[Reference template functions]

[id="ocp-release-notes-tuning-hcp-performance-profile_{context}"]
==== Tuning {hcp} using a performance profile

With this update, you can now tune nodes in {hcp} for low latency by applying a performance profile. For more information, see xref:../scalability_and_performance/cnf-tuning-low-latency-hosted-cp-nodes-with-perf-profile.adoc#cnf-create-performance-profiles-hosted-cp[Creating a performance profile for hosted control planes].

[id="ocp-release-notes-security_{context}"]
=== Security

[id="ocp-release-notes-tls-modern-profile-control-plane_{context}"]
==== Control plane now supports TLS 1.3 and the Modern TLS security profile

With this release, the control plane supports TLS 1.3. You can now use the `Modern` TLS security profile for the control plane.

For more information, see xref:../security/tls-security-profiles.adoc#tls-profiles-kubernetes-configuring_tls-security-profiles[Configuring the TLS security profile for the control plane].

[id="ocp-release-notes-eso-operator_{context}"]
==== The {external-secrets-operator} (Technology Preview)
With this release, you can use the {external-secrets-operator} to authenticate with the external secrets store, retrieve secrets, and inject the retrieved secrets into a native Kubernetes secret. The {external-secrets-operator} is available as a Technology Preview.

For more information, see xref:../security/external_secrets_operator/index.adoc#index_external-secrets-operator-about[External Secrets Operator for Red Hat OpenShift overview]

[id="ocp-release-notes-storage_{context}"]
=== Storage

[id="ocp-release-notes-sscsi-disconnected-environment-support_{context}"]
==== Support for the Secrets Store CSI driver in disconnected environments
With this release, the secrets store providers support by using the {secrets-store-driver} in disconnected clusters.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-secrets-store.adoc#persistent-storage-csi-secrets-store-disconnect-environment_persistent-storage-csi-secrets-store[Support for disconnected environments].

[id="ocp-release-notes-storage-azure-file-cross-sub-support_{context}"]
==== Azure File cross-subscription support is generally available
Cross-subscription support allows you to have an {product-title} cluster in one Azure subscription and mount your Azure file share in another Azure subscription using the Azure File Container Storage Interface (CSI) driver. The subscriptions must be in the same tenant.

This feature is generally available in {product-title} 4.19.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-aws-efs.adoc#persistent-storage-csi-efs-cross-account_persistent-storage-csi-aws-efs[AWS EFS CSI cross account support].

[id="ocp-release-notes-storage-vol-attributes_{context}"]
==== Volume Attributes Classes (Technology Preview)
Volume Attributes Classes provide a way for administrators to describe "classes" of storage they offer. Different classes might correspond to different quality-of-service levels.

Volume Attributes Classes in {product-title} 4.19 is available only with AWS Elastic Block Storage (EBS) and Google Cloud Platform (GCP) persistent disk (PD) Container Storage Interface (CSI).

You can apply a Volume Attributes Classes to a persistent volume claim (PVC). If a new Volume Attributes Class becomes available in the cluster, you can update the PVC with the new Volume Attributes Classes if needed.

Volume Attributes Classes have parameters that describe volumes belonging to them. If a parameter is omitted, the default is used at volume provisioning. If a user applies the PVC with a different Volume Attributes Class with omitted parameters, the default value of the parameters might be used depending on the CSI driver implementation. For more information, see the related CSI driver documentation.

Volume Attributes Classes is available in {product-title} 4.19 with Technology Preview status.

For more information, see xref:..//storage/understanding-persistent-storage.adoc#storage-persistent-storage-pvc-volumeattributesclass_understanding-persistent-storage[Volume Attributes Classes].

[id="ocp-release-notes-storage-cli-cmd-pvc-usage_{context}"]
==== New CLI command to show PVC usage (Technology Preview)
{product-title} 4.19 introduces a new command to view persistent volume claim usage. This feature has Technology Preview status.

For more information, see xref:../storage/understanding-persistent-storage.adoc#pvc-cli-command-usage_understanding-persistent-storage[Viewing PVC usage statistics].

[id="ocp-release-notes-storage-cli-cmd-resize-recovery_{context}"]
==== CSI volume resizing recovery is generally available

Previously, you might expand a persistent volume claim (PVC) to a size that is not supported by the underlying storage provider. In this case, the expansion controller typically tries forever to expand the volume and keeps failing.

This new feature allows you to recover and provide another resize value for the PVC. Resizing recovery is supported as generally available in {product-title} 4.19.

For more information about resizing volumes, see xref:../storage/expanding-persistent-volumes.adoc[Expanding persistent volumes].

For more information about recovering when resizing volumes, see xref:../storage/expanding-persistent-volumes.adoc#expanding-recovering-from-failure_expanding-persistent-volumes[Recovering from failure when expanding volumes].

[id="ocp-release-notes-storage-resize-migrated-vsphere-in-tree-vols_{context}"]
==== Support for resizing vSphere in-tree migrated volumes is generally available
Previously, VMware vSphere persistent volumes that were migrated from in-tree to Container Storage Interface (CSI) could not be resized. With {product-title} 4.19, resizing migrated volumes is supported. This feature is generally available.

For more information about resizing volumes, see xref:../storage/expanding-persistent-volumes.adoc[Expanding persistent volumes].

[id="ocp-release-notes-storage-disable-vsphere_{context}"]
==== Disabling and enabling storage on vSphere is generally available
Cluster administrators might want to disable the VMware vSphere Container Storage Interface (CSI) Driver as a Day 2 operation, so the vSphere CSI Driver does not interface with your vSphere setup.

This features was introduced in {product-title} 4.17 with Technology Preview status. This feature is now supported as generally available in {product-title} 4.19.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere-disable-storage-overview_persistent-storage-csi-vsphere[Disabling and enabling storage on vSphere].

[id="ocp-release-notes-storage-increase-max-vols-per-node-vsphere"]
==== Increasing the maximum number of volumes per node for vSphere (Technology Preview)
For VMware vSphere version 7, {product-title} restricts the maximum number of volumes per node to 59.

However, with {product-title} 4.19 for vSphere version 8 or later, you can increase the allowable number of volumes per node to a maximum of 255. Otherwise, the default value remains at 59.

This feature has Technology Preview status.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere-increase-max-vols-per-node-overview_persistent-storage-csi-vsphere[Increasing maximum volumes per node for vSphere].

[id="ocp-release-notes-storage-vsphere-migrating-cns-vols-between-datastores_{context}"]
==== Migrating CNS volumes between datastores for vSphere is fully supported
If you are running out of space in your current datastore, or want to move to a more performant datastore, you can migrate VMware vSphere Cloud Native Storage (CNS) volumes between datastores. This applies to both attached and detached volumes.

{product-title} now fully supports migration of CNS volume using the vCenter UI. Migrated volumes should work
as expected and should not result in non-functional persistent volumes. CNS volumes can also be migrated while in use by pods.

This feature was introduced as a Development Preview in {product-title} 4.17, but is now fully supported  in 4.19.

Migrating CNS volumes between datastores requires VMware vSphere 8.0.2 or later or vSphere 7.0 Update 3o or later.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere-migrating-cns-vols-between-datastores_persistent-storage-csi-vsphere[Migrating CNS volumes between datastores for vSphere].

[id="ocp-release-notes-storage-nfs-export-options-filestore"]
==== NFS export options for Filestore storage class is generally available.
By default, a Filestore instance grants root level read/write access to all clients that share the same Google Cloud project and virtual private cloud (VPC) network. Network File System (NFS) export options can limit this access to certain IP ranges and specific user/group IDs for the Filestore instance. When creating a storage class, you can set these options using the `nfs-export-options-on-create` parameter.

NFS export options is supported as generally available in {product-title} 4.19.

For more information, see xref:..//storage/container_storage_interface/persistent-storage-csi-google-cloud-file.adoc#persistent-storage-csi-gcp-filestore-nfs-export-options_persistent-storage-csi-google-cloud-file[NFS export options].

[id="ocp-release-notes-web-console_{context}"]
=== Web console

Starting with {product-title} 4.19, the perspectives in the web console have unified to simplify navigation, reduce context switching, streamline tasks, and provide users with a more cohesive {product-title} experience.

With this unified design, there is no longer a *Developer* perspective in the default view; however, _all_ {product-title} web console features are discoverable to all users. If you are not the cluster owner, you might need to request permission for certain features from the cluster owner. The *Developer* perspective can still be manually enabled if you prefer.

The *Getting Started* pane in the web console provides resources such as, a tour of the console, information on setting up your cluster, a quick start for enabling the *Developer* perspective, and links to explore new features and capabilities.

//See xref:../web_console/web-console-overview#enabling-developer-perspective_web-console_web-console-overview for more information on enabling the *Developer* perspective.
//https://github.com/openshift/openshift-docs/pull/93644

[id="ocp-release-notes-patternfly-6-upgrade_{context}"]
==== Patternfly 6 upgrade

The web console now uses Patternfly 6. Support for Patternfly 4 in the web console is no longer available.


This release also introduces the following updates to the web console. You can now do the following actions:

* Specify distinct console logos for both light and dark themes using the `logos` field in the `.spec.customization.logos` configuration, allowing for more comprehensive branding.
* Easily delete identity providers (IDPs) directly from the web console, streamlining authentication configuration without manual YAML file edits.
* Easily set the default `StorageClass` directly in the web console.
* Quickly find specific jobs in the web console by sorting the *Created* column by creation date and time.

[id="ocp-4-19-notable-technical-changes_{context}"]
== Notable technical changes

[id="ocp-4-19-notable-technical-changes-readonlyrootfilesystem_{context}"]
=== Pods deploy with readOnlyRootFilesystem set to true

With this release, Cloud Credential Operator pods now deploy with the `readOnlyRootFilesystem` security context setting set to `true`. This enhances security by ensuring that the container root file system is mounted as read-only.

[id="ocp-4-19-notable-technical-changes-loopback-cert_{context}"]
=== Extended loopback certificate validity to three years for kube-apiserver

Previously, the self-signed loopback certificate for the Kubernetes API Server expired after one year. With this release, the expiration date of the certificate is extended to three years.

[id="ocp-release-notes-readiness-probes-etcd_{context}"]
=== Readiness probes exclude etcd checks

The readiness probes for the API server have been modified to exclude etcd checks. This prevents client connections from being closed if etcd is temporarily unavailable. This means that client connections persist through brief etcd unavailability and minimizes temporary API server outages.

[id="ocp-4-19-notable-technical-changes-remove-cns_{context}"]
=== Installer automatically removes leftover Cloud Native Storage (CNS) volumes

The OpenShift installation program now automatically detects and removes leftover persistent storage volumes on {vmw-full} when you delete a cluster. This prevents orphaned volumes from consuming disk space and creating unnecessary alerts in vCenter.

[id="ocp-4-19-rhcos-split-layers_{context}"]
=== {op-system-first} versioning uses {op-system-base-full} instead of {product-title}

As part of aligning with Image Mode for {op-system-base}, {op-system} is now built as a layer on top of a shared {op-system-base} base image. The most noticeable change for users is around versioning. For example, `VERSION_ID` in `/etc/os-release` now reflects the version of {op-system-base}, such as {op-system-base} 9.6, rather than the version of {product-title}, such as {product-title} {product-version}. This version change might show up in other places, such as in the output of the command `rpm-ostree status`, or in boot loader entries. `OPENSHIFT_VERSION` in `/etc/os-release` on the node image still uses the version of {product-title} and is unaffected by this change.

Because {op-system} now uses the {op-system-base} base image, any new operating system features added to that base image will usually be inherited by all {product-title} releases that use this base image.

[id="vmw-7-vcf-4-eogs_{context}"]
=== {vmw-full} 7 and VMware Cloud Foundation 4 end of general support

Broadcom has ended general support for {vmw-full} 7 and VMware Cloud Foundation (VCF) 4. If your existing {product-title} cluster is running on either of these platforms, you must plan to migrate or upgrade your VMware infrastructure to a supported version. {product-title} supports installation on {vmw-short} 8 Update 1 or later, or VCF 5 or later.

[id="ocp-4-19-deprecated-removed-features_{context}"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* _Not Available_
* _Technology Preview_
* _General Availability_
* _Deprecated_
* _Removed_


[id="ocp-release-note-bare-metal-dep-rem_{context}"]
=== Bare metal monitoring deprecated and removed features

.Bare Metal Event Relay Operator tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Bare Metal Event Relay Operator
|Removed
|Removed
|Removed
|====


[id="ocp-release-note-images-dep-rem_{context}"]
=== Images deprecated and removed features

.Images deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated
|====


[id="ocp-release-note-install-dep-rem_{context}"]
=== Installation deprecated and removed features

.Installation deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`--cloud` parameter for `oc adm release extract`
|Deprecated
|Deprecated
|Deprecated

|CoreDNS wildcard queries for the `cluster.local` domain
|Deprecated
|Deprecated
|Deprecated

|`compute.platform.openstack.rootVolume.type` for {rh-openstack}
|Deprecated
|Deprecated
|Deprecated

|`controlPlane.platform.openstack.rootVolume.type` for {rh-openstack}
|Deprecated
|Deprecated
|Deprecated

|`ingressVIP` and `apiVIP` settings in the `install-config.yaml` file for installer-provisioned infrastructure clusters
|Deprecated
|Deprecated
|Deprecated

|Package-based {op-system-base} compute machines
|Deprecated
|Deprecated
|Removed

|`platform.aws.preserveBootstrapIgnition` parameter for {aws-first}
|Deprecated
|Deprecated
|Deprecated

|Installing a cluster on {aws-short} with compute nodes in {aws-short} Outposts
|Deprecated
|Deprecated
|Deprecated
|====

// No deprecated or removed features for 3 consecutive releases
//
// [id="ocp-release-note-monitoring-dep-rem_{context}"]
// === Monitoring deprecated and removed features

// .Monitoring deprecated and removed tracker
// [cols="4,1,1,1",options="header"]
// |====
// |Feature |4.17 |4.18 |4.19
// |====


[id="ocp-release-note-networking-dep-rem_{context}"]
=== Networking deprecated and removed features

.Networking deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|iptables
|Deprecated
|Deprecated
|Deprecated

|====


[id="ocp-release-note-node-dep-rem_{context}"]
=== Node deprecated and removed features

.Node deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`ImageContentSourcePolicy` (ICSP) objects
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/zone`
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/region`
|Deprecated
|Deprecated
|Deprecated

|cgroup v1
|Deprecated
|Deprecated
|Removed
|====


[id="ocp-release-note-cli-dep-rem_{context}"]
=== OpenShift CLI (oc) deprecated and removed features

.OpenShift CLI (oc) deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|oc-mirror plugin v1
|General Availability
|Deprecated
|Deprecated
|====


[id="ocp-release-note-operators-dep-rem_{context}"]
=== Operator lifecycle and development deprecated and removed features

// "Operator lifecycle" refers to OLMv0 and "development" refers to Operator SDK

.Operator lifecycle and development deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Operator SDK
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Ansible-based Operator projects
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Helm-based Operator projects
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Go-based Operator projects
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Hybrid Helm-based Operator projects
|Deprecated
|Removed
|Removed

|Scaffolding tools for Java-based Operator projects
|Deprecated
|Removed
|Removed

// Do not remove the SQLite database... entry until otherwise directed by the Operator Framework PM
|SQLite database format for Operator catalogs
|Deprecated
|Deprecated
|Deprecated
|====


=== Storage deprecated and removed featuresmco-rn-tp-fix

.Storage deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Persistent storage using FlexVolume
|Deprecated
|Deprecated
|Deprecated

|AliCloud Disk CSI Driver Operator
|Removed
|Removed
|Removed

|Shared Resources CSI Driver Operator
|Deprecated
|Removed
|Removed
|====


[id="ocp-clusters-dep-rem_{context}"]
=== Updating clusters deprecated and removed features

.Updating clusters deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19
|====


[id="ocp-release-note-web-console-dep-rem_{context}"]
=== Web console deprecated and removed features

.Web console deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`useModal` hook for dynamic plugin SDK
|General Availability
|General Availability
|Deprecated

|Patternfly 4
|Deprecated
|Deprecated
|Removed

|====


[id="ocp-release-note-workloads-dep-rem_{context}"]
=== Workloads deprecated and removed features

.Workloads deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`DeploymentConfig` objects
|Deprecated
|Deprecated
|Deprecated
|====

[id="ocp-4-19-deprecated-features_{context}"]
=== Deprecated features

[id="ocp-4-19-oc-adm-pod-network-removed_{context}"]
==== `oc adm pod-network` command deprecated

The `oc adm pod-network` command for working with OpenShift SDN multitenant mode has been removed from the `oc adm --help` output. If the `oc adm pod-network` command is used, an error message is displayed to tell users that it has been deprecated.

[id="ocp-4-19-useModal-dynamic-plugin-removed_{context}"]
==== useModal hook for dynamic plugin SDK
With this release, support for the `useModal` hook in dynamic plugins are deprecated.

Starting with this release, use the `useOverlay` API hook to launch modals

//For more information about `useOverlay`, see [Doc link to dynamic plugin api docs TBD.]

[id="ocp-4-19-kubernetes-api-deprecation_{context}"]
==== Kubernetes API deprecation

{product-title} 4.17 inadvertently reintroduced a removed Kubernetes API, `admissionregistration.k8s.io/v1beta1`. This API is deprecated and is planned for removal in a future {product-title} release. Migrate any instances of this API to `admissionregistration.k8s.io/v1`.

For information about how to check your cluster for Kubernetes APIs that are planned for removal, see link:https://access.redhat.com/articles/6955985[Navigating Kubernetes API deprecations and removals].

[id="ocp-4-19-removed-features_{context}"]
=== Removed features

[id="ocp-4-19-cgroup-v1-removed_{context}"]
==== cgroup v1 has been removed

cgroup v1, which was deprecated in {product-title} 4.16, is no longer supported and has been removed from {product-title}. If your cluster is using cgroup v1, you must configure cgroup v2 before you can upgrade to  {product-title} {product-version}. All workloads must now be compatible with cgroup v2.

For information on configuring cgroup v2 in your cluster, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/nodes/working-with-clusters#nodes-clusters-cgroups-2_nodes-cluster-cgroups-2[Configuring Linux cgroup] in the {product-title} version 4.18 documentation.

For more information on cgroup v2, see xref:../architecture/index.adoc#architecture-about-cgroup-v2_architecture-overview[About Linux cgroup version 2] and link:https://www.redhat.com/en/blog/rhel-9-changes-context-red-hat-openshift-workloads[Red Hat Enterprise Linux 9 changes in the context of Red Hat OpenShift workloads] (Red{nbsp}Hat blog).

[id="ocp-4-19-rhel-worker-nodes-removed_{context}"]
==== Package-based {op-system-base} compute machines
With this release, support for the installation of packaged-based {op-system-base} worker nodes is removed.

{op-system} image layering replaces this feature and supports installing additional packages on the base operating system of your worker nodes.

For information on how to identify and remove {op-system-base} nodes in your cluster, see xref:../updating/preparing_for_updates/updating-cluster-prepare-past-4-18.adoc#updating-cluster-prepare-past-4-18[Preparing to update from {product-title} 4.18 to a newer version]. For more information on image layering, see xref:../machine_configuration/mco-coreos-layering.adoc#mco-coreos-layering[{op-system} image layering].

[id="ocp-4-19-removed-kube-1-32-apis_{context}"]
==== APIs removed from Kubernetes 1.32

Kubernetes 1.32 removed the following deprecated APIs, so you must migrate manifests and API clients to use the appropriate API version. For more information about migrating removed APIs, see the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-32[Kubernetes documentation].

.APIs removed from Kubernetes 1.32
[cols="2,2,2,1",options="header",]
|===
|Resource |Removed API |Migrate to |Notable changes

|`FlowSchema`
|`flowcontrol.apiserver.k8s.io/v1beta3`
|`flowcontrol.apiserver.k8s.io/v1`
|No

|`PriorityLevelConfiguration`
|`flowcontrol.apiserver.k8s.io/v1beta3`
|`flowcontrol.apiserver.k8s.io/v1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#flowcontrol-resources-v132[Yes]
|===

[id="ocp-4-19-removed-osdk_{context}"]
==== Operator SDK CLI and related scaffolding and testing tools

With this release, the Red{nbsp}Hat-supported version of the Operator SDK CLI tool, including the related scaffolding and testing tools for Operator projects, is no longer released with {product-title}.

Red{nbsp}Hat will provide bug fixes and support for versions of the Operator SDK that were released with earlier versions of {product-title} according to the link:https://access.redhat.com/product-life-cycles?product=OpenShift%20Container%20Platform%204[Product Life Cycles for {product-title} 4] (Red{nbsp}Hat Customer Portal).

Operator authors with existing Operator projects can use link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/operators/developing-operators#osdk-about[the version of the Operator SDK CLI tool released with {product-title} 4.18] to maintain their projects and create Operator releases that target newer versions of {product-title}. For more information, see link:https://access.redhat.com/node/7123119[Updating the base image for existing Ansible- or Helm-based Operator projects for {product-title} 4.19 and later] (Red{nbsp}Hat Knowledgebase).

For more information about the unsupported, community-maintained, version of the Operator SDK, see link:https://sdk.operatorframework.io[Operator SDK (Operator Framework)].

[id="ocp-4-19-bug-fixes_{context}"]
== Bug fixes
//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / TALO
//Telco Edge / ZTP


[id="ocp-release-note-api-auth-bug-fixes_{context}"]
=== API Server and Authentication

* Previously, contents of the `MachineConfig` and `ControllerConfig` resources from group `machineconfiguration.openshift.io` were not excluded from audit logs. With this release, they are excluded from audit logs because they might contain secrets. (link:https://issues.redhat.com/browse/OCPBUGS-55709[OCPBUGS-55709])

* Previously, the kube-apiserver service level objective (SLO) alert expression incorrectly summed read and write success ratios independently of total request volume. This led to misleading burn rate calculations during disruptions. With this release, the fix adjusts the calculation to properly weight success ratios by total request count. This results in accurate and reliable alerting based on the true proportion of successful requests. (link:https://issues.redhat.com/browse/OCPBUGS-49764[OCPBUGS-49764])

* Previously, cluster bootstrap removal could break kube-apiserver readiness if etcd access was lost which could lead to downtime. With this release, each kube-apiserver has 2 stable etcd endpoints before removing bootstrap which maintains availability during rollout. (link:https://issues.redhat.com/browse/OCPBUGS-48673[OCPBUGS-48673])

* Previously, the Static Pod Operator API allowed invalid node statuses with unset `currentRevision` and multiple nonzero `targetRevision` entries, which led to failures in node and installer controllers. With this release, new validation rules were added to enforce correct revision fields to ensure stable and consistent static pod status handling. (link:https://issues.redhat.com/browse/OCPBUGS-46380[OCPBUGS-46380])

* Previously, the node controller applied stale `NodeStatus` data from its lister, unintentionally overwriting recent updates from other controllers. With this release, the fix uses managed fields to let controllers update separate entries without conflict which preserves accurate and concurrent node status updates. (link:https://issues.redhat.com/browse/OCPBUGS-46372[OCPBUGS-46372])

* Previously, a fixed five minute timeout for removing the etcd bootstrap member started too early. This led to premature failures in HA clusters despite sufficient overall time. With this release, the narrow timeout is removed to rely on overall bootstrap progress instead, which ensures reliable and quorum-safe etcd bootstrap removal. (link:https://issues.redhat.com/browse/OCPBUGS-46363[OCPBUGS-46363])

* Previously, bootstrapping would unblock after detecting two kube-apiserver endpoints, including the bootstrap instance, causing periods of 0% availability as rollouts occur with only one permanent instance. With this release, the teardown is delayed until multiple permanent instances are ready. This ensures continuous kube-apiserver availability during rollout. (link:https://issues.redhat.com/browse/OCPBUGS-46010[OCPBUGS-46010])

* Previously, when the temporary control plane was down, the `networkConfig.status.ServiceNetwork` was not populated, and when generated certificates did not have the Kubernetes service IP in the SANs, the clients would fail to connect to the kube-apiserver through the default kubernetes service. With this release, a guard has been added to skip certificated generation if `networkConfig.status.ServiceNetwork` is nil. Client connections will be stable and valid. (link:https://issues.redhat.com/browse/OCPBUGS-45943[OCPBUGS-45943])

* Previously, the installer deleted the bootstrap machine before the etcd member was removed. This led to quorum loss in HA clusters. With this release, the check from SNO is extended to all topologies, using the etcd operator's condition as a safe removal sign, which ensures etcd cluster stability during bootstrap teardown. (link:https://issues.redhat.com/browse/OCPBUGS-45482[OCPBUGS-45482])

* Previously, the openshift-apiserver could panic when both image and error fields were unset during CRD request handling, this led to runtime crashes and instability in the API server under certain conditions. With this release, a guard is added to ensure no panic occurs by safely handling the case when both fields are unset, resulting in a more robust and stable CRD request handling process without crashes. (link:https://issues.redhat.com/browse/OCPBUGS-45861[OCPBUGS-45861])


[id="ocp-release-note-bare-metal-hardware-bug-fixes_{context}"]
=== Bare Metal Hardware Provisioning

* Previously, NetworkManager logs from the Ironic Python Agent (IPA) were not included in the ramdisk logs; instead only `dmesg` logs were included in the ramdisk logs. With this release, the ramdisk logs that exist in the `metal3-ramdisk-logs` container of a metal3 pod now contain the entire journal from the host instead of just `dmesg` logs and IPA. (link:https://issues.redhat.com/browse/OCPBUGS-56042[OCPBUGS-56042])

* Previously, ramdisk logs did not include clear file separators, causing the content from one file to merge into random lines of another file. Because of this issue, distinguishing what content belonged to which file was difficult. With this release, file entries now include file separators so that each file is clearly indicated from the contents of the other file being merged into a ramdisk log file. (link:https://issues.redhat.com/browse/OCPBUGS-55743[OCPBUGS-55743])

* Previously, if you forgot to include a Redfish system ID, such as `redfish://host/redfish/v1/` instead of `redfish://host/redfish/v1/Self`, in a Baseboard Management Console (BMC) URL, a JSON parsing issue existed in Ironic. With this release, BMO can now handle URLs without a Redfish system ID as a valid address without causing a JSON parsing issue. (link:https://issues.redhat.com/browse/OCPBUGS-56026[OCPBUGS-56026])

* Previously, a race condition existed during provisioning which, in case of a slow DHCP response, could cause different hostnames to be used for machine and node objects. This could prevent CSRs of worker nodes from being automatically approved. With this release, the race condition was fixed and CSRs of worker nodes are now properly approved.  (link:https://issues.redhat.com/browse/OCPBUGS-55315[OCPBUGS-55315])

* Previously, certain models of SuperMicro machines, such as `ars-111gl-nhr`, use a different virtual media device string than other SuperMicro machines, which could cause virtual media boot attempts to fail on these servers. With this release, an extra conditional check was added to check for the specific model affected and adjust behavior accordingly, so that SuperMicro models such as ars-111gl-nhr can now boot from virtual media. (link:https://issues.redhat.com/browse/OCPBUGS-56639[OCPBUGS-56639])

* Previously, after deleting a `BaremetalHost` that has a related `DataImage`, the `DataImage` was still present. With this release, the `DataImage` is deleted if it exists after the `BaremetalHost` has been deleted. (link:https://issues.redhat.com/browse/OCPBUGS-51294[OCPBUGS-51294])


[id="ocp-release-note-cloud-compute-bug-fixes_{context}"]
=== Cloud Compute

* When upgrading {gcp-short} clusters that use a boot disk that is not compatible with UEFI, you cannot enable Shielded VM support.
Previously, this prevented the creation of new compute machines.
With this release, disks with known UEFI incompatibility have Shielded VM support disabled.
This primarily affects customers upgrading from {product-title} version 4.12 to 4.13 using the {gcp-short} marketplace images.
(link:https://issues.redhat.com/browse/OCPBUGS-17079[OCPBUGS-17079])

* Previously, VMs in a cluster that ran on {azure-short} failed because the attached network interface controller (NIC) was in a `ProvisioningFailed` state.
With this release, the Machine API controller checks the provisioning status of a NIC and refreshes the VMs on a regular basis to prevent this issue.
(link:https://issues.redhat.com/browse/OCPBUGS-31515[OCPBUGS-31515])

* Previously, in larger clusters that had other subsystems using certificate signing requests (CSRs), the CSR approver counted unrelated, unapproved CSRs towards its total and prevented further approvals.
With this release, the CSR approver uses a `signerName` property as a filter and only includes CSRs that it can approve.
As a result, the CSR approver only prevents new approvals when there are a large number of unapproved CSRs for the relevant `signerName` values.
(link:https://issues.redhat.com/browse/OCPBUGS-36404[OCPBUGS-36404])

* Previously, the Machine API controller read only the zone number to populate machine zone information.
For machines in {azure-short} regions that only support availability sets, the set number represents the zone, so the Machine API controller did not populate their zone information.
With this release, the Machine API controller references the {azure-short} fault domain property.
This property works for availability sets and availability zones, so the controller correctly reads the fault domain in each case and machines always report a zone.
(link:https://issues.redhat.com/browse/OCPBUGS-38570[OCPBUGS-38570])

* Previously, increased granularity in {gcp-short} zone API error messages caused the machine controller to mistakenly mark some machines with invalid configurations as valid with a temporary cloud error.
This behavior prevented invalid machines from transitioning to a failed state.
With this release, the machine controller handles the more granular error messages correctly so that machines with an invalid zone or project ID correctly move to a failed state.
(link:https://issues.redhat.com/browse/OCPBUGS-43531[OCPBUGS-43531])

* Previously, some permissions required for linked actions were missing.
Linked actions create the subresources necessary for other {azure-short} resources that the cloud controller manager and {product-title} require.
With this release, the cloud controller manager for {azure-short} has the following permissions for linked actions:
+
--
** `Microsoft.Network/applicationGateways/backendAddressPools/join/action`
** `Microsoft.Network/applicationSecurityGroups/joinIpConfiguration/action`
** `Microsoft.Network/applicationSecurityGroups/joinNetworkSecurityRule/action`
** `Microsoft.Network/ddosProtectionPlans/join/action`
** `Microsoft.Network/gatewayLoadBalancerAliases/join/action`
** `Microsoft.Network/loadBalancers/backendAddressPools/join/action`
** `Microsoft.Network/loadBalancers/frontendIPConfigurations/join/action`
** `Microsoft.Network/loadBalancers/inboundNatRules/join/action`
** `Microsoft.Network/networkInterfaces/join/action`
** `Microsoft.Network/networkSecurityGroups/join/action`
** `Microsoft.Network/publicIPAddresses/join/action`
** `Microsoft.Network/publicIPPrefixes/join/action`
** `Microsoft.Network/virtualNetworks/subnets/join/action`
--
+
(link:https://issues.redhat.com/browse/OCPBUGS-44126[OCPBUGS-44126])

* Previously, some permissions required for linked actions were missing.
Linked actions create the subresources necessary for other {azure-short} resources that the Machine API and {product-title} require.
With this release, the Machine API provider for {azure-short} has the following permissions for linked actions:
+
--
** `Microsoft.Compute/disks/beginGetAccess/action`
** `Microsoft.KeyVault/vaults/deploy/action`
** `Microsoft.ManagedIdentity/userAssignedIdentities/assign/action`
** `Microsoft.Network/applicationGateways/backendAddressPools/join/action`
** `Microsoft.Network/applicationSecurityGroups/joinIpConfiguration/action`
** `Microsoft.Network/applicationSecurityGroups/joinNetworkSecurityRule/action`
** `Microsoft.Network/ddosProtectionPlans/join/action`
** `Microsoft.Network/gatewayLoadBalancerAliases/join/action`
** `Microsoft.Network/loadBalancers/backendAddressPools/join/action`
** `Microsoft.Network/loadBalancers/frontendIPConfigurations/join/action`
** `Microsoft.Network/loadBalancers/inboundNatPools/join/action`
** `Microsoft.Network/loadBalancers/inboundNatRules/join/action`
** `Microsoft.Network/networkInterfaces/join/action`
** `Microsoft.Network/networkSecurityGroups/join/action`
** `Microsoft.Network/publicIPAddresses/join/action`
** `Microsoft.Network/publicIPPrefixes/join/action`
** `Microsoft.Network/virtualNetworks/subnets/join/action`
--
+
(link:https://issues.redhat.com/browse/OCPBUGS-44130[OCPBUGS-44130])

* Previously, installing an {aws-short} cluster failed in certain environments on existing subnets when the `publicIp` parameter in the compute machine set CR was set to `false`.
With this release, a fix ensures that a configuration value set for `publicIp` no longer causes issues when the installation program provisions machines for your {aws-short} cluster in certain environments.
(link:https://issues.redhat.com/browse/OCPBUGS-44373[OCPBUGS-44373])

* Previously, {gcp-short} clusters that used non-UEFI disks failed to load.
This release adds a check to ensure that disks are UEFI-compatible before enabling features that require UEFI, such as secure boot.
This change adds `compute.images.get` and `compute.images.getFromFamily` permissions requirements.
As a result, you can use non-UEFI disks if you do not need these features.
(link:https://issues.redhat.com/browse/OCPBUGS-44671[OCPBUGS-44671])

* Previously, when the {aws-short} `DHCPOptionSet` parameter was configured to use a custom domain name that contains a trailing period (`.`), {product-title} installation failed.
With this release, the logic that extracts the hostname of EC2 instances and turns them into kubelet node names trims trailing periods so that the resulting Kubernetes object name is valid.
Trailing periods in this parameter no longer cause installation to fail. (link:https://issues.redhat.com/browse/OCPBUGS-45306[OCPBUGS-45306])

* Previously, the number of {azure-short} availability set fault domains used a fixed value of `2`.
This setting works in most {azure-short} regions because fault domain counts are typically at least 2.
However, this setting failed in the `centraluseuap` and `eastusstg` regions.
With this release, the number of availability set fault domains in a region is set dynamically.
(link:https://issues.redhat.com/browse/OCPBUGS-45663[OCPBUGS-45663])

* Previously, the {azure-short} cloud controller manager panicked when there was a temporary API server disconnection.
With this release, the {azure-short} cloud controller manager correctly recovers from temporary disconnection.
(link:https://issues.redhat.com/browse/OCPBUGS-45859[OCPBUGS-45859])

* Previously, some services became stuck in a pending state due to incorrect or missing annotations.
With this release, validation added to the {azure-short} `service.beta.kubernetes.io/azure-load-balancer-tcp-idle-timeout` and {gcp-short} `cloud.google.com/network-tier` annotations resolves the issue.
(link:https://issues.redhat.com/browse/OCPBUGS-48481[OCPBUGS-48481])

* Previously, the method used to fetch the provider ID from {aws-short} could fail to provide this value to the kubelet when needed.
As a result, sometimes machines could get stuck in different states and fail to complete initialization.
With this release, the provider ID is consistently set when the kubelet starts up.
(link:https://issues.redhat.com/browse/OCPBUGS-50905[OCPBUGS-50905])

* Previously, an incorrect endpoint in the {azure-short} cloud controller manager caused installations on {azure-full} Government Cloud to fail.
The issue is resolved in this release.
(link:https://issues.redhat.com/browse/OCPBUGS-50969[OCPBUGS-50969])

* Previously, the Machine API sometimes detected an unhealthy control plane node during cluster creation on {ibm-cloud-title} and attempted to replace the node.
This effectively destroyed the cluster.
With this release, the Machine API only attempts to replace unhealthy compute nodes during cluster creation and does not attempt to replace unhealthy control plane nodes.
(link:https://issues.redhat.com/browse/OCPBUGS-51864[OCPBUGS-51864])

* Previously, {azure-short} spot machines that were evicted before their node became ready could get stuck in the `provisioned` state.
With this release, {azure-short} spot instances now use a delete-eviction policy.
This policy ensures that the machines correctly move to the `failed` state upon preemption.
(link:https://issues.redhat.com/browse/OCPBUGS-54617[OCPBUGS-54617])

* Previously, a bug fix altered the availability set configuration by changing the fault domain count to use the maximum available value instead of a fixed value of `2`.
This inadvertently caused scaling issues for compute machine sets created before the bug fix, as the controller attempted to change immutable availability sets.
With this release, availability sets are no longer modified after creation, allowing affected compute machine sets to scale properly.
(link:https://issues.redhat.com/browse/OCPBUGS-56653[OCPBUGS-56653])

* Previously, the `openshift-cnv` namespace components did not feature the `openshift.io/required-scc` annotation. Workloads were not requesting their required security content constraints (SCCs). With this release, the `openshift.io/required-scc` annotation is added to the `openshift-cnv` namespace components so that workloads can request the required SCCs. (link:https://issues.redhat.com/browse/OCPBUGS-49657[OCPBUGS-49657])


[id="ocp-release-note-cloud-cred-operator-bug-fixes_{context}"]
=== Cloud Credential Operator

* Previously, the `aws-sdk-go-v2` software development kit (SDK) failed to authenticate an `AssumeRoleWithWebIdentity` API operation on an {aws-first} {sts-first} cluster. With this release, `pod-identity-webhook` now includes a default region so that this issue no longer persists. (link:https://issues.redhat.com/browse/OCPBUGS-41727[OCPBUGS-41727])


[id="ocp-release-note-cluster-autoscaler-bug-fixes_{context}"]
=== Cluster Autoscaler

* Previously, when a Machine Set was scaled down and had reached its minimum size, the Cluster Autoscaler could leave the last remaining node with a no schedule taint that prevented use of a node. This issues was caused by a counting error in the Cluster Autoscaler. With this release, the counting error has been fixed so that the Cluster Autoscaler works as expected when a Machine Set is scaled down and has reached its minimum size. (link:https://issues.redhat.com/browse/OCPBUGS-54231[OCPBUGS-54231])

* Previously, some cluster autoscaler metrics were not initialized, and therefore were not available. With this release, these metrics are initialized and available. (link:https://issues.redhat.com/browse/OCPBUGS-25852[OCPBUGS-25852])

* Previously the Cluster Autoscaler could stop scaling because of a failed machine in a machine set. This condition occurred because of inaccuracies in the way the Cluster Autoscaler counts machines in various non-running phases. With this release, the inaccuracies have been fixed, so that the Cluster Autoscaler accurately counts machines. (link:https://issues.redhat.com/browse/OCPBUGS-11115[OCPBUGS-11115])


[id="ocp-release-note-cluster-override-admin-operator-bug-fixes_{context}"]
=== Cluster Resource Override Admission Operator

* Previously, the Cluster Resource Admission Override Operator failed to delete old secrets during upgrading from {product-title} 4.16 to {product-title} 4.17. This situation caused the Cluster Resource Override Admission Operator webhook to stop working and prevented pods from being created in namespaces that had the Cluster Resource Override Admission Operator enabled. With this release, old secrets are deleted, error handling by the Cluster Resource Override Admission Operator is improved, and the issue with creating pods in namespaces is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-54886[OCPBUGS-54886])

* Previously, if you deleted the `clusterresourceoverride-operator` service or uninstalled the Cluster Resource Admission Override Operator, the `v1.admission.autoscaling.openshift.io` API service was unreachable and prevented needed cluster functions, such as installing other Operators on the cluster. With this release, a fix ensures that if the Cluster Resource Admission Override Operator is uninstalled, the `v1.admission.autoscaling.openshift.io` API service is also deleted so that the cluster functions are not impacted. (link:https://issues.redhat.com/browse/OCPBUGS-48115[OCPBUGS-48115])

* Previously, if you specified a `forceSelinuxRelabel` parameter in a `ClusterResourceOverride` CR and then changed the parameter to another value, the changed value would not be reflected in the `clusterresourceoverride-configuration` Config Map. This Config Map is required for applying the selinux relabelling workaround feature to your cluster. With this release, this issue is fixed so that when the `forceSelinuxRelabel` parameter is changed, the `clusterresourceoverride-configuration` Config Map received the update. (link:https://issues.redhat.com/browse/OCPBUGS-44649[OCPBUGS-44649])


[id="ocp-release-note-cluster-version-operator-bug-fixes_{context}"]
=== Cluster Version Operator

* Previously, the status of the `ClusterVersion` condition could  changed from `ImplicitlyEnabled` to `ImplicitlyEnabledCapabilities`. With this release, the `ClusterVersion` condition type has been fixed and changed from `ImplicitlyEnabled` to `ImplicitlyEnabledCapabilities`. (link:https://issues.redhat.com/browse/OCPBUGS-56771[OCPBUGS-56771])

* Previously, a custom Security Context Constraint (SCC) impacted any pod that was generated by the Cluster Version Operator from receiving a cluster version upgrade. With this release, {product-title} now sets a default SCC to each pod, so that any custom SCC created does not impact a pod. (link:https://issues.redhat.com/browse/OCPBUGS-31462[OCPBUGS-31462])

* Previously, when a Cluster Operator takes a long time to upgrade, Cluster Version Operator does not report anything as it cannot determine if the upgrade is still progressing or already stuck. With this release, a new unknown status is added for the failing condition in status of the Cluster Version reported by Cluster Version Operator to remind the cluster administrators to check the cluster and avoid waiting on a blocked Cluster Operator upgrade. (link:https://issues.redhat.com/browse/OCPBUGS-23514[OCPBUGS-23514])

[id="ocp-release-note-etcd-bug-fixes_{context}"]
=== etcd

* Before this update, during rolling cluster updates from etcd 3.5.19 to a release of 3.6, the wrong membership data could be propagated to new members. As a consequence, cluster updates failed with an error about too many learner members in the cluster. With this release, etcd is updated to 3.5.24, which includes fixes so that the membership-related errors no longer occur. (link:https://issues.redhat.com/browse/OCPBUGS-63473[OCPBUGS-63473])

[id="ocp-release-note-image-streams-bug-fixes_{context}"]
=== ImageStreams

* Previously, image import blocked registries that would fail if those registries were configured with `NeverContactSource`, even when mirror registries were set up. With this update, image importing is no longer blocked when a registry has mirrors configured. This ensures that image imports succeed even if the original source was set to `NeverContactSource` in the `ImageDigestMirrorSet` or `ImageTagMirrorSet` resources. (link:https://issues.redhat.com/browse/OCPBUGS-44432[OCPBUGS-44432])


[id="ocp-release-note-installer-bug-fixes_{context}"]
=== Installer

* Previously, if you attempted to install an {aws-first} cluster with minimum privileges and you did not specify an instance type in the `install-config.yaml` file, installation of the cluster failed. This issue happened because the installation program could not find supported instance types that the cluster could use in supported availability zones. For example, the `m6i.xlarge` default instance type was unavailable in `ap-southeast-4` and `eu-south-2` availability zones. With this release, the `openshift-install` program now requires the `ec2:DescribeInstanceTypeOfferings` {aws-short} permission to prevent the installation of the cluster from failing in situations where `m6i.xlarge` or another supported instance type is unavailable in a supported availability zone. (link:https://issues.redhat.com/browse/OCPBUGS-46596[OCPBUGS-46596])

* Previously, the installation program did not prevent users from attempting to install a single-node cluster on bare metal, which resulted in a failed installation. With this update, the installation program prevents single-node cluster installations on unsupported platforms. (link:https://issues.redhat.com/browse/OCPBUGS-56811[OCPBUGS-56811])

* Previously, when you diagnosed issues related to running the `openshift-install destroy cluster` command for {vmw-first}, the logging information provided insufficient detail. As a consequence, it was unclear why clusters were not removed from virtual machines (VMs). With this release, when you destroy a cluster, enhanced debug logging is provided and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-56372[OCPBUGS-56372])

* Previously, when installing into an existing virtual private cloud (VPC) on {aws-first}, a potential mismatch could occur in the subnet information in the {aws-short} Availability Zone between the machine set custom resources for control plane nodes and their corresponding {aws-short} EC2 instances. As a consequence, where the control plane nodes were spread across three Availability Zones and one was recreated the discrepancy could result in an unbalanced control plane as two nodes occurred within the same Availability Zone. With this release, it is ensured that the subnet Availability Zone information in the machine set custom resources and in the EC2 instances match and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-55492[OCPBUGS-55492])

* Previously, when installing a cluster with the `OVNKubernetes` network plugin, the installation could fail if the plugin is specified as `OVNkubernetes` with a lowercase "k". With this update, the installation program correctly interprets the plugin name regardless of case. (link:https://issues.redhat.com/browse/OCPBUGS-54606[OCPBUGS-54606])

* When a Proxy is configured, the installation program adds the `machineNetwork` CIDR to the `noProxy` field. Previously, if the `machineNetwork` CIDR had also been configured by the user in the `noProxy` field, this would result in a duplicate entry, which is not allowed by ignition and could prevent the host from booting properly. With this release, the installation program will not add the `machineNetwork` CIDR to the `noProxy` field if it has already been set. (link:https://issues.redhat.com/browse/OCPBUGS-53183[OCPBUGS-53183])

* Previously, API and ingress VIPs were automatically assigned even when a user-managed load balancer was in use. This behavior was unintended. Now, API and ingress VIPs are no longer automatically assigned. If these values are not explicitly set in the `install-config.yaml` file, the installation fails with an error, prompting the user to provide them. (link:https://issues.redhat.com/browse/OCPBUGS-53140[OCPBUGS-53140])

* Previously, when using the Agent-based Installer, the WWN of Fibre Channel (FC) multipath volumes was not detected during hardware discovery. As a result, when the `wwn` root device hint was specified, all multipath FC volumes were excluded by it. With this release, the WWN is now collected for multipath FC volumes, so when more than one multipath volume is present, users can select between them by using the `wwn` root device hint. (link:https://issues.redhat.com/browse/OCPBUGS-52994[OCPBUGS-52994])

* Previously, when installing a cluster on {azure-short}, the installation program did not include support for NVMe or SCSI, which prevented the use of VM instance families that require it. With this update, the installation program can make use of VM instance families that require NVMe or SCSI support. (link:https://issues.redhat.com/browse/OCPBUGS-52658[OCPBUGS-52658])

* Previously, when installing a cluster on {gcp-short} with a user-provided encryption key, the installation program could fail to find the key ring. With this update, the installation program finds the user-provided encryption key ring so the installation does not fail. (link:https://issues.redhat.com/browse/OCPBUGS-52203[OCPBUGS-52203])

* Previously, when installing a cluster on {gcp-short}, the installation could fail if network instability prevented the fetching of {gcp-short} tags during installation. With this update, the installation program has been improved to tolerate network instability during installation. (link:https://issues.redhat.com/browse/OCPBUGS-50919[OCPBUGS-50919])

* Previously, the installer was not checking for ESXi hosts that were powered off within a {vmw-first} cluster, which caused the installation to fail because the OVA could not be uploaded. With this release, the installer now checks the power status of each ESXi host and skips any that are powered off, which resolves the issue and allows the OVA to be imported successfully. (link:https://issues.redhat.com/browse/OCPBUGS-50649[OCPBUGS-50649])

* Previously, when using the Agent-based Installer, erroneous error messages regarding `unable to read image` were output when building the Agent ISO image in a disconnected environment. With this release, these erroneous messages have been removed and no longer appear. (link:https://issues.redhat.com/browse/OCPBUGS-50637[OCPBUGS-50637])

* Previously, when installing a cluster on {azure-short}, the installation program would crash with a segmentation fault error if it did not have the correct permissions to check IP address availability. With this update, the installation program correctly identifies the missing permission and fails gracefully. (link:https://issues.redhat.com/browse/OCPBUGS-50534[OCPBUGS-50534])

* Previously, when the `ClusterNetwork` classless inter-domain routing (CIDR) mask value is greater than the `hostPrefix` value and the `networking.ovnKubernetesConfig.ipv4.internalJoinSubnet` section is provided in the `install-config.yaml` file, the installation program failed a validation check and returned a Golang runtime error. With this release, the installation program still fails the validation check and now outputs a descriptive error message that indicates the invalid `hostPrefix` value. (link:https://issues.redhat.com/browse/OCPBUGS-49784[OCPBUGS-49784])

* Previously, when installing a cluster on {ibm-cloud-name}, the installation program failed to install on the `ca-mon` region even though it is available. With this update, the installation program is up to date with the latest available {ibm-cloud-name} regions. (link:https://issues.redhat.com/browse/OCPBUGS-49623[OCPBUGS-49623])

* Previously, after installing a cluster on {aws-short} with minimum permissions in an existing VPC with a user-provided public IPv4 pool, the cluster could not be destroyed due to a missing permission. With this update, the installation program propagates the `ec2:ReleaseAddress` permission so that the cluster can be destroyed. (link:https://issues.redhat.com/browse/OCPBUGS-49594[OCPBUGS-49594])

* Previously, the installer for {vmw-first} did not validate the number of networks provided in the `install-config.yaml` for failure domains. This caused the installation to proceed with an unsupported configuration if more than the maximum of 10 networks were specified, without providing an error. With this release, the installer now validates the number of configured networks, which resolves the issue by preventing the use of a configuration that exceeds the maximum limit. (link:https://issues.redhat.com/browse/OCPBUGS-49351[OCPBUGS-49351])

* Previously, installing a cluster on {aws-short} with existing subnets (BYO VPC) in Local or Wavelength zones resulted in the edge subnets resource missing the `kubernetes.io/cluster/<InfraID>:shared` tag. With this release, a fix ensures that all subnets used in the `install-config.yaml` file have the required tags. (link:https://issues.redhat.com/browse/OCPBUGS-48827[OCPBUGS-48827])

* Previously, an issue prevented configuring multiple subnets in the failure domain of a Nutanix cluster during installation. The issue is resolved in this release. (link:https://issues.redhat.com/browse/OCPBUGS-49885[OCPBUGS-49885])

* Previously, when installing a cluster on {aws-short}, the `ap-southeast-5` region was not available in the installation program survey, even though this region was supported by {product-title}. With this update, the `ap-southeast-5` region is available. (link:https://issues.redhat.com/browse/OCPBUGS-47681[OCPBUGS-47681])

* Previously, when destroying a cluster installed on {gcp-short}, some resources could be left behind because the installation program did not wait to ensure that all destroy operations completed successfully. With this update, the destroy API waits to ensure that all resources are appropriately deleted. (link:https://issues.redhat.com/browse/OCPBUGS-47489[OCPBUGS-47489])

* Previously, when installing a cluster on {aws-short} in the `us-east-1` regions, the installation could fail if no zone is specified in the `install-config.yaml` file because the `use1-az3` zone does not support any instance types supported by {product-title}. With this update, the installation program prevents the use of the `use1-az3` zone when no zones are specified in the installation configuration file. (link:https://issues.redhat.com/browse/OCPBUGS-47477[OCPBUGS-47477])

* Previously, when installing a cluster on {gcp-short}, the installation would fail if you enabled the `constraints/compute.vmCanIpForward` constraint on your project. With this update, the installation program disables this constraint if it is enabled, allowing the installation to succeed. (link:https://issues.redhat.com/browse/OCPBUGS-46571[OCPBUGS-46571])

* Previously, when installing a cluster on {gcp-short}, the installation program would fail to detect if the user provided an encryption key ring that did not exist, causing the installation to fail. With this update, the installation program correctly validates the existence of user provided encryption key rings, preventing failure. (link:https://issues.redhat.com/browse/OCPBUGS-46488[OCPBUGS-46488])

* Previously, when destroying a cluster that was installed on {azure-first}, the inbound NAT rules and security groups for the bootstrap node were not deleted. With this update, the correct resource group ensures that all resources are deleted when the cluster is destroyed. (link:https://issues.redhat.com/browse/OCPBUGS-45429[OCPBUGS-45429])

* Previously, when installing a cluster on {aws-short} in the `ap-southeast-5` region, the installation could fail due to a malformed load balancer hostname. With this update, the installation program has been improved to form the correct hostname so that installation succeeds. (link:https://issues.redhat.com/browse/OCPBUGS-45289[OCPBUGS-45289])

* Previously, when installing a cluster on {gcp-short}, the installation program could fail to locate the service account it created due to a delay in activating the service account on Google's servers. With this update, the installation program waits an appropriate amount of time before attempting to use the created service account. (link:https://issues.redhat.com/browse/OCPBUGS-45280[OCPBUGS-45280])

* Previously, when installing a cluster on {aws-short}, the installation could fail if you specified an edge machine pool but did not specify an instance type. With this update, the installation program requires that an instance type be provided for edge machine pools. (link:https://issues.redhat.com/browse/OCPBUGS-45218[OCPBUGS-45218])

* Previously, when destroying a cluster installed on {gcp-short}, PVC disks with the label `kubernetes-io-cluster-<cluster-id>: owned` were not deleted. With this update, the installation program correctly locates and deletes these resources when the cluster is destroyed. (link:https://issues.redhat.com/browse/OCPBUGS-45162[OCPBUGS-45162])

* Previously, during a disconnected installation, when the `imageContentSources` parameter was configured for more than one mirror for a source, the command to create the agent ISO image could fail, depending on the sequence of the mirror configuration. With this release, multiple mirrors are handled correctly when the agent ISO is created and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-44938[OCPBUGS-44938])

* Previously, when installing a cluster on {aws-short}, if the `publicIPv4Pool` parameter was set but the `ec2:AllocateAddress` permission was not present, the installation would fail. With this update, the installation program requires that this permission is present. (link:https://issues.redhat.com/browse/OCPBUGS-44925[OCPBUGS-44925])

* Previously, during a shared Virtual Private Cloud (VPC) installation, the installer added the records to a private DNS zone created by the installer instead of adding the records to the clusters private DNS zone. As a consequence, the installation failed. With this release, the installer searches for an existing private DNS zone and, if found, pairs that zone with the network that is supplied by the `install-config.yaml` file and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-44641[OCPBUGS-44641])

* Previously, you could add white space to {aws-first} tag names but the installation program did not support them. This situation resulted in the installation program outputting an `ERROR failed to fetch Metadata` message. With this release, the regular expression for {aws-short} tags now validates any tag name that has white space so that the installation program accepts these tags and no longer outputs an error because of white space. (link:https://issues.redhat.com/browse/OCPBUGS-44199[OCPBUGS-44199])

* Previously, when destroying a cluster that was installing on {gcp-short}, forwarding rules, health checks and firewall rules were not deleted, leading to errors. With this update, all resources are deleted when the cluster is destroyed. (link:https://issues.redhat.com/browse/OCPBUGS-43779[OCPBUGS-43779])

* Previously, when installing a cluster on {azure-full}, specifying the `Standard_M8-4ms` instance type resulted in an error due to that instance type specifying its memory in decimal format instead of integer format. With this update, the installation program correctly parses the memory value. (link:https://issues.redhat.com/browse/OCPBUGS-42241[OCPBUGS-42241])

* Previously, when installing a cluster on {vmw-full}, installations could fail if the API and Ingress server virtual IPs were outside of the machine network. With this update, the installation program includes the API and Ingress server virtual IPs in the machine network by default. If you specify the API and Ingress server virtual IPs, ensure that they are in the machine network. (link:https://issues.redhat.com/browse/OCPBUGS-36553[OCPBUGS-36553])

* Previously, when installing a cluster on {ibm-power-server-title}, the installation failed if you selected the Madrid zone due to an image import error. With this update, the installation program has been modified to use the correct storage bucket name and continue the installation successfully. (link:https://issues.redhat.com/browse/OCPBUGS-50899[OCPBUGS-50899])

* Previously, when destroying a cluster installed on {ibm-power-server-title}, some resources including network subnets were not deleted. With this update, all network resources are deleted when the cluster is destroyed. (link:https://issues.redhat.com/browse/OCPBUGS-50657[OCPBUGS-50657])

* Previously, when installing a cluster using the Assisted Installer, the installation could fail due to a timeout when pulling images. With this update, the timeout has been increased so that the installation program completes pulling images. (link:https://issues.redhat.com/browse/OCPBUGS-50655[OCPBUGS-50655])

* Previously, in some slower PrismCentral environments, the installation program would fail with a timeout when make the prism-api call to load the {op-system} image. The timeout value was previously 5 minutes. With this release, the prism-api call timeout value is a configurable parameter in the `install-config.yaml` file as `platform.nutanix.prismAPICallTimeout`, with a default value of 10 minutes. (link:https://issues.redhat.com/browse/OCPBUGS-48570[OCPBUGS-48570])

* Previously, an issue prevented configuring multiple subnets in the failure domain of a Nutanix cluster during installation. The issue is resolved in this release. (link:https://issues.redhat.com/browse/OCPBUGS-48044[OCPBUGS-48044])

* Previously, when installing a cluster on {ibm-power-server-title} using installer-provisioned infrastructure, the installation program selected a random machine network instead of using the one provided by the user. With this update, the installation program uses the machine network that the user provides. (link:https://issues.redhat.com/browse/OCPBUGS-45286[OCPBUGS-45286])

* Previously, the temporary directory that was created when the `openshift-install agent create pxe-files` command was run was not removed after the command completed. With this release, the temporary directory is now removed properly after the command completes. (link:https://issues.redhat.com/browse/OCPBUGS-39583[OCPBUGS-39583])


[id="ocp-release-note-machine-config-operator-bug-fixes_{context}"]
=== Machine Config Operator

* Previously, the `ContainerRuntimeConfig` incorrectly set the `--root` path for the `runc` runtime. This caused containers to run with an incorrect root path and led to issues with the container operations. With this release, the `--root` path for container runtime is correct and matches the specified runtime, providing consistent operation. (link:https://issues.redhat.com/browse/OCPBUGS-47629[OCPBUGS-47629])

* Previously, users were not warned if their cluster contained {op-system-base-full} worker nodes, which are no longer supported in {product-title} 4.19 and later. With this release, the Machine Config Operator detects {op-system-base} nodes and notifies users that are not compatible with {product-title} 4.19. (link:https://issues.redhat.com/browse/OCPBUGS-54611[OCPBUGS-54611])

* Previously, when the Machine Config Operator (MCO) rebooted a node too quickly after staging an update, the update failed. With this release, the MCO waits for the staging operation to finish before rebooting the system, allowing the update to complete. (link:https://issues.redhat.com/browse/OCPBUGS-51150[OCPBUGS-51150])

* Previously, after deleting a `MachineOSConfig` object, the associated `MachineOSBuild` object was not deleted as expected. This was because the ownership of the `MachineOSBuild` object was not set. With this release, all of the objects are created for the build and all associated objects are deleted when the `MachineOSConfig` object is deleted. (link:https://issues.redhat.com/browse/OCPBUGS-44602[OCPBUGS-44602])


[id="ocp-release-note-management-console-bug-fixes_{context}"]
=== Management Console

* Previously, the *Projects details* in the *Developer Perspective* erroneously did not include breadcrumbs. With this release, the breadcrumbs have been added. (link:https://issues.redhat.com/browse/OCPBUGS-52298[OCPBUGS-52298])

* Previously, when opening the *Project* drop-down list while the web terminal is open, there were visual artifacts. After this update, the artifact was fixed, so you can use the *Project* drop-down list when the web terminal is open. (link:https://issues.redhat.com/browse/OCPBUGS-45325[OCPBUGS-45325])

* Previously, a `PipelineRuns` CR that used a resolver could not be rerun on the {product-title} web console. If you attempted to rerun the CR, an "Invalid `PipelineRun` configuration, unable to start Pipeline" was generated. With this release, you can now rerun a `PipelineRuns` CR that uses resolver without experiencing this issue. (link:https://issues.redhat.com/browse/OCPBUGS-44265[OCPBUGS-44265])

* Previously, when you used the *Form View* to edit `Deployment` or `DeploymentConfig` API objects on the {product-title} web console, duplicate `ImagePullSecrets` parameters existed in the YAML configuration for either object. With this release, a fix ensures that duplicate `ImagePullSecrets` parameters do not get automatically added for either object. (link:https://issues.redhat.com/browse/OCPBUGS-41974[OCPBUGS-41974])

* Previously, a `TaskRun` for a particular `Pipelinerun` was fetched based on the `PipelineRun` name. If any two `PipelineRuns` had the same name, the `TaskRun` for both `PipelineRuns` was fetched and displayed. With this release, the `TaskRun` for the particular `PipelineRun` will be fetched based on `PipelineRun` UID instead of `PipelineRun` name. (link:https://issues.redhat.com/browse/OCPBUGS-36658[OCPBUGS-36658])

* Previously, the *Test Serverless function* button did not respond if there were no pods running. With this update, the button is disabled when there are no pods running. (link:https://issues.redhat.com/browse/OCPBUGS-32406[OCPBUGS-32406])

* Previously, the results of a failed `TaskRun` would not show up in the UI. With this update, the results of a `TaskRun` are always available, regardless of failure.(link:https://issues.redhat.com/browse/OCPBUGS-23924[OCPBUGS-23924])

* Previously, the console alerted users that compute nodes must be updated within 60 days when performing a control plane only update. With this release, the console no longer displays this invalid alert. (link:https://issues.redhat.com/browse/OCPBUGS-56077[OCPBUGS-56077])

* Previously, the *Critical Alerts* section of the *Notification Drawer* could not be collapsed. With this release, the section can be collapsed. (link:https://issues.redhat.com/browse/OCPBUGS-55702[OCPBUGS-55702])

* Previously, when viewing the list of installed Operators, an Operator displayed twice in the list if the currently selected project matched an Operator's default namespace while copied CSVs were disabled in {olm-first}. With this release, the Operator displays only once in such cases. (link:https://issues.redhat.com/browse/OCPBUGS-54601[OCPBUGS-54601])

* Previously, the link to *OperatorHub* on the *Installed Operators* page would trigger a hard reload. With this release, this link no longer triggers a hard reload. (link:https://issues.redhat.com/browse/OCPBUGS-54536[OCPBUGS-54536])

* Previously, selecting *All Projects* from the project picker while on the *Create VolumeSnapshot* page resulted in a page not found error. With this release, the VolumeSnapshot list page is correctly displayed. (link:https://issues.redhat.com/browse/OCPBUGS-53227[OCPBUGS-53227])

* Previously, there was incorrect logic for calculating the pod container count causing it to be inaccurate. With this release, the `Ready` and `Started` status in count logic was added so the correct pod container count is displayed, which is consist with `oc` CLI. (link:https://issues.redhat.com/browse/OCPBUGS-53118[OCPBUGS-53118])

* Previously, the *Select* menu above the *Node Logs* section did not close when opened, unless the *Select* menu's toggle was clicked again or one of the Select's menu items was clicked. With this release, the *Select* menu closes after clicking outside of the menu or by pressing the appropriate key on the keyboard. (link:https://issues.redhat.com/browse/OCPBUGS-52316[OCPBUGS-52316])

* Previously, the shared timestamp component referenced an undefined property when calculating relative times. As a result, most times displayed in the console were not correctly displaying relative strings such as `Just now` or `Less than a minute ago`. With this release, the issue is fixed and relative time strings are correctly rendered in the console. (link:https://issues.redhat.com/browse/OCPBUGS-51202[OCPBUGS-51202])

* Previously, the *Observe* menu only displayed based on the current user and console configuration for monitoring. This caused other items added by observability plugins to be hidden. With this release, the *Observe* menu also displays items from different observability plugins. (link:https://issues.redhat.com/browse/OCPBUGS-50693[OCPBUGS-50693])

* Previously, when logging in to the console for the first time, automatic perspective detection caused the console to ignore the specific URL path that the user clicked to get onto the console, and instead load a different page. With this release, the current path is followed. (link:https://issues.redhat.com/browse/OCPBUGS-50650[OCPBUGS-50650])

* Previously, there was an issue when creating new tabs in the horizontal navigation present in the web console from plugins. With this release, you can use plugins to create tabs on the web console horizontal navigation. (link:https://issues.redhat.com/browse/OCPBUGS-49996[OCPBUGS-49996])

* Previously, the *Cluster Settings* page would not properly render during a cluster update if the `ClusterVersion` did not receive a `Completed` update. With this release, the *Cluster Setting* page properly renders even if the `ClusterVersion` has not received a `Completed` update. (link:https://issues.redhat.com/browse/OCPBUGS-49839[OCPBUGS-49839])

* Previously, the links on the CLI downloads page were not sorted by operating system. With this release, the links are sorted by their operating system in alphabetical order. (link:https://issues.redhat.com/browse/OCPBUGS-48413[OCPBUGS-48413])

* Previously, multiple external link icons could appear in the primary *Action* button of the *OperatorHub* modal. With this release, only a single external link icon is displayed. (link:https://issues.redhat.com/browse/OCPBUGS-46555[OCPBUGS-46555])

* Previously, clicking the `Don't show again` link in the {ols-official} modal did not correctly navigate to the general *User Preference* tab when one of the other *User Preference* tabs was displayed. After this update, clicking the `Don't show again` link correctly navigates to the general *User Preference* tab. (link:https://issues.redhat.com/browse/OCPBUGS-46511[OCPBUGS-46511])

* Previously, a console plugin could be enabled multiple times in the *Console plugin enablement* modal, resulting in multiple entries for the plugin appearing the Console Operator Configuration. With this release, it is no longer possible to enable a plugin if it is already enabled. (link:https://issues.redhat.com/browse/OCPBUGS-44595[OCPBUGS-44595])

* Previously, the {product-title} web console login page always allowed you to click the *Login* button. You could still click when no username or password was entered, or if the *Login* button was already clicked. With this release, the *Login* button is disabled so you are unable to click the *Login* button without a username or password. (link:https://issues.redhat.com/browse/OCPBUGS-43610[OCPBUGS-43610])

* Previously, `PackageManifest` by name only was selected on the *Operator installation* status page. In some cases, this caused the incorrect `PackageManifest` to be used for displaying the logo and provider since there can be name collisions. With this release, `PackageManifests` are selected by name and label selector to make sure the correct one is selected for the current installation. As a result, the correct logo and provider are always shown on the operator install status page. (link:https://issues.redhat.com/browse/OCPBUGS-21755[OCPBUGS-21755])


[id="ocp-release-note-monitoring-bug-fixes_{context}"]
=== Monitoring

* Previously, if a scrape failed, Prometheus erroneously considered the samples from the very next scrape as duplicates and dropped them. This issue affected only the scrape immediately following a failure, while subsequent scrapes were processed correctly. With this release, the scrape following a failure is now correctly handled, ensuring that no valid samples are mistakenly dropped. (link:https://issues.redhat.com/browse/OCPBUGS-53025[OCPBUGS-53025])


[id="ocp-release-note-networking-bug-fixes_{context}"]
=== Networking

* Previously, when a pod used the CNI plugin for DHCP address assignment, in conjunction with other CNI plugins, the network interface of the pod might have been unexpectedly deleted. As a consequence, when the DHCP lease expired for the pod, the DHCP proxy entered a loop when trying to re-create a new lease, leading to the node becoming unresponsive. With this release, the DHCP lease maintenance terminates if the network interface does not exist. As a result, interface deletions are handled gracefully, ensuring node stability. (link:https://issues.redhat.com/browse/OCPBUGS-45272[OCPBUGS-45272])

* Previously, the Kubernetes NMState Operator Operator did not create the `nmstate-console-plugin` pod  because of an issue with the `pluginPort` template. With this release, a fix to the template ensures that the Operator can now successfully create the `nmstate-console-plugin` pod. (link:https://issues.redhat.com/browse/OCPBUGS-54295[OCPBUGS-54295])

* Previously, the pod controller in the Whereabouts reconciler was not passing the namespace to the leader election function, so the pod controller was not deleting orphaned allocations. This lead to repeated log error messages. With this release, the namespace is passed in and the orphaned allocations are deleted properly. (link:https://issues.redhat.com/browse/OCPBUGS-53397[OCPBUGS-53397])

* Previously, the `SriovOperatorConfig` Operator removed any parameters that had default values in the `SriovOperatorConfig` resources. This situation caused certain information to be missing from the output of a resource.  With this release, the Operator uses the PATCH method for API servers to preserve parameters with default values so that no information is missing from the output for a resource. (link:https://issues.redhat.com/browse/OCPBUGS-53346[OCPBUGS-53346])

* Previously, the `SriovNetworkNodePolicy` object reconciler executed with every node resource update. This resulted in excessive resource consumption by the SR-IOV Operator pod and an overabundance of log entries. This release changes the behavior so that the reconciler only runs when a node label changes, thereby reducing resource consumption and log entry generation. (link:https://issues.redhat.com/browse/OCPBUGS-52955[OCPBUGS-52955])

* Previously, a cluster with the `clusterNetwork` parameter listing multiple networks of the same IP address family entered a `crashloopbackoff` state when upgrading to the latest version of {product-title}. With this release,  a fix ensures that the cluster with this configuration no longer enters the `crashloopbackoff` state during a cluster upgrade. (link:https://issues.redhat.com/browse/OCPBUGS-49994[OCPBUGS-49994])

* Previously, the `resolv-prepender` service triggered earlier than expected. This situation caused the service to fail and resulted in an incorrectly configured setting for the host DNS. With this release, the configuration for the `resolv-prepender` service is updated so that when the service triggers earlier than expected, it no longer causes incorrect configuration of the host DNS settings. (link:https://issues.redhat.com/browse/OCPBUGS-49436[OCPBUGS-49436])

* Previously, the `nmstate-configuration` service was only enabled for deployments that had the `platform` parameter set to `baremetal`. However, you could also use the Assisted Installer to configure a bare-metal deployment by setting the `platform` parameter set to  `None`,  but the NMState `br-ex` network bridge creation feature did not work with this installation method.  With this release, the `nmstate-configuration` service is moved to the base directory in the cluster installation path so that any deployments configured with the `platform` parameter set to  `None`, do not impact the NMState `br-ex` network bridge creation feature. (link:https://issues.redhat.com/browse/OCPBUGS-48566[OCPBUGS-48566])

* Previously, for a layer 2 or layer 3 topology network that had the gateway mode set to `local`, OVN-Kubernetes experienced an issue when it was restarted. The issue caused the Egress IP to be selected as the primary IP address for the network. With this release, a fix ensures that this behavior no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-46585[OCPBUGS-46585])

* Previously, the DNS-based egress firewall incorrectly prevented creation of any firewall rule that contained a DNS name in uppercase characters. With this release, a fix to the egress firewall means that creation of a firewall rule that contains a DNS name in uppercase occurs. (link:https://issues.redhat.com/browse/OCPBUGS-46564[OCPBUGS-46564])

* Previously, when a pod was running on a node on which an egress on the IPv6 protocol was assigned, the pod was not able to communicate with the OVN-Kubernetes service in a dual-stack cluster. This resulted in the traffic with the IP address family, that the `egressIP` was not applicable to, being dropped. With this release, only the source network address translation (SNAT) for the IP address family that the egress IPs applied to is deleted, eliminating the risk of traffic being dropped. (link:https://issues.redhat.com/browse/OCPBUGS-46543[OCPBUGS-46543])

* Previously, when you used static IP addresses in the customized `br-ex` network bridge configuration of a manifest object, a race condition was added and caused a node reboot operation that further impacted deployment of a cluster. With this release, the `nodeip-configuration` service now starts after the `br-ex` network bridge is up, preventing the race condition and node reboot. (link:https://issues.redhat.com/browse/OCPBUGS-46072[OCPBUGS-46072])

* Previously, the HAProxy router incorrectly assumed that only SHA1 leaf certificates were rejected by HAProxy, causing the router to fail by not rejecting SHA1 intermediate certificates. With this update, the router now inspects and rejects all non-self-signed SHA1 certificates, thereby preventing crashes and improving stability for your cluster stability. (link:https://issues.redhat.com/browse/OCPBUGS-45290[OCPBUGS-45290])

* Previously, when a node restarted the `openvswitch` daemon, the `nmstate-handler` container could not access the OpenVSwitch (OVS) database and this caused all OVS-related NNCP configurations to fail. With this release, the issue is fixed. The `nmstate-handler` container can access the OVS database even after restarting the OVS process on the node. The `nmstate-handler` no longer requires a manual restart. (link:https://issues.redhat.com/browse/OCPBUGS-44596[OCPBUGS-44596])

* Previously, a `MultiNetworkPolicy` API was not enforced when the `protocol` parameter was specified, but the `port` parameter was not, in the cluster configuration. This situation caused all network traffic to reach the cluster. With this release, the `MultiNetworkPolicy` API policy only allows connections from and to the ports specified with the `protocol` parameter so that only specific traffic reaches the cluster. (link:https://issues.redhat.com/browse/OCPBUGS-44354[OCPBUGS-44354])

* Previously, HAProxy left idle connections open when it reloaded its configuration until either the next time a client sent a request by using the idle connection or the `hard-stop-after` period elapsed. This release adds a new `IdleConnectionTerminationPolicy` API field to control HAProxy behavior for idle connections during reloads. The new default setting is `Immediate`, which means that HAProxy immediately terminates any idle connections when it reloads its configuration. The previous behavior can be specified by using the `Deferred` setting for `IdleConnectionTerminationPolicy`. (link:https://issues.redhat.com/browse/OCPBUGS-43745[OCPBUGS-43745])

* Previously, if an application did not use the Path MTU discovery (PMTUD) mechanism while sending UDP packets larger than the network MTU, an issue with the `OVN` package caused the a packet to be dropped while fragmenting the packet. With this release, the `OVN` package has been fixed so that large UDP packets are properly fragmented and sent over the network. (link:https://issues.redhat.com/browse/OCPBUGS-43649[OCPBUGS-43649])

* Previously, a pod with a secondary interface in an OVN-Kubernetes `Localnet` network that was plugged into a `br-ex` interface bridge was out of reach by other pods on the same node, but used the default network for communication. The communication between pods on different nodes was not impacted. With this release, the communication between a `Localnet` pod and a default network pod running on the same node is possible, however the IP addresses that are used in the `Localnet` network must be within the same subnet as the host network. (link:https://issues.redhat.com/browse/OCPBUGS-43004[OCPBUGS-43004])

* Previously, when specific network changes were made to a running cluster, a `NetworkManager` connection profile was permanently created by the `ovs-configuration` service and the profile was incorrectly saved to storage. This profile file would persist through a reboot operation and caused the `ovs-configuration` service to fail. With this release, the `ovs-configuration` cleanup process is updated to remove any unnecessary files, preventing such files from causing issues after a reboot operation. (link:https://issues.redhat.com/browse/OCPBUGS-41489[OCPBUGS-41489])

* Previously, the `parseIPList` function failed to handle IP address lists that contained both valid and invalid IP addresses or CIDR ranges. This situation caused the function to return an empty string when it encountered an invalid entry and skip processing valid entries. With this release, the `haproxy.router.openshift.io/ip_allowlist` route annotation skips any invalid IP addresses or CIDR ranges so that the `parseIPList` function can process all listed entries. (link:https://issues.redhat.com/browse/OCPBUGS-39403[OCPBUGS-39403])

* Previously, the HAProxy router lacked out-of-bounds validation for the `router.openshift.io/haproxy.health.check.interval` annotation. If you set a value that exceeded the maximum value that the HAProxy router could handle, the `router-default` pod could not reach the `Ready` state. With this release, the router now validates the value for the annotation and excludes values that are out of bounds. The router now functions as expected. (link:https://issues.redhat.com/browse/OCPBUGS-38078[OCPBUGS-38078])

* Previously, in certain situations the gateway IP address for a node changed and caused the `OVN` cluster router, which manages the static route to the cluster subnet, to add a new static route with the new gateway IP address, without deleting the original one. As a result, a stale route still pointed to the switch subnet and this caused intermittent drops during egress traffic transfer. With this release, a patch applied to the `OVN` cluster router ensures that if the gateway IP address changes, the `OVN` cluster router updates the existing static route with the new gateway IP address. A stale route no longer points to the `OVN` cluster router so that egress traffic flow does not drop. (link:https://issues.redhat.com/browse/OCPBUGS-32754[OCPBUGS-32754])

* Previously, there was no event logged when an error occurred from a failed conversion from ingress to route. With this update, an error for a failed conversion is logged. (link:https://issues.redhat.com/browse/OCPBUGS-29354[OCPBUGS-29354])

* Previously, the PowerVS installer used a hard-coded list of supported machine types. However, this list was not always updated as new types were added. With this release, datacenter is queried to get the current list of supported types. (link:https://issues.redhat.com/browse/OCPBUGS-49940[OCPBUGS-49940])

* Previously, when a RootDiskHint was defined and the installation failed with the error `Requested installation disk is not part of the host's valid disks`, it was difficult to determine the valid disk names that could be used as the hint. With this release, logging was added for the list of acceptable disks so that a user can quickly determine what the root disk hint should be. (link:https://issues.redhat.com/browse/OCPBUGS-43578[OCPBUGS-43578])

* Previously, the `oc adm node-image monitor` command returned an EOF error when there was an API server interruption or temporary connection issue. This caused the command to terminate. With this release, the command now detects API server interruptions and temporary connection issues and reconnects to the API server without terminating the command. (link:https://issues.redhat.com/browse/OCPBUGS-38975[OCPBUGS-38975])

* Previously, when you created a virtual machine (VM) and no IP addresses existed in the IP pool, the VM did not start. An error message was generated in the `virt-launcher-<vm_name>` pod, but the message was unclear as to the source of the issue. With this release, for this situation where no IP addresses exist in the IP pool, the `virt-launcher-<vm-name>` pod includes a clear error message that is similar to the following example:
+
[source,terminal]
----
Warning ErrorAllocatingPod 4s (x7 over 79s)  ovnk-controlplane  failed to update pod localnet-ipam/virt-launcher-vmb-localnet-ipam-hlnmf: failed to assign pod addresses for localnet-ipam/ipam-localnet-nad/localnet-ipam/virt-launcher-vmb-localnet-ipam-hlnmf: failed to allocate new IPs for tenantblue-network: subnet address pool exhausted
----
+
(link:https://issues.redhat.com/browse/OCPBUGS-54245[OCPBUGS-54245])


[id="ocp-release-note-node-bug-fixes_{context}"]
=== Node

* Previously, if your cluster used Zscaler and scanned all transfers, it could experience a timeout when pulling images. This problem was due to a hard-coded timeout value for image pulls. The pull progress timeout of CRI-O is now increased to 30 seconds. As a result, previously affected clusters should not experience timeouts. (link:https://issues.redhat.com/browse/OCPBUGS-54662[OCPBUGS-54662])

* Previously, containers that used the `container_logreader_t` SELinux domain to watch container logs on the host at the `/var/log` location could not access the logs. This behavior occurred because the logs in the `var/log/containers` location were symbolic links. With this fix, containers can watch logs as expected.  (link:https://issues.redhat.com/browse/OCPBUGS-48555[OCPBUGS-48555])

* Previously, an end-of-file error occurred in the `json.NewDecoder` file when the file was in a loop operation. This error caused inconsistent application updates to namespaced policies that existed in multiple namespaces. This issue could potential cause security vulnerabilities for the cluster. With this release, a new policy buffer is added to the `json.NewDecoder` file when it enters each loop operation, and a test case is added for multiple namespaces. As a result, the policy buffer provides a robust decoding process for JSON policy files so that namespace policies receive updates without any issues. (link:https://issues.redhat.com/browse/OCPBUGS-48195[OCPBUGS-48195])

* Previously, there was an issue in the image reference digest calculation that led to failed container creation based on the `schemaVersion 1` image. This issue prevented new deployment creations. With this release, the image digest calculation has been fixed and new operators can be installed. (link:https://issues.redhat.com/browse/OCPBUGS-42844[OCPBUGS-42844])

* Previously, for a Technology Preview-enabled cluster with Sigstore verification for payload images in the `policy.json` file, the Podman version in the base image did not support Sigstore configuration. This lack of support caused the new node to be unavailable. With this release, the issue is fixed and the node is available. (link:https://issues.redhat.com/browse/OCPBUGS-38809[OCPBUGS-38809])

* Previously, CPUs for the last guaranteed pod admitted to a node remained allocated after the pod was deleted. This behavior caused scheduling domain inconsistencies. With this release, CPUs that are allocated to guaranteed pods return to the pool of available CPU resources as expected, ensuring correct CPU scheduling for subsequent pods. (link:https://issues.redhat.com/browse/OCPBUGS-17792[OCPBUGS-17792])


[id="ocp-release-note-node-tuning-operator-bug-fixes_{context}"]
=== Node Tuning Operator (NTO)

* Previously, when applying a performance profile to a node, {product-title} selected the appropriate profile based on the vendor identifier of the CPU unit on the node. Because of this, if a CPU uses a different vendor identifier that is not recognized, {product-title} failed to include the proper profile. For example the identifier might include APM, rather than ARM. With this fix, for CPUs that use the ARM architecture, the Operator now selects a profile based only on the architecture, and not the vendor identifier. As a result, the correct profile is applied. (link:https://issues.redhat.com/browse/OCPBUGS-52352[OCPBUGS-52352])


[id="ocp-release-note-observability-bug-fixes_{context}"]
=== Observability

* Previously, the *Silence details* page had an incorrect link URL that was missing the `namespace` parameter, which caused users to be unable to silence specific alerts in the dev console for certain versions. This resulted in poor alert management. With this release, the undefined link in `SilencedAlertsList` has been fixed using the active namespace. As a result, the 'No Alert found' error is now resolved, and the *Alert details* page in {product-title} Monitoring is navigated to correctly. (link:https://issues.redhat.com/browse/OCPBUGS-48142[OCPBUGS-48142])

* Previously, console updates deprecated PatternFly 4, rendering an incorrect layout of the monitoring plug-in table rendered. With this release, the tables and styles are upgraded to PatternFly 5 and are rendered correctly. (link:https://issues.redhat.com/browse/OCPBUGS-47535[OCPBUGS-47535])

* Previously, a namespace was passed to a full cluster query on the alerts graph, and this caused the tenancy API path to be used. The API lacked permissions to retrieve data so no data was shown on the alerts graph. With this release, the namespace is no longer passed to a full cluster query for an alert graph. A non-tenancy API path is now used because this API has the correct permissions to retrieve data. Data is not available on an alert graph. (link:https://issues.redhat.com/browse/OCPBUGS-45896[OCPBUGS-45896])

* Previously, a {rh-rhacm-first} Alerting UI refactor update caused an `isEmpty` check to go missing on the *Observe > Metrics* menu. The missing check inverted the behavior of the *Show all Series* and *Hide all Series* states. This release readds `isEmpty` check so that *Show all Series* is now visible when series are hidden and *Hide all Series* is now visible when the series are shown. (link:https://issues.redhat.com/browse/OCPBUGS-45816[OCPBUGS-45816])

* Previously, on the *Observe -> Alerting -> Silences* tab, the `DateTime` component changed the ordering of an event and its value. Because of this issue, you could not edit the `until` parameter for a silent alert in the web console. With this release, a fix means to the `DateTime` component means that you can now edit the `until` parameter for a silent alert. (link:https://issues.redhat.com/browse/OCPBUGS-45801[OCPBUGS-45801])

* Previously, bounds were based on the first bar in a bar chart. If a bar was larger in size than the first bar, the bar would extend beyond the bar chart boundary. With this release, the bound for a bar chart is based on the largest bar, so no bars extend outside the boundary of a bar chart. (link:https://issues.redhat.com/browse/OCPBUGS-45174[OCPBUGS-45174])


[id="ocp-release-note-oc-mirror-bug-fixes_{context}"]
=== oc-mirror

* Previously, oc-mirror plugin v2 did not display any progress output during the local cache population phase. For mirror configurations involving a large number of images, this could make the process appear unresponsive or stuck.
With this update, a progress bar has been added to provide the cache population status, enabling the user to see the current progress of the cache population. (link:https://issues.redhat.com/browse/OCPBUGS-56563[OCPBUGS-56563])

* Previously, when mirroring Operators using oc-mirror plugin v2, some community Operators with long lists of `skips` and `replaces` entries in their channel graphs caused the mirroring process to run out of memory and fail. With this update, oc-mirror plugin v2 improves the filtering logic by avoiding repeated evaluation of entries referenced in multiple `skips` and `replaces` stanzas, resulting in better memory handling during Operator mirroring. (link:https://issues.redhat.com/browse/OCPBUGS-52471[OCPBUGS-52471])

* Previously, when re-running oc-mirror plugin v2 with the same working directory, existing `tar` archive files from previous runs were not removed. This resulted in a mix of outdated and new archives, which could cause mirroring failures when pushing to the target registry. With this update, oc-mirror plugin v2 automatically deletes old `tar` archive files at the beginning of each run, ensuring that the working directory only contains archives from the current execution (link:https://issues.redhat.com/browse/OCPBUGS-56433[OCPBUGS-56433])

* Previously, oc-mirror plugin v2 would terminate with an error if the source registry responded with any of the following HTTP status codes during image copying: 502, 503, 504. With this update, oc-mirror plugin v2 automatically retries the copy operation when encountering these temporary server errors. (link:https://issues.redhat.com/browse/OCPBUGS-56185[OCPBUGS-56185])

* Previously, when mirroring a Helm chart that included container images with both a tag and digest in the reference, oc-mirror plugin v2 failed with the following error:
+
[source,text]
----
Docker references with both a tag and digest are currently not supported.
----
+
With this update, oc-mirror plugin v2 supports Helm charts that reference images using both tag and digest. The tool mirrors the image using the digest as the source and applies the tag at the destination. (link:https://issues.redhat.com/browse/OCPBUGS-54891[OCPBUGS-54891])

* Previously, during image cleanup, oc-mirror plugin v2 would stop the deletion process if any error occurred while removing an image. With this release, oc-mirror plugin v2 continues attempting to delete remaining images even after encountering errors. After the process completes, it displays a list of any failed deletions. (link:https://issues.redhat.com/browse/OCPBUGS-54653[OCPBUGS-54653])

* Previously, it was possible to mirror an empty catalog during the mirror-to-disk (m2d) phase if an invalid Operator was specified in the `ImageSetConfiguration` file. This led to a failure during the subsequent disk-to-mirror (d2m) phase. With this release, oc-mirror plugin v2 prevents mirroring empty catalogs by validating operator references in the configuration, ensuring a more reliable mirroring process. (link:https://issues.redhat.com/browse/OCPBUGS-52588[OCPBUGS-52588])

* Previously, when using oc-mirror plugin v2 with the `--dry-run` flag, the `cluster-resources` folder inside the working directory was cleared. As a result, previously generated files such as `idms-oc-mirror.yaml` and `itms-oc-mirror.yaml` were deleted. With this release, the `cluster-resources` folder is no longer cleared during dry-run operations, preserving any previously generated configuration files. (link:https://issues.redhat.com/browse/OCPBUGS-50963[OCPBUGS-50963])

* Previously, the oc-mirror plugin v2 returned an exit status of `0` (success) even when mirroring errors occurred. As a result, oc-mirror plugin v2 run failures in automated workflows could go undetected. With this release, oc-mirror plugin v2 has been updated to return a non -`0` exit status when mirror failures occur. Despite this fix, users should not rely solely on the exit status in automated workflows. Users are advised to manually check the `mirroring_errors_XXX_XXX.txt` file generated by oc-mirror plugin v2 to identify any potential issues. (link:https://issues.redhat.com/browse/OCPBUGS-49880[OCPBUGS-49880])

* Previously, when mirroring using an internal oc-mirror reserved keywordsuch as `release-images`in the destination or `--from` path flags, the operation could fail or behave unexpectedly. With this release, oc-mirror plugin v2 correctly handles reserved keywords used in destination or source paths. (link:https://issues.redhat.com/browse/OCPBUGS-42862[OCPBUGS-42862])


[id="ocp-release-note-oc-cli-bug-fixes_{context}"]
=== OpenShift CLI (oc)

* Previously, if you tried to add a node to a disconnected environment using the `oc adm node-image` command, private registry images were inaccessible to the command, causing node addition failure. This error only occurred if the cluster was initially installed with an installer binary downloaded from (link:https://mirror.openshift.com/pub/[mirror.openshift.com]). With this release, a fix has been implemented that enables successful image pull and node creation in disconnected environments. (link:https://issues.redhat.com/browse/OCPBUGS-53106[OCPBUGS-53106])

* For clusters that were installed with the Agent-based Installer for versions 4.15.0 to 4.15.26, root certificates that were built in from CoreOS were added to the user-ca-bundle, even though they were not explicitly specified by the user. In previous releases, when adding a node to one of these clusters using the `oc adm node-image create` command, the `additionalTrustBundle` obtained from the cluster's user-ca-bundle was too large to process, resulting in a failure to add the node. With this release, the built-in certificates are filtered out when generating the `additionalTrustBundle`, so that only explicitly user-configured certificates are included, and nodes can be added successfully. (link:https://issues.redhat.com/browse/OCPBUGS-43990[OCPBUGS-43990])

* Previously, a bug on `oc adm inspect --all-namespaces` command construction meant that must-gather was not correctly gathering information about leases, `csistoragecapacities`, and the assisted-installer namespace. With this release, the issue is fixed and must-gather will gather the information correctly. (link:https://issues.redhat.com/browse/OCPBUGS-44857[OCPBUGS-44857])

* Previously, the `oc adm node-image create --pxe generated` command did not create only the Preboot Execution Environment (PXE) artifacts. Instead, the command created the PXE artifacts with other artifacts from a `node-joiner` pod and stored them all in the wrong subdirectory. Additionally, the PXE artifacts were incorrectly prefixed with `agent` instead of `node`. With this release, generated PXE artifacts are stored in the correct directory and receive the correct prefix. (link:https://issues.redhat.com/browse/OCPBUGS-45311[OCPBUGS-45311])


[id="ocp-release-note-olm-bug-fixes_{context}"]
=== Operator Lifecycle Manager (OLM)

* Previously, if an Operator did not have the required `olm.managed=true` label, the Operator might fail and enter a `CrashLoopBackOff` state. When this happened, the logs did not report the status as an error. As a result, the failure was difficult to diagnose. With this update, this type of failure is reported as an error. (link:https://issues.redhat.com/browse/OCPBUGS-56034[OCPBUGS-56034])

* Previously, the Machine Config Operator (MCO) did not search the `/etc/docker/certs.d` directory for certificates required to mount images. As a result, Operator Controller and catalogd failed to start because they did not have access to the certificates hosted in this directory. With this update, the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-54175[OCPBUGS-54175])

* Before this release, cluster extension updates sometimes failed with the following error from the `CRDUpgradeCheck` resource: `unknown change, refusing to determine that change is safe`. This error occurred due to the way that {olmv1} calculated the difference between version schemas. This update fixes the issue. (link:https://issues.redhat.com/browse/OCPBUGS-53019[OCPBUGS-53019])

* Previously, Operator Controller sometimes failed to mount CA certificates properly. As a result, the Operator Controller failed connect to catalogd due to a TLS certificate validation error. This update fixes the issue. (link:https://issues.redhat.com/browse/OCPBUGS-49860[OCPBUGS-49860])

* Previously, {olmv1} did not wait for certificates to reach a ready state before mounting Operator Controller and catalogd pods. These updates fix the issue. link:https://issues.redhat.com/browse/OCPBUGS-48830[OCPBUGS-48830] and (link:https://issues.redhat.com/browse/OCPBUGS-49418[OCPBUGS-49418])

* Previously, {olmv1} did not apply all of the metadata provided by cluster extension authors in Operator bundles. As a result, {olmv1} did not apply properties, such as update constraints, that were specified in the `metadata/properties.yaml` file. This update fixes the issue. (link:https://issues.redhat.com/browse/OCPBUGS-44808[OCPBUGS-44808])


[id="ocp-release-note-operator-controller-manager-bug-fixes_{context}"]
=== Operator Controller Manager

* Previously, the `HTTP_PROXY`, `http_proxy`, `HTTPS_PROXY`, `https_proxy`, `NO_PROXY`, and `no_proxy` variables were set on build containers regardless of default proxy settings. With this release, the variables are only added if they are defined in defaults and are not null. (link:https://issues.redhat.com/browse/OCPBUGS-55642[OCPBUGS-55642])

* Previously, an image pull secret generated for the internal Image Registry would not be regenerated until after the embedded credentials had expired This resulted in a small period of time in which the image pull secrets were invalid. With this release, the image pull secrets are refreshed before the embedded credentials have expired. (link:https://issues.redhat.com/browse/OCPBUGS-50507[OCPBUGS-50507])

* Previously, {olmv1} did not search the `/etc/docker/` directory for the certificates required to mount images. As a result, {olmv1} failed to mount custom certificates. This update fixes the issue. (link:https://issues.redhat.com/browse/OCPBUGS-48795[OCPBUGS-48795])

* Previously, {olmv1} would send an error message during temporary outages that occur during routine cluster maintenance, such as leader election. This update fixes the issue. (link:https://issues.redhat.com/browse/OCPBUGS-48765[OCPBUGS-48765])

* Previously, {olmv0-first} incorrectly reported failures to `Subscription` resources during concurrent attempts to reconcile Operators in the same namespace. When this occurred, Operators failed to install. This update fixes the issue. (link:https://issues.redhat.com/browse/OCPBUGS-48486[OCPBUGS-48486])

* Previously, {olmv0} took a snapshot of the catalog source for every installed Operator when it reconciled a subscription. This behavior resulted in high CPU usage. With this update, {olmv0} caches catalog sources and limits calls to the gRPC Remote Procedure Calls (gRPC) server to resolve the issue. (link:https://issues.redhat.com/browse/OCPBUGS-48468[OCPBUGS-48468])


[id="ocp-release-note-pao-bug-fixes_{context}"]
=== Performance Addon Operator

* Previously, if you specified a long string of isolated CPUs in a performance profile, such as `0,1,2,...,512`, the `tuned`, Machine Config Operator and `rpm-ostree` components failed to process the string as expected. As a consequence, after you applied the performance profile, the expected kernel arguments were missing. The system failed silently with no reported errors. With this release, the string for isolated CPUs in a performance profile is converted to sequential ranges, such as `0-512`. As a result, the kernel arguments are applied as expected in most scenarios. (link:https://issues.redhat.com/browse/OCPBUGS-45264[OCPBUGS-45264])
+
[NOTE]
====
The issue might still occur with some combinations of input for isolated CPUs in a performance profile, such as a long list of odd numbers `1,3,5,...,511`.
====

* Previously, the Performance Profile Creator (PPC) failed to build a performance profile for compute nodes that had different core ID numbering (core per socket) for their logical processors and the nodes existed under the same node pool. For example, the PPC failed in a situation for two compute nodes that have logical processors `2` and `18`, where one node groups them as core ID `2` and the other node groups them as core ID `9`.
+
With this release, PPC no longer fails to create the performance profile because PPC can now build a performance profile for a cluster that has compute nodes that each have different core ID numbering for their logical processors. The PPC now outputs a warning message that indicates to use the generated performance profile with caution, because different core ID numbering might impact system optimization and isolated management of tasks. (link:https://issues.redhat.com/browse/OCPBUGS-44372[OCPBUGS-44372])


[id="ocp-release-note-samples-operator-bug-fixes_{context}"]
=== Samples Operator

* Previously, the Samples Operator updated the `lastTransitionTime` spec in the `Progressing` condition even when the condition did not change. This made the Operator show as less stable than it was. With this release, the `lastTransitionTime` spec updates only when the `Progressing` condition changes. (link:https://issues.redhat.com/browse/OCPBUGS-54591[OCPBUGS-54591])

* Previously, unsorted image stream names in the `Progressing` condition caused unnecessary updates. This caused excessive user updates and reduced system performance. With this release, the `activeImageStreams` function sorts falling image imports. This action improves Cluster Samples Operator efficiency, reduces unnecessary updates, and enhances overall performance. (link:https://issues.redhat.com/browse/OCPBUGS-54590[OCPBUGS-54590])

* Previously, the Samples Operator established a watch for all cluster Operators, which caused the sync loop of the Samples Operator to run when any Operator changed. With this release, the Samples Operator watches only the Operators that it needs to monitor. (link:https://issues.redhat.com/browse/OCPBUGS-54589[OCPBUGS-54589])


[id="ocp-release-note-storage-bug-fixes_{context}"]
=== Storage

* Previously, using the `oc adm top pvc` command would not show usage statistics for persistent volume claims (PVCs) for clusters with restricted network configurations, such as clusters with a proxy or clusters in a disconnected environment. With this release, usage statistics can be acquired for clusters in these environments. (link:https://issues.redhat.com/browse/OCPBUGS-54168[OCPBUGS-54168])

* Previously, the VMware vSphere CSI driver Operator entered panic mode if the vCenter address was incorrect. With this release, the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-43273[OCPBUGS-43273])

* Previously, a {gcp-first} Persistent Disk cluster with C3-standard-2, C3-standard-4, N4-standard-2, and N4-standard-4 nodes could erroneously exceed the maximum attachable disk number, which should be 16, which could prevent you from successfully creating or attaching volumes to your pods. With this release, the maximum is not exceeded, and thus does not interfere with successfully creating or attaching volumes to your pods.  (link:https://issues.redhat.com/browse/OCPBUGS-39258[OCPBUGS-39258])

* Previously, when persistent volumes (PVs) are deleted, the Local Storage Operator (LSO) did not reliably recreate the symlinks. With this release, when creating PVs, previously specified symlinks are picked before finding new symlinks.  (link:https://issues.redhat.com/browse/OCPBUGS-31059[OCPBUGS-31059])

* Previously, when the Cloud Credential Operator (CCO) did not provide credentials for Container Storage Interface (CSI) driver Operators, the CSI driver Operators remain in `Progressing=true` indefinitely, with a message indicating the `operator is waiting for deployment/unavailable`. With this release, when progressing state lasts 15 minutes or longer, the Operator changes to `Degraded=True`. (link:https://issues.redhat.com/browse/OCPBUGS-24588[OCPBUGS-24588])

* Previously, compute nodes with names that are 53 characters long and when using hostpath Container Storage Interface (CSI) driver cause volume provisioning to fail when using `--enable-node-deployment flag` on external-provisioner. With this release, this issue is resolved and there is no restriction on compute node name lengths.  (link:https://issues.redhat.com/browse/OCPBUGS-49805[OCPBUGS-49805])

* Previously, on Azure Red Hat OpenShift when using {hcp} to create hosted clusters, the Azure Disk Container Storage Interface (CSI) driver would not provision volumes successfully. With this release, this issue has been resolved, and the Azure Disk CSI driver can provision volumes successfully. (link:https://issues.redhat.com/browse/OCPBUGS-46575[OCPBUGS-46575])

* Previously, Internet Small Computer System Interface (iSCSI) and Fibre Channel devices that were attached to a multipath device did not resolve correctly when these devices were partitioned. With this relase, a fix ensures that partitioned multipath storage devices can now correctly resolve. (link:https://issues.redhat.com/browse/OCPBUGS-46038[OCPBUGS-46038])

* Previously, when creating a hosted cluster with specified labels, the AWS EBS driver, Driver Operator, snapshot controller, and snapshot webhook pods do not get theses specified labels propagated to them. With this release, the specified labels are propagated. (link:https://issues.redhat.com/browse/OCPBUGS-45073[OCPBUGS-45073])

* Previously, the Manila Container Storage Interface (CSI) driver had services running on unintended hosts. This occurred because the Manila CSI driver uses a single binary for both the controller and node (worker) services.  With this release, the CSI driver controller pods run only controller services, and CSI driver node pods run only node services. (https://issues.redhat.com/browse/OCPBUGS-54447[OCPBUGS-54447])

* Previously, the Container Storage Interface (CSI) Operator was issuing warnings in the log about missing items that would become fatal in the future. With this release, the warnings are no longer issued. (link:https://issues.redhat.com/browse/OCPBUGS-44374[OCPBUGS-44374])

* Previously, the VMWare vSphere CSI driver Operator would panic if the vCenter address was incorrect. With this release, the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-43273[OCPBUGS-43273])


[id="ocp-release-note-rhcos-bug-fixes_{context}"]
=== {op-system-first}

* Previously, the `GRUB` bootloader was not automatically updated on {op-system} nodes. As a result, when nodes were created on {op-system-base} 8 and were subsequently updated to {op-system-base}, `GRUB` could not load the kernel as it uses a format that is not supported by older `GRUB` versions. With this release, a `GRUB` bootloader update is forced on nodes during updates to {product-title} 4.18 so that the issue does not occur on {product-title} {product-version}. (link:https://issues.redhat.com/browse/OCPBUGS-55144[OCPBUGS-55144])

[id="ocp-4-19-technology-preview-tables_{context}"]
== Technology Preview features status

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red{nbsp}Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the following tables, features are marked with the following statuses:

* _Not Available_
* _Technology Preview_
* _General Availability_
* _Deprecated_
* _Removed_



[id="ocp-release-notes-auth-tech-preview_{context}"]
=== Authentication and authorization Technology Preview features

.Authentication and authorization Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Pod security admission restricted enforcement
|Technology Preview
|Technology Preview
|Technology Preview

|Direct authentication with an external OIDC identity provider
|Not Available
|Not Available
|Technology Preview

|====


[id="ocp-release-notesedge-computing-tp-features_{context}"]
=== Edge computing Technology Preview features

.Edge computing Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Accelerated provisioning of {ztp}
|Technology Preview
|Technology Preview
|Technology Preview

|Enabling disk encryption with TPM and PCR protection
|Technology Preview
|Technology Preview
|Technology Preview

|Configuring a local arbiter node
|Not Available
|Not Available
|Technology Preview
|====


[id="ocp-release-notes-extensions-tech-preview_{context}"]
=== Extensions Technology Preview features

// "Extensions" refers to OLMv1

.Extensions Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{olmv1-first}
|Technology Preview
|General Availability
|General Availability

|{olmv1} runtime validation of container images using sigstore signatures
|Not Available
|Technology Preview
|Technology Preview

|{olmv1} permissions preflight check for cluster extensions
|Not Available
|Not Available
|Technology Preview

|{olmv1} deploying a cluster extension in a specified namespace
|Not Available
|Not Available
|Technology Preview
|====


[id="ocp-release-notes-installing-tech-preview_{context}"]
=== Installation Technology Preview features

.Installation Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

// All GA in 4.17 notes for oci-first
|Adding kernel modules to nodes with kvc
|Technology Preview
|Technology Preview
|Technology Preview

|Enabling NIC partitioning for SR-IOV devices
|General Availability
|General Availability
|General Availability

|User-defined labels and tags for {gcp-first}
|General Availability
|General Availability
|General Availability

|Installing a cluster on Alibaba Cloud by using Assisted Installer
|Technology Preview
|Technology Preview
|Technology Preview

|Installing a cluster on {azure-first} with confidential VMs
|Not Available
|Technology Preview
|General Availability

|Mount shared entitlements in BuildConfigs in RHEL
|Technology Preview
|Technology Preview
|Technology Preview

|OpenShift zones support for vSphere host groups
|Not Available
|Not Available
|Technology Preview

|Selectable Cluster Inventory
|Technology Preview
|Technology Preview
|Technology Preview

|Installing a cluster on {gcp-short} using the Cluster API implementation
|General Availability
|General Availability
|General Availability

|Enabling a user-provisioned DNS on {gcp-short}
|Not Available
|Not Available
|Technology Preview

|Installing a cluster on {vmw-full} with multiple network interface controllers
|Not Available
|Technology Preview
|Technology Preview

|Using bare metal as a service
|Not Available
|Not Available
|Technology Preview
|====


[id="ocp-release-notes-mco-tech-preview_{context}"]
=== Machine Config Operator Technology Preview features

.Machine Config Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Improved MCO state reporting (`oc get machineconfignode`)
|Technology Preview
|Technology Preview
|General Availability

|Image mode for OpenShift/On-cluster RHCOS image layering
|Technology Preview
|General Availability ^1^
|General Availability

|Pinned Image Sets
|Technology Preview
|Technology Preview
|General Availability ^2^

|====
[.small]
. This feature is GA starting in {product-title} 4.18.20. Earlier 4.18.x versions remain in Technology Preview.
. This feature is GA starting in {product-title} 4.19.12. Earlier 4.19.x versions remain in Technology Preview.

[id="ocp-release-notes-machine-management-tech-preview_{context}"]
=== Machine management Technology Preview features

.Machine management Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Managing machines with the Cluster API for {aws-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {gcp-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {ibm-power-server-name}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {azure-full}
|Not Available
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {rh-openstack}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {vmw-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for bare metal
|Not Available
|Not Available
|Technology Preview

|Cloud controller manager for {ibm-power-server-name}
|Technology Preview
|Technology Preview
|Technology Preview

|Adding multiple subnets to an existing {vmw-full} cluster by using compute machine sets
|Not Available
|Technology Preview
|Technology Preview

|Configuring Trusted Launch for {azure-full} virtual machines by using machine sets
|Technology Preview
|Technology Preview
|General Availability

|Configuring {azure-short} confidential virtual machines by using machine sets
|Technology Preview
|Technology Preview
|General Availability
|====


[id="ocp-release-notes-monitoring-tech-preview_{context}"]
=== Monitoring Technology Preview features

.Monitoring Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Metrics Collection Profiles
|Technology Preview
|Technology Preview
|General Availability

|====


[id="ocp-release-notes-multi-arch-tech-preview_{context}"]
=== Multi-Architecture Technology Preview features

.Multi-Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`kdump` on `arm64` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `s390x` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `ppc64le` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|Support for configuring the image stream import mode behavior
|Not Available
|Technology Preview
|Technology Preview
|====


[id="ocp-release-notes-networking-tech-preview_{context}"]
=== Networking Technology Preview features

.Networking Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|eBPF manager Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Advertise using L2 mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Technology Preview
|Technology Preview
|Technology Preview

|Updating the interface-specific safe sysctls list
|Technology Preview
|Technology Preview
|Technology Preview

|Egress service custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|VRF specification in `BGPPeer` custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|VRF specification in `NodeNetworkConfigurationPolicy` custom resource
|Technology Preview
|Technology Preview
|General Availability

|Host network settings for SR-IOV VFs
|General Availability
|General Availability
|General Availability

|Integration of MetalLB and FRR-K8s
|General Availability
|General Availability
|General Availability

|Automatic leap seconds handling for PTP grandmaster clocks
|General Availability
|General Availability
|General Availability

|PTP events REST API v2
|General Availability
|General Availability
|General Availability

|OVN-Kubernetes customized `br-ex` bridge on bare metal
|General Availability
|General Availability
|General Availability

|OVN-Kubernetes customized `br-ex` bridge on {vmw-short} and {rh-openstack}
|Technology Preview
|Technology Preview
|Technology Preview

|Live migration to OVN-Kubernetes from OpenShift SDN
|General Availability
|Not Available
|Not Available

|User-defined network segmentation
|Technology Preview
|General Availability
|General Availability

|Dynamic configuration manager
|Not Available
|Technology Preview
|Technology Preview

|SR-IOV Network Operator support for Intel C741 Emmitsburg Chipset
|Not Available
|Technology Preview
|Technology Preview

|SR-IOV Network Operator support on ARM architecture
|Not Available
|General Availability
|General Availability

|Gateway API and Istio for Ingress management
|Not Available
|Technology Preview
|General Availability

|Dual-port NIC for PTP ordinary clock
|Not Available
|Not Available
|Technology Preview

|DPU Operator
|Not Available
|Not Available
|Technology Preview

|Fast IPAM for the Whereabouts IPAM CNI plugin
|Not Available
|Not Available
|Technology Preview

|Unnumbered BGP peering
|Not Available
|Not Available
|Technology Preview
|====


[id="ocp-release-notes-nodes-tech-preview_{context}"]
=== Node Technology Preview features

.Nodes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`MaxUnavailableStatefulSet` featureset
|Technology Preview
|Technology Preview
|Technology Preview

|sigstore support
|Technology Preview
|Technology Preview
|Technology Preview

|====


[id="ocp-release-notes-oc-cli-tech-preview_{context}"]
=== OpenShift CLI (oc) Technology Preview features

.OpenShift CLI (`oc`) Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|oc-mirror plugin v2
|Technology Preview
|General Availability
|General Availability

|oc-mirror plugin v2 enclave support
|Technology Preview
|General Availability
|General Availability

|oc-mirror plugin v2 delete functionality
|Technology Preview
|General Availability
|General Availability
|====


[id="ocp-release-notes-operator-lifecycle-tech-preview_{context}"]
=== Operator lifecycle and development Technology Preview features

// "Operator lifecycle" refers to OLMv0 and "development" refers to Operator SDK

.Operator lifecycle and development Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{olmv1-first}
|Technology Preview
|General Availability
|General Availability

|Scaffolding tools for Hybrid Helm-based Operator projects
|Deprecated
|Removed
|Removed

|Scaffolding tools for Java-based Operator projects
|Deprecated
|Removed
|Removed
|====


[id="ocp-release-notes-rhcos-tech-preview_{context}"]
=== {rh-openstack-first} Technology Preview features

.{rh-openstack} Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{rh-openstack} integration into the {cluster-capi-operator}
|Technology Preview
|Technology Preview
|Technology Preview

|Control plane with `rootVolumes` and `etcd` on local disk
|General Availability
|General Availability
|General Availability

|Hosted control planes on {rh-openstack} 17.1
|Not Available
|Not Available
|Technology Preview
|====


[id="ocp-release-notes-scalability-tech-preview_{context}"]
=== Scalability and performance Technology Preview features

.Scalability and performance Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{factory-prestaging-tool}
|Technology Preview
|Technology Preview
|Technology Preview

|Hyperthreading-aware CPU manager policy
|Technology Preview
|Technology Preview
|Technology Preview

|Mount namespace encapsulation
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Increasing the etcd database size
|Technology Preview
|Technology Preview
|Technology Preview

|Using {rh-rhacm} `PolicyGenerator` resources to manage {ztp} cluster policies
|Technology Preview
|Technology Preview
|General Availability

|NUMA-aware scheduling supported on {hcp}
|Not Available
|Not Available
|Technology Preview
|====


[id="ocp-release-notes-storage-tech-preview_{context}"]
=== Storage Technology Preview features

.Storage Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|AWS EFS storage CSI usage metrics
|General Availability
|General Availability
|General Availability

|Automatic device discovery and provisioning with Local Storage Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Azure File CSI snapshot support
|Technology Preview
|Technology Preview
|Technology Preview

|Azure File cross-subscription support
|Not Available
|Not Available
|General Availability

|Shared Resources CSI Driver in OpenShift Builds
|Technology Preview
|Technology Preview
|Technology Preview

|{secrets-store-operator}
|Technology Preview
|General Availability
|General Availability

|CIFS/SMB CSI Driver Operator
|Technology Preview
|General Availability
|General Availability

|VMware vSphere multiple vCenter support
|Technology Preview
|Technology Preview
|General Availability

|Disabling/enabling storage on vSphere
|Technology Preview
|Technology Preview
|General Availability

|Increasing max number of volumes per node for vSphere
|Not Available
|Not Available
|Technology Preview

|RWX/RWO SELinux Mount
|Developer Preview
|Developer Preview
|Developer Preview

|Migrating CNS Volumes Between Datastores
|Developer Preview
|Developer Preview
|General Availability

|CSI volume group snapshots
|Not Available
|Technology Preview
|Technology Preview

|GCP PD supports C3/N4 instance types and hyperdisk-balanced disks
|Not Available
|General Availability
|General Availability

|GCP Filestore supports Workload Identity
|General Availability
|General Availability
|General Availability

|OpenStack Manila support for CSI resize
|Not Available
|General Availability
|General Availability

|Volume Attribute Classes
|Not Available
|Not Available
|Technology Preview
|====


[id="ocp-release-notes-web-console-tech-preview_{context}"]
=== Web console Technology Preview features

.Web console Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{ols-official} in the {product-title} web console
|Technology Preview
|Technology Preview
|Technology Preview
|====

[id="ocp-4-19-known-issues_{context}"]
== Known issues

* In {product-title} {product-version}, clusters using IPsec for network encryption might experience intermittent loss of pod-to-pod connectivity. This prevents some pods on certain nodes from reaching services on other nodes, resulting in connection timeouts. Internal testing could not reproduce this issue on clusters with 120 nodes or less. There is no workaround for this issue. (link:https://issues.redhat.com/browse/OCPBUGS-55453[OCPBUGS-55453])

* {product-title} clusters that are installed on {aws-short} in the Mexico Central region, `mx-central-1`, cannot be destroyed. (link:https://issues.redhat.com/browse/OCPBUGS-56020[OCPBUGS-56020])

* When installing a cluster on {azure-short}, if you set any of the `compute.platform.azure.identity.type`, `controlplane.platform.azure.identity.type`, or `platform.azure.defaultMachinePlatform.identity.type` field values to `None`, your cluster is unable to pull images from the Azure Container Registry. You can avoid this issue by either providing a user-assigned identity, or by leaving the identity field blank. In both cases, the installation program generates a user-assigned identity. (link:https://issues.redhat.com/browse/OCPBUGS-56008[OCPBUGS-56008])

* Previously, the kubelet would not account for probes that ran in the `syncPod` method, which periodically checks the state of a pod and does a readiness probe outside of the normal probe period. With this release, a bug is fixed for when the kubelet incorrectly calculates `readinessProbe` periods. However, pod authors might see that the readiness latency of pods configured with readiness probes might increase. This behavior is more accurate to the configured probe. For more information, see (link:https://issues.redhat.com/browse/OCPBUGS-50522[OCPBUGS-50522])

* When installing a cluster on {aws-short}, if you do not configure {aws-short} credentials before running any `openshift-install create` command, the installation program fails. (link:https://issues.redhat.com/browse/OCPBUGS-56658[OCPBUGS-56658])

* When installing a cluster on {aws-short}, if you do not configure {aws-short} credentials before running any `openshift-install create` command, the installation program fails. (link:https://issues.redhat.com/browse/OCPBUGS-56658[OCPBUGS-56658])

* The `must-gather` tool does not collect IPsec information for a cluster that was upgraded from {product-title} 4.14. This issue occurs because the `ipsecConfig` configuration in the `networks.operator.openshift.io cluster` CR has an empty construct, `{}`. The empty construct is passed to the upgraded version of {product-title}. As a workaround for this issue, run the following command with the following `ipsecConfig`  configuration in the Cluster Network Operator (CNO) CR:
+
[source,terminal]
----
$ oc patch networks.operator.openshift.io cluster --type=merge -p \
  '{
  "spec":{
    "defaultNetwork":{
      "ovnKubernetesConfig":{
        "ipsecConfig":{
          "mode":"Full"
        }}}}}'
----
+
After you run the command, the CNO collects `must-gather` logs that you can inspect.
+
(link:https://issues.redhat.com/browse/OCPBUGS-52367[OCPBUGS-52367])

* There is a known issue with Gateway API and {aws-first}, {gcp-first}, and {azure-first} private clusters. The load balancer that is provisioned for a gateway is always configured to be external, which can cause errors or unexpected behavior:
+
--
** In an {aws-short} private cluster, the load balancer becomes stuck in the `pending` state and reports the error: `Error syncing load balancer: failed to ensure load balancer: could not find any suitable subnets for creating the ELB`.

** In {gcp-short} and {azure-short} private clusters, the load balancer is provisioned with an external IP address, when it should not have an external IP address.
--
+
There is no supported workaround for this issue. (link:https://issues.redhat.com/browse/OCPBUGS-57440[OCPBUGS-57440])

* The `istiod` component, which supports Gateway API, previously created an `istio-ca-root-cert` ConfigMap in all namespaces watched by OpenShift Service Mesh (OSSM). A fix is included in OSSM 3.0.1 (part of {product-title} {product-version}) that limits this behavior, so ConfigMaps are only created in namespaces that contain a Gateway API resource. However, this fix does not automatically clean up existing ConfigMaps from namespaces that do not contain a Gateway. As a result, clusters might still contain numerous unnecessary `istio-ca-root-cert` ConfigMaps after an upgrade. These extra ConfigMaps are benign and can be safely ignored, or they can be manually deleted from namespaces that do not host a Gateway. (link:http://issues.redhat.com/browse/OCPBUGS-43093[OCPBUGS-43093])

* When the same hostname is specified in two or more different `Gateway` resources, the system does not report a conflict. The upstream Gateway API specification treats each `Gateway` as a separate independent proxy. This is the intended behavior of the upstream Gateway API specification. The cluster will not prevent this configuration. The routing of traffic to a specific Gateway is determined by external configuration, such as which Gateway's service IP address is used in the DNS record for the hostname. (link:http://issues.redhat.com/browse/NE-2180[NE-2180])

* Gateway API is not supported on on-premise platforms, including Bare Metal, vSphere, Nutanix, KubeVirt, and clusters using the `None` platform type. The Gateway API implementation automatically creates a service of `type: LoadBalancer`. On-premise platforms do not have a default cloud controller manager or other provisioner to assign an external IP address to this service. As a result, the `Gateway` resource will never be marked as "accepted" and will remain inaccessible from outside the cluster. (link:http://issues.redhat.com/browse/OCPBUGS-37385[OCPBUGS-37385])

* If the `appProtocol` field is not explicitly defined in a Kubernetes `Service`, the Gateway API implementation infers the network protocol from the `Service` port name. For example, a port named `https-web` is interpreted as using the HTTPS protocol. If the port name does not match the actual protocol used by the application, this inference can cause unexpected traffic behavior or connection failures. To avoid this issue, explicitly define the `appProtocol` field in your `Service` resources or ensure that port names strictly match the protocol in use. (link:http://issues.redhat.com/browse/OSSM-7231[OSSM-7231])

* The reverse proxy implementation for Gateway API (based on Istio and Envoy) currently demonstrates lower performance compared to the default HAProxy-based Ingress Controller. HAProxy has been extensively tuned for {product-title} over many releases. Consequently, users might observe higher latency or lower throughput when using Gateway API compared to standard OpenShift Routes. Performance optimizations for the Gateway API implementation are ongoing. (link:http://issues.redhat.com/browse/OSSM-6842[OSSM-6842])

* OpenShift Service Mesh (OSSM) 3.x, which provides the Gateway API implementation, cannot coexist with OSSM 2.x on the same cluster, except during a supported migration process. Installing both versions simultaneously for standard operations is not supported and might lead to resource conflicts; for example, the Ingress Operator might report a `degraded` status. Additionally, the Network Observability Operator (NID) no longer supports OSSM 2.x. It is recommended that users use OSSM 3.x for ongoing support and compatibility. Users must choose a single Service Mesh version for their cluster or follow the official migration procedure when moving from 2.x to 3.x. (link:https://issues.redhat.com/browse/OSSM-5407[OSSM-5407])

* The Gateway API implementation currently does not support the automatic idling of services. Unlike standard OpenShift Routes, which can idle services to conserve resources when they are not in use, workloads managed by Gateway API do not automatically scale to zero or enter an idle state based on traffic inactivity.

* The Gateway API implementation currently lacks dedicated integration with the {product-title} web console and the OpenShift CLI (`oc`). Consequently, Gateway API resources, such as `Gateway`, `GatewayClass`, and `HTTPRoute`, do not appear in specific console dashboards or views. To manage these resources, users must utilize standard `oc` commands (for example, `oc get gateway` or `oc edit httproute`) and apply configurations using YAML manifests.

* The Gateway API implementation currently does not provide a mechanism for users to configure advanced Istio or Envoy options. Specifically, capabilities such as configuring access logs or tuning low-level proxy settings are not exposed. Users must rely on the default configuration provided by the Gateway API controller.

* The Gateway API implementation currently does not support Cluster User-Defined Networks (CUDNs). Gateway API resources can only be used with the default cluster network and cannot be attached to user-defined secondary networks.

* The `HTTPRoute` resource in the Gateway API implementation currently does not support `Passthrough` or `Re-encrypt` TLS termination strategies. Unlike standard OpenShift Routes, which offer flexible termination options, `HTTPRoute` currently supports only Edge termination (terminating TLS at the gateway). Users requiring `Passthrough` or `Re-encrypt` functionality cannot currently achieve this using `HTTPRoute` resources.

[id="ocp-telco-ran-4-19-known-issues_{context}"]

* In the event of a crash, the `mlx5_core` NIC driver causes an out-of-memory issue and `kdump` does not save the `vmcore` file in `/var/crash`.
To save the `vmcore` file, use the `crashkernel` setting to reserve 1024 MB of memory for the `kdump` kernel. (link:https://issues.redhat.com/browse/OCPBUGS-54520[OCPBUGS-54520], link:https://issues.redhat.com/browse/RHEL-90663[RHEL-90663])

* There is a known latency issue on 4th Gen Intel Xeon processors. (link:https://issues.redhat.com/browse/OCPBUGS-42495[OCPBUGS-42495])

[id="ocp-telco-core-4-19-known-issues_{context}"]

* Currently, pods that use a `guaranteed` QoS class and request whole CPUs might not restart automatically after a node reboot or kubelet restart. The issue might occur in nodes configured with a static CPU Manager policy and using the `full-pcpus-only` specification, and when most or all CPUs on the node are already allocated by such workloads. As a workaround, manually delete and re-create the affected pods. (link:https://issues.redhat.com/browse/OCPBUGS-43280[OCPBUGS-43280])

* Currently, when a `irqbalance` service runs on a specific AArch64 machine, a buffer overflow issue might cause the service to crash. As a consequence, latency sensitive workloads might be affected by unmanaged interrupts that are not properly distributed across CPUs, leading to performance degradation. There is currently no workaround for this issue. (link:https://issues.redhat.com/browse/RHEL-89986[RHEL-89986])

* Currently, on clusters with SR-IOV network virtual functions configured, a race condition might occur between system services responsible for network device renaming and the TuneD service managed by the Node Tuning Operator. As a consequence, the TuneD profile might become degraded after the node restarts, leading to performance degradation. As a workaround, restart the TuneD pod to restore the profile state. (link:https://issues.redhat.com/browse/OCPBUGS-41934[OCPBUGS-41934])

[id="ocp-storage-core-4-19-known-issues_{context}"]

* NFS volumes exported from VMware vSAN Files cannot be mounted by clusters running {product-title} 4.19 due to RHEL-83435. To avoid this issue, ensure that you are running VMware ESXi and vSAN at the latest patch versions of 8.0 P05, or later. (link:https://issues.redhat.com/browse/OCPBUGS-55978[OCPBUGS-55978])

[id="ocp-4-19-asynchronous-errata-updates_{context}"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red{nbsp}Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red{nbsp}Hat Customer Portal users can enable errata notifications in the account settings for Red{nbsp}Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red{nbsp}Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating_a_cluster/updating-cluster-web-console.adoc#updating-cluster-web-console[updating your cluster] properly.
====

// 4.19.23 about
include::modules/zstream-4-19-23-about.adoc[leveloffset=+2]

// 4.19.23 fixes
include::modules/zstream-4-19-23-fixed-issues.adoc[leveloffset=+2]

// 4.19.23 updating
include::modules/zstream-4-19-23-updating.adoc[leveloffset=+2]

// 4.19.21 about
include::modules/zstream-4-19-22-about.adoc[leveloffset=+2]

// 4.19.21 fixes
include::modules/zstream-4-19-22-fixed-issues.adoc[leveloffset=+2]

// 4.19.21 updating
include::modules/zstream-4-19-22-updating.adoc[leveloffset=+2]

// 4.19.21 about
include::modules/zstream-4-19-21-about.adoc[leveloffset=+2]

// 4.19.21 fixes
include::modules/zstream-4-19-21-bug-fixes.adoc[leveloffset=+2]

// 4.19.21 updating
include::modules/zstream-4-19-21-updating.adoc[leveloffset=+2]


// 4.19.20 about
include::modules/zstream-4-19-20-about.adoc[leveloffset=+2]

// 4.19.20 fixes
include::modules/zstream-4-19-20-bug-fixes.adoc[leveloffset=+2]

// 4.19.20 updating
include::modules/zstream-4-19-20-updating.adoc[leveloffset=+2]


// 4.19.19 about
include::modules/zstream-4-19-19-about.adoc[leveloffset=+2]

// 4.19.19 fixes
include::modules/zstream-4-19-19-bug-fixes.adoc[leveloffset=+2]

// 4.19.19 updating
include::modules/zstream-4-19-19-updating.adoc[leveloffset=+2]

// 4.19.18
[id="ocp-4-19-18_{context}"]
=== RHBA-2025:19301 - {product-title} {product-version}.18 image release and bug fix advisory

Issued: 05 November 2025

{product-title} release {product-version}.18 is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2025:19301[RHBA-2025:19301] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:19299[RHBA-2025:19299] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.18 --pullspecs
----

[id="ocp-4-19-18-bug-fixes_{context}"]
==== Bug fixes

* Before this update, the `OAuth` route was not accepted by the private router when the DNS record was not registered in the `external-dns` Operator. This action led to improper URL resolution, and the console failed to access the `OAuth` route. As a consequence, the `Console ClusterOperator` was stuck. With this release, the `OAuth` route admission and URL resolution issues in the hosted cluster is fixed. As a result, the `OAuth` route is accessible and the console access is approved. (link:https://issues.redhat.com/browse/OCPBUGS-61407[OCPBUGS-61407])

* Before this release, deleting a `VolumeSnapshot` object for a persistent volume provisioned by the {azure-short} file CSI driver also deleted the underlying fileshare and resulted in data loss. With this release, this issue is corrected by updating the driver to ensure that only the snapshot is removed. The source fileshare is preserved. (link:https://issues.redhat.com/browse/OCPBUGS-62911[OCPBUGS-62911])

* Before this update, frequent driver configuration updates that were caused by an inconsistent storage class order in a hosted cluster namespace caused `ConfigMap` content flaps. This issue resulted in inconsistent storage class enforcement and affected user experience. With this release, the `ConfigMap` driver configuration is stabilized, which prevents storage classes from flapping, improves `ConfigMap` driver configuration stability, and prevents frequent flapping of a storage class order in a hosted cluster namespace. (link:https://issues.redhat.com/browse/OCPBUGS-62807[OCPBUGS-62807])

* Before this update, Redfish transactions in {sno} nodes failed due to an empty `eTag` field in `metal3-ironic` container logs. As a consequence, users experienced failed Redfish transactions on {sno} nodes. With this release, the Redfish transaction `eTag` field issue is resolved, and results in correct `eTags`. As a result, the Redfish transaction does not fail, allowing the telecommunications company to use the `HostFirmwareSettings` parameter in {sno} nodes. (link:https://issues.redhat.com/browse/OCPBUGS-62961[OCPBUGS-62961])

* Before this update, excessive CPU overcommitment that was greater than 200% caused the `KubeCPUOvercommit` alert to stop triggering after 10 minutes. As a consequence, users were unaware of the CPU overcommitment due to the missing alert. With this release, the `KubeCPUOvercommit` alert triggers correctly when CPU limits are overcommitted, and ensures timely resource management and improved cluster stability. (link:https://issues.redhat.com/browse/OCPBUGS-62965[OCPBUGS-62965])

* Before this update, the `KubeMemoryOvercommit` alert falsely triggered on small multi-node clusters after memory-consuming spikes occurred within the permitted limits. With this release, the alert expression is adjusted to correctly account for small multi-node clusters. As a result, the `KubeMemoryOvercommit` alert does not falsely trigger after these instances. (link:https://issues.redhat.com/browse/OCPBUGS-62966[OCPBUGS-62966])

* Before this update, a Kubernetes `StatefulSet` status replica alert did not activate for invalid pod specifications when a controller failed to create a pod. As a consequence, users received false assurance when the `StatefulSet` failed to create the required number of replicas. With this release, a Kubernetes `StatefulSet` replica count alert triggers for unsuccessful pod creation. As a result, the alert displays correctly when `StatefulSet` replicas do not match the configured amount. (link:https://issues.redhat.com/browse/OCPBUGS-62967[OCPBUGS-62967])

* Before this update, the `KubeAggregatedAPIErrors` alert triggered based on the total number of errors across instances, and caused sensitive user alerts for multiple instances of an API. With this release, the alerting function for the `KubeAggregatedAPIErrors` alert is changed to operate at the instance level, reducing false alarms for APIs with multiple instances. (link:https://issues.redhat.com/browse/OCPBUGS-62968[OCPBUGS-62968])

* Before this update, alerts were not filtered for cordoned nodes, leading to false positives for nodes under maintenance. As a consequence, users experienced false positives due to the filtering of cordoned nodes in alerts. With this release, the cordoned nodes are filtered from alerts to reduce false positives during maintenance. As a result, maintenance alerts are correct and false positives are reduced for cordoned nodes. (link:https://issues.redhat.com/browse/OCPBUGS-62969[OCPBUGS-62969])

* Before this update, destroying an {product-title} cluster on {gcp-full} caused a panic error due to a null pointer reference cancellation in the `waitFor` method. This was due to a failed {gcp-full} API call or an uninitialized client. As a consequence, users experienced a panic error during cluster destruction on {gcp-full}. With this release, the null pointer issue in the cluster uninstaller is fixed, and panic errors during a {gcp-full} destruction are prevented. As a result, the panic error during cluster destruction on {gcp-full} is resolved, ensuring smooth deletion of resources. (link:https://issues.redhat.com/browse/OCPBUGS-62981[OCPBUGS-62981])

* Before this update, an administrative user with a single namespace role created a pod and encountered a blank page while viewing metrics due to an incorrect perspective in the URL. As a consequence, the user could not view CPU usage metrics. With this release, the administrative user can view pod metrics in the developer perspective, and the user can view CPU usage metrics. (link:https://issues.redhat.com/browse/OCPBUGS-62999[OCPBUGS-62999])

* Before this update, the {oci-csi-full} node registrar container logged `gRPC` connection checks every 10 seconds, excessively filling Elasticsearch space. This rapid log filling increased user costs. With this release, the logging frequency of `gRPC` connection checks in the `csi-node-registrar` container is reduced. As a result, the Elasticsearch log capacity is increased, which lowers costs and improves performance. (link:https://issues.redhat.com/browse/OCPBUGS-63193[OCPBUGS-63193])

* Before this update, if a user did not define an EgressIP failover between gateway nodes with a proper IP address, the EgressIP address was not reassigned to the second gateway node after a reboot. This resulted in a communication failure between the pod and the external system. With this release, the EgressIP failover logic is improved for dual-stack environments, ensuring that the EgressIP address is properly reassigned to the available gateway node after a reboot. The communication between pods and external systems after a gateway node reboot is not interrupted. (link:https://issues.redhat.com/browse/OCPBUGS-63234[OCPBUGS-63234])

* Before this update, the upgrade to {product-title} 4.18.24 triggered a kubelet failure on primary nodes due to `ocp-tuned-one-shot` service issues. As a consequence, the kubelet failed to start on primary nodes during upgrades. With this release, the kubelet issue is resolved by fixing the `ocp-tuned-one-shot` service. As a result, the kubelet starts on primary nodes after the upgrade. (link:https://issues.redhat.com/browse/OCPBUGS-63418[OCPBUGS-63418])

[id="ocp-4-19-18-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.17
[id="ocp-4-19-17_{context}"]
=== RHSA-2025:18233 - {product-title} {product-version}.17 image release and bug fix advisory

Issued: 22 October 2025

{product-title} release {product-version}.17 is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2025:18233[RHSA-2025:18233] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:18201[RHBA-2025:18201] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.17 --pullspecs
----

[id="ocp-4-19-17-bug-fixes_{context}"]
==== Bug fixes

* Before this update, the Cluster Version Operator (CVO) in 4.19.9 and 4.18.23 started to require bearer token authentication in metrics requests. As a consequence, HyperShift and Hosted clusters were broken because the metrics scraper did not provide client authentication. With this release, the CVO does not require client authentication for metrics requests. As a result, access to CVO metrics scraping is recovered on HyperShift and Hosted clusters. (link:https://issues.redhat.com/browse/OCPBUGS-62868[OCPBUGS-62868])

* Before this update, the installation program did not allow IPv6 primary dual-stack installations on platforms that were designated as `None` or `External`. As a consequence, an error or configuration block occurred when you proceeded with a dual-stack installation on these platform types. With this release, you can successfully install IPv6 primary dual-stack configurations on `None` and `External` platforms. (link:https://issues.redhat.com/browse/OCPBUGS-62911[OCPBUGS-62911])

[id="ocp-4-19-17-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.16
[id="ocp-4-19-16_{context}"]
=== RHBA-2025:17663 - {product-title} {product-version}.16 image release and bug fix advisory

Issued: 14 October 2025

{product-title} release {product-version}.16 is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2025:17663[RHBA-2025:17663] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:17660[RHBA-2025:17660] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.16 --pullspecs
----

[id="ocp-4-19-16-bug-fixes_{context}"]
==== Bug fixes

* Before this update, the timeout on one etcd member caused context deadlines to exceed. As a consequence, all members were declared unhealthy, even though some were reachable. With this release, if one member times out, other members are no longer incorrectly marked as unhealthy. (link:https://issues.redhat.com/browse/OCPBUGS-60941[OCPBUGS-60941])

* Before this update, a pod with a secondary interface in an OVN-Kubernetes local network (mapped to the br-ex bridge) could communicate with pods on the same node that used the default network for connectivity only if the local network IP addresses were within the same subnet as the host network. With this release, you can extract the local network IP addresses from any subnet. In this generalized case, an external router outside the cluster is expected to connect the local network subnet to the host network. (link:https://issues.redhat.com/browse/OCPBUGS-61454[OCPBUGS-61454])

* Before this update, an external actor could uncordon a node that the Machine Config Operator (MCO) is draining. As a consequence, the MCO and the scheduler would schedule and unschedule pods at the same time, prolonging the drain process. With this fix, the MCO attempts to recordon the node if an external actor uncordons it during the drain process. As a result, the MCO and scheduler no longer schedule and remove pods at the same time. (link:https://issues.redhat.com/browse/OCPBUGS-62003[OCPBUGS-62003])

* Before this update, the omission of binary version display in `oc-mirror` output hindered debugging, causing delays in identifying required fixes and slowing user experience. With this release, `oc-mirror` now displays its version in output for easier debugging. As a result, end users can easily identify the `oc-mirror` version for faster debugging. (link:https://issues.redhat.com/browse/OCPBUGS-62311[OCPBUGS-62311])

* Before this update, Prometheus for both the platform and user workload monitoring would negotiate and accept UTF-8 metrics, even though the monitoring stack does not yet fully support UTF-8. With this release, Prometheus no longer accepts UTF-8 metrics. (link:https://issues.redhat.com/browse/OCPBUGS-62429[OCPBUGS-62429])

* Before this update, a race condition would sometimes cause an intermittent failure, or "flake", when a persistent volume claim (PVC) was resized too quickly after being created. As a consequence, this resulted in an error where the system would incorrectly report that the bound persistent volume (PV) could not be found. With this release, the timing issue was fixed, so resizing a PVC right after its creation works. (link:https://issues.redhat.com/browse/OCPBUGS-62468[OCPBUGS-62468])

[id="ocp-4-19-16-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.15
[id="ocp-4-19-15_{context}"]
=== RHBA-2025:17237 - {product-title} {product-version}.15 image release and bug fix advisory

Issued: 07 October 2025

{product-title} release {product-version}.15 is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2025:17237[RHBA-2025:17237] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:17235[RHBA-2025:17235] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.15 --pullspecs
----

[id="ocp-4-19-15-bug-fixes_{context}"]
==== Bug fixes

* With this release, as part of the `oc-mirror` v1 deprecation process, a warning message indicates that the `--v1` or `--v2` flag is mandatory. As a result, `oc-mirror` fails if you do not specify these flags. (link:https://issues.redhat.com/browse/OCPBUGS-62062[OCPBUGS-62062])

* Before this update, the `/auth/error` page did not render correctly. As a consequence, the page was empty and error details did not appear. With this release, the front end error page content appears on the `/auth/error` page. (link:https://issues.redhat.com/browse/OCPBUGS-62083[OCPBUGS-62083])

[id="ocp-4-19-15-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.14
[id="ocp-4-19-14_{context}"]
=== RHBA-2025:16693 - {product-title} {product-version}.14 image release and bug fix advisory

Issued: 30 September 2025

{product-title} release {product-version}.14 is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2025:16693[RHBA-2025:16693] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:16691[RHBA-2025:16691] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.14 --pullspecs
----

[id="ocp-4-19-14-bug-fixes_{context}"]
==== Bug fixes

* Before this update, the ignition server deployment used a global `mirroredReleaseImage` state that could be modified by concurrent image lookup operations, which caused race conditions. As a consequence, the `MIRRORED_RELEASE_IMAGE` environment variable flipped between the original image and its mirror registry, which triggered constant deployment regenerations. With this release, the global mirror state is replaced with an image-specific lookup logic, which ensures deterministic mirror resolution and eliminates defensive filtering for empty registry entries. As a result, ignition server deployments remain stable with consistent `MIRRORED_RELEASE_IMAGE` values, which eliminates unnecessary pod restarts and deployment churn. (link:https://issues.redhat.com/browse/OCPBUGS-61677[OCPBUGS-61677])

* Before this update, the *Expand* button in the *Pod* and *Node logs* page in the web console did not work correctly. As a consequence, you could not provide input at the prompt in the terminal. With this release, the browser is set to full-screen when you click the *Expand* button. As a result, you can successfully provide input in the terminal. (link:https://issues.redhat.com/browse/OCPBUGS-61821[OCPBUGS-61821])

* Before this update, an `NMState` service failure occurred in {product-title} deployments because of a `NetworkManager-wait-online` dependency issue in baremetal and multiple network interface controller (NIC) environments. As a consequence, an incorrect network configuration caused deployment failures. With this release, the `NetworkManager-wait-online` dependency for baremetal deployments is updated, which reduces deployment failures and ensures `NMState` service stability. (link:https://issues.redhat.com/browse/OCPBUGS-61835[OCPBUGS-61835])

[id="ocp-4-19-14-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.13
[id="ocp-4-19-13_{context}"]
=== RHBA-2025:16148 - {product-title} {product-version}.13 image release, bug fix, and security update advisory

Issued: 23 September 2025

{product-title} release {product-version}.13, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2025:16148[RHBA-2025:16148] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:16146[RHBA-2025:16146] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.13 --pullspecs
----

[id="ocp-4-19-13-bug-fixes_{context}"]
==== Bug fixes

* Before this update, the `config-sync-controller` did not log results due to missing logging statements in the code. As a consequence, users experienced silent failures in the `config-sync-controller`. With this release, the `config-sync-controller` logs results enhancing error diagnosis for users. (link:https://issues.redhat.com/browse/OCPBUGS-56788[OCPBUGS-56788])

* Before this update, calls to retrieve an image manifest and metadata using a tagged image name did not cache the result of the lookup. As a consequence, hosted control plane memory usage quickly grew, which created performance issues. With this release, images in hosted control plane using a named tag or canonical name are cached for 12 hours. As a result, hosted control plane memory usage is optimized. (link:https://issues.redhat.com/browse/OCPBUGS-59933[OCPBUGS-59933])

* Before this update, the `agent-based-installer` set the permissions for the etcd directory `/var/lib/etcd/member` as 0755 when using {sno} deployment instead of 0700, which is correctly set on a multi-node deployment. With this release, the etcd directory `/var/lib/etcd/member` permissions are set to 0700 for {sno} deployments. (link:https://issues.redhat.com/browse/OCPBUGS-61313[OCPBUGS-61313])

* Before this update, the `PrometheusRemoteWriteBehind` alert fired if the remote endpoint never received any data. With this release, the `PrometheusRemoteWriteBehind` alert no longer fires if the remote endpoint has not yet received any data. (link:https://issues.redhat.com/browse/OCPBUGS-61486[OCPBUGS-61486])

* Before this update, it was possible for webhook failures to trigger a `kube-apiserver` crash while generating an audit log entry for a request. As a consequence, API server disruptions were possible. With this release, the audit system has been updated so that the `kube-apiserver` no longer crashes and the API disruptions are resolved. (link:https://issues.redhat.com/browse/OCPBUGS-61488[OCPBUGS-61488])

* Before this update, the *Operand details page* in the web console would show additional status items in a third column, which resulted in the content appearing squashed. With this update, the defect has been corrected, so that only two columns display in the details page. (link:https://issues.redhat.com/browse/OCPBUGS-61781[OCPBUGS-61781])

[id="ocp-4-19-13-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.12
[id="ocp-4-19-12_{context}"]
=== RHBA-2025:15694 - {product-title} {product-version}.12 image release, bug fix, and security update advisory

Issued: 16 September 2025

{product-title} release {product-version}.12, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2025:15694[RHBA-2025:1694] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:15692[RHBA-2025:15692] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.12 --pullspecs
----

[id="ocp-4-19-12-enhancements_{context}"]
==== Enhancements

* With this update, the `cluster-etcd-operator` Operator now implements a multi-stage notification system for the `etcdDatabaseQuotaLowSpace` alert to proactively manage etcd storage quotas. This enhancement prevents API server instability by providing earlier warnings of low database space. As etcd disk space usage reaches 65%, 75% and 85%, administrators now receive alerts with a severity level of info, warning, or critical. (link:https://issues.redhat.com/browse/OCPBUGS-60443[OCPBUGS-60443])

* With this update, the collection of command line logs from `virt-launcher` pods across a Kubernetes cluster are enabled. JSON-encoded logs are saved at the path `namespaces/<namespace_name>/pods/<pod_name>/virt-launcher.json`, facilitating troubleshooting and debugging of virtual machines. (link:https://issues.redhat.com/browse/OCPBUGS-61485[OCPBUGS-61485])

* With this update, the machine config nodes custom resource, which you can use to monitor the progress of machine configuration updates to nodes, is now generally available. With the promotion to General Availability, you can view the status of updates to custom machine config pools, in addition to the control plane and worker pools. The functionality for the feature has not changed. However, some of the information in the command output and in the status fields in the `MachineConfigNode` object have been updated. The `must-gather` for the Machine Config Operator includes all `MachineConfigNodes` objects in the cluster. For more information, see xref:../machine_configuration/index.adoc#checking-mco-node-status_machine-config-overview[About checking machine config node status].

* With this update, the `PinnedImageSet` object, which you can use to get the container images in advance, before they are actually needed, is now generally available. You can associate these images with a machine config pool. In clusters with slow, unreliable connections to an image registry, pinning images ensures that the images are available when needed. The `must-gather` for the Machine Config Operator now includes all `PinnedImageSet` objects in the cluster. For more information, see xref:../machine_configuration/machine-config-pin-preload-images-about.adoc#machine-config-pin-preload-images_machine-config-operator[Pinning images to nodes].

[id="ocp-4-19-12-bug-fixes_{context}"]
==== Bug fixes

* Before this update, if the cluster was created without an SSH key, creating a node image with the `oc adm node-image create` command failed due to the absence of the `99-worker-ssh` machine configuration. This prevented worker node image creation. With this release, the `machineConfig` for `worker-ssh` is created, enabling node image creation. As a result, node image creation for worker nodes now succeeds. (link:https://issues.redhat.com/browse/OCPBUGS-60832[OCPBUGS-60832])

* Before this update, executing `ccoctl` multiple times while using the {aws-first} platform and the `--create-private-s3-bucket` parameter caused the wrong URL to be configured for the OpenID Connect (OIDC) issuer. As a consequence, some cluster Operators could not authenticate to the {aws-short} API. With this release, `ccoctl` properly configures the correct URL for the OIDC issuer. As a result, the cluster Operators continue to authenticate as expected. (link:https://issues.redhat.com/browse/OCPBUGS-60970[OCPBUGS-60970])

* Before this update, the `MachineHealthCheck` custom resource (CR) did not show the default value for the `maxUnhealthy` field. With this release, the CR also documents the value it defaults to when not set. (link:https://issues.redhat.com/browse/OCPBUGS-61096[OCPBUGS-61096])

* Before this update, the time it took to apply updates to the `multus-networkpolicy` DaemonSet scaled linearly with the node count. With this release, the DaemonSet has been updated to allow for 10% `maxUnavailable` so that the DaemonSet updates immediately in clusters larger than 10 nodes. (link:https://issues.redhat.com/browse/OCPBUGS-61460[OCPBUGS-61460])

[id="ocp-4-19-12-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.11
[id="ocp-4-19-11_{context}"]
=== RHBA-2025:15293 - {product-title} {product-version}.11 image release, bug fix, and security update advisory

Issued: 09 September 2025

{product-title} release {product-version}.11, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2025:15293[RHBA-2025:15293] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2025:15291[RHSA-2025:15291] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.11 --pullspecs
----

[id="ocp-4-19-11-enhancements_{context}"]
==== Enhancements

* With this update, the Kubernetes API server distribution is optimized, ensuring a balanced load among all primary nodes after a quorum is reestablished. This addresses the issue of a single API server receiving the majority of live connections, and causing high CPU usage. Resource utilization is improved and CPU spikes are reduced during primary node or API server restarts. (link:https://issues.redhat.com/browse/OCPBUGS-60121[OCPBUGS-60121])

[id="ocp-4-19-11-bug-fixes_{context}"]
==== Bug fixes

* Before this update, disabling the {product-registry} retained legacy pull secret finalizers and caused hung secret deletion during the registry removal. This issue blocked cluster deletion. With this release, secret finalizers do not block namespace deletion when the registry is disabled, and ensures cluster deletion. (link:https://issues.redhat.com/browse/OCPBUGS-56614[OCPBUGS-56614])

* Before this update, the `oc mirror` command failed on RHEL 8 systems with a `noexec-mounted /tmp` directory because it could not start the temporary files or scripts. As a consequence, image mirroring was prevented. With this release, the `oc mirror` command includes an exception for the `noexec-mounted /tmp` drectory, and mirroring on RHEL 8 systems is successful. As a result, the `oc mirror` command lists output and mirror container images on RHEL 8 systems with a `noexec-mounted /tmp` directory. (link:https://issues.redhat.com/browse/OCPBUGS-59760[OCPBUGS-59760])

* Before this update, temporary `apiserver` downtime caused the `cluster-etcd-operator` to incorrectly report that the `openshift-etcd` namespace did not exist. As a consequence, users saw incorrect messages about a missing namespace during the downtime. With this release, a fix is implemented to improve the error message for a missing etcd namespace. As a result, the error message is corrected, and ensures that the `cluster-etcd-operator` status accurately reflects the issue during temporary `apiserver` downtime. (link:https://issues.redhat.com/browse/OCPBUGS-59802[OCPBUGS-59802])

* Before this update, duplicate link buttons on the *Quickstarts* page appeared only in the `/quickstart` path, and confused users. With this release, the *Quickstart* link buttons display correctly and duplicates are eliminated. (link:https://issues.redhat.com/browse/OCPBUGS-60420[OCPBUGS-60420])

* Before this update, a {hcp-capital} cluster rejected certificates with multiple Storage Area Network (SAN) entries due to conflicting DNS names. As a consequence, users encountered certificate deployment errors with multi-SAN hostnames in {hcp-capital} clusters. With this release, certificate validation for multiple SAN entries is supported in {hcp-capital} clusters. As a result, certificates with multiple SAN entries are accepted, improving {hcp-capital} cluster deployments. (link:https://issues.redhat.com/browse/OCPBUGS-60483[OCPBUGS-60483])

* Before this update, the last node retained the `ToBeDeletedByClusterAutoscaler` taint during the scale-down process because of incorrect machine deletion handling. As a consequence, the last node affected the cluster autoscaling efficiency. With this release, the `ToBeDeletedByClusterAutoscaler` taint is removed from the last node after scaling down. The last node does not retain the unwanted taint, and the cluster stability is improved. (link:https://issues.redhat.com/browse/OCPBUGS-60900[OCPBUGS-60900])

* Before this release, the stale IP address entries in the `address_set` configuration element that corresponded to the DNS egress firewall rule were not removed. This resulted in an increasing `address_set`, and led to memory leak issues. With this release, the issue is fixed by removing the IP addresses from the `address_set` after a 5-second grace period that follows the Time to Live (TTL) expiration of the IP addresses. (link:https://issues.redhat.com/browse/OCPBUGS-60979[OCPBUGS-60979])

[id="ocp-4-19-11-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.10
[id="ocp-4-19-10_{context}"]
=== RHSA-2025:14823 - {product-title} {product-version}.10 image release, bug fix, and security update advisory

Issued: 02 September 2025

{product-title} release {product-version}.10, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2025:14823[RHBA-2025:14823] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:14817[RHBA-2025:14817] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.10 --pullspecs
----

[id="ocp-4-19-10-enhancements_{context}"]
==== Enhancements

* The name of the `MachineOSConfig` object used with {image-mode-os-on-lower} must now be the same as the machine config pool where you want to deploy the custom layered image. This change prevents using multiple `MachineOSConfig` objects with each machine config pool. (link:https://issues.redhat.com/browse/OCPBUGS-60414[OCPBUGS-60414])

:olm-np-z-stream: 10
* {empty}
+
--
include::snippets/olmv0-and-v1-rn-np-support.adoc[]
--
(link:https://issues.redhat.com/browse/OCPBUGS-60525[OCPBUGS-60525] and link:https://issues.redhat.com/browse/OCPBUGS-60521[OCPBUGS-60521])

:!olm-np-z-stream:

[id="ocp-4-19-10-bug-fixes_{context}"]
==== Bug fixes

* Before this update, the Hosted Control Plane (HCP) did not query payload repositories sequentially when the management cluster was configured with many image repositories, causing the hosted cluster deployments to fail in disconnected environments if the first mirror was unavailable. The system errored out instead of searching for the next available image. With this release, the HCP payload iterates through the entire list of mirrors until an available image is found, allowing deployments to succeed as expected. (link:https://issues.redhat.com/browse/OCPBUGS-57141[OCPBUGS-57141])

* Before this update, when deploying {sno} by using zero-touch provisioning (ZTP) in release 4.19 with many IP addresses configured on the primary interface, the `apiserver` pod would fail to connect to etcd. As a result, the etcd certificate did not include all the configured IP addresses, leading to Transport Layer Security (TLS) authentication errors. With this release, the `apiserver` pod can now successfully connect to etcd in these configurations, allowing {sno} deployments with many primary interface IP addresses to initialize correctly. (link:https://issues.redhat.com/browse/OCPBUGS-59285[OCPBUGS-59285])

* Before this update, {ibm-cloud-title} was not included in the validation check for {sno} installs, causing a validation error when attempting to install {sno} on {ibm-cloud-title}. With this release, {ibm-cloud-title} now supports {sno} installs improving the installation experience for end users on {ibm-cloud-title}. (link:https://issues.redhat.com/browse/OCPBUGS-59607[OCPBUGS-59607])

* Before this update, the `Delete` workflow erroneously displayed `workflow mode: diskToMirror / delete`, leading to user confusion regarding the correct workflow mode. With this release, `workflow mode: delete` displays during delete operations. (link:https://issues.redhat.com/browse/OCPBUGS-59761[OCPBUGS-59761])

* Before this update, sharing duplicated images between different container images resulted in a wrong count calculation in `oc-mirror` for total mirrored images of Helm charts. As a consequence, some Helm images were not mirrored. With this release, the wrong count of mirrored Helm images in `oc-mirror` has been fixed, improving the accuracy of mirrored image counts. (link:https://issues.redhat.com/browse/OCPBUGS-60086[OCPBUGS-60086])

* Before this update, the `HorizontalPodAutoscaler` temporarily scaled `istiod-openshift-gateway` to two replicas, causing Continuous Integration (CI) failure due to the tests expecting only one replica. With this release, `HorizontalPodAutoscaler` scaling is adjusted to support a single replica for `istiod-openshift-gateway`.  (link:https://issues.redhat.com/browse/OCPBUGS-60204[OCPBUGS-60204])

* Before this update, an upgrade to a version before 4.15 or a new install of 4.15 deployed `MachineConfigNode` custom resource definitions (CRDs) despite being in Technology Preview. As a result, clusters failed to upgrade due to unneeded CRDs. With this release, Technology Preview `MachineConfigNode` CRDs were removed from default clusters ensuring seemless upgrades. (link:https://issues.redhat.com/browse/OCPBUGS-60265[OCPBUGS-60265])

* Before this update, on dual-stack clusters with IPv6 as the primary networking stack, the bare-metal Installer-Provisioned Infrastructure (IP) would incorrectly supply an IPv4 URL for the virtual media ISO image. This caused installation failures on Baseboard Management Controllers (BMCs) that were configured only for IPv6 networking, as the BMCs could not reach the IPv4 address. With this release, the installation program logic was updated to always supply an IPv6 URL when a BMC is using IPv6 addressing and the installation process now completes successfully. (link:https://issues.redhat.com/browse/OCPBUGS-60402[OCPBUGS-60402])

* Before this update, {aws-first} `machinesets` could have a null `userDataSecret` name, leading to machines remaining in a provisioning state. With this release, a non-empty `userDataSecret` name is required, preventing unexpected machine behavior. (link:https://issues.redhat.com/browse/OCPBUGS-60427[OCPBUGS-60427])

* Before this update, a limitation prevented a certificate's validity from exceeding that of the signer. This impacted the `localhost-recovery.kubeconfig`, as the node-system-admin-client certificate was incorrectly generated with a one-year lifespan instead of the intended two years, causing the premature expiration of the `localhost-recovery.kubeconfig`. With this release, the signer certificate's validity is extended to three years, ensuring the node-system-admin-client certificate now has a two-year lifespan. (link:https://issues.redhat.com/browse/OCPBUGS-60495[OCPBUGS-60495])

* Before this update, {product-title} clusters on {aws-short} that were created with version 4.13 or earlier could not update to version 4.19. Clusters that were created with version 4.14 and later have an {aws-short} `cloud-conf` ConfigMap by default, and this ConfigMap is required starting in {product-title} 4.19. With this release, the Cloud Controller Manager Operator is updated to create a default `cloud-conf` ConfigMap when none is present on the cluster. This change enables clusters that were created with version 4.13 or earlier to update to version 4.19. (link:https://issues.redhat.com/browse/OCPBUGS-60950[OCPBUGS-60950])

[id="ocp-4-19-10-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.9
[id="ocp-4-19-9_{context}"]
=== RHSA-2025:13848 - {product-title} {product-version}.9 image release, bug fix, and security update advisory

Issued: 19 August 2025

{product-title} release {product-version}.9, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2025:13848[RHSA-2025:13848] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:13827[RHBA-2025:13827] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.9 --pullspecs
----

[id="ocp-4-19-9-enhancements_{context}"]
==== Enhancements

* With this update, you can install the NUMA Resources Operator on {hcp}, enabling NUMA-aware scheduling support. For more information, see xref:../scalability_and_performance/cnf-numa-aware-scheduling.adoc#cnf-creating-nrop-cr-hosted-control-plane_numa-aware[Creating the NUMAResourcesOperator custom resource for hosted control planes]. This enhancement is available as a Technology Preview feature.

[id="ocp-4-19-9-bug-fixes_{context}"]
==== Bug fixes

* Before this update, boot images for 4.1 and 4.2 failed to work with {product-title} {product-version}, and caused degraded cluster operation. With this release, a static Grand Unified Bootloader (GRUB) configuration is installed for Extensible Firmware Interface (EFI) and firmware components, and the cluster operates normally during node scaling. (link:https://issues.redhat.com/browse/OCPBUGS-52485[OCPBUGS-52485])

* Before this update, the {gcp-first} machine API was blocked by sequential reconciliation processing. As a consequence, users experienced slow scaling of nodes during GCP integration. With this release, the GCP machine API performance is improved by enabling parallel execution of many reconciliation processes. As a result, GCP node scaling performance improves. (link:https://issues.redhat.com/browse/OCPBUGS-59386[OCPBUGS-59386])

* Before this update, users configured {product-title} Vertical Pod Autoscaler (VPA) custom recommenders using a version with an upstream issue in VPA. As a consequence, the issue caused instability in VPA updates. With this release, the custom VPA checkpoint garbage collector does not remove untracked checkpoints, and prevents instability in {product-title}. As a result, {product-title} VPA updates are stable and constant pod rescheduling does not occur. (link:https://issues.redhat.com/browse/OCPBUGS-59638[OCPBUGS-59638])

* Before this update, the machine config daemon failed Domain Name System (DNS) lookups during the {product-title} 4.16 manifest application on {vmw-first} infrastructure. As a consequence, user DNS lookups failed during the {product-title} 4.16 upgrade, halting upgrades indefinitely. With this release, retries of remote operating system updates with backoff are implemented to avoid failing because of CoreDNS pod restarts during the upgrades. (link:https://issues.redhat.com/browse/OCPBUGS-59899[OCPBUGS-59899])

* Before this update, cluster upgrade failures occurred because of an increased reconcile attempts limit. This failure caused Prometheus pod unavailability and resulted in service degradation. With this release, the Operator allows an additional reconcile attempt before reporting failures. As a result, the cluster upgrade test stability is improved, reducing failure rate, and enhancing upgrade reliability. (link:https://issues.redhat.com/browse/OCPBUGS-59932[OCPBUGS-59932])

* Before this update, the sidecar of a {product-title} Precision Time Protocol (PTP) pod unexpectedly restarted after termination, and caused the clock class termination to fail with an `exit code 7` error. As a consequence, the metrics were unavailable. With this release, the sidecar restart does not cause the clock class termination error in a {product-title} PTP pod, and does not stop during the restart. (link:https://issues.redhat.com/browse/OCPBUGS-59970[OCPBUGS-59970])

* Before this update, when a user upgraded to {product-title} {product-version}, the Machine Config Operator (MCO) rotated a Transport Layer Security (TLS) certificate. This caused an issue where nodes could not join the cluster during the scale-up process. With this release, the MCO provides a custom ARO resource that determines the necessary Subject Alternative Name (SAN) IP address, and adds it in the rotated TLS certificate. As a result, nodes can join the cluster during the scale-up process. (link:https://issues.redhat.com/browse/OCPBUGS-59978[OCPBUGS-59978])

* Before this update, an interpolation error in the `ResourceEventStream` code format caused incorrect error messages when users connected to the event stream. With this release, the interpolation format for error messages in the event stream is correct. As a result, users see accurate error messages when they connect to the event stream. (link:https://issues.redhat.com/browse/OCPBUGS-60039[OCPBUGS-60039])

* Before this update, the primary node port in the communication matrix project was unbound, and caused a missing communication flow and service unavailability on the primary node. With this release, the port on the controller manager is closed and is available only from the `localhost`. As a result, the service is bound to the correct port. (link:https://issues.redhat.com/browse/OCPBUGS-60132[OCPBUGS-60132])

* Before this update, a `MachineSet` custom resource update failure occurred because of multiple arch annotation labels. As a consequence, the machine update failed. With this release, the update issue is corrected by allowing multiple labels in the `{{capacity.cluster-autoscaler.kubernetes.io/labels}}` annotation, and properly parses the architecture value. As a result, the Machine Config Operator does not fail during an update. (link:https://issues.redhat.com/browse/OCPBUGS-60224[OCPBUGS-60224])

* Before this update, the `LeaderWorkerSet` Operator description was outdated. As a consequence, users experienced incorrect descriptions. With this release, the `LeaderWorkerSet` Operator description is updated and the description accurately describes the concept. (link:https://issues.redhat.com/browse/OCPBUGS-60225[OCPBUGS-60225])

* Before this update, the `cloud-event-proxy` sidecar process terminated and caused the notification API to remain in a `clockClass=0` state even when the pod recovered. As a consequence, the notification API remained inactive after the sidecar process terminated. With this release, the `cloud-event-proxy` process recovery does not result in a `clockClass=0` state for the notification API. Now, the notification API correctly updates the `clockClass` variable when the `cloud-event-proxy` recovers. (link:https://issues.redhat.com/browse/OCPBUGS-60261[OCPBUGS-60261])

* Before this update, insufficient obfuscation of new network data types in an OVN-K hosted cluster exposed sensitive data. As a consequence, user data was exposed. With this release, the anonymizer is updated to discover and obfuscate new network data types, and ensure secure communication. (link:https://issues.redhat.com/browse/OCPBUGS-60295[OCPBUGS-60295])

[id="ocp-4-19-9-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.7
[id="ocp-4-19-7_{context}"]
=== RHSA-2025:12341 - {product-title} {product-version}.7 image release, bug fix, and security update advisory

Issued: 05 August 2025

{product-title} release {product-version}.7, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2025:12341[RHSA-2025:12341] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:12342[RHBA-2025:12342] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.7 --pullspecs
----

[id="ocp-4-19-7-enhancements_{context}"]
==== Enhancements

* The KubeVirt Container Storage Interface (CSI) driver now supports volume expansion. Users can dynamically increase the size of their persistent volumes in their tenant cluster. This capability simplifies storage management, allowing for more flexible and scalable infrastructure. (link:https://issues.redhat.com/browse/OCPBUGS-58239[OCPBUGS-58239])

[id="ocp-4-19-7-bug-fixes_{context}"]
==== Bug fixes

* Before this update, a plugin conflict in the console modal occurred due to multiple plugins using the same `CreateProjectModal` extension point. As a consequence, only one plugin extension was used and the list order could not be changed. With this release, an update to the plugin store resolves extensions in the same order that are defined in the console operator configuration. As a result, anyone with permission to update the operator configuration can set the priority of the plugin. (link:https://issues.redhat.com/browse/OCPBUGS-56280[OCPBUGS-56280])

* Before this update, when you clicked *Configure* in an `AlertmanagerReceiversNotConfigured` alert on the *Overview* page, a runtime error occurred.  With this release, improved navigation handling ensures that no runtime errors occur when you click *Configure*. (link:https://issues.redhat.com/browse/OCPBUGS-57105[OCPBUGS-57105])

* Before this update, the `/metrics/usage` endpoint was updated to include authentication and Cross-Site Request Forgery (CSRF) protections. Because of this, requests to this endpoint started failing with a "forbidden" error message because the requests lacked the necessary CSRF token in the request cookie. With this release, a CSRF token was added to the `/metrics/usage` request cookie, which resolved the forbidden error message. (link:https://issues.redhat.com/browse/OCPBUGS-58331[OCPBUGS-58331])

* Before this update, when you configured an OpenID Connect (OIDC) provider for a `HostedCluster` resource with an Open ID cluster that did not specify a client secret, a default secret name was automatically generated. As a consequence, you could not configure OIDC public clients because these clients cannot use client secrets. With this release, a default secret name is not generated when no client secret is provided. As a result, you can configure OIDC public clients. (link:https://issues.redhat.com/browse/OCPBUGS-58683[OCPBUGS-58683])

* Before this update, when a Bare Metal Host (BMH) was marked as `Provisioned` or `ExternallyProvisioned`, the system would try to deprovision it or power it off first and the `DataImage` attached to the BMH would also prevent deletion. This issue blocked or slowed down host removal, creating operational inefficiencies. With this release, if the BMH has the `detached annotation` status and deletion is requested, the BMH transitions to the deleting state, allowing for direct deletion. (link:https://issues.redhat.com/browse/OCPBUGS-59133[OCPBUGS-59133])

* Before this update, downloads on control plane nodes were inconsistently scheduled because of a mismatch between the node selector for downloads and the console pods. As a consequence, downloads were scheduled on random nodes, which caused potential resource contention and sub-optimal performance. With this release, downloaded workloads consistently schedule on control plane nodes, which improves resource allocation. (link:https://issues.redhat.com/browse/OCPBUGS-59488[OCPBUGS-59488])

* Before this update, a cluster upgrade to {product-title} 4.18 caused inconsistent egress IP allocation due to stale Network Address Translation (NAT) handling. This issue occurred only when you deleted an egress IP pod while the OVN-Kubernetes controller for an egress node was down. As a consequence, duplicate Logical Router Policies and egress IP usage occurred, which caused inconsistent traffic flow and outage. With this release, egress IP allocation cleanup ensures consistent and reliable egress IP allocation in {product-title} 4.18 clusters. (link:https://issues.redhat.com/browse/OCPBUGS-59530[OCPBUGS-59530])

* Before this update, if you did not have sufficient privileges when you logged into the console, the `get started` message occupied excessive space on pages. This issue prevented the complete display of important status messages such as `no resources found`. As a consequence, truncated versions of the messages were displayed. With this release, the `get started` message is resized and the page's disable property is removed to use less screen space and to allow scrolling. This fix allows users to view complete statuses and information on all pages. You can now view complete statuses and information on all pages. As a result, the `get started` content remains fully accessible through scrolling, which ensures the visibility of new user guidance and important system messages. (link:https://issues.redhat.com/browse/OCPBUGS-59639[OCPBUGS-59639])

* Before this update, when you cloned a `.tar` file with zero length, the `oc-mirror` ran indefinitely due to an empty archive file. As a consequence, no progress occurred when you mirrored a 0-byte `.tar` file. With this release, 0-byte `.tar` files are detected and reported as errors, which prevents the `oc-mirror` from hanging. (link:https://issues.redhat.com/browse/OCPBUGS-59779[OCPBUGS-59779])

* Before this update, the `oc-mirror` did not detect Helm Chart images that used an aliased sub-chart. As a consequence, the Helm Chart images were missing after mirroring. With this release, the `oc-mirror` detects and mirrors Helm Chart images with an aliased sub-chart. (link:https://issues.redhat.com/browse/OCPBUGS-59799[OCPBUGS-59799])

[id="ocp-4-19-7-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.6
[id="ocp-4-19-6_{context}"]
=== RHSA-2025:11673 - {product-title} {product-version}.6 image release, bug fix, and security update advisory

Issued: 29 July 2025

{product-title} release {product-version}.6, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2025:11673[RHSA-2025:11673] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:11674[RHBA-2025:11674] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.6 --pullspecs
----

[id="ocp-4-19-6-enhancements_{context}"]
==== Enhancements

* The KubeVirt Container Storage Interface (CSI) driver now supports volume expansion. Users can dynamically increase the size of their persistent volumes in their tenant cluster. This capability simplifies storage management, allowing for more flexible and scalable infrastructure. (link:https://issues.redhat.com/browse/OCPBUGS-58239[OCPBUGS-58239])

[id="ocp-4-19-6-bug-fixes_{context}"]
==== Bug fixes

* Before this update, the `/metrics/usage` endpoint was updated to include authentication and Cross-Site Request Forgery (CSRF) protections. Because of this, requests to this endpoint started failing with a "forbidden" error message because the requests lacked the necessary CSRF token in the request cookie. With this release, a CSRF token was added to the `/metrics/usage` request cookie, resolving the forbidden error message. (link:https://issues.redhat.com/browse/OCPBUGS-58331[OCPBUGS-58331])

* Before this update, the `console.flag/model` extension point did not work, preventing flags from being properly set when their associated model was provided. With this release, the `console.flag/model` works as expected and properly sets a flag when the associated model is provided. (link:https://issues.redhat.com/browse/OCPBUGS-59513[OCPBUGS-59513])

[id="ocp-4-19-6-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.5
[id="ocp-4-19-5_{context}"]
=== RHSA-2025:11363 - {product-title} {product-version}.5 image release, bug fix, and security update advisory

Issued: 22 July 2025

{product-title} release {product-version}.5, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2025:11363[RHSA-2025:11363] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:11364[RHBA-2025:11364] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.5 --pullspecs
----

[id="ocp-4-19-5-bug-fixes_{context}"]
==== Bug fixes

* Before this update, bundle unpack jobs did not inherit control-plane tolerances from the catalog-operator that created them. As a consequence, the bundle unpack jobs ran on only worker nodes. If no worker nodes were available due to taints, then admins were unable to install or upgrade Operators on the cluster. With this release, control-plane tolerations are adopted for bundle unpack jobs so that the jobs are executed on primary nodes as part of the control plane. (link:https://issues.redhat.com/browse/OCPBUGS-59258[OCPBUGS-59258])

* Before this update, intermittent egress internet protocol (IP) handling due to inconsistent state updates in `OVNkubernetes`caused packet drops. These packet drops affected network traffic flow. With this release, `OVNkubernetes`pods consistently use their assigned egress IPs. As a result, dropped packages are reduced and network traffic flow is improved. (link:https://issues.redhat.com/browse/OCPBUGS-59234[OCPBUGS-59234])

* Before this update, the {aws-first} Cloud Provider did not set the default ping target of `HTTP:10256/healthz` for the  {aws-short} Load Balancer. For the LoadBalancer Services that ran on {aws-short}, the Load Balancer object created in {aws-short} had a ping target of `TCP:32518`. As a consequence, the health probes for cluster-wide services did not work and the services were down during upgrades. With this release, the cloud config `ClusterServiceLoadBalancerHealthProbeMode` property is set to `Shared` to ensure that the config is passed to the {aws-short} Cloud Provider. As a result, the {aws-short} Load Balancers have the correct health check ping target of `HTTP:10256/healthzwhich`. (link:https://issues.redhat.com/browse/OCPBUGS-59101[OCPBUGS-59101])

* Before this update, the `MachineConfigOperator` (MCO) installed the `podman-etcd` agent to enable testing while waiting for the RPM Package Manager (RPM) version to reach the repositories. With this release, the agent that was installed by MCO is removed because the RPM version is available. (link:https://issues.redhat.com/browse/OCPBUGS-58894[OCPBUGS-58894])

* Before this update, when you ran the `oc-mirror v2` disk-to-mirror workflow without valid mirror tar files, the returned error messages did not correctly identify the problem. With this release, the `oc-mirror v2` workflow returns an error message that states `no tar archives matching "mirror_[0-9]{6}\.tar" found in "<directory>"`. (link:https://issues.redhat.com/browse/OCPBUGS-58341[OCPBUGS-58341])

* Before this update, the build controller searched for secrets that were linked for general use rather than specifically for the image pull. With this release, when the controller searches for the default image pull secrets, the builds use `ImagePullSecrets` that are linked to the service account. (link:https://issues.redhat.com/browse/OCPBUGS-57951[OCPBUGS-57951])

* Before this update, combined specification and status updates lists triggered unnecessary firmware upgrades, which caused system downtime. With this release, a firmware upgrade optimization skips unnecessary firmware upgrades when a Baseboard Management Controller (BMC) URL is added. (link:https://issues.redhat.com/browse/OCPBUGS-56765[OCPBUGS-56765])

* Before this update, when you defined the `blockedImages` value in the `imageSetConfiguration` parameter for `oc-mirror v2`, you were required to provide an extensive list of image references for excluding images from mirroring. This requirement sometimes prevented the exclusion of images from mirroring because the image digests changed between executions. With this release, you can use regular expressions for the `blockedImages` value to facilitate the exclusion of images from mirroring. (link:https://issues.redhat.com/browse/OCPBUGS-56728[OCPBUGS-56728])

* Before this update the *Observe > Metrics > query > QueryKebab > Export as csv* drop-down item did not handle a undefined title element. As a consequence, you could not export the CSV file for certain queries on the *Metrics* tab of {product-title} Lister versions 4.16, 4.17, and 4.18. With this release, the metrics download for all queries correctly handles object properties in the drop-down menu items. As a result, the CSV export for all queries works on the *Metrics* page. (link:https://issues.redhat.com/browse/OCPBUGS-52592[OCPBUGS-52592])

[id="ocp-4-19-5-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.4
[id="ocp-4-19-4_{context}"]
=== RHSA-2025:10771 - {product-title} {product-version}.4 image release, bug fix, and security update advisory

Issued: 15 July 2025

{product-title} release {product-version}.4, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2025:10771[RHSA-2025:10771] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2025:10772[RHBA-2025:10772] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.4 --pullspecs
----

[id="ocp-4-19-4-bug-fixes_{context}"]
==== Bug fixes

* Previously, when the Gateway API feature was enabled, it installed an Istio control plane configured with one pod replica and an associated `PodDisruptionBudget` setting. The `PodDisruptionBudget` setting prevented the only pod replica from being evicted, blocking cluster upgrades. With this release, the Ingress Operator prevents the Istio control plane from being configured with the `PodDisruptionBudget` setting allowing cluster upgrades. (link:https://issues.redhat.com/browse/OCPBUGS-58394[OCPBUGS-58394])

* Previously, a runtime error occurred when clicking *Edit HorizontalPodAutoscaler* using the form view. With this release, the *Edit HorizontalPodAutoscaler* form view renders as expected. (link:https://issues.redhat.com/browse/OCPBUGS-58377[OCPBUGS-58377])

* Previously, forward slashes were permitted in `console.tab/horizontalNav` `href` values. Starting in version 4.15, a regression resulted in forward slashes no longer working correctly when used in `href` values. With this release, forward slashes in `console.tab/horizontalNav` `href` values continue to work as expected. (link:https://issues.redhat.com/browse/OCPBUGS-58375[OCPBUGS-58375])

* Previously, when a hosted cluster was configured with a proxy URL such as `\http://user:pass@host`, the authentication header was not getting forwarded by the Konnectivity proxy to the user proxy, which caused authentication to fail. With this release, the proper authentication header is sent when a user and password is specified in the proxy URL. (link:https://issues.redhat.com/browse/OCPBUGS-58335[OCPBUGS-58335])

* Previously, a subset of endpoints on the console backend were gated by `TokenReview` requests to the API server. In some cases, the API server would throttle these requests, causing slower load times in the UI. With this release, the `TokenReview` gating was removed from all but one of our endpoints resulting in improved performance. (link:https://issues.redhat.com/browse/OCPBUGS-58316[OCPBUGS-58316])

* Previously, the amount of requests that the oc-mirror plugin v2 sent many requests to container registries caused container registries to reject some requests with a `too many requests` error. With this release, the default values for several related parameters were adjusted to result in fewer requests being sent to the container registries. (link:https://issues.redhat.com/browse/OCPBUGS-58279[OCPBUGS-58279])

* Previously, the kubelet server certificate was not updated after certificate rotation due to unauthorized access to the API server causing the cluster to start in an unhealthy state. With this release, the kubelet server certificate is updated after certificate rotation, ensuring a healthy cluster state. (link:https://issues.redhat.com/browse/OCPBUGS-58116[OCPBUGS-58116])

* Previously, when on-premise installer-provisioned infrastructure deployments used the Cilium container network interface (CNI), the firewall rule that redirected traffic to the load balancer was ineffective. With this release, the rule works with the Cilium CNI and `OVNKubernetes`. (link:https://issues.redhat.com/browse/OCPBUGS-57781[OCPBUGS-57781])

* Previously, deleting an `istag` resource with the `--dry-run=server` option unintentionally caused actual deletion of the image from the server. This unexpected deletion occurred due to the dry-run option being implemented incorrectly in the `oc delete istag` command. With this release, the dry-run option is now wired to the `oc delete istag` command, preventing accidental deletion of image objects and the `istag` object remains intact when using the `--dry-run=server` option. (link:https://issues.redhat.com/browse/OCPBUGS-57206[OCPBUGS-57206])

* Previously, an outdated version of the Azure API prevented specifying a Capacity Reservation Group for a machine set, if that group resided in a different subscription than the one originating the server creation. With this release, {product-title} uses a newer version of the Azure API that is compatible with this configuration. (link:https://issues.redhat.com/browse/OCPBUGS-56163[OCPBUGS-56163])

[id="ocp-4-19-4-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.3
[id="ocp-4-19-3_{context}"]
=== RHBA-2025:10290 - {product-title} {product-version}.3 image release, bug fix, and security update advisory

Issued: 08 July 2025

{product-title} release {product-version}.3, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2025:10290[RHBA-2025:10290] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2025:10291[RHSA-2025:10291] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.3 --pullspecs
----

[id="ocp-4-19-3-bug-fixes_{context}"]
==== Bug fixes

* Previously, useful error messages were not generated when the `oc adm node-image create` command failed. With this release, the `oc adm node-image create` command provides error messages when the command fails. (link:https://issues.redhat.com/browse/OCPBUGS-58077[OCPBUGS-58077])

* Previously, when on-prem installer-provisioned infrastructure (IPI) deployments used the Cilium container network interface (CNI), the firewall rule that redirected traffic to the load balancer was ineffective. With this release, the rule works with the Cilium CNI and `OVNKubernetes`. (link:https://issues.redhat.com/browse/OCPBUGS-57781[OCPBUGS-57781])

* Previously, when you defined the `blockedImages` value in the `imageSetConfiguration` parameter for `oc-mirror v2`, you were required to provide an extensive list of image references for excluding images from mirroring. This requirement sometimes prevented the exclusion of images from mirroring because the image digests changed between executions. With this release, you can use regular expressions for the `blockedImages` value to facilitate the exclusion of images from mirroring. (link:https://issues.redhat.com/browse/OCPBUGS-56728[OCPBUGS-56728])

* Previously, certain traffic patterns with large packets running between {product-title} nodes and pods triggered an {product-title} host to send Internet Control Message Protocol (ICMP) needs frag to another {product-title} host. This situation lowered the viable maximum transmission unit (MTU) in the cluster. As a consequence, executing the `ip route show cache` command generated a cached route with a lower MTU than the physical link. Packets were dropped and {product-title} components were degrading because the host did not send pod-to-pod traffic with the large packets. With this release, NF Tables rules prevent the {product-title} nodes from lowering their MTU in response to traffic patterns with large packets. (link:https://issues.redhat.com/browse/OCPBUGS-55997[OCPBUGS-55997])

* Previously, you needed to update vSAN files to 8.0 P05 or later to enable clusters running {product-title} 4.19 to mount the network file system (NFS) volumes that were exported from the VMWare vSAN Files. With this release, you do not need to upgrade existing vSAN File Services versions to mount VMWare vSAN File volumes. (link:https://issues.redhat.com/browse/OCPBUGS-55978[OCPBUGS-55978])

[id="ocp-4-19-3-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.2
[id="ocp-4-19-2_{context}"]
=== RHSA-2025:9750 - {product-title} {product-version}.2 image release, bug fix, and security update advisory

Issued: 01 July 2025

{product-title} release {product-version}.2, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2025:9750[RHSA-2025:9750] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2025:9751[RHSA-2025:9751] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.2 --pullspecs
----

[id="ocp-4-19-2-bug-fixes_{context}"]
==== Bug fixes

* Previously, the installation program checked only the first compute machine pool entry in the install configuration to determine whether to disable the Machine Config Operator (MCO) boot image management feature. If multiple compute pools were specified ({aws-first} edge nodes are the only supported scenario), but another compute machine pool had a custom Amazon Machine Image (AMI), the installation program would not disable the MCO boot image management and the custom AMI would be overwritten by the MCO. With this release, the installation program checks all compute machine pool entries and if a custom image is found, the MCO boot image management is disabled. (link:https://issues.redhat.com/browse/OCPBUGS-58060[OCPBUGS-58060])

* Previously, if a user specified a custom boot image for {aws-first} or the {gcp-first}, the Machine Config Operator (MCO) would overwrite it with the default boot image during installation. With this release, a manifest generation was added for MCO configuration which disables the default boot image during installation if a custom image is specified. (link:https://issues.redhat.com/browse/OCPBUGS-57796[OCPBUGS-57796])

* Previously, a validation issue within `oc-mirror` plugin caused the command to reject the `file://.` reference. Users attempting to use `file://.` for a content path received an error message stating `content filepath is tainted`. With this release, the `oc-mirror` plugin properly validates the '.' directory reference. (link:https://issues.redhat.com/browse/OCPBUGS-57786[OCPBUGS-57786])

* Previously, the `oc-mirror v2` command was not using the correct filtered catalog during its operations, which led to errors such as including more operators than specified in the configuration, and trying to connect to the catalog registry during disk-to-mirror workflows even in air-gapped environments. With this release, the correct filtered catalog is used. (link:https://issues.redhat.com/browse/OCPBUGS-57784[OCPBUGS-57784])

* Previously, the {ols-official} UI would disappear when the *Create Project* modal was opened or when modals on the Networking pages were triggered. This was due to the modals using the `useModal` hook causing the modals to overwrite each other. With this release, the modals no longer overwrite each other allowing multiple UI elements to be displayed simultaneously. (link:https://issues.redhat.com/browse/OCPBUGS-57755[OCPBUGS-57755])

* Previously, the HAProxy configuration used the `/version` endpoint for health checks causing unreliable health checks to be generated. With this release, the liveness probe is customized to use `/livez?exclude=etcd&exclude=log` on IBM Cloud for more accurate health checks avoiding disruptions due to inappropriate probe configurations on Hypershift, while retaining `/version` for other platforms. (link:https://issues.redhat.com/browse/OCPBUGS-57485[OCPBUGS-57485])

* Previously, the installer failed when AWS credentials were not found and the survey was attempting AWS regions preventing users from creating the `install-config` file. With this release, the installer no longer fails when AWS credentials are not set, allowing users to input them during the survey. (link:https://issues.redhat.com/browse/OCPBUGS-57394[OCPBUGS-57394])

* Previously, cloning a persistent volume claim (PVC) in the web console resulted in an error due to an unsupported unit *B* for storage size. Because of this, users encountered errors when cloning the Red Hat OpenShift console PVC due to incorrect parsing of the storage size unit. With this release, support for *B* as unit of storage size has been removed from the Red{nbsp}Hat OpenShift console PVC. (link:https://issues.redhat.com/browse/OCPBUGS-57391[OCPBUGS-57391])

* Previously, {olmv1-first} was used to install Operators with the `olm.maxOpenShiftVersion` set to  `4.19`.  Due to an issue with the {olmv1} parsing logic for floating-point formatted `olm.maxOpenShiftVersion`values, the system failed to prevent upgrades to {product-title}. With this release, the parsing logic for  `olm.maxOpenShiftVersion` has been corrected preventing upgrades to {product-title} when Operators with `olm.maxOpenShiftVersion:4.19` are installed. (link:https://issues.redhat.com/browse/OCPBUGS-56852[OCPBUGS-56852])


* Previously, one of the `keepalived` health check scripts was failing due to missing permissions. This could result in the incorrect assignment of the ingress Virtual IP Address (VIP) in environments using shared ingress services.  With this release, the necessary permission was added back to the container so the health check now works correctly.  (link:https://issues.redhat.com/browse/OCPBUGS-56623[OCPBUGS-56623])


[id="ocp-4-19-2-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

// 4.19.1
[id="ocp-4-19-1_{context}"]
=== RHSA-2025:9278 - {product-title} {product-version}.1 image release, bug fix, and security update advisory

Issued: 24 June 2025

{product-title} release {product-version}.1, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2025:9278[RHSA-2025:9278] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2025:9279[RHSA-2025:9279] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.1 --pullspecs
----

[id="ocp-4-19-1-bug-fixes_{context}"]
==== Bug fixes

* Previously, when you added vCenter cloud credentials for the post installation of the {ai-full}, a bug was triggered because of an invalid `ConfigMap` object for the cloud provider configuration. As a result, a `missing vcenterplaceholder` error was displayed. With this release, the `ConfigMap` data is correct, and the error is not displayed. (link:https://issues.redhat.com/browse/OCPBUGS-57384[OCPBUGS-57384])

* Previously, a network issue during an API call in a cluster caused a timeout in {olmv0-first}. As a consequence, Operator installations often failed because of timeout issues. With this release, the catalog cache refresh interval is updated to resolve timeout issues. As a result, the likelihood of Operator installation timeouts is reduced. (link:https://issues.redhat.com/browse/OCPBUGS-57352[OCPBUGS-57352])

* Previously, Operator group reconciliation in {olmv0-first} triggered unnecessary `ClusterRole` updates because of the changing order of aggregation rule selectors. As a result, unnecessary API server writes occurred. With this release, a bug fix ensures the deterministic order of a `ClusterRoleSelectors` array in the aggregation rule, reducing unnecessary API server writes and improving cluster stability. (link:https://issues.redhat.com/browse/OCPBUGS-57279[OCPBUGS-57279])

* Previously, ignoring the `AdditionalTrustBundlePolicy` setting in the assisted-service's installation configuration led to Federal Information Processing Standard (FIPS) and other installation configuration overrides. With this release, the installation configuration includes an `AdditionalTrustBundlePolicy` field, which you can set to ensure that FIPS and other installation configuration overrides function as intended. (link:https://issues.redhat.com/browse/OCPBUGS-57208[OCPBUGS-57208])

* Previously, the authentication process for the `/metrics` endpoint was missing a token review check and caused unauthorized requests. As a result, the {product-title} console was prone to `TargetDown` alerts. With this release, the token review for unauthorized requests occurs with the user token provided in the request context. As a result, unauthorized requests to the {product-title} console do not cause `TargetDown` alerts. (link:https://issues.redhat.com/browse/OCPBUGS-57180[OCPBUGS-57180])

* Previously, the *Started* column was hidden when screen size was reduced. As a consequence, the `VirtualizedTable` component malfunctioned because of a missing sort function, and the table sorting functionality was affected on the `PipelineRun` list pages. With this release, the table component handles missing sort functions correctly for reduced screen sizes. (link:https://issues.redhat.com/browse/OCPBUGS-57110[OCPBUGS-57110])

* Previously, if you configured a masthead logo for a theme but used the default settings for the rest of the theme, the logo shown on the user interface was inconsistent. With this release, the masthead logo displays a default option for both light and dark themes, improving the interface consistency. (link:https://issues.redhat.com/browse/OCPBUGS-57054[OCPBUGS-57054])

* Previously, the cluster installation failed because of an invalid security group configuration for a Network Load Balancer (NLB). This failure prevented the traffic from both primary subnets for bootstrapping. With this release, the security group allows traffic from both primary subnets for bootstrapping, and the cluster installation does not fail because of security group restrictions on additional primary subnets. (link:https://issues.redhat.com/browse/OCPBUGS-57039[OCPBUGS-57039])

* Previously, users without project access saw an incomplete roles list on the *Roles* page because of improper API group access. With this release, users without project access cannot see an incomplete roles list on the *Roles* page. (link:https://issues.redhat.com/browse/OCPBUGS-56987[OCPBUGS-56987])

* Previously, the `node-image create` command modified directory permissions and caused user directories to lose original permissions during the operation. With this release, the `node-image create` command preserves file permissions during the file-copying process by using the `rsync` tool, and ensures that user directories maintain original permissions during the operation. (link:https://issues.redhat.com/browse/OCPBUGS-56905[OCPBUGS-56905])

* Previously, the `delete` keyword in image names was allowed in the `ImageSetConfiguration` file, which is not supported. As a consequence, users encountered errors while mirroring images. With this release, the error for image names ending with `delete` in the `ImageSetConfiguration` file has been removed. As a result, users can now successfully mirror images with names ending in `delete`. (link:https://issues.redhat.com/browse/OCPBUGS-56798[OCPBUGS-56798])

* Previously, the user interface asyncin the *Observe Alerting* field displayed incorrect alert severity icons for information alerts. With this release, the alert severity icons match in the *Observe Alerting* field. As a result, alert icons match consistently, reducing potential confusion for users. (link:https://issues.redhat.com/browse/OCPBUGS-56470[OCPBUGS-56470])

* Previously, if you used an unauthorized access configuration file in the `oc-mirror` command, an `Unauthorized` error was displayed when you synchronized your image sets. With this release, the Docker configuration is updated to use a custom authorization file for authentication. You can successfully synchronize your image sets without encountering the `Unauthorized` error. (link:https://issues.redhat.com/browse/OCPBUGS-55701[OCPBUGS-55701])

[id="ocp-4-19-1-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

//Update with relevant advisory information
[id="ocp-4-19-0-ga_{context}"]
=== RHSA-2024:11038 - {product-title} {product-version}.0 image release, bug fix, and security update advisory

Issued: 17 June 2025

{product-title} release {product-version}.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:11038[RHSA-2024:11038] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHEA-2025:2851[RHEA-2025:2851] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.0 --pullspecs
----

[id="ocp-4-19-0-updating_{context}"]
==== Updating
To update an {product-title} 4.19 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].
