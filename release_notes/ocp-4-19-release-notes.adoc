:_mod-docs-content-type: ASSEMBLY
[id="ocp-4-19-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red{nbsp}Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-19-about-this-release_{context}"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-202X:XXXX[RHSA-202X:XXXX]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.32.md[Kubernetes 1.32] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. From the {hybrid-console}, you can deploy {product-title} clusters to either on-premises or cloud environments.

You must use {op-system} machines for the control plane and for the compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517
//Removed paragraph about the RHEL package because mode workers are removed from 4.19, per Scott Dodson
//Even-numbered release lifecycle verbiage (Comment in for even-numbered releases)
////
Starting from {product-title} 4.14, the Extended Update Support (EUS) phase for even-numbered releases increases the total available lifecycle to 24 months on all supported architectures, including `x86_64`, 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures. Beyond this, Red{nbsp}Hat also offers a 12-month additional EUS add-on, denoted as _Additional EUS Term 2_, that extends the total available lifecycle from 24 months to 36 months. The Additional EUS Term 2 is available on all architecture variants of {product-title}. For more information about support for all versions, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].
////

//Odd-numbered release lifecycle verbiage (Comment in for odd-numbered releases)

The support lifecycle for odd-numbered releases, such as {product-title} {product-version}, on all supported architectures, including `x86_64`, 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures is 18 months. For more information about support for all versions, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

Commencing with the {product-title} 4.14 release, Red{nbsp}Hat is simplifying the administration and management of Red{nbsp}Hat shipped cluster Operators with the introduction of three new life cycle classifications; Platform Aligned, Platform Agnostic, and Rolling Stream. These life cycle classifications provide additional ease and transparency for cluster administrators to understand the life cycle policies of each Operator and form cluster maintenance and upgrade plans with predictable support boundaries. For more information, see link:https://access.redhat.com/webassets/avalon/j/includes/session/scribe/?redirectTo=https%3A%2F%2Faccess.redhat.com%2Fsupport%2Fpolicy%2Fupdates%2Fopenshift_operators[OpenShift Operator Life Cycles].

// Added in 4.14. Language came directly from Kirsten Newcomer.
{product-title} is designed for FIPS. When running {op-system-base-full} or {op-system-first} booted in FIPS mode, {product-title} core components use the {op-system-base} cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the `x86_64`, `ppc64le`, and `s390x` architectures.

For more information about the NIST validation program, see link:https://csrc.nist.gov/Projects/cryptographic-module-validation-program/validated-modules[Cryptographic Module Validation Program]. For the latest NIST status for the individual versions of {op-system-base} cryptographic libraries that have been submitted for validation, see link:https://access.redhat.com/articles/2918071#fips-140-2-and-fips-140-3-2[Compliance Activities and Government Standards].

[id="ocp-4-19-add-on-support-status_{context}"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-19-new-features-and-enhancements_{context}"]
== New features and enhancements

This release adds improvements related to the following components and concepts:

[id="ocp-release-notes-auth_{context}"]
=== Authentication and authorization

[id="ocp-release-notes-auth-direct_{context}"]
==== Enabling direct authentication with an external OIDC identity provider (Technology Preview)

With this release, you can enable direct integration with an external OpenID Connect (OIDC) identity provider to issue tokens for authentication. This bypasses the built-in OAuth server and uses the external identity provider directly.

By integrating directly with an external OIDC provider, you can leverage the advanced capabilities of your preferred OIDC provider instead of being limited by the capabilities of the built-in OAuth server. Your organization can manage users and groups from a single interface, while also streamlining authentication across multiple clusters and in hybrid environments. You can also integrate with existing tools and solutions.

Direct authentication is available as a Technology Preview feature.

For more information, see xref:../authentication/external-auth.adoc#external-auth[Enabling direct authentication with an external OIDC identity provider].

[id="ocp-4-19-auth-ServiceAccountTokenNodeBinding_{context}"]
==== Enable ServiceAccountTokenNodeBinding Kubernetes feature by default

In {product-title} {product-version}, the `ServiceAccountTokenNodeBinding` feature is now enabled by default, aligning with upstream Kubernetes behavior. This feature allows service account tokens to be bound directly to node objects in addition to the existing binding options. Benefits of this change include enhanced security through automatic token invalidation when bound nodes are deleted and better protection against token replay attacks across different nodes.

[id="ocp-release-notes-backup-restore_{context}"]
=== Backup and restore

[id="ocp-release-notes-builds_{context}"]
=== Builds

[id="ocp-release-notes-cro_{context}"]
=== Cluster Resource Override Admission Operator

[id="ocp-release-notes-documentation_{context}"]
=== Documentation

[id="ocp-release-notes-documentation-etcd_{context}"]
==== Consolidated etcd documentation

This release includes an _etcd_ section, which consolidates all of the existing documentation about etcd for {product-title}. For more information, see xref:../etcd/etcd-overview.adoc#etc-overview[Overview of etcd].

[id="ocp-release-notes-documentation-tutorials_{context}"]
==== Tutorials guide

{product-title} 4.19 now includes a _Tutorials_ guide, which takes the place of the _Getting started_ guide in previous releases. The existing tutorials were refreshed and the guide now focuses solely on hands-on tutorial content. It also provides a jumping off point to other recommended hands-on learning resources for {product-title} across Red{nbsp}Hat.

For more information, see xref:../tutorials/index.adoc#tutorials-overview[Tutorials].

[id="ocp-release-notes-edge-computing_{context}"]
=== Edge computing

[id="ocp-4-19-edge-computing_pg-ztp-rhacm_{context}"]
==== Using {rh-rhacm} PolicyGenerator resources to manage {ztp} cluster policies (General Availability)

You can now use `PolicyGenerator` resources and {rh-rhacm-first} to deploy polices for managed clusters with {ztp}.
The `PolicyGenerator` API is part of the link:https://open-cluster-management.io/[Open Cluster Management] standard and provides a generic way of patching resources, which is not possible with the `PolicyGenTemplate` API.
Using `PolicyGenTemplate` resources to manage and deploy polices will be deprecated in an upcoming {product-title} release.

For more information, see xref:../edge_computing/policygenerator_for_ztp/ztp-configuring-managed-clusters-policygenerator.adoc#ztp-configuring-managed-clusters-policygenerator[Configuring managed cluster policies by using PolicyGenerator resources].

[id="ocp-release-notes-extensions_{context}"]
=== Extensions ({olmv1})

[id="ocp-release-notes-olmv1-preflight-permissions-check_{context}"]
==== Preflight permissions check for cluster extensions (Technology Preview)

With this release, the Operator Controller performs a dry run of the installation process when you try to install an extension. This dry run verifies that the specified service account has the required role-based access control (RBAC) rules for the roles and bindings defined by the bundle.

If the service account is missing any required RBAC rules, the preflight check fails before the actual installation proceeds and generates a report.

For more information, see xref:../extensions/ce/managing-ce.adoc#olmv1-troubleshooting-rbac-errors-with-preflight-check_managing-ce[Preflight permissions check for cluster extensions (Technology Preview)]

[id="ocp-release-notes-olmv1-deploying-a-cluster-extension-to-a-specific-namespace_{context}"]
==== Deploying a cluster extension in a specific namespace (Technology Preview)

With this release, you can deploy an extension in a specific namespace by using the `OwnNamespace` or `SingleNamespace` install modes as a Technology Preview feature for `registry+v1` Operator bundles.

For more information, see xref:../extensions/ce/managing-ce.adoc#olmv1-deploying-a-ce-in-a-specific-namespace_managing-ce[Deploying a cluster extension in a specific namespace (Technology Preview)]

[id="ocp-release-notes-hcp_{context}"]
=== Hosted control planes

Because {hcp} releases asynchronously from {product-title}, it has its own release notes. For more information, see xref:../hosted_control_planes/hosted-control-planes-release-notes.adoc#hosted-control-planes-release-notes[{hcp-capital} release notes].

[id="ocp-release-notes-ibm-power_{context}"]
=== {ibm-power-title}

The {ibm-power-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on {ibm-power-title}:

* Expand Compliance Operator support with profiles for Defense Information Systems Agency Security Technical Implementation Guide (DISA STIG)

[id="ocp-release-notes-ibm-z_{context}"]
=== {ibm-z-title} and {ibm-linuxone-title}

The {ibm-z-name} and {ibm-linuxone-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on {ibm-z-name} and {ibm-linuxone-name}:

* Support for {ibm-name} z17 and {ibm-linuxone-name} 5
* Boot volume Linux Unified Key Setup (LUKS) encryption via {ibm-name} Crypto Express (CEX)

[discrete]
[id="ocp-release-notes-ibm-z-power-support-matrix_{context}"]
=== {ibm-power-title}, {ibm-z-title}, and {ibm-linuxone-title} support matrix

Starting in {product-title} 4.14, Extended Update Support (EUS) is extended to the {ibm-power-name} and the {ibm-z-name} platform. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

.CSI Volumes
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Cloning
|Supported
|Supported

|Expansion
|Supported
|Supported

|Snapshot
|Supported
|Supported
|====

.Multus CNI plugins
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Bridge
|Supported
|Supported

|Host-device
|Supported
|Supported

|IPAM
|Supported
|Supported

|IPVLAN
|Supported
|Supported
|====

.{product-title} features
[cols="3,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Adding compute nodes to on-premise clusters using {oc-first}
|Supported
|Supported

|Alternate authentication providers
|Supported
|Supported

|Agent-based Installer
|Supported
|Supported

|Assisted Installer
|Supported
|Supported

|Automatic Device Discovery with Local Storage Operator
|Unsupported
|Supported

|Automatic repair of damaged machines with machine health checking
|Unsupported
|Unsupported

|Cloud controller manager for {ibm-cloud-name}
|Supported
|Unsupported

|Controlling overcommit and managing container density on nodes
|Unsupported
|Unsupported

|CPU manager
|Supported
|Supported

|Cron jobs
|Supported
|Supported

|Descheduler
|Supported
|Supported

|Egress IP
|Supported
|Supported

|Encrypting data stored in etcd
|Supported
|Supported

|FIPS cryptography
|Supported
|Supported

|Helm
|Supported
|Supported

|Horizontal pod autoscaling
|Supported
|Supported

|Hosted control planes
|Supported
|Supported

|IBM Secure Execution
|Unsupported
|Supported

|Installer-provisioned Infrastructure Enablement for {ibm-power-server-name}
|Supported
|Unsupported

|Installing on a single node
|Supported
|Supported

|IPv6
|Supported
|Supported

|Monitoring for user-defined projects
|Supported
|Supported

|Multi-architecture compute nodes
|Supported
|Supported

|Multi-architecture control plane
|Supported
|Supported

|Multipathing
|Supported
|Supported

|Network-Bound Disk Encryption - External Tang Server
|Supported
|Supported

|Non-volatile memory express drives (NVMe)
|Supported
|Unsupported

|nx-gzip for Power10 (Hardware Acceleration)
|Supported
|Unsupported

|oc-mirror plugin
|Supported
|Supported

|OpenShift CLI (`oc`) plugins
|Supported
|Supported

|Operator API
|Supported
|Supported

|OpenShift Virtualization
|Unsupported
|Supported

|OVN-Kubernetes, including IPsec encryption
|Supported
|Supported

|PodDisruptionBudget
|Supported
|Supported

|Precision Time Protocol (PTP) hardware
|Unsupported
|Unsupported

|{openshift-local-productname}
|Unsupported
|Unsupported

|Scheduler profiles
|Supported
|Supported

|Secure Boot
|Unsupported
|Supported

|Stream Control Transmission Protocol (SCTP)
|Supported
|Supported

|Support for multiple network interfaces
|Supported
|Supported

|The `openshift-install` utility to support various SMT levels on {ibm-power-name} (Hardware Acceleration)
|Supported
|Unsupported

|Three-node cluster support
|Supported
|Supported

|Topology Manager
|Supported
|Unsupported

|z/VM Emulated FBA devices on SCSI disks
|Unsupported
|Supported

|4K FCP block device
|Supported
|Supported
|====

.Operators
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|{cert-manager-operator}
|Supported
|Supported

|Cluster Logging Operator
|Supported
|Supported

|Cluster Resource Override Operator
|Supported
|Supported

|Compliance Operator
|Supported
|Supported

|Cost Management Metrics Operator
|Supported
|Supported

|File Integrity Operator
|Supported
|Supported

|HyperShift Operator
|Supported
|Supported

|{ibm-power-server-name} Block CSI Driver Operator
|Supported
|Unsupported

|Ingress Node Firewall Operator
|Supported
|Supported

|Local Storage Operator
|Supported
|Supported

|MetalLB Operator
|Supported
|Supported

|Network Observability Operator
|Supported
|Supported

|NFD Operator
|Supported
|Supported

|NMState Operator
|Supported
|Supported

|OpenShift Elasticsearch Operator
|Supported
|Supported

|Vertical Pod Autoscaler Operator
|Supported
|Supported
|====

.Persistent storage options
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}
|Persistent storage using iSCSI
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using local volumes (LSO)
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using hostPath
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Fibre Channel
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Raw Block
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using EDEV/FBA
|Supported ^[1]^
|Supported ^[1]^,^[2]^
|====
[.small]
--
1. Persistent shared storage must be provisioned by using either {rh-storage-first} or other supported storage protocols.
2. Persistent non-shared storage must be provisioned by using local storage, such as iSCSI, FC, or by using LSO with DASD, FCP, or EDEV/FBA.
--

[id="ocp-release-notes-insights-operator-enhancements_{context}"]
=== Insights Operator


[id="ocp-release-notes-insights-operator-runtime-extractor_{context}"]
==== Insights Runtime Extractor is generally available

In {product-title} 4.18, the Insights Operator introduced the _Insights Runtime Extractor_ workload data collection feature as a Technology Preview feature to help Red{nbsp}Hat better understand the workload of your containers.
Now, in version 4.19, the feature is generally available.
The Insights Runtime Extractor feature gathers runtime workload data and sends it to Red{nbsp}Hat.

[id="ocp-release-notes-installation-and-update_{context}"]
=== Installation and update

[id="ocp-4-19-installation-and-update-remove-terraform-ibm-cloud_{context}"]
==== Cluster API replaces Terraform on {ibm-cloud-title} installations

In {product-title} {product-version}, the installation program uses the Cluster API instead of Terraform to provision cluster infrastructure during installations on {ibm-cloud-title}.

[id="ocp-4-19-installation-and-update-aws-malaysia-thailand_{context}"]
==== Installing a cluster on {aws-short} in the Malaysia and Thailand regions

You can now install an {product-title} cluster on {aws-first} in the Malaysia (`ap-southeast-5`) and Thailand (`ap-southeast-7`) regions.

For more information, see xref:../installing/installing_aws/installing-aws-account.adoc#installation-aws-regions_installing-aws-account[Supported {aws-first} regions].

[id="ocp-4-19-installation-and-update-remove-terraform-azure-stack-hub_{context}"]
==== Cluster API replaces Terraform on {azure-first} Stack Hub installations

In {product-title} {product-version}, the installation program uses the Cluster API instead of Terraform to provision clusters during installer-provisioned infrastructure installations on {azure-first} Stack Hub.

[id="ocp-4-19-installation-and-update-support-azure-instance-types_{context}"]
==== Support added for additional {azure-first} instance types

Additional {azure-first} instance types for machine types based on 64-bit x86 architecture have been tested with {product-title} {product-version}.

For the Dxv6 machine series, the following instance types have been tested:

* `StandardDdsv6Family`
* `StandardDldsv6Family`
* `StandardDlsv6Family`
* `StandardDsv6Family`

For the Lsv4 and Lasv4 machine series, the following instance types have been tested:

* `standardLasv4Family`
* `standardLsv4Family`

For the ND and NV machine series, the following instance types have been tested:

* `StandardNVadsV710v5Family`
* `Standard NDASv4_A100 Family`

For more information, see xref:../installing/installing_azure/ipi/installing-azure-network-customizations.adoc#installation-azure-tested-machine-types_installing-azure-network-customizations[Tested instance types for {azure-short}] and link:https://learn.microsoft.com/en-us/azure/?product=popular[{azure-short} documentation] (Microsoft documentation).

[id="ocp-4-19-installation-and-update-azure-outbound-access-vms_{context}"]
==== Outbound access for VMs in {azure-first} will be retired

On 30 September 2025, the default outbound access connectivity for all new virtual machines (VMs) in {azure-first} will be retired. To enhance security, {azure-short} is moving towards a secure-by-default model where default outbound access to the internet will be turned off. However, configuration changes to {product-title} are not required. By default, the installation program creates an outbound rule for the load balancer.

For more information, see link:https://azure.microsoft.com/en-us/updates?id=default-outbound-access-for-vms-in-azure-will-be-retired-updates-and-more-information[Azure Updates] (Microsoft documentation), link:https://learn.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections#scenarios[Azure's outbound connectivity methods] (Microsoft documentation), and xref:../installing/installing_azure/upi/installing-azure-preparing-upi.adoc#installing-azure-preparing-upi[Preparing to install a cluster on {azure-short}].

[id="ocp-4-19-installation-and-update-gcp-confidential-computing-expansion_{context}"]
==== Additional Confidential Computing platforms for {gcp-short}
With this release, you can use additional Confidential Computing platforms on {gcp-short}. The new supported platforms, which can be enabled in the `install-config.yaml` file prior to installation, or configured after installation using machine sets and control plane machine sets, are as follows:

* `AMDEncryptedVirtualization`, which enables Confidential Computing with AMD Secure Encrypted Virtualization (AMD SEV)
* `AMDEncryptedVirtualizationNestedPaging`, which enables Confidential Computing with AMD Secure Encrypted Virtualization Secure Nested Paging (AMD SEV-SNP)
* `IntelTrustedDomainExtensions`, which enables Confidential Computing with Intel Trusted Domain Extensions (Intel TDX)

For more information, see xref:../installing/installing_gcp/installation-config-parameters-gcp.adoc#installation-config-parameters-additional-gcp_installation-config-parameters-gcp[Installation configuration parameters for {gcp-full}], xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-gcp.adoc#machineset-gcp-confidential-vm_cpmso-config-options-gcp[Configuring Confidential VM by using machine sets (control plane)], and xref:../machine_management/creating_machinesets/creating-machineset-gcp.html#machineset-gcp-confidential-vm_creating-machineset-gcp[Configuring Confidential VM by using machine sets (compute)].

[id="ocp-4-19-GCP-custom-dns_{context}"]
==== Installing a cluster on {gcp-first} with a user-provisioned DNS (Technology Preview)

With this release, you can enable a user-provisioned domain name server (DNS) instead of the default cluster-provisioned DNS solution. For example, your organization's security policies might not allow the use of public DNS services such as Google Cloud DNS. You can manage your DNS only for the IP addresses of the API and Ingress servers. If you use this feature, you must provide your own DNS solution that includes records for `api.<cluster_name>.<base_domain>.` and `*.apps.<cluster_name>.<base_domain>.`. Enabling a user-provisioned DNS is available as a Technology Preview feature.

For more information, see xref:../installing/installing_gcp/installing-gcp-customizations.adoc#installation-gcp-enabling-user-managed-DNS_installing-gcp-customizations[Enabling user-managed DNS].

[id="ocp-4-19-installation-and-update-vsphere-multidisk_{context}"]
==== Installing a cluster on {vmw-first} with multiple disks (Technology Preview)
With this release, you can install a cluster on {vmw-first} with multiple storage disks as a Technology Preview feature. You can assign these additional disks to special functions within the cluster, such as etcd storage.

For more information, see xref:../installing/installing_vsphere/installation-config-parameters-vsphere.adoc#installation-configuration-parameters-optional-vsphere_installation-config-parameters-vsphere[Optional vSphere configuration parameters].

[id="ocp-4-19-installation-and-update-azure-boot-diagnostics_{context}"]
==== Enabling boot diagnostics collection during installation on {azure-first}

With this release, you can enable boot diagnostics collection when you install a cluster on {azure-first}. Boot diagnostics is a debugging feature for {azure-short} virtual machines (VMs) to identify VM boot failures. You can set the `bootDiagnostics` parameter in the `install-config.yaml` file for compute machines, for control plane machines, or for all machines.

For more information, see xref:../installing/installing_azure/installation-config-parameters-azure.adoc#installation-configuration-parameters-additional-azure_installation-config-parameters-azure[Additional {azure-short} configuration parameters].

[id="ocp-4-19-admin-ack-updating_{context}"]
==== Required administrator acknowledgment when updating from {product-title} 4.18 to 4.19

{product-title} 4.19 uses Kubernetes 1.32, which removed several xref:../release_notes/ocp-4-19-release-notes.adoc#ocp-4-19-removed-kube-1-32-apis_{context}[deprecated APIs].

A cluster administrator must provide manual acknowledgment before the cluster can be updated from {product-title} 4.18 to 4.19. This is to help prevent issues after updating to {product-title} 4.19, where APIs that have been removed are still in use by workloads, tools, or other components running on or interacting with the cluster. Administrators must evaluate their cluster for any APIs in use that will be removed and migrate the affected components to use the appropriate new API version. After this is done, the administrator can provide the administrator acknowledgment.

All {product-title} 4.18 clusters require this administrator acknowledgment before they can be updated to {product-title} 4.19.

For more information, see xref:../updating/preparing_for_updates/updating-cluster-prepare.adoc#updating-cluster-prepare[Preparing to update to {product-title} 4.19].

[id="ocp-4-19-vsphere-host-groups_{context}"]
==== OpenShift zones support for vSphere host groups (Technology Preview)

With this release, you can map {product-title} failure domains to {vmw-full} host groups. This enables you to make use of the high availability offered by a {vmw-short} stretched cluster configuration. This feature is available as a Technology Preview in {product-title} {product-version}.

For information on configuring host groups at installation, see xref:../installing/installing_vsphere/ipi/installing-vsphere-installer-provisioned-customizations.adoc#installation-vsphere-regions-zones-host-groups_installing-vsphere-installer-provisioned-customizations[VMware vSphere host group enablement].

For information on configuring host groups for existing clusters, see xref:../installing/installing_vsphere/post-install-vsphere-zones-regions-configuration.adoc#specifying-host-groups-vsphere_post-install-vsphere-zones-regions-configuration[Specifying multiple host groups for your cluster on vSphere].

[id="ocp-release-notes-agent-nutanix_{context}"]
==== Nutanix support for the Agent-based Installer
With this release, you can now use the Agent-based Installer to install a cluster on Nutanix.
Installing a cluster on Nutanix with the Agent-based Installer is enabled by setting the `platform` parameter to `nutanix` in the `install-config.yaml` file.

For more information, see xref:../installing/installing_with_agent_based_installer/installation-config-parameters-agent.adoc#installation-configuration-parameters-required_installation-config-parameters-agent[Required configuration parameters] in the Agent-based Installer documentation.

[id="ocp-release-notes-machine-config-operator_{context}"]
=== Machine Config Operator

[id="ocp-release-notes-machine-config-operator-naming_{context}"]
==== New naming for features

_{op-system-first} image layering_ is now called _image mode for OpenShift_. As a part of this change, _on-cluster layering_ is now  called _on-cluster image mode_ and _out-of-cluster layering_ is now _out-of-cluster image mode_.

The _updated boot images_ feature is now called _boot image management_.

[id="ocp-release-notes-machine-config-operator-ocl-ga_{context}"]
==== Image mode for OpenShift is now generally available

Image mode for OpenShift, formerly called on-cluster layering, is now Generally Available (GA). The following changes have been introduced with the promotion to GA:

* The API version is now `machineconfiguration.openshift.io/v1`. The new version includes the following changes:
** The `baseImagePullSecret` parameter is now optional. If not specified, the default `global-pull-secret-copy` is used.
** The `buildInputs` parameter is no longer required. All parameters previously under the `buildInputs` parameter are promoted one level.
** The `containerfileArch` parameter now supports multiple architectures. Previously, only `noarch` was supported.
** The required `imageBuilderType` is now `Job`. Previously, the required builder was `PodImageBuilder`.
** The `renderedImagePushspec` parameter is now `renderedImagePushSpec`.
** The `buildOutputs` and `currentImagePullSecret` parameters are no longer required.

* The output of the `oc describe MachineOSConfig` and `oc describe MachineOSBuild` commands have multiple differences.

* The `global-pull-secret-copy` is automatically added to the `openshift-machine-config-operator` namespace.

* You can now revert an on-cluster custom layered image back to the base image by removing a label from the `MachineOSConfig` object

* You can now automatically delete an on-cluster custom layered image by deleting the associated `MachineOSBuild` object.

* The `must-gather` for the Machine Config Operator now includes data on the `MachineOSConfig` and `MachineOSBuild` objects.

* On-cluster layering is now supported in disconnected environments.

* On-cluster layering is now supported in single node OpenShift (SNO) clusters.

[id="ocp-release-notes-machine-config-operator-boot-image_{context}"]
==== Boot image management is now default for {gcp-first} and {aws-first}

The boot image management feature, previously called updated boot images, is now the default behavior in {gcp-first} and {aws-first} clusters. As such, after updating to {product-title} {product-version}, the boot images in your cluster are automatically updated to version {product-version}. With subsequent updates, the Machine Config Operator (MCO) again updates the boot images in your cluster. A boot images is associated with a machine set and is used when scaling new nodes. Any new nodes you create after updating are based on the new version. Current nodes are not affected by this feature.

Before upgrading to {product-version}, you must opt-out of this default behavior or acknowledge this change before proceeding. For more information, see xref:../machine_configuration/mco-update-boot-images.adoc#mco-update-boot-images-disable_machine-configs-configure[Disabling boot image management].

[NOTE]
====
The managed boot images feature is available for only {gcp} and {aws} clusters. For all other platforms, the MCO does not update the boot image with each cluster update.
====
[id="ocp-release-notes-machine-config-operator-cert-changes_{context}"]
==== Changes to the Machine Config Operator certificates
The Machine Config Server (MCS) CA bundle created by the installation program is now stored in the `machine-config-server-ca` config map in the `openshift-machine-config-operator` namespace. The bundle was previously stored in the `root-ca` configmap in the `kube-system namespace`. The `root-ca` configmap is no longer used in a cluster that cluster that is updated to {product-title} {product-version}. This change was made to make it clear that this CA bundle is managed by the Machine Config Operator (MCO).

The MCS signing key is stored in the `machine-config-server-ca` secret in the `openshift-machine-config-operator` namespace.

The MCS CA and MCS cert are valid for 10 years and are automatically rotated by the MCO at approximately 8 years. Upon update to {product-title} {product-version}, the CA signing key is not present. As a result, the CA bundle is immediately considered expired when the MCO certificate controller comes up. This expiration causes an immediate certificate rotation, even if the cluster is not 10 years old. After that point, the next rotation takes place at the standard 8 year period.

For more information about the MCO certificates, see xref:../security/certificate_types_descriptions/machine-config-operator-certificates.adoc#cert-types-machine-config-operator-certificates[Machine Config Operator certificates].

[id="ocp-release-notes-management-console_{context}"]
=== Management console

[id="ocp-release-notes-machine-management_{context}"]
=== Machine management

[id="ocp-4-19-cpms-prefix_{context}"]
==== Custom prefixes for control plane machine names

With this release, you can customize the prefix of machine names for machines created by the control plane machine set.
This feature is enabled by modifying the `spec.machineNamePrefix` parameter of the `ControlPlaneMachineSet` custom resource.

For more information, see xref:../machine_management/control_plane_machine_management/cpmso-configuration.adoc#cpmso-config-prefix_cpmso-configuration[Adding a custom prefix to control plane machine names].

[id="ocp-4-19-aws-capacity-reservations_{context}"]
==== Configuring Capacity Reservations on {aws-full} clusters

With this release, you can deploy machines that use Capacity Reservations, including On-Demand Capacity Reservations and Capacity Blocks for ML, on {aws-full} clusters.

You can configure these features with xref:../machine_management/creating_machinesets/creating-machineset-aws.adoc#machineset-capacity-reservation_creating-machineset-aws[compute] and xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-aws.adoc#machineset-capacity-reservation_cpmso-config-options-aws[control plane] machine sets.

[id="ocp-4-19-vmw-multi-disk_{context}"]
==== Support for multiple {vmw-full} data disks (Technology Preview)

With this release, you can add up to 29 disks to the virtual machine (VM) controller for your {vmw-short} cluster as a Technology Preview feature.
This capability is available for xref:../machine_management/creating_machinesets/creating-machineset-vsphere.adoc#machineset-vsphere-data-disks_creating-machineset-vsphere[compute] and xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-vsphere.adoc#machineset-vsphere-data-disks_cpmso-config-options-vsphere[control plane] machine sets.

[id="ocp-release-notes-monitoring_{context}"]
=== Monitoring

The in-cluster monitoring stack for this release includes the following new and modified features:

[id="ocp-4-19-monitoring-updates-to-monitoring-stack-components-and-dependencies"]
==== Updates to monitoring stack components and dependencies

This release includes the following version updates for in-cluster monitoring stack components and dependencies:

* Alertmanager to 0.28.1
* Prometheus to 3.2.1
* Prometheus Operator to 0.81.0
* Thanos to 0.37.2
* kube-state-metrics to 2.15.0
* node-exporter to 1.9.1

[id="ocp-4-19-monitoring-changes-to-alerting-rules"]
==== Changes to alerting rules

[NOTE]
====
Red{nbsp}Hat does not guarantee backward compatibility for recording rules or alerting rules.
====

* Added the `PrometheusPossibleNarrowSelectors` alert to warn users when PromQL queries or metric relabel configurations use selectors that could be too restrictive and might not take into account that values on the `le` label of classic histograms or the `quantile` label of summaries are floats in Prometheus v3. For more information, see the "Prometheus v3 upgrade" section.

[id="ocp-4-19-monitoring-prometheus-v3-upgrade"]
==== Prometheus v3 upgrade

This release introduces a major update to the Prometheus component, transitioning from v2 to v3. The monitoring stack and other core components include all of the necessary adjustments to ensure a smooth upgrade. However, some user-managed configurations might require modifications. The key changes include the following items:

* The values of the `le` label for classic histograms and the `quantile` label for summaries are normalized during ingestion. For example, the `example_bucket{le="10"}` metric selector is ingested as `example_bucket{le="10.0"}`. As a result, alerts, recording rules, dashboards, and relabeling configurations that reference label values as whole numbers, for example, `le="10"`, might no longer work as intended.
+
To mitigate the issue, update your selectors:

** If your queries need to cover data from both before and after the Prometheus upgrade, ensure both values are considered, for example, use a regular expression, `example_bucket{le=~"10(.0)?"}`.

** For queries that only cover data after the upgrade, use float values, for example, `le="10.0"`.

* Configurations that send alerts to additional Alertmanager instances through `additionalAlertmanagerConfigs` by using the Alertmanager v1 API are no longer supported.
+
To mitigate the issue, upgrade any affected Alertmanager instances to support the v2 API, which is supported since Alertmanager `v0.16.0`, and update your monitoring configuration to use the v2 scheme.

For more information about the changes between Prometheus v2 and v3, see link:https://prometheus.io/docs/prometheus/latest/migration/[Prometheus 3.0 migration guide].

[id="ocp-4-19-monitoring-metrics-collection-profiles-ga"]
==== Metrics collection profiles is generally available

{product-title} 4.13 introduced the ability to set a metrics collection profile for default platform monitoring to collect either the default amount of metrics data or a minimal amount of metrics data. In {product-title} {product-version}, metrics collection profiles are now generally available.

For more information, see xref:../observability/monitoring/about-ocp-monitoring/key-concepts.adoc#configuring-metrics-collection-profiles_key-concepts[About metrics collection profiles] and xref:../observability/monitoring/configuring-core-platform-monitoring/configuring-performance-and-scalability.adoc#choosing-a-metrics-collection-profile_configuring-performance-and-scalability[Choosing a metrics collection profile].

[id="ocp-4-19-monitoring-added-cluster-proxy-support-for-external-alertmanager-instances"]
==== Added cluster proxy support for external Alertmanager instances

With this release, external Alertmanager instances now use the cluster-wide HTTP proxy settings for communication. The {cmo-first} reads the cluster-wide proxy settings and configures the appropriate proxy URL for the Alertmanager endpoints.

[id="ocp-4-19-monitoring-strict-validation-for-cmo-is-improved"]
==== Strict validation for the {cmo-full} is improved

With this release, the strict validation introduced in {product-title} 4.18 is improved. Error messages now clearly identify the affected field, and validation is case-sensitive to ensure more accurate and consistent configuration.

For more information, see (link:https://issues.redhat.com/browse/OCPBUGS-42671[OCPBUGS-42671]) and (link:https://issues.redhat.com/browse/OCPBUGS-54516[OCPBUGS-54516]).

[id="ocp-release-notes-network-observability-operator_{context}"]
=== Network Observability Operator

[id="ocp-release-notes-networking_{context}"]
=== Networking

[id="ocp-4-19-networking-support-load-secrets_{context}"]
==== Creating a route with externally managed certificate (General Availability)

With this release, {product-title} routes can be configured with third-party certificate management solutions, utilising the `.spec.tls.externalCertificate` field in the route API. This allows you to reference externally managed TLS certificates through secrets, streamlining the process by eliminating manual certificate management. By using externally managed certificates, you reduce errors, ensure a smoother certificate update process, and enable the OpenShift router to promptly serve renewed certificates. For more information, see xref:../networking/routes/secured-routes.adoc#nw-ingress-route-secret-load-external-cert_secured-routes[Creating a route with externally managed certificate].

[id="ocp-4-19-networking-gateway-api-controller_{context}"]
==== Support for using Gateway API to configure cluster ingress traffic (General Availability)
With this release, support for managing ingress cluster traffic using Gateway API resources is Generally Available. Gateway API provides a robust networking solution within the transport layer, L4, and the application layer, L7, for {product-title} clusters using a standardized open source ecosystem.

For more information, see xref:../networking/configuring_ingress_cluster_traffic/ingress-gateway-api.adoc#ingress-gateway-api[Gateway API with {product-title} networking].

[IMPORTANT]
====
Gateway API resources must conform to the supported {product-title} API surface. This means you cannot use another vendor-specific resource, such as Istio's VirtualService, with {product-title}'s implementation of Gateway API. For more information, see xref:../networking/configuring_ingress_cluster_traffic/ingress-gateway-api.adoc#nw-ingress-gateway-api-implementation[Gateway API implementation for {product-title}].
====

[id="ocp-4-19-networking-gateway-api-crd-lifecycle_{context}"]
==== Support for managing Gateway API custom resource definition (CRD) lifecycle
With this release, {product-title} manages the lifecycle of Gateway API CRDs. This means that the Ingress Operator handles the required versioning and management of resources. Any Gateway API resources created in a previous {product-title} version must be re-created and redeployed so that it conforms to the specifications required by the Ingress Operator.

For more information, see xref:../updating/preparing_for_updates/updating-cluster-prepare.adoc#nw-ingress-gateway-api-manage-succession[Preparing for Gateway API management succession by the Ingress Operator].

[id="ocp-4-19-networking-gateway-api-ossm-version-bump_{context}"]
==== Updates to Gateway API custom resource definitions (CRDs)
{product-title} {product-version} updates {SMProductName} to version 3.0.2, and Gateway API to version 1.2.1. See the link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_mesh/3.0/html/release_notes/ossm-release-notes[{SMProductShortName} 3.0.0 release notes] and the link:https://github.com/kubernetes-sigs/gateway-api/blob/main/CHANGELOG/1.2-CHANGELOG.md#v121[Gateway API 1.2.1 changelog] for more information.

[id="ocp-4-19-networking-balance-slb-mode_{context}"]
==== Enable OVS balance-slb mode for your cluster (General Availability)

You can enable the Open vSwitch (OVS) `balance-slb` mode on infrastructure where your cluster runs so that two or more physical interfaces can share their network traffic. For more information, see xref:../installing/installing_bare_metal/ipi/ipi-install-installation-workflow.adoc#enabling-OVS-balance-slb-mode_ipi-install-installation-workflow[Enabling OVS balance-slb mode for your cluster].

[id="ocp-4-19-networking-ptp-dual-oc_{context}"]
==== Dual-port NICs for improved redundancy in PTP ordinary clocks (Technology Preview)
With this release, you can use a dual-port network interface controller (NIC) to improve redundancy for Precision Time Protocol (PTP) ordinary clocks.
Available as a Technology Preview, in a dual-port NIC configuration for an ordinary clock, if one port fails, the standby port takes over, maintaining PTP timing synchronization.

[NOTE]
====
You can configure PTP ordinary clocks with added redundancy on `x86` architecture nodes with dual-port NICs only.
====

For more information, see xref:../networking/ptp/about-ptp.adoc#ptp-dual-ports-oc_about-ptp[Using dual-port NICs to improve redundancy for PTP ordinary clocks].

[id="ocp-4-19-dpu-device-management-with-dpu-operator_{context}"]
==== Enabling DPU device management with the DPU Operator
With this release, {product-title} introduces the Data Processing Unit (DPU) Operator, enabling management of DPU devices. The DPU Operator manages components on compute nodes with DPUs, enabling the offloading of data workloads such as networking, storage, and security. This leads to improved cluster performance, reduced latency, and enhanced security, contributing to a more efficient infrastructure. For more information, see xref:../networking/networking_operators/dpu-operator/about-dpu.adoc#about-dpu-and-dpu-operator[About DPU and the DPU Operator].

[id="ocp-4-19-cluster-user-defined-networks-localnet_{context}"]
==== Localnet topology for user-defined networks (Generally Available)

Administrators can now use the `ClusterUserDefinedNetwork` custom resource to deploy secondary networks on a `Localnet` topology. This feature allows pods and virtual machines connected to the localnet network to egress to the physical network. For more information, see xref:../networking/multiple_networks/primary_networks/about-user-defined-networks.adoc#nw-cudn-localnet[Creating a ClusterUserDefinedNetwork CR for a Localnet topology].

[id="ocp-4-19-port-isolation-linux-bridge_{context}"]
==== Enable port isolation for a Linux bridge NAD (Generally Available)

You can enable port isolation for a Linux bridge network attachment definition (NAD) so that virtual machines (VMs) or pods that run on the same virtual LAN (VLAN) can operate in isolation from one another. For more information, see xref:../virt/vm_networking/virt-connecting-vm-to-linux-bridge.adoc#virt-linux-bridge-nad-port-isolation_virt-connecting-vm-to-linux-bridge[Enabling port isolation for a Linux bridge NAD].

[id="ocp-4-19-whereabouts-ipam_{context}"]
==== Fast IPAM configuration for the Whereabouts IPAM CNI plugin (Technology Preview)

To improve the performance of Whereabouts, especially if nodes in your cluster run a high amount of pods, you can now enable the Fast IP Address Management (IPAM) feature. The Fast IPAM feature uses `nodeslicepools`, which are managed by the Whereabouts Controller, to optimize IP address allocation for nodes. For more information, see xref:../networking/multiple_networks/secondary_networks/configuring-ip-secondary-nwt.adoc#nw-multus-whereabouts-fast-ipam_configuring-additional-network[Fast IPAM configuration for the Whereabouts IPAM CNI plugin].
[id="ocp-4-19-metallb-unnumbered-bgp-peering_{context}"]

[id="ocp-4-19-unnumbered-bgp-peering_{context}"]
==== Unnumbered BGP peering (Technology Preview)

With this release, {product-title} introduces unnumbered BGP peering.
Available as a Technology Preview feature, you can use the `spec.interface` field of the BGP peer custom resource to configure unnumbered BGP peering.

[id="ocp-4-19-custom-dns-host-name-disconnected_{context}"]
==== Create a custom DNS host name to resolve DNS connectivity issues

In a disconnected environment where the external DNS server cannot be reached, you can resolve Kubernetes NMState Operator health probe issues by specifying a custom DNS host name in the `NMState` custom resource definition (CRD). For more information, see xref:../networking/k8s_nmstate/k8s-nmstate-troubleshooting-node-network.adoc#k8s-nmstate-troubleshooting-dns-disconnected-env-resolv_k8s-nmstate-troubleshooting-node-network[Creating a custom DNS host name to resolve DNS connectivity issues].

[id="ocp-4-19-ptp-fast-events-rest-api-v2_{context}"]
==== Removal of PTP events REST API v1 and events consumer application sidecar
With this release, the PTP events REST API v1 and events consumer application sidecar support are removed.

You must use the O-RAN compliant PTP events REST API v2 instead.

For more information, see xref:../networking/ptp/ptp-cloud-events-consumer-dev-reference-v2.adoc#ptp-cloud-events-consumer-dev-reference-v2[Developing PTP event consumer applications with the REST API v2].

[id="ocp-4-19-sr-iov-arm_{context}"]
==== Deploying the SR-IOV Network Operator on a cluster that runs on ARM architecture

You can now deploy the SR-IOV Network Operator on a cluster that runs on ARM architecture. (link:https://issues.redhat.com/browse/OCPBUGS-56271[OCPBUGS-56271]) 

[id="ocp-4-19-route-external-cert-feature-gate_{context}"]
==== Re-add a previously deleted secret with `RouteExternalCertificate` feature gate enabled

If you enabled the `RouteExternalCertificate` feature gate for your cluster, you can now re-add a previously deleted secret. (link:https://issues.redhat.com/browse/OCPBUGS-33958[OCPBUGS-33958])

[id="ocp-release-notes-nodes_{context}"]
=== Nodes

[id="ocp-release-notes-machine-config-operator-cgroup-v1_{context}"]
==== cgroup v1 has been removed

cgroup v1, which was deprecated in {product-title} 4.16, is no longer supported and has been removed from {product-title}. If your cluster is using cgroup v1, you must configure cgroup v2 before you can upgrade to  {product-title} {product-version}. All workloads must now be compatible with cgroup v2.

For more information on cgroup v2, see xref:../architecture/index.adoc#architecture-about-cgroup-v2_architecture-overview[About Linux cgroup version 2] and link:https://www.redhat.com/en/blog/rhel-9-changes-context-red-hat-openshift-workloads[Red Hat Enterprise Linux 9 changes in the context of Red Hat OpenShift workloads] (Red{nbsp}Hat blog).

[id="ocp-release-notes-openshift-cli_{context}"]
=== OpenShift CLI (oc)

[id="ocp-release-notes-osdk_{context}"]
=== Operator development

[id="ocp-release-notes-osdk-base-images_{context}"]
==== Supported Operator base images

The following base images for Operator projects are updated for compatibility with {product-title} {product-version}. The runtime functionality and configuration APIs for these base images are supported for bug fixes and for addressing CVEs.

* The base image for Ansible-based Operator projects
* The base image for Helm-based Operator projects

For more information, see link:https://access.redhat.com/node/7123119[Updating the base image for existing Ansible- or Helm-based Operator projects for {product-title} 4.19 and later] (Red{nbsp}Hat Knowledgebase).

// NOTE: The KCS article link will be published on the GA date. It is a draft right now.

[id="ocp-release-notes-oci_{context}"]
=== {oci-first}

[id="ocp-release-notes-postinstallation-configuration_{context}"]
=== Postinstallation configuration

[id="ocp-release-notes-rhcos_{context}"]
=== {op-system-first}

[id="ocp-4-19-rhcos-rhel-9-6_{context}"]
==== {op-system} uses {op-system-base} 9.6
{op-system} uses {op-system-base-full} 9.6 packages in {product-title} {product-version}. These packages ensure that your {product-title} instances receive the latest fixes, features, enhancements, hardware support, and driver updates.

[id="ocp-release-notes-registry_{context}"]
=== Registry

[id="ocp-release-notes-scalability-and-performance_{context}"]
=== Scalability and performance

[id="ocp-release-notes-scalability-and-performance_kernelpagesize_{context}"]
==== Performance profile kernel page size configuration

With this update, you can specify larger kernel page sizes to improve performance for memory-intensive, high-performance workloads on ARM infrastructure nodes with the realtime kernel disabled. For more information, see xref:../scalability_and_performance/cnf-tuning-low-latency-nodes-with-perf-profile.adoc#cnf-configuring-kernal-page-size_cnf-low-latency-perf-profile[Configuring kernel page sizes].

[id="ocp-release-notes-tuning-hcp-performance-profile_{context}"]
==== Tuning {hcp} using a performance profile

With this update, you can now tune nodes in {hcp} for low latency by applying a performance profile. For more information, see xref:../scalability_and_performance/cnf-tuning-low-latency-hosted-cp-nodes-with-perf-profile.adoc#cnf-create-performance-profiles-hosted-cp[Creating a performance profile for hosted control planes].

[id="ocp-release-notes-security_{context}"]
=== Security

[id="ocp-release-notes-tls-modern-profile-control-plane_{context}"]
==== Control plane now supports TLS 1.3 and the Modern TLS security profile

With this release, the control plane supports TLS 1.3. You can now use the `Modern` TLS security profile for the control plane.

For more information, see xref:../security/tls-security-profiles.adoc#tls-profiles-kubernetes-configuring_tls-security-profiles[Configuring the TLS security profile for the control plane].

[id="ocp-release-notes-storage_{context}"]
=== Storage

[id="ocp-release-notes-sscsi-disconnected-environment-support_{context}"]
==== Support for the Secrets Store CSI driver in disconnected environments
With this release, the secrets store providers support using the {secrets-store-driver} in disconnected clusters.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-secrets-store.adoc#persistent-storage-csi-secrets-store-disconnect-environment_persistent-storage-csi-secrets-store[Support for disconnected environments].

[id="ocp-release-notes-storage-azure-file-cross-sub-support_{context}"]
==== Azure File cross-subscription support is generally available
Cross-subscription support allows you to have an {product-title} cluster in one Azure subscription and mount your Azure file share in another Azure subscription using the Azure File Container Storage Interface (CSI) driver. The subscriptions must be in the same tenant.

This feature is generally available in {product-title} 4.19.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-aws-efs.adoc#persistent-storage-csi-efs-cross-account_persistent-storage-csi-aws-efs[AWS EFS CSI cross account support].

[id="ocp-release-notes-storage-vol-attributes_{context}"]
==== Volume Attributes Classes (Technology Preview)
Volume Attributes Classes provide a way for administrators to describe "classes" of storage they offer. Different classes might correspond to different quality-of-service levels.

Volume Attributes Classes in {product-title} 4.19 is available only with AWS Elastic Block Storage (EBS) and Google Cloud Platform (GCP) persistent disk (PD) Container Storage Interface (CSI).

You can apply a Volume Attributes Classes to a persistent volume claim (PVC). If a new Volume Attributes Class becomes available in the cluster, you can update the PVC with the new Volume Attributes Classes if needed.

Volume Attributes Classes have parameters that describe volumes belonging to them. If a parameter is omitted, the default is used at volume provisioning. If a user applies the PVC with a different Volume Attributes Class with omitted parameters, the default value of the parameters might be used depending on the CSI driver implementation. For more information, see the related CSI driver documentation.

Volume Attributes Classes is available in {product-title} 4.19 with Technology Preview status.

For more information, see xref:..//storage/understanding-persistent-storage.adoc#storage-persistent-storage-pvc-volumeattributesclass_understanding-persistent-storage[Volume Attributes Classes].

[id="ocp-release-notes-storage-cli-cmd-pvc-usage_{context}"]
==== New CLI command to show PVC usage (Technology Preview)
{product-title} 4.19 introduces a new command to view persistent volume claim usage. This feature has Technology Preview status.

For more information, see xref:../storage/understanding-persistent-storage.adoc#pvc-cli-command-usage_understanding-persistent-storage[Viewing PVC usage statistics].

[id="ocp-release-notes-storage-cli-cmd-resize-recovery_{context}"]
==== CSI volume resizing recovery is generally available

Previously, you might expand a persistent volume claim (PVC) to a size that is not supported by the underlying storage provider. In this case, the expansion controller typically tries forever to expand the volume and keeps failing.

This new feature allows you to recover and provide another resize value for the PVC. Resizing recovery is supported as generally available in {product-title} 4.19.

For more information about resizing volumes, see xref:../storage/expanding-persistent-volumes.adoc[Expanding persistent volumes].

For more information about recovering when resizing volumes, see xref:../storage/expanding-persistent-volumes.adoc#expanding-recovering-from-failure_expanding-persistent-volumes[Recovering from failure when expanding volumes].

[id="ocp-release-notes-storage-resize-migrated-vsphere-in-tree-vols_{context}"]
==== Support for resizing vSphere in-tree migrated volumes is generally available
Previously, VMware vSphere persistent volumes that were migrated from in-tree to Container Storage Interface (CSI) could not be resized. With {product-title} 4.19, resizing migrated volumes is supported. This feature is generally available.

For more information about resizing volumes, see xref:../storage/expanding-persistent-volumes.adoc[Expanding persistent volumes].

[id="ocp-release-notes-storage-disable-vsphere_{context}"]
==== Disabling and enabling storage on vSphere is generally available
Cluster administrators might want to disable the VMWare vSphere Container Storage Interface (CSI) Driver as a Day 2 operation, so the vSphere CSI Driver does not interface with your vSphere setup.

This features was introduced in {product-title} 4.17 with Technology Preview status. This feature is now supported as generally available in {product-title} 4.19.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere-disable-storage-overview_persistent-storage-csi-vsphere[Disabling and enabling storage on vSphere].

[id="ocp-release-notes-storage-increase-max-vols-per-node-vsphere"]
==== Increasing the maximum number of volumes per node for vSphere (Technology Preview)
For VMware vSphere version 7, {product-title} restricts the maximum number of volumes per node to 59.

However, with {product-title} 4.19 for vSphere version 8 or later, you can increase the allowable number of volumes per node to a maximum of 255. Otherwise, the default value remains at 59.

This feature has Technology Preview status.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere-increase-max-vols-per-node-overview_persistent-storage-csi-vsphere[Increasing maximum volumes per node for vSphere].

[id="ocp-release-notes-storage-vsphere-migrating-cns-vols-between-datastores_{context}"]
==== Migrating CNS volumes between datastores for vSphere is fully supported
If you are running out of space in your current datastore, or want to move to a more performant datastore, you can migrate VMware vSphere Cloud Native Storage (CNS) volumes between datastores. This applies to both attached and detached volumes.

{product-title} now fully supports migration of CNS volume using the vCenter UI. Migrated volumes should work
as expected and should not result in non-functional persistent volumes. CNS volumes can also be migrated while in use by pods.

This feature was introduced as a Development Preview in {product-title} 4.17, but is now fully supported  in 4.19.

Migrating CNS volumes between datastores requires VMware vSphere 8.0.2 or later or vSphere 7.0 Update 3o or later.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere-migrating-cns-vols-between-datastores_persistent-storage-csi-vsphere[Migrating CNS volumes between datastores for vSphere].

[id="ocp-release-notes-storage-nfs-export-options-filestore"]
==== NFS export options for Filestore storage class is generally available.
By default, a Filestore instance grants root level read/write access to all clients that share the same Google Cloud project and virtual private cloud (VPC) network. Network File System (NFS) export options can limit this access to certain IP ranges and specific user/group IDs for the Filestore instance. When creating a storage class, you can set these options using the `nfs-export-options-on-create` parameter.

NFS export options is supported as generally available in {product-title} 4.19.

For more information, see xref:..//storage/container_storage_interface/persistent-storage-csi-google-cloud-file.adoc#persistent-storage-csi-gcp-filestore-nfs-export-options_persistent-storage-csi-google-cloud-file[NFS export options].

[id="ocp-release-notes-web-console_{context}"]
=== Web console

Starting with {product-title} 4.19, the perspectives in the web console have unified to simplify navigation, reduce context switching, streamline tasks, and provide users with a more cohesive {product-title} experience.

With this unified design, there is no longer a *Developer* perspective in the default view; however, _all_ {product-title} web console features are discoverable to all users. If you are not the cluster owner, you might need to request permission for certain features from the cluster owner. The *Developer* perspective can still be manually enabled if you prefer.

The *Getting Started* pane in the web console provides resources such as, a tour of the console, information on setting up your cluster, a quick start for enabling the *Developer* perspective, and links to explore new features and capabilities.

//See xref:../web_console/web-console-overview#enabling-developer-perspective_web-console_web-console-overview for more information on enabling the *Developer* perspective.
//https://github.com/openshift/openshift-docs/pull/93644

[id="ocp-release-notes-patternfly-6-upgrade_{context}"]
==== Patternfly 6 upgrade

The web console now uses Patternfly 6. Support for Patternfly 4 in the web console is no longer available.

[discrete]
This release also introduces the following updates to the web console. You can now do the following actions:

* Specify distinct console logos for both light and dark themes using the `logos` field in the `.spec.customization.logos` configuration, allowing for more comprehensive branding.
* Easily delete identity providers (IDPs) directly from the web console, streamlining authentication configuration without manual YAML file edits.
* Easily set the default `StorageClass` directly in the web console.
* Quickly find specific jobs in the web console by sorting the *Created* column by creation date and time.

[id="ocp-4-19-notable-technical-changes_{context}"]
== Notable technical changes

[discrete]
[id="ocp-4-19-notable-technical-changes-readonlyrootfilesystem_{context}"]
=== Pods deploy with readOnlyRootFilesystem set to true

With this release, Cloud Credential Operator pods now deploy with the `readOnlyRootFilesystem` security context setting set to `true`. This enhances security by ensuring that the container root file system is mounted as read-only.

[discrete]
[id="ocp-4-19-notable-technical-changes-loopback-cert_{context}"]
=== Extended loopback certificate validity to three years for kube-apiserver

Previously, in-memory loopback certificates in kube-apiserver were issued for one year. This meant that if the kube-apiserver ran without rollouts or restarts, the kube-apiserver would stop functioning after one year. In {product-title} {product-version} the in-memory loopback certificates are extended so the kube-apiserver can run for three years without interruption.

[id="ocp-release-notes-readiness-probes-etcd_{context}"]
=== Readiness probes exclude etcd checks

The readiness probes for the API server have been modified to exclude etcd checks. This prevents client connections from being closed if etcd is temporarily unavailable. This means that client connections persist through brief etcd unavailability and minimizes temporary API server outages.

[id="ocp-4-19-deprecated-removed-features_{context}"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* _Not Available_
* _Technology Preview_
* _General Availability_
* _Deprecated_
* _Removed_

[discrete]
[id="ocp-release-note-bare-metal-dep-rem_{context}"]
=== Bare metal monitoring deprecated and removed features

.Bare Metal Event Relay Operator tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Bare Metal Event Relay Operator
|Removed
|Removed
|Removed
|====

[discrete]
[id="ocp-release-note-images-dep-rem_{context}"]
=== Images deprecated and removed features

.Images deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
[id="ocp-release-note-install-dep-rem_{context}"]
=== Installation deprecated and removed features

.Installation deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`--cloud` parameter for `oc adm release extract`
|Deprecated
|Deprecated
|Deprecated

|CoreDNS wildcard queries for the `cluster.local` domain
|Deprecated
|Deprecated
|Deprecated

|`compute.platform.openstack.rootVolume.type` for {rh-openstack}
|Deprecated
|Deprecated
|Deprecated

|`controlPlane.platform.openstack.rootVolume.type` for {rh-openstack}
|Deprecated
|Deprecated
|Deprecated

|`ingressVIP` and `apiVIP` settings in the `install-config.yaml` file for installer-provisioned infrastructure clusters
|Deprecated
|Deprecated
|Deprecated

|Package-based {op-system-base} compute machines
|Deprecated
|Deprecated
|Removed

|`platform.aws.preserveBootstrapIgnition` parameter for {aws-first}
|Deprecated
|Deprecated
|Deprecated

|Installing a cluster on {aws-short} with compute nodes in {aws-short} Outposts
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
=== Machine management deprecated and removed features

.Machine management deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Placeholder
|Status
|Status
|Status

|====

// No deprecated or removed features for 3 consecutive releases
// [discrete]
// [id="ocp-release-note-monitoring-dep-rem_{context}"]
// === Monitoring deprecated and removed features

// .Monitoring deprecated and removed tracker
// [cols="4,1,1,1",options="header"]
// |====
// |Feature |4.17 |4.18 |4.19
// |====

[discrete]
[id="ocp-release-note-networking-dep-rem_{context}"]
=== Networking deprecated and removed features

.Networking deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|iptables
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
[id="ocp-release-note-node-dep-rem_{context}"]
=== Node deprecated and removed features

.Node deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`ImageContentSourcePolicy` (ICSP) objects
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/zone`
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/region`
|Deprecated
|Deprecated
|Deprecated

|cgroup v1
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
[id="ocp-release-note-cli-dep-rem_{context}"]
=== OpenShift CLI (oc) deprecated and removed features

.OpenShift CLI (oc) deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|oc-mirror plugin v1
|General Availability
|Deprecated
|Deprecated
|====

[discrete]
[id="ocp-release-note-operators-dep-rem_{context}"]
=== Operator lifecycle and development deprecated and removed features

// "Operator lifecycle" refers to OLMv0 and "development" refers to Operator SDK

.Operator lifecycle and development deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Operator SDK
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Ansible-based Operator projects
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Helm-based Operator projects
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Go-based Operator projects
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Hybrid Helm-based Operator projects
|Deprecated
|Removed
|Removed

|Scaffolding tools for Java-based Operator projects
|Deprecated
|Removed
|Removed

// Do not remove the SQLite database... entry until otherwise directed by the Operator Framework PM
|SQLite database format for Operator catalogs
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
[id="ocp-hardware-an-driver-dep-rem_{context}"]
=== Specialized hardware and driver enablement deprecated and removed features

.Specialized hardware and driver enablement deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19
|====

[discrete]
=== Storage deprecated and removed features

.Storage deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Persistent storage using FlexVolume
|Deprecated
|Deprecated
|Deprecated

|AliCloud Disk CSI Driver Operator
|Removed
|Removed
|Removed

|Shared Resources CSI Driver Operator
|Deprecated
|Removed
|Removed
|====

[discrete]
[id="ocp-clusters-dep-rem_{context}"]
=== Updating clusters deprecated and removed features

.Updating clusters deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19
|====

[discrete]
[id="ocp-release-note-web-console-dep-rem_{context}"]
=== Web console deprecated and removed features

.Web console deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`useModal` hook for dynamic plugin SDK
|General Availability
|General Availability
|Deprecated

|Patternfly 4
|Deprecated
|Deprecated
|Removed

|====

[discrete]
[id="ocp-release-note-workloads-dep-rem_{context}"]
=== Workloads deprecated and removed features

.Workloads deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`DeploymentConfig` objects
|Deprecated
|Deprecated
|Deprecated
|====

[id="ocp-4-19-deprecated-features_{context}"]
=== Deprecated features

[id="ocp-4-19-oc-adm-pod-network-removed_{context}"]
==== `oc adm pod-network` command deprecated

The `oc adm pod-network` command for working with OpenShift SDN multitenant mode has been removed from the `oc adm --help` output. If the `oc adm pod-network` command is used, an error message is displayed to tell users that it has been deprecated.

[id="ocp-4-19-useModal-dynamic-plugin-removed_{context}"]
==== useModal hook for dynamic plugin SDK
With this release, support for the `useModal` hook in dynamic plugins are deprecated.

Starting with this release, use the `useOverlay` API hook to launch modals

//For more information about `useOverlay`, see [Doc link to dynamic plugin api docs TBD.]

[id="ocp-4-19-removed-features_{context}"]
=== Removed features

[id="ocp-4-19-rhel-worker-nodes-removed_{context}"]
==== Package-based {op-system-base} compute machines
With this release, support for the installation of packaged-based {op-system-base} worker nodes is removed.

{op-system} image layering replaces this feature and supports installing additional packages on the base operating system of your worker nodes.

For information on how to identify and remove {op-system-base} nodes in your cluster, see xref:../updating/preparing_for_updates/updating-cluster-prepare-past-4-18.adoc#updating-cluster-prepare-past-4-18[Preparing to update from {product-title} 4.18 to a newer version]. For more information on image layering, see xref:../machine_configuration/mco-coreos-layering.adoc#mco-coreos-layering[{op-system} image layering].

[id="ocp-4-19-removed-kube-1-32-apis_{context}"]
==== APIs removed from Kubernetes 1.32

Kubernetes 1.32 removed the following deprecated APIs, so you must migrate manifests and API clients to use the appropriate API version. For more information about migrating removed APIs, see the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-32[Kubernetes documentation].

.APIs removed from Kubernetes 1.32
[cols="2,2,2,1",options="header",]
|===
|Resource |Removed API |Migrate to |Notable changes

|`FlowSchema`
|`flowcontrol.apiserver.k8s.io/v1beta3`
|`flowcontrol.apiserver.k8s.io/v1`
|No

|`PriorityLevelConfiguration`
|`flowcontrol.apiserver.k8s.io/v1beta3`
|`flowcontrol.apiserver.k8s.io/v1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#flowcontrol-resources-v132[Yes]
|===

[id="ocp-4-19-removed-osdk_{context}"]
==== Operator SDK CLI and related scaffolding and testing tools

With this release, the Red{nbsp}Hat-supported version of the Operator SDK CLI tool, including the related scaffolding and testing tools for Operator projects, is no longer released with {product-title}.

Red{nbsp}Hat will provide bug fixes and support for versions of the Operator SDK that were released with earlier versions of {product-title} according to the link:https://access.redhat.com/product-life-cycles?product=OpenShift%20Container%20Platform%204[Product Life Cycles for {product-title} 4] (Red{nbsp}Hat Customer Portal).

Operator authors with existing Operator projects can use link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/operators/developing-operators#osdk-about[the version of the Operator SDK CLI tool released with {product-title} 4.18] to maintain their projects and create Operator releases that target newer versions of {product-title}. For more information, see link:https://access.redhat.com/node/7123119[Updating the base image for existing Ansible- or Helm-based Operator projects for {product-title} 4.19 and later] (Red{nbsp}Hat Knowledgebase).

For more information about the unsupported, community-maintained, version of the Operator SDK, see link:https://sdk.operatorframework.io[Operator SDK (Operator Framework)].

[id="ocp-4-19-future-deprecation_{context}"]
=== Notice of future deprecation

[id="ocp-4-19-bug-fixes_{context}"]
== Bug fixes
//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / TALO
//Telco Edge / ZTP

[discrete]
[id="ocp-release-note-api-auth-bug-fixes_{context}"]
==== API Server and Authentication

[discrete]
[id="ocp-release-note-bare-metal-hardware-bug-fixes_{context}"]
==== Bare Metal Hardware Provisioning

* Previously, NetworkManager logs from the Ironic Python Agent (IPA) were not included in the ramdisk logs; instead only `dmesg` logs were included in the ramdisk logs. With this release, the ramdisk logs that exist in the `metal3-ramdisk-logs` container of a metal3 pod now contain the entire journal from the host instead of just `dmesg` logs and IPA. (link:https://issues.redhat.com/browse/OCPBUGS-56042[OCPBUGS-56042])

* Previously, ramdisk logs did not include clear file separators, causing the content from one file to merge into random lines of another file. Because of this issue, distinguishing what content belonged to which file was difficult. With this release, file entries now include file separators so that each file is clearly indicated from the contents of the other file being merged into a ramdisk log file. (link:https://issues.redhat.com/browse/OCPBUGS-55743[OCPBUGS-55743])

* Previously, if you forgot to include a Redfish system ID, such as `redfish://host/redfish/v1/` instead of `redfish://host/redfish/v1/Self`, in a Baseboard Management Console (BMC) URL, a JSON parsing issue existed in Ironic. With this release, BMO can now handle URLs without a Redfish system ID as a valid address without causing a JSON parsing issue. (link:https://issues.redhat.com/browse/OCPBUGS-56026[OCPBUGS-56026])

* Previously, a race condition existed during provisioning which, in case of a slow DHCP response, could cause different hostnames to be used for machine and node objects. This could prevent CSRs of worker nodes from being automatically approved. With this release, the race condition was fixed and CSRs of worker nodes are now properly approved.  (link:https://issues.redhat.com/browse/OCPBUGS-55315[OCPBUGS-55315]) 

* Previously, certain models of SuperMicro machines, such as `ars-111gl-nhr`, use a different virtual media device string than other SuperMicro machines, which could cause virtual media boot attempts to fail on these servers. With this release, an extra conditional check was added to check for the specific model affected and adjust behavior accordingly, so that SuperMicro models such as ars-111gl-nhr can now boot from virtual media. (link:https://issues.redhat.com/browse/OCPBUGS-56639[OCPBUGS-56639]) 

* Previously, after deleting a `BaremetalHost` that has a related `DataImage`, the `DataImage` was still present. With this release, the `DataImage` is deleted if it exists after the `BaremetalHost` has been deleted. (link:https://issues.redhat.com/browse/OCPBUGS-51294[OCPBUGS-51294]) 

[discrete]
[id="ocp-release-note-builds-bug-fixes_{context}"]
==== Builds

[discrete]
[id="ocp-release-note-cloud-compute-bug-fixes_{context}"]
==== Cloud Compute

* When upgrading {gcp-short} clusters that use a boot disk that is not compatible with UEFI, you cannot enable Shielded VM support.
Previously, this prevented the creation of new compute machines.
With this release, disks with known UEFI incompatiblity have Shielded VM support disabled.
This primarily affects customers upgrading from {product-title} version 4.12 to 4.13 using the {gcp-short} marketplace images.
(link:https://issues.redhat.com/browse/OCPBUGS-17079[OCPBUGS-17079])

* Previously, VMs in a cluster that ran on {azure-short} failed because the attached network interface controller (NIC) was in a `ProvisioningFailed` state.
With this release, the Machine API controller checks the provisioning status of a NIC and refreshes the VMs on a regular basis to prevent this issue.
(link:https://issues.redhat.com/browse/OCPBUGS-31515[OCPBUGS-31515])

* Previously, in larger clusters that had other subsystems using certificate signing requests (CSRs), the CSR approver counted unrelated, unapproved CSRs towards its total and prevented further approvals.
With this release, the CSR approver uses a `signerName` property as a filter and only includes CSRs that it can approve.
As a result, the CSR approver only prevents new approvals when there are a large number of unapproved CSRs for the relevant `signerName` values.
(link:https://issues.redhat.com/browse/OCPBUGS-36404[OCPBUGS-36404])

* Previously, the Machine API controller read only the zone number to populate machine zone information.
For machines in {azure-short} regions that only support availability sets, the set number represents the zone, so the Machine API controller did not populate their zone information.
With this release, the Machine API controller references the {azure-short} fault domain property.
This property works for availability sets and availability zones, so the controller correctly reads the fault domain in each case and machines always report a zone.
(link:https://issues.redhat.com/browse/OCPBUGS-38570[OCPBUGS-38570])

* Previously, increased granularity in {gcp-short} zone API error messages caused the machine controller to mistakenly mark some machines with invalid configurations as valid with a temporary cloud error.
This behavior prevented invalid machines from transitioning to a failed state.
With this release, the machine controller handles the more granular error messages correctly so that machines with an invalid zone or project ID correctly move to a failed state.
(link:https://issues.redhat.com/browse/OCPBUGS-43531[OCPBUGS-43531])

* Previously, some permissions required for linked actions were missing.
Linked actions create the subresources necessary for other {azure-short} resources that the cloud controller manager and {product-title} require.
With this release, the cloud controller manager for {azure-short} has the following permissions for linked actions:
+
--
** `Microsoft.Network/applicationGateways/backendAddressPools/join/action`
** `Microsoft.Network/applicationSecurityGroups/joinIpConfiguration/action`
** `Microsoft.Network/applicationSecurityGroups/joinNetworkSecurityRule/action`
** `Microsoft.Network/ddosProtectionPlans/join/action`
** `Microsoft.Network/gatewayLoadBalancerAliases/join/action`
** `Microsoft.Network/loadBalancers/backendAddressPools/join/action`
** `Microsoft.Network/loadBalancers/frontendIPConfigurations/join/action`
** `Microsoft.Network/loadBalancers/inboundNatRules/join/action`
** `Microsoft.Network/networkInterfaces/join/action`
** `Microsoft.Network/networkSecurityGroups/join/action`
** `Microsoft.Network/publicIPAddresses/join/action`
** `Microsoft.Network/publicIPPrefixes/join/action`
** `Microsoft.Network/virtualNetworks/subnets/join/action`
--
+
(link:https://issues.redhat.com/browse/OCPBUGS-44126[OCPBUGS-44126])

* Previously, some permissions required for linked actions were missing.
Linked actions create the subresources necessary for other {azure-short} resources that the Machine API and {product-title} require.
With this release, the Machine API provider for {azure-short} has the following permissions for linked actions:
+
--
** `Microsoft.Compute/disks/beginGetAccess/action`
** `Microsoft.KeyVault/vaults/deploy/action`
** `Microsoft.ManagedIdentity/userAssignedIdentities/assign/action`
** `Microsoft.Network/applicationGateways/backendAddressPools/join/action`
** `Microsoft.Network/applicationSecurityGroups/joinIpConfiguration/action`
** `Microsoft.Network/applicationSecurityGroups/joinNetworkSecurityRule/action`
** `Microsoft.Network/ddosProtectionPlans/join/action`
** `Microsoft.Network/gatewayLoadBalancerAliases/join/action`
** `Microsoft.Network/loadBalancers/backendAddressPools/join/action`
** `Microsoft.Network/loadBalancers/frontendIPConfigurations/join/action`
** `Microsoft.Network/loadBalancers/inboundNatPools/join/action`
** `Microsoft.Network/loadBalancers/inboundNatRules/join/action`
** `Microsoft.Network/networkInterfaces/join/action`
** `Microsoft.Network/networkSecurityGroups/join/action`
** `Microsoft.Network/publicIPAddresses/join/action`
** `Microsoft.Network/publicIPPrefixes/join/action`
** `Microsoft.Network/virtualNetworks/subnets/join/action`
--
+
(link:https://issues.redhat.com/browse/OCPBUGS-44130[OCPBUGS-44130])

* Previously, installing an {aws-short} cluster failed in certain environments on existing subnets when the `publicIp` parameter in the compute machine set CR was set to `false`.
With this release, a fix ensures that a configuration value set for `publicIp` no longer causes issues when the installation program provisions machines for your {aws-short} cluster in certain environments.
(link:https://issues.redhat.com/browse/OCPBUGS-44373[OCPBUGS-44373])

* Previously, {gcp-short} clusters that used non-UEFI disks failed to load.
This release adds a check to ensure that disks are UEFI-compatible before enabling features that require UEFI, such as secure boot.
This change adds `compute.images.get` and `compute.images.getFromFamily` permissions requirements.
As a result, you can use non-UEFI disks if you do nto need these features.
(link:https://issues.redhat.com/browse/OCPBUGS-44671[OCPBUGS-44671])

* Previously, when the {aws-short} `DHCPOptionSet` parameter was configured to use a custom domain name that contains a trailing period (`.`), {product-title} installation failed.
With this release, the logic that extracts the hostname of EC2 instances and turns them into kubelet node names trims trailing periods so that the resulting Kubernetes object name is valid.
Trailing periods in this parameter no longer cause installation to fail. (link:https://issues.redhat.com/browse/OCPBUGS-45306[OCPBUGS-45306])

* Previously, the number of {azure-short} availability set fault domains used a fixed value of `2`.
This setting works in most {azure-short} regions because fault domain counts are typically at least 2.
However, this setting failed in the `centraluseuap` and `eastusstg` regions.
With this release, the number of availability set fault domains in a region is set dynamically.
(link:https://issues.redhat.com/browse/OCPBUGS-45663[OCPBUGS-45663])

* Previously, the {azure-short} cloud controller manager panicked when there was a temporary API server disconnection.
With this release, the {azure-short} cloud controller manager correctly recovers from temporary disconnection.
(link:https://issues.redhat.com/browse/OCPBUGS-45859[OCPBUGS-45859])

* Previously, some services became stuck in a pending state due to incorrect or missing annotations.
With this release, validation added to the {azure-short} `service.beta.kubernetes.io/azure-load-balancer-tcp-idle-timeout` and {gcp-short} `cloud.google.com/network-tier` annotations resolves the issue.
(link:https://issues.redhat.com/browse/OCPBUGS-48481[OCPBUGS-48481])

* Previously, the method used to fetch the provider ID from {aws-short} could fail to provide this value to the kubelet when needed.
As a result, sometimes machines could get stuck in different states and fail to complete initialization.
With this release, the provider ID is consistently set when the kubelet starts up.
(link:https://issues.redhat.com/browse/OCPBUGS-50905[OCPBUGS-50905])

* Previously, an incorrect endpoint in the {azure-short} cloud controller manager caused installations on {azure-full} Government Cloud to fail.
The issue is resolved in this release.
(link:https://issues.redhat.com/browse/OCPBUGS-50969[OCPBUGS-50969])

* Previously, the Machine API sometimes detected an unhealthy control plane node during cluster creation on {ibm-cloud-title} and attempted to replace the node.
This effectively destroyed the cluster.
With this release, the Machine API only attempts to replace unhealthy compute nodes during cluster creation and does not attempt to replace unhealthy control plane nodes.
(link:https://issues.redhat.com/browse/OCPBUGS-51864[OCPBUGS-51864])

* Previously, {azure-short} spot machines that were evicted before their node became ready could get stuck in the `provisioned` state.
With this release, {azure-short} spot instances now use a delete-eviction policy.
This policy ensures that the machines correctly move to the `failed` state upon preemption.
(link:https://issues.redhat.com/browse/OCPBUGS-54617[OCPBUGS-54617])

* Previously, a bug fix altered the availability set configuration by changing the fault domain count to use the maximum available value instead of a fixed value of `2`.
This inadvertently caused scaling issues for compute machine sets created before the bug fix, as the controller attempted to change immutable availability sets.
With this release, availability sets are no longer modified after creation, allowing affected compute machine sets to scale properly.
(link:https://issues.redhat.com/browse/OCPBUGS-56653[OCPBUGS-56653])

[discrete]
[id="ocp-release-note-cloud-cred-operator-bug-fixes_{context}"]
==== Cloud Credential Operator

* Previously, the `aws-sdk-go-v2` software development kit (SDK) failed to authenticate an `AssumeRoleWithWebIdentity` API operation on an {aws-first} {sts-first} cluster. With this release, `pod-identity-webhook` now includes a default region so that this issue no longer persists. (link:https://issues.redhat.com/browse/OCPBUGS-41727[OCPBUGS-41727])

[discrete]
[id="ocp-release-note-cluster-autoscaler-bug-fixes_{context}"]
==== Cluster Autoscaler

* Previously, when a Machine Set was scaled down and had reached its minimum size, the Cluster Autoscaler could leave the last remaining node with a no schedule taint that prevented use of a node. This issues was caused by a counting error in the Cluster Autoscaler. With this release, the counting error has been fixed so that the Cluster Autoscaler works as expected when a Machine Set is scaled down and has reached its minimum size. (link:https://issues.redhat.com/browse/OCPBUGS-54231[OCPBUGS-54231])

* Previously, some cluster autoscaler metrics were not initialized, and therefore were not available. With this release, these metrics are initialized and available. (link:https://issues.redhat.com/browse/OCPBUGS-25852[OCPBUGS-25852])

* Previously the Cluster Autoscaler could stop scaling because of a failed machine in a machine set. This condition occurred because of inaccuracies in the way the Cluster Autoscaler counts machines in various non-running phases. With this release, the inaccuracies have been fixed, so that the Cluster Autoscaler accurately counts machines. (link:https://issues.redhat.com/browse/OCPBUGS-11115[OCPBUGS-11115])

[discrete]
[id="ocp-release-note-cluster-override-admin-operator-bug-fixes_{context}"]
==== Cluster Resource Override Admission Operator

* Previously, the Cluster Resource Admission Override Operator failed to delete old secrets during upgrading from {product-title} 4.16 to {product-title} 4.17. This situation caused the Cluster Resource Override Admission Operator webhook to stop working and prevented pods from being created in namespaces that had the Cluster Resource Override Admission Operator enabled. With this release, old secrets are deleted, error handling by the Cluster Resource Override Admission Operator is improved, and the issue with creating pods in namespaces is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-54886[OCPBUGS-54886])

* Previously, if you deleted the `clusterresourceoverride-operator` service or uninstalled the Cluster Resource Admission Override Operator, the `v1.admission.autoscaling.openshift.io` API service was unreachable and prevented needed cluster functionas, such as installing other Operators on the cluster. With this release, a fix ensures that if the Cluster Resource Admission Override Operator is uninstalled, the `v1.admission.autoscaling.openshift.io` API service is also deleted so that the cluster functions are not impacted. (link:https://issues.redhat.com/browse/OCPBUGS-48115[OCPBUGS-48115])

* Previously, if you specified a `forceSelinuxRelabel` parameter in a `ClusterResourceOverride` CR and then changed the parameter to another value, the changed value would not be reflected in the `clusterresourceoverride-configuration` Config Map. This Config Map is required for applying the selinux relabelling workaround feature to your cluster. With this release, this issue is fixed so that when the `forceSelinuxRelabel` parameter is changed, the `clusterresourceoverride-configuration` Config Map received the update. (link:https://issues.redhat.com/browse/OCPBUGS-44649[OCPBUGS-44649])

[discrete]
[id="ocp-release-note-cluster-version-operator-bug-fixes_{context}"]
==== Cluster Version Operator

* Previously, the status of the `ClusterVersion` condition could  changed from `ImplicitlyEnabled` to `ImplicitlyEnabledCapabilities`. With this release, the `ClusterVersion` condition type has been fixed and changed from `ImplicitlyEnabled` to `ImplicitlyEnabledCapabilities`. (link:https://issues.redhat.com/browse/OCPBUGS-56771[OCPBUGS-56771])

* Previously, a custom Security Context Constraint (SCC) impacted any pod that was generated by the Cluster Version Operator from receiving a cluster version upgrade. With this release, {product-title} now sets a default SCC to each pod, so that any custom SCC created does not impact a pod. (link:https://issues.redhat.com/browse/OCPBUGS-31462[OCPBUGS-31462])

* Previously, when a Cluster Operator takes a long time to upgrade, Cluster Version Operator does not report anything as it cannot determine if the upgrade is still progressing or already stuck. With this release, a new unknown status is added for the failing condition in status of the Cluster Version reported by Cluster Version Operator to remind the cluster administrators to check the cluster and avoid waiting on a blocked Cluster Operator upgrade. (link:https://issues.redhat.com/browse/OCPBUGS-23514[OCPBUGS-23514])

[discrete]
[id="ocp-release-note-dev-console-bug-fixes_{context}"]
==== Developer Console

[discrete]
[id="ocp-release-note-driver-toolkit-bug-fixes_{context}"]
==== Driver ToolKit (DTK)



[discrete]
[id="ocp-release-note-cloud-etcd-operator-bug-fixes_{context}"]
==== etcd Cluster Operator

[discrete]
[id="ocp-release-note-image-streams-bug-fixes_{context}"]
==== ImageStreams

* Previously, image import blocked registries that would fail if those registries were configured with `NeverContactSource`, even when mirror registries were set up. With this update, image importing is no longer blocked when a registry has mirrors configured. This ensures that image imports succeed even if the original source was set to `NeverContactSource` in the `ImageDigestMirrorSet` or `ImageTagMirrorSet` resources. (link:https://issues.redhat.com/browse/OCPBUGS-44432[OCPBUGS-44432])


[discrete]
[id="ocp-release-note-installer-bug-fixes_{context}"]
==== Installer

* Previously, if you attempted to install an {aws-first} cluster with minimum privileges and you did not specify an instance type in the `install-config.yaml` file, installation of the cluster failed. This issue happened because the installation program could not find supported instance types that the cluster could use in supported availability zones. For example, the `m6i.xlarge` default instance type was unavailable in `ap-southeast-4` and `eu-south-2` availability zones. With this release, the `openshift-install` program now requires the `ec2:DescribeInstanceTypeOfferings` {aws-short} permission to prevent the installation of the cluster from failing in situations where `m6i.xlarge` or another supported instance type is unavailable in a supported availability zone. (link:https://issues.redhat.com/browse/OCPBUGS-46596[OCPBUGS-46596])

* Previously, the installation program did not prevent users from attempting to install a single-node cluster on bare metal, which resulted in a failed installation. With this update, the installation program prevents single-node cluster installations on unsupported platforms. (link:https://issues.redhat.com/browse/OCPBUGS-56811[OCPBUGS-56811])

* Previously, when you diagnosed issues related to running the `openshift-install destroy cluster` command for {vmw-first}, the logging information provided insufficient detail. As a consequence, it was unclear why clusters were not removed from virtual machines (VMs). With this release, when you destroy a cluster, enhanced debug logging is provided and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-56372[OCPBUGS-56372])

* Previously, when installing into an existing virtual private cloud (VPC) on {aws-first}, a potential mismatch could occur in the subnet information in the {aws-short} Availability Zone between the machine set custom resources for control plane nodes and their corresponding {aws-short} EC2 instances. As a consequence, where the control plane nodes were spread across three Availability Zones and one was recreated the discrepancy could result in an unbalanced control plane as two nodes occurred within the same Availability Zone. With this release, it is ensured that the subnet Availability Zone information in the machine set custom resources and in the EC2 instances match and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-55492[OCPBUGS-55492])

* Previously, when installing a cluster with the `OVNKubernetes` network plugin, the installation could fail if the plugin is specified as `OVNkubernetes` with a lowercase "k". With this update, the installation program correctly interprets the plugin name regardless of case. (link:https://issues.redhat.com/browse/OCPBUGS-54606[OCPBUGS-54606])

* When a Proxy is configured, the installation program adds the `machineNetwork` CIDR to the `noProxy` field. Previously, if the `machineNetwork` CIDR had also been configured by the user in the `noProxy` field, this would result in a duplicate entry, which is not allowed by ignition and could prevent the host from booting properly. With this release, the installation program will not add the `machineNetwork` CIDR to the `noProxy` field if it has already been set. (link:https://issues.redhat.com/browse/OCPBUGS-53183[OCPBUGS-53183])

* Previously, API and ingress VIPs were automatically assigned even when a user-managed load balancer was in use. This behavior was unintended. Now, API and ingress VIPs are no longer automatically assigned. If these values are not explicitly set in the `install-config.yaml` file, the installation fails with an error, prompting the user to provide them. (link:https://issues.redhat.com/browse/OCPBUGS-53140[OCPBUGS-53140]) 

* Previously, when using the Agent-based Installer, the WWN of Fibre Channel (FC) multipath volumes was not detected during hardware discovery. As a result, when the `wwn` root device hint was specified, all multipath FC volumes were excluded by it. With this release, the WWN is now collected for multipath FC volumes, so when more than one multipath volume is present, users can select between them by using the `wwn` root device hint. (link:https://issues.redhat.com/browse/OCPBUGS-52994[OCPBUGS-52994])

* Previously, when installing a cluster on {azure-short}, the installation program did not include support for NVMe or SCSI, which prevented the use of VM instance families that require it. With this update, the installation program can make use of VM instance families that require NVMe or SCSI support. (link:https://issues.redhat.com/browse/OCPBUGS-52658[OCPBUGS-52658])

* Previously, when installing a cluster on {gcp-short} with a user-provided encryption key, the installation program could fail to find the key ring. With this update, the installation program finds the user-provided encryption key ring so the installation does not fail. (link:https://issues.redhat.com/browse/OCPBUGS-52203[OCPBUGS-52203])

* Previously, when installing a cluster on {gcp-short}, the installation could fail if network instability prevented the fetching of {gcp-short} tags during installation. With this update, the installation program has been improved to tolerate network instability during installation. (link:https://issues.redhat.com/browse/OCPBUGS-50919[OCPBUGS-50919])

* Previously, the installer was not checking for ESXi hosts that were powered off within a {vmw-first} cluster, which caused the installation to fail because the OVA could not be uploaded. With this release, the installer now checks the power status of each ESXi host and skips any that are powered off, which resolves the issue and allows the OVA to be imported successfully. (link:https://issues.redhat.com/browse/OCPBUGS-50649[OCPBUGS-50649])

* Previously, when using the Agent-based Installer, erroneous error messages regarding "unable to read image" were output when building the Agent ISO image in a disconnected environment. With this release, these erroneous messages have been removed and no longer appear. (link:https://issues.redhat.com/browse/OCPBUGS-50637[OCPBUGS-50637])

* Previously, when installing a cluster on {azure-short}, the installation program would crash with a segmentation fault error if it did not have the correct permissions to check IP address availability. With this update, the installation program correctly identifies the missing permission and fails gracefully. (link:https://issues.redhat.com/browse/OCPBUGS-50534[OCPBUGS-50534])

* Previously, when the `ClusterNetwork` classless inter-domain routing (CIDR) mask value is greater than the `hostPrefix` value and the `networking.ovnKubernetesConfig.ipv4.internalJoinSubnet` section is provided in the `install-config.yaml` file, the installation program failed a validation check and returned a Golang runtime error. With this release, the installation program still fails the validation check and now outputs a descriptive error message that indicates the invalid `hostPrefix` value. (link:https://issues.redhat.com/browse/OCPBUGS-49784[OCPBUGS-49784])

* Previously, when installing a cluster on {ibm-cloud-name}, the installation program failed to install on the `ca-mon` region even though it is available. With this update, the installation program is up to date with the latest available {ibm-cloud-name} regions. (link:https://issues.redhat.com/browse/OCPBUGS-49623[OCPBUGS-49623])

* Previously, after installing a cluster on {aws-short} with minimum permissions in an existing VPC with a user-provided public IPv4 pool, the cluster could not be destroyed due to a missing permission. With this update, the installation program propagates the `ec2:ReleaseAddress` permission so that the cluster can be destroyed. (link:https://issues.redhat.com/browse/OCPBUGS-49594[OCPBUGS-49594])

* Previously, the installer for {vmw-first} did not validate the number of networks provided in the `install-config.yaml` for failure domains. This caused the installation to proceed with an unsupported configuration if more than the maximum of 10 networks were specified, without providing an error. With this release, the installer now validates the number of configured networks, which resolves the issue by preventing the use of a configuration that exceeds the maximum limit. (link:https://issues.redhat.com/browse/OCPBUGS-49351[OCPBUGS-49351])

* Previously, installing a cluster on {aws-short} with existing subnets (BYO VPC) in Local or Wavelength zones resulted in the edge subnets resource missing the `kubernetes.io/cluster/<InfraID>:shared` tag. With this release, a fix ensures that all subnets used in the `install-config.yaml` file have the required tags. (link:https://issues.redhat.com/browse/OCPBUGS-48827[OCPBUGS-48827])

* Previously, an issue prevented configuring multiple subnets in the failure domain of a Nutanix cluster during installation. The issue is resolved in this release. (link:https://issues.redhat.com/browse/OCPBUGS-49885[OCPBUGS-49885])

* Previously, when installing a cluster on {aws-short}, the `ap-southeast-5` region was not available in the installation program survey, even though this region was supported by {product-title}. With this update, the `ap-southeast-5` region is available. (link:https://issues.redhat.com/browse/OCPBUGS-47681[OCPBUGS-47681])

* Previously, when destroying a cluster installed on {gcp-short}, some resources could be left behind because the installation program did not wait to ensure that all destroy operations completed successfully. With this update, the destroy API waits to ensure that all resources are appropriately deleted. (link:https://issues.redhat.com/browse/OCPBUGS-47489[OCPBUGS-47489])

* Previously, when installing a cluster on {aws-short} in the `us-east-1` regions, the installation could fail if no zone is specified in the `install-config.yaml` file because the `use1-az3` zone does not support any instance types supported by {product-title}. With this update, the installation program prevents the use of the `use1-az3` zone when no zones are specified in the installation configuration file. (link:https://issues.redhat.com/browse/OCPBUGS-47477[OCPBUGS-47477])

* Previously, if you attempted to install an {aws-first} cluster with minimum privileges and you did not specify an instance type in the `install-config.yaml` file, installation of the cluster failed. This issue happened because the installation program could not find supported instance types that the cluster uses in availability zones. For example, the `m6i.xlarge` default instance type was unavailable in `ap-southeast-4` and `eu-south-2` regions. With this release, the `openshift-install` program now requires the `ec2:DescribeInstanceTypeOfferings` {aws-short} permission so to prevent the installation of the cluster from failing in situations where `m6i.xlarge` or another support instance type is unavailable in a supported availability zone. (link:https://issues.redhat.com/browse/OCPBUGS-46596[OCPBUGS-46596])

* Previously, when installing a cluster on {gcp-short}, the installation would fail if you enabled the `constraints/compute.vmCanIpForward` constraint on your project. With this update, the installation program disables this constraint if it is enabled, allowing the installation to succeed. (link:https://issues.redhat.com/browse/OCPBUGS-46571[OCPBUGS-46571])

* Previously, when installing a cluster on {gcp-short}, the installation program would fail to detect if the user provided an encryption key ring that did not exist, causing the installation to fail. With this update, the installation program correctly validates the existence of user provided encryption key rings, preventing failure. (link:https://issues.redhat.com/browse/OCPBUGS-46488[OCPBUGS-46488])

* Previously, when destroying a cluster that was installed on {azure-first}, the inbound NAT rules and security groups for the bootstrap node were not deleted. With this update, the correct resource group ensures that all resources are deleted when the cluster is destroyed. (link:https://issues.redhat.com/browse/OCPBUGS-45429[OCPBUGS-45429])

* Previously, when installing a cluster on {aws-short} in the `ap-southeast-5` region, the installation could fail due to a malformed load balancer hostname. With this update, the installation program has been improved to form the correct hostname so that installation succeeds. (link:https://issues.redhat.com/browse/OCPBUGS-45289[OCPBUGS-45289])

* Previously, when installing a cluster on {gcp-short}, the installation program could fail to locate the service account it created due to a delay in activating the service account on Google's servers. With this update, the installation program waits an appropriate amount of time before attempting to use the created service account. (link:https://issues.redhat.com/browse/OCPBUGS-45280[OCPBUGS-45280])

* Previously, when installing a cluster on {aws-short}, the installation could fail if you specified an edge machine pool but did not specify an instance type. With this update, the installation program requires that an instance type be provided for edge machine pools. (link:https://issues.redhat.com/browse/OCPBUGS-45218[OCPBUGS-45218])

* Previously, when destroying a cluster installed on {gcp-short}, PVC disks with the label `kubernetes-io-cluster-<cluster-id>: owned` were not deleted. With this update, the installation program correctly locates and deletes these resources when the cluster is destroyed. (link:https://issues.redhat.com/browse/OCPBUGS-45162[OCPBUGS-45162])

* Previously, during a disconnected installation, when the `imageContentSources` parameter was configured for more than one mirror for a source, the command to create the agent ISO image could fail, depending on the sequence of the mirror configuration. With this release, multiple mirrors are handled correctly when the agent ISO is created and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-44938[OCPBUGS-44938])

* Previously, when installing a cluster on {aws-short}, if the `publicIPv4Pool` parameter was set but the `ec2:AllocateAddress` permission was not present, the installation would fail. With this update, the installation program requires that this permission is present. (link:https://issues.redhat.com/browse/OCPBUGS-44925[OCPBUGS-44925])

* Previously, during a shared Virtual Private Cloud (VPC) installation, the installer added the records to a private DNS zone created by the installer instead of adding the records to the clusters private DNS zone. As a consequence, the installation failed. With this release, the installer searches for an existing private DNS zone and, if found, pairs that zone with the network that is supplied by the `install-config.yaml` file and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-44641[OCPBUGS-44641])

* Previously, you could add white space to {aws-first} tag names but the installation program did not support them. This situation resulted in the installation program outputting an `ERROR failed to fetch Metadata` message. With this release, the regular expression for {aws-short} tags now validates any tag name that has white space so that the installation program accepts these tags and no longer outputs an error because of white space. (link:https://issues.redhat.com/browse/OCPBUGS-44199[OCPBUGS-44199])

* For clusters that were installed with the Agent-based Installer for versions 4.15.0 to 4.15.26, root certificates that were built in from CoreOS were added to the user-ca-bundle, even though they were not explicitly specified by the user. In previous releases, when adding a node to one of these clusters using the `oc adm node-image create` command, the `additionalTrustBundle` obtained from the cluster's user-ca-bundle was too large to process, resulting in a failure to add the node. With this release, the built-in certificates are filtered out when generating the `additionalTrustBundle`, so that only explicitly user-configured certificates are included, and nodes can be added successfully. (link:https://issues.redhat.com/browse/OCPBUGS-43990[OCPBUGS-43990])

* Previously, when destroying a cluster that was installing on {gcp-short}, forwarding rules, health checks and firewall rules were not deleted, leading to errors. With this update, all resources are deleted when the cluster is destroyed. (link:https://issues.redhat.com/browse/OCPBUGS-43779[OCPBUGS-43779])

* Previously, when installing a cluster on {azure-full}, specifying the `Standard_M8-4ms` instance type resulted in an error due to that instance type specifying its memory in decimal format instead of integer format. With this update, the installation program correctly parses the memory value. (link:https://issues.redhat.com/browse/OCPBUGS-42241[OCPBUGS-42241])

* Previously, when installing a cluster on {vmw-full}, installations could fail if the API and Ingress server virtual IPs were outside of the machine network. With this update, the installation program includes the API and Ingress server virtual IPs in the machine network by default. If you specify the API and Ingress server virtual IPs, ensure that they are in the machine network. (link:https://issues.redhat.com/browse/OCPBUGS-36553[OCPBUGS-36553])

[discrete]
[id="ocp-release-note-insights-operator-bug-fixes_{context}"]
==== Insights Operator

[discrete]
[id="ocp-release-note-kube-controller-bug-fixes_{context}"]
==== Kubernetes Controller Manager

[discrete]
[id="ocp-release-note-kube-scheduler-bug-fixes_{context}"]
==== Kubernetes Scheduler

[discrete]
[id="ocp-release-note-machine-config-operator-bug-fixes_{context}"]
==== Machine Config Operator

[discrete]
[id="ocp-release-note-management-console-bug-fixes_{context}"]
==== Management Console

[discrete]
[id="ocp-release-note-monitoring-bug-fixes_{context}"]
==== Monitoring

[discrete]
[id="ocp-release-note-networking-bug-fixes_{context}"]
==== Networking

* Previously, when a pod used the CNI plugin for DHCP address assignment, in conjunction with other CNI plugins, the pod's network interface might have been unexpectedly deleted. As a consequence, when the pod's DHCP lease expired, the DHCP proxy entered a loop when trying to re-create a new lease, leading to the node becoming unresponsive. With this release, the DHCP lease maintenance terminates if the network interface does not exist. As a result, interface deletions are handled gracefully, ensuring node stability. (link:https://issues.redhat.com/browse/OCPBUGS-45272[OCPBUGS-45272])

* Previously, the Kubernetes NMState Operator Operator did not create the `nmstate-console-plugin` pod  because of an issue with the `pluginPort` template. With this release, a fix to the template ensures that the Operator can now succesfully create the `nmstate-console-plugin` pod. (link:https://issues.redhat.com/browse/OCPBUGS-54295[OCPBUGS-54295])

* Previously, the pod controller in the Whereabouts reconciler was not passing the namespace to the leader election function, so the pod controller was not deleting orphaned allocations. This lead to repeated log error messages. With this release, the namespace is passed in and the orphaned allocations are deleted properly. (link:https://issues.redhat.com/browse/OCPBUGS-53397[OCPBUGS-53397])

* Previously, the `SriovOperatorConfig` Operator removed any parameters that had default values in the `SriovOperatorConfig` resources. This situation caused certain information to be missing from the output of a resource.  With this release, the Operator uses the PATCH method for API servers to preserve parameters with default values so that no information is missing from the output for a resource. (link:https://issues.redhat.com/browse/OCPBUGS-53346[OCPBUGS-53346])

* Previously, the `SriovNetworkNodePolicy` object reconciler executed with every node resource update. This resulted in excessive resource consumption by the SR-IOV Operator pod and an overabundance of log entries. This release changes the behavior so that the reconciler only runs when a node label changes, thereby reducing resource consumption and log entry generation. (link:https://issues.redhat.com/browse/OCPBUGS-52955[OCPBUGS-52955])

* Previously, a cluster with the `clusterNetwork` parameter listing multiple networks of the same IP address family entered a `crashloopbackoff` state when upgrading to the latest version of {product-title}. With this release,  a fix ensures that the cluster with this configuration no longer enters the `crashloopbackoff` state during a cluster upgrade. (link:https://issues.redhat.com/browse/OCPBUGS-49994[OCPBUGS-49994])

* Previously, the `resolv-prepender` service triggered earlier than expected. This situation caused the service to fail and resulted in an incorrectly configured setting for the host DNS. With this release, the configuration for the `resolv-prepender` service is updated so that when the service triggers earlier than expected, it no longer causes incorrect configuration of the host DNS settings. (link:https://issues.redhat.com/browse/OCPBUGS-49436[OCPBUGS-49436])

* Previously, the `nmstate-configuration` service was only enabled for deployments that had the `platform` parameter set to `baremetal`. However, you could also use the Assisted Installer to configure a bare-metal deployment by setting the `platform` parameter set to  `None`,  but the NMState `br-ex` network bridge creation feature did not work with this installation method.  With this release, the `nmstate-configuration` service is moved to the base directory in the cluster installation path so that any deployments configured with the `platform` parameter set to  `None`, do not impact the NMState `br-ex` network bridge creation feature. (link:https://issues.redhat.com/browse/OCPBUGS-48566[OCPBUGS-48566])

* Previously, for a layer 2 or layer 3 topology network that had the gateway mode set to `local`, OVN-Kubernetes experienced an issue when it was restarted. The issue caused the Egress IP to be selected as the primary IP address for the network. With this release, a fix ensures that this behavior no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-46585[OCPBUGS-46585])

* Previously, the DNS-based egress firewall incorrectly prevented creation of any firewall rule that contained a DNS name in uppercase characters. With this release, a fix to the egress firewall means that creation of a firewall rule that contains a DNS name in uppercase occurs. (link:https://issues.redhat.com/browse/OCPBUGS-46564[OCPBUGS-46564])

* Previously, when a pod was running on a node on which an egress on the IPv6 protocol was assigned, the pod was not able to communicate with the OVN-Kubernetes service in a dual-stack cluster. This resulted in the traffic with the IP address family, that the `egressIP` was not applicable to, being dropped. With this release, only the source network address translation (SNAT) for the IP address family that the egress IPs applied to is deleted, eliminating the risk of traffic being dropped. (link:https://issues.redhat.com/browse/OCPBUGS-46543[OCPBUGS-46543])

* Previously, when you used static IP addresses in the customized `br-ex` network bridge configuration of a manifest object, a race condition was added and caused a node reboot operation that further impacted deployment of a cluster. With this release, the `nodeip-configuration` service now starts after the `br-ex` network bridge is up, preventing the race condition and node reboot. (link:https://issues.redhat.com/browse/OCPBUGS-46072[OCPBUGS-46072])

* Previously, the HAProxy router incorrectly assumed that only SHA1 leaf certificates were rejected by HAProxy, causing the router to fail by not rejecting SHA1 intermediate certificates. With this update, the router now inspects and rejects all non-self-signed SHA1 certificates, thereby preventing crashes and improving stability for your cluster stability. (link:https://issues.redhat.com/browse/OCPBUGS-45290[OCPBUGS-45290])

* Previously, when a node restarted the `openvswitch` daemon, the `nmstate-handler` container could not access the OpenVSwtich (OVS) database and this caused all OVS-related NNCP configurations to fail. With this release, the issue is fixed. The `nmstate-handler` container can access the OVS database even after restarting the OVS process on the node. The `nmstate-handler` no longer requires a manual restart. (link:https://issues.redhat.com/browse/OCPBUGS-44596[OCPBUGS-44596])

* Previously, a `MultiNetworkPolicy` API was not enforced when the `protocol` parameter was specified, but the `port` parameter was not, in the cluster configuration. This situation caused all network traffic to reach the cluster. With this release, the `MultiNetworkPolicy` API policy only allows connections from and to the ports specified with the `protocol` parameter so that only specific traffic reaches the cluster. (link:https://issues.redhat.com/browse/OCPBUGS-44354[OCPBUGS-44354])

* Previously, HAProxy left idle connections open when it reloaded its configuration until either the next time a client sent a request by using the idle connection or the `hard-stop-after` period elapsed. This release adds a new `IdleConnectionTerminationPolicy` API field to control HAProxy behavior for idle connections during reloads. The new default setting is `Immediate`, which means that HAProxy immediately terminates any idle connections when it reloads its configuration. The previous behavior can be specified by using the `Deferred` setting for `IdleConnectionTerminationPolicy`. (link:https://issues.redhat.com/browse/OCPBUGS-43745[OCPBUGS-43745])

* Previously, if an application did not use the Path MTU discovery (PMTUD) mechanism while sending UDP packets larger than the network MTU, an issue with the `OVN` package caused the a packet to be dropped while fragmenting the packet. With this release, the `OVN` package has been fixed so that large UDP packets are properly fragmented and sent over the network. (link:https://issues.redhat.com/browse/OCPBUGS-43649[OCPBUGS-43649])

* Previously, a pod with an IP address in an OVN-Kubernetes `Localnet` network was unreachable by other pods that ran on the same node but used the default network for communication. Communication between pods on different nodes was not impacted by this communication issue.  With this release, communication between a `Localnet` pod and a default network pod that both run on the same node is possible. (link:https://issues.redhat.com/browse/OCPBUGS-43004[OCPBUGS-43004])

* Previously, when specific network changes were made to a running cluster, a `NetworkManager` connection profile was permanently created by the `ovs-configuration` service and the profile was incorrectly saved to storage. This profile file would persist through a reboot operation and caused the `ovs-configuration` service to fail. With this release, the `ovs-configuration` cleanup process is updated to remove any unnecessary files, preventing such files from causing issues after a reboot operation. (link:https://issues.redhat.com/browse/OCPBUGS-41489[OCPBUGS-41489])

* Previously, the `parseIPList` function failed to handle IP address lists that contained both valid and invalid IP addresses or CIDR ranges. This situation caused the function to return an empty string when it encountered an invalid entry and skip processing valid entries. With this release, the `haproxy.router.openshift.io/ip_allowlist` route annotation skips any invalid IP addresses or CIDR ranges so that the `parseIPList` function can process all listed entries. (link:https://issues.redhat.com/browse/OCPBUGS-39403[OCPBUGS-39403])

* Previously, the HAProxy router lacked out-of-bounds validation for the `router.openshift.io/haproxy.health.check.interval` annotation. If you set a value that exceeded the maximum value that the HAProxy router could handle, the `router-default` pod could not reach the `Ready` state. With this release, the router now validates the value for the annotation and excludes values that are out of bounds. The router now functions as expected. (link:https://issues.redhat.com/browse/OCPBUGS-38078[OCPBUGS-38078])

* Previously, in certain situations the gateway IP address for a node changed and caused the `OVN` cluster router, which manages the static route to the cluster subnet, to add a new static route with the new gateway IP address, without deleting the original one. As a result, a stale route still pointed to the switch subnet and this caused intermittent drops during egress traffic transfer. With this release, a patch applied to the `OVN` cluster router ensures that if the gateway IP address changes, the `OVN` cluster router updates the existing static route with the new gateway IP address. A stale route no longer points to the `OVN` cluster router so that egress traffic flow does not drop. (link:https://issues.redhat.com/browse/OCPBUGS-32754[OCPBUGS-32754])

* Previously, there was no event logged when an error occurred from a failed conversion from ingress to route. With this update, an error for a failed conversion is logged. (link:https://issues.redhat.com/browse/OCPBUGS-29354[OCPBUGS-29354])

[discrete]
[id="ocp-release-note-node-bug-fixes_{context}"]
==== Node

[discrete]
[id="ocp-release-note-node-tuning-operator-bug-fixes_{context}"]
==== Node Tuning Operator (NTO)

[discrete]
[id="ocp-release-note-observability-bug-fixes_{context}"]
==== Observability

[discrete]
[id="ocp-release-note-oc-mirror-bug-fixes_{context}"]
==== oc-mirror

[discrete]
[id="ocp-release-note-openshift-cli-bug-fixes_{context}"]
==== OpenShift CLI (oc)

[discrete]
[id="ocp-release-note-olm-bug-fixes_{context}"]
==== Operator Lifecycle Manager (OLM)

[discrete]
[id="ocp-release-note-openshift-api-server-bug-fixes_{context}"]
==== OpenShift API server

[discrete]
[id="ocp-release-note-pao-bug-fixes_{context}"]
==== Performance Addon Operator

[discrete]
[id="ocp-release-note-rhcos-bug-fixes_{context}"]
==== {op-system-first}

[discrete]
[id="ocp-release-note-scalability-and-performance-bug-fixes_{context}"]
==== Scalability and performance

* Before this release, CPUs for the last guaranteed pod admitted to a node remained allocated after the pod was deleted. As a consequence, this caused scheduling domain inconsistencies. With this release, CPUs allocated to guaranteed pods return to the pool of available CPU resources as expected, ensuring correct CPU scheduling for subsequent pods. (link:https://issues.redhat.com/browse/OCPBUGS-17792[OCPBUGS-17792])

[discrete]
[id="ocp-release-note-storage-bug-fixes_{context}"]
==== Storage

[discrete]
[id="ocp-release-note-image-registry-bug-fixes_{context}"]
==== Registry

* Previously, image importing from blocked registries would fail if those registries were configured with `NeverContactSource`, even when mirror registries were set up. With this update, image importing is no longer blocked when a registry has mirrors configured. This ensures that image imports succeed even if the original source was set to `NeverContactSource` in the `ImageDigestMirrorSet` or `ImageTagMirrorSet` resources. (link:https://issues.redhat.com/browse/OCPBUGS-44432[OCPBUGS-44432])

[discrete]
[id="ocp-release-note-windows-containers-bug-fixes_{context}"]
==== Windows containers

[id="ocp-4-19-technology-preview-tables_{context}"]
== Technology Preview features status

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red{nbsp}Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the following tables, features are marked with the following statuses:

* _Not Available_
* _Technology Preview_
* _General Availability_
* _Deprecated_
* _Removed_


[discrete]
[id="ocp-release-notes-auth-tech-preview_{context}"]
=== Authentication and authorization Technology Preview features

.Authentication and authorization Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Pod security admission restricted enforcement
|Technology Preview
|Technology Preview
|Technology Preview

|Direct authentication with an external OIDC identity provider
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
[id="ocp-release-notesedge-computing-tp-features_{context}"]
=== Edge computing Technology Preview features

.Edge computing Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Accelerated provisioning of {ztp}
|Technology Preview
|Technology Preview
|Technology Preview

|Enabling disk encryption with TPM and PCR protection
|Technology Preview
|Technology Preview
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-extensions-tech-preview_{context}"]
=== Extensions Technology Preview features

// "Extensions" refers to OLMv1

.Extensions Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{olmv1-first}
|Technology Preview
|General Availability
|General Availability

|{olmv1} runtime validation of container images using sigstore signatures
|Not Available
|Technology Preview
|Technology Preview

|{olmv1} permissions preflight check for cluster extensions
|Not Available
|Not Available
|Technology Preview

|{olmv1} deploying a cluster extension in a specified namespace
|Not Available
|Not Available
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-installing-tech-preview_{context}"]
=== Installation Technology Preview features

.Installation Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

// All GA in 4.17 notes for oci-first
|Adding kernel modules to nodes with kvc
|Technology Preview
|Technology Preview
|Technology Preview

|Enabling NIC partitioning for SR-IOV devices
|General Availability
|General Availability
|General Availability

|User-defined labels and tags for {gcp-first}
|General Availability
|General Availability
|General Availability

|Installing a cluster on Alibaba Cloud by using Assisted Installer
|Technology Preview
|Technology Preview
|Technology Preview

|Mount shared entitlements in BuildConfigs in RHEL
|Technology Preview
|Technology Preview
|Technology Preview

|OpenShift zones support for vSphere host groups
|Not Available
|Not Available
|Technology Preview

|Selectable Cluster Inventory
|Technology Preview
|Technology Preview
|Technology Preview

|Installing a cluster on {gcp-short} using the Cluster API implementation
|General Availability
|General Availability
|General Availability

|Enabling a user-provisioned DNS on {gcp-short}
|Not Available
|Not Available
|Technology Preview

|Installing a cluster on {vmw-full} with multiple network interface controllers
|Not Available
|Technology Preview
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-mco-tech-preview_{context}"]
=== Machine Config Operator Technology Preview features

.Machine Config Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Improved MCO state reporting (`oc get machineconfigpool`)
|Technology Preview
|Technology Preview
|Technology Preview

|Image mode for OpenShift/On-cluster RHCOS image layering
|Technology Preview
|Technology Preview
|General Availability

|====

[discrete]
[id="ocp-release-notes-machine-management-tech-preview_{context}"]
=== Machine management Technology Preview features

.Machine management Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Managing machines with the Cluster API for {aws-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {gcp-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {azure-full}
|Not Available
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {vmw-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for {ibm-power-server-name}
|Technology Preview
|Technology Preview
|Technology Preview

|Adding multiple subnets to an existing {vmw-full} cluster by using compute machine sets
|Not Available
|Technology Preview
|Technology Preview

|Configuring Trusted Launch for {azure-full} virtual machines by using machine sets
|Technology Preview
|Technology Preview
|General Availability

|Configuring {azure-short} confidential virtual machines by using machine sets
|Technology Preview
|Technology Preview
|General Availability
|====

[discrete]
[id="ocp-release-notes-monitoring-tech-preview_{context}"]
=== Monitoring Technology Preview features

.Monitoring Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Metrics Collection Profiles
|Technology Preview
|Technology Preview
|General Availability

|====

[discrete]
[id="ocp-release-notes-multi-arch-tech-preview_{context}"]
=== Multi-Architecture Technology Preview features

.Multi-Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`kdump` on `arm64` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `s390x` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `ppc64le` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|Support for configuring the image stream import mode behavior
|Not Available
|Technology Preview
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-networking-tech-preview_{context}"]
=== Networking Technology Preview features

.Networking Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|eBPF manager Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Advertise using L2 mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Technology Preview
|Technology Preview
|Technology Preview

|Updating the interface-specific safe sysctls list
|Technology Preview
|Technology Preview
|Technology Preview

|Egress service custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|VRF specification in `BGPPeer` custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|VRF specification in `NodeNetworkConfigurationPolicy` custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|Host network settings for SR-IOV VFs
|General Availability
|General Availability
|General Availability

|Integration of MetalLB and FRR-K8s
|General Availability
|General Availability
|General Availability

|Automatic leap seconds handling for PTP grandmaster clocks
|General Availability
|General Availability
|General Availability

|PTP events REST API v2
|General Availability
|General Availability
|General Availability

|OVN-Kubernetes customized `br-ex` bridge on bare metal
|General Availability
|General Availability
|General Availability

|OVN-Kubernetes customized `br-ex` bridge on {vmw-short} and {rh-openstack}
|Technology Preview
|Technology Preview
|Technology Preview

|Live migration to OVN-Kubernetes from OpenShift SDN
|General Availability
|Not Available
|Not Available

|User-defined network segmentation
|Technology Preview
|General Availability
|General Availability

|Dynamic configuration manager
|Not Available
|Technology Preview
|Technology Preview

|SR-IOV Network Operator support for Intel C741 Emmitsburg Chipset
|Not Available
|Technology Preview
|Technology Preview

|Gateway API and Istio for Ingress management
|Not Available
|Technology Preview
|General Availability

|Dual-port NIC for PTP ordinary clock
|Not Available
|Not Available
|Technology Preview

|DPU Operator
|Not Available
|Not Available
|Technology Preview

|Fast IPAM for the Whereabouts IPAM CNI plugin
|Not Available
|Not Available
|Technology Preview

|Unnumbered BGP peering
|Not Available
|Not Available
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-nodes-tech-preview_{context}"]
=== Node Technology Preview features

.Nodes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`MaxUnavailableStatefulSet` featureset
|Technology Preview
|Technology Preview
|Technology Preview

|sigstore support
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
[id="ocp-release-notes-oc-cli-tech-preview_{context}"]
=== OpenShift CLI (oc) Technology Preview features

.OpenShift CLI (`oc`) Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|oc-mirror plugin v2
|Technology Preview
|General Availability
|General Availability

|oc-mirror plugin v2 enclave support
|Technology Preview
|General Availability
|General Availability

|oc-mirror plugin v2 delete functionality
|Technology Preview
|General Availability
|General Availability
|====

[discrete]
[id="ocp-release-notes-operator-lifecycle-tech-preview_{context}"]
=== Operator lifecycle and development Technology Preview features

// "Operator lifecycle" refers to OLMv0 and "development" refers to Operator SDK

.Operator lifecycle and development Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{olmv1-first}
|Technology Preview
|General Availability
|General Availability

|Scaffolding tools for Hybrid Helm-based Operator projects
|Deprecated
|Removed
|Removed

|Scaffolding tools for Java-based Operator projects
|Deprecated
|Removed
|Removed
|====

[discrete]
[id="ocp-release-notes-rhcos-tech-preview_{context}"]
=== {rh-openstack-first} Technology Preview features

.{rh-openstack} Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{rh-openstack} integration into the {cluster-capi-operator}
|Technology Preview
|Technology Preview
|Technology Preview

|Control plane with `rootVolumes` and `etcd` on local disk
|General Availability
|General Availability
|General Availability
|====

[discrete]
[id="ocp-release-notes-scalability-tech-preview_{context}"]
=== Scalability and performance Technology Preview features

.Scalability and performance Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{factory-prestaging-tool}
|Technology Preview
|Technology Preview
|Technology Preview

|Hyperthreading-aware CPU manager policy
|Technology Preview
|Technology Preview
|Technology Preview

|Mount namespace encapsulation
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Increasing the etcd database size
|Technology Preview
|Technology Preview
|Technology Preview

|Using {rh-rhacm} `PolicyGenerator` resources to manage {ztp} cluster policies
|Technology Preview
|Technology Preview
|General Availability

|Pinned Image Sets
|Technology Preview
|Technology Preview
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-special-hardware-tech-preview_{context}"]
=== Specialized hardware and driver enablement Technology Preview features

.Specialized hardware and driver enablement Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19
|====

[discrete]
[id="ocp-release-notes-storage-tech-preview_{context}"]
=== Storage Technology Preview features

.Storage Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|AWS EFS storage CSI usage metrics
|General Availability
|General Availability
|General Availability

|Automatic device discovery and provisioning with Local Storage Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Azure File CSI snapshot support
|Technology Preview
|Technology Preview
|Technology Preview

|Azure File cross-subscription support
|Not Available
|Not Available
|General Availability

|Shared Resources CSI Driver in OpenShift Builds
|Technology Preview
|Technology Preview
|Technology Preview

|{secrets-store-operator}
|Technology Preview
|General Availability
|General Availability

|CIFS/SMB CSI Driver Operator
|Technology Preview
|General Availability
|General Availability

|VMware vSphere multiple vCenter support
|Technology Preview
|General Availability
|General Availability

|Disabling/enabling storage on vSphere
|Technology Preview
|Technology Preview
|General Availability

|Increasing max number of volumes per node for vSphere
|Not Available
|Not Available
|Technology Preview

|RWX/RWO SELinux Mount
|Developer Preview
|Developer Preview
|Developer Preview

|Migrating CNS Volumes Between Datastores
|Developer Preview
|Developer Preview
|General Availability

|CSI volume group snapshots
|Not Available
|Technology Preview
|Technology Preview

|GCP PD supports C3/N4 instance types and hyperdisk-balanced disks
|Not Available
|General Availability
|General Availability

|GCP Filestore supports Workload Identity
|General Availability
|General Availability
|General Availability

|OpenStack Manila support for CSI resize
|Not Available
|General Availability
|General Availability

|Volume Attribute Classes
|Not Available
|Not Available
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-web-console-tech-preview_{context}"]
=== Web console Technology Preview features

.Web console Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{ols-official} in the {product-title} web console
|Technology Preview
|Technology Preview
|Technology Peview
|====

[id="ocp-4-19-known-issues_{context}"]
== Known issues

* In {product-title} {product-version}, clusters using IPsec for network encryption might experience intermittent loss of pod-to-pod connectivity. This prevents some pods on certain nodes from reaching services on other nodes, resulting in connection timeouts.
+
Internal testing could not reproduce this issue on clusters with 120 nodes or less.
+
There is no workaround for this issue. (link:https://issues.redhat.com/browse/OCPBUGS-55453[OCPBUGS-55453])

* {product-title} clusters that are installed on {aws-short} in the Mexico Central region, `mx-central-1`, cannot be destroyed. (link:https://issues.redhat.com/browse/OCPBUGS-56020[OCPBUGS-56020])

* When installing a cluster on {azure-short}, if you set any of the`compute.platform.azure.identity.type`, `controlplane.platform.azure.identity.type`, or `platform.azure.defaultMachinePlatform.identity.type` field values to `None`, your cluster is unable to pull images from the Azure Container Registry. You can avoid this issue by either providing a user-assigned identity, or by leaving the identity field blank. In both cases, the installation program generates a user-assigned identity. (link:https://issues.redhat.com/browse/OCPBUGS-56008[OCPBUGS-56008])

* When installing a cluster on {aws-short}, if you did not configure {aws-short} credentials before running any `openshift-install create` command, the installation program will fail. (link:https://issues.redhat.com/browse/OCPBUGS-56658[OCPBUGS-56658])

Previously, the kubelet would not account for probes that ran in the `syncPod` method, which periodically checks the state of a pod and does a readiness probe outside of the normal probe period. With this release, a bug is fixed for when the kubelet incorrectly calculates `readinessProbe` periods. However, pod authors might see that the readiness latency of pods configured with readiness probes might increase. This behavior is more accurate to the configured probe. For more information, see (link:https://issues.redhat.com/browse/OCPBUGS-50522[OCPBUGS-50522])

* There is a known issue with RHEL 8 worker nodes that  use `cgroupv1` Linux Control Groups (cgroup). The following is an example of the error message displayed for impacted nodes: `UDN are not supported on the node ip-10-0-51-120.us-east-2.compute.internal as it uses cgroup v1.` As a workaround, migrate worker nodes from `cgroupv1` to `cgroupv2`. (link:https://issues.redhat.com/browse/OCPBUGS-49933[OCPBUGS-49933])

* There is a known issue when the grandmaster clock (T-GM) transitions to the `Locked` state too soon. This happens before the Digital Phase-Locked Loop (DPLL) completes its transition to the `Locked-HO-Acquired` state, and after the Global Navigation Satellite Systems (GNSS) time source is restored. (link:https://issues.redhat.com/browse/OCPBUGS-49826[OCPBUGS-49826]) 

* When a pod uses the CNI plugin for DHCP address assignment, in conjunction with other CNI plugins, the pod's network interface might be unexpectedly deleted. As a result, when the pod's DHCP lease expires, the DHCP proxy enters a loop when trying to re-create a new lease, leading to the node becoming unresponsive. There is currently no known workaround. (link:https://issues.redhat.com/browse/OCPBUGS-45272[OCPBUGS-45272])

* When installing a cluster on {azure-short}, if you set any of the `compute.platform.azure.identity.type`, `controlplane.platform.azure.identity.type`, or `platform.azure.defaultMachinePlatform.identity.type` field values to `None`, your cluster is unable to pull images from the Azure Container Registry. You can avoid this issue by either providing a user-assigned identity, or by leaving the identity field blank. In both cases, the installation program generates a user-assigned identity. (link:https://issues.redhat.com/browse/OCPBUGS-56008[OCPBUGS-56008])

* When installing a cluster on {aws-short}, if you do not configure {aws-short} credentials before running any `openshift-install create` command, the installation program fails. (link:https://issues.redhat.com/browse/OCPBUGS-56658[OCPBUGS-56658])

* The `must-gather` tool does not collect IPsec information for a cluster that was upgraded from {product-title} 4.14. This issue occurs because the `ipsecConfig` configuration in the `networks.operator.openshift.io cluster` CR has an empty construct, `{}`. The empty construct is passed to the upgraded version of {product-title}. As a workaround for this issue, run the following command with the following `ipsecConfig`  configuration in the Cluster Network Operator (CNO) CR:
+
[source,terminal]
----
$ oc patch networks.operator.openshift.io cluster --type=merge -p \
  '{
  "spec":{
    "defaultNetwork":{
      "ovnKubernetesConfig":{
        "ipsecConfig":{
          "mode":"Full" 
        }}}}}'
----
+
After you run the command, the CNO collects `must-gather` logs that you can inspect.
+
(link:https://issues.redhat.com/browse/OCPBUGS-52367[OCPBUGS-52367])

[id="ocp-telco-ran-4-19-known-issues_{context}"]

* In the event of a crash, the `mlx5_core` NIC driver causes an out-of-memory issue and `kdump` does not save the `vmcore` file in `/var/crash`.
To save the `vmcore` file, use the `crashkernel` setting to reserve 1024 MB of memory for the `kdump` kernel. (link:https://issues.redhat.com/browse/OCPBUGS-54520[OCPBUGS-54520], link:https://issues.redhat.com/browse/RHEL-90663[RHEL-90663])

* There is a known latency issue on 4th Gen Intel Xeon processors. (link:https://issues.redhat.com/browse/OCPBUGS-42495[OCPBUGS-42495])

[id="ocp-telco-core-4-19-known-issues_{context}"]

* Currently, pods that use a `guaranteed` QoS class and request whole CPUs might not restart automatically after a node reboot or kubelet restart. The issue might occur in nodes configured with a static CPU Manager policy and using the `full-pcpus-only` specification, and when most or all CPUs on the node are already allocated by such workloads. As a workaround, manually delete and re-create the affected pods. (link:https://issues.redhat.com/browse/OCPBUGS-43280[OCPBUGS-43280])

* Currently, when a `irqbalance` service runs on a specific AArch64 machine, a buffer overflow issue might cause the service to crash. As a consequence, latency sensitive workloads might be affected by unmanaged interrupts that are not properly distributed across CPUs, leading to performance degradation. There is currently no workaround for this issue. (link:https://issues.redhat.com/browse/RHEL-89986[RHEL-89986])

* Currently, on clusters with SR-IOV network virtual functions configured, a race condition might occur between system services responsible for network device renaming and the TuneD service managed by the Node Tuning Operator. As a consequence, the TuneD profile might become degraded after the node restarts, leading to performance degradation. As a workaround, restart the TuneD pod to restore the profile state. (link:https://issues.redhat.com/browse/OCPBUGS-41934[OCPBUGS-41934])

[id="ocp-nodes-4-19-known-issues_{context}"]

[id="ocp-storage-core-4-19-known-issues_{context}"]

* NFS volumes exported from VMWare vSAN Files cannot be mounted by clusters running {product-title} 4.19 due to RHEL-83435. To avoid this issue, ensure that you are running VMWare ESXi and vSAN at the latest patch versions of 8.0 P05, or later. (link:https://issues.redhat.com/browse/OCPBUGS-55978[OCPBUGS-55978])

[id="ocp-hosted-control-planes-4-19-known-issues_{context}"]

[id="ocp-4-19-asynchronous-errata-updates_{context}"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red{nbsp}Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red{nbsp}Hat Customer Portal users can enable errata notifications in the account settings for Red{nbsp}Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red{nbsp}Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating_a_cluster/updating-cluster-web-console.adoc#updating-cluster-web-console[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-19-0-ga_{context}"]
=== RHXA-2025:XXXX - {product-title} {product-version}.0 image release, bug fix, and security update advisory

Issued: DAY-MONTH-YEAR

{product-title} release {product-version}.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHXA-2025:XXXX[RHXA-2025:XXXX] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHXA-2025:XXXX[RHXA-2025:XXXX] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.0 --pullspecs
----

[id="ocp-4-19-0-updating_{context}"]
==== Updating
To update an {product-title} 4.17 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

//replace 4.y.z for the correct values for the release. You do not need to update oc to run this command.
