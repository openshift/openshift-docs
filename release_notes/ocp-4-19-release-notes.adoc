:_mod-docs-content-type: ASSEMBLY
[id="ocp-4-19-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red{nbsp}Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-19-about-this-release_{context}"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-202X:XXXX[RHSA-202X:XXXX]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.32.md[Kubernetes 1.32] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. From the {hybrid-console}, you can deploy {product-title} clusters to either on-premises or cloud environments.

You must use {op-system} machines for the control plane and for the compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517
//Removed paragraph about the RHEL package because mode workers are removed from 4.19, per Scott Dodson
//Even-numbered release lifecycle verbiage (Comment in for even-numbered releases)
////
Starting from {product-title} 4.14, the Extended Update Support (EUS) phase for even-numbered releases increases the total available lifecycle to 24 months on all supported architectures, including `x86_64`, 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures. Beyond this, Red{nbsp}Hat also offers a 12-month additional EUS add-on, denoted as _Additional EUS Term 2_, that extends the total available lifecycle from 24 months to 36 months. The Additional EUS Term 2 is available on all architecture variants of {product-title}. For more information about support for all versions, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].
////

//Odd-numbered release lifecycle verbiage (Comment in for odd-numbered releases)

The support lifecycle for odd-numbered releases, such as {product-title} {product-version}, on all supported architectures, including `x86_64`, 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures is 18 months. For more information about support for all versions, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

Commencing with the {product-title} 4.14 release, Red{nbsp}Hat is simplifying the administration and management of Red{nbsp}Hat shipped cluster Operators with the introduction of three new life cycle classifications; Platform Aligned, Platform Agnostic, and Rolling Stream. These life cycle classifications provide additional ease and transparency for cluster administrators to understand the life cycle policies of each Operator and form cluster maintenance and upgrade plans with predictable support boundaries. For more information, see link:https://access.redhat.com/webassets/avalon/j/includes/session/scribe/?redirectTo=https%3A%2F%2Faccess.redhat.com%2Fsupport%2Fpolicy%2Fupdates%2Fopenshift_operators[OpenShift Operator Life Cycles].

// Added in 4.14. Language came directly from Kirsten Newcomer.
{product-title} is designed for FIPS. When running {op-system-base-full} or {op-system-first} booted in FIPS mode, {product-title} core components use the {op-system-base} cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the `x86_64`, `ppc64le`, and `s390x` architectures.

For more information about the NIST validation program, see link:https://csrc.nist.gov/Projects/cryptographic-module-validation-program/validated-modules[Cryptographic Module Validation Program]. For the latest NIST status for the individual versions of {op-system-base} cryptographic libraries that have been submitted for validation, see link:https://access.redhat.com/articles/2918071#fips-140-2-and-fips-140-3-2[Compliance Activities and Government Standards].

[id="ocp-4-19-add-on-support-status_{context}"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-19-new-features-and-enhancements_{context}"]
== New features and enhancements

This release adds improvements related to the following components and concepts:

[id="ocp-release-notes-auth_{context}"]
=== Authentication and authorization

[id="ocp-release-notes-auth-direct_{context}"]
==== Enabling direct authentication with an external OIDC identity provider (Technology Preview)

With this release, you can enable direct integration with an external OpenID Connect (OIDC) identity provider to issue tokens for authentication. This bypasses the built-in OAuth server and uses the external identity provider directly.

By integrating directly with an external OIDC provider, you can leverage the advanced capabilities of your preferred OIDC provider instead of being limited by the capabilities of the built-in OAuth server. Your organization can manage users and groups from a single interface, while also streamlining authentication across multiple clusters and in hybrid environments. You can also integrate with existing tools and solutions.

Direct authentication is available as a Technology Preview feature.

For more information, see xref:../authentication/external-auth.adoc#external-auth[Enabling direct authentication with an external OIDC identity provider].

[id="ocp-4-19-auth-ServiceAccountTokenNodeBinding_{context}"]
==== Enable ServiceAccountTokenNodeBinding Kubernetes feature by default

In {product-title} {product-version}, the `ServiceAccountTokenNodeBinding` feature is now enabled by default, aligning with upstream Kubernetes behavior. This feature allows service account tokens to be bound directly to node objects in addition to the existing binding options. Benefits of this change include enhanced security through automatic token invalidation when bound nodes are deleted and better protection against token replay attacks across different nodes.

[id="ocp-release-notes-backup-restore_{context}"]
=== Backup and restore

[id="ocp-release-notes-builds_{context}"]
=== Builds

[id="ocp-release-notes-cro_{context}"]
=== Cluster Resource Override Admission Operator

[id="ocp-release-notes-documentation_{context}"]
=== Documentation

[id="ocp-release-notes-documentation-etcd_{context}"]
==== Consolidated etcd documentation

This release includes an _etcd_ section, which consolidates all of the existing documentation about etcd for {product-title}. For more information, see xref:../etcd/etcd-overview.adoc#etc-overview[Overview of etcd].

[id="ocp-release-notes-documentation-tutorials_{context}"]
==== Tutorials guide

{product-title} 4.19 now includes a _Tutorials_ guide, which takes the place of the _Getting started_ guide in previous releases. The existing tutorials were refreshed and the guide now focuses solely on hands-on tutorial content. It also provides a jumping off point to other recommended hands-on learning resources for {product-title} across Red{nbsp}Hat.

For more information, see xref:../tutorials/index.adoc#tutorials-overview[Tutorials].

[id="ocp-release-notes-edge-computing_{context}"]
=== Edge computing

[id="ocp-4-19-edge-computing_pg-ztp-rhacm_{context}"]
==== Using {rh-rhacm} PolicyGenerator resources to manage {ztp} cluster policies (General Availability)

You can now use `PolicyGenerator` resources and {rh-rhacm-first} to deploy polices for managed clusters with {ztp}.
The `PolicyGenerator` API is part of the link:https://open-cluster-management.io/[Open Cluster Management] standard and provides a generic way of patching resources, which is not possible with the `PolicyGenTemplate` API.
Using `PolicyGenTemplate` resources to manage and deploy polices will be deprecated in an upcoming {product-title} release.

For more information, see xref:../edge_computing/policygenerator_for_ztp/ztp-configuring-managed-clusters-policygenerator.adoc#ztp-configuring-managed-clusters-policygenerator[Configuring managed cluster policies by using PolicyGenerator resources].

[id="ocp-release-notes-extensions_{context}"]
=== Extensions ({olmv1})

[id="ocp-release-notes-olmv1-preflight-permissions-check_{context}"]
==== Preflight permissions check for cluster extensions (Technology Preview)

With this release, the Operator Controller performs a dry run of the installation process when you try to install an extension. This dry run verifies that the specified service account has the required role-based access control (RBAC) rules for the roles and bindings defined by the bundle.

If the service account is missing any required RBAC rules, the preflight check fails before the actual installation proceeds and generates a report.

For more information, see xref:../extensions/ce/managing-ce.adoc#olmv1-troubleshooting-rbac-errors-with-preflight-check_managing-ce[Preflight permissions check for cluster extensions (Technology Preview)]

[id="ocp-release-notes-olmv1-deploying-a-cluster-extension-to-a-specific-namespace_{context}"]
==== Deploying a cluster extension in a specific namespace (Technology Preview)

With this release, you can deploy an extension in a specific namespace by using the `OwnNamespace` or `SingleNamespace` install modes as a Technology Preview feature for `registry+v1` Operator bundles.

For more information, see xref:../extensions/ce/managing-ce.adoc#olmv1-deploying-a-ce-in-a-specific-namespace_managing-ce[Deploying a cluster extension in a specific namespace (Technology Preview)]

[id="ocp-release-notes-hcp_{context}"]
=== Hosted control planes

Because {hcp} releases asynchronously from {product-title}, it has its own release notes. For more information, see xref:../hosted_control_planes/hosted-control-planes-release-notes.adoc#hosted-control-planes-release-notes[{hcp-capital} release notes].

[id="ocp-release-notes-ibm-power_{context}"]
=== {ibm-power-title}

The {ibm-power-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on {ibm-power-title}:

* Expand Compliance Operator support with profiles for Defense Information Systems Agency Security Technical Implementation Guide (DISA STIG)

[id="ocp-release-notes-ibm-z_{context}"]
=== {ibm-z-title} and {ibm-linuxone-title}

The {ibm-z-name} and {ibm-linuxone-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on {ibm-z-name} and {ibm-linuxone-name}:

* Support for {ibm-name} z17 and {ibm-linuxone-name} 5
* Boot volume Linux Unified Key Setup (LUKS) encryption via {ibm-name} Crypto Express (CEX)

[discrete]
[id="ocp-release-notes-ibm-z-power-support-matrix_{context}"]
=== {ibm-power-title}, {ibm-z-title}, and {ibm-linuxone-title} support matrix

Starting in {product-title} 4.14, Extended Update Support (EUS) is extended to the {ibm-power-name} and the {ibm-z-name} platform. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

.CSI Volumes
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Cloning
|Supported
|Supported

|Expansion
|Supported
|Supported

|Snapshot
|Supported
|Supported
|====

.Multus CNI plugins
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Bridge
|Supported
|Supported

|Host-device
|Supported
|Supported

|IPAM
|Supported
|Supported

|IPVLAN
|Supported
|Supported
|====

.{product-title} features
[cols="3,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Adding compute nodes to on-premise clusters using {oc-first}
|Supported
|Supported

|Alternate authentication providers
|Supported
|Supported

|Agent-based Installer
|Supported
|Supported

|Assisted Installer
|Supported
|Supported

|Automatic Device Discovery with Local Storage Operator
|Unsupported
|Supported

|Automatic repair of damaged machines with machine health checking
|Unsupported
|Unsupported

|Cloud controller manager for {ibm-cloud-name}
|Supported
|Unsupported

|Controlling overcommit and managing container density on nodes
|Unsupported
|Unsupported

|CPU manager
|Supported
|Supported

|Cron jobs
|Supported
|Supported

|Descheduler
|Supported
|Supported

|Egress IP
|Supported
|Supported

|Encrypting data stored in etcd
|Supported
|Supported

|FIPS cryptography
|Supported
|Supported

|Helm
|Supported
|Supported

|Horizontal pod autoscaling
|Supported
|Supported

|Hosted control planes
|Supported
|Supported

|IBM Secure Execution
|Unsupported
|Supported

|Installer-provisioned Infrastructure Enablement for {ibm-power-server-name}
|Supported
|Unsupported

|Installing on a single node
|Supported
|Supported

|IPv6
|Supported
|Supported

|Monitoring for user-defined projects
|Supported
|Supported

|Multi-architecture compute nodes
|Supported
|Supported

|Multi-architecture control plane
|Supported
|Supported

|Multipathing
|Supported
|Supported

|Network-Bound Disk Encryption - External Tang Server
|Supported
|Supported

|Non-volatile memory express drives (NVMe)
|Supported
|Unsupported

|nx-gzip for Power10 (Hardware Acceleration)
|Supported
|Unsupported

|oc-mirror plugin
|Supported
|Supported

|OpenShift CLI (`oc`) plugins
|Supported
|Supported

|Operator API
|Supported
|Supported

|OpenShift Virtualization
|Unsupported
|Supported

|OVN-Kubernetes, including IPsec encryption
|Supported
|Supported

|PodDisruptionBudget
|Supported
|Supported

|Precision Time Protocol (PTP) hardware
|Unsupported
|Unsupported

|{openshift-local-productname}
|Unsupported
|Unsupported

|Scheduler profiles
|Supported
|Supported

|Secure Boot
|Unsupported
|Supported

|Stream Control Transmission Protocol (SCTP)
|Supported
|Supported

|Support for multiple network interfaces
|Supported
|Supported

|The `openshift-install` utility to support various SMT levels on {ibm-power-name} (Hardware Acceleration)
|Supported
|Unsupported

|Three-node cluster support
|Supported
|Supported

|Topology Manager
|Supported
|Unsupported

|z/VM Emulated FBA devices on SCSI disks
|Unsupported
|Supported

|4K FCP block device
|Supported
|Supported
|====

.Operators
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|{cert-manager-operator}
|Supported
|Supported

|Cluster Logging Operator
|Supported
|Supported

|Cluster Resource Override Operator
|Supported
|Supported

|Compliance Operator
|Supported
|Supported

|Cost Management Metrics Operator
|Supported
|Supported

|File Integrity Operator
|Supported
|Supported

|HyperShift Operator
|Supported
|Supported

|{ibm-power-server-name} Block CSI Driver Operator
|Supported
|Unsupported

|Ingress Node Firewall Operator
|Supported
|Supported

|Local Storage Operator
|Supported
|Supported

|MetalLB Operator
|Supported
|Supported

|Network Observability Operator
|Supported
|Supported

|NFD Operator
|Supported
|Supported

|NMState Operator
|Supported
|Supported

|OpenShift Elasticsearch Operator
|Supported
|Supported

|Vertical Pod Autoscaler Operator
|Supported
|Supported
|====

.Persistent storage options
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}
|Persistent storage using iSCSI
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using local volumes (LSO)
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using hostPath
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Fibre Channel
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Raw Block
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using EDEV/FBA
|Supported ^[1]^
|Supported ^[1]^,^[2]^
|====
[.small]
--
1. Persistent shared storage must be provisioned by using either {rh-storage-first} or other supported storage protocols.
2. Persistent non-shared storage must be provisioned by using local storage, such as iSCSI, FC, or by using LSO with DASD, FCP, or EDEV/FBA.
--

[id="ocp-release-notes-insights-operator-enhancements_{context}"]
=== Insights Operator


[id="ocp-release-notes-insights-operator-runtime-extractor_{context}"]
==== Insights Runtime Extractor is generally available

In {product-title} 4.18, the Insights Operator introduced the _Insights Runtime Extractor_ workload data collection feature as a Technology Preview feature to help Red{nbsp}Hat better understand the workload of your containers.
Now, in version 4.19, the feature is generally available.
The Insights Runtime Extractor feature gathers runtime workload data and sends it to Red{nbsp}Hat.

[id="ocp-release-notes-installation-and-update_{context}"]
=== Installation and update

[id="ocp-4-19-installation-and-update-remove-terraform-ibm-cloud_{context}"]
==== Cluster API replaces Terraform on {ibm-cloud-title} installations

In {product-title} {product-version}, the installation program uses the Cluster API instead of Terraform to provision cluster infrastructure during installations on {ibm-cloud-title}.

[id="ocp-4-19-installation-and-update-aws-malaysia-thailand_{context}"]
==== Installing a cluster on {aws-short} in the Malaysia and Thailand regions

You can now install an {product-title} cluster on {aws-first} in the Malaysia (`ap-southeast-5`) and Thailand (`ap-southeast-7`) regions.

For more information, see xref:../installing/installing_aws/installing-aws-account.adoc#installation-aws-regions_installing-aws-account[Supported {aws-first} regions].

[id="ocp-4-19-installation-and-update-remove-terraform-azure-stack-hub_{context}"]
==== Cluster API replaces Terraform on {azure-first} Stack Hub installations

In {product-title} {product-version}, the installation program uses the Cluster API instead of Terraform to provision clusters during installer-provisioned infrastructure installations on {azure-first} Stack Hub.

[id="ocp-4-19-installation-and-update-support-azure-instance-types_{context}"]
==== Support added for additional {azure-first} instance types

Additional {azure-first} instance types for machine types based on 64-bit x86 architecture have been tested with {product-title} {product-version}.

For the Dxv6 machine series, the following instance types have been tested:

* `StandardDdsv6Family`
* `StandardDldsv6Family`
* `StandardDlsv6Family`
* `StandardDsv6Family`

For the Lsv4 and Lasv4 machine series, the following instance types have been tested:

* `standardLasv4Family`
* `standardLsv4Family`

For the ND and NV machine series, the following instance types have been tested:

* `StandardNVadsV710v5Family`
* `Standard NDASv4_A100 Family`

For more information, see xref:../installing/installing_azure/ipi/installing-azure-network-customizations.adoc#installation-azure-tested-machine-types_installing-azure-network-customizations[Tested instance types for {azure-short}] and link:https://learn.microsoft.com/en-us/azure/?product=popular[{azure-short} documentation] (Microsoft documentation).

[id="ocp-4-19-installation-and-update-azure-outbound-access-vms_{context}"]
==== Outbound access for VMs in {azure-first} will be retired

On 30 September 2025, the default outbound access connectivity for all new virtual machines (VMs) in {azure-first} will be retired. To enhance security, {azure-short} is moving towards a secure-by-default model where default outbound access to the internet will be turned off. However, configuration changes to {product-title} are not required. By default, the installation program creates an outbound rule for the load balancer.

For more information, see link:https://azure.microsoft.com/en-us/updates?id=default-outbound-access-for-vms-in-azure-will-be-retired-updates-and-more-information[Azure Updates] (Microsoft documentation), link:https://learn.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections#scenarios[Azure's outbound connectivity methods] (Microsoft documentation), and xref:../installing/installing_azure/upi/installing-azure-preparing-upi.adoc#installing-azure-preparing-upi[Preparing to install a cluster on {azure-short}].

[id="ocp-4-19-installation-and-update-gcp-confidential-computing-expansion_{context}"]
==== Additional Confidential Computing platforms for {gcp-short}
With this release, you can use additional Confidential Computing platforms on {gcp-short}. The new supported platforms, which can be enabled in the `install-config.yaml` file prior to installation, or configured after installation using machine sets and control plane machine sets, are as follows:

* `AMDEncryptedVirtualization`, which enables Confidential Computing with AMD Secure Encrypted Virtualization (AMD SEV)
* `AMDEncryptedVirtualizationNestedPaging`, which enables Confidential Computing with AMD Secure Encrypted Virtualization Secure Nested Paging (AMD SEV-SNP)
* `IntelTrustedDomainExtensions`, which enables Confidential Computing with Intel Trusted Domain Extensions (Intel TDX)

For more information, see xref:../installing/installing_gcp/installation-config-parameters-gcp.adoc#installation-config-parameters-additional-gcp_installation-config-parameters-gcp[Installation configuration parameters for {gcp-full}], xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-gcp.adoc#machineset-gcp-confidential-vm_cpmso-config-options-gcp[Configuring Confidential VM by using machine sets (control plane)], and xref:../machine_management/creating_machinesets/creating-machineset-gcp.html#machineset-gcp-confidential-vm_creating-machineset-gcp[Configuring Confidential VM by using machine sets (compute)].

[id="ocp-4-19-GCP-custom-dns_{context}"]
==== Installing a cluster on {gcp-first} with a user-provisioned DNS (Technology Preview)

With this release, you can enable a user-provisioned domain name server (DNS) instead of the default cluster-provisioned DNS solution. For example, your organization's security policies might not allow the use of public DNS services such as Google Cloud DNS. You can manage your DNS only for the IP addresses of the API and Ingress servers. If you use this feature, you must provide your own DNS solution that includes records for `api.<cluster_name>.<base_domain>.` and `*.apps.<cluster_name>.<base_domain>.`. Enabling a user-provisioned DNS is available as a Technology Preview feature.

For more information, see xref:../installing/installing_gcp/installing-gcp-customizations.adoc#installation-gcp-enabling-user-managed-DNS_installing-gcp-customizations[Enabling user-managed DNS].

[id="ocp-4-19-installation-and-update-vsphere-multidisk_{context}"]
==== Installing a cluster on {vmw-first} with multiple disks (Technology Preview)
With this release, you can install a cluster on {vmw-first} with multiple storage disks as a Technology Preview feature. You can assign these additional disks to special functions within the cluster, such as etcd storage.

For more information, see xref:../installing/installing_vsphere/installation-config-parameters-vsphere.adoc#installation-configuration-parameters-optional-vsphere_installation-config-parameters-vsphere[Optional vSphere configuration parameters].

[id="ocp-4-19-installation-and-update-azure-boot-diagnostics_{context}"]
==== Enabling boot diagnostics collection during installation on {azure-first}

With this release, you can enable boot diagnostics collection when you install a cluster on {azure-first}. Boot diagnostics is a debugging feature for {azure-short} virtual machines (VMs) to identify VM boot failures. You can set the `bootDiagnostics` parameter in the `install-config.yaml` file for compute machines, for control plane machines, or for all machines.

For more information, see xref:../installing/installing_azure/installation-config-parameters-azure.adoc#installation-configuration-parameters-additional-azure_installation-config-parameters-azure[Additional {azure-short} configuration parameters].

[id="ocp-4-19-admin-ack-updating_{context}"]
==== Required administrator acknowledgment when updating from {product-title} 4.18 to 4.19

{product-title} 4.19 uses Kubernetes 1.32, which removed several xref:../release_notes/ocp-4-19-release-notes.adoc#ocp-4-19-removed-kube-1-32-apis_{context}[deprecated APIs].

A cluster administrator must provide manual acknowledgment before the cluster can be updated from {product-title} 4.18 to 4.19. This is to help prevent issues after updating to {product-title} 4.19, where APIs that have been removed are still in use by workloads, tools, or other components running on or interacting with the cluster. Administrators must evaluate their cluster for any APIs in use that will be removed and migrate the affected components to use the appropriate new API version. After this is done, the administrator can provide the administrator acknowledgment.

All {product-title} 4.18 clusters require this administrator acknowledgment before they can be updated to {product-title} 4.19.

For more information, see xref:../updating/preparing_for_updates/updating-cluster-prepare.adoc#updating-cluster-prepare[Preparing to update to {product-title} 4.19].

[id="ocp-4-19-vsphere-host-groups_{context}"]
==== OpenShift zones support for vSphere host groups (Technology Preview)

With this release, you can map {product-title} failure domains to {vmw-full} host groups. This enables you to make use of the high availability offered by a {vmw-short} stretched cluster configuration. This feature is available as a Technology Preview in {product-title} {product-version}.

For information on configuring host groups at installation, see xref:../installing/installing_vsphere/ipi/installing-vsphere-installer-provisioned-customizations.adoc#installation-vsphere-regions-zones-host-groups_installing-vsphere-installer-provisioned-customizations[VMware vSphere host group enablement].

For information on configuring host groups for existing clusters, see xref:../installing/installing_vsphere/post-install-vsphere-zones-regions-configuration.adoc#specifying-host-groups-vsphere_post-install-vsphere-zones-regions-configuration[Specifying multiple host groups for your cluster on vSphere].

[id="ocp-release-notes-agent-nutanix_{context}"]
==== Nutanix support for the Agent-based Installer
With this release, you can now use the Agent-based Installer to install a cluster on Nutanix.
Installing a cluster on Nutanix with the Agent-based Installer is enabled by setting the `platform` parameter to `nutanix` in the `install-config.yaml` file.

For more information, see xref:../installing/installing_with_agent_based_installer/installation-config-parameters-agent.adoc#installation-configuration-parameters-required_installation-config-parameters-agent[Required configuration parameters] in the Agent-based Installer documentation.

[id="ocp-release-notes-machine-config-operator_{context}"]
=== Machine Config Operator

[id="ocp-release-notes-machine-config-operator-naming_{context}"]
==== New naming for features

_{op-system-first} image layering_ is now called _image mode for OpenShift_. As a part of this change, _on-cluster layering_ is now  called _on-cluster image mode_ and _out-of-cluster layering_ is now _out-of-cluster image mode_.

The _updated boot images_ feature is now called _boot image management_.

[id="ocp-release-notes-machine-config-operator-ocl-ga_{context}"]
==== Image mode for OpenShift is now generally available

Image mode for OpenShift, formerly called on-cluster layering, is now Generally Available (GA). The following changes have been introduced with the promotion to GA:

* The API version is now `machineconfiguration.openshift.io/v1`. The new version includes the following changes:
** The `baseImagePullSecret` parameter is now optional. If not specified, the default `global-pull-secret-copy` is used.
** The `buildInputs` parameter is no longer required. All parameters previously under the `buildInputs` parameter are promoted one level.
** The `containerfileArch` parameter now supports multiple architectures. Previously, only `noarch` was supported.
** The required `imageBuilderType` is now `Job`. Previously, the required builder was `PodImageBuilder`.
** The `renderedImagePushspec` parameter is now `renderedImagePushSpec`.
** The `buildOutputs` and `currentImagePullSecret` parameters are no longer required.

* The output of the `oc describe MachineOSConfig` and `oc describe MachineOSBuild` commands have multiple differences.

* The `global-pull-secret-copy` is automatically added to the `openshift-machine-config-operator` namespace.

* You can now revert an on-cluster custom layered image back to the base image by removing a label from the `MachineOSConfig` object

* You can now automatically delete an on-cluster custom layered image by deleting the associated `MachineOSBuild` object.

* The `must-gather` for the Machine Config Operator now includes data on the `MachineOSConfig` and `MachineOSBuild` objects.

* On-cluster layering is now supported in disconnected environments.

* On-cluster layering is now supported in single node OpenShift (SNO) clusters.

[id="ocp-release-notes-machine-config-operator-boot-image_{context}"]
==== Boot image management is now default for {gcp-first} and {aws-first}

The boot image management feature, previously called updated boot images, is now the default behavior in {gcp-first} and {aws-first} clusters. As such, after updating to {product-title} {product-version}, the boot images in your cluster are automatically updated to version {product-version}. With subsequent updates, the Machine Config Operator (MCO) again updates the boot images in your cluster. A boot images is associated with a machine set and is used when scaling new nodes. Any new nodes you create after updating are based on the new version. Current nodes are not affected by this feature.

Before upgrading to {product-version}, you must opt-out of this default behavior or acknowledge this change before proceeding. For more information, see xref:../machine_configuration/mco-update-boot-images.adoc#mco-update-boot-images-disable_machine-configs-configure[Disabling boot image management].

[NOTE]
====
The managed boot images feature is available for only {gcp} and {aws} clusters. For all other platforms, the MCO does not update the boot image with each cluster update.
====
[id="ocp-release-notes-machine-config-operator-cert-changes_{context}"]
==== Changes to the Machine Config Operator certificates
The Machine Config Server (MCS) CA bundle created by the installation program is now stored in the `machine-config-server-ca` config map in the `openshift-machine-config-operator` namespace. The bundle was previously stored in the `root-ca` configmap in the `kube-system namespace`. The `root-ca` configmap is no longer used in a cluster that cluster that is updated to {product-title} {product-version}. This change was made to make it clear that this CA bundle is managed by the Machine Config Operator (MCO).

The MCS signing key is stored in the `machine-config-server-ca` secret in the `openshift-machine-config-operator` namespace.

The MCS CA and MCS cert are valid for 10 years and are automatically rotated by the MCO at approximately 8 years. Upon update to {product-title} {product-version}, the CA signing key is not present. As a result, the CA bundle is immediately considered expired when the MCO certificate controller comes up. This expiration causes an immediate certificate rotation, even if the cluster is not 10 years old. After that point, the next rotation takes place at the standard 8 year period.

For more information about the MCO certificates, see xref:../security/certificate_types_descriptions/machine-config-operator-certificates.adoc#cert-types-machine-config-operator-certificates[Machine Config Operator certificates].

[id="ocp-release-notes-management-console_{context}"]
=== Management console

[id="ocp-release-notes-machine-management_{context}"]
=== Machine management

[id="ocp-4-19-cpms-prefix_{context}"]
==== Custom prefixes for control plane machine names

With this release, you can customize the prefix of machine names for machines created by the control plane machine set.
This feature is enabled by modifying the `spec.machineNamePrefix` parameter of the `ControlPlaneMachineSet` custom resource.

For more information, see xref:../machine_management/control_plane_machine_management/cpmso-configuration.adoc#cpmso-config-prefix_cpmso-configuration[Adding a custom prefix to control plane machine names].

[id="ocp-4-19-aws-capacity-reservations_{context}"]
==== Configuring Capacity Reservations on {aws-full} clusters

With this release, you can deploy machines that use Capacity Reservations, including On-Demand Capacity Reservations and Capacity Blocks for ML, on {aws-full} clusters.

You can configure these features with xref:../machine_management/creating_machinesets/creating-machineset-aws.adoc#machineset-capacity-reservation_creating-machineset-aws[compute] and xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-aws.adoc#machineset-capacity-reservation_cpmso-config-options-aws[control plane] machine sets.

[id="ocp-4-19-vmw-multi-disk_{context}"]
==== Support for multiple {vmw-full} data disks (Technology Preview)

With this release, you can add up to 29 disks to the virtual machine (VM) controller for your {vmw-short} cluster as a Technology Preview feature.
This capability is available for xref:../machine_management/creating_machinesets/creating-machineset-vsphere.adoc#machineset-vsphere-data-disks_creating-machineset-vsphere[compute] and xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-vsphere.adoc#machineset-vsphere-data-disks_cpmso-config-options-vsphere[control plane] machine sets.

[id="ocp-release-notes-monitoring_{context}"]
=== Monitoring

The in-cluster monitoring stack for this release includes the following new and modified features:

[id="ocp-4-19-monitoring-updates-to-monitoring-stack-components-and-dependencies"]
==== Updates to monitoring stack components and dependencies

This release includes the following version updates for in-cluster monitoring stack components and dependencies:

* Alertmanager to 0.28.1
* Prometheus to 3.2.1
* Prometheus Operator to 0.81.0
* Thanos to 0.37.2
* kube-state-metrics to 2.15.0
* node-exporter to 1.9.1

[id="ocp-4-19-monitoring-changes-to-alerting-rules"]
==== Changes to alerting rules

[NOTE]
====
Red{nbsp}Hat does not guarantee backward compatibility for recording rules or alerting rules.
====

* Added the `PrometheusPossibleNarrowSelectors` alert to warn users when PromQL queries or metric relabel configurations use selectors that could be too restrictive and might not take into account that values on the `le` label of classic histograms or the `quantile` label of summaries are floats in Prometheus v3. For more information, see the "Prometheus v3 upgrade" section.

[id="ocp-4-19-monitoring-prometheus-v3-upgrade"]
==== Prometheus v3 upgrade

This release introduces a major update to the Prometheus component, transitioning from v2 to v3. The monitoring stack and other core components include all of the necessary adjustments to ensure a smooth upgrade. However, some user-managed configurations might require modifications. The key changes include the following items:

* The values of the `le` label for classic histograms and the `quantile` label for summaries are normalized during ingestion. For example, the `example_bucket{le="10"}` metric selector is ingested as `example_bucket{le="10.0"}`. As a result, alerts, recording rules, dashboards, and relabeling configurations that reference label values as whole numbers, for example, `le="10"`, might no longer work as intended.
+
To mitigate the issue, update your selectors:

** If your queries need to cover data from both before and after the Prometheus upgrade, ensure both values are considered, for example, use a regular expression, `example_bucket{le=~"10(.0)?"}`.

** For queries that only cover data after the upgrade, use float values, for example, `le="10.0"`.

* Configurations that send alerts to additional Alertmanager instances through `additionalAlertmanagerConfigs` by using the Alertmanager v1 API are no longer supported.
+
To mitigate the issue, upgrade any affected Alertmanager instances to support the v2 API, which is supported since Alertmanager `v0.16.0`, and update your monitoring configuration to use the v2 scheme.

For more information about the changes between Prometheus v2 and v3, see link:https://prometheus.io/docs/prometheus/latest/migration/[Prometheus 3.0 migration guide].

[id="ocp-4-19-monitoring-metrics-collection-profiles-ga"]
==== Metrics collection profiles is generally available

{product-title} 4.13 introduced the ability to set a metrics collection profile for default platform monitoring to collect either the default amount of metrics data or a minimal amount of metrics data. In {product-title} {product-version}, metrics collection profiles are now generally available.

For more information, see xref:../observability/monitoring/about-ocp-monitoring/key-concepts.adoc#configuring-metrics-collection-profiles_key-concepts[About metrics collection profiles] and xref:../observability/monitoring/configuring-core-platform-monitoring/configuring-performance-and-scalability.adoc#choosing-a-metrics-collection-profile_configuring-performance-and-scalability[Choosing a metrics collection profile].

[id="ocp-4-19-monitoring-added-cluster-proxy-support-for-external-alertmanager-instances"]
==== Added cluster proxy support for external Alertmanager instances

With this release, external Alertmanager instances now use the cluster-wide HTTP proxy settings for communication. The {cmo-first} reads the cluster-wide proxy settings and configures the appropriate proxy URL for the Alertmanager endpoints.

[id="ocp-4-19-monitoring-strict-validation-for-cmo-is-improved"]
==== Strict validation for the {cmo-full} is improved

With this release, the strict validation introduced in {product-title} 4.18 is improved. Error messages now clearly identify the affected field, and validation is case-sensitive to ensure more accurate and consistent configuration.

For more information, see (link:https://issues.redhat.com/browse/OCPBUGS-42671[OCPBUGS-42671]) and (link:https://issues.redhat.com/browse/OCPBUGS-54516[OCPBUGS-54516]).

[id="ocp-release-notes-network-observability-operator_{context}"]
=== Network Observability Operator

[id="ocp-release-notes-networking_{context}"]
=== Networking

[id="ocp-4-19-networking-support-load-secrets_{context}"]
==== Creating a route with externally managed certificate (General Availability)

With this release, {product-title} routes can be configured with third-party certificate management solutions, utilising the `.spec.tls.externalCertificate` field in the route API. This allows you to reference externally managed TLS certificates through secrets, streamlining the process by eliminating manual certificate management. By using externally managed certificates, you reduce errors, ensure a smoother certificate update process, and enable the OpenShift router to promptly serve renewed certificates. For more information, see xref:../networking/routes/secured-routes.adoc#nw-ingress-route-secret-load-external-cert_secured-routes[Creating a route with externally managed certificate].

[id="ocp-4-19-networking-gateway-api-controller_{context}"]
==== Support for using Gateway API to configure cluster ingress traffic (General Availability)
With this release, support for managing ingress cluster traffic using Gateway API resources is Generally Available. Gateway API provides a robust networking solution within the transport layer, L4, and the application layer, L7, for {product-title} clusters using a standardized open source ecosystem.

For more information, see xref:../networking/configuring_ingress_cluster_traffic/ingress-gateway-api.adoc#ingress-gateway-api[Gateway API with {product-title} networking].

[IMPORTANT]
====
Gateway API resources must conform to the supported {product-title} API surface. This means you cannot use another vendor-specific resource, such as Istio's VirtualService, with {product-title}'s implementation of Gateway API. For more information, see xref:../networking/configuring_ingress_cluster_traffic/ingress-gateway-api.adoc#nw-ingress-gateway-api-implementation[Gateway API implementation for {product-title}].
====

[id="ocp-4-19-networking-gateway-api-crd-lifecycle_{context}"]
==== Support for managing Gateway API custom resource definition (CRD) lifecycle
With this release, {product-title} manages the lifecycle of Gateway API CRDs. This means that the Ingress Operator handles the required versioning and management of resources. Any Gateway API resources created in a previous {product-title} version must be re-created and redeployed so that it conforms to the specifications required by the Ingress Operator.

For more information, see xref:../updating/preparing_for_updates/updating-cluster-prepare.adoc#nw-ingress-gateway-api-manage-succession[Preparing for Gateway API management succession by the Ingress Operator].

[id="ocp-4-19-networking-gateway-api-ossm-version-bump_{context}"]
==== Updates to Gateway API custom resource definitions (CRDs)
{product-title} {product-version} updates {SMProductName} to version 3.0.2, and Gateway API to version 1.2.1. See the link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_mesh/3.0/html/release_notes/ossm-release-notes[{SMProductShortName} 3.0.0 release notes] and the link:https://github.com/kubernetes-sigs/gateway-api/blob/main/CHANGELOG/1.2-CHANGELOG.md#v121[Gateway API 1.2.1 changelog] for more information.

[id="ocp-4-19-networking-balance-slb-mode_{context}"]
==== Enable OVS balance-slb mode for your cluster (General Availability)

You can enable the Open vSwitch (OVS) `balance-slb` mode on infrastructure where your cluster runs so that two or more physical interfaces can share their network traffic. For more information, see xref:../installing/installing_bare_metal/ipi/ipi-install-installation-workflow.adoc#enabling-OVS-balance-slb-mode_ipi-install-installation-workflow[Enabling OVS balance-slb mode for your cluster].

[id="ocp-4-19-networking-ptp-dual-oc_{context}"]
==== Dual-port NICs for improved redundancy in PTP ordinary clocks (Technology Preview)
With this release, you can use a dual-port network interface controller (NIC) to improve redundancy for Precision Time Protocol (PTP) ordinary clocks.
Available as a Technology Preview, in a dual-port NIC configuration for an ordinary clock, if one port fails, the standby port takes over, maintaining PTP timing synchronization.

[NOTE]
====
You can configure PTP ordinary clocks with added redundancy on `x86` architecture nodes with dual-port NICs only.
====

For more information, see xref:../networking/ptp/about-ptp.adoc#ptp-dual-ports-oc_about-ptp[Using dual-port NICs to improve redundancy for PTP ordinary clocks].

[id="ocp-4-19-dpu-device-management-with-dpu-operator_{context}"]
==== Enabling DPU device management with the DPU Operator
With this release, {product-title} introduces the Data Processing Unit (DPU) Operator, enabling management of DPU devices. The DPU Operator manages components on compute nodes with DPUs, enabling the offloading of data workloads such as networking, storage, and security. This leads to improved cluster performance, reduced latency, and enhanced security, contributing to a more efficient infrastructure. For more information, see xref:../networking/networking_operators/dpu-operator/about-dpu.adoc#about-dpu-and-dpu-operator[About DPU and the DPU Operator].

[id="ocp-4-19-cluster-user-defined-networks-localnet_{context}"]
==== Localnet topology for user-defined networks (Generally Available)

Administrators can now use the `ClusterUserDefinedNetwork` custom resource to deploy secondary networks on a `Localnet` topology. This feature allows pods and virtual machines connected to the localnet network to egress to the physical network. For more information, see xref:../networking/multiple_networks/primary_networks/about-user-defined-networks.adoc#nw-cudn-localnet[Creating a ClusterUserDefinedNetwork CR for a Localnet topology].

[id="ocp-4-19-port-isolation-linux-bridge_{context}"]
==== Enable port isolation for a Linux bridge NAD (Generally Available)

You can enable port isolation for a Linux bridge network attachment definition (NAD) so that virtual machines (VMs) or pods that run on the same virtual LAN (VLAN) can operate in isolation from one another. For more information, see xref:../virt/vm_networking/virt-connecting-vm-to-linux-bridge.adoc#virt-linux-bridge-nad-port-isolation_virt-connecting-vm-to-linux-bridge[Enabling port isolation for a Linux bridge NAD].

[id="ocp-4-19-whereabouts-ipam_{context}"]
==== Fast IPAM configuration for the Whereabouts IPAM CNI plugin (Technology Preview)

To improve the performance of Whereabouts, especially if nodes in your cluster run a high amount of pods, you can now enable the Fast IP Address Management (IPAM) feature. The Fast IPAM feature uses `nodeslicepools`, which are managed by the Whereabouts Controller, to optimize IP address allocation for nodes. For more information, see xref:../networking/multiple_networks/secondary_networks/configuring-ip-secondary-nwt.adoc#nw-multus-whereabouts-fast-ipam_configuring-additional-network[Fast IPAM configuration for the Whereabouts IPAM CNI plugin].
[id="ocp-4-19-metallb-unnumbered-bgp-peering_{context}"]

[id="ocp-4-19-unnumbered-bgp-peering_{context}"]
==== Unnumbered BGP peering (Technology Preview)

With this release, {product-title} introduces unnumbered BGP peering.
Available as a Technology Preview feature, you can use the `spec.interface` field of the BGP peer custom resource to configure unnumbered BGP peering.

[id="ocp-4-19-custom-dns-host-name-disconnected_{context}"]
==== Create a custom DNS host name to resolve DNS connectivity issues

In a disconnected environment where the external DNS server cannot be reached, you can resolve Kubernetes NMState Operator health probe issues by specifying a custom DNS host name in the `NMState` custom resource definition (CRD). For more information, see xref:../networking/k8s_nmstate/k8s-nmstate-troubleshooting-node-network.adoc#k8s-nmstate-troubleshooting-dns-disconnected-env-resolv_k8s-nmstate-troubleshooting-node-network[Creating a custom DNS host name to resolve DNS connectivity issues].

[id="ocp-4-19-ptp-fast-events-rest-api-v2_{context}"]
==== Removal of PTP events REST API v1 and events consumer application sidecar
With this release, the PTP events REST API v1 and events consumer application sidecar support are removed.

You must use the O-RAN compliant PTP events REST API v2 instead.

For more information, see xref:../networking/ptp/ptp-cloud-events-consumer-dev-reference-v2.adoc#ptp-cloud-events-consumer-dev-reference-v2[Developing PTP event consumer applications with the REST API v2].

[id="ocp-release-notes-nodes_{context}"]
=== Nodes

[id="ocp-release-notes-machine-config-operator-cgroup-v1_{context}"]
==== cgroup v1 has been removed

cgroup v1, which was deprecated in {product-title} 4.16, is no longer supported and has been removed from {product-title}. If your cluster is using cgroup v1, you must configure cgroup v2 before you can upgrade to  {product-title} {product-version}. All workloads must now be compatible with cgroup v2.

For more information on cgroup v2, see xref:../architecture/index.adoc#architecture-about-cgroup-v2_architecture-overview[About Linux cgroup version 2] and link:https://www.redhat.com/en/blog/rhel-9-changes-context-red-hat-openshift-workloads[Red Hat Enterprise Linux 9 changes in the context of Red Hat OpenShift workloads] (Red{nbsp}Hat blog).

[id="ocp-release-notes-openshift-cli_{context}"]
=== OpenShift CLI (oc)

[id="ocp-release-notes-osdk_{context}"]
=== Operator development

[id="ocp-release-notes-osdk-base-images_{context}"]
==== Supported Operator base images

The following base images for Operator projects are updated for compatibility with {product-title} {product-version}. The runtime functionality and configuration APIs for these base images are supported for bug fixes and for addressing CVEs.

* The base image for Ansible-based Operator projects
* The base image for Helm-based Operator projects

For more information, see link:https://access.redhat.com/node/7123119[Updating the base image for existing Ansible- or Helm-based Operator projects for {product-title} 4.19 and later] (Red{nbsp}Hat Knowledgebase).

// NOTE: The KCS article link will be published on the GA date. It is a draft right now.

[id="ocp-release-notes-oci_{context}"]
=== {oci-first}

[id="ocp-release-notes-postinstallation-configuration_{context}"]
=== Postinstallation configuration

[id="ocp-release-notes-rhcos_{context}"]
=== {op-system-first}

[id="ocp-4-19-rhcos-rhel-9-6_{context}"]
==== {op-system} uses {op-system-base} 9.6
{op-system} uses {op-system-base-full} 9.6 packages in {product-title} {product-version}. These packages ensure that your {product-title} instances receive the latest fixes, features, enhancements, hardware support, and driver updates.

[id="ocp-release-notes-registry_{context}"]
=== Registry

[id="ocp-release-notes-scalability-and-performance_{context}"]
=== Scalability and performance

[id="ocp-release-notes-scalability-and-performance_kernelpagesize_{context}"]
==== Performance profile kernel page size configuration

With this update, you can specify larger kernel page sizes to improve performance for memory-intensive, high-performance workloads on ARM infrastructure nodes with the realtime kernel disabled. For more information, see xref:../scalability_and_performance/cnf-tuning-low-latency-nodes-with-perf-profile.adoc#cnf-configuring-kernal-page-size_cnf-low-latency-perf-profile[Configuring kernel page sizes].

[id="ocp-release-notes-tuning-hcp-performance-profile_{context}"]
==== Tuning {hcp} using a performance profile

With this update, you can now tune nodes in {hcp} for low latency by applying a performance profile. For more information, see xref:../scalability_and_performance/cnf-tuning-low-latency-hosted-cp-nodes-with-perf-profile.adoc#cnf-create-performance-profiles-hosted-cp[Creating a performance profile for hosted control planes].

[id="ocp-release-notes-security_{context}"]
=== Security

[id="ocp-release-notes-tls-modern-profile-control-plane_{context}"]
==== Control plane now supports TLS 1.3 and the Modern TLS security profile

With this release, the control plane supports TLS 1.3. You can now use the `Modern` TLS security profile for the control plane.

For more information, see xref:../security/tls-security-profiles.adoc#tls-profiles-kubernetes-configuring_tls-security-profiles[Configuring the TLS security profile for the control plane].

[id="ocp-release-notes-storage_{context}"]
=== Storage

[id="ocp-release-notes-sscsi-disconnected-environment-support_{context}"]
==== Support for the Secrets Store CSI driver in disconnected environments
With this release, the secrets store providers support using the {secrets-store-driver} in disconnected clusters.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-secrets-store.adoc#persistent-storage-csi-secrets-store-disconnect-environment_persistent-storage-csi-secrets-store[Support for disconnected environments].

[id="ocp-release-notes-storage-azure-file-cross-sub-support_{context}"]
==== Azure File cross-subscription support is generally available
Cross-subscription support allows you to have an {product-title} cluster in one Azure subscription and mount your Azure file share in another Azure subscription using the Azure File Container Storage Interface (CSI) driver. The subscriptions must be in the same tenant.

This feature is generally available in {product-title} 4.19.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-aws-efs.adoc#persistent-storage-csi-efs-cross-account_persistent-storage-csi-aws-efs[AWS EFS CSI cross account support].

[id="ocp-release-notes-storage-vol-attributes_{context}"]
==== Volume Attributes Classes (Technology Preview)
Volume Attributes Classes provide a way for administrators to describe "classes" of storage they offer. Different classes might correspond to different quality-of-service levels.

Volume Attributes Classes in {product-title} 4.19 is available only with AWS Elastic Block Storage (EBS) and Google Cloud Platform (GCP) persistent disk (PD) Container Storage Interface (CSI).

You can apply a Volume Attributes Classes to a persistent volume claim (PVC). If a new Volume Attributes Class becomes available in the cluster, you can update the PVC with the new Volume Attributes Classes if needed.

Volume Attributes Classes have parameters that describe volumes belonging to them. If a parameter is omitted, the default is used at volume provisioning. If a user applies the PVC with a different Volume Attributes Class with omitted parameters, the default value of the parameters might be used depending on the CSI driver implementation. For more information, see the related CSI driver documentation.

Volume Attributes Classes is available in {product-title} 4.19 with Technology Preview status.

For more information, see xref:..//storage/understanding-persistent-storage.adoc#storage-persistent-storage-pvc-volumeattributesclass_understanding-persistent-storage[Volume Attributes Classes].

[id="ocp-release-notes-storage-cli-cmd-pvc-usage_{context}"]
==== New CLI command to show PVC usage (Technology Preview)
{product-title} 4.19 introduces a new command to view persistent volume claim usage. This feature has Technology Preview status.

For more information, see xref:../storage/understanding-persistent-storage.adoc#pvc-cli-command-usage_understanding-persistent-storage[Viewing PVC usage statistics].

[id="ocp-release-notes-storage-cli-cmd-resize-recovery_{context}"]
==== CSI volume resizing recovery is generally available

Previously, you might expand a persistent volume claim (PVC) to a size that is not supported by the underlying storage provider. In this case, the expansion controller typically tries forever to expand the volume and keeps failing.

This new feature allows you to recover and provide another resize value for the PVC. Resizing recovery is supported as generally available in {product-title} 4.19.

For more information about resizing volumes, see xref:../storage/expanding-persistent-volumes.adoc[Expanding persistent volumes].

For more information about recovering when resizing volumes, see xref:../storage/expanding-persistent-volumes.adoc#expanding-recovering-from-failure_expanding-persistent-volumes[Recovering from failure when expanding volumes].

[id="ocp-release-notes-storage-resize-migrated-vsphere-in-tree-vols_{context}"]
==== Support for resizing vSphere in-tree migrated volumes is generally available
Previously, VMware vSphere persistent volumes that were migrated from in-tree to Container Storage Interface (CSI) could not be resized. With {product-title} 4.19, resizing migrated volumes is supported. This feature is generally available.

For more information about resizing volumes, see xref:../storage/expanding-persistent-volumes.adoc[Expanding persistent volumes].

[id="ocp-release-notes-storage-disable-vsphere_{context}"]
==== Disabling and enabling storage on vSphere is generally available
Cluster administrators might want to disable the VMWare vSphere Container Storage Interface (CSI) Driver as a Day 2 operation, so the vSphere CSI Driver does not interface with your vSphere setup.

This features was introduced in {product-title} 4.17 with Technology Preview status. This feature is now supported as generally available in {product-title} 4.19.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere-disable-storage-overview_persistent-storage-csi-vsphere[Disabling and enabling storage on vSphere].

[id="ocp-release-notes-storage-increase-max-vols-per-node-vsphere"]
==== Increasing the maximum number of volumes per node for vSphere (Technology Preview)
For VMware vSphere version 7, {product-title} restricts the maximum number of volumes per node to 59.

However, with {product-title} 4.19 for vSphere version 8 or later, you can increase the allowable number of volumes per node to a maximum of 255. Otherwise, the default value remains at 59.

This feature has Technology Preview status.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere-increase-max-vols-per-node-overview_persistent-storage-csi-vsphere[Increasing maximum volumes per node for vSphere].

[id="ocp-release-notes-storage-vsphere-migrating-cns-vols-between-datastores_{context}"]
==== Migrating CNS volumes between datastores for vSphere is fully supported
If you are running out of space in your current datastore, or want to move to a more performant datastore, you can migrate VMware vSphere Cloud Native Storage (CNS) volumes between datastores. This applies to both attached and detached volumes.

{product-title} now fully supports migration of CNS volume using the vCenter UI. Migrated volumes should work
as expected and should not result in non-functional persistent volumes. CNS volumes can also be migrated while in use by pods.

This feature was introduced as a Development Preview in {product-title} 4.17, but is now fully supported  in 4.19.

Migrating CNS volumes between datastores requires VMware vSphere 8.0.2 or later or vSphere 7.0 Update 3o or later.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere-migrating-cns-vols-between-datastores_persistent-storage-csi-vsphere[Migrating CNS volumes between datastores for vSphere].

[id="ocp-release-notes-storage-nfs-export-options-filestore"]
==== NFS export options for Filestore storage class is generally available.
By default, a Filestore instance grants root level read/write access to all clients that share the same Google Cloud project and virtual private cloud (VPC) network. Network File System (NFS) export options can limit this access to certain IP ranges and specific user/group IDs for the Filestore instance. When creating a storage class, you can set these options using the `nfs-export-options-on-create` parameter.

NFS export options is supported as generally available in {product-title} 4.19.

For more information, see xref:..//storage/container_storage_interface/persistent-storage-csi-google-cloud-file.adoc#persistent-storage-csi-gcp-filestore-nfs-export-options_persistent-storage-csi-google-cloud-file[NFS export options].

[id="ocp-release-notes-web-console_{context}"]
=== Web console

Starting with {product-title} 4.19, the perspectives in the web console have unified to simplify navigation, reduce context switching, streamline tasks, and provide users with a more cohesive {product-title} experience.

With this unified design, there is no longer a *Developer* perspective in the default view; however, _all_ {product-title} web console features are discoverable to all users. If you are not the cluster owner, you might need to request permission for certain features from the cluster owner. The *Developer* perspective can still be manually enabled if you prefer.

The *Getting Started* pane in the web console provides resources such as, a tour of the console, information on setting up your cluster, a quick start for enabling the *Developer* perspective, and links to explore new features and capabilities.

//See xref:../web_console/web-console-overview#enabling-developer-perspective_web-console_web-console-overview for more information on enabling the *Developer* perspective.
//https://github.com/openshift/openshift-docs/pull/93644

[id="ocp-release-notes-patternfly-6-upgrade_{context}"]
==== Patternfly 6 upgrade

The web console now uses Patternfly 6. Support for Patternfly 4 in the web console is no longer available.

[discrete]
This release also introduces the following updates to the web console. You can now do the following actions:

* Specify distinct console logos for both light and dark themes using the `logos` field in the `.spec.customization.logos` configuration, allowing for more comprehensive branding.
* Easily delete identity providers (IDPs) directly from the web console, streamlining authentication configuration without manual YAML file edits.
* Easily set the default `StorageClass` directly in the web console.
* Quickly find specific jobs in the web console by sorting the *Created* column by creation date and time.

[id="ocp-4-19-notable-technical-changes_{context}"]
== Notable technical changes

[discrete]
[id="ocp-4-19-notable-technical-changes-readonlyrootfilesystem_{context}"]
=== Pods deploy with readOnlyRootFilesystem set to true

With this release, Cloud Credential Operator pods now deploy with the `readOnlyRootFilesystem` security context setting set to `true`. This enhances security by ensuring that the container root file system is mounted as read-only.

[discrete]
[id="ocp-4-19-notable-technical-changes-loopback-cert_{context}"]
=== Extended loopback certificate validity to three years for kube-apiserver

Previously, in-memory loopback certificates in kube-apiserver were issued for one year. This meant that if the kube-apiserver ran without rollouts or restarts, the kube-apiserver would stop functioning after one year. In {product-title} {product-version} the in-memory loopback certificates are extended so the kube-apiserver can run for three years without interruption.

[id="ocp-release-notes-readiness-probes-etcd_{context}"]
=== Readiness probes exclude etcd checks

The readiness probes for the API server have been modified to exclude etcd checks. This prevents client connections from being closed if etcd is temporarily unavailable. This means that client connections persist through brief etcd unavailability and minimizes temporary API server outages.

[id="ocp-4-19-deprecated-removed-features_{context}"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* _Not Available_
* _Technology Preview_
* _General Availability_
* _Deprecated_
* _Removed_

[discrete]
[id="ocp-release-note-bare-metal-dep-rem_{context}"]
=== Bare metal monitoring deprecated and removed features

.Bare Metal Event Relay Operator tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Bare Metal Event Relay Operator
|Removed
|Removed
|Removed
|====

[discrete]
[id="ocp-release-note-images-dep-rem_{context}"]
=== Images deprecated and removed features

.Images deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
[id="ocp-release-note-install-dep-rem_{context}"]
=== Installation deprecated and removed features

.Installation deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`--cloud` parameter for `oc adm release extract`
|Deprecated
|Deprecated
|Deprecated

|CoreDNS wildcard queries for the `cluster.local` domain
|Deprecated
|Deprecated
|Deprecated

|`compute.platform.openstack.rootVolume.type` for {rh-openstack}
|Deprecated
|Deprecated
|Deprecated

|`controlPlane.platform.openstack.rootVolume.type` for {rh-openstack}
|Deprecated
|Deprecated
|Deprecated

|`ingressVIP` and `apiVIP` settings in the `install-config.yaml` file for installer-provisioned infrastructure clusters
|Deprecated
|Deprecated
|Deprecated

|Package-based {op-system-base} compute machines
|Deprecated
|Deprecated
|Removed

|`platform.aws.preserveBootstrapIgnition` parameter for {aws-first}
|Deprecated
|Deprecated
|Deprecated

|Installing a cluster on {aws-short} with compute nodes in {aws-short} Outposts
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
=== Machine management deprecated and removed features

.Machine management deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Placeholder
|Status
|Status
|Status

|====

// No deprecated or removed features for 3 consecutive releases
// [discrete]
// [id="ocp-release-note-monitoring-dep-rem_{context}"]
// === Monitoring deprecated and removed features

// .Monitoring deprecated and removed tracker
// [cols="4,1,1,1",options="header"]
// |====
// |Feature |4.17 |4.18 |4.19
// |====

[discrete]
[id="ocp-release-note-networking-dep-rem_{context}"]
=== Networking deprecated and removed features

.Networking deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|iptables
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
[id="ocp-release-note-node-dep-rem_{context}"]
=== Node deprecated and removed features

.Node deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`ImageContentSourcePolicy` (ICSP) objects
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/zone`
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/region`
|Deprecated
|Deprecated
|Deprecated

|cgroup v1
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
[id="ocp-release-note-cli-dep-rem_{context}"]
=== OpenShift CLI (oc) deprecated and removed features

.OpenShift CLI (oc) deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|oc-mirror plugin v1
|General Availability
|Deprecated
|Deprecated
|====

[discrete]
[id="ocp-release-note-operators-dep-rem_{context}"]
=== Operator lifecycle and development deprecated and removed features

// "Operator lifecycle" refers to OLMv0 and "development" refers to Operator SDK

.Operator lifecycle and development deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Operator SDK
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Ansible-based Operator projects
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Helm-based Operator projects
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Go-based Operator projects
|Deprecated
|Deprecated
|Removed

|Scaffolding tools for Hybrid Helm-based Operator projects
|Deprecated
|Removed
|Removed

|Scaffolding tools for Java-based Operator projects
|Deprecated
|Removed
|Removed

// Do not remove the SQLite database... entry until otherwise directed by the Operator Framework PM
|SQLite database format for Operator catalogs
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
[id="ocp-hardware-an-driver-dep-rem_{context}"]
=== Specialized hardware and driver enablement deprecated and removed features

.Specialized hardware and driver enablement deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19
|====

[discrete]
=== Storage deprecated and removed features

.Storage deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Persistent storage using FlexVolume
|Deprecated
|Deprecated
|Deprecated

|AliCloud Disk CSI Driver Operator
|Removed
|Removed
|Removed

|Shared Resources CSI Driver Operator
|Deprecated
|Removed
|Removed
|====

[discrete]
[id="ocp-clusters-dep-rem_{context}"]
=== Updating clusters deprecated and removed features

.Updating clusters deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19
|====

[discrete]
[id="ocp-release-note-web-console-dep-rem_{context}"]
=== Web console deprecated and removed features

.Web console deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`useModal` hook for dynamic plugin SDK
|General Availability
|General Availability
|Deprecated

|Patternfly 4
|Deprecated
|Deprecated
|Removed

|====

[discrete]
[id="ocp-release-note-workloads-dep-rem_{context}"]
=== Workloads deprecated and removed features

.Workloads deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`DeploymentConfig` objects
|Deprecated
|Deprecated
|Deprecated
|====

[id="ocp-4-19-deprecated-features_{context}"]
=== Deprecated features

[id="ocp-4-19-oc-adm-pod-network-removed_{context}"]
==== `oc adm pod-network` command deprecated

The `oc adm pod-network` command for working with OpenShift SDN multitenant mode has been removed from the `oc adm --help` output. If the `oc adm pod-network` command is used, an error message is displayed to tell users that it has been deprecated.

[id="ocp-4-19-useModal-dynamic-plugin-removed_{context}"]
==== useModal hook for dynamic plugin SDK
With this release, support for the `useModal` hook in dynamic plugins are deprecated.

Starting with this release, use the `useOverlay` API hook to launch modals

//For more information about `useOverlay`, see [Doc link to dynamic plugin api docs TBD.]

[id="ocp-4-19-removed-features_{context}"]
=== Removed features

[id="ocp-4-19-rhel-worker-nodes-removed_{context}"]
==== Package-based {op-system-base} compute machines
With this release, support for the installation of packaged-based {op-system-base} worker nodes is removed.

{op-system} image layering replaces this feature and supports installing additional packages on the base operating system of your worker nodes.

For information on how to identify and remove {op-system-base} nodes in your cluster, see xref:../updating/preparing_for_updates/updating-cluster-prepare-past-4-18.adoc#updating-cluster-prepare-past-4-18[Preparing to update from {product-title} 4.18 to a newer version]. For more information on image layering, see xref:../machine_configuration/mco-coreos-layering.adoc#mco-coreos-layering[{op-system} image layering].

[id="ocp-4-19-removed-kube-1-32-apis_{context}"]
==== APIs removed from Kubernetes 1.32

Kubernetes 1.32 removed the following deprecated APIs, so you must migrate manifests and API clients to use the appropriate API version. For more information about migrating removed APIs, see the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-32[Kubernetes documentation].

.APIs removed from Kubernetes 1.32
[cols="2,2,2,1",options="header",]
|===
|Resource |Removed API |Migrate to |Notable changes

|`FlowSchema`
|`flowcontrol.apiserver.k8s.io/v1beta3`
|`flowcontrol.apiserver.k8s.io/v1`
|No

|`PriorityLevelConfiguration`
|`flowcontrol.apiserver.k8s.io/v1beta3`
|`flowcontrol.apiserver.k8s.io/v1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#flowcontrol-resources-v132[Yes]
|===

[id="ocp-4-19-removed-osdk_{context}"]
==== Operator SDK CLI and related scaffolding and testing tools

With this release, the Red{nbsp}Hat-supported version of the Operator SDK CLI tool, including the related scaffolding and testing tools for Operator projects, is no longer released with {product-title}.

Red{nbsp}Hat will provide bug fixes and support for versions of the Operator SDK that were released with earlier versions of {product-title} according to the link:https://access.redhat.com/product-life-cycles?product=OpenShift%20Container%20Platform%204[Product Life Cycles for {product-title} 4] (Red{nbsp}Hat Customer Portal).

Operator authors with existing Operator projects can use link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/operators/developing-operators#osdk-about[the version of the Operator SDK CLI tool released with {product-title} 4.18] to maintain their projects and create Operator releases that target newer versions of {product-title}. For more information, see link:https://access.redhat.com/node/7123119[Updating the base image for existing Ansible- or Helm-based Operator projects for {product-title} 4.19 and later] (Red{nbsp}Hat Knowledgebase).

For more information about the unsupported, community-maintained, version of the Operator SDK, see link:https://sdk.operatorframework.io[Operator SDK (Operator Framework)].

[id="ocp-4-19-future-deprecation_{context}"]
=== Notice of future deprecation

[id="ocp-4-19-bug-fixes_{context}"]
== Bug fixes
//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / TALO
//Telco Edge / ZTP

[discrete]
[id="ocp-release-note-api-auth-bug-fixes_{context}"]
==== API Server and Authentication

[discrete]
[id="ocp-release-note-bare-metal-hardware-bug-fixes_{context}"]
==== Bare Metal Hardware Provisioning

[discrete]
[id="ocp-release-note-builds-bug-fixes_{context}"]
==== Builds

[discrete]
[id="ocp-release-note-cloud-compute-bug-fixes_{context}"]
==== Cloud Compute

* When upgrading {gcp-short} clusters that use a boot disk that is not compatible with UEFI, you cannot enable Shielded VM support.
Previously, this prevented the creation of new compute machines.
With this release, disks with known UEFI incompatiblity have Shielded VM support disabled.
This primarily affects customers upgrading from {product-title} version 4.12 to 4.13 using the {gcp-short} marketplace images.
(link:https://issues.redhat.com/browse/OCPBUGS-17079[OCPBUGS-17079])

* Previously, VMs in a cluster that ran on {azure-short} failed because the attached network interface controller (NIC) was in a `ProvisioningFailed` state.
With this release, the Machine API controller checks the provisioning status of a NIC and refreshes the VMs on a regular basis to prevent this issue.
(link:https://issues.redhat.com/browse/OCPBUGS-31515[OCPBUGS-31515])

* Previously, in larger clusters that had other subsystems using certificate signing requests (CSRs), the CSR approver counted unrelated, unapproved CSRs towards its total and prevented further approvals.
With this release, the CSR approver uses a `signerName` property as a filter and only includes CSRs that it can approve.
As a result, the CSR approver only prevents new approvals when there are a large number of unapproved CSRs for the relevant `signerName` values.
(link:https://issues.redhat.com/browse/OCPBUGS-36404[OCPBUGS-36404])

* Previously, the Machine API controller read only the zone number to populate machine zone information.
For machines in {azure-short} regions that only support availability sets, the set number represents the zone, so the Machine API controller did not populate their zone information.
With this release, the Machine API controller references the {azure-short} fault domain property.
This property works for availability sets and availability zones, so the controller correctly reads the fault domain in each case and machines always report a zone.
(link:https://issues.redhat.com/browse/OCPBUGS-38570[OCPBUGS-38570])

* Previously, increased granularity in {gcp-short} zone API error messages caused the machine controller to mistakenly mark some machines with invalid configurations as valid with a temporary cloud error.
This behavior prevented invalid machines from transitioning to a failed state.
With this release, the machine controller handles the more granular error messages correctly so that machines with an invalid zone or project ID correctly move to a failed state.
(link:https://issues.redhat.com/browse/OCPBUGS-43531[OCPBUGS-43531])

* Previously, some permissions required for linked actions were missing.
Linked actions create the subresources necessary for other {azure-short} resources that the cloud controller manager and {product-title} require.
With this release, the cloud controller manager for {azure-short} has the following permissions for linked actions:
+
--
** `Microsoft.Network/applicationGateways/backendAddressPools/join/action`
** `Microsoft.Network/applicationSecurityGroups/joinIpConfiguration/action`
** `Microsoft.Network/applicationSecurityGroups/joinNetworkSecurityRule/action`
** `Microsoft.Network/ddosProtectionPlans/join/action`
** `Microsoft.Network/gatewayLoadBalancerAliases/join/action`
** `Microsoft.Network/loadBalancers/backendAddressPools/join/action`
** `Microsoft.Network/loadBalancers/frontendIPConfigurations/join/action`
** `Microsoft.Network/loadBalancers/inboundNatRules/join/action`
** `Microsoft.Network/networkInterfaces/join/action`
** `Microsoft.Network/networkSecurityGroups/join/action`
** `Microsoft.Network/publicIPAddresses/join/action`
** `Microsoft.Network/publicIPPrefixes/join/action`
** `Microsoft.Network/virtualNetworks/subnets/join/action`
--
+
(link:https://issues.redhat.com/browse/OCPBUGS-44126[OCPBUGS-44126])

* Previously, some permissions required for linked actions were missing.
Linked actions create the subresources necessary for other {azure-short} resources that the Machine API and {product-title} require.
With this release, the Machine API provider for {azure-short} has the following permissions for linked actions:
+
--
** `Microsoft.Compute/disks/beginGetAccess/action`
** `Microsoft.KeyVault/vaults/deploy/action`
** `Microsoft.ManagedIdentity/userAssignedIdentities/assign/action`
** `Microsoft.Network/applicationGateways/backendAddressPools/join/action`
** `Microsoft.Network/applicationSecurityGroups/joinIpConfiguration/action`
** `Microsoft.Network/applicationSecurityGroups/joinNetworkSecurityRule/action`
** `Microsoft.Network/ddosProtectionPlans/join/action`
** `Microsoft.Network/gatewayLoadBalancerAliases/join/action`
** `Microsoft.Network/loadBalancers/backendAddressPools/join/action`
** `Microsoft.Network/loadBalancers/frontendIPConfigurations/join/action`
** `Microsoft.Network/loadBalancers/inboundNatPools/join/action`
** `Microsoft.Network/loadBalancers/inboundNatRules/join/action`
** `Microsoft.Network/networkInterfaces/join/action`
** `Microsoft.Network/networkSecurityGroups/join/action`
** `Microsoft.Network/publicIPAddresses/join/action`
** `Microsoft.Network/publicIPPrefixes/join/action`
** `Microsoft.Network/virtualNetworks/subnets/join/action`
--
+
(link:https://issues.redhat.com/browse/OCPBUGS-44130[OCPBUGS-44130])

* Previously, installing an {aws-short} cluster failed in certain environments on existing subnets when the `publicIp` parameter in the compute machine set CR was set to `false`.
With this release, a fix ensures that a configuration value set for `publicIp` no longer causes issues when the installation program provisions machines for your {aws-short} cluster in certain environments.
(link:https://issues.redhat.com/browse/OCPBUGS-44373[OCPBUGS-44373])

* Previously, {gcp-short} clusters that used non-UEFI disks failed to load.
This release adds a check to ensure that disks are UEFI-compatible before enabling features that require UEFI, such as secure boot.
This change adds `compute.images.get` and `compute.images.getFromFamily` permissions requirements.
As a result, you can use non-UEFI disks if you do nto need these features.
(link:https://issues.redhat.com/browse/OCPBUGS-44671[OCPBUGS-44671])

* Previously, when the {aws-short} `DHCPOptionSet` parameter was configured to use a custom domain name that contains a trailing period (`.`), {product-title} installation failed.
With this release, the logic that extracts the hostname of EC2 instances and turns them into kubelet node names trims trailing periods so that the resulting Kubernetes object name is valid.
Trailing periods in this parameter no longer cause installation to fail. (link:https://issues.redhat.com/browse/OCPBUGS-45306[OCPBUGS-45306])

* Previously, the number of {azure-short} availability set fault domains used a fixed value of `2`.
This setting works in most {azure-short} regions because fault domain counts are typically at least 2.
However, this setting failed in the `centraluseuap` and `eastusstg` regions.
With this release, the number of availability set fault domains in a region is set dynamically.
(link:https://issues.redhat.com/browse/OCPBUGS-45663[OCPBUGS-45663])

* Previously, the {azure-short} cloud controller manager panicked when there was a temporary API server disconnection.
With this release, the {azure-short} cloud controller manager correctly recovers from temporary disconnection.
(link:https://issues.redhat.com/browse/OCPBUGS-45859[OCPBUGS-45859])

* Previously, some services became stuck in a pending state due to incorrect or missing annotations.
With this release, validation added to the {azure-short} `service.beta.kubernetes.io/azure-load-balancer-tcp-idle-timeout` and {gcp-short} `cloud.google.com/network-tier` annotations resolves the issue.
(link:https://issues.redhat.com/browse/OCPBUGS-48481[OCPBUGS-48481])

* Previously, the method used to fetch the provider ID from {aws-short} could fail to provide this value to the kubelet when needed.
As a result, sometimes machines could get stuck in different states and fail to complete initialization.
With this release, the provider ID is consistently set when the kubelet starts up.
(link:https://issues.redhat.com/browse/OCPBUGS-50905[OCPBUGS-50905])

* Previously, an incorrect endpoint in the {azure-short} cloud controller manager caused installations on {azure-full} Government Cloud to fail.
The issue is resolved in this release.
(link:https://issues.redhat.com/browse/OCPBUGS-50969[OCPBUGS-50969])

* Previously, the Machine API sometimes detected an unhealthy control plane node during cluster creation on {ibm-cloud-title} and attempted to replace the node.
This effectively destroyed the cluster.
With this release, the Machine API only attempts to replace unhealthy compute nodes during cluster creation and does not attempt to replace unhealthy control plane nodes.
(link:https://issues.redhat.com/browse/OCPBUGS-51864[OCPBUGS-51864])

* Previously, {azure-short} spot machines that were evicted before their node became ready could get stuck in the `provisioned` state.
With this release, {azure-short} spot instances now use a delete-eviction policy.
This policy ensures that the machines correctly move to the `failed` state upon preemption.
(link:https://issues.redhat.com/browse/OCPBUGS-54617[OCPBUGS-54617])

* Previously, a bug fix altered the availability set configuration by changing the fault domain count to use the maximum available value instead of a fixed value of `2`.
This inadvertently caused scaling issues for compute machine sets created before the bug fix, as the controller attempted to change immutable availability sets.
With this release, availability sets are no longer modified after creation, allowing affected compute machine sets to scale properly.
(link:https://issues.redhat.com/browse/OCPBUGS-56653[OCPBUGS-56653])

[discrete]
[id="ocp-release-note-cloud-cred-operator-bug-fixes_{context}"]
==== Cloud Credential Operator

[discrete]
[id="ocp-release-note-cluster-override-admin-operator-bug-fixes_{context}"]
==== Cluster Resource Override Admission Operator

[discrete]
[id="ocp-release-note-cluster-version-operator-bug-fixes_{context}"]
==== Cluster Version Operator

[discrete]
[id="ocp-release-note-dev-console-bug-fixes_{context}"]
==== Developer Console

[discrete]
[id="ocp-release-note-driver-toolkit-bug-fixes_{context}"]
==== Driver ToolKit (DTK)

[discrete]
[id="ocp-release-note-cloud-etcd-operator-bug-fixes_{context}"]
==== etcd Cluster Operator

[discrete]
[id="ocp-release-note-image-registry-bug-fixes_{context}"]
==== Registry

* Previously, image importing from blocked registries would fail if those registries were configured with `NeverContactSource`, even when mirror registries were set up. With this update, image importing is no longer blocked when a registry has mirrors configured. This ensures that image imports succeed even if the original source was set to `NeverContactSource` in the `ImageDigestMirrorSet` or `ImageTagMirrorSet` resources. (link:https://issues.redhat.com/browse/OCPBUGS-44432[OCPBUGS-44432])

[discrete]
[id="ocp-release-note-installer-bug-fixes_{context}"]
==== Installer

* Previously, if you attempted to install an {aws-first} cluster with minimum privileges and you did not specify an instance type in the `install-config.yaml` file, installation of the cluster failed. This issue happened because the installation program could not find supported instance types that the cluster could use in supported availability zones. For example, the `m6i.xlarge` default instance type was unavailable in `ap-southeast-4` and `eu-south-2` availability zones. With this release, the `openshift-install` program now requires the `ec2:DescribeInstanceTypeOfferings` {aws-short} permission to prevent the installation of the cluster from failing in situations where `m6i.xlarge` or another supported instance type is unavailable in a supported availability zone. (link:https://issues.redhat.com/browse/OCPBUGS-46596[OCPBUGS-46596])

[discrete]
[id="ocp-release-note-insights-operator-bug-fixes_{context}"]
==== Insights Operator

[discrete]
[id="ocp-release-note-kube-controller-bug-fixes_{context}"]
==== Kubernetes Controller Manager

[discrete]
[id="ocp-release-note-kube-scheduler-bug-fixes_{context}"]
==== Kubernetes Scheduler

[discrete]
[id="ocp-release-note-machine-config-operator-bug-fixes_{context}"]
==== Machine Config Operator

[discrete]
[id="ocp-release-note-management-console-bug-fixes_{context}"]
==== Management Console

* Previously, the console alerted users that compute nodes must be updated within 60 days when performing a control plane only update. With this release, the console no longer displays this invalid alert. (link:https://issues.redhat.com/browse/OCPBUGS-56077[OCPBUGS-56077])

* Previously, the *Critical Alerts* section of the *Notification Drawer* could not be collapsed. With this release, the section can be collapsed. (link:https://issues.redhat.com/browse/OCPBUGS-55702[OCPBUGS-55702])

* Previously, when viewing the list of installed Operators, an Operator displayed twice in the list if the currently selected project matched an Operator's default namespace while copied CSVs were disabled in {olm-first}. With this release, the Operator displays only once in such cases. (link:https://issues.redhat.com/browse/OCPBUGS-54601[OCPBUGS-54601])

* Previously, the link to *OperatorHub* on the *Installed Operators* page would trigger a hard reload. With this release, this link no longer triggers a hard reload. (link:https://issues.redhat.com/browse/OCPBUGS-54536[OCPBUGS-54536])

* Previously, selecting *All Projects* from the project picker while on the *Create VolumeSnapshot* page resulted in a page not found error. With this release, the VolumeSnapshot list page is correctly displayed. (link:https://issues.redhat.com/browse/OCPBUGS-53227[OCPBUGS-53227])

* Previously, there was incorrect logic for calculating the pod container count causing it to be inaccurate. With this release, the `Ready` and `Started` status in count logic was added so the correct pod container count is displayed, which is consist with `oc` CLI. (link:https://issues.redhat.com/browse/OCPBUGS-53118[OCPBUGS-53118])

* Previously, the *Select* menu above the *Node Logs* section did not close when opened, unless the *Select* menu's toggle was clicked again or one of the Select's menu items was clicked. With this release, the *Select* menu closes after clicking outside of the menu or by pressing the appropriate key on the keyboard. (link:https://issues.redhat.com/browse/OCPBUGS-52316[OCPBUGS-52316])

* Previously, the shared timestamp component referenced an undefined property when calculating relative times. As a result, most times displayed in the console were not correctly displaying relative strings such as 'Just now' or 'Less than a minute ago. With this release, the issue is fixed and relative time strings are correctly rendered in the console. (link:https://issues.redhat.com/browse/OCPBUGS-51202[OCPBUGS-51202])

* Previously, the *Observe* menu only displayed based on the current user and console configuration for monitoring. This caused other items added by observability plugins to be hidden. With this release, the *Observe* menu also displays items from different observability plugins. (link:https://issues.redhat.com/browse/OCPBUGS-50693[OCPBUGS-50693])

* Previously, when logging in to the console for the first time, automatic perspective detection caused the console to ignore the specific URL path that the user clicked to get onto the console, and instead load a different page. With this release, the current path is followed. (link:https://issues.redhat.com/browse/OCPBUGS-50650[OCPBUGS-50650])

* Previously, there was an issue when creating new tabs in the horizontal navigation present in the web console from plugins. With this release, you can use plugins to create tabs on the web console horizontal navigation. (link:https://issues.redhat.com/browse/OCPBUGS-49996[OCPBUGS-49996])

* Previously, the *Cluster Settings* page would not properly render during a cluster update if the `ClusterVersion` did not receive a `Completed` update. With this release, the *Cluster Setting* page properly renders even if the `ClusterVersion` has not received a `Completed` update. (link:https://issues.redhat.com/browse/OCPBUGS-49839[OCPBUGS-49839]) 

* Previously, the links on the CLI downloads page were not sorted by operating system. With this release, the links are sorted by their operating system in alphabetical order. (link:https://issues.redhat.com/browse/OCPBUGS-48413[OCPBUGS-48413])

* Previously, multiple external link icons could appear in the primary *Action* button of the *OperatorHub* modal. With this release, only a single external link icon is displayed. (link:https://issues.redhat.com/browse/OCPBUGS-46555[OCPBUGS-46555]) 

* Previously, clicking the `Don't show again` link in the {ols-official} modal did not correctly navigate to the general *User Preference* tab when one of the other *User Preference* tabs was displayed. After this update, clicking the `Don't show again` link correctly navigates to the general *User Preference* tab. (link:https://issues.redhat.com/browse/OCPBUGS-46511[(*OCPBUGS-46511)] 

* Previously, a console plugin could be enabled multiple times in the *Console plugin enablement* modal, resulting in multiple entries for the plugin appearing the Console Operator Configuration. With this release, it is no longer possible to enable a plugin if it is already enabled. (link:https://issues.redhat.com/browse/OCPBUGS-44595[OCPBUGS-44595])

* Previously, the {product-title} web console login page always allowed you to click the *Login* button. You could still click when no username or password was entered, or if the *Login* button was already clicked. With this release, the *Login* button is disabled so you are unable to click the *Login* button without a username or password. (link:https://issues.redhat.com/browse/OCPBUGS-43610[OCPBUGS-43610])

* Previously, `PackageManifest` by name only was selected on the *Operator installation* status page. In some cases, this caused the incorrect `PackageManifest` to be used for displaying the logo and provider since there can be name collisions. With this release, `PackageManifests` are selected by name and label selector to make sure the correct one is selected for the current installation. As a result, the correct logo and provider are always shown on the operator install status page. (link:https://issues.redhat.com/browse/OCPBUGS-21755[(OCPBUGS-21755)])

[discrete]
[id="ocp-release-note-monitoring-bug-fixes_{context}"]
==== Monitoring

[discrete]
[id="ocp-release-note-networking-bug-fixes_{context}"]
==== Networking

* Before this update, when a pod used the CNI plugin for DHCP address assignment, in conjunction with other CNI plugins, the pod's network interface might have been unexpectedly deleted. As a consequence, when the pod's DHCP lease expired, the DHCP proxy entered a loop when trying to re-create a new lease, leading to the node becoming unresponsive. With this release, the DHCP lease maintenance terminates if the network interface does not exist. As a result, interface deletions are handled gracefully, which ensures node stability.(link:https://issues.redhat.com/browse/OCPBUGS-45272[*OCPBUGS-45272*])

[discrete]
[id="ocp-release-note-node-bug-fixes_{context}"]
==== Node

[discrete]
[id="ocp-release-note-node-tuning-operator-bug-fixes_{context}"]
==== Node Tuning Operator (NTO)

[discrete]
[id="ocp-release-note-observability-bug-fixes_{context}"]
==== Observability

[discrete]
[id="ocp-release-note-oc-mirror-bug-fixes_{context}"]
==== oc-mirror

[discrete]
[id="ocp-release-note-openshift-cli-bug-fixes_{context}"]
==== OpenShift CLI (oc)

[discrete]
[id="ocp-release-note-olm-bug-fixes_{context}"]
==== Operator Lifecycle Manager (OLM)

[discrete]
[id="ocp-release-note-openshift-api-server-bug-fixes_{context}"]
==== OpenShift API server

[discrete]
[id="ocp-release-note-pao-bug-fixes_{context}"]
==== Performance Addon Operator

[discrete]
[id="ocp-release-note-rhcos-bug-fixes_{context}"]
==== {op-system-first}

[discrete]
[id="ocp-release-note-scalability-and-performance-bug-fixes_{context}"]
==== Scalability and performance

* Before this release, CPUs for the last guaranteed pod admitted to a node remained allocated after the pod was deleted. As a consequence, this caused scheduling domain inconsistencies. With this release, CPUs allocated to guaranteed pods return to the pool of available CPU resources as expected, ensuring correct CPU scheduling for subsequent pods. (link:https://issues.redhat.com/browse/OCPBUGS-17792[*OCPBUGS-17792*])

[discrete]
[id="ocp-release-note-storage-bug-fixes_{context}"]
==== Storage

[discrete]
[id="ocp-release-note-windows-containers-bug-fixes_{context}"]
==== Windows containers

[id="ocp-4-19-technology-preview-tables_{context}"]
== Technology Preview features status

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red{nbsp}Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the following tables, features are marked with the following statuses:

* _Not Available_
* _Technology Preview_
* _General Availability_
* _Deprecated_
* _Removed_


[discrete]
[id="ocp-release-notes-auth-tech-preview_{context}"]
=== Authentication and authorization Technology Preview features

.Authentication and authorization Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Pod security admission restricted enforcement
|Technology Preview
|Technology Preview
|Technology Preview

|Direct authentication with an external OIDC identity provider
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
[id="ocp-release-notesedge-computing-tp-features_{context}"]
=== Edge computing Technology Preview features

.Edge computing Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Accelerated provisioning of {ztp}
|Technology Preview
|Technology Preview
|Technology Preview

|Enabling disk encryption with TPM and PCR protection
|Technology Preview
|Technology Preview
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-extensions-tech-preview_{context}"]
=== Extensions Technology Preview features

// "Extensions" refers to OLMv1

.Extensions Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{olmv1-first}
|Technology Preview
|General Availability
|General Availability

|{olmv1} runtime validation of container images using sigstore signatures
|Not Available
|Technology Preview
|Technology Preview

|{olmv1} permissions preflight check for cluster extensions
|Not Available
|Not Available
|Technology Preview

|{olmv1} deploying a cluster extension in a specified namespace
|Not Available
|Not Available
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-installing-tech-preview_{context}"]
=== Installation Technology Preview features

.Installation Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

// All GA in 4.17 notes for oci-first
|Adding kernel modules to nodes with kvc
|Technology Preview
|Technology Preview
|Technology Preview

|Enabling NIC partitioning for SR-IOV devices
|General Availability
|General Availability
|General Availability

|User-defined labels and tags for {gcp-first}
|General Availability
|General Availability
|General Availability

|Installing a cluster on Alibaba Cloud by using Assisted Installer
|Technology Preview
|Technology Preview
|Technology Preview

|Mount shared entitlements in BuildConfigs in RHEL
|Technology Preview
|Technology Preview
|Technology Preview

|OpenShift zones support for vSphere host groups
|Not Available
|Not Available
|Technology Preview

|Selectable Cluster Inventory
|Technology Preview
|Technology Preview
|Technology Preview

|Installing a cluster on {gcp-short} using the Cluster API implementation
|General Availability
|General Availability
|General Availability

|Enabling a user-provisioned DNS on {gcp-short}
|Not Available
|Not Available
|Technology Preview

|Installing a cluster on {vmw-full} with multiple network interface controllers
|Not Available
|Technology Preview
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-mco-tech-preview_{context}"]
=== Machine Config Operator Technology Preview features

.Machine Config Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Improved MCO state reporting (`oc get machineconfigpool`)
|Technology Preview
|Technology Preview
|Technology Preview

|Image mode for OpenShift/On-cluster RHCOS image layering
|Technology Preview
|Technology Preview
|General Availability

|====

[discrete]
[id="ocp-release-notes-machine-management-tech-preview_{context}"]
=== Machine management Technology Preview features

.Machine management Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Managing machines with the Cluster API for {aws-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {gcp-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {azure-full}
|Not Available
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {vmw-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for {ibm-power-server-name}
|Technology Preview
|Technology Preview
|Technology Preview

|Adding multiple subnets to an existing {vmw-full} cluster by using compute machine sets
|Not Available
|Technology Preview
|Technology Preview

|Configuring Trusted Launch for {azure-full} virtual machines by using machine sets
|Technology Preview
|Technology Preview
|General Availability

|Configuring {azure-short} confidential virtual machines by using machine sets
|Technology Preview
|Technology Preview
|General Availability
|====

[discrete]
[id="ocp-release-notes-monitoring-tech-preview_{context}"]
=== Monitoring Technology Preview features

.Monitoring Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|Metrics Collection Profiles
|Technology Preview
|Technology Preview
|General Availability

|====

[discrete]
[id="ocp-release-notes-multi-arch-tech-preview_{context}"]
=== Multi-Architecture Technology Preview features

.Multi-Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`kdump` on `arm64` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `s390x` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `ppc64le` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|Support for configuring the image stream import mode behavior
|Not Available
|Technology Preview
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-networking-tech-preview_{context}"]
=== Networking Technology Preview features

.Networking Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|eBPF manager Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Advertise using L2 mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Technology Preview
|Technology Preview
|Technology Preview

|Updating the interface-specific safe sysctls list
|Technology Preview
|Technology Preview
|Technology Preview

|Egress service custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|VRF specification in `BGPPeer` custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|VRF specification in `NodeNetworkConfigurationPolicy` custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|Host network settings for SR-IOV VFs
|General Availability
|General Availability
|General Availability

|Integration of MetalLB and FRR-K8s
|General Availability
|General Availability
|General Availability

|Automatic leap seconds handling for PTP grandmaster clocks
|General Availability
|General Availability
|General Availability

|PTP events REST API v2
|General Availability
|General Availability
|General Availability

|OVN-Kubernetes customized `br-ex` bridge on bare metal
|General Availability
|General Availability
|General Availability

|OVN-Kubernetes customized `br-ex` bridge on {vmw-short} and {rh-openstack}
|Technology Preview
|Technology Preview
|Technology Preview

|Live migration to OVN-Kubernetes from OpenShift SDN
|General Availability
|Not Available
|Not Available

|User-defined network segmentation
|Technology Preview
|General Availability
|General Availability

|Dynamic configuration manager
|Not Available
|Technology Preview
|Technology Preview

|SR-IOV Network Operator support for Intel C741 Emmitsburg Chipset
|Not Available
|Technology Preview
|Technology Preview

|Gateway API and Istio for Ingress management
|Not Available
|Technology Preview
|General Availability

|Dual-port NIC for PTP ordinary clock
|Not Available
|Not Available
|Technology Preview

|DPU Operator
|Not Available
|Not Available
|Technology Preview

|Fast IPAM for the Whereabouts IPAM CNI plugin
|Not Available
|Not Available
|Technology Preview

|Unnumbered BGP peering
|Not Available
|Not Available
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-nodes-tech-preview_{context}"]
=== Node Technology Preview features

.Nodes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|`MaxUnavailableStatefulSet` featureset
|Technology Preview
|Technology Preview
|Technology Preview

|sigstore support
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
[id="ocp-release-notes-oc-cli-tech-preview_{context}"]
=== OpenShift CLI (oc) Technology Preview features

.OpenShift CLI (`oc`) Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|oc-mirror plugin v2
|Technology Preview
|General Availability
|General Availability

|oc-mirror plugin v2 enclave support
|Technology Preview
|General Availability
|General Availability

|oc-mirror plugin v2 delete functionality
|Technology Preview
|General Availability
|General Availability
|====

[discrete]
[id="ocp-release-notes-operator-lifecycle-tech-preview_{context}"]
=== Operator lifecycle and development Technology Preview features

// "Operator lifecycle" refers to OLMv0 and "development" refers to Operator SDK

.Operator lifecycle and development Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{olmv1-first}
|Technology Preview
|General Availability
|General Availability

|Scaffolding tools for Hybrid Helm-based Operator projects
|Deprecated
|Removed
|Removed

|Scaffolding tools for Java-based Operator projects
|Deprecated
|Removed
|Removed
|====

[discrete]
[id="ocp-release-notes-rhcos-tech-preview_{context}"]
=== {rh-openstack-first} Technology Preview features

.{rh-openstack} Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{rh-openstack} integration into the {cluster-capi-operator}
|Technology Preview
|Technology Preview
|Technology Preview

|Control plane with `rootVolumes` and `etcd` on local disk
|General Availability
|General Availability
|General Availability
|====

[discrete]
[id="ocp-release-notes-scalability-tech-preview_{context}"]
=== Scalability and performance Technology Preview features

.Scalability and performance Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{factory-prestaging-tool}
|Technology Preview
|Technology Preview
|Technology Preview

|Hyperthreading-aware CPU manager policy
|Technology Preview
|Technology Preview
|Technology Preview

|Mount namespace encapsulation
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Increasing the etcd database size
|Technology Preview
|Technology Preview
|Technology Preview

|Using {rh-rhacm} `PolicyGenerator` resources to manage {ztp} cluster policies
|Technology Preview
|Technology Preview
|General Availability

|Pinned Image Sets
|Technology Preview
|Technology Preview
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-special-hardware-tech-preview_{context}"]
=== Specialized hardware and driver enablement Technology Preview features

.Specialized hardware and driver enablement Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19
|====

[discrete]
[id="ocp-release-notes-storage-tech-preview_{context}"]
=== Storage Technology Preview features

.Storage Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|AWS EFS storage CSI usage metrics
|General Availability
|General Availability
|General Availability

|Automatic device discovery and provisioning with Local Storage Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Azure File CSI snapshot support
|Technology Preview
|Technology Preview
|Technology Preview

|Azure File cross-subscription support
|Not Available
|Not Available
|General Availability

|Shared Resources CSI Driver in OpenShift Builds
|Technology Preview
|Technology Preview
|Technology Preview

|{secrets-store-operator}
|Technology Preview
|General Availability
|General Availability

|CIFS/SMB CSI Driver Operator
|Technology Preview
|General Availability
|General Availability

|VMware vSphere multiple vCenter support
|Technology Preview
|General Availability
|General Availability

|Disabling/enabling storage on vSphere
|Technology Preview
|Technology Preview
|General Availability

|Increasing max number of volumes per node for vSphere
|Not Available
|Not Available
|Technology Preview

|RWX/RWO SELinux Mount
|Developer Preview
|Developer Preview
|Developer Preview

|Migrating CNS Volumes Between Datastores
|Developer Preview
|Developer Preview
|General Availability

|CSI volume group snapshots
|Not Available
|Technology Preview
|Technology Preview

|GCP PD supports C3/N4 instance types and hyperdisk-balanced disks
|Not Available
|General Availability
|General Availability

|GCP Filestore supports Workload Identity
|General Availability
|General Availability
|General Availability

|OpenStack Manila support for CSI resize
|Not Available
|General Availability
|General Availability

|Volume Attribute Classes
|Not Available
|Not Available
|Technology Preview
|====

[discrete]
[id="ocp-release-notes-web-console-tech-preview_{context}"]
=== Web console Technology Preview features

.Web console Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.17 |4.18 |4.19

|{ols-official} in the {product-title} web console
|Technology Preview
|Technology Preview
|Technology Peview
|====

[id="ocp-4-19-known-issues_{context}"]
== Known issues

* In {product-title} {product-version}, clusters using IPsec for network encryption might experience intermittent loss of pod-to-pod connectivity. This prevents some pods on certain nodes from reaching services on other nodes, resulting in connection timeouts.
+
Internal testing could not reproduce this issue on clusters with 120 nodes or less.
+
There is no workaround for this issue. (link:https://issues.redhat.com/browse/OCPBUGS-55453[OCPBUGS-55453])

* {product-title} clusters that are installed on {aws-short} in the Mexico Central region (`mx-central-1`) cannot be destroyed. (link:https://issues.redhat.com/browse/OCPBUGS-56020[*OCPBUGS-56020*])

* When installing a cluster on {azure-short}, if you set any of the`compute.platform.azure.identity.type`, `controlplane.platform.azure.identity.type`, or `platform.azure.defaultMachinePlatform.identity.type` field values to `None`, your cluster is unable to pull images from the Azure Container Registry. You can avoid this issue by either providing a user-assigned identity, or by leaving the identity field blank. In both cases, the installation program generates a user-assigned identity. (link:https://issues.redhat.com/browse/OCPBUGS-56008[*OCPBUGS-56008*])

* When installing a cluster on {aws-short}, if you did not configure {aws-short} credentials before running any `openshift-install create` command, the installation program will fail. (link:https://issues.redhat.com/browse/OCPBUGS-56658[*OCPBUGS-56658*])

[id="ocp-telco-ran-4-19-known-issues_{context}"]

* In the event of a crash, the `mlx5_core` NIC driver causes an out-of-memory issue and `kdump` does not save the `vmcore` file in `/var/crash`.
To save the `vmcore` file, use the `crashkernel` setting to reserve 1024 MB of memory for the `kdump` kernel. (link:https://issues.redhat.com/browse/OCPBUGS-54520[OCPBUGS-54520], link:https://issues.redhat.com/browse/RHEL-90663[RHEL-90663])

* There is a known latency issue on 4th Gen Intel Xeon processors. (link:https://issues.redhat.com/browse/OCPBUGS-42495[OCPBUGS-42495])

[id="ocp-telco-core-4-19-known-issues_{context}"]

* Currently, pods that use a `guaranteed` QoS class and request whole CPUs might not restart automatically after a node reboot or kubelet restart. The issue might occur in nodes configured with a static CPU Manager policy and using the `full-pcpus-only` specification, and when most or all CPUs on the node are already allocated by such workloads. As a workaround, manually delete and re-create the affected pods. (link:https://issues.redhat.com/browse/OCPBUGS-43280[*OCPBUGS-43280*])

* Currently, when a `irqbalance` service runs on a specific AArch64 machine, a buffer overflow issue might cause the service to crash. As a consequence, latency sensitive workloads might be affected by unmanaged interrupts that are not properly distributed across CPUs, leading to performance degradation. There is currently no workaround for this issue. (link:https://issues.redhat.com/browse/RHEL-89986[*RHEL-89986*])

* Currently, on clusters with SR-IOV network virtual functions configured, a race condition might occur between system services responsible for network device renaming and the TuneD service managed by the Node Tuning Operator. As a consequence, the TuneD profile might become degraded after the node restarts, leading to performance degradation. As a workaround, restart the TuneD pod to restore the profile state. (link:https://issues.redhat.com/browse/OCPBUGS-41934[*OCPBUGS-41934*])

[id="ocp-nodes-4-19-known-issues_{context}"]

[id="ocp-storage-core-4-19-known-issues_{context}"]

* NFS volumes exported from VMWare vSAN Files cannot be mounted by clusters running {product-title} 4.19 due to RHEL-83435. To avoid this issue, ensure that you are running VMWare ESXi and vSAN at the latest patch versions of 8.0 P05, or later. (link:https://issues.redhat.com/browse/OCPBUGS-55978[OCPBUGS-55978])

[id="ocp-hosted-control-planes-4-19-known-issues_{context}"]

[id="ocp-4-19-asynchronous-errata-updates_{context}"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red{nbsp}Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red{nbsp}Hat Customer Portal users can enable errata notifications in the account settings for Red{nbsp}Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red{nbsp}Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating_a_cluster/updating-cluster-web-console.adoc#updating-cluster-web-console[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-19-0-ga_{context}"]
=== RHXA-2025:XXXX - {product-title} {product-version}.0 image release, bug fix, and security update advisory

Issued: DAY-MONTH-YEAR

{product-title} release {product-version}.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHXA-2025:XXXX[RHXA-2025:XXXX] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHXA-2025:XXXX[RHXA-2025:XXXX] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.19.0 --pullspecs
----

[id="ocp-4-19-0-updating_{context}"]
==== Updating
To update an {product-title} 4.17 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI].

//replace 4.y.z for the correct values for the release. You do not need to update oc to run this command.
