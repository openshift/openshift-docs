[[release-notes-ocp-3-11-release-notes]]
= {product-title} 3.11 Release Notes
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

Red Hat {product-title} provides developers and IT organizations with a hybrid
cloud application platform for deploying both new and existing applications on
secure, scalable resources with minimal configuration and management overhead.
{product-title} supports a wide selection of programming languages and
frameworks, such as Java, Javascript, Python, Ruby, and PHP.

Built on Red Hat Enterprise Linux and Kubernetes, {product-title} provides a
secure and scalable multi-tenant operating system for today’s enterprise-class
applications, while providing integrated application runtimes and libraries.
{product-title} enables organizations to meet security, privacy, compliance, and
governance requirements.

[[ocp-311-about-this-release]]
== About This Release

Red Hat {product-title} version 3.11
(link:https://access.redhat.com/errata/RHBA-2018:2652[RHBA-2018:2652]) is now
available. This release is based on
link:https://github.com/openshift/origin/releases/tag/v3.11.0-alpha.0[OKD 3.11] and
it uses Kubernetes 1.11. New features, changes, bug fixes, and known issues that
pertain to {product-title} 3.11 are included in this topic.

{product-title} 3.11 is supported on RHEL 7.4 and 7.5 with the latest packages
from Extras, including CRI-O 1.11 and Docker 1.13. It is also supported on
Atomic Host 7.5 and newer.

For initial installations, see the
xref:../install/index.adoc#install-planning[Installing Clusters] documentation.

To upgrade to this release from a previous version, see the
xref:../upgrading/index.adoc#install-config-upgrading-index[Upgrading Clusters]
documentation.

[[ocp-311-major-changes-in-40]]
=== Major Changes Coming in Version 4.0

{product-title} 3.11 is the last release in the 3.x stream. Large changes to the
underlying architecture and installation process are coming in version 4.0.

.Features Deprecated in Version 4.0
[cols="2",options="header"]
|====
|Feature |Justification

|Hawkular
|Replaced by Prometheus monitoring.

|Cassandra
|Replaced by Prometheus monitoring.

|Heapster
|Replaced by Metrics-Server or Prometheus metrics adapter.

|Atomic Host
|Replaced by Red Hat CoreOs.

|System containers
|Replaced by Red Hat CoreOs.

|projectatomic/docker-1.13 additional search registries
|CRI-O is the default container runtime for 4.x on RHCOS and RHEL.

|`oc adm diagnostics`
|Operator-based diagnostics

|`oc adm registry`
|Replaced by the registry operator.

|Custom Docker Build Strategy on Builder Pods
|If you want to continue using custom builds, you must replace your docker
invocations with podman and buildah. The custom build strategy will not be
removed, but the functionality will change significantly in {product-title} 4.0.

|Cockpit
|Replaced by Quay.

|Standalone Registry Installations
|Quay is our enterprise container registry.

|DNSmasq
|CoreDNS will be the default.

|External etcd nodes
|For 4.0, etcd is on the cluster always.

|CFME OpenShift Provider and Podified CFME
|Replaced by Prometheus.

|Volume Provisioning via installer
|Replaced by dynamic volumes or, if NFS is required, NFS provisioner.

|xref:../upgrading/blue_green_deployments.adoc#upgrading-blue-green-deployments[blue-green-installation method]
|Ease of upgrade is a core value of 4.0.

|====

[[ocp-311-new-features-and-enhancements]]
== New Features and Enhancements

This release adds improvements related to the following components and concepts.

[[ocp-311-operators]]
=== Operators

[[ocp-311-operator-lifecycle-manager]]
==== Operator Lifecycle Manager (Technology Preview)

Operator Lifecycle Manager is currently in
xref:ocp-311-technology-preview[Technology Preview] and not for production
workloads.

The Operator Lifecycle Manager (OLM) aids cluster administrators in installing, upgrading, and granting access to Operators running on their cluster.

* Includes a catalog of curated Operators, with the ability to load other Operators into the cluster.
* Operator Lifecycle Manager handles rolling updates of all Operators to new versions.
* There is access control (RBAC) for certain teams to use certain Operators.

See
xref:../install_config/installing-operator-framework.adoc#whats-in-the-technology-preview[Installing
the Operator Framework] for more information.

[[ocp-311-operator-sdk]]
==== Operator SDK

Operator SDK:

* Provides tools to get started quickly embedding application business logic into an Operator.
* Saves you from doing the work to set up scaffolding
* Helps run end-to-end tests of your logic on a local or remote cluster
* Is used by Couchbase, MongoDB, Redis and more.

[[ocp-311-brokers]]
=== Brokers

[[ocp-311-automation-broker-ansible]]
==== {product-title} Automation Broker Integration with Ansible Galaxy

Support for discovering and running APB sources published to Ansible Galaxy from
the {product-title} Automation Broker. See xref:../architecture/service_catalog/ansible_service_broker.adoc#arch-ansible-service-broker[OpenShift Automation Broker] for more information.

[[ocp-311-broker-support-authenticated-registries]]
==== Broker Support for Authenticated Registries

{product-title} added support for authenticated registries. The broker uses
`cluster-wide` as the default setting for registry authentication credentials.
You can define `oreg_auth_user` and `oreg_auth_password` in the inventory file
to configure the credentials.

[[ocp-311-service-catalog-access-control]]
==== Service Catalog Access Control

The broker now confirms to the Open Service Broker specification, which means
you can register the broker with the service catalog as either a cluster-scoped
`ClusterServiceBroker` or a namespace-scoped `ServiceBroker` kind. Depending on
the broker's scope, its services and plans are available to the entire cluster
or scoped to a specific namespace. When installing the broker, you can set the
`kind` argument as `ServiceBroker` (namespace specific) or
`ClusterServiceBroker` (cluster-wide).

[[ocp-311-installtion-and-upgrade]]
=== Installation and Upgrade

[[ocp-311-checks-for-expiring-certificates]]
==== Checks for Expiring Certificates During Upgrade

In {product-title} 3.11, `openshift_certificate_expiry_warning_days`, which
indicates the amount of time the auto-generated certificates must be valid for
an upgrade to proceed, is added.

Additionally, `openshift_certificate_expiry_fail_on_warn` is added, which determines
whether the upgrade fails if the auto-generated certificates are not valid for the
period specified by the `openshift_certificate_expiry_warning_days` parameter.

See
xref:../install/configuring_inventory_file.adoc#install-config-configuring-inventory-file[Configuring
Your Inventory File] for more information.

[[ocp-311-support-for-ansible-2-6]]
==== Support for Ansible 2.6

`openshift-ansible` now supports Ansible 2.6 for both installation of
{product-title} 3.11 and upgrading from verion 3.10.

The minimum version of Ansible required for OCP 3.11 will now be 2.6.x to run
playbooks. On both master and node, use `subscription-manager` to enable the
repositories that are necessary in order to install {product-title} using
Ansible 2.6. For example:

----
$ subscription-manager repos --enable="rhel-7-server-rpms" \
    --enable="rhel-7-server-extras-rpms" \
    --enable="rhel-7-server-ose-3.11-rpms" \
    --enable="rhel-7-server-ansible-2.6-rpms"
----

[[ocp-311-registry-auth-credentials-required]]
==== Registry Auth Credentials Are Now Required

Registry auth credentials are now required for {product-title} so that images and metadata can be
pulled from authenticated registry, registry.redhat.io.

Registry auth credentials are required prior to installing  and upgrading when:

* `openshift_deployment_type` == `‘openshift-enterprise’`
* `oreg_url` == `‘registry.redhat.io’` or undefined

To configure authentication, `oreg_auth_user` and
`oreg_auth_password` must be defined in the inventory file.

Pods can also be allowed to reference images from other secure registries.

See xref:../dev_guide/managing_images.adoc#private-registries[Importing Images
from Private Registries] for more information.

[[ocp-311-customer-installations-are-logged]]
==== Customer Installations Are Now logged

Ansible configuration is now updated to ensure {product-title} installations are
logged by default.

The Ansible configuration parameter `log_path` is now defined. Users must be in
the *_/usr/share/ansible/openshift-ansible_* directory prior to running any
playbooks.

[[ocp-311-storage]]
=== Storage

[[ocp-311-container-storage-Interface]]
==== Container Storage Interface (Technology Preview)

Container Storage Interface (CSI) is currently in
xref:ocp-311-technology-preview[Technology Preview] and not for production
workloads.

CSI allows {product-title} to consume storage from storage backends that
implement the link:https://github.com/container-storage-interface/spec[CSI
interface] as
xref:../architecture/additional_concepts/storage.adoc#architecture-additional-concepts-storage[persistent
storage].

See
xref:../install_config/persistent_storage/persistent_storage_csi.adoc#install-config-persistent-storage-persistent-storage-csi[Persistent
Storage Using Container Storage Interface (CSI)] for more information.

[[ocp-311-local-ephemeral-storage]]
==== Protection of Local Ephemeral Storage (Technology Preview)

Protection of Local Ephemeral Storage is currently in
xref:ocp-311-technology-preview[Technology Preview] and not for production
workloads.

You can now control the use of the local ephemeral storage feature on your nodes
in order to prevent users from exhausting node local storage with their pods and
other pods that happen to be on the same node.

This feature is disabled by default. If enabled, the {product-title} cluster uses
ephemeral storage to store information that does not need to persist after the
cluster is destroyed.

See
xref:../install_config/configuring_ephemeral.adoc#install-config-configuring-ephemeral-storage[Configuring
Ephemeral Storage] for more information.

[[ocp-311-pv-provisioning-using-openstack-manilla]]
==== Persistent Volume (PV) Provisioning Using OpenStack Manila (Technology Preview)

Persistent volume (PV) provisioning using OpenStack Manila is currently in
xref:ocp-311-technology-preview[Technology Preview] and not for production
workloads.

{product-title} is capable of provisioning PVs using the
link:https://wiki.openstack.org/wiki/Manila[OpenStack Manila] shared file system
service.

See
xref:../install_config/persistent_storage/persistent_storage_manila.adoc#persistent_storage_manila[Persistent
Storage Using OpenStack Manila] for more information.

[[ocp-311-pv-resize]]
==== PV Resize (Technology Preview)

Persistent volume (PV) resize is currently in
xref:ocp-311-technology-preview[Technology Preview] and not for production
workloads.

You can expand persistent volume claims online from {product-title} for glusterFS.

. Create a storage class with `allowVolumeExpansion=true`.
. The PVC uses the storage class and submits a claim.
. The PVC specifies a new increased size.
. The underlying PV is resized.

Block storage volume types such as GCE-PD, AWS-EBS, Azure Disk, Cinder, and Ceph
RBD typically require a file system expansion before the additional space of an
expanded volume is usable by pods. Kubernetes takes care of this automatically
whenever the pod or pods referencing your volume are restarted.

Network attached file systems (like Glusterfs and Azure File) can be expanded
without having to restart the referencing pod, as these systems do not require
special file system expansion.

See
xref:../dev_guide/expanding_persistent_volumes.adoc#expanding_persistent_volumes[Expanding
Persistent Volumes] for more information.

[[ocp-311-tenant-driven-storage-snapshotting]]
==== Tenant-driven Storage Snapshotting (Technology Preview)

Tenant-driven storage snapshotting is currently in
xref:ocp-311-technology-preview[Technology Preview] and not for production
workloads.

Tenants now have the ability to leverage the underlying storage technology
backing the persistent volume (PV) assigned to them to make a snapshot of their
application data. Tenants can also now restore a given snapshot from the past to
their current application.

An external provisioner is used to access the EBS, GCE pDisk, and HostPath. This
Technology Preview feature has tested EBS and HostPath. The tenant must stop the
pods and start them manually.

. The administrator runs an external provisioner for the cluster. These are images
from the Red hat Container Catalog.

. The tenant made a PVC and owns a PV from one of the supported storage
solutions.The administrator must create a new `StorageClass` in the cluster with:
+
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: snapshot-promoter
provisioner: volumesnapshot.external-storage.k8s.io/snapshot-promoter
----

. The tenant can create a snapshot of a PVC named `gce-pvc` and the resulting
snapshot will be called `snapshot-demo`.
+
----
$ oc create -f snapshot.yaml

apiVersion: volumesnapshot.external-storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: snapshot-demo
  namespace: myns
spec:
  persistentVolumeClaimName: gce-pvc
----

. Now, they can restore their pod to that snapshot.
+
----
$ oc create -f restore.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: snapshot-pv-provisioning-demo
  annotations:
    snapshot.alpha.kubernetes.io/snapshot: snapshot-demo
spec:
  storageClassName: snapshot-promoter
----

[[ocp-311-scale]]
=== Scale

[[ocp-311-scale-cluster-limits]]
==== Cluster Limits

Updated guidance around
xref:../scaling_performance/cluster_limits.adoc#scaling-performance-cluster-limits[Cluster
Limits] for {product-title} 3.11 is now available.

*New Recommended Guidance for Master*

For large and/or dense clusters, the API server might get overloaded because of
the default QPS limits. Edit *_/etc/origin/master/master-config.yaml_* and
double or quadruple the QPS (queries per second) limits.

See
xref:../scaling_performance/host_practices.adoc#scaling-performance-capacity-host-practices-master[Recommended
Practices for OpenShift Container Platform Master Hosts] for more information.

[[ocp-311-scaling-the-cluster-monitoring-operator]]
==== Scaling the Cluster Monitoring Operator

{product-title} exposes metrics that can be collected and stored in back-ends by
the
link:https://github.com/openshift/cluster-monitoring-operator[*cluster-monitoring-operator*].
As an {product-title} administrator, you can view system resources, containers,
and components metrics in one dashboard interface, Grafana.

In {product-title} 3.11, the Cluster Monitoring Operator installation is enabled
by default as `node-role.kubernetes.io/infra=true` in your cluster.  You can
update this by setting `openshift_cluster_monitoring_operator_node_selector` in
the inventory file of your customized node selector.Ensure there is an available
node in your cluster to avoid unexpected failures.

See
xref:../scaling_performance/scaling_cluster_monitoring.adoc#scaling-performance-cluster-monitoring[Scaling
Cluster Monitoring Operator] for capacity planning details.

[[ocp-311-metrics-and-logging]]
=== Metrics and Logging

[[ocp-311-prometheus]]
==== Prometheus Cluster Monitoring

Prometheus cluster monitoring is now fully supported in {product-title}.

You can deploy Prometheus on an {product-title} cluster, collect Kubernetes and
infrastructure metrics, and get alerts. You can see and query metrics and alerts
on the Prometheus web dashboard.

See xref:../install_config/cluster_metrics.adoc#openshift-prometheus[Prometheus
on OpenShift] for more information.

[[ocp-311-syslog-output-plugin-for-fluentd]]
==== syslog Output Plug-in for fluentd (Technology Preview)

syslog Output Plug-in for fluentd is a feature currently in
xref:ocp-311-technology-preview[Technology Preview] and not for production
workloads.

You can send system and container logs from {product-title} nodes to external
endpoints using the syslog protocol. The fluentd syslog output plug-in supports
this.

[IMPORTANT]
====
Logs sent via syslog are not encrypted and, therefore, insecure.
====

See
xref:../install_config/aggregate_logging.adoc#sending-logs-to-external-rsyslog[Sending
Logs to an External Syslog Server] for more information.

[[ocp-311-elasticsearch-5-kibana-5]]
==== Elasticsearch 5 and Kibana 5

Elasticsearch 5 and Kibana 5 are now available. Kibana dashboards can be saved
and shared between users. Elasticsearch 5 introduces better resource usage and
performance and better resiliency.

Additionally, new numeric types, `half_float` and `scaled_float` are now added.
There are now instant aggregations in Kibana 5, making it faster. There is also
a new API that returns an explanation of why Elasticsearch shards are unassigned.

[[ocp-311-developer-experience]]
=== Developer Experience

[[ocp-311-cli-plug-ins]]
==== CLI Plug-ins (Technology Preview)

CLI plug-ins remain in xref:ocp-311-technology-preview[Technology Preview]
and are not for production workloads.

Usually called _plug-ins_ or _binary extensions_, this feature allows you to
extend the default set of `oc` commands available and, therefore, allows you to
perform new tasks.

See xref:../cli_reference/extend_cli.adoc#cli-reference-extend-cli[Extending the
CLI] for information on how to install and write extensions for the CLI.

[[ocp-311-configure-build-trigger-without-triggering-immediately]]
==== Configure a Build Trigger Behavior without Triggering a Build Immediately

You can pause an image change trigger to allow multiple changes on the referenced
image stream before a build is started. You can also set the `paused` attribute
to true when initially adding an `ImageChangeTrigger` to a `BuildConfig` to prevent
a build from being immediately triggered.

See
xref:../dev_guide/builds/triggering_builds.adoc#image-change-trigger[Triggering
Builds] for more information.

[[ocp-311-more-flexibility-providing-configuration-options-to-builds-using-configmaps]]
==== More Flexibility in Providing Configuration Options to Builds Using ConfigMaps

In some scenarios, build operations require credentials or other configuration
data to access dependent resources, but it is undesirable for that information
to be placed in source control. You can define _input secrets_ and _input
ConfigMaps_ for this purpose.

See xref:../dev_guide/builds/build_inputs.adoc#dev-guide-build-inputs[Build
Inputs] for additional details.

[[ocp-311-accessing-and-configuring-red-hat-registry]]
==== Accessing and Configuring the Red Hat Registry

All container images available through the Red Hat Container Catalog are hosted
on an image registry, `registry.access.redhat.com`. The Red Hat Container Catalog
is moving from `registry.access.redhat.com` to `registry.redhat.io`.
The new registry, `registry.redhat.io`, requires authentication for access to
images and hosted content on {product-title}. Following the move to the new
registry, the existing registry will be available for a period of time.

See Accessing and Configuring the Red Hat Registry for more information.

[[ocp-311-red-hat-quay-registries]]
==== Red Hat Quay Registries
If you need an enterprise quality container image registry, Red Hat Quay is
available both as a hosted service and as software you can install in your own
data center or cloud environment. Advanced registry features in Red Hat Quay
include geo-replication, image scanning, and the ability to rollback images.
Visit the link:https://quay.io[Quay.io] site to set up your own hosted Quay
registry account.

See
xref:../architecture/infrastructure_components/image_registry.adoc#architecture-infrastructure-components-image-registry[Container
Registry] for more information.

[[ocp-311-networking]]
=== Networking

[[ocp-311-kuryr]]
==== Improved {product-title} and Red Hat OpenStack Integration with Kuryr (Technology Preview)

This feature is currently in xref:ocp-311-technology-preview[Technology
Preview] and is not for production workloads.

See xref:../admin_guide/kuryr.adoc#admin-guide-kuryr[Kuryr SDN Administration]
and
xref:../install_config/configuring_kuryrsdn.adoc#install-config-configuring-kuryr-sdn[Configuring
Kuryr SDN] for best practices in {product-title} and Red Hat OpenStack
integration.

[[ocp-311-haproxy-enhancements]]
==== Router (HAProxy) Enhancements

The router (HAProxy) enhancements for {product-title} 3.11 are listed in the table below.

.Router (HAProxy) enhancements
|===
|Feature |Feature enhancements |Command syntax

|HTTP/2
|Implements HAProxy router HTTP/2 support (terminating at the router).
|`$ oc set env dc/router ROUTER_ENABLE_HTTP2=true`

|Performance
|Increases the number of threads that can be used by HAProxy to serve more routes
a| . Scale down the default router and create a new router using two threads:
+
----
$ oc scale dc/router --replicas=0
$ oc adm router myrouter --threads=2 --images='openshift3/ose-haproxy-router:v3.x
----
. Set a new thread count (for, example `7`) for the HAProxy router:
----
$ oc set env dc/myrouter ROUTER_THREADS=7
----

|Dynamic changes
|Implements changes to the HAProxy router without requiring a full router reload.
|`$ oc set env dc/router ROUTER_HAPROXY_CONFIG_MANAGER=true`

|Client SSL/TLS cert validation
|Enables mTLS for route support of older clients/services that do not support
SNI, but where certificate verification is a requirement.
|`$ oc adm router myrouter --mutual-tls-auth=optional --mutual-tls-auth-ca=/root/ca.pem --images="$image"`

|Logs captured by aggregated logging/EFK
|Collects access logs so that Operators can see them.
a| . Create a router with an rsyslog container:
+
----
$ oc adm router myrouter --extended-logging --images='xxxx'
----
. Set the log level:
+
----
$ oc set env dc/myrouter ROUTER_LOG_LEVEL=debug
----
. Check the access logs in the rsyslog container:
+
----
$ oc logs -f myrouter-x-xxxxx -c syslog
----
|===

[[ocp-311-ha-namespace-wide-egress-ip]]
==== HA Namespce-wide Egress IP

As a cluster administrator, you can assign specific, static IP addresses to projects, so that traffic is externally easily recognizable. This is different from the default egress router, which is used to send traffic to specific destinations.

See xref:../admin_guide/managing_networking.adoc#enabling-static-ips-for-external-project-traffic[Enabling Static IPs for External Project Traffic] for more information.

[[ocp-311-fully-automatic-namespace-wide-egress-ip]]
==== Fully-automatic Namespce-wide Egress IP

Projects/namespaces are automatically allocated a single egress IP on a node in
the cluster, and that IP is automatically migrated from a failed node to a
healthy node.

[[ocp-311-configurable-vxlan-port]]
==== Configurable VXLAN Port

The {product-title} SDN overlay VXLAN port is now configurable (default is
`4789`).

To configure the VXLAN port:

. Modify the VXLAN port in *_master-config.yaml_* with the new port number (for example, `4889` instead of `4789`):
+
----
vxlanPort: 4889
----

. Delete `clusternetwork` and restart the master API and controller:
+
----
$ oc delete clusternetwork default
$ master-restart api controller
----

. Restart all SDN pods in the `openshift-sdn` project:
+
----
$ oc delete pod -n openshift-sdn -l app=sdn
----

. Allow the new port on the firewall on all nodes:
+
----
# iptables -i OS_FIREWALL_ALLOW -p udp -m state --state NEW -m udp --dport 4889 -j ACCEPT
----

[[ocp-311-master]]
=== Master

[[ocp-311-pod-priority-and-preemption]]
==== Pod Priority and Preemption

You can enable pod priority and preemption in your cluster. Pod priority
indicates the importance of a pod relative to other pods and queues the pods
based on that priority. Pod preemption allows the cluster to evict, or preempt,
lower-priority pods so that higher-priority pods can be scheduled if there is no
available space on a suitable node Pod priority also affects the scheduling
order of pods and out-of-resource eviction ordering on the node.

See
xref:../admin_guide/scheduling/priority_preemption.adoc#admin-guide-priority-preemption[Pod
Priority and Preemption] for more information.

[[ocp-311-the-descheduler]]
==== The Descheduler (Technology Preview)

The Descheduler continues to be in xref:ocp-311-technology-preview[Technology
Preview] for this release and is not for production workloads.

The descheduler moves pods from less desirable nodes to new nodes. Pods can be
moved for various reasons, such as:

* Some nodes are under- or over-utilized.
* The original scheduling decision does not hold true any more, as taints or
labels are added to or removed from nodes, pod/node affinity requirements are
not satisfied any more.
* Some nodes failed and their pods moved to other nodes.
* New nodes are added to clusters.

See
xref:../admin_guide/scheduling/descheduler.adoc#admin-guide-descheduler[Descheduling]
for more information.

[[ocp-311-podman]]
==== Podman (Technology Preview)

Podman continues to be in xref:ocp-311-technology-preview[Technology Preview]
for this release and is not for production workloads.

Podman is a daemon-less CLI/API for running, managing, and debugging OCI containers and pods. It:

* Is fast and lightweight.
* Leverages runC.
* Provides a syntax for working with containers.
* Has remote management API via Varlink.
* Provides systemd integration and advanced namespace isolation.

For more information, see link:https://blog.openshift.com/crictl-vs-podman/[Crictl Vs Podman].

[[ocp-311-node-problem-detector]]
==== Node Problem Detector (Technology Preview)

The Node Problem Detector continues to be in xref:ocp-311-technology-preview[Technology
Preview]for this release and is not for production workloads.

The Node Problem Detector monitors the health of your nodes by finding certain
problems and reporting these problems to the API server, where external
controllers could take action. The Node Problem Detector is a daemon that runs
on each node as a daemonSet.  The daemon tries to make the cluster aware of node
level faults that should make the node not schedulable. When you start the Node
Problem Detector, you tell it a port over which it should broadcast the issues
it finds. The detector allows you to load sub-daemons to do the data collection.
There are three as of today.  Issues found by the problem daemon can be
classified as `NodeCondition`.

Problem daemons:

* Kernel Monitor: Monitors kernel log via journald and reports problems according
to regex patterns.
* AbrtAdaptor: Monitors the node for kernel problems and application crashes from
journald.
* CustomerPluginMonitor: Allows you to test for any condition and exit on a `0` or
`1` should your condition not be met.

See
xref:../admin_guide/node_problem_detector.adoc#admin-guide-node-problem-detector[Node
Problem Detector] for more information.

[[ocp-311-cluster-autoscaling]]
==== Cluster Autoscaling (AWS Only)

You can configure an auto-scaler on your {product-title} cluster in
Amazon Web Services (AWS) to provide elasticity for yor application
workload. The auto-scaler ensures that enough nodes are active to run
your pods and that the number of active nodes is proportional to
current demand.

See
xref:../admin_guide/cluster-autoscaler.adoc#configuring-cluster-auto-scaler-AWS[Configuring
the cluster auto-scaler in AWS] for more information.

[[ocp-311-web-console]]
=== Web console

[[ocp-311-cluster-admin-console]]
==== Cluster Administrator Console

{product-title} 3.11 introduces a cluster administrator console tailored toward
application development and cluster administrator personas.

Users have a choice of experience based on their role or technical abilities, including:

* An admin/CaaS experience with heavy exposure to Kubernetes.
* An AppDev/PaaS experience with standard OpenShift UX.

Sessions are not shared across the consoles but credentials are.

See
xref:../install/configuring_inventory_file.adoc#configuring-the-admin-console[Configuring
Your Inventory File] for details on configuring the cluster console.

image::311-cluster-console.png[cluster console]

[[ocp-311-visibility-into-nodes]]
==== Visibility into Nodes

There is now an expanded ability to manage and troubleshoot cluster nodes.

* Node status events are extremely helpful in diagnosing resource pressure and
other failures.
* Runs *node-exporter* as a DaemonSet on all nodes, with a default set of scraped
metrics from the *kube-state-metrics* project
* Metrics are protected by RBAC.
* Those with *cluster-reader* access and above can view metrics.

[[ocp-311-containers-as-a-service]]
==== Containers as a Service
You can view, edit and delete the full range of Kubernetes objects.

Networking:

* Routes and Ingress

Storage:

* PVs and PVCs
* Storage Classes

Admin:

* Projects and Namespaces
* Nodes
* Roles and RoleBindings
* CRDs

[[ocp-311-access-control-management]]
==== Access Control Management

There is now visual management of the cluster’s RBAC Roles and RoleBindings.

* Track down users and service accounts with a specific role.
* View cluster-wide or namespaced bindings.
* Visually audit a role’s verbs and objects.

Project admins can self-manage roles and bindings scoped to their namespace.

[[ocp-311-cluster-wide-event-stream]]
==== Cluster-wide Event Stream

The cluster-wide event stream helps you debug very quickly.

* All namespaces are accessible by anyone who can list the namespaces and events.
* Per-namespace is accessible for all project viewers.
* There is an ptionally filter by category and object type.

image::311-cluster-wide-event-stream.png[cluster-wide event stream]

[[ocp-311-security]]
=== Security

[[ocp-311-control-sharing-pid-namespace-between-containers]]
==== Control Sharing the PID Namespace Between Containers (Technology Preview)

Control Sharing the PID Namespace Between Containers is currently in
xref:ocp-311-technology-preview[Technology Preview] and not for production
workloads.

Use this feature to configure cooperating containers in a pod, such as a log
handler sidecar container, or to troubleshoot container images that do not
include debugging utilities like a shell.

* The feature gate `PodShareProcessNamespace` is set to `false` by default.
* Set `feature-gates=PodShareProcessNamespace=true` in  the API server,
controllers, and kubelet.
* Restart the API server, controller, and node service.
* Create a pod with the specification of `shareProcessNamespace: true`.
* Run `oc create -f <pod spec file>`.

*Caveats*

When the PID namespace is shared between containers:

* Sidecar containers are not isolated.
* Environment variables are now visible to all other processes.
* Any *kill all* semantics used within the process are now broken.
* Any `exec` processes from other containers will now show up.

See
xref:../dev_guide/expanding_persistent_volumes.adoc#expanding_persistent_volumes[Expanding
Persistent Volumes] for more information.

[[ocp-311-github-enterprise-added-as-auth-Provider]]
==== GitHub Enterprise Added as Auth Provider

GitHub Enterprise is now added as an auth provider. OAuth facilitates a token
exchange flow between {product-title} and GitHub or GitHub Enterprise. You can
use the GitHub integration to connect to either GitHub or GitHub Enterprise. For
GitHub Enterprise integrations, you must provide the `hostname` of your instance
and can optionally provide a `ca` certificate bundle to use in requests to the
server.

See xref:../install_config/configuring_authentication.adoc#GitHub[Configuring
Authentication and User Agent] for more information.

[[ocp-311-sspi-connection-support-on-windows]]
==== SSPI Connection Support on Microsoft Windows (Technology Preview)

SSPI Connection Support on Microsoft Windows is currently in
xref:ocp-311-technology-preview[Technology Preview] and not for production
workloads.

`oc` now supports the Security Support Provider Interface (SSPI) to allow for SSO
flows on Windows. If you use the request header identity provider with a
GSSAPI-enabled proxy to connect an Active Directory server to {product-title},
users can automatically authenticate to {product-title} by using the `oc`  command
line interface from a domain-joined Windows computer.

See
xref:../install_config/configuring_authentication.adoc#windows-sspi-using-request-header[Configuring
Authentication and User Agent] for more information.

[[ocp-311-microservices]]
=== Microservices

[[ocp-311-red-hat-openshit-service-mesh]]
==== Red Hat OpenShift Service Mesh (Technology Preview)

Red Hat OpenShift Service Mesh is currently a Technology Preview feature only
and not for production workloads.

Red Hat OpenShift Service Mesh is a platform that provides behavioral insights
and operational control over the service mesh, providing a uniform way to
connect, secure, and monitor microservice applications.

The term service mesh is often used to describe the network of microservices
that make up applications based on a distributed microservice architecture and
the interactions between those microservices. As a service mesh grows in size
and complexity, it can become harder to understand and manage.

Based on the open source link:https://istio.io/[Istio] project, Red Hat OpenShift Service Mesh layers
transparently onto existing distributed applications, without requiring any
changes in service code.

See
xref:../servicemesh-install/servicemesh-install.adoc#product-overview[Installing
Red Hat OpenShift Service Mesh] for more information.

[[ocp-311-notable-technical-changes]]
== Notable Technical Changes

{product-title} 3.11 introduces the following notable technical changes.

[discrete]
[[ocp-311-cluster-scoped]]
==== subjectaccessreviews.authorization.openshift.io and resourceaccessreviews.authorization.openshift.io Are Cluster-scoped Only

*_subjectaccessreviews.authorization.openshift.io_* and
*_resourceaccessreviews.authorization.openshift.io_* are now cluster-scoped
only. If you need namespace-scoped requests, use
*_localsubjectaccessreviews.authorization.openshift.io_* and
*_localresourceaccessreviews.authorization.openshift.io_*.

[discrete]
[[ocp-311-scc-new-options]]
==== New SCC options

*No New Privs*

Security Context Constraints have two new options to manage use of the (docker)
no new privs flag to prevent containers from gaining new privileges.

* `AllowPrivilegeEscalation` : Gates whether or not a user is allowed to set the security context of a container.
* `DefaultAllowPrivilegeEscalation`: Sets the default for the `allowPrivilegeEscalation` option.

For backward compatibility, the SCC flag `AllowPrivilegeEscalation` defaults to
allowed. If that behavior is not desired, this field can be used to default to
disallow, while still permitting pods to request `allowPrivilegeEscalation`
explicitly.

*Forbidden and Unsafe sysctls*

Security Context Constraints have two new options to control which sysctl
options can be defined in a pod spec:

* `forbiddenSysctls`: Exclude specific sysctls.
* `allowedUnsafeSysctls`: Used for specific needs such as high performance or real-time application tuning.

All safe sysctls are enabled by default; all unsafe sysctls are disabled by
default and must be manually allowed by the cluster admin.

[discrete]
[[ocp-311-oc-deploy-removed]]
==== Removed oc deploy Command

The `oc deploy` command, which was deprecated since {product-title} 3.7, is now
fully removed. Use `oc rollout` instead.

[discrete]
[[ocp-311-oc-env-and-oc-volume-removed]]
==== Removed oc env and oc volume Commands

The deprecated `oc env` and `oc volume` commands are now removed. Use `oc set
env` and `oc set volume` instead.

[discrete]
[[ocp-311-oc-ex-config-patch-command-removed]]
==== Removed the oc ex config patch Command

The `oc ex config patch` command will be removed in a future release, as it is
replaced by the `oc patch` command.

[discrete]
[[ocp-311-oc-export-deprecated]]
==== oc export Now Deprecated

In {product-title} 3.10, `oc export` was deprecated. It will be removed in a
future release. Use `oc get --export` instead.

[discrete]
[[ocp-311-oc-types-now-deprecated]]
==== oc types Now Deprecated

In {product-title} 3.11, `oc types` is now deprecated. It will be removed in a
future release. Use official documentation instead.

[discrete]
[[ocp-311-pipeline-plugin-now-deprecated]]
====  Pipeline Plug-in Is Deprecated

The {product-title} Pipeline Plug-in is deprecated but continues to work with
{product-title} versions up to version 3.11. For later versions of
{product-title}, either use the `oc` binary directly from your Jenkins
Pipelines, or use the {product-title} Client Plug-in.

[discrete]
[[ocp-311-logging-es5]]
====  Logging: Elasticsearch 5

Curator now works with Elasticsearch 5.

See
xref:../install_config/aggregate_logging.adoc#install-config-aggregate-logging[Aggregating
Container Logs] for additional information.

[discrete]
[[ocp-311-hawkular-now-deprecated]]
==== Hawkular Now Deprecated

Hawkular is now deprecated and will be removed in a future release.

[discrete]
[[ocp-311-ocp-uses-registry-redhat-io]]
==== New registry source for Red Hat images

Instead of `registry.access.redhat.com`, {product-title} now uses
`registry.redhat.io` as the source of images for version 3.11. For access,
`registry.redhat.io` requires credentials.

[discrete]
[[ocp-311-new-storage-driver]]
==== {product-title} New storage driver recommendation

Red Hat strongly recommends xref:../scaling_performance/optimizing_storage.adoc#choosing-a-graph-driver[using the overlayFS storage driver over Device Mapper].
For better performance, use overlayfs2 for Docker engine or overlayFS for CRI-O.
Previously, we recommended using Device Mapper.

[[ocp-311-bug-fixes]]
== Bug Fixes

This release fixes bugs for the following components:

*Builds*

* ConfigMap Build Sources allows you to use ConfigMaps as a build source, which
are transparent and easier to maintain than secrets. ConfigMaps can be injected
into any OpenShift build.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1540978[*BZ#1540978*])

* Information about out of memory (OOM) killed build pods get propagated to a
build object. This simplifies debugging and helps you discover what went wrong
if appropriate failure reasons are described to the user. A build controller
populates correctly the status reason and message when a build pod is OOM killed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1596440[*BZ#1596440*])

* The logic for updating the build status waited to update the log snippet
containing the tail of the build log only ran after the build status was updated
to the failed state. The build would first transition to a failed state, then
get updated again with the log snippet. This means code watching for the build
to enter a failed state would not see the log snippet value populated initially.
The code is now changed to populate the log snippet field when the build
transitions to failed, so the build update will contain both the failed state
and the log snippet. Code that watches the build for a transition to failed
state will see the log snippet as part of the update that transitioned the build
to failed, instead of seeing a secondary update later.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1596449[*BZ#1596449*])

* If a job used the `JenkinsPipelineStrategy` build strategy the prune settings
were being ignored. As a result, setting `successfulBuildsHistoryLimit`and
`failedBuildsHistoryLimit` was not properly pruning older jobs. The code has
been changed to properly prune jobs.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1543916[*BZ#1543916*])

*Cloud Compute*

* You can now configure NetworkManager for `dns=none` during installation. This is
commonly used when deploying on Microsoft Azure, but can also be useful in other
scenarios. In order to configure this, set
`openshift_node_dnsmasq_disable_network_manager_dns=true`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1535340[*BZ#1535340*])

*Image*

* Previously, because of improper handling of empty image stream updates, updates
to an image stream that did not result in a change in tags resulted in a request
to the image import API that included no content to be imported, which was
invalid and lead to errors in the controller. Now, updates to the image stream
that result in no new or updated tags that need to be imported will not result
in an import API call. With this fix, no invalid request is sent to the import
API and no errors occur in the controller.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1613979[*BZ#1613979*])

* Image pruning stopped on encountering any unexpected error while deleting blobs.
In the case of an image deletion error, image pruning failed to delete any image
object from etcd. Images are now being pruned concurrently in separated jobs. As
a result, image pruning does not stop on a single unexpected blob deletion
failure.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1567657[*BZ#1567657*])

*Installer*

* When deploying to AWS the `build_ami` play failed to clean *_/var/lib/cloud_*.
An unclean *_/var/lib/cloud_* directory causes cloud-init to skip execution.
This causes newly deployed node to fail to bootstrap and auto-register to
OpenShift. This bug fix cleans the *_/var/lib/cloud_* directory during
`seal_ami` play.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1599354[*BZ#1599354*])

* The installer now enables the router's extended route validation by default.
This validation performs additional validation and sanitation of routes' TLS
configuration and certificates. Extended route validation was added to the
router in {product-title} 3.3 and enhanced with certificate sanitation in
{product-title} 3.6. However, the installer did not previously enable extended
route validation. There was initial concern that the validation might be too
strict and reject valid routes and certificates, so it was disabled by default.
But it has been determined to be safe to enable by default on new installs. As a
result, extended route validation will be enabled by default on new clusters. It
can be disabled using by setting
`openshift_hosted_router_extended_validation=False` in the Ansible inventory.
Upgrading an existing cluster will *not* enable extended route validation.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1542711[*BZ#1542711*])

* Without the fully defined *_azure.conf_* file when a load balancer service was
 requested through {product-title} the load balancer would never fully register
 and provide the external IP address. Now the *_azure.conf_*, with all the
 required variables, allows the load balancer to be deployed and provides the
 external IP address.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1613546[*BZ#1613546*])

* To facilitate using CRI-O as the container-runtime for {product-title}, the
*_node-config.yaml_* file must be updated with the correct endpoint settings. The
`openshift_node_groups` defaults have been extended to include CRI-O variants
for each of the existing default node groups. To use the CRI-O runtime for a
group of compute nodes, use the following inventory variables:
+
** `openshift_use_crio=True`
** `openshift_node_group_name="node-config-compute-crio"`
+
Additionally, to deploy the docker garbage collector, `docker gc`, the following
variable must be set to `True`. This variable is changed from its previous
default of `True` to `False`:
+
** `openshift_crio_enable_docker_gc=True`
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1615884[*BZ#1615884*])

* The *_ansible.cfg_* file distributed with `openshift-ansible` now sets a default
log path of *_~/openshift-ansible.log_*. This ensures that logs are written in a
predictable location by default. In order to use the distributed *_ansible.cfg_*
file, you must first change directories to
*_/usr/share/ansible/openshift-ansible_* before running Ansible playbooks. This
*_ansible.cfg_* file  also sets other options meant to increase the performance
and reliability of `openshift-ansible`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1458018[*BZ#1458018*])

* Installing Prometheus in a multi-zone or region cluster using dynamic storage
provisioning causes the prometheus pod to become unschedulable in some cases.
The Prometheus pod requires three PVs (physical volumes): one for the Prometheus
server, one for the Alertmanager, and one for the alert-buffer. In a multi-zone
cluster with dynamic storage, it is possible that one or more of these volumes
are allocated in a different zone than the others. This causes the Prometheus
pod to become unschedulable due to each node in the cluster only able to access
PVs in its own zone. Therefore, there is no node which can run the Prometheus
pod and access all three PVs. The recommended solution is to create a storage
class which restricts volumes to a single zone by using the `zone:` parameter,
and assigning this storage class to the Prometheus volumes using the Ansible
installer inventory variable,
`openshift_prometheus_<COMPONENT>_storage_class=<zone_restricted_storage_class>`.
With this workaround, all three volumes will be created in the same zone or
region, and the Prometheus pod will be automatically scheduled to a node in the
same zone.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1554921[*BZ#1554921*])

*Logging*

* Previously, the `openshift-ansible installer` only supported `shared_ops` and
`unique` as Kibana index methods. This bug fix allows users in a non-ops EFK
cluster to share the default index in Kibana, to share queries, dashboards, and
so on. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1608984[*BZ#1608984*])

* As part of installing the ES5 stack, users need to create a `sysctl` file for the
nodes that ES runs on. This bug fix evaluates which nodes/Ansible hosts to run
the tasks against.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1609138[*BZ#1609138*])

* Additional memory is required to support Prometheus metrics and retry queues to
avoid periodic restarts from out-of-the-box memory. This bug fix increases
out-of-the-box memory for Fluentd. As a result, Fluentd pods avoid
out-of-the-box memory restarts.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1590920[*BZ#1590920*])

* Fluentd will now reconnect to Elasticsearch every 100 operations by default. If
one Elasticsearch is started before the others in the cluster, the load balancer
in the Elasticsearch service will connect to that one and that one only, and so
will all of the Fluentd connecting to Elasticsearch. With this enhancement, by
having Fluentd reconnect periodically, the load balancer will be able to spread
the load evenly among all of the Elasticsearch in the cluster.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1489533[*BZ#1489533*])

*Management Console*

* The log viewer was not accounting for multi-line or partial line responses. If a
response contained a multi-line message, it would be directly appended and
treated as a single line, causing the line numbers to be incorrect. Similarly,
if a partial line was received, it would be treated as a full line, causing
longer log lines to sometimes be split into multiple lines, again making the
line count incorrect. This bug fix adds logic in the log viewer to account for
multi-line and partial line responses. As a result, line numbers are now
accurate.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1607305[*BZ#1607305*])

*Monitoring*

* The `9100` port was blocked on all nodes by default. Prometheus could not scrape
the `node_exporter` service running on the other nodes, which listens on port
`9100`. This bug fix modifies the firewall configuration to allow incoming TCP
traffic for the `9000` - `1000` port range. As a result, Prometheus can now scrape
the `node_exporter` services.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1563888[*BZ#1563888*])

* `node_exporter` starts with the `wifi` collector enabled by default. The `wifi`
collector requires SELinux permissions that are not enabled, which causes AVC
denials though it does not stop `node_exporter`. This bug fix ensures
`node_exporter` starts with the `wifi` collector being explicitly disabled. As a
result, SELinux no longer reports AVC denials.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1593211[*BZ#1593211*])

* Uninstalling Prometheus currently deletes the entire `openshift-metrics`
namespace. This has the potential to delete objects which have been created in
the same namespace, but are not part of the Prometheus installation. This bug
fix changes the uninstall process to delete only the specific objects which were
created by the Prometheus install and only delete the namespace if there are no
remaining objects. This allows Prometheus to be installed and uninstalled while
sharing a namespace with other objects.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1569400[*BZ#1569400*])

*Pod*

* Previously, a Kubernetes bug caused `kubectl drain` to stop when pods returned
an error. With the
link:https://github.com/kubernetes/kubernetes/pull/64896[Kubernetes fix], the
command no longer hangs if pods return an error.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1586120[*BZ#1586120*])

*Routing*

* Because dnsmasq was exhausting the available file descriptors after the
OpenShift Extended Comformance Tests and the Node Vertical Test, dnsmasq
was hanging and new pods were not being created. A change to the code increases
the maximum number of open file descriptors so the node can pass the tests.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1608571[*BZ#1608571*])

* If 62 or more IP addresses are specified using an
`haproxy.router.openshift.io/ip_whitelist` annotation on a route, the router
will error due to exceeding the maximum parameters on the command (63). The
router will not reload. To fix this issue, the code was changed to use a an
overflow map if the there are too many IPs in the whitelist annotation and pass
the map to the HA-proxy ACL.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1598738[*BZ#1598738*])

* By design, using a route with several services, configuring a service with `set
 route-backend` set to a `0` weight would drop all existing connections and
 associated end-user connections. With this bug fix, a value of `0` means the
 server will not participate in load-balancing but will still accept persistent
 connections.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1584701[*BZ#1584701*])

* Because the liveness and readiness probe could not differentiate between a pod
that was alive and one that was ready, a router with
`ROUTER_BIND_PORTS_AFTER_SYNC=true` was reported as failed. The liveness and
readiness probe is split into separate probes, one for readiness and one for
liveness. As a result, a router pod can be alive but not yet ready.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1550007[*BZ#1550007*])

* When the HAproxy router contains a large number of routes (10,000 or more) the
router will not pass the Liveness and Readiness due to low performance, which
kills the router repeatedly. The root cause of this issue is likely that a
health check can not be completed within the default Readiness and Liveness
detection cycle. To prevent this problem, increase the interval of the probes.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1595513[*BZ#1595513*])

*Service Broker*

* The deprovision process for Ansible Service Broker was not deleting secrets from
the *openshift-ansible-service-broker* project. With this bug fix, the code was
changed to delete all associated secrets upon Ansible Service Broker
deprovisioning.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1585951[*BZ#1585951*])

* Previously, the broker's reconciliation feature would delete its image
references before getting the updated information from the registry, and there
would be a period of time before the records appeared in the broker's data store
while other jobs were still running. The reconciliation feature was redesigned
to do an in-place update for items that have changed. For items that were
removed from the registry, the broker will delete only those that have not been
provisioned already. It will also mark those items for deletion, which filters
them out of the UI, preventing future provisions of those items. As a result,
the broker’s reconciliation feature makes provisioning and deprovisioning more
resilient to registry changes.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1577810[*BZ#1577810*])

* Previously, users would see an error message when an item was not found, even if
it is normal not to be found. As a result, successful jobs might have an error
message logged, causing the user concern that there might be a problem when
there was none. The logging level of the message has now been changed from error
to debug, because the message is still useful for debugging purposes, but not
useful for a production installation, which usually has the level set to `info` or
higher. As a result, users will not see an error message when the instance is
not found unless there was an actual problem.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1583587[*BZ#1583587*])

* If the cluster is not running or is not reachable, the command `svcat  version`
resulted in an error. The code has been changed to always report the client
version, and if the server is reachable, it then reports the server version. As
a result, the command `svcat version` now always reports the client version. If
the server is available, it also reports the server version.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1585127[*BZ#1585127*])

* In some scenarios, using the command `svcat deprovision <service-instance-name>
--wait` sometimes resulted in the `svcat` command terminating with a panic
error. When this happened, the deprovision command was executed, and then the
program encountered a code bug when attempting to wait for the instance to be
fully deprovisioned. This issue is now resolved.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1595065[*BZ#1595065*])

*Storage*

* Previously, because the kubelet system containers could not write to the
*_/var/lib/iscsi_* directory, iSCSI volumes could not be attached. Now, you can
mount the host *_/var/lib/iscsi_* into the kubelet system container so that
iSCSI volumes can be attached.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1598271[*BZ#1598271*])

[[ocp-311-technology-preview]]
== Technology Preview Features

Some features in this release are currently in Technology Preview. These
experimental features are not intended for production use. Please note the
following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview
Features Support Scope]

In the table below, features marked *TP* indicate _Technology Preview_ and
features marked *GA* indicate _General Availability_.

.Technology Preview Tracker
[cols="4",options="header"]
|====
|Feature |OCP 3.9 |OCP 3.10 |OCP 3.11

|xref:ocp-311-prometheus[Prometheus Cluster Monitoring]
|TP
|TP
|GA

|xref:../install_config/persistent_storage/persistent_storage_local.adoc#install-config-persistent-storage-persistent-storage-local[Local Storage Persistent Volumes]
|TP
|TP
|TP

|CRI-O for runtime pods
|GA
|GA* footnoteref:[disclaimer, Features marked with `*` indicate delivery in a z-stream patch.]
|GA

|xref:ocp-311-tenant-driven-storage-snapshotting[Tenant Driven Snapshotting]
|TP
|TP
|TP

|xref:ocp-311-cli-plug-ins[`oc` CLI Plug-ins]
|TP
|TP
|TP

|Service Catalog
|GA
|GA
|GA

|xref:../architecture/service_catalog/template_service_broker.adoc#arch-template-service-broker[Template Service Broker]
|GA
|GA
|GA

|xref:../architecture/service_catalog/ansible_service_broker.adoc#arch-ansible-service-broker[OpenShift Automation Broker]
|GA
|GA
|GA

|xref:../admin_guide/managing_networking.adoc#admin-guide-networking-networkpolicy[Network Policy]
|GA
|GA
|GA

|Service Catalog Initial Experience
|GA
|GA
|GA

|New Add Project Flow
|GA
|GA
|GA

|Search Catalog
|GA
|GA
|GA

|CFME Installer
|GA
|GA
|GA

|xref:../dev_guide/cron_jobs.adoc#dev-guide-cron-jobs[Cron Jobs]
|GA
|GA
|GA

|xref:../dev_guide/deployments/kubernetes_deployments.adoc#dev-guide-kubernetes-deployments-support[Kubernetes Deployments]
|GA
|GA
|GA

|StatefulSets
|GA
|GA
|GA

|xref:../admin_guide/quota.adoc#limited-resources-quota[Explicit Quota]
|GA
|GA
|GA

|xref:../architecture/additional_concepts/storage.adoc#pv-mount-options[Mount Options]
|
|GA
|GA

|System Containers for docker, CRI-O
|Dropped
|-
|-

|xref:../install/running_install.adoc#running-the-advanced-installation-system-container[Installing from a System Container]
|GA
|GA
|GA

|Hawkular Agent
|-
|-
|-

|Pod PreSets
|-
|-
|-

|xref:../admin_guide/overcommit.adoc#configuring-reserve-resources[experimental-qos-reserved]
|TP
|TP
|TP

|xref:../admin_guide/sysctls.adoc#admin-guide-sysctls[Pod sysctls]
|TP
|TP
|TP

|xref:../install_config/master_node_configuration.adoc#master-node-config-audit-config[Central Audit]
|GA
|GA
|GA

|xref:../admin_guide/managing_networking.adoc#enabling-static-ips-for-external-project-traffic[Static IPs for External Project Traffic]
|GA
|GA
|GA

|xref:../dev_guide/templates.adoc#waiting-for-template-readiness[Template Completion Detection]
|GA
|GA
|GA

|xref:../cli_reference/basic_cli_operations.adoc#object-types[`replicaSet`]
|GA
|GA
|GA

|xref:../install_config/aggregate_logging.adoc#aggregated-fluentd[Mux]
|TP
|TP
|TP

|Clustered MongoDB Template
|-
|-
|-

|Clustered MySQL Template
|-
|-
|-

|xref:../dev_guide/managing_images.adoc#using-is-with-k8s[Image Streams with Kubernetes Resources]
|GA
|GA
|GA

|xref:../dev_guide/device_manager.adoc#using-device-manager[Device Manager]
|TP
|GA
|GA

|xref:ocp-311-pv-resize[Persistent Volume Resize]
|TP
|TP
|TP

|xref:../scaling_performance/managing_hugepages.adoc#scaling-performance-managing-huge-pages[Huge Pages]
|TP
|GA
|GA

|xref:../scaling_performance/using_cpu_manager.adoc#scaling-performance-using-cpu-manager[CPU Manager]
|TP
|GA
|GA

|xref:../dev_guide/device_plugins.adoc#using-device-plugins[Device Plug-ins]
|TP
|GA
|GA

|xref:ocp-311-syslog-output-plugin-for-fluentd[syslog Output Plug-in for fluentd]
|TP
|TP
|TP

|xref:ocp-311-container-storage-Interface[Container Storage Interface (CSI)]
|-
|TP
|TP

|xref:ocp-311-pv-provisioning-using-openstack-manilla[Persistent Volume (PV) Provisioning Using OpenStack Manila]
|-
|TP
|TP

|xref:ocp-311-node-problem-detector[Node Problem Detector]
|-
|TP
|TP

|xref:ocp-311-local-ephemeral-storage[Protection of Local Ephemeral Storage]
|-
|TP
|TP

|xref:ocp-311-the-descheduler[Descheduler]
|-
|TP
|TP

|xref:ocp-311-podman[Podman]
|-
|TP
|TP

|xref:ocp-311-kuryr[Kuryr CNI Plug-in]
|-
|TP
|TP

|xref:ocp-311-control-sharing-pid-namespace-between-containers[Sharing Control of the PID Namespace]
|-
|TP
|TP

|xref:ocp-311-cluster-admin-console[Cluster Administrator console]
|-
|-
|GA

|xref:ocp-311-cluster-autoscaling[Cluster Autoscaling (AWS Only)]
|-
|-
|GA

|xref:ocp-311-operator-lifecycle-manager[Operator Lifecycle Manager]
|-
|-
|TP

|====

[[ocp-311-known-issues]]
== Known Issues

* Due to a change in the authentication for the Kibana web console, you must log
back into the console after upgrade and every 168 hours after initial log in.
The Kibana console was migrated to *oauth-proxy*.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1614255[*BZ#1614255*])

[[ocp-311-asynchronous-errata-updates]]
== Asynchronous Errata Updates

Security, bug fix, and enhancement updates for {product-title} 3.11 are released
as asynchronous errata through the Red Hat Network. All {product-title} 3.11
errata is https://access.redhat.com/downloads/content/290/[available on the Red
Hat Customer Portal]. See the
https://access.redhat.com/support/policy/updates/openshift[{product-title}
Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account
settings for Red Hat Subscription Management (RHSM). When errata notifications
are enabled, users are notified via email whenever new errata relevant to their
registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming
{product-title} entitlements for {product-title} errata notification
emails to generate.
====

This section will continue to be updated over time to provide notes on
enhancements and bug fixes for future asynchronous errata releases of
{product-title} 3.11. Versioned asynchronous releases, for example with the form
{product-title} 3.11.z, will be detailed in subsections. In addition, releases in
which the errata text cannot fit in the space provided by the advisory will be
detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on
xref:../upgrading/index.adoc#install-config-upgrading-index[upgrading your cluster] properly.
====
