[[release-notes-ocp-3-3-release-notes]]
= {product-title} 3.3 Release Notes
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

Red Hat {product-title} is a Platform as a Service (PaaS) that provides
developers and IT organizations with a cloud application platform for deploying
new applications on secure, scalable resources with minimal configuration and
management overhead. OpenShift Enterprise supports a wide selection of
programming languages and frameworks, such as Java, Ruby, and PHP.

Built on Red Hat Enterprise Linux and Google Kubernetes, {product-title}
provides a secure and scalable multi-tenant operating system for today’s
enterprise-class applications, while providing integrated application runtimes
and libraries. {product-title} brings the OpenShift PaaS platform to customer
data centers, enabling organizations to implement a private PaaS that meets
security, privacy, compliance, and governance requirements.

[[ocp-33-about-this-release]]
== About This Release

Red Hat {product-title} version 3.3 is now available. This release is based on
link:https://github.com/openshift/origin/releases/tag/v1.3.0[OpenShift Origin 1.3]. New features, changes, bug fixes, and known issues that
pertain to {product-title} 3.3 are included in this topic.

For initial installations, see the
xref:../install_config/install/planning.adoc#install-config-install-planning[Installing a Cluster] topics in the
xref:../install_config/index.adoc#install-config-index[Installation and Configuration] documentation.

To upgrade to this release from a previous version, see the xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[Upgrading a Cluster] topics in the xref:../install_config/index.adoc#install-config-index[Installation and Configuration] documentation.

[[ocp-33-new-product-name]]
== Same Product, New Name: {product-title}

OpenShift Enterprise is now officially known as {product-title} starting with
version 3.3. It is the same product in every way; only the name has been
changed. The product itself (code, CLI, web console) has been updated
starting with version 3.3 to reflect this new name, as has the documentation.

While {product-title} is now the official name of the product, the name change
is not being backported to previous minor releases of {product-title} 3. This
means the product and documentation for versions 3.0, 3.1, and 3.2 will remain
showing the OpenShift Enterprise name. However, all version 3.3 and later
releases of the product and documentation will continue to use the
{product-title} name.

[[ocp-33-new-features-and-enhancements]]
== New Features and Enhancements

[[ocp-33-enterprise-container-registry]]
=== Enterprise Container Registry

This release adds the following improvements to the registry and its user
experience.

[[ocp-33-registry-user-interface]]
==== Registry User Interface

The {product-title} provides a registry to manage container images. The most
noticeable improvements to the registry in 3.3 are in the user interface and its
deployment options. The registry can be deployed one of two ways: as an
integrated registry in the {product-title} web console or as a stand-alone
registry.

[[ocp-33-integrated-registry-ui]]
*Integrated Registry User Interface*

The updated {product-title} integrated registry provides details about the
container images it manages and their tagged versions from within the
{product-title} web console. Each tag of an image stream, from the *Builds →
Images* view, is now a hyperlink that leads to a multi-tabbed page with more
information about the selected image.

.Integrated Registry User Interface
image::ocp33-integrated-registry-ui.png["Integrated Registry User Interface]

The *Details* tab shows the image author, built-on date, digest hash ID, labels,
annotations, and docker version for compatibility comparisons. The *Config* tab
shows how the container image was built and its set of defined metadata labels.
The *Layers* tab shows the size and digest ID for each layer of the image.

.Integrated Registry User Interface: Details Tab
image::ocp33-integrated-registry-ui-2.png["Integrated Registry User Interface: Details Tab"]

[[ocp-33-standalone-registry-ui]]
*Stand-Alone Registry User Interface*

The {product-title} registry can alternatively be installed as a
xref:../install_config/install/stand_alone_registry.adoc#install-config-installing-stand-alone-registry[stand-alone container image registry] to run on-premise or in the cloud, deployed as either
an all-in-one cluster (running the master, node, etcd and registry components)
or in a highly-available configuration (three hosts running all components on
each, with the masters configured for native high-availability).

The web console of the stand-alone registry is based on the
link:http://cockpit-project.org/[Cockpit project], with full API and CLI access.
Traffic to/from the stand-alone registry is secured by default with TLS.

.Stand-alone Registry User Interface: Details Tab
image::ocp33-standalone-registry-overview.png["Stand-alone Registry User Interface: Details Tab"]

.Stand-alone Registry User Interface: Image Metadata
image::ocp33-standalone-registry-metadata.png["Stand-alone Registry User Interface: Image Metadata"]

This container image registry viewer provides a more "hub-like" direct access
experience with registry images, and allows users to manage role-based access
control (RBAC) to authorize image delivery on a per user and per project basis
for `oc policy` and `oc projects` functionality right from within the user
interface.

.Stand-alone Registry User Interface: Project RBAC
image::ocp33-standalone-registry-rbac.png["Stand-alone Registry User Interface: Project RBAC"]

It has its own built-in OAuth server for a single sign-on (SSO) user experience
and is easily integrated with common enterprise and cloud identity providers.
The stand-alone registry management interface also has flexible options for
persistent storage of the images it manages and their metadata.

[[ocp-33-unauthenticated-image-pull]]
==== Unauthenticated Image Pull (Anonymous Access)

This new feature provides the ability to pull images from the {product-title}
integrated registry without a docker login, to facilitate automation and users
who want the ability to simply pull an image.

To enable this, the project administrator (a user with the *registry-admin*
role) can assign the *registry-viewer* role with the following command:

----
$ oc policy add-role-to-group registry-viewer system:unauthenticated
----

[[ocp-33-gcs-registry-storage]]
==== Google Cloud Storage as the Registry Storage Back End

{product-title} 3.3 adds a Google Cloud Storage (GCS) driver to enable its use
as the storage back end for the registry's container images. Prior to GCS driver
initialization, the {product-title} admin must set a
link:https://github.com/docker/distribution/blob/master/docs/storage-drivers/gcs.md[bucket
parameter] to define the name of the GCS bucket to store objects in.

[[ocp-support-docker-distribution-2-4]]
==== Support for docker distribution 2.4

The {product-title} 3.3 registry provides support for docker distribution
registry 2.4, and the features will be backported to {product-title} 3.2.
Version 2.4 of the registry includes a variety of performance and usability
enhancements, notably:

*Cross-repo Mounting When Pushing Images That Already Exist in the Registry*

When a client wishes to push a blob to a target repository from a primary
source, and knows that the blob already exists in a secondary source repository
on the same server as the target, this feature gives the user the ability to
optimize the push by requesting the server cross-mount the blob from the
secondary source repository, speeding up push time.

Of course, the client must have proper authorizations (pull and push on the
target repository, pull on the secondary source repository). If the client is
not authorized to pull from the secondary source repository, the blob push will
proceed, unoptimized, and the client will push the entire blob to the target
repository from the primary source repository without assistance from the
secondary source repository.

*Support for the New schema2 Storage Format for Images*

The image manifest version 2, schema2, allows multi-architecture images via a
manifest list which references image manifests for one or more platform-specific
versions of an image (e.g., `amd64` versus `ppc64le`). Schema 2 also supports
the ability to hash an image's configuration, to create an ID for the image and
provide docker content-addressable information about the image.

To preserve compatibility with older docker versions, support for schema 2 must
be manually enabled:

----
$ oc login -u system:admin
$ oc set env dc/docker-registry -n default REGISTRY_MIDDLEWARE_REPOSITORY_OPENSHIFT_ACCEPTSCHEMA2=true
----

[[ocp-33-allow-image-pull-through]]
==== Allow Image "Pull-Through" from a Remote Registry

The {product-title} integrated registry allows remote public and private images
to be tagged into an image stream and "pulled-through" it, as if the image were
already pushed to the {product-title} registry. Authentication credentials
required for private images to create the image stream are re-used by the
integrated registry for subsequent pull-through requests to the remote registry.

The content-offload optimization configuration is still honored by pull-through
requests. If the pull-through request points to a remote registry configured
with both a storage back end (for example, GCS, S3, or Swift storage) and
content-offload enabled, a redirect URL that points directly to the blobs on the
remote back end storage will be passed through the local registry to the local
docker daemon, creating a direct connection to the remote storage for the blobs.

To optimize image and blob lookups for pull-through requests, a small cache is
kept in the registry to track which image streams have the manifest for the
requested blobs, avoiding a potentially costly multi-server search.

[[ocp-33-networking]]
=== Networking

This release adds the following improvements to networking components.

[[ocp-33-controllable-source-ip]]
==== Controllable Source IP

Platform administrators can now identify a node in the cluster and allocate a
number of static IP addresses to the node at the host level. If a developer needs
an unchanging source IP for their application service, they can request access
to one during the process they use to ask for firewall access. Platform
administrators can then deploy an egress router from the developer's project,
leveraging a `*nodeSelector*` in the deployment configuration to ensure the pod
lands on the host with the pre-allocated static IP address.

The egress pod's deployment declares one of the source IPs, the
destination IP of the protected service, and a gateway IP to reach the
destination. After the pod is deployed, the platform administrator can create a
service to access the egress router pod. They then add that source IP to the
corporate firewall and close out the ticket. The developer then has access
information to the egress router service that was created in their project
(e.g., `service.project.cluster.domainname.com`).

When the developer would like to reach the external, firewalled service, they can
call out to the ergress router pod's service (e.g.,
`service.project.cluster.domainname.com`) in their application (e.g., the JDBC
connection information) rather than the actual protected service url.

[[ocp-33-router-sharding]]
==== Router Sharding

{product-title} offers a
xref:../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[multi-tenant],
docker-compliant platform. Thousands of tenants can be placed on the platform,
some of which may be subsidiary corporations or have drastically different
affiliations. With such diversity, often times business rules and regulatory
requirements will dictate that tenants not flow through the same routing tier.

To solve this issue, {product-title} 3.3 introduces
xref:../architecture/core_concepts/routes.adoc#router-sharding[router sharding].
With router sharding, a platform administrator can xref:../install_config/router/default_haproxy_router.adoc#using-router-shards[group specific routes or namespaces into shards] and then assign those shards to routers that may be up
and running on the platform or be external to the platform. This allows tenants
to have separation of egress traffic at the routing tiers.

[[ocp-33-non-standard-ports]]
==== Non-Standard Ports

{product-title} has always been able to support non-standard TCP ports via SNI
routing with SSL. As the internet of things (IoT) has exploded, so to has the
need to speak to dumb devices or aggregation points without SNI routing. At the
same time, with more and more people running data sources (such as databases) on
{product-title}, many more people want to expose ports other than 80 or 433 for
their applications so that people outside of the platform can leverage their
service.

Previously, the solution for this in Kubernetes was to leverage NodePorts or
External IPs. The problem with NodePorts is that only one developer can have the
port on all the nodes in the cluster. The problem with External IPs is that
duplications can be common if the administrator is not carefully assigning them
out.

{product-title} 3.3 solves this problem through xref:../admin_guide/tcp_ingress_external_ports.adoc#admin-guide-unique-external-ips-ingress-traffic[the clever use of edge routers].
Platform administrators can either select one or more of the nodes (more than
one for high availability) in the cluster to become edge routers or they can
just run additional pods on the HAProxy nodes.

For example, a platform administrator can run additional pods that are
ipfailover pods. A pool of available Ingress IPs are specified that are routable
to the nodes in the cluster and resolvable externally via the corporate DNS.
This pool of IP addresses are served out to developers who want to use a port other
than 80 and 433. In these use cases, there are services outside of the cluster
trying to connect to services inside the cluster that are running on ports other
than 80 or 433. This means they are coming into the cluster (ingress) as opposed
to leaving the cluster (egress). By resolving through the edge routers, the
cluster can ensure each developers gets their desired port by pairing it with a
Ingress IP from the available pool rather than giving them a random port.

In order to trigger this allocation of an Ingress IP, the developer declares a
`*LoadBalancer*` type in their service definition for their application.
Afterwards, they can use the `oc get <service_name>` command to see what Ingress IP was
assigned to them. See xref:../dev_guide/getting_traffic_into_cluster.adoc#getting-traffic-into-cluster[Getting Traffic into the Cluster] for details.

[[ocp-33-ab-service-annotation]]
==== A/B Service Annotation

{product-title} 3.3 adds service lists to routes, making it easier to perform
A/B testing. Each route can now have multiple services assigned to it, and those
services can come from different applications or pods. New automation enables
HAProxy to be able to read weight annotations on the route for the services.
This enables developers to declare traffic flow (for example, 70% to application
A and 30% to application B) using the CLI or web console.

[[ocp-33-security]]
=== Security

This release adds the following improvements to cluster security.

[[ocp-33-scc-profiles-seccomp]]
==== SCC Profiles for seccomp

The *seccomp* feature in Red Hat Enterprise Linux (RHEL) has been enabled for docker 1.10 or higher. This feature allows containers to define interactions with the kernel using *syscall* filtering. This reduces the risk of a malicious container exploiting a kernel vulnerability, thereby reducing the guest attack surface.

{product-title} adds the ability to create *seccomp* policies with security
context constraints (SCCs). This allows platform administrators to set SCC
policies on developers that imposes a filter on their containers for Linux-level
system calls.

[[ocp-33-kerb-support-oc-client-linux]]
==== Kerberos Support in oc client for Linux

The `oc` client on Linux can now recognize and handle the `kinit` process of
generating a Kerberos ticket during developer interactions with the CLI. For
example:

----
$ kinit <user>@<domain>
$ oc login <openshift_master>
----

[[ocp-33-cert-maintenance]]
==== Certificate Maintenance

{product-title} leverages TLS encryption and token-based authentication between
its framework components. In order to accelerate and ease the installation of
the product, certificates are self-signed during automated installation.
{product-title} 3.3 adds the ability to update and change those certificates
that govern the communication between framework components. This allows platform
administrators to more easily maintain the life cycles of their {product-title}
installations.

[[ocp-33-cluster-longevity]]
=== Cluster Longevity

This release adds the following improvements to cluster longevity.

[[ocp-33-pod-eviction]]
==== Pod Eviction

{product-title} 3.3 allows platform administrators more control over what
happens over the lifecycle of the workload on the cluster after the process
(container) is started. By leveraging limits and request setting at deployment
time, the cluster can determine automatically how the developer wants their
workload handled in terms of resources. Three positions can be taken:

- If the developer declares no resource requirements (best effort), slack resources
are offered on the cluster. More importantly, workloads are re-deployed first
should an individual node become exhausted.
- If the developer sets minimum resource requirements but does not ask for a very
specific range of consumption (burstable), their minimum is set while also
giving them an ability to consume slack resources should any exist. This
workload is considered more important than best effort in terms of re-deployment
during a node eviction.
- If a developer sets the minimum and maximum resource requirements (guaranteed),
a node with those resources is found and the workload is set as most important
on the node. These workloads remain as the last survivor on a node should it go
into a memory starvation situation.

The decision to evict is a configurable setting. Platform
administrators can turn on the ability to hand a pod (container) back to the
scheduler for re-deployment on a different node should out of memory errors
start to occur.

[[ocp-33-scale]]
==== Scale

1000 nodes per cluster at 250 pods per node (with a
recommendation of 10 pods per hyper-threaded core) are now supported. See
xref:../install_config/install/planning.adoc#sizing[Sizing Considerations] for
more details.

[[ocp-33-idling-unidling]]
==== Idling and Unidling

{product-title} 3.3 adds an API to idle an application's pods (containers). This
allows for monitoring solutions to call the API when a threshold to a metric of
interest is crossed.  At the routing tier, the HAProxy holds the declared route
URL that is connected to the service open and the pods are shut down. Should
someone hit this application URL, the pods are re-launched on available
resources in the cluster and connected to the existing route.

////
[[ocp-33-storage-labels]]
==== Storage Labels

{product-title} already included the ability to offer remote persistence block
and file based storage, and this release adds the ability for developers to
select a storage provider on the cluster in a more granular manner using storage
labels. Storage labels help developers call out to a specific provider in a
simple manner by adding a label request to their persistent volume claim (PVC).
////

[[ocp-33-framework-services]]
=== Framework Services

{product-title} provides resource usage metrics and log access to developers based on the Hawkular and Elasticsearch open source projects. This release adds the following improvements to these components.

[[ocp-33-logging-enhancements]]
==== Logging Enhancements

A new xref:../install_config/aggregate_logging.adoc#configuring-curator[log curator] utility helps platform administrators deal with the storage requirements of
storing tenant logs over time.

Integration with existing ELK stacks you might already own or be invested in has
also been enhanced by allowing logs to more easily be sent to multiple
locations.

[[ocp-33-metrics-installation-enhancements]]
==== Metrics Installation Enhancement

This release adds network usage attributes to the core metrics tracked for
tenants. Metrics deployment is also now a core installation feature instead of a
post-installation activity.  The {product-title} installer now guides you
through the Ansible playbooks required to successfully deploy metrics, thus
driving more usage of the feature in the user interface and Red Hat CloudForms.

[[ocp-33-developer-experience]]
=== Developer Experience

This release adds the following improvements to the developer workflow when
developing and testing applications on {product-title}.

[[ocp-33-pipelines]]
==== Pipelines (Technology Preview)

Previously with CI/CD, it was possible to define small pipeline-like workflows (such as triggering deployments after a new image was built or building an image when upstream source code changed), OpenShift Pipelines (currently in Technology Preview) expose a true first class pipeline execution capability. OpenShift Pipelines are based on the link:https://jenkins.io/solutions/pipeline/[Jenkins Pipeline plug-in]. By integrating Jenkins Pipelines into OpenShift, you can now leverage the full power and flexibility of the Jenkins ecosystem while managing your workflow from within OpenShift.

[NOTE]
====
See xref:ocp-33-web-console-pipelines[New Features and Enhancements: Web Console] for
more details on the new pipelines user interface.
====

Pipelines are defined as a new build strategy within {product-title}, meaning you can start, cancel, and view your pipelines in the same way as any other build. Because your pipeline is executed by Jenkins, you can also use the Jenkins console to view and manage your pipeline.

Finally, your pipelines can utilize the link:https://github.com/jenkinsci/openshift-pipeline-plugin[OpenShift Pipeline plug-in] to easily
perform first class actions in your {product-title} cluster, such as triggering
builds and deployments, tagging images, or verifying application status.

To keep the system fully integrated, the Jenkins server executing your pipeline
can run within your cluster, launch Jenkins slaves on that same cluster, and
{product-title} can even automatically deploy a Jenkins server if one does not
already exist when you first declare a new pipeline build configuration.

See the following for more on pipelines:

- xref:../architecture/core_concepts/builds_and_image_streams.adoc#pipeline-build[Pipeline Concept]
- xref:../install_config/configuring_pipeline_execution.adoc#install-config-configuring-pipeline-execution[Configuring Pipeline Execution]
- xref:../dev_guide/builds.adoc#pipeline-strategy-options[Pipeline Strategy Option]

[[ocp-33-jenkins-plugin-enhancements]]
==== Jenkins Plug-in Enhancements

The Jenkins plug-in now provides full integration with the Jenkins Pipeline,
exposing the same {product-title} build steps available in the classic,
"freestyle" jobs as Jenkins Pipeline DSL methods (replacing the Java language
invocations previously available from the Jenkins Pipeline Groovy scripts).

Several user requested features have also been introduced, including:

- Exposing "Scale OpenShift Deployments" as a post-build action
- Additional configuration available at the specific step level for triggering
builds and deployments
- Embeddable use of job parameters for configuration of specific step fields

[[ocp-33-development-cluster-setup]]
==== Easy and Quick Development Cluster Setup

Often a developer will want to have a stand-alone {product-title} instance
running on their desktop to enable evaluation of various features or developer
and testing locally of their containerized applications containers. Launching a
local instance of {product-title} for application development is now as easy as
downloading the latest client tools and running:

----
$ oc cluster up
----

This provides a running cluster using your local *docker* daemon or Docker
Machine. All the basic infrastructure of the cluster is automatically configured
for you: a registry, router, image streams for standard images, and sample
templates.

It also creates a normal user and system administrator accounts for managing the
cluster.

[[ocp-33-serialized-build-execution]]
==== Serialized Build Execution

Prior to {product-title} 3.3, if multiple builds were created for a given build
configuration, they all ran in parallel. This resulted in a race to the finish,
with the last build to push an application image to the registry winning. This
also lead to higher resource utilization peaks when multiple builds ran at the
same time.

Now with {product-title} 3.3, builds run serially by default. It is still
possible to revert to the parallel build policy if desired. In addition, the new
`*SerialLatestOnly*` policy runs builds in serial, but skips intermediary
builds. In other words, if build 1 is running and builds 2, 3, 4, and 5 are in
the queue, when build 1 completes the system will cancel builds 2 through 4 and
immediately run build 5. This allows you to optimize your build system around
building the latest code and not waste time building intermediate commits.

For more information, see xref:../dev_guide/builds.adoc#build-run-policy[Build Run Policy].


[[ocp-33-enhancement-source-code-synchronization]]
==== Enhanced Source Code Synchronization

The `oc rsync` command was added previously, allowing synchronizing of a local
file system to a running container. This is a very useful tool for copying files
into a container in general, but in particular it can be used to synchronize
local source code into a running application framework. For frameworks that
support hot deployment when files change, this enables an extremely responsive
"code -> save -> debug" workflow with source on the developer's machine using the their
IDE of choice, while the application runs in the cloud with access to any
service it depends on, such as databases.

This sync flow is made even easier with this release by coupling it with a file
system watch. Instead of manually syncing changes, developers can now run `oc
rsync --watch`, which launches a long running process that monitors the local
file system for changes and continuously syncs them to the target container.
Assuming the target container is running a framework that supports hot reload of
source code, the development workflow is now: "save file in IDE -> reload
application page in browser -> see changes."

For more information, see xref:../dev_guide/copy_files_to_container.adoc#continuous-syncing-on-file-change[Continuous Syncing on File Change].

[[ocp-33-build-trigger-cause-tracking]]
==== Build Trigger Cause Tracking

While {product-title} has always automatically run a build of your application
when source changes or an upstream image that your application is built on top
of has been updated, prior to {product-title} 3.3 it was not easy to know why
your application had been rebuilt.  With {product-title} 3.3, builds now include
information explaining what triggered the build (manual, image change, webhook,
etc.) as well as details about the change, such as the image or commit ID
associated with the change.

*A build triggered by an image change*

Output provided by CLI command `oc describe build`:

====
----
$ oc describe build ruby-sample-build-2
Name: ruby-sample-build-2
…………….
Status: Running
Started: Fri, 09 Sep 2016 16:39:46 EDT
Duration: running for 10s
Build Config: ruby-sample-build
Build Pod: ruby-sample-build-2-build

Strategy: Source
URL: https://github.com/openshift/ruby-hello-world.git
From Image: DockerImage centos/ruby-23-centos7@sha256:940584acbbfb0347272112d2eb95574625c0c60b4e2fdadb139de5859cf754bf
Output to: ImageStreamTag origin-ruby-sample:latest
Post Commit Hook: ["", "bundle", "exec", "rake", "test"]
Push Secret: builder-dockercfg-awr0v

Build trigger cause:Image change
Image ID:centos/ruby-23-centos7@sha256:940584acbbfb0347272112d2eb95574625c0c60b4e2fdadb139de5859cf754bf
Image Name/Kind: ruby:latest / ImageStreamTag
----
====

Then, within the web console:

.Build Triggered by Image Change
image::ocp33-triggered-by-imagechange.png["Build Triggered by Image Change"]

*A build triggered by a webhook*

Output provided by CLI command `oc describe build`:

====
----
$ oc describe build mynodejs-4
Name: mynodejs-4
…………...
Status: Complete
Started: Mon, 12 Sep 2016 04:57:44 EDT
Duration: 20s
Build Config: mynodejs
Build Pod: mynodejs-4-build

Strategy: Source
URL: https://github.com/bparees/nodejs-ex.git
Ref: master
Commit: 7fe8ad9 (update welcome page text)
Author/Committer: Ben Parees
From Image: DockerImage centos/nodejs-4-centos7@sha256:f525982280a22eb35c48bac38ee5dc65d545ac0431ce152e351d7efa0a34a82d
Output to: ImageStreamTag mynodejs:latest
Push Secret: builder-dockercfg-nt9xq

Build trigger cause:GitHub WebHook
Commit:7fe8ad9 (update welcome page text)
Author/Committer:Ben Parees
Secret: 34c64fd2***
----
====

Then, within the web console:

.Build Triggered by Webhook
image::ocp33-triggered-by-webhook.png["Build Triggered by Webhook"]

[[ocp-33-webhook-improvements]]
==== Webhook Improvements

It is now possible to provide additional inputs to webhook triggered builds. Previously, the generic webhook simply started a new build with all the default values inherited from the build configuration. It is now possible to provide a payload to the webhook API.

The payload can provide Git information so that a specific commit or branch can
be built. Environment variables can also be provided in the payload. Those
environment variables are made available to the build in the same way as
environment variables defined in the build configuration.

For examples of how to define a payload and invoke the webhook, see xref:../dev_guide/builds.adoc#build-triggers[Generic Webhooks].

[[ocp-33-self-tuning-images]]
==== Self-tuning Images

{product-title} provides a number of framework images for working with Java,
Ruby, PHP, Python, NodeJS, and Perl code. It also provides a few database images
(MySQL, MongoDB, PostgreSQL) out of the box. For {produc-title} 3.3, these
images are improved by making them self-tuning.

Based on the container memory limits specified when the images are deployed,
these images will automatically configure parameters like heap sizes, cache
sizes, number of worker threads, and more. All these automatically-tuned values
can easily be overridden by environment variables, as well.

[[ocp-33-web-console]]
=== Web Console

This release adds the following improvements to the web console, including
updates to existing features, usability overhauls, and a few brand new concepts.

[[ocp-33-usability-project-overview]]
==== Usability Improvements: Project Overview

The web console's *Overview* is the landing page for your project. At a glance,
you should be able to see what is running in your project, how things are
related, and what state they are in. To that end, the re-designed overview now
includes the following:

.New Project Overview
image::ocp33-project-overview.png["New Project Overview"]
<1> Warnings, suggestions, and other notifications in context
<2> Metrics for a deployment or pod
<3> Better awareness of deployment status (animation of rolling deployments, cancel
in-progress deployments, and wake up idled deployments)
<4> Grouping of related services

[[ocp-33-usability-project-navigation]]
==== Usability Improvements: Project Navigation

Previously, most of the concepts in {product-title} were hidden underneath a
generic *Browse* menu. An exercise to define the information architecture
resulted in the new left sidebar project navigation.

[horizontal]
Overview:: The dashboard for your project.
Applications:: Everything that make up your running application. This means pods, things that create or replicate pods, and anything that controls the flow of network traffic to pods.
Builds:: Builds, pipelines, and build artifacts, like images.
Resources:: Resource restrictions like limit ranges, project quotas, and cluster quotas. Also, other advanced resources in your project that do not fit into one of the top level concepts.
Storage:: View your existing persistent volume claims (PVCs) and request persistent storage.
Monitoring:: A single page that gives you access to logs, metrics, and events.

[[ocp-33-web-console-pipelines]]
==== New Concept: OpenShift Pipelines

A new set of pages have been added dedicated to the new
xref:ocp-33-web-console-pipelines[OpenShift Pipelines] feature (currently in
Technology Preview) that allow you to visualize your pipeline's stages, edit the
configuration, and manually kick off a build. Pipelines paused waiting for
manual user intervention provide a link to the Jenkins pipeline interface.

.OpenShift Pipelines Details
image::ocp33-pipelines.png["OpenShift Pipelines Overview"]

Running or recently completed pipeline builds also show up on the new *Overview*
page if they are related to a deployment configuration.

.Project Overview with Pipelines
image::ocp33-pipelines2.png["Project Overview with Pipelines"]

OpenShift Pipelines are currently in Technology Preview. To enable pipelines in
the primary navigation of the web console, see
xref:../install_config/web_console_customization.html#install-config-web-console-customization[Customizing the Web Console].

[[ocp-33-web-console-ab-routing]]
==== New Concept: A/B Routing

In {product-title} 3.3, routes can now point to multiple back end services,
commonly called xref:ocp-33-ab-service-annotation[A/B deployments]. Routes
configured in this way will automatically group the related services and
visualize the percentage of traffic configured to go to each one.

.A/B Routes
image::ocp33-abroutes.png["A/B Routes"]

Modifying the route's back end services can be done in the new GUI editor, which
also lets you change the route’s target ports, path, and TLS settings.

[[ocp-33-web-console-deploy-image]]
==== Deploy Image

The *Add to Project* page now a *Deploy Image* option. The behavior is similar
to the `oc run` command, allowing you to pick any existing image or tag from an
image stream, or to look for an image using a docker pull spec. After you have
picked an image, it generates the service, deployment configuration, and an
image stream if it is from a pull spec.

.Deploy Image
image::ocp33-deployimage.png["Deploy Image"]

You can also take advantage of the new and improved key value editor for
environment variables and labels.

[[ocp-33-web-console-import-yaml-json]]
==== Import YAML / JSON

The *Add to Project* page now has an *Import YAML / JSON* option, which behaves
like the `oc create -f` command. You can paste, upload, or drag and drop your
file, and even edit the YAML or JSON before submitting it. If your file
contained a template resource, you can choose whether you want to create and/or
process the template resource.

.Import YAML / JSON
image::ocp33-importyamljson.png["Import YAML / JSON"]

Processing a template goes to the existing experience for creating from a
template, and now supports showing a message to the user on the next steps page.
This message can be defined by the template author and can include generated
parameters like passwords and other keys.

[[ocp-33-web-console-other-resources]]
==== Other Resources

The *Other Resources* page gives you access to all the other content that exists
in your project that do not have dedicated pages yet. You can select the type of
resource you want to list and get actions to *Edit YAML* (similar to `oc edit`)
and *Delete*. Due to a new feature that has been applied to the whole web
console, only the resource types you have permission to list are shown, and only
actions that you can actually perform.

.Other Resources
image::ocp33-otherresources.png["Other Resources"]

[[ocp33-web-console-monitoring]]
==== Monitoring

While the *Overview* provides some simple metrics and pod status, the new
*Monitoring* page provides a deeper dive into the logs, metrics, and events
happening in your project.

.Monitoring
image::ocp33-monitoring.png["Monitoring"]

Metrics and logs both received some minor improvements including:

- Network sent and received metrics for deployments and pods
- Deployment metrics show a separate line for each pod
- Log viewer supports ANSI color codes and ANSI carriage returns (treated as new lines)
- Log viewer turns URLs into links

[[ocp33-web-console-debugging]]
==== Debugging

When a pod's containers are not starting cleanly, a link is now shown on the pod
details page to debug it in a terminal. This starts a pod with identical
settings, but changes the container's entrypoint to `/bin/sh` instead, giving
you access to the runtime environment of the container.

.Debugging
image::ocp33-debugging.png["Debugging"]

A number of small improvements to the container terminal have also been added
that create a smoother experience, including:

- Automatically focusing the keyboard input when the terminal connection is established
- Resizing based on the available space in the browser window
- Setting the `*TERM*` environment variable so common shell actions like `clear` behave the way you expect
- Better support for multi-container pods

.Terminal
image::ocp33-terminal.png["Terminal"]

[[ocp-33-web-console-image-details]]
==== Image Details

Before {product-title} 3.3, there was no information in the web console about
the images in your image streams, aside from the SHAs. This made it difficult to
know the specifics of how your image was defined unless you used the CLI. Now,
for any image stream tag you can see the metadata, cofiguration, and layers.

.Image Stream Tag Details
image::ocp33-imagedetails.png["Image Stream Tag Details"]

.Image Stream Tag Configuration
image::ocp33-imagedetails2.png["Image Stream Tag Configuration"]

[[ocp-33-networking]]
=== Networking

This release adds the following improvements to networking components.

[[ocp-33-controllable-source-ip]]
==== Controllable Source IP

Platform administrators can now identify a node in the cluster and allocate a
number of static IP addresses to the node at the host level. If a developer needs
an unchanging source IP for their application service, they can request access
to one during the process they use to ask for firewall access. Platform
administrators can then deploy an egress router from the developer's project,
leveraging a `*nodeSelector*` in the deployment configuration to ensure the pod
lands on the host with the pre-allocated static IP address.

The egress pod's deployment declares one of the source IPs, the
destination IP of the protected service, and a gateway IP to reach the
destination. After the pod is deployed, the platform administrator can create a
service to access the egress router pod. They then add that source IP to the
corporate firewall and close out the ticket. The developer then has access
information to the egress router service that was created in their project
(e.g., `service.project.cluster.domainname.com`).

When the developer would like to reach the external, firewalled service, they can
call out to the ergress router pod's service (e.g.,
`service.project.cluster.domainname.com`) in their application (e.g., the JDBC
connection information) rather than the actual protected service url.

See xref:../admin_guide/managing_networking.adoc#admin-guide-controlling-egress-traffic[Controlling Egress Traffic] for more details.

[[ocp-33-router-sharding]]
==== Router Sharding

{product-title} offers a
xref:../architecture/networking/sdn.adoc#architecture-additional-concepts-sdn[multi-tenant],
docker-compliant platform. Thousands of tenants can be placed on the platform,
some of which may be subsidiary corporations or have drastically different
affiliations. With such diversity, often times business rules and regulatory
requirements will dictate that tenants not flow through the same routing tier.

To solve this issue, {product-title} 3.3 introduces
xref:../architecture/networking/routes.adoc#router-sharding[router sharding].
With router sharding, a platform administrator can xref:../install_config/router/default_haproxy_router.adoc#using-router-shards[group specific routes or namespaces into shards] and then assign those shards to routers that may be up
and running on the platform or be external to the platform. This allows tenants
to have separation of egress traffic at the routing tiers.

[[ocp-33-non-standard-ports]]
==== Non-Standard Ports

{product-title} has always been able to support non-standard TCP ports via SNI
routing with SSL. As the internet of things (IoT) has exploded, so to has the
need to speak to dumb devices or aggregation points without SNI routing. At the
same time, with more and more people running data sources (such as databases) on
{product-title}, many more people want to expose ports other than 80 or 433 for
their applications so that people outside of the platform can leverage their
service.

Previously, the solution for this in Kubernetes was to leverage NodePorts or
External IPs. The problem with NodePorts is that only one developer can have the
port on all the nodes in the cluster. The problem with External IPs is that
duplications can be common if the administrator is not carefully assigning them
out.

{product-title} 3.3 solves this problem through xref:../admin_guide/tcp_ingress_external_ports.adoc#admin-guide-unique-external-ips-ingress-traffic[the clever use of edge routers].
Platform administrators can either select one or more of the nodes (more than
one for high availability) in the cluster to become edge routers or they can
just run additional pods on the HAProxy nodes.

For example, a platform administrator can run additional pods that are
ipfailover pods. A pool of available Ingress IPs are specified that are routable
to the nodes in the cluster and resolvable externally via the corporate DNS.
This pool of IP addresses are served out to developers who want to use a port other
than 80 and 433. In these use cases, there are services outside of the cluster
trying to connect to services inside the cluster that are running on ports other
than 80 or 433. This means they are coming into the cluster (ingress) as opposed
to leaving the cluster (egress). By resolving through the edge routers, the
cluster can ensure each developers gets their desired port by pairing it with a
Ingress IP from the available pool rather than giving them a random port.

In order to trigger this allocation of an Ingress IP, the developer declares a
`*LoadBalancer*` type in their service definition for their application.
Afterwards, they can use the `oc get <service_name>` command to see what Ingress IP was
assigned to them. See xref:../dev_guide/getting_traffic_into_cluster.adoc#getting-traffic-into-cluster[Getting Traffic into the Cluster] for details.

[[ocp-33-ab-service-annotation]]
==== A/B Service Annotation

{product-title} 3.3 adds service lists to routes, making it easier to perform
A/B testing. Each route can now have multiple services assigned to it, and those
services can come from different applications or pods.

New automation enables HAProxy to be able to read weight annotations on the
route for the services. This enables developers to declare traffic flow (for
example, 70% to application A and 30% to application B) using the CLI or web
console.

[NOTE]
====
See xref:ocp-33-web-console-ab-routing[New Features and Enhancements: Web Console] for
more details on the new A/B routing user interface.
====

See xref:../dev_guide/routes.adoc#routes-load-balancing-for-AB-testing[Load Balancing for A/B Testing] for more details.

[[ocp-33-security]]
=== Security

This release adds the following improvements to cluster security.

[[ocp-33-scc-profiles-seccomp]]
==== SCC Profiles for seccomp

The *seccomp* feature in Red Hat Enterprise Linux (RHEL) has been enabled for docker 1.10 or higher. This feature allows containers to define interactions with the kernel using *syscall* filtering. This reduces the risk of a malicious container exploiting a kernel vulnerability, thereby reducing the guest attack surface.

{product-title} adds the ability to create *seccomp* policies with security
context constraints (SCCs). This allows platform administrators to set SCC
policies on developers that imposes a filter on their containers for Linux-level
system calls.

See the xref:../architecture/additional_concepts/authorization.adoc#authorization-seccomp[Authorization] concept for more details.

[[ocp-33-kerb-support-oc-client-linux]]
==== Kerberos Support in oc client for Linux

The `oc` client on Linux can now recognize and handle the `kinit` process of
generating a Kerberos ticket during developer interactions with the CLI. For
example:

----
$ kinit <user>@<domain>
$ oc login <openshift_master>
----

[[ocp-33-cert-maintenance]]
==== Certificate Maintenance

{product-title} leverages TLS encryption and token-based authentication between
its framework components. In order to accelerate and ease the installation of
the product, certificates are self-signed during automated installation.

{product-title} 3.3 adds the ability to update and change those certificates
that govern the communication between framework components. This allows platform
administrators to more easily maintain the life cycles of their {product-title}
installations.

See xref:../install_config/redeploying_certificates.adoc#install-config-redeploying-certificates[Redeploying Certificates] for more details.

[[ocp-33-cluster-longevity]]
=== Cluster Longevity

This release adds the following improvements to cluster longevity.

[[ocp-33-pod-eviction]]
==== Pod Eviction

.Image Stream Configuration
image::ocp33-imagedetails2.png["Image Stream Configuration"]

[[ocp-33-notable-technical-changes]]
== Notable Technical Changes

{product-title} 3.3 introduces the following notable technical changes.

[[ocp-33-updated-infrastructure-components]]
*Updated Infrastructure Components*

- Kubernetes has been updated to v1.3.0+52492b4.
- etcd has been updated to 2.3.0+git.
- OpenShift Enterprise 3.3 requires Docker 1.10.

[[ocp-33-manual-endpoints-clusternetworkcidr]]
*Manually-Created Endpoints Inside ClusterNetworkCIDR*

In OpenShift Enterprise 3.2 and earlier, if the cluster was using the
*redhat/openshift-ovs-multitenant* network plug-in, and a service endpoint was
manually created pointing to a pod or service owned by another tenant, then that
endpoint would be ignored. In {product-title} 3.3, it is no longer possible for
regular users to create such an endpoint
(link:https://github.com/openshift/origin/pull/9383[*openshift/origin#9383*]).
As a result, the plug-in now no longer filters them out
(link:https://github.com/openshift/origin/pull/9982[*openshift/origin#9982*]).

However, previously-created illegal endpoints might still exist; if so, the old,
pre-upgrade logs will show warnings like the following, indicating the illegal
endpoints object:

====
----
Service 'foo' in namespace 'bob' has an Endpoint inside the service network (172.30.99.99)
Service 'foo' in namespace 'bob' has an Endpoint pointing to non-existent pod (10.130.0.8)
Service 'foo' in namespace 'bob' has an Endpoint pointing to pod 10.130.0.4 in namespace 'alice'
----
====

These log messages are the simplest way to find such illegal endpoints, but if
you no longer have the pre-upgrade logs, you can try commands like the following
to search for them.

To find endpoints pointing to the default `*ServiceNetworkCIDR*`
(172.30.0.0/16):

----
$ oc get endpoints --all-namespaces --template \
    '{{ range .items }}{{ .metadata.namespace }}:{{ .metadata.name }} \
    {{ range .subsets }}{{ range .addresses }}{{ .ip }} \
    {{ end }}{{ end }}{{ "\n" }}{{ end }}' | awk '/ 172\.30\./ { print $1 }'
----

To find endpoints pointing to the default `*ClusterNetworkCIDR*`
(10.128.0.0/14):

----
$ for ep in $(oc get services --all-namespaces --template \
    '{{ range .items}}{{ range .spec.selector }}{{ else }}{{ .metadata.namespace}}:{{ .metadata.name }} \
    {{ end }}{{ end }}'); do \
        oc get endpoints --namespace $(echo $ep | sed -e 's/:.*//') $(echo $ep | sed -e 's/.*://') \
        --template '{{ .metadata.namespace }}:{{ .metadata.name }} {{ range .subsets }}{{ range \
        .addresses }}{{ .ip }} {{ end }}{{ end }}{{ "\n" }}' | awk '/ \
        10\.(12[8-9]|1[3-9][0-9]|2[0-5][0-9])\./ { print $1 }' \
done
----

[[ocp-33-pull-access-tagging-is]]
*Pull Access When Tagging Image Streams*

When tagging images across projects, for example:

----
$ oc tag <project_1>/<image_stream_a>:<tag_a> <project_b>/<image_stream_b>:<tag_b>
----

a user must have pull permission on the source image stream
(link:https://github.com/openshift/origin/pull/10109[*openshift/origin#10109*]).
This means they must get access on the *imagestreams/layers* resource in the
source project. The *admin*, *edit*, and *system:image-puller* roles all grant
this permission.

[[ocp-33-changes-dns-records-srv-requests]]
*Changes to DNS Records Returned by SRV Requests*

{product-title} 3.3 has altered the DNS records returned by SRV requests for
services to be compatible with Kubernetes 1.3 to support `*PetSets*` objects
(link:https://github.com/openshift/origin/pull/9972[*openshift/origin#9972*]).
The primary change is that SRV records for a name no longer enumerate the list
of all available ports; instead, if you want to find a port named `http` over
protocol `tcp`, you must specifically ask for that SRV record.

. The SRV records returned for service names (`<service>.<namespace>.svc.cluster.local`)
have changed.
+
Previously, {product-title} returned one SRV record per service port, but to be
compatible with Kubernetes 1.3, SRV records are now returned representing
endpoints (`<endpoint>.<service>.<namespace>.svc.cluster.local`) without port
info (a port of `0`).
+
A clustered service (type `*ClusterIP*`) will have one record pointing to a
generated name (e.g., `340982409.<service>.<namespace>.svc.cluster.local`) and
an associated A record pointing to the cluster IP.
+
A headless service (with `*clusterIP=None*`) returns one record per address
field in the `*Endpoints*` record (typically one per pod). The endpoint name is
either the `hostname` field in the endpoint (read from an annotation on the pod)
or a hash of the endpoint address, and has an associated A record pointing to
the address matching that name.

. The SRV records returned for an endpoint name
(`<endpoint>.<service>.<namespace>.svc.cluster.local`) have changed: a single
SRV record is returned if the endpoint exists (the name matches the generated
endpoint name described above) or no record if the endpoint does not exist.

. The SRV records for a given port
(`_<portname>._<protocol>.<service>.<namespace>.svc.cluster.local`) behave as
they did before, returning port info.

[[ocp-33-bug-fixes]]
== Bug Fixes

This release fixes bugs for the following components:

*Authentication*

* Multiple API servers starting simultaneously with an empty etcd datastore would race to populate the default system policy. A partially created policy could result, leaving a new cluster with a policy that would forbid system components from making some API calls. This bug fix updates the policy APIs to perform the same `*resourceVersion*` checking as other APIs, and fault-tolerant logic was added to the initial policy population step. As a result, new clusters populate default policy as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1359900[*BZ#1359900*])

*Builds*

* The transition between serial and parallel builds was not handled correctly. If parallel builds were queued after a running serial build, the first parallel build would also run serially, instead of running all the parallel builds in parallel when the serial build completed. After this bug fix, when the first parallel build is run, any other parallel builds in the queue are also run. As a result, all parallel builds in the queue start simultaneously when the last serial build finishes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1357786[*BZ#1357786*])

* The S2I builder image value was not getting properly set on an `s2i rebuild` invocation, causing these invocations to fail. This bug fix changes the code so that it inspects the existing image on rebuild and populates the configuration from its labels instead of the builder's labels. The builder image is still inspected on typical `s2i build` invocations. As a result, both `s2i build` and `s2i rebuild` now work as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1366475[*BZ#1366475*])

* Updates to a build configuration via the replace mechanism would previously reset the build sequence count to zero if no value was specified in the update. Builds would fail to start if the reset sequence number caused collisions with existing builds that used those the sequence number previously. After this bug fix, the sequence number is no longer reset during updates to the build configuration. As a result, build configurations can be updated and the existing sequence number is preserved, so new builds do not collide with previously used sequence numbers. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1357791[*BZ#1357791*])

*Command Line Interface*

* An improper argument parsing rejected valid values caused parameter values containing equal signs to be incorrectly rejected. This bug fix changes parsing to tolerate values containing equal signs. As a result, parameter values containing equal signs are tolerated. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1375275[*BZ#1375275*])

*Image*

* This enhancement updates the Perl S2I builder image to support proxy configurations. Previously, the image could not access remote resources if the customer network required a proxy be used. The Perl image now respects the `*HTTP_PROXY*` environment variable for configuring the proxy to use when requesting remote resources during the build process. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1348945[*BZ#1348945*])

* Previously, the timeout for liveness probe for the Jenkins readiness check was too short. This caused Jenkins pods to fail to report as ready then get restarted. This bug fix increases the timeout for the readiness probe, and Jenkins pods now have sufficient time to start before the readiness probe fails. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1368967[*BZ#1368967*])

*Image Registry*

* The S3 communication library was not efficient enough to support high loads of data. This caused some pushes to the registry to take relatively long. This bug fix updates both the docker distribution code along with the S3 driver. As a result, docker push operations experience improved stability and performance. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1314381[*BZ#1314381*])

* A bug in an older registry version prevented it from working with a Swift storage back-end while having the content-offload feature turned off, causing the registry to be unusable in these conditions. This bug fix updates the registry version, which has reworked storage drivers. As a result, the registry is now usable in these conditions. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1348031[*BZ#1348031*])

* When pruning images, a user was previously presented with too many log details by default. This bug fix hides some debug information behind increased `--loglevel` settings. As a result, logs presented to user should be more readable. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1341527[*BZ#1341527*])

*Installer*

* Previously, the installer did not correctly format the registry 2.4 configuration file when using S3 storage. This bug fix corrects this formatting issue and the installer now correctly provisions S3-based registry components when configured to do so. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1356823[*BZ#1356823*])

* Previously, installation would fail with an unrelated error message when `*openshift_hosted_registry_storage_kind=nfs*` was specified in the inventory but no NFS hosts were configured via `*openshift_hosted_registry_storage_host*` or the `*nfs*` host group. Playbooks now output an error message indicating that no storage hosts have been configured. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1357984[*BZ#1357984*])

* Previously, containerized nodes mounted *_/sys_* read-only, which prevented the node from mounting Ceph volumes. This mount for the containerized node has been updated to be read-write, allowing the node to mount Ceph volumes properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1367937[*BZ#1367937*])

* The quick installer previously did not verify file system paths when read from a configuration file. This caused the quick installer to attempt to read a file which did not exist, throw a stack trace, and abort the installation. This bug fix ensures that the file system path is now verified to exist when read from a configuration file, and as a result the quick installer no longer crashes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1368296[*BZ#1368296*])

*Kubernetes*

* This enhancement adds volume affinity to {product-title} (OCP). Cloud providers typically use multiple zones/regions for their virtual machines and storage offerings. A virtual machine in one zone/region can only mount storage from the same zone/region in which it resides. OCP pods that use cloud storage must be scheduled onto virtual machines in the same zone/region for their associated storage; otherwise, the pods will fail to run. With this enhancement, pods are now scheduled to the same zone/region as their associated storage. Note that if you are not using the default scheduler configuration, you must ensure that the `*NoVolumeZoneConflict*` scheduler predicate is enabled in your scheduler configuration file in order for volume affinity to function correctly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1356010[*BZ#1356010*])

* The trigger controller used for handling triggers for deployments was not handling `*ImageChangeTriggers*` correctly from different namespaces, resulting in hot looping between deployments. This bug fix addresses the issue and it no longer occurs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1366936[*BZ#1366936*])

* The Horizontal Pod Autoscaler scales based on CPU usages as a percentage of the requested CPU for a pod. It is possible that the desired percentage be over 100 (if the user wants to scale only when the CPU usage of a pod is higher than the amount requested for the pod, but below the limit for the pod). Previously, the CLI  would prevent the user from setting such values. Now, it allows setting a target CPU percentage of over 100. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1336692[*BZ#1336692*])

* Jobs were an experimental feature in OpenShift Enterprise 3.1, and templates did not work with jobs. This bug fix stabilizes the job feature. Jobs have been migrated to stable API allowing full support of all the necessary features, including templates. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1319929[*BZ#1319929*])

* Diagnostics previously reported an error when the registry was not backed by a persistent storage volume on the pod, without considering alternative methods of storage. If the registry had been reconfigured to use S3 as storage, for example, diagnostics reported this error. This bug fix updates the diagnostic check to see if registry configuration has been customized and does not report an error if so. As a result, it is assumed the cluster administrator that does the configuration knows what they are doing, and false alerts on S3-configured registries are no longer reported. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1359771[*BZ#1359771*])

*Logging*

* This enhancement adds auto-tuning for Elasticsearch memory heap usage based on container limit. Elasticsearch recommends hard limits for proper usage and these limits may significantly exceed what is available to the container. Elasticsearch should limit itself from the onset. With this enhancement, the container runscript evaluates the available memory and sets the minimum and maximum heap size. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1370115[*BZ#1370115*])

* When image streams are created, only a subset of the available tags are imported, and this often excluded the desired tag. If the desired tag is not imported, then the corresponding component never deploys. To work around this issue, import each tag manually:
+
----
$ oc import-image <name>:<version> --from <prefix><name>:<tag>
----
+
This bug is fixed in {product-title} 3.3 by not relying on image streams and deployment configuration triggers for deployment. As a result, deployment occurs as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1338965[*BZ#1338965*])

* When a project was deleted, the plug-in for Fluentd was not properly handling the fetching of metadata and would exit, restarting the Fluentd pod. This bug fix updates the *kubeclient* and *rest-client* gems for Fluentd. As a result, Fluentd is able to properly handle cases where the project was deleted for logs it is processing. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1365422[*BZ#1365422*])

* When reading in rolled over log messages into Fluentd, if the rolled over file name was not in a specific format, Fluentd would fail while processing the date for that record. This was to adjust for a gap where logs from the previous year would be interpreted as logs that take place in the future since there was not a year field on the log records. This could cause a loss of log records. With this bug fix, in addition to container logs, Fluentd now only reads in records from *_/var/log/messages_* instead of  *_/var/log/messages*_*. As a result, Fluentd no longer reads in log records from rolled over files. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1347871[*BZ#1347871*])

* The *OpenShift-Elasticsearch-Plugin* did not remove the `.all` Kibana mapping for users that were *cluster-admin* but then had the role reverted. If a user was no longer a *cluster-admin*, they could still be able to view the `.all` Kibana mapping. They would not be able to see the logs for projects they did not have access to, but they would still incorrectly see the mapping. This bug fix updates the *OpenShift-Elasticsearch-Plugin* to remove the `.all` Kibana mapping to users that are not *cluster-admin*. As a result, non-*cluster-admin* users are not able to see the `.all` mapping if they are no longer *cluster-admin*. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1372277[*BZ#1372277*])

*Web Console*

* The builder images in the web console were not ordered by semantic version. In some cases, a newer technology version could be hidden under a *See All* link because it had a lower sort order. With this bug fix, the builders are now properly ordered by their semantic version. As a result, more recent version are sorted to the top and are no longer hidden. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1325069[*BZ#1325069*])

* When configuring a build to use a GitHub git source and setting a context directory or reference, the source repository appeared as the full link to the context directory or reference in GitHub, which is a long unreadable URL. This bug fix updates the web console to not show the full link. As a result, the visual representation of the source repository is only the source repository, and the target of the link includes the context directory and reference. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1364950[*BZ#1364950*])

* The web console prevented users from deleting replication controllers with active pods to avoid orphaning them. The *Delete* menu item was disabled for replication controllers when they have active replicas, but it was not obvious why. The web console now provides help text explaining as well as example commands for deleting from the CLI (which will scale the replication controller down automatically). (link:https://bugzilla.redhat.com/show_bug.cgi?id=1365582[*BZ#1365582*])

* This enhancement adds a cancel deployment link to the *Overview* page. The cancel deployment action could be difficult to discover on the deployment details page, so deployments can now be canceled directly from the *Overview*. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1365666[*BZ#1365666*])

* The web console did not set a `*TERM*` environment variable when the terminal execs into a pod using the `/bin/sh` command. This caused certain commands like `clear`, `less`, and `top` to not behave as expected. This bug fix sets the environment variable `*TERM=xterm*` when `/bin/sh` is used to connect to the pod. As a result, commands like `clear`, `less`, and `top` now behave properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1367337[*BZ#1367337*])

* In some cases, a warning could be resolved while the tooltip describing the warning was open. When this happened, the tooltip could not be dismissed. This bug fix updates the web console to now properly close the tooltip when the warning disappears, and as a result the open tooltip will disappear with the warning icon. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1347520[*BZ#1347520*])

* On the pod metrics tab in the web console, the available CPU and memory is shown for pods that have resource limits. If a pod was using more CPU or memory than its limit, the available amount would show as a negative value. This bug fix updates the web console to show the amount over the limit in these cases. As a result, negative values no longer display for available pod CPU and memory. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1369160[*BZ#1369160*])

*Metrics*

* The web console previously used the client's clock to calculate the start time for displaying metrics. If the client's clock was more than one hour faster than the server clock, an occur would occur when opening the metrics tab in the web console. The web console now uses the server time for calculating start and end times for metrics. As a result, metrics display properly even if the client clock is out of sync with the server. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1361061[*BZ#1361061*])

*Networking*

* The new unidling feature had a bug where it removed the service proxier when unidling was disabled, causing the service to not work. This bug fix addresses this issue, and the service now works properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1370435[*BZ#1370435*])

* When ipfailover was configured for the router, *keepalived* pods were previously being labeled with the selector of the router service. The router service then selected both router pods and *keepalived* pods. Because both types of pods use host networking by default, their IP addresses would be the same if deployed to the same hosts, and the service would appear to be selecting duplicate endpoints. This bug fix ensures that *keepalived* pods are now given a label that is distinct from that applied to the router pods. As a result, the router service no longer displays duplicate IP addresses when ipfailover is configured. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1365176[*BZ#1365176*])

*Quick Starts*

* This enhancement adds default resource limits to templates. Systems which require limits be set would prevent deployment of templates when the template did not specify resource limits. Templates can now be deployed on systems that require resource limits be specified. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1314899[*BZ#1314899*])

*REST API*

* Access to new endpoints was not automatically added to existing discovery roles during an upgrade. Checking the server version from the command line using `oc version` would display a forbidden error. This bug fix correctly adds permission to the new endpoint during an upgrade. As a result, `oc version` displays the server version as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1372579[*BZ#1372579*])

*Routing*

* Erroneous xref:../install_config/router/default_haproxy_router.adoc#preventing-connection-failures-during-restarts[Patch the Router Deployment Configuration to Create a Privileged Container] documentation caused pods to not have enough privilege to edit `iptables`. This bug fix updates the documentation with the correct procedure. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1269488[*BZ#1269488*])

* Multiple routers may be needed to support different features (sharding). This enhancement adds the ability to set the internal SNI port with an environment variable, allowing all ports to be changed so that multiple routers can be run on a single node. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1343083[*BZ#1343083*])

* Editing a route then deleting it and re-creating it caused the router to panic and crash. This was due to the deletion code leading to a different, unexpected state, with an empty array after an edit was made. This bug fix hardens the code to not result in that state and to tolerate the state should it accidentally occur. As a result, the router is more robust. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1371826[*BZ#1371826*])

* When an edge-terminated route had `*insecureEdgeTerminationPolicy*` set to `Allow` (meaning that the route could be accessed by both HTTP and HTTPS), the inserted session cookie was always flagged as Secure. When a client connected over HTTP, the secure cookie would be dropped, breaking session persistence. This bug fix ensures that cookies for edge-terminated routes that allow insecure connections are now set to be non-secure. As a result, session persistence for such routes is maintained. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1368525[*BZ#1368525*])

* The F5 iControl REST API usually returns JSON payloads in its responses, but it sometimes returns error responses with HTML payloads. In particular, it can return HTML payloads with HTTP 401 and 404 responses. Previously, the router would always try to decode the payload as JSON. If the F5 iControl REST API returned an HTML response, the router logs would show the following: "error: Decoder.Decode failed: invalid character '<' looking for beginning of value". This bug fix updates the F5 router plug-in to now gracefully handle HTML responses by ignoring the response payload for HTTP 4xx and 5xx responses if decoding as JSON fails. As a result, if the F5 iControl REST API returns an HTML response, the router logs will now show a message similar to the following: "error: HTTP code: 401." (link:https://bugzilla.redhat.com/show_bug.cgi?id=1316463[*BZ#1316463*])

* A comment in the *_haproxy-config.template_* file about creating back ends was incomplete, causing confusion. The comment has now been completed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1368031[*BZ#1368031*])

*Storage*

* A race condition in {product-title} (OCP) code could cause persistent volume (PV) objects to not be deleted when their retention policy was set to Delete and the appropriate persistent volume claim (PVC) was deleted. PV handling was rewritten in OCP 3.3, and as a result PVs are now deleted at the end of their lifetime. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1339154[*BZ#1339154*])

* A race condition in {product-title} (OCP) code could cause an AWS EBS volume not to be detached from a node when a pod that used the volume was terminated. The volume would be attached to the node forever, consuming AWS resources. This volume had to be detached manually. The code that attaches and detaches volume to and from nodes has been rewritten in OCP 3.3, and as a result AWS EBS volumes are now detached from nodes when the last pod that uses the volume is terminated. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1327384[*BZ#1327384*])

*Upgrade*

* Previous versions allowed the user to specify `*AWS_ACCESS_KEY_ID*` and `*AWS_SECRET_ACCESS_KEY*` in their *_/etc/sysconfig/_* files for {product-title} services. During upgrade, these files were updated according to a template, and if the user had not yet switched to using the new cloud provider framework their pre-existing AWS variables would be overwritten. The upgrade process has been modified to preserve these variables if they are present during upgrade, and a cloud provider is not configured. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1353354[*BZ#1353354*])

* Previously, a bug in a script that cleans out all pre-existing images and containers during a *docker* 1.10 upgrade would cause the script to miss some images with name and tag *none*, potentially resulting in a slower or failed *docker* upgrade. This script has been updated to use a more robust method of clean-up which also catches orphaned images. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1351406[*BZ#1351406*])

* Previously, nodes had their schedulability state reset to the state defined in the inventory used during an upgrade. If the scheduling state had been modified since the inventory file was created, this would be a surprise to administrators. The upgrade process has been modified to preserve the current schedulability state during upgrade so that nodes do not change state after an upgrade. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1372594[*BZ#1372594*])

[[ocp-33-technology-preview]]
== Technology Preview Features

Some features in this release are currently in Technology Preview. These
experimental features are not intended for production use. Please note the
following scope of support on the Red Hat Customer Portal for these features:

https://access.redhat.com/support/offerings/techpreview[Technology Preview
Features Support Scope]

The following features are in Technology Preview:

- xref:ocp-33-pipelines[OpenShift Pipelines]
- xref:../dev_guide/builds.adoc#extended-builds[Extended Builds]
- xref:../dev_guide/secrets.adoc#service-serving-certificate-secrets[Service Serving Certificate Secrets]
- Introduced in OpenShift Enterprise 3.1.1,
xref:../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[dynamic
provisioning] of persistent storage volumes from Amazon EBS, Google Compute
Disk, OpenStack Cinder storage providers remains in Technology Preview for
{product-title} 3.3.

////
[[ocp-33-known-issues]]
== Known Issues
////

[[ocp-33-asynchronous-errata-updates]]
== Asynchronous Errata Updates

Security, bug fix, and enhancement updates for {product-title} 3.3 are released
as asynchronous errata through the Red Hat Network. All {product-title} 3.3
errata is https://access.redhat.com/downloads/content/290/[available on the Red
Hat Customer Portal]. See the
https://access.redhat.com/support/policy/updates/openshift[{product-title}
Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account
settings for Red Hat Subscription Management (RHSM). When errata notifications
are enabled, users are notified via email whenever new errata relevant to their
registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming
OpenShift Enterprise entitlements for OpenShift Enterprise errata notification
emails to generate.
====

This section will continue to be updated over time to provide notes on
enhancements and bug fixes for future asynchronous errata releases of
{product-title} 3.3. Versioned asynchronous releases, for example with the form
{product-title} 3.3.z, will be detailed in subsections. In addition, releases in
which the errata text cannot fit in the space provided by the advisory will be
detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[upgrading
your cluster] properly.
====
