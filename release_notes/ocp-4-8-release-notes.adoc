[id="ocp-4-8-release-notes"]
= {product-title} {product-version} release notes
include::modules/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multi-tenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-8-about-this-release"]
== About this release

// TODO: Update k8s link once there is a version-specific URL for the Kubernetes release
{product-title} (link:https://access.redhat.com/errata/RHBA-2021:1234[RHBA-2021:1234]) is now available. This release uses link:https://kubernetes.io/docs/setup/release/notes/[Kubernetes 1.21] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

//Red Hat did not publicly release {product-title} 4.8.0 as the GA version and, instead, is releasing {product-title} 4.8.z as the GA version.

{product-title} {product-version} clusters are available at https://cloud.redhat.com/openshift. The {cloud-redhat-com} application for {product-title} allows you to deploy OpenShift clusters to either on-premise or cloud environments.

{product-title} {product-version} is supported on {op-system-base-full} 7.9 or later, as well as on {op-system-first} 4.8.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base-full} 7.9 or later for compute machines.
release-note-bux-fix-4.8
[IMPORTANT]
====
Because only {op-system-base} 7.9 or later is supported for compute machines, you must not upgrade the {op-system-base} compute machines to {op-system-base} 8.
====

//{product-title} 4.6 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

With the release of {product-title} 4.8, version 4.5 is now end of life. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift Container Platform Life Cycle Policy].

[id="ocp-4-8-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-8-rhcos"]
=== {op-system-first}

[id="ocp-4-8-rhcos-rhel-8-4-packages"]
==== {op-system} now supports RHEL 8.4

{op-system} is now using {op-system-base-full} 8.4 packages. This enables you to have the latest fixes, features, and enhancements, as well as the latest hardware support and driver updates. {product-title} 4.7 is currently using {op-system-base} 8.3 packages and will upgrade to {op-system-base} 8.4 in a future update. {product-title} 4.6 is an Extended Update Support (EUS) release that will continue to use {op-system-base} 8.2 EUS packages for the entirety of its lifecycle.

[id="ocp-4-8-stream-metadata"]
==== Using stream metadata for improved boot image automation

Stream metadata provides a standardized JSON format for injecting metadata into the cluster during {product-title} installation. For improved automation, the new `openshift-install coreos print-stream-json` command provides a method for printing stream metadata in a scriptable, machine-readable format.

For user-provisioned installations, the `openshift-install` binary contains references to the version of {op-system} boot images that are tested for use with {product-title}, such as the AWS AMI. You can now parse the stream metadata from a Go program by using the official `stream-metadata-go` library at https://github.com/coreos/stream-metadata-go.

For more information, see xref:../installing/installing_aws/installing-aws-user-infra.adoc#installation-aws-ami-stream-metadata_installing-aws-user-infra[Accessing {op-system} AMIs with stream metadata].

[id="ocp-4-8-rhcos-butane"]
==== Butane config transpiler simplifies creation of machine configs

{product-title} now includes the Butane config transpiler to assist with producing and validating machine configs. Documentation now recommends using Butane to create machine configs for LUKS disk encryption, boot disk mirroring, and custom kernel modules.

For more information, see xref:../installing/install_config/installing-customizing.adoc#installation-special-config-butane_installing-customizing[Creating machine configs with Butane].

[id="ocp-4-8-rhcos-chrony-default"]
==== Change to custom chrony.conf default on cloud platforms

If a cloud administrator has already set a custom `/etc/chrony.conf` configuration, {op-system} no longer sets the `PEERNTP=no` option by default on cloud platforms. Otherwise, the `PEERNTP=no` option is still set by default. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1924869[BZ#1924869] for more information.

[id="ocp-4-8-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-8-installation-azure-empty-rg"]
==== Installing a cluster to an existing, empty resource group on Azure

You can now define an already existing resource group to install your cluster to on Azure by defining the `platform.azure.resourceGroupName` field in the `install-config.yaml` file. This resource group must be empty and only used for a single cluster; the cluster components assume ownership of all resources in the resource group.

If you limit the service principal scope of the installation program to this resource group, you must ensure all other resources used by the installation program in your environment have the necessary permissions, such as the public DNS zone and virtual network. Destroying the cluster using the installation program deletes the user-defined resource group.

[id="ocp-4-8-installation-existing-iam-roles-aws"]
==== Using existing IAM roles for clusters on AWS

You can now define a pre-existing Amazon Web Services (AWS) IAM role for your machine instance profiles by setting the `compute.platform.aws.iamRole` and `controlPlane.platform.aws.iamRole` fields in the `install-config.yaml` file. This allows you to do the following for your IAM roles:

* Match naming schemes
* Include predefined permissions boundaries

[id="ocp-4-8-installation-pre-existing-hosted-zones-aws"]
==== Using pre-existing Route53 hosted private zones on AWS

You can now define an existing Route 53 private hosted zone for your cluster by setting the `platform.aws.hostedZone` field in the `install-config.yaml` file. You can only use a pre-existing hosted zone when also supplying your own VPC.

[id="ocp-4-8-installation-increase-gcp-subnets-within-machine-cidr"]
==== Increasing the size of GCP subnets within the machine CIDR

The {product-title} installation program for Google Cloud Platform (GCP) now creates subnets as large as possible within the machine CIDR. This allows the cluster to use a machine CIDR appropriately sized to accommodate the number of nodes in the cluster.

[id="ocp-4-8-improved-upgrade-duration"]
==== Improved upgrade duration

With this release, the upgrade duration for cluster Operators that deploy daemon sets to all nodes is significantly reduced. For example, the upgrade duration of a 250-node test cluster is reduced from 7.5 hours to 1.5 hours, resulting in upgrade duration scaling of less than one minute per additional node.

[NOTE]
====
This change does not affect machine config pool rollout duration.
====

[id="ocp-4-8-mco-upgrade-complete"]
==== MCO waits for all machine config pools to update before reporting the update is complete

When updating, the Machine Config Operator (MCO) now reports an `Upgradeable=False` status to the Cluster Version Operator (CVO) if any machine config pool has not completed updating. This status blocks future minor updates, but does not block future patch updates, or the current update. Previously, the MCO reported the `Upgradeable` status based only on the state of the master machine config pool, even if the worker pools were not done updating.

[id="ocp-4-8-installation-fujitsu-irmc"]
==== Using Fujitsu iRMC for installation on bare metal nodes

In {product-title} 4.8, you can use Fujitsu hardware and the Fujitsu iRMC base board management controller protocol when deploying installer-provisioned clusters on bare metal. Currently Fujitsu supports iRMC S5 firmware version `3.05P` and above for installer-provisioned installation on bare metal. Enhancements and bug fixes for {product-title} 4.8 include:

* Supporting soft power-off on iRMC hardware.

* Stopping the provisioning services once the installer deploys the control plane on the bare metal nodes. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1949859[BZ#1949859] for more information.

* Adding an Ironic health check to the bootstrap `keepalived` checks. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1949859[BZ#1949859] for more information.

* Verifying that the unicast peers list isn't empty on the control plane nodes. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1957708[BZ#1957708] for more information.

* Updating the Bare Metal Operator to align the iRMC PowerInterface. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1957869[BZ#1957869] for more information.

* Updating the `pyghmi` library version. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1920294[BZ#1920294] for more information.

* Updating the Bare Metal Operator to address missing IPMI credentials. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1965182[BZ#1965182] for more information.

* Removing iRMC from `enabled_bios_interfaces`. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1969212[BZ#1969212] for more information.

* Adding `ironicTlsMount` and `inspectorTlsMount` to the
the bare metal pod definition. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1968701[BZ#1968701] for more information.

* Disabling the RAID feature for iRMC server. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1969487[BZ#1969487] for more information.

* Disabling RAID for all drivers. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1969487[BZ#1969487] for more information.

[id="ocp-4-8-installation-openstack-installer-provisioned-sr-iov"]
==== SR-IOV network support for clusters with installer-provisioned infrastructure on {rh-openstack}

You can now deploy clusters on {rh-openstack} that use single-root I/O virtualization (SR-IOV) networks for compute machines.

See xref:../installing/installing_openstack/installing-openstack-installer-sr-iov.adoc#installing-openstack-installer-sr-iov[
Installing a cluster on OpenStack that supports SR-IOV-connected compute machines
] for more information.

[id="installation-ironic-agent-vlan-support"]
==== Ironic Python Agent support for VLAN interfaces

With this update, the Ironic Python Agent now reports VLAN interfaces in the list of interfaces during introspection. Additionally, the IP address is included on the interfaces, which allows for proper creation of a CSR. As a result, a CSR can be obtained for all interfaces, including VLAN interfaces. For more information, see link:https://bugzilla.redhat.com/show_bug.cgi?id=1888712[BZ#1888712].

[id="ocp-4-8-osus"]
==== Over-the-air updates with the OpenShift Update Service

The OpenShift Update Service (OSUS) provides over-the-air updates to {product-title}, including Red Hat Enterprise Linux CoreOS (RHCOS). It was previously only accessible as a Red Hat hosted service located behind public APIs, but can now be installed locally. The OpenShift Update Service is composed of an Operator and one or more application instances and is now generally available in {product-title} 4.6 and higher.

For more information, see xref:../updating/understanding-the-update-service.adoc#understanding-the-update-service[Understanding the OpenShift Update Service].

[id="ocp-4-8-web-console"]
=== Web console
[id="ocp-4-8-custom-console-routes-use-custom-domains-cluster-api"]
==== Custom console routes now use the new CustomDomains cluster API

For `console` and `downloads` routes, custom routes functionality is now implemented to use the new `ingress` config route configuration API `spec.componentRoutes`. The Console Operator config already contained custom route customization, but for the `console` route only. The route configuration via `console-operator` config is being deprecated. Therefore, if the `console` custom route is set up in both the `ingress` config and `console-operator` config, then the new `ingress` config custom route configuration takes precedent.

For more information, see xref:../web_console/customizing-the-web-console.adoc#customizing-the-web-console-url_customizing-web-console[Customizing console routes].

[id="ocp-4-8-access-code-snippet-from-quick-start"]
==== Access a code snippet from a Quick Start

You can now execute a CLI snippet when it is included in a Quick Start from the web console. To use this feature, you must first install the Web Terminal Operator. The web terminal and code snippet actions that execute in the web terminal are not present if you do not install the Web Terminal Operator. Alternatively, you can copy a code snippet to the clipboard regardless of whether you have the Web Terminal Operator installed or not.

[id="ocp-4-8-improved-presentation-quick-start-prereqs"]
==== Improved presentation of Quick Start prerequisites

Previously, Quick Start prerequisites were displayed as combined text instead of a list on the Quick Start card. With scalability in mind, the prerequisites are now presented in a popover rather than on the card.

image::quick-start-prerequisites.png[Quick Start prerequisites displayed as popover,338,336]

[id="ocp-4-8-ibm-z"]
=== IBM Z and LinuxONE

With this release, IBM Z and LinuxONE are now compatible with {product-title} {product-version}. The installation can be performed with z/VM or {op-system-base} KVM. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_z/installing-ibm-z.adoc#installing-ibm-z[Installing a cluster with z/VM on IBM Z and LinuxONE]
* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc#installing-restricted-networks-ibm-z[Installing a cluster with z/VM on IBM Z and LinuxONE in a restricted network]
* xref:../installing/installing_ibm_z/installing-ibm-z-kvm.adoc#installing-ibm-z-kvm[Installing a cluster with {op-system-base} KVM on IBM Z and LinuxONE]
* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z-kvm.adoc#installing-restricted-networks-ibm-z-kvm[Installing a cluster with RHEL KVM on IBM Z and LinuxONE in a restricted network]

[discrete]
==== Notable enhancements

The following new features are supported on IBM Z and LinuxONE with {product-title} {product-version}:

* KVM on {op-system-base} 8.3 or later is supported as a hypervisor for user-provisioned installation of {product-title} {product-version} on IBM Z and LinuxONE. Installation with static IP addresses as well as installation in a restricted network are now also supported.
* Encrypting data stored in etcd.
* 4K FCP block device.
* Three-node cluster support.

[discrete]
==== Supported features

The following features are also supported on IBM Z and LinuxONE:

* Multipathing
* Persistent storage using iSCSI
* Persistent storage using local volumes (Local Storage Operator)
* Persistent storage using hostPath
* Persistent storage using Fibre Channel
* Persistent storage using Raw Block
* OVN-Kubernetes with an initial installation of {product-title} 4.8
* z/VM Emulated FBA devices on SCSI disks

These features are available only for {product-title} on IBM Z for {product-version}:

* HyperPAV enabled on IBM Z /LinuxONE for the virtual machines for FICON attached ECKD storage

[discrete]
==== Restrictions

Note the following restrictions for {product-title} on IBM Z and LinuxONE:

* {product-title} for IBM Z does not include the following Technology Preview features:
** Precision Time Protocol (PTP) hardware

* The following {product-title} features are unsupported:
** Automatic repair of damaged machines with machine health checking
** CodeReady Containers (CRC)
** Controlling overcommit and managing container density on nodes
** CSI volume cloning
** CSI volume snapshots
** FIPS cryptography
** Helm command-line interface (CLI) tool
** Multus CNI plug-in
** NVMe
** OpenShift Metering
** OpenShift Virtualization
** Tang mode disk encryption during {product-title} deployment

* Worker nodes must run {op-system-first}
* Persistent shared storage must be provisioned by using either NFS or other supported storage protocols
* Persistent non-shared storage must be provisioned using local storage, like iSCSI, FC, or using LSO with DASD, FCP, or EDEV/FBA

[id="ocp-4-8-ibm-power"]
=== IBM Power Systems

With this release, IBM Power Systems are now compatible with {product-title} {product-version}. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_power/installing-ibm-power.adoc#installing-ibm-power_installing-ibm-power[Installing a cluster on IBM Power Systems]
* xref:../installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc#installing-restricted-networks-ibm-power_installing-restricted-networks-ibm-power[Installing a cluster on IBM Power Systems in a restricted network]

[discrete]
==== Notable enhancements

The following new features are supported on IBM Power Systems with {product-title} {product-version}:

* Encrypting data stored in etcd
* Three-node cluster support
* Multus SR-IOV

[discrete]
==== Supported features

The following features are also supported on IBM Power Systems:

* Currently, five Operators are supported:
** Cluster-Logging-Operator
** Cluster-NFD-Operator
** Elastic Search-Operator
** Local Storage Operator
** SR-IOV Network Operator

* Multipathing
* Persistent storage using iSCSI
* Persistent storage using local volumes (Local Storage Operator)
* Persistent storage using hostPath
* Persistent storage using Fibre Channel
* Persistent storage using Raw Block
* OVN-Kubernetes with an initial installation of {product-title} 4.8
* 4K Disk Support
* NVMe

[discrete]
==== Restrictions

Note the following restrictions for {product-title} on IBM Power Systems:

* {product-title} for IBM Power Systems does not include the following Technology Preview features:
** Precision Time Protocol (PTP) hardware

* The following {product-title} features are unsupported:
** Automatic repair of damaged machines with machine health checking
** CodeReady Containers (CRC)
** Controlling overcommit and managing container density on nodes
** FIPS cryptography
** Helm command-line interface (CLI) tool
** OpenShift Metering
** OpenShift Virtualization
** Tang mode disk encryption during {product-title} deployment

* Worker nodes must run {op-system-first}
* Persistent storage must be of the Filesystem type that uses local volumes, Network File System (NFS), or Container Storage Interface (CSI)

[id="ocp-4-8-security"]
=== Security and compliance

[id="ocp-4-8-security-log-oauth-tokens"]
==== Audit logging for OAuth access token logout requests

The `Default` audit log policy now logs request bodies for OAuth access token creation (login) and deletion (logout) requests. Previously, deletion request bodies were not logged.

For more information about audit log policies, see xref:../security/audit-log-policy-config.adoc#audit-log-policy-config[Configuring the node audit log policy].

[id="ocp-4-8-security-wildcard-subjects-headless-services"]
==== Wildcard subject for service serving certificates for headless services

Generating a service serving certificate for headless services now includes a wildcard subject in the format of `*.<service.name>.<service.namespace>.svc`. This allows for TLS-protected connections to individual stateful set pods without having to manually generate certificates for these pods.

[IMPORTANT]
====
Because the generated certificates contain wildcard subjects for headless services, do not use the service CA if your client must differentiate between individual pods. In this case:

* Generate individual TLS certificates by using a different CA.
* Do not accept the service CA as a trusted CA for connections that are directed to individual pods and must not be impersonated by other pods. These connections must be configured to trust the CA that was used to generate the individual TLS certificates.
====

For more information, see xref:../security/certificates/service-serving-certificate.adoc#add-service-certificate_service-serving-certificate[Add a service certificate].

[id="ocp-4-8-security-oc-compliance-plug-in"]
==== The oc-compliance plug-in is now available

The xref:../security/compliance_operator/compliance-operator-understanding.adoc#understanding-compliance-operator[Compliance Operator] automates many of the checks and remediations for an {product-title} cluster. However, the full process of bringing a cluster into compliance often requires administrator interaction with the Compliance Operator API and other components. The `oc-compliance` plug-in is now available and makes the process easier.

For more information, see xref:../security/oc_compliance_plug_in/oc-compliance-plug-in-using.adoc#using-oc-compliance-plug-in[Using the `oc-compliance` plug-in]

[id="ocp-4-8-security-tls-security-profile-control-plane"]
==== TLS security profile for the Kubernetes control plane

The Kubernetes API server TLS security profile setting is now also honored by the Kubernetes scheduler and Kubernetes controller manager.

For more information, see xref:../security/tls-security-profiles.adoc#tls-security-profiles[Configuring TLS security profiles].

[id="ocp-4-8-security-tls-security-profile-kubelet"]
==== TLS security profile for the kubelet as a server

You can now set a TLS security profile for kubelet when it acts as an HTTP server for the Kubernetes API server.

For more information, see xref:../security/tls-security-profiles.adoc#tls-security-profiles[Configuring TLS security profiles].

[id="ocp-4-8-security-bcrypt-hashing"]
==== Support for `bcrypt` password hashing

Previously, the `oauth-proxy` command only allowed the use of SHA-1 hashed passwords in `htpasswd` files used for authentication. `oauth-proxy` now includes support for `htpasswd` entries that use `bcrypt` password hashing. For more information, see link:https://bugzilla.redhat.com/show_bug.cgi?id=1874322[BZ#1874322].

[id="ocp-4-8-managed-secure-boot"]
==== Enabling managed Secure Boot with installer-provisioned clusters

{product-title} 4.8 supports automatically turning on UEFI Secure Boot mode for provisioned control plane and worker nodes and turning it back off when removing the nodes. To use this feature, set the node's `bootMode` configuration setting to `UEFISecureBoot` in the `install-config.yaml` file. Red Hat only supports installer-provisioned installation with managed Secure Boot on 10th generation HPE hardware or 13th generation Dell hardware running firmware version `2.75.75.75` or greater. For additional details, see xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.html#configuring-managed-secure-boot-in-the-install-config-file_ipi-install-configuration-files[Configuring managed Secure Boot in the install-config.yaml file].

[id="ocp-4-8-networking"]
=== Networking

[id="ocp-4-8-dual-stack-support-bare-metal-ovn-kubernetes-network-provider"]
==== Dual-stack support on bare metal infrastructure with the OVN-Kubernetes cluster network provider

For clusters installed on bare metal infrastructure, the OVN-Kubernetes cluster network provider supports both IPv4 and IPv6 address families.

For clusters upgrading from previous versions of {product-title}, you must convert your cluster to support dual-stack networking. For more information, see xref:../networking/ovn_kubernetes_network_provider/converting-to-dual-stack.adoc#converting-to-dual-stack[Converting to IPv4/IPv6 dual stack networking].

[id="ocp-4-8-migrate-openshift-sdn-ovn-kubernetes-user-provisioned"]
==== Migrate from OpenShift SDN to OVN-Kubernetes on user-provisioned infrastructure

An OpenShift SDN cluster network provider migration to the OVN-Kubernetes cluster network provider is supported for user-provisioned clusters. For more information, see xref:../networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc#nw-ovn-kubernetes-migration-about_migrate-from-openshift-sdn[Migrate from the OpenShift SDN cluster network provider].

[id="ocp-4-8-openshift-sdn-egress-ips-balance"]
==== OpenShift SDN cluster network provider egress IP feature balances across nodes

The egress IP feature of OpenShift SDN now balances network traffic roughly equally across nodes for a given namespace, if that namespace is assigned multiple egress IP addresses. Each IP address must reside on a different node.
For more information, refer to xref:../networking/openshift_sdn/assigning-egress-ips.adoc#assigning-egress-ips[Configuring egress IPs for a project] for OpenShift SDN.

[id="ocp-4-8-network-policy-host-network-ingress-controllers"]
==== Network policy supports selecting host network Ingress Controllers

When using the OpenShift SDN or OVN-Kubernetes cluster network providers, you can select traffic from Ingress Controllers in a network policy rule regardless of whether an Ingress Controller runs on the cluster network or the host network.
In a network policy rule, the `network.openshift.io/policy-group: ingress` namespace selector matches traffic from an Ingress Controller.

In earlier releases of {product-title} the following limitations existed:

- A cluster using the OpenShift SDN cluster network provider could select traffic from an Ingress Controller on the host network only by applying the `network.openshift.io/policy-group: ingress` label to the `default` namespace.
- A cluster using the OVN-Kubernetes cluster network provider could not select traffic from an Ingress Controller on the host network.

For more information, refer to xref:../networking/network_policy/about-network-policy.adoc#about-network-policy[About network policy].

[id="ocp-4-8-network-policy-host-network-policy-group"]
==== Network policy supports selecting host network traffic

When using either the OVN-Kubernetes cluster network provider or the OpenShift SDN cluster network provider, you can use the `policy-group.network.openshift.io/host-network: ""` namespace selector to select host network traffic in a network policy rule.

[id="ocp-4-8-network-policy-audit-logs"]
==== Network policy audit logs

If you use the OVN-Kubernetes cluster network provider, you can enable audit logging for network policies in a namespace. The logs are in a syslog compatible format and can be saved locally, sent over a UDP connection, or directed to a UNIX domain socket. You can specify whether to log allowed, dropped, or both allowed and dropped connections.
For more information, see xref:../networking/network_policy/logging-network-policy.adoc#logging-network-policy[Logging network policy events].

[id="ocp-4-8-macvlan-multi-network-policy"]
==== Network policy support for macvlan additional networks

You can create network policies that apply to macvlan additional networks by using the `MultiNetworkPolicy` API, which implements the `NetworkPolicy` API.
For more information, see xref:../networking/multiple_networks/configuring-multi-network-policy.adoc#configuring-multi-network-policy[Configuring multi-network policy].

[id="ocp-4-8-supported-hardware-sriov"]
==== Supported hardware for SR-IOV

{product-title} {product-version} adds support for additional Intel and Mellanox hardware.

* Intel X710 and XL710 controllers
* Four Intel E810 family controllers: E810-CQDA2, E810-2CQDA2, E810-XXVDA2, E810-XXVDA4
* Mellanox ConnectX-5 Ex

For more information, see the xref:../networking/hardware_networks/about-sriov.adoc#supported-devices_about-sriov[supported devices].

[id="ocp-4-8-enhancements-sriov-network-operator"]
==== Enhancements to the SR-IOV Network Operator

The Network Resources Injector that is deployed with the Operator is enhanced to expose information about huge pages requests and limits with the Downward API. When a pod specification includes a huge pages request or limit, the information is exposed in the `/etc/podnetinfo` path.

For more information, see xref:../networking/hardware_networks/about-sriov.adoc#nw-sriov-hugepages_about-sriov[Huge pages resource injection for Downward API].

[id="ocp-4-8-tracking-network-flows"]
==== Tracking network flows

{product-title} {product-version} adds support for sending the metadata about network flows on the pod network to a network flows collector. The following protocols are supported:

* NetFlow

* sFlow

* IPFIX

Packet data is not sent to the network flows collector. Packet-level metadata such as the protocol, source address, destination address, port numbers, number of bytes, and other packet-level information is sent to the network flows collector.

For more information, see xref:../networking/ovn_kubernetes_network_provider/tracking-network-flows.adoc#tracking-network-flows[Tracking network flows].

[id="ocp-4-8-coredns-mdns"]
==== CoreDNS-mDNS no longer used to resolve node names to IP addresses

{product-title} 4.8 and later releases include functionality that uses cluster membership information to generate A/AAAA records. This resolves the node names to their IP addresses. Once the nodes are registered with the API, the cluster can disperse node information without using CoreDNS-mDNS. This eliminates the network traffic associated with multicast DNS.

[id="ocp-4-8-preparation-haproxy-2.2"]
==== Converting HTTP header names to support upgrading to {product-title} 4.8

{product-title} updated to HAProxy 2.2, which changes HTTP header names to lowercase by default, for example, changing `Host: xyz.com` to `host: xyz.com`. For legacy applications that are sensitive to the capitalization of HTTP header names, use the Ingress Controller `spec.httpHeaders.headerNameCaseAdjustments` API field to accommodate legacy applications until they can be fixed. Make sure to add the necessary configuration by using `spec.httpHeaders.headerNameCaseAdjustments` before upgrading {product-title} now that HAProxy 2.2 is available.

For more information, see xref:../networking/ingress-operator.adoc#nw-ingress-converting-http-header-case_configuring-ingress[Converting HTTP header case].


[id="ocp-4-8-ingress-configuring-global-access-gcp"]
==== Configuring global access for an Ingress Controller on GCP

{product-title} {product-version} adds support for the global access option for Ingress Controllers created on GCP with an internal load balancer. When the global access option is enabled, clients in any region within the same VPC network and compute region as the load balancer can reach the workloads running on your cluster.

For more information, see xref:../networking/ingress-operator.adoc#nw-ingress-controller-configuration-gcp-global-access_configuring-ingress[Configuring global access for an Ingress Controller on GCP].

[id="ocp-4-8-setting-ingress-configuring-thread-count"]
==== Setting Ingress Controller thread count

{product-title} 4.8 adds support for setting the thread count to increase the amount of incoming connections a cluster can handle.

For more information, see xref:../networking/ingress-operator.adoc#nw-ingress-setting-thread-count_configuring-ingress[Setting Ingress Controller thread count].

[id="ocp-4-8-ingress-configuring-proxy-protocol"]
==== Configuring the PROXY protocol for an Ingress Controller

{product-title} 4.8 adds support for configuring the PROXY protocol for the Ingress Controller on non-cloud platforms, specifically for `HostNetwork` or `NodePortService` endpoint publishing strategy types.

For more information, see xref:../networking/ingress-operator.adoc#nw-ingress-controller-configuration-proxy-protocol_configuring-ingress[Configuring PROXY protocol for an Ingress Controller].

[id="ocp-4-8-networking-ntp-on-control-plane"]
==== NTP servers on control plane nodes

In {product-title} {product-version}, installer-provisioned clusters can configure and deploy Network Time Protocol (NTP) servers on the control plane nodes and NTP clients on worker nodes. This enables workers to retrieve the date and time from the NTP servers on the control plane nodes, even when disconnected from a routable network. You can also configure and deploy NTP servers and NTP clients after deployment.

[id="ocp-4-8-enhancements-kuryr-kubernetes"]
==== Changes to default API load balancer management for Kuryr

In {product-title} 4.8 deployments on {rh-openstack-first} with Kuryr-Kubernetes, the API load balancer for the `default/kubernetes` service is no longer managed by the Cluster Network Operator (CNO), but instead by the kuryr-controller itself. This means that:

* When upgrading to {product-title} 4.8, the `default/kubernetes` service will have downtime.
+
[NOTE]
====
In deployments where no Open Virtual Network (OVN) Octavia is available, more downtime should be expected
====

* The `default/kubernetes` load balancer is no longer required to use the Octavia Amphora driver. Instead, OVN Octavia will be used to implement the `default/kubernetes` service if it is available in the OpenStack cloud.

[id="ocp-4-8-networking-enabling-provisioning-network-day2"]
==== Enabling a provisioning network after installation

The assisted installer and installer-provisioned installation for bare metal clusters provide the ability to deploy a cluster without a `provisioning` network. In {product-title} 4.8 and later, you can enable a `provisioning` network after installation by using the Cluster Baremetal Operator (CBO).

[id="ocp-4-8-configure-vips-to-run-on-control-plane"]
==== Configure network components to run on the control plane

If you need the virtual IP (VIP) addresses to run on the control plane nodes in a bare metal installation, you must configure the `apiVIP` and `ingressVIP` VIP addresses to run exclusively on the control plane nodes. By default, {product-title} allows any node in the worker machine configuration pool to host the `apiVIP` and `ingressVIP` VIP addresses. Because many bare metal environments deploy worker nodes in separate subnets from the control plane nodes, configuring the `apiVIP` and `ingressVIP` virtual IP addresses to run exclusively on the control plane nodes prevents issues from arising due to deploying worker nodes in separate subnets. For additional details, see xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.html#configure-network-components-to-run-on-the-control-plane_ipi-install-configuration-files[Configure network components to run on the control plane].

[id="ocp-4-8-networking-external-load-balancer"]
==== Configuring an external load balancer for apiVIP and ingressVIP traffic

In {product-title} 4.8, you can configure an external load balancer to handle `apiVIP` and `ingressVIP` traffic to the control plane of installer-provisioned clusters. External load balancing services and the control plane nodes must run on the same L2 network, and on the same VLAN when using VLANs to route traffic between the load balancing services and the control plane nodes.

[id="ocp-4-8-IPsec-support-dual-stack"]
==== OVN-Kubernetes IPsec support for dual-stack networking

{product-title} 4.8 adds OVN-Kubernetes IPsec support for clusters that are configured to use dual-stack networking.

[id="ocp-4-8-egress-router-ovn-kubernetes"]
==== Egress router CNI for OVN-Kubernetes

The egress router CNI plug-in is generally available. The Cluster Network Operator is enhanced to support an `EgressRouter` API object. The process for adding an egress router on a cluster that uses OVN-Kubernetes is simplified. When you create an egress router object, the Operator automatically adds a network attachment definition and a deployment. The pod for the deployment acts as the egress router.

For more information, see xref:../networking/ovn_kubernetes_network_provider/using-an-egress-router-ovn.adoc[Considerations for the use of an egress router pod].

[id="ocp-4-8-ip-failover-support"]
==== IP failover support on {product-title}

IP failover is now supported on {product-title} clusters on bare metal. IP failover uses Keepalived to host a set of externally accessible VIP addresses on a set of hosts. Each VIP is only serviced by a single host at a time. Keepalived uses the Virtual Router Redundancy Protocol (VRRP) to determine which host, from the set of hosts, services which VIP. If a host becomes unavailable, or if the service that Keepalived is watching does not respond, the VIP is switched to another host from the set. This means a VIP is always serviced as long as a host is available.

For more information, see xref:../networking/configuring-ipfailover.adoc#configuring-ipfailover[Configuring IP failover].

[id="ocp-4-8-ingress-controller-power-of-two-random-choices"]
==== Power of Two Random Choices balancing algorithm for Ingress Controller

{product-title} Ingress Controllers now use the link:https://www.haproxy.com/blog/power-of-two-load-balancing/[Power of Two Random Choices] balancing algorithm by default for more even balancing after reloading. As in earlier releases, the balancing algorithm for a specific route can still be configured using the `haproxy.router.openshift.io/balance` route annotation. For more information, see xref:../networking/routes/route-configuration.adoc#nw-route-specific-annotations_route-configuration[Route-specific annotations].

To revert the Ingress Controller back to use the Least Connections balancing algorithm, use the following `oc patch` command to specify `leastconn` as the default:

[source,terminal]
----
$ oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge --patch='{"spec":{"unsupportedConfigOverrides":{"loadBalancingAlgorithm":"leastconn"}}}'
----

[id="ocp-4-8-control-dns-pod-placement"]
==== Control DNS pod placement

In {product-title} 4.8, you can use a custom node selector and tolerations to configure the daemon set for CoreDNS to run or not run on certain nodes.

For more information, see xref:../networking/dns-operator.adoc#nw-controlling-dns-pod-placement_dns-operator[Controlling DNS pod placement].

[id="ocp-4-8-networking-openstack-provider-networks"]
==== Provider networks support for clusters that run on {rh-openstack}
{product-title} clusters on {rh-openstack-first} now support provider networks for all deployment types.

[id="ocp-4-8-configurable-headerBufferMaxRewriteByte-headerBufferBytes-parameters"]
==== Configurable tune.maxrewrite and tune.bufsize for HAProxy

Cluster Administrators can now set `headerBufferMaxRewriteByte` and `headerBufferBytes` Ingress Controller tuning parameters to configure `tune.maxrewrite `and `tune.bufsize` HAProxy memory options per-Ingress Controller.

See xref:../networking/ingress-operator.html#nw-ingress-controller-configuration-parameters_configuring-ingress[Ingress Controller configuration parameters] for more information.

[id="ocp-4-8-storage"]
=== Storage
[id="ocp-4-8-storage-gcp-pd-csi-ga"]
==== Persistent storage using GCP PD CSI driver operator is generally available
The Google Cloud Platform (GCP) persistent disk (PD) Container Storage Interface (CSI) driver is automatically deployed and managed on GCP environments, allowing you to dynamically provision these volumes without having to install the driver manually.
This feature was previously introduced as a Technology Preview feature in {product-title} 4.7 and is now generally available and enabled by default in {product-title} 4.8.

For more information, xref:../storage/container_storage_interface/persistent-storage-csi-gcp-pd.adoc#persistent-storage-csi-gcp-pd[GCP PD CSI Driver Operator].

[id="ocp-4-8-storage-azure-csi-tp"]
==== Persistent storage using the Azure Disk CSI Driver Operator (Technology Preview)
The Azure Disk CSI Driver Operator provides a storage class by default that you can use to create persistent volume claims (PVCs).
The Azure Disk CSI Driver Operator that manages this driver is in Technology Preview.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-azure.adoc#persistent-storage-csi-azure[Azure Disk CSI Driver Operator].

[id="ocp-4-8-storage-vsphere-csi-tp"]
==== Persistent storage using the vSphere CSI Driver Operator (Technology Preview)
The vSphere CSI Driver Operator provides a storage class by default that you can use to create persistent volume claims (PVCs).
The vSphere CSI Driver Operator that manages this driver is in Technology Preview.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere[vSphere CSI Driver Operator].

[id="ocp-4-8-storage-csi-migration"]
==== Automatic CSI migration (Technology Preview)
Starting with {product-title} 4.8, automatic migration for the following in-tree volume plug-ins to their equivalent CSI drivers is available as a Technology Preview feature:

* Amazon Web Services (AWS) Elastic Block Storage (EBS)

* OpenStack Cinder

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-migration.adoc#persistent-storage-csi-migration[Automatic CSI Migration].

[id="ocp-4-8-storage-aws-efs-removed-1st"]
==== External provisioner for AWS EFS (Technology Preview) feature has been removed
The Amazon Web Services (AWS) Elastic File System (EFS) Technology Preview feature has been removed and is no longer supported.

[id="ocp-4-8-storage-openstack-cinder-availability-zones"]
==== Improved control over Cinder volume availability zones for clusters that run on {rh-openstack}

You can now select availability zones for Cinder volumes during installation. You can also use Cinder volumes in particular availability zones for your xref:../registry/configuring_registry_storage/configuring-registry-storage-osp.adoc#installation-registry-osp-creating-custom-pvc_configuring-registry-storage-openstack[image registry].

[id="ocp-4-8-registry"]
=== Registry


[id="ocp-4-8-olm"]
=== Operator lifecycle

[id="ocp-4-8-admin-error-reporting"]
==== Enhanced error reporting for administrators

A cluster administrator using Operator Lifecycle Manager (OLM) to install an Operator can encounter error conditions that are related either to the current API or low-level APIs. Previously, there was little insight into why OLM could not fulfill a request to install or update an Operator. These errors could range from trivial issues like typos in object properties or missing RBAC, to more complex issues where items could not be loaded from the catalog due to metadata parsing.

Because administrators should not require understanding of the interaction process between the various low-level APIs or access to the OLM pod logs to successfully debug such issues, {product-title} 4.8 introduces the following enhancements in OLM to provide administrators with more comprehensible error reporting and messages:

[id="ocp-4-8-installplan-errors"]
Retrying install plans::
Install plans, defined by an `InstallPlan` object, can encounter transient errors, for example, due to API server availability or conflicts with other writers. Previously, these errors would result in the termination of partially-applied install plans that required manual cleanup. With this enhancement, the Catalog Operator now retries errors during install plan execution for up to one minute. The new `.status.message` field provides a human-readable indication when retries are occurring.

[id="ocp-4-8-operatorgroup-errors"]
Indicating invalid Operator groups::
Creating a subscription in a namespace with no Operator groups or multiple Operator groups would previously result in a stalled Operator installation with an install plan that stays in `phase=Installing` forever. With this enhancement, the install plan immediately transitions to `phase=Failed` so that the administrator can correct the invalid Operator group, and then delete and re-create the subscription again.

[id="ocp-4-8-candidate-operator-errors"]
Specific reporting when no candidate Operators found::
`ResolutionFailed` events, which are created when dependency resolution in a namespace fails, now provide more specific text when the namespace contains a subscription that references a package or channel that does not exist in the referenced catalog source. Previously, this message was generic:
+
[source,terminal]
----
no candidate operators found matching the spec of subscription '<name>'
----
+
With this enhancement, the messages are more specific:
+
.Operator does not exist
[source,terminal]
----
no operators found in package <name> in the catalog referenced by subscription <name>
----
+
.Catalog does not exist
[source,terminal]
----
no operators found from catalog <name> in namespace openshift-marketplace referenced by subscription <name>
----
+
.Channel does not exist
[source,terminal]
----
no operators found in channel <name> of package <name> in the catalog referenced by subscription <name>
----
+
.Cluster service version (CSV) does not exist
[source,terminal]
----
no operators found with name <name>.<version> in channel <name> of package <name> in the catalog referenced by subscription <name>
----

[id="ocp-4-8-osdk"]
=== Operator development

[id="ocp-4-8-pkgman-to-bundle"]
==== Migration of Operator projects from package manifest format to bundle format

Support for the legacy package manifest format for Operators is removed in {product-title} 4.8 and later. The bundle format is the preferred Operator packaging format for Operator Lifecycle Manager (OLM) starting in {product-title} 4.6. If you have an Operator project that was initially created in the package manifest format, which has been deprecated, you can now use the Operator SDK `pkgman-to-bundle` command to migrate the project to the bundle format.

For more information, see xref:../operators/operator_sdk/osdk-pkgman-to-bundle.adoc#osdk-pkgman-to-bundle[Migrating package manifest projects to bundle format].

[id="ocp-4-8-service-ca-operator-enhancement"]
==== Service-ca Operator enhancement
{product-title} 4.8 allows users to run `service-ca-operator` pods as a non-root user to suit their organization's needs. When run as a non-root user, the `service-ca-operator runs with an user ID of `UID=1001(1001) GID =1001 groups=1001`. For more information, see link:https://bugzilla.redhat.com/show_bug.cgi?id=1914446[*BZ#1914446*].

[discrete]
[id="ocp-4-8-builds"]
=== Builds

// https://bugzilla.redhat.com/show_bug.cgi?id=1958367 4.8
// https://bugzilla.redhat.com/show_bug.cgi?id=1969963 4.7 errata
[id="ocp-4-8-builds-telemetry-metric"]
==== New Telemetry metric for number of builds by strategy
Telemetry includes a new `openshift:build_by_strategy:sum` gauge metric, which sends the number of builds by strategy type to the Telemeter Client. This metric gives site reliability engineers (SREs) and product managers visibility into the kinds of builds that run on {product-title} clusters.

// https://bugzilla.redhat.com/show_bug.cgi?id=1895053
[id="ocp-4-8-builds-mount-custom-pki-ca"]
==== Mount custom PKI certificate authorities
Previously, builds could not use the cluster PKI certificate authorities that were sometimes required to access corporate artifact repositories. Now, you can configure the `BuildConfig` object to mount cluster custom PKI certificate authorities by setting `mountTrustedCA` to `true`.


[id="ocp-4-8-images"]
=== Images


[id="ocp-4-8-machine-api"]
=== Machine API

[id="ocp-4-8-vsphere-machine-autoscaler-to-from-zero"]
==== Scaling machines running in vSphere to and from zero with the cluster autoscaler

When running machines in vSphere, you can now set the `minReplicas` value to `0` in the `MachineAutoscaler` resource definition. When this value is set to `0`, the cluster autoscaler scales the machine set to and from zero depending on if the machines are in use. For more information, see the xref:../machine_management/applying-autoscaling.html#machine-autoscaler-cr_applying-autoscaling[MachineAutoscaler resource definition].

[id="ocp-4-8-mco-no-reboot-cert"]
==== Automatic rotation of kubelet-ca.crt does not require node draining or reboot

The automatic rotation of the `/etc/kubernetes/kubelet-ca.crt` certificate authority (CA) no longer requires the Machine Config Operator (MCO) to drain nodes or reboot the cluster.

As part of this change, the following modifications do not require the MCO to drain nodes:

** changes to the SSH key in the `spec.config.ignition.passwd.users.sshAuthorizedKeys` parameter of a machine config
** changes to the global pull secret or pull secret in the `openshift-config` namespace

When the MCO detects any of these changes, it applies the changes and uncordons the node.

For more information, see xref:../architecture/control-plane.adoc#understanding-machine-config-operator_control-plane[Understanding the Machine Config Operator].

[id="ocp-4-8-machine-set-policy-enhancement"]
==== Machine set policy enhancement

Previously, creating machine sets required users to manually configure their CPU pinning settings, NUMA pinning settings, and CPU topology changes to get better performance from the host. With this enhancement, users can select a policy in the `MachineSet` resource to populate settings automatically. For more information, see link:https://bugzilla.redhat.com/show_bug.cgi?id=1941334[BZ#1941334].

[id="ocp-4-8-machine-set-hugepage-enhancement"]
==== Machine set hugepage enhancement

Providing a `hugepages` property into the `MachineSet` resource is now possible. This enhancement creates the `MachineSet` resource's nodes with a custom property in oVirt and instructs those nodes to use the `hugepages` of the hypervisor. For more information, see  link:https://bugzilla.redhat.com/show_bug.cgi?id=1948963[BZ#1948963].

[id="ocp-4-8-nodes"]
=== Nodes

[id="ocp-4-8-nodes-descheduler-apigroup"]
==== Descheduler operator.openshift.io/v1 API group is now available

The `operator.openshift.io/v1` API group is now available for the descheduler. Support for the `operator.openshift.io/v1beta1` API group for the descheduler might be removed in a future release.

[id="ocp-4-8-nodes-descheduler-metrics"]
==== Prometheus metrics for the descheduler

You can now enable Prometheus metrics for the descheduler by adding the `openshift.io/cluster-monitoring=true` label to the `openshift-kube-descheduler-operator` namespace where you installed the descheduler.

The following descheduler metrics are available:

* `descheduler_build_info` - Provides build information about the descheduler.
* `descheduler_pods_evicted` - Provides the number of pods that have been evicted for each combination of strategy, namespace, and result. There must be at least one evicted pod for this metric to appear.

[id="ocp-4-8-nodes-huge-pages-downward-api"]
==== Support for huge pages with the Downward API

With this release, when you set requests and limits for huge pages in a pod specification, you can use the Downward API to view the allocation for the pod from within a container. This enhancements relies on the `DownwardAPIHugePages` feature gate. {product-title} {product-version} enables the feature gate.

For more information, see xref:../scalability_and_performance/what-huge-pages-do-and-how-they-are-consumed-by-apps.adoc#consuming-huge-pages-resource-using-the-downward-api_huge-pages[Consuming huge pages resources using the Downward API].

[id='ocp-4-8-nodes-nfd-operator-new-labels_{context}']
==== New labels for the Node Feature Discovery Operator

The Node Feature Discovery (NFD) Operator detects hardware features available on each node in an {product-title} cluster. Then, it modifies node objects with node labels. This enables the NFD Operator to advertise the features of specific nodes. {product-title} {product-version} supports three additional labels for the NFD Operator.

- `pstate intel-pstate`: When the Intel `pstate` driver is enabled and in use, the `pstate intel-pstate` label reflects the status of the Intel `pstate` driver. The status is either `active` or `passive`.

- `pstate scaling_governor`: When the Intel `pstate` driver status is `active`, the `pstate scaling_governor` label reflects the scaling governor algorithm. The algorithm is either `powersave` or `performance`.

- `cstate status`: If the `intel_idle` driver has C-states or idle states, the `cstate status` label is `true`. Otherwise, it is `false`.

[id="ocp-4-8-nodes-no-reboot-cert"]
==== Automatic rotation of kubelet-ca.crt does not require reboot

The automatic rotation of the `/etc/kubernetes/kubelet-ca.crt` certificate authority (CA) no longer requires the Machine Config Operator (MCO) to drain nodes or reboot the cluster.

As part of this change, the following modifications do not require the MCO to drain nodes:

** changes to the SSH key in the `spec.config.ignition.passwd.users.sshAuthorizedKeys` parameter of a machine config
** changes to the global pull secret or pull secret in the `openshift-config` namespace

When the MCO detects any of these changes, it applies the changes and uncordons the node.

For more information, see xref:../architecture/control-plane.adoc#understanding-machine-config-operator_control-plane[Understanding the Machine Config Operator].

[id="ocp-4-8-nodes-vpa-ga"]
==== Vertical pod autoscaling is generally available

The {product-title} vertical pod autoscaler (VPA) is now generally available. The VPA automatically reviews the historic and current CPU and memory resources for containers in pods and can update the resource limits and requests based on the usage values it learns.

You can also use the VPA with pods that require only one replica by modifying the `VerticalPodAutoscalerController` object as described below. Previously, the VPA worked only with pods that required two or more replicas.

For more information, see xref:../nodes/pods/nodes-pods-vertical-autoscaler.adoc#nodes-pods-vertical-autoscaler[Automatically adjust pod resource levels with the vertical pod autoscaler].

[id="ocp-4-8-nodes-vpa-replicas"]
==== Vertical pod autoscaling minimum can be configured

By default, workload objects must specify a minimum of two replicas in order for the VPA to automatically update pods. As a result, workload objects that specify fewer than two replicas are not acted upon by the VPA. You can change this cluster-wide minimum value by modifying the `VerticalPodAutoscalerController` object to add the `minReplicas` parameter.

For more information, see xref:../nodes/pods/nodes-pods-vertical-autoscaler.adoc#nodes-pods-vertical-autoscaler[Automatically adjust pod resource levels with the vertical pod autoscaler].

[id="ocp-4-8-nodes-resources-configuring-auto_{context}"]
==== Automatically allocate CPU and memory resources for nodes

{product-title} can automatically determine the optimal sizing value of the `system-reserved` setting when a node starts. Previously, the CPU and memory allocations in the `system-reserved` setting were fixed limits that you needed to manually determine and set.

When automatic resource allocation is enabled, a script on each node calculates the optimal values for the respective reserved resources based on the installed CPU and memory capacity on the node.

For more information, see xref:../nodes/nodes/nodes-nodes-resources-configuring.adoc#nodes-nodes-resources-configuring-auto_nodes-nodes-resources-configuring[Automatically allocating resources for nodes].

[id="ocp-4-8-nodes-registry-allowed-repo_{context}"]
==== Adding specific repositories to pull images

You can now specify an individual repository within a registry when creating lists of allowed and blocked registries for pulling and pushing images. Previously, you could specify only a registry.

For more information, see xref:../openshift_images/image-configuration.adoc#images-configuration-allowed_image-configuration[Adding specific registries] and xref:../openshift_images/image-configuration.adoc#images-configuration-blocked_image-configuration[Blocking specific registries].

[id="ocp-4-8-nodes-cronjob-qa_{context}"]
==== Cron jobs are generally available

The cron job custom resource is now generally available. As part of this change, a new controller has been implemented that substantially improves the performance of cron jobs. For more information on cron jobs, see xref:../nodes/jobs/nodes-nodes-jobs.adoc#nodes-nodes-jobs-about_nodes-nodes-jobs[Understanding jobs and cron jobs].

[id="ocp-4-8-logging"]
=== Red Hat OpenShift Logging

In {product-title} 4.7, _Cluster Logging_ became _Red Hat OpenShift Logging_. For more information, see xref:../logging/cluster-logging-release-notes.adoc[Release notes for Red Hat OpenShift Logging].

[id="ocp-4-8-monitoring"]
=== Monitoring

[id="ocp-4-8-monitoring-alerting-rule-changes"]
==== Alerting rule changes

{product-title} 4.8 includes the following alerting rule changes:

.Alerting rule changes
[%collapsible]
====
* The `ThanosSidecarPrometheusDown` alert severity is updated from _critical_ to _warning_.
* The `ThanosSidecarUnhealthy` alert severity is updated from _critical_ to _warning_.
* The `ThanosQueryHttpRequestQueryErrorRateHigh` alert severity is updated from _critical_ to _warning_.
* The `ThanosQueryHttpRequestQueryRangeErrorRateHigh` alert severity is updated from _critical_ to _warning_.
* The `ThanosQueryInstantLatencyHigh` critical alert is removed. This alert fired if Thanos Querier had a high latency for instant queries.
* The `ThanosQueryRangeLatencyHigh` critical alert is removed. This alert fired if Thanos Querier had a high latency for range queries.
* For all Thanos Querier alerts, the `for` duration is increased to 1 hour.
* For all Thanos sidecar alerts, the `for` duration is increased to 1 hour.

[NOTE]
=====
Red Hat does not guarantee backward compatibility for metrics, recording rules, or alerting rules.
=====
====

[id="ocp-4-8-monitoring-removed-API-alerts"]
==== Alerts and information on APIs in use that will be removed in the next release

{product-title} 4.8 introduces two new alerts that fire when an API that will be removed in the next release is in use:

* `APIRemovedInNextReleaseInUse` - for APIs that will be removed in the next {product-title} release.
* `APIRemovedInNextEUSReleaseInUse` - for APIs that will be removed in the next {product-title} link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[Extended Update Support] (EUS) release.

You can use the new `APIRequestCount` API to track what is using the deprecated APIs. This allows you to plan whether any actions are required in order to upgrade to the next release.

[id="ocp-4-8-monitoring-stack-dependency-version-updates"]
==== Version updates to monitoring stack components and dependencies

{product-title} 4.8 includes version updates to the following monitoring stack components and dependencies:

* The Prometheus Operator is now on version 0.48.1.
* Prometheus is now on version 2.26.1.
* The `node-exporter` agent is now on version 1.1.2.
* Thanos is now on version 0.20.2.
* Grafana is now on version 7.5.5.

[id="ocp-4-8-monitoring-kube-state-metrics-v2.0.0-upgrade"]
==== kube-state-metrics upgraded to version 2.0.0

`kube-state-metrics` is upgraded to version 2.0.0. The following metrics were deprecated in `kube-state-metrics` version 1.9 and are effectively removed in version 2.0.0:

* Non-generic resource metrics for pods:
** kube_pod_container_resource_requests_cpu_cores
** kube_pod_container_resource_limits_cpu_cores
** kube_pod_container_resource_requests_memory_bytes
** kube_pod_container_resource_limits_memory_bytes
* Non-generic resource metrics for nodes:
** kube_node_status_capacity_pods
** kube_node_status_capacity_cpu_cores
** kube_node_status_capacity_memory_bytes
** kube_node_status_allocatable_pods
** kube_node_status_allocatable_cpu_cores
** kube_node_status_allocatable_memory_bytes

[id="ocp-4-8-monitoring-removed-grafana-alertmanager-ui-links"]
==== Removed Grafana and Alertmanager UI links

The link to the third-party Alertmanager UI is removed from the *Monitoring* -> *Alerting* page in the {product-title} web console. Additionally, the link to the third-party Grafana UI is removed from the *Monitoring* -> *Dashboards* page. You can still access the routes to the Grafana and Alertmanager UIs in the web console in the *Administrator* perspective by navigating to the *Networking* -> *Routes* page in the `openshift-monitoring` project.

[id="ocp-4-8-monitoring-dashboards-updates-web-console"]
==== Monitoring dashboard enhancements in the web console

New enhancements are available on the *Monitoring* -> *Dashboards* page in the {product-title} web console:

* When you zoom in on a single graph by selecting an area with the mouse, all other graphs now update to reflect the same time range.
* Dashboard panels are now organized into groups, which you can expand and collapse.
* Single-value panels now support changing color depending on their value.
* Dashboard labels now display in the *Dashboard* drop-down list.
* You can now specify a custom time range for a dashboard by selecting *Custom time range* in the *Time Range* drop-down list.
* When applicable, you can now select the *All* option in a dashboard filter drop-down menu to display data for all of the options in that filter.

[id="ocp-4-8-metering"]
=== Metering
The Metering Operator is deprecated as of {product-title} 4.6, and is scheduled to be removed in {product-title} 4.9.

[id="ocp-4-8-scale"]
=== Scale

[id="ocp-4-8-scale-running-in-a-single-node-cluster"]
==== Running on a single node cluster

Running tests on a single node cluster causes longer timeouts for certain tests, including SR-IOV and SCTP tests, and tests requiring master and worker nodes are skipped. Reconfiguration requiring node reboots causes a reboot of the entire environment, including the OpenShift control plane, and therefore takes longer to complete. All PTP tests requiring a master and worker node are skipped. No additional configuration is needed because the tests check for the number of nodes at startup and adjust test behavior accordingly.

PTP tests can run in Discovery mode. The tests look for a PTP master configured outside of the cluster. The following parameters are required:

* `ROLE_WORKER_CNF=master` - Required because master is the only machine pool to which the node will belong.
* `XT_U32TEST_HAS_NON_CNF_WORKERS=false` - Required to instruct the `xt_u32` negative test to skip because there are only nodes where the module is loaded.
* `SCTPTEST_HAS_NON_CNF_WORKERS=false` - Required to instruct the SCTP negative test to skip because there are only nodes where the module is loaded.

[id="ocp-4-8-reducing-nic-queues-using-the-performance-addon-operator"]
==== Reducing NIC using the Performance Addon Operator

The Performance Addon Operator allows you to adjust the Network Interface Card (NIC) queue count for each network device by configuring the performance profile. Device network queues allow packets to be distributed among different physical queues, and each queue gets a separate thread for packet processing.

For Data Plane Development Kit (DPDK) based workloads, it is important to reduce the NIC queues to only the number of reserved or housekeeping CPUs to ensure the desired low latency is achieved.

For more information, see xref:../scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc#reducing-nic-queues-using-the-performance-addon-operator_cnf-master[Reducing NIC queues using the Performance Addon Operator].

[id="ocp-4-8-scale-cluster-maximums"]
==== Cluster maximums

Updated guidance around xref:../scalability_and_performance/planning-your-environment-according-to-object-maximums.adoc#planning-your-environment-according-to-object-maximums[cluster maximums] for {product-title} {product-version} is now available.

[IMPORTANT]
====
No large scale testing for performance against OVN-Kubernetes testing was executed for this release.
====

Use the link:https://access.redhat.com/labs/ocplimitscalculator/[{product-title} Limit Calculator] to estimate cluster limits for your environment.

[id="ocp-4-8-performance-creator-tool"]
==== Creating a performance profile
You can now create a performance profile using the Performance Profile Creator (PPC) tool. The tool consumes `must-gather` data from the cluster and several user-supplied profile arguments, and using this information it generates a performance profile that is appropriate for your hardware and topology.

For more information, see xref:../scalability_and_performance/cnf-create-performance-profiles.adoc#cnf-create-performance-profiles[Creating a Performance Profile].

[id="ocp-4-8-nfd-operator"]
==== Node Feature Discovery Operator

The xref:../scalability_and_performance/psap-node-feature-discovery-operator.adoc#node-feature-discovery-operator[Node Feature Discovery (NFD) Operator] is now available. Use it to expose node-level information by orchestrating Node Feature Discovery, a Kubernetes add-on for detecting hardware features and system configuration.

[id="ocp-4-8-driver-toolkit"]
==== The Driver Toolkit

You can now use xref:../scalability_and_performance/psap-driver-toolkit.adoc#driver-toolkit[the Driver Toolkit] as a base image for driver containers so that you can enable special software and hardware devices on Kubernetes.

[id="ocp-4-8-dev-exp"]
=== Developer experience

[id="ocp-4-8-insights-operator"]
=== Insights Operator

[id="ocp-4-8-insights-operator-restricted-network"]
==== Insights Advisor recommendations for restricted networks

In {product-title} 4.8, users operating in restricted networks can gather and upload Insights Operator archives to Insights Advisor to diagnose potential issues. Additionally, users can obfuscate sensitive data contained in the Insights Operator archive before upload.

For more information, see xref:../support/remote_health_monitoring/remote-health-reporting-from-restricted-network.adoc[Using remote health reporting in a restricted network].

[id="ocp-4-8-insights-advisor-improvements"]
==== Insights Advisor improvements

Insights Advisor in the {product-title} web console now correctly reports 0 issues found. Previously, Insights Advisor gave no information.

[id="ocp-4-8-insights-operator-data-collection-enhancements"]
==== Insights Operator data collection enhancements

In {product-title} 4.8, the Insights Operator collects the following additional information:

* Non-identifiable cluster workload information to find known security and version issues.
* The `MachineHealthCheck` and `MachineAutoscaler` definitions.
* The `virt_platform` and `vsphere_node_hw_version_total` metrics.
* Information about unhealthy SAP pods to assist in the installation of SAP Smart Data Integration.
* The `datahubs.installers.datahub.sap.com` resources to identfy SAP clusters.
* A summary of failed `PodNetworkConnectivityChecks` to enhance networking.
* Information about the `cluster-version` pods and events from the `openshift-cluster-operator` namespace to debug issues with the `cluster-version` Operator.

With this additional information, Red Hat can provide improved remediation steps in Insights Advisor.

[id="ocp-4-8-insights-operator-gathering-sap"]
==== Insights Operator enhancement for unhealthy SAP pods

The Insights Operator can now gather data for unhealthy SAP pods. When the SDI installation fails, it is possible to detect the problem by looking at which of the initialization pods have failed. The Insights Operator now gathers information about failed pods in the SAP/SDI namespaces. For more information, see link:https://bugzilla.redhat.com/show_bug.cgi?id=1930393[BZ#1930393].

[id="ocp-4-8-gathering-sap-datahubs"]
==== Insights Operator enhancement for gathering SAP pod data

The Insights Operator can now gather `Datahubs` resources from SAP clusters. This data allows SAP clusters to be distinguished from non-SAP clusters in the Insights Operator archives, even in situations in which all of the data gathered exclusively from SAP clusters is missing and it would otherwise be impossible to determine if a cluster has an SDI installation. For more information, see link:https://bugzilla.redhat.com/show_bug.cgi?id=1940432[BZ#1940432].

[id="ocp-4-8-auth"]
=== Authentication and authorization

[id="ocp-4-8-auth-tp-aws-sts"]
==== Running {product-title} using AWS Security Token Service (STS) for credentials is generally available

You can now use the Cloud Credential Operator (CCO) utility (`ccoctl`) to configure the CCO to use the Amazon Web Services Security Token Service (AWS STS). When the CCO is configured to use STS, it assigns IAM roles that provide short-term, limited-privilege security credentials to components.

This feature was previously introduced as a Technology Preview feature in {product-title} 4.7, and is now generally available in {product-title} 4.8.

For more information, see xref:../authentication/managing_cloud_provider_credentials/cco-mode-sts.adoc#cco-mode-sts[Using manual mode with STS].

[id="ocp-4.8-sandboxed-containers"]
=== {sandboxed-containers-first}

[id="ocp-4.8-sandboxed-containers-tp"]
==== {sandboxed-containers-first} support on {product-title} (Technology Preview)

{sandboxed-containers-first} 1.0.0 Technology Preview release introduces built-in support for running Kata Containers as an additional runtime. {sandboxed-containers-first} enables users to choose Kata Containers as an additional runtime to provide additional isolation for their workloads. The {sandboxed-containers-operator} automates the tasks of installing, removing, and updating Kata Containers. It allows for tracking the state of those tasks by describing the `KataConfig` custom resource.

{sandboxed-containers-first} are only supported on bare metal. {op-system-first} is the only supported operating system for {sandboxed-containers-first} 1.0.0. Disconnected environments are not supported in {product-title} {product-version}.

For more information, see xref:../sandboxed_containers/understanding-sandboxed-containers.adoc#understanding-sandboxed-containers[Understanding OpenShift sandboxed containers]

[id="ocp-4-8-notable-technical-changes"]
== Notable technical changes

{product-title} 4.8 introduces the following notable technical changes.

[discrete]
[id="ocp-4-8-oauth-tokens"]
==== OAuth tokens without a SHA-256 prefix can no longer be used

Prior to {product-title} 4.6, OAuth access and authorize tokens used secret information for the object names.

Starting with {product-title} 4.6, OAuth access token and authorize token object names are stored as non-sensitive object names, with a SHA-256 prefix. OAuth tokens that do not contain a SHA-256 prefix can no longer be used or created in {product-title} 4.8.

[discrete]
[id="ocp-4-8-moderate-profiles"]
==== The Federal Risk and Authorization Management Program (FedRAMP) moderate controls

In {product-title} 4.8, the `rhcos4-moderate` profile is now complete. The `ocp4-moderate` profile will be completed in a future release.

[discrete]
[id="ocp-4-8-haproxy-2.2.13-upgrade"]
==== Ingress Controller upgraded to HAProxy 2.2.13

The {product-title} Ingress Controller is upgraded to HAProxy version 2.2.13.

[discrete]
[id="ocp-4-8-coreDNS-version-update"]
==== CoreDNS update to version 1.8.1

In {product-title} 4.8, CoreDNS uses version 1.8.1, which has several bug fixes, renamed metrics, and dual-stack IPv6 enablement.

[discrete]
[id="ocp-4-8-etcd-zap-logger"]
==== etcd now uses the zap logger

In {product-title} 4.8, etcd now uses zap as the default logger instead of capnslog. Zap is a structured logger that provides machine consumable JSON log messages. You can use `jq` to easily parse these log messages.

If you have a log consumer that is expecting the capnslog format, you might need to adjust it for the zap logger format.

.Example capnslog format ({product-title} 4.7)
[source,terminal]
----
2021-06-03 22:40:16.984470 W | etcdserver: read-only range request "key:\"/kubernetes.io/operator.openshift.io/clustercsidrivers/\" range_end:\"/kubernetes.io/operator.openshift.io/clustercsidrivers0\" count_only:true " with result "range_response_count:0 size:8" took too long (100.498102ms) to execute
----

.Example zap format ({product-title} 4.8)
[source,terminal]
----
{"level":"warn","ts":"2021-06-14T13:13:23.243Z","caller":"etcdserver/util.go:163","msg":"apply request took too long","took":"163.262994ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/kubernetes.io/namespaces/default\" serializable:true keys_only:true ","response":"range_response_count:1 size:53"}
----

[discrete]
[id="ocp-4-8-daemonset-merge-lso"]
==== Multiple daemon sets merged for LSO

In {product-title} 4.8, multiple daemon sets are merged for Local Storage Object (LSO). When you create a local volume custom resource, only `daemonset.apps/diskmaker-manager` is created.

[discrete]
[id="ocp-4-8-bound-svc-acct-token-vol"]
==== Bound service account token volumes are enabled

Previously, service account tokens were secrets that were mounted into pods. Starting with {product-title} 4.8, projected volumes are used instead. As a result of this change, service account tokens no longer have an underlying corresponding secret.

Bound service account tokens are audience-bound and time-bound. For more information, see xref:../authentication/bound-service-account-tokens.adoc#bound-service-account-tokens[Using bound service account tokens].

Additionally, the kubelet refreshes tokens automatically after they reach 80% of duration, and `client-go` watches for token changes and reloads automatically. The combination of these two behaviors means that most usage of bound tokens is no different from usage of legacy tokens that never expire. Non-standard usage outside of `client-go` might cause issues.

[discrete]
[id="ocp-4-8-operator-sdk-v-1-8-0"]
==== Operator SDK v1.8.0

{product-title} 4.8 supports Operator SDK v1.8.0. See xref:../cli_reference/osdk/cli-osdk-install.adoc#cli-osdk-install[Installing the Operator SDK CLI] to install or update to this latest version.

[NOTE]
====
Operator SDK v1.8.0 supports Kubernetes 1.20.
====

If you have any Operator projects that were previously created or maintained with Operator SDK v1.3.0, see xref:../operators/operator_sdk/osdk-upgrading-projects.adoc#osdk-upgrading-projects[Upgrading projects for newer Operator SDK versions] to ensure your projects are upgraded to maintain compatibility with Operator SDK v1.8.0.

[id="ocp-4-8-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more fine-grained functionality that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *TP*: _Technology Preview_
* *DEP*: _Deprecated_
* *REM*: _Removed_

//TODO: remove anything that has been REM since 4.6

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.6 |OCP 4.7 |OCP 4.8

|`OperatorSource` objects
|REM
|REM
|REM

|Package manifest format (Operator Framework)
|DEP
|DEP
|REM

|`oc adm catalog build`
|DEP
|DEP
|REM

|`--filter-by-os` flag for `oc adm catalog mirror`
|GA
|DEP
|REM

|v1beta1 CRDs
|DEP
|DEP
|

|Docker Registry v1 API
|DEP
|DEP
|

|Metering Operator
|DEP
|DEP
|DEP

|Scheduler policy
|GA
|DEP
|DEP

|`ImageChangesInProgress` condition for Cluster Samples Operator
|GA
|DEP
|

|`MigrationInProgress` condition for Cluster Samples Operator
|GA
|DEP
|

|Use of `v1` in `apiVersion` for {product-title} resources
|GA
|DEP
|

|Use of `dhclient` in {op-system-first}
|DEP
|DEP
|DEP

|Cluster Loader
|GA
|GA
|DEP

|External provisioner for AWS EFS
|REM
|REM
|REM

|`lastTriggeredImageID` field in the `BuildConfig` spec for Builds
|GA
|GA
|DEP

|Jenkins Operator
|TP
|TP
|DEP

|HPA custom metrics adapter based on Prometheus
|TP
|TP
|REM

|====


[id="ocp-4-8-deprecated-features"]
=== Deprecated features

[id="ocp-4-8-descheduler-apigroup-deprecated"]
==== Descheduler operator.openshift.io/v1beta1 API group is deprecated

The `operator.openshift.io/v1beta1` API group for the descheduler is deprecated and might be removed in a future release. Use the `operator.openshift.io/v1` API group instead.

[id="ocp-4-8-dhclient-deprecated"]
==== Use of dhclient in {op-system-first} is deprecated

Starting with {product-title} 4.6, {op-system-first} switched to using `NetworkManager` in the `initramfs` to configure networking during early boot. As part of this change, the use of the `dhclient` binary for DHCP was deprecated. Use the `NetworkManager` internal DHCP client for networking configuration instead. The `dhclient` binary will be removed from {op-system-first} in a future release. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1908462[BZ#1908462] for more information.

[id="ocp-4-8-cluster-loader-deprecated"]
==== Cluster Loader is deprecated

Cluster Loader is now deprecated and will be removed in a future release.

[id="ocp-4-8-builds-lasttriggeredimageid-parameter"]
==== The lastTriggeredImageID parameter in builds is deprecated

This release deprecates the `lastTriggeredImageID` in the `ImageChangeTrigger` object, which is one of the `BuildTriggerPolicy` types that can be set on a `BuildConfig` spec.

{product-title} version 4.9 will remove support for `lastTriggeredImageID` and ignore it. Then, image change triggers will not start a build based on changes to the `lastTriggeredImageID` field in the `BuildConfig` spec. Instead, the image IDs that trigger a build will be recorded in the status of the `BuildConfig` object, which most users cannot change.

Therefore, update scripts and jobs that inspect `buildConfig.spec.triggers[i].imageChange.lastTriggeredImageID` accordingly. (link:https://issues.redhat.com/browse/BUILD-213[*BUILD-213*])

[id="ocp-4-8-builds-jenkins-operator"]
==== The Jenkins Operator (Technology Preview) is deprecated

This release deprecates the Jenkins Operator, which was a Technology Preview feature. A future version of {product-title} will remove the Jenkins Operator from OperatorHub in the {product-title} web console interface. Then, upgrades for the Jenkins Operator will no longer be available, and the Operator will not be supported.

Customers can continue to deploy Jenkins on {product-title} using the templates provided by the Samples Operator.

[id="ocp-4-8-removed-features"]
=== Removed features
[id="ocp-4-8-storage-aws-efs-removed-2nd"]
.External provisioner for AWS EFS (Technology Preview) feature has been removed
The Amazon Web Services (AWS) Elastic File System (EFS) Technology Preview feature has been removed and is no longer supported.

[id="ocp-4-8-images-removed-from-samples-imagestreams"]
==== Images removed from samples imagestreams

The following images are no longer included in the samples imagestreams provided with {product-title}:

----
registry.redhat.io/rhscl/nodejs-10-rhel7
registry.redhat.io/ubi7/nodejs-10
registry.redhat.io/rhscl/perl-526-rhel7
registry.redhat.io/rhscl/postgresql-10-rhel7
registry.redhat.io/rhscl/ruby-25-rhel7
registry.redhat.io/ubi7/ruby-25
registry.redhat.io/rhdm-7/rhdm-decisioncentral-rhel8:7.9.0
registry.redhat.io/rhdm-7/rhdm-kieserver-rhel8:7.9.0
registry.redhat.io/rhpam-7/rhpam-businesscentral-monitoring-rhel8:7.9.0
registry.redhat.io/rhpam-7/rhpam-businesscentral-rhel8:7.9.0
registry.redhat.io/rhpam-7/rhpam-smartrouter-rhel8:7.9.0
----

[id="ocp-4-8-pkgman-format"]
==== Package manifest format for Operators no longer supported

Support for the legacy package manifest format for Operators is removed in {product-title} 4.8 and later. This removal of support includes custom catalogs that were built with the legacy format and Operator projects initially created in the legacy format with the Operator SDK. The bundle format is the preferred Operator packaging format for Operator Lifecycle Manager (OLM) starting in {product-title} 4.6.

For more information about working with the bundle format, see xref:../operators/admin/olm-managing-custom-catalogs.adoc#olm-managing-custom-catalogs[Managing custom catalogs] and xref:../operators/operator_sdk/osdk-pkgman-to-bundle.adoc#osdk-pkgman-to-bundle[Migrating package manifest projects to bundle format].

In addition, the following commands related to the format have been removed from OpenShift CLI (`oc`) and the Operator SDK CLI:

* `oc adm catalog build`
* `operator-sdk generate packagemanifest`
* `operator-sdk run packagemanifest`

[id="ocp-4-8-hpa-prometheus"]
==== Support for HPA custom metrics adapter based on Prometheus is removed

This release removes the Prometheus Adapter, which was a Technology Preview feature.

[id="ocp-4-8-bug-fixes"]
== Bug fixes
*api-server-auth*



*assisted-installer*



*Bare Metal Hardware Provisioning*

* Previously, the machine instance `state` annotation was not set. Consequently, the `STATE` column was empty. With this update, the machine instance `state` annotation is now set, and information in the `STATE` column automatically populates. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1857008[*BZ#1857008*])


*Builds*

* Previously, after CVE-2021-3344 was fixed, builds did not automatically mount entitlement keys on the {product-title} node. As a result, when the entitlement certificates were stored on the host or node, the fix prevented entitled builds from working seamlessly. The failure to bring in entitlement certificates stored on the host or node was fixed with link:https://bugzilla.redhat.com/show_bug.cgi?id=1945692[BZ#1945692] in 4.7.z and link:https://bugzilla.redhat.com/show_bug.cgi?id=1946363[BZ#1946363] in 4.6.z; however, those fixes introduced a benign warning message for builds running on {op-system-first} worker nodes. The current release fixes this issue by allowing builds to automatically mount entitlements only on {op-system-base} worker nodes, and avoiding mount attempts on {op-system} worker nodes. Now, there will not be any benign warnings around entitlement mounts when running builds on {op-system} nodes.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1951084[*BZ#1951084*])


* Some users pulling images from Docker Hub get a `container image registry lookup failed…​toomanyrequests: You have reached your pull rate limit` error. This error happens because the `docker.io` login they used to call the `oc new-app` does not have sufficient paid support with `docker.io`. The resulting application is subject to image pull throttling, which can produce failures. The current release updates the `oc new-app` help to remind users how defaulting works for the image registry and repository specs, so users can, when possible, use non-default image references to avoid similar errors.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1928850[*BZ#1928850*])


* Previously, builds did not perform an error check to see if an image push had failed. As a result, builds always logged the `Successfully pushed` message. Now, builds check if an error has occurred, and only log the `Successfully pushed` message after an image push has succeeded.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1947164[*BZ#1947164*])


* Previously, the documentation and `oc explain` help text did not convey that the `buildArgs` field in the `BuildConfig` object does not support the `valueFrom` field of its underlying Kubernetes `EnvVar` type. As a result, users believed it was supported and tried to use it. The current release updates the documentation and help text, so it is more apparent that the `BuildConfig` object's `buildArgs` field does not support the `valueFrom` field.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1956826[*BZ#1956826*])

* When builds interact with image registries, such as pulling base images, intermittent communications issues can produce build failures. The current release increases the number of retries to these interactions. Now, OpenShift builds are more resilient when they encounter intermittent communication issues with image registries.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1937535[*BZ#1937535*])

*Cloud Compute*

* Previously, a proxy update caused a full cluster configuration update, including API server restart, during continuous integration (CI) runs. Consequently, some clusters in the Machine API Operator would time out because of unexpected API server outages. This update separates proxy tests and adds postconditions so that clusters in the Machine API Operator become stable again during CI runs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1913341[*BZ#1913341*])

* Previously, deleting a machine stuck in `Insufficient disk space on datastore` took longer than expected because there was no distinction between various vCenter task types. With this update, the machine controller deletion procedure checks the vCenter task type, and no longer blocks the deletion of the machine controller. As a result, the machine controller is deleted quickly.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=1918101[*BZ#1918101*])

* Previously, scaling from zero annotations requeued even if the instance type was missing. Consequently, there were constant requeue and error space messages in the MachineSet controller logs. With this update, users can set the annotation manually if the instance type is not resolved automatically. As a result, scaling from zero for unknown instance types works if users manually provide the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1918910[*BZ#1918910*])

* Previously, HTTP responses were not closed properly by the Machine API termination handler. Consequently, goroutines were leaked in `net.http` read and write loops, which led to high memory usage. This update ensures that HTTP responses are always closed properly. As a result, memory usage is now stable. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1934021[*BZ#1934021*])

* Previously, multiple client sets created inside of the MachineSet controller caused slow startup times, which resulted in pods failing readiness checks in some large clusters. Consequently, the MachineSet controller would get stuck in an endless loop. This update fixes the MachineSet controller so that it uses a single client. As a result, the MachineSet controller behaves as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1934216[*BZ#1934216*])

* Previously, instances took longer to boot when an upgrade was performed by the Machine Config Daemon on the first boot. Consequently, worker nodes were stuck in restart loops, and machine healthchecks (MCHs) removed the worker nodes because they did not properly start. With this update, MHCs no longer remove nodes that have not started correctly. Instead, MHCs only remove nodes when explicitly requested. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1939054[*BZ#1939054*])

* Previously, a certificate signing request (CSR) approval was delayed for an unknown reason. Consequently, new machines appearing in the cluster during installation were not approved quickly enough, prolonging cluster installation. To mitigate occasional API server unavailability in early installation phases, this update changes the cache resync period from 10 hours to 10 minutes. As a result, master machines are now approved faster so that cluster installation is no longer prolonged. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1940972[*BZ#1940972*])

* Previously, the default Google Cloud Platform (GCP) image was out of date and referenced a version from the {product-title} 4.6 release that did not support newer Ignition versions. Consequently, new machines in clusters that used the default GCP image were not able to boot {product-title} 4.7 and later. With this update, the GCP image is updated to match the release version. As a result, new machines can now boot with the default GCP image. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1954597[*BZ#1954597*])

* Previously, due to a strict check of the virtual machine's (VM) ProvisioningState value, the VM would sometimes fail during an existence check. With this update, the check is more lenient so that only deleted machines go into a `Failed` phase during an existence check. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1957349[*BZ#1957349*])

* Previously, if you deleted a master machine using `oc delete machine` in an AWS cluster, the machine was not removed from the load balancers. As a result, the load balancer continued to serve requests to the removed master machine. With this fix, when you remove a master machine, the load balancer no longer serves requests to the machine. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1880757[*BZ#1880757*])

* Previously, when deleting an unreachable machine, the vSphere Virtual Machine Disk (VMDK) that is created for persistent volumes and attached to the node was incorrectly deleted. As a result, the data on the VMDK was unrecoverable. With this fix, the vSphere cloud provider checks for and detaches these disks from the node if the kubelet is not reachable. As a result, you can delete an unreachable machine without losing the VMDK. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1883993[*BZ#1883993*])

* Previously, because a generated list of AWS instance types was out of date, some newer Amazon Web Services (AWS) instance types were not able to scale from zero when using the Cluster Autoscaler Operator and machine sets with zero replicas. The list of AWS instance types is now updated to include newer instance types. With this fix, more instance types are available to the Cluster Autoscaler Operator for scaling from zero replicas. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1896321[*BZ#1896321*])

* Previously, pod disruption budgets did not drain pods on an unreachable node due to missing upstream eviction API features. As a result, machines on unreachable nodes could take excessive amounts of time to be removed after being deleted. Now, the grace period timeout is changed to 1 second when deleting machines on unreachable nodes. With this fix, the Machine API can successfully drain and delete nodes that are unreachable. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1905709[*BZ#1905709*])

*Cloud Credential Operator*

 * Previously, the Cloud Credential Operator repeated an `unsupported platform type: BareMetal` warning on bare metal platforms. With this update, bare metal platforms are no longer treated as unknown platforms. As a result, misleading logging messages are reduced. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1864116[*BZ#1864116*])

 * Previously, a recurring error message stored in the `credentialsRequest` custom resources (CRs) of the Cloud Credential Operator led to excessive CPU usage and logging in some error scenarios, such as Amazon Web Services (AWS) rate limiting. This update removes the request ID coming back from the cloud provider so that error messages are stored in conditions where users can more easily find them, and eliminates recurring error messages in the `credentialsRequest` CR. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1910396[*BZ#1910396*])

 * Previously, both the Cloud Credential Operator (CCO) and the Cluster Version Operator (CVO) reported if the CCO deployment was unhealthy. This resulted in double reporting if there was an issue. With this release, the CCO no longer reports if its deployment is unhealthy. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1957424[*BZ#1957424*])

*Cluster Version Operator*



*Compliance Operator*



*Console Kubevirt Plugin*



*Console Storage Plugin*



*Web console (Developer perspective)*



*DNS*



*etcd*



*Image Registry*



*ImageStreams*



*Insights Operator*

* Previously, the Insights Operator did not collect Cluster Version Operator (CVO) pods or events in the `openshift-cluster-version` namespace. As a result, the Insights Operator did not display information about any problems that the CVO might experience, and users could not get diagnostic information about the CVO. The Insights Operator is now updated to gather the CVO pods and events from the `openshift-cluster-operator` namespace so that issues with the the CVO are reported by the Insights Operator. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1942271[*BZ#1942271*])


*Installer*

//section 1

* Previously, the installer did not take into account the region where the bootstrap Ignition config should be located when generating its URL. Consequently, the bootstrap machine could not fetch the config from the provided URL because it was incorrect. This update takes the user's region into account when generating the URL and selects the correct public endpoint. As a result, the installer always generates correct bootstrap Ignition URLs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1934123[*BZ#1934123*])

* Previously, the default version of Azure's Minimum TLS was 1.0 when creating a storage account. Consequently, policy checks would fail. This update configures the openshift-installer Azure client to set the Minimum TLS version to 1.2 when creating a storage account. As a result, policy checks now pass. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1943157[*BZ#1943157*])
//section 2

*ISV Operators*


*kube-apiserver*

* Previously, Google Cloud Platform (GCP) load balancer health checkers left stale conntrack entries on the host, which caused network interruptions to the API server traffic that used the GCP load balancers. The health check traffic no longer loops through the host, so there is no longer network disruption against the API server. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1925698[*BZ#1925698*])

*Machine Config Operator*

* When upgrading from 4.6 to 4.7, the host name set by the *vsphere-hostname* service was applied only when a node was installed. If the host name was not statically set prior to upgrading, the host name could be lost. This update removes the condition which allowed the *vsphere-hostname* service to run only when a node is installed. As a result, vSphere host names are no longer lost when upgrading. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1942207[*BZ#1942207*])

* Due to a bug in Keepalived 2.0.10, if a liveness probe killed a Keepalived container, any virtual IP addresses (VIPs) that were assigned to the system remained and were not cleaned up when Keepalived restarted. As a result, multiple nodes could hold the same VIP. Now, the VIPs are removed when Keepalived is started. As a result, VIPs are held by a single node. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1931505[*BZ#1931505*])

* Previously, empty static pod files were being written to the `/etc/kubernetes/manifests` directory. As a result, the kubelet log was reporting errors that could cause confusion with some users. Empty manifests are now moved to a different location when they are not needed. As a result, the errors do not appear in the kubelet log. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1927042[*BZ#1927042*])

*Web console (Administrator perspective)*

//section 1

//section 2

//section 3

//section 4

*Metering Operator*

* Previously, the Reporting Operator incorrectly handled `Report` custom resources (CRs) that contained a user-provided retention period when reconciling events. Consequently, an expired `Report` CR would cause the Reporting Operator to continually loop, as the affected custom resources are requeued indefinitely.  This update avoids requeueing expired `Report` CRs that have specified a retention period. As a result, the Reporting Operator correctly handles events for expired `Report` CRs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1926984[*BZ#1926984*])

*Monitoring*

* Previously, the `mountstats` collector for the `node-exporter` daemontset caused high memory usage on nodes with NFS mount points. With this update, users can now disable the `mountstats` collector to reduce memory usage. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1955467[*BZ#1955467*])

*Networking*



*Node*

* Previously, the CRI-O logs did not contain information about the source where images were pulled from. With this fix, the log pull source is added to the info level info-level CRI-O logs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1881694[*BZ#1881694*])

* Previously, when pods were created and deleted rapidly, a pod might not have enough time to complete the pod sandbox creation before the pod started deletion. As a result, pod deletion could fail with a `ErrCreatePodSandbox`error. This error is now ignored if a pod is terminating. As a result, pod termination no longer fails if a pod could not complete the pod sandbox creation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1908378[*BZ#1908378*])

* Previously, the Machine Config Operator (MCO) did not accept *trace* as a valid log level. As a result, the MCO could not provide a method to enable trace-level logging, even though CRI-O supports it. The MCO is now updated to support the *trace* log level. As a result, users can see a trace log level through the MCO configurations. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1930620[*BZ#1930620*])

* Previously, the kubelet tried to get the status of images that are not completely pulled. As a result, `crictl` reports a `error locating item named "manifest"` error for these images. CRI-O is now updated to not list images that do not have a manifest. As a result, `crictl` no longer reports these errors. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1942608[*BZ#1942608*])

* Previously, outdated status messages were not removed. Because of this, the Machine Config Operator (MCO) was sometimes unable to locate the proper machine config pool. With this release, a cleanup function is added to limit the number of statuses. As a result,  the MCO keeps at most 3 different kubeletConfig status. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1950133[*BZ#1950133*])

* Previously, when upgrading from {product-title} version 4.6.25, in clusters with more than one `kubeletconfig` CR or `ContainerRuntimeConfig` CR, the Machine Config Operator (MCO) could generate duplicate machine configs for the same configuration. Consequently, the upgrade failed because the MCO would use the old controller version (IGNITIONVERSION 3.1.0). This update cleans up outdated duplicate machine configs and allows proper upgrading from version 4.6.25. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1955517[*BZ#1955517*])

*oauth-apiserver*

* Previously, some OAuth server metrics were not initialized properly and did not appear in searches in the Prometheus UI. The missing OAuth server metrics are now initialized properly and appear in the Prometheus UI metrics searches. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1892642[*BZ#1892642*])


*oc*



*OLM*

* Previously, `CustomResourceDefinition` (CRD) objects applied as part of an Operator installation could sometimes satisfy the installation requirements of a newer version of the same Operator. Consequently, during an Operator upgrade, the version being replaced could be prematurely removed. In some cases, the upgrade would stop. With this update, the CRDs that are created or updated as part of the Operator bundle installation are annotated to indicate their bundle of origin. These annotations are used by the `ClusterServiceVersion` (CSV) object to distinguish between pre-existing CRDs and same-bundle CRDs. As a result, upgrades will not complete until the current version's CRDs have been applied. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1947946[*BZ#1947946*])

* Previously, pods that ran an index referenced by a `CatalogSource` object did not have `readOnlyRootFileSystem: false` explicitly set in their `securityContext` field. Consequently, if an SCC existed that enforced `readOnlyRootFileSystem: true` and matched the `securityContext` of the that pod, it would be assigned to that pod and cause it to fail repeatedly. This update explicitly sets `readOnlyRootFileSystem: false` in the `securityContext` field. As a result, pods that are referenced by a `CatalogSource` object are no longer matched to SCCs that enforce a read-only root filesystem and no longer fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1961472[*BZ#1961472*])

*openshift-apiserver*



*Performance Addon Operator*



*RHCOS*



*Routing*

* Previously, the HAProxyDown alert message was vague. Consequently, end users thought the alert meant that router pods, instead of just HAProxy pods, were unavailable. This update makes the HAProxyDown alert message clearer. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1941592[*BZ#1941592*])

* Previously, HAProxy's helper function template that was responsible for generating a file for whitelist IPs expected a wrong argument type. Consequently, no whitelist ACL was applied for the backend in long IP lists. With this update, argument types of the helper function template are changed so that whitelist ACL is applied to the backend of long IP lists. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1964486[*BZ#1964486*])


*Samples*



*service-ca*



*Storage*



[id="ocp-4-8-bug-fixes-bare-metal-hardware-provisioning"]
=== Bare metal hardware provisioning

* Previously, Ironic failed to download an image for installation because Ironic uses HTTPS by default and did not have the correct certificate bundle available. This issue is fixed by setting the image download as insecure to request a transfer without the certificate. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1953795[*BZ#1953795*])

* Previously, when using dual-stack networking, worker node host names sometimes did not match the host name that Ironic inspected before deployment. This caused nodes to need manual approval. This has been fixed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1955114[*BZ#1955114*])

* Previously, in UEFI mode, the `ironic-python-agent` created a UEFI bootloader entry after downloading the RHCOS image. When using an RHCOS image based on RHEL 8.4, the image could fail to boot using this entry. If the entry installed by Ironic was used when booting the image, the boot could fail and output a BIOS error screen. This is fixed by the `ironic-python-agent` configuring the boot entry based on a CSV file located in the image, instead of using a fixed boot entry. The image boots properly without error. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1966129[*BZ#1966129*])

* Previously, a node sometimes selected the incorrect IP version upon startup (IPv6 instead of IPv4, or vice versa). The node would fail to start because it did not receive an IP address. This is fixed by the Cluster Bare Metal Operator passing the IP option to the downloader (`ip=dhcp` or `ip=dhcp6`), so this is set correctly at startup and the node starts as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1946079[*BZ#1946079*])

* Previously, the image caching mechanism in Ironic was disabled to enable a direct connection to the HTTP server that hosts the virtualmedial iso to prevent local storage issues. Non-standard compliant HTTP clients and redfish implementations caused failures on BMC connections. This has been fixed by reverting to the default Ironic behavior where the virtualmedia iso is cached and served from the Ironic conductor node. Issues caused by non-standard compliant HTTP clients and redfish implementations have been fixed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1962905[*BZ#1962905*])

[id="ocp-4-8-bug-fixes-assisted-installer"]
=== Assisted Installer

* Previously, the `assisted-service` container did not wait for `postgres` to start up and be ready to accept connections. The `assisted-service` container attempted to establish a database connection, failed, and the `assisted-service` container failed and restarted. This issue has been fixed by the `assisted-service` container attempting to connect to the database for up to 10 seconds. If `postgres` starts and is ready to accept connection within 10 seconds, the `assisted-service` container connects without going into an error state. If the `assisted-service` container is unable to connect to `postgres` within 10 seconds, it goes into an error state, restarts, and tries again. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1941859[*BZ#1941859*])

[id="ocp-4-8-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_
* *DEP*: _Deprecated_

//TODO: remove anything that has been GA since 4.6

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.6 |OCP 4.7 |OCP 4.8

|Precision Time Protocol (PTP)
|TP
|TP
|TP

|`oc` CLI plug-ins
|TP
|TP
|GA

|Descheduler
|TP
|GA
|GA

|OVN-Kubernetes Pod network provider
|GA
|GA
|GA

|HPA for memory utilization
|TP
|GA
|GA

|Service Binding
|TP
|GA
|

|Log forwarding
|GA
|GA
|GA

|Monitoring for user-defined projects
|GA
|GA
|GA

|Raw Block with Cinder
|TP
|TP
|

|CSI volume snapshots
|TP
|GA
|GA

|CSI volume cloning
|GA
|GA
|GA

|CSI volume expansion
|TP
|TP
|TP

|vSphere Problem Detector Operator
|-
|GA
|GA

|CSI Azure Disk Driver Operator
|-
|-
|TP

|CSI GCP PD Driver Operator
|-
|TP
|GA

|CSI OpenStack Cinder Driver Operator
|-
|TP
|TP

|CSI AWS EBS Driver Operator
|TP
|TP
|TP

|CSI automatic migration
|-
|-
|TP

|Red Hat Virtualization (oVirt) CSI Driver Operator
|GA
|GA
|GA

|CSI inline ephemeral volumes
|TP
|TP
|TP

|CSI vSphere Driver Operator
|-
|-
|TP

|Automatic device discovery and provisioning with Local Storage Operator
|TP
|TP
|TP

|OpenShift Pipelines
|TP
|TP
|GA

|OpenShift GitOps
|TP
|TP
|GA

|OpenShift sandboxed containers
|-
|-
|TP

|Vertical Pod Autoscaler
|TP
|TP
|GA

|Cron jobs
|TP
|TP
|GA

|PodDisruptionBudget
|TP
|TP
|GA

|Operator API
|GA
|GA
|GA

|Adding kernel modules to nodes with kvc
|TP
|TP
|TP

|Egress router CNI plug-in
|-
|TP
|GA

|Scheduler profiles
|-
|TP
|TP

|Non-preempting priority classes
|-
|TP
|TP

|Kubernetes NMState Operator
|-
|TP
|

|Assisted Installer
|-
|TP
|TP

|AWS Security Token Service (STS)
|-
|TP
|GA

|Kdump
|-
|TP
|TP

|OpenShift Serverless
|-
|-
|GA

|Serverless functions
|-
|-
|TP

|Jenkins Operator
|TP
|TP
|DEP

|====

[id="ocp-4-8-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.8 and beyond!
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to 4.8, you can either revoke or continue to allow unauthenticated access. It is recommended to revoke unauthenticated access unless there is a specific need for it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* When powering on a virtual machine on vSphere with user-provisioned infrastructure, the process of scaling up a node might not work as expected. A known issue in the hypervisor configuration causes machines to be created within the hypervisor but not powered on. If a node appears to be stuck in the `Provisioning` state after scaling up a machine set, you can investigate the status of the virtual machine in the vSphere instance itself. Use the VMware commands `govc tasks` and `govc events` to determine the status of the virtual machine. Check for a similar error message to the following:
+
[source,terminal]
----
[Invalid memory setting: memory reservation (sched.mem.min) should be equal to memsize(8192). ]
----
+
You can attempt to resolve the issue with the steps in this link:https://kb.vmware.com/s/article/2002779[VMware KBase article]. For more information, see the Red Hat Knowledgebase solution link:https://access.redhat.com/solutions/5785341[[UPI vSphere\] Node scale-up doesn't work as expected]. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1918383[*BZ#1918383*])
* The installation of {op-system} on a {op-system-base} KVM installation on IBM Z fails when using an ECKD type DASD as a VirtIO block device. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1960485[*BZ#1960485*])

* An Open Virtual Network (OVN) bug causes persistent connectivity issues with Octavia load balancers. When Octavia load balancers are created, OVN might not plug them into some Neutron subnets. These load balancers might be unreachable for some of the Neutron subnets. This problem affects Neutron subnets, which are created for each OpenShift namespace, at random when Kuryr is configured. As a result, when this problem occurs the load balancer that implements OpenShift `Service` objects will be unreachable from OpenShift namespaces affected by the issue. Because of this bug, {product-title} 4.8 deployments that use Kuryr SDN are not recommended on {rh-openstack-first} 16.1 with OVN and OVN Octavia configured until the bug is fixed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1937392[*BZ#1937392*])

* The Console Operator does not properly update the `Ingress` resource with the `componentRoutes` conditions for either of the console's routes (`console` or `downloads`). (link:https://bugzilla.redhat.com/show_bug.cgi?id=1954148[*BZ#1954148*])

* If you are using {sandboxed-containers-first}, you cannot use the `hostPath` volume in a {product-title} cluster to mount a file or directory from the host node’s file system into your pod.
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1904609[*BZ#1904609*])

* If you are running Fedora on {sandboxed-containers-first}, you need a workaround to install some packages. Some packages, like `iputils`, require file access permission changes that {product-title} does not grant to containers by default. To run containers that require such special permissions, it is necessary to add an annotation to the YAML file describing the workload, which tells `virtiofsd` to accept such file permissions for that workload. The required annotations are:
+
[source,yaml]
----
io.katacontainers.config.hypervisor.virtio_fs_extra_args: [ "-o", "modcaps=+sys_admin", "-o", "xattr" ]
----
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1915377[*BZ#1915377*])

* In the 4.8 release, adding a value to `kataConfgPoolSelector` by using the {product-title} web console causes `scheduling.nodeSelector` to be populated with an empty value. Pods that use `RuntimeClass` with the value of `kata` might be scheduled to nodes that do not have the Kata Containers runtime installed.
+
To work around this issue, specify the `nodeSelector` value manually in the `RuntimeClass` `kata` by running the following command:
+
[source,terminal]
----
$ oc edit runtimeclass kata
----
+
The following is an example of a `RuntimeClass` with the correct `nodeSelector` statement.
+
[source,yaml]
----
apiVersion: node.k8s.io/v1
handler: kata
kind: RuntimeClass
metadata:
  creationTimestamp: "2021-06-14T12:54:19Z"
  name: kata
overhead:
  podFixed:
    cpu: 250m
    memory: 350Mi
scheduling:
  nodeSelector:
    custom-kata-pool: "true"
----
+
(link:https://issues.redhat.com/browse/KATA-764[*KATA-764*])

* The {sandboxed-containers-operator} details page on Operator Hub contains a few missing fields. The missing fields do not prevent you from installing the {sandboxed-containers-operator} in 4.8.
+
(link:https://issues.redhat.com/browse/KATA-826[*KATA-826*])

* Creating multiple `KataConfig` custom resources results in a silent failure. The {product-title} web console does not provide a prompt to notify the user that creating more than one custom resource has failed.
+
(link:https://issues.redhat.com/browse/KATA-725[*KATA-725*])

* Sometimes the Operator Hub in the {product-title} web console does not display icons for an Operator.
+
(link:https://issues.redhat.com/browse/KATA-804[*KATA-804*])

* The OVN-Kubernetes network provider does not support the `externalTrafficPolicy` feature for `NodePort`- and `LoadBalancer`-type services.  The `service.spec.externalTrafficPolicy` field determines whether traffic for a service is routed to node-local or cluster-wide endpoints. Currently, such traffic is routed by default to cluster-wide endpoints, and there is no way to limit traffic to node-local endpoints. This will be resolved in a future release. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1903408[*BZ#1903408*])

* Currently, a Kubernetes port collision issue can cause a breakdown in pod-to-pod communication, even after pods are redeployed. For detailed information and a workaround, see the Red Hat Knowledge Base solution link:https://access.redhat.com/solutions/5940711[Port collisions between pod and cluster IPs on OpenShift 4 with OVN-Kubernetes]. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1939676[*BZ#1939676*], link:https://bugzilla.redhat.com/show_bug.cgi?id=1939045[*BZ#1939045*])

* The description for the `tlsSecurityProfile` field of the `kubeletconfig` resource (for example when using the `oc explain` command) does not list the correct ciphers for the TLS security profiles. As a workaround, review the list of ciphers in the `/etc/kubernetes/kubelet.conf` file of an affected node. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1971899[*BZ#1971899*])

* When running CNF tests in regular mode on a single node, the logic in place to understand if the cluster is ready is missing details. In particular, creating an SR-IOV network will not create a network attachment definition until at least one minute elapses. All the DPDK tests fail in cascade. Run the CNF tests in regular mode skipping the DPDK feature when running against an installation on a single node, with the `-ginkgo.skip` parameter. Run CNF tests in Discovery mode to execute tests against an installation on a single node. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1970409[*BZ#1970409*])

* Currently, CNF-tests does not support secure boot with MLX NICs for SR-IOV and DPDK tests. You can run the CNF tests skipping the SR-IOV feature when running against a secure boot enabled environment in regular mode, with the `-ginkgo.skip` parameter. Running in Discovery mode is the recommended way to execute tests against a secure boot enabled environment with MLX cards. This will be resolved in a future release. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1975216[*BZ#1975216*])

* When the `ArgoCD` Operator is subscribed to and an ArgoCD and AppProject are started, launching the example application named `guestbook` fails because the image does not work in more restrictive {product-title} environments. As a temporary workaround, users can ensure the `ArgoCD` Operator works properly by deploying the following example:
+
[source,yaml]
----
PROJ=younamespace
cat > $PROJ-app.yaml <<EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: simple-restricted-webserver
  namespace: $PROJ
spec:
  destination:
    namespace: $PROJ
    server: https://kubernetes.default.svc
  project: default
  source:
    path: basic-nginx
    repoURL: 'https://github.com/opdev/argocd-example-restricted-apps.git'
    targetRevision: HEAD
EOF
oc create -f $PROJ-app.yaml
----

For more information, see link:https://bugzilla.redhat.com/show_bug.cgi?id=1812212[*BZ#1812212*].


[id="ocp-4-8-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} 4.8 are released as asynchronous errata through the Red Hat Network. All {product-title} 4.8 errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified via email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} 4.8. Versioned asynchronous releases, for example with the form {product-title} 4.8.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster.adoc#updating-cluster[updating your cluster] properly.
====

[id="ocp-4-8-0-ga"]
=== RHBA-2021:1234 - {product-title} 4.8.0 image release, bug fix, and security update advisory

Issued: 2021-xx-xx

{product-title} release 4.8.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2021:1234[RHBA-2021:1234] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2021:5678[RHBA-2021:5678] advisory.

Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:

link:https://access.redhat.com/solutions/<ARTICLE_ID>[{product-title} 4.8.0 container image list]
