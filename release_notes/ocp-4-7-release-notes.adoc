[id="ocp-4-7-release-notes"]
= {product-title} {product-version} release notes
include::modules/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multi-tenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-7-about-this-release"]
== About this release

// TODO: Update k8s link once there is a version-specific URL for the Kubernetes release
(link:https://access.redhat.com/errata/RHBA-2020:1234[RHBA-2020:1234]) is now available. This release uses link:https://kubernetes.io/docs/setup/release/notes/[Kubernetes 1.20] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

//Red Hat did not publicly release {product-title} 4.6.0 as the GA version and, instead, is releasing {product-title} 4.6.1 as the GA version.

{product-title} {product-version} clusters are available at https://cloud.redhat.com/openshift. The {cloud-redhat-com} application for {product-title} allows you to deploy OpenShift clusters to either on-premise or cloud environments.

{product-title} {product-version} is supported on {op-system-base-full} 7.7 or later, as well as {op-system-first} 4.6.

You must use {op-system} machines for the control plane, which are also known as master machines, and you can use either {op-system} or {op-system-base-full} 7.7 or later for compute machines, which are also known as worker machines.

[IMPORTANT]
====
Because only {op-system-base-full} version 7.7 or later is supported for compute machines, you must not upgrade the {op-system-base} compute machines to version 8.
====

//{product-title} 4.6 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

With the release of {product-title} 4.7, version 4.4 is now end of life. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift Container Platform Life Cycle Policy].

[id="ocp-4-7-inclusive-language"]
== Making open source more inclusive

Red Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases. For more details, see link:https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language[Red Hat CTO Chris Wrightâ€™s message].

[id="ocp-4-7-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-7-rhcos"]
=== {op-system-first}

[id="ocp-4-7-enhanced-disk-provisioning"]
==== Enhanced disk provisioning for LUKS, RAID, and FBA DASD

{product-title} 4.7 includes several improvements to disk provisioning for bare metal deployments. The following features are currently supported for new 4.7 clusters only:

* Native Ignition support for LUKS disk encryption provides additional configurability for encrypted root filesystems, as well as support for encryption of additional data filesystems.
* {op-system} now supports boot disk mirroring, except on s390x, providing redundancy in the case of disk failure.
* {op-system} on s390x can be installed onto fixed-block architecture (FBA)-type direct access storage device (DASD) disks.

* Multipathing is now supported during initial deployment and early boot for storage devices when attached to clusters that are created using {product-title} 4.7 or higher.

[NOTE]
====
On new clusters, LUKS configuration must use the native Ignition mechanism, as provisioning fails if the legacy `/etc/clevis.json` file is included in the machine config. On clusters that are upgrading from {product-title} 4.6 or earlier, LUKS can only be configured by using `/etc/clevis.json`.
====

[id="ocp-4-7-bootupd"]
==== Update the bootloader by using bootupd

With `bootupd`, {op-system} users now have access to a cross-distribution, system-agnostic OS update tool that manages firmware and boot updates in UEFI and legacy BIOS boot modes that run on modern architectures.

[id="ocp-4-7-rhcos-rhel-8-3-packages"]
==== {op-system} now supports RHEL 8.3

{op-system} is now using Red Hat Enterprise Linux (RHEL) 8.3 packages. {product-title} 4.6 and below will stay with RHEL 8.2 packages. This enables you to have the latest fixes, features, and enhancements, such as NetworkManager features, as well as the latest hardware support and driver updates.

[id="ocp-4-7-rhcos-kdump"]
==== {op-system} now supports kdump service (Technical Preview)
The `kdump` service is introduced in Technical Preview in {op-system} to provide a crash-dumping mechanism for debugging kernel issues. You can use this service to save system memory content for later analysis. The `kdump` service is not managed at the cluster-level and must be enabled and configured manually on a per-node basis.

[id="ocp-4-7-ignition"]
==== Ignition updates
The following Ignition updates are now available:

* {op-system} now supports Ignition config spec 3.2.0. This update provides support for disk partition resizing, LUKS encrypted storage, and `gs://` URLs.
* When executing in non-default AWS partitions, such as GovCloud or AWS China, Ignition now fetches `s3://` resources from the same partition.
* Ignition now supports AWS EC2 Instance Metadata Service Version 2 (IMDSv2).

[id="ocp-4-7-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-7-aws-c2s-secret-region"]
==== Installing a cluster into the AWS C2S Secret Region

You can now install a cluster on Amazon Web Services (AWS) into the Commercial Cloud Services (C2S) Secret Region. Because the C2S region does not have an {op-system} AMI published by Red Hat, you must upload a custom AMI that belongs to that region. You are also required to include the CA certificates for C2S in the `additionalTrustBundle` field of the `install-config.yaml` file during cluster installation. Clusters deployed to the C2S Secret Region do not have access to the Internet; therefore, you must configure a private image registry.

The installation program does not support destroying a cluster deployed to the C2S region; you must manually remove the resources of the cluster.

For more information, see xref:../installing/installing_aws/installing-aws-government-region.adoc#installation-aws-about-government-region_installing-aws-government-region[AWS government and secret regions].

[id="ocp-4-7-gcp-disk-encryption"]
==== Installing cluster on GCP with disk encryption using a personal encryption key

You can now install a cluster on Google Cloud Platform (GCP) and use a personal encryption key to encrypt both virtual machines and persistent volumes. This is done by setting the `controlPlane.platform.gcp.osDisk.encryptionKey`, `compute.platform.gcp.osDisk.encryptionKey`, or `gcp.defaultMachinePlatform.osDisk.encryptionKey` field in the `install-config.yaml` file.

[id="ocp-4-7-osp-bare-metal-machines"]
==== Installing a cluster on {rh-openstack} that uses bare metal machines

You can now install a cluster on your own {rh-openstack-first} infrastructure that uses bare metal machines. The cluster can have both control plane and compute machines running on bare metal, or just compute machines. For more information, see xref:../installing/installing_openstack/installing-openstack-user.adoc#installation-osp-deploying-bare-metal-machines_installing-openstack-user[Deploying a cluster with bare metal machines].

This feature is not supported on clusters that use Kuryr.

[id="ocp-4-7-osp-validations"]
==== Improved {rh-openstack} requirements validation at installation

The {product-title} installer now performs additional validations before attempting to install a cluster on {rh-openstack}. These new validations include:

* Resource quotas

* Floating IP addresses duplication

* Custom cluster OS image availability

[id="ocp-4-7-osp-custom-subnets"]
==== Custom subnets for new compute machines on {rh-openstack}

You can now create compute machines in clusters that run on {rh-openstack} that use a network and subnet of your choice.

[id="ocp-4-7-upi-playbooks"]
==== Easier access to {rh-openstack} user-provisioned infrastructure playbooks

Ansible playbooks for installing a cluster on your own {rh-openstack} infrastructure are now packaged for retrieval by using a script in the installation documentation.

// Pull if not verified before GA.
// [id="ocp-4-7-sr-iov"]
// ==== Support for SR-IOV machines on {rh-openstack}
// You can now configure compute machines to run on single root I/O virtualization (SR-IOV) networks during user-provisioned infrastructure installation.

[id="ocp-4-7-osp-qemu"]
==== Support for the QEMU Guest Agent on {rh-openstack}

You can now enable QEMU Guest Agent support during installation.

[id="ocp-4-7-additional-pvs"]
==== Increasing the persistent volume limit for clusters on {rh-openstack}

You can now configure nodes to have more than 26 persistent Cinder volumes in clusters on {rh-openstack} during installation.

[id="ocp-4-7-osp-computeflavor-property-deprecation"]
==== Deprecation of the computeFlavor property in the install-config.yaml file

The `computeFlavor` property that is used in the `install-config.yaml` file is deprecated. As an alternative, you can now configure machine pool flavors in the `platform.openstack.defaultMachinePlatform` property.

[id="ocp-4-7-enhancements-to-installer-provisioned-installation"]
==== Enhancements to installer-provisioned installation

The installer for installer-provisioned installation on bare metal nodes now automatically creates a storage pool for storing relevant data files required during the installation, such as ignition files.

The installer for installer-provisioned installation on bare metal nodes provides a survey which asks the user a minimal set of questions, and generates an `install-config.yaml` file with reasonable defaults. You can use the generated `install-config.yaml` file to create the cluster, or edit the file manually before creating the cluster.

[id="ocp-4-7-dhcp-lease-to-static-ip-address"]
==== Installer-provisioned clusters can convert DHCP leases to static IP addresses

Cluster nodes deployed with installer-provisioned installation on bare metal clusters can deploy with static IP addresses. To deploy a cluster so that nodes use static IP addresses, configure a DHCP server to provide infinite leases to cluster nodes. After the installer finishes provisioning each node, a dispatcher script will execute on each provisioned node and convert the DHCP infinite lease to a static IP address using the same static IP address provided by the DHCP server.

[id="ocp-4-7-web-console"]
=== Web console

[id="ocp-4-7-web-console-localization"]
==== Web console localization

The web console is now localized and provides language support for global users. English, Japanese, and Simplified Chinese are currently supported. The displayed language follows your browser preferences, but you can also select a language to override the browser default. From the *User* drop-down menu, select *Language preferences* to update your language setting. Localized date and time is now also supported.

[id="ocp-4-7-web-console-quick-starts"]
==== Quick start tutorials

A quick start is a guided tutorial with user tasks. In the web console, you can access quick starts under the *Help* menu. They are especially useful for getting oriented with an application, Operator, or other product offering.

See xref:../web_console/creating-quick-start-tutorials.adoc#creating-quick-start-tutorials[Creating quick start tutorials in the web console] for more information.

[id="ocp-4-7-web-console-insights-plugin"]
==== Insights plug-in

The xref:../support/remote_health_monitoring/using-insights-to-identify-issues-with-your-cluster.adoc#displaying-the-insights-status-in-the-web-console_using-insights-to-identify-issues-with-your-cluster[Insights plug-in] is now integrated into the {product-title} web console. Insights provides cluster health data, such as the number of total issues and total risks of the issues. Risks are labeled as *Critical*, *Important*, *Moderate*, or *Low*. You can quickly navigate to {cloud-redhat-com} for further details about the issues and how to fix them.

[id="ocp-4-7-web-console-developer-perspective"]
==== Developer perspective

* The console now provides an extensibility mechanism that allows Red Hat Operators to build and package their own user interface extending the console. It also enables customers and Operators to add their own quick starts.
Hints, filters, and access from both *Administrator* and *Developer* perspectives are now added to make quick starts and the relevant content more accessible.
* You can now quickly search for deployed workloads and application groupings in the topology *List* and *Graph* views to add them to your application.
* Persistent storage of user preferences is now provided so that when users move from one machine or browser to another they have a consistent experience.
* If you have the OpenShift GitOps Operator installed on your cluster, you can use the *Argo CD* link in the *Environments* view to navigate to the Argo CD user interface.
* Usability enhancements such as, the in-context menus mapping to the *Developer Catalog* features and *Form* or *YAML* options to update Pipelines, Helm, and Event Sources configurations have been added.
* Ability to see filtered entries is now added in the *Developer Catalog* for specified services such as Operator Backed, Helm, Builder Image, Template, and Event Source.
* After you have the Container Security Operator installed on your cluster:
** You can view a list of the following vulnerabilities for a selected project:
*** The total count of vulnerabilities and vulnerable images,
*** Severity-based counts of all vulnerable images,
*** Count of fixable vulnerabilities,
*** Number of affected pods for each vulnerable image
** You can see the severity details of a vulnerability and also launch the Quay user interface, in the context of the manifest of the vulnerable image stored in that repository, to get more details about the vulnerability.
* After you have the OpenShift Virtualization Operator installed in your cluster, you can create virtual machines by selecting the *Virtual Machines* option on the *+Add* view and then using the templates in the *Developer Catalog*.
* The web terminal usability is now enhanced:
** All users can access the web terminal on the console regardless of their privilege level.
** When the web terminal is inactive for a long period, it stops and provides the user an option to restart it.
* The pipelines workflow is now enhanced:
** The pipeline creation process now makes better use of pipelines over the default build config system. Build configs are no longer created by default along with the Pipelines using the *Import from git* workflow and the pipeline starts as soon as you create the application.
** You can now configure pipelines in the *Pipeline builder* page using either the *Pipeline builder* option or the *YAML view* option. You can also use the Operator-installed, reusable snippets and samples to create detailed Pipelines.
** The *PipelineRun* page now contains a *TaskRuns* tab that lists the associated task runs. You can click on the required task run to see the details of the task run and debug your pipelines.
** You can now see the following metrics for your pipelines in the *Pipeline Details* page, per pipeline: pipeline run duration, task run duration, number of pipeline runs per day and the pipeline success ratio per day.
** An *Events* tab is now available on the *Pipeline Run details* and the *Task Run details* pages, which shows the events for a particular PipelineRun or TaskRun.
* The serverless usability is now enhanced:
** You can access the *Serving* and *Eventing* pages from the *Administrator* perspective and create serverless components using the console.
** You can create Camel connectors using the event source creation workflow.
* The Helm charts usability is now enhanced.
** As a cluster administrator, you can:
*** Add or remove Chart Repositories.
*** Remove the ability to use Helm charts.
*** Use the quick start to learn how to manage Helm Chart Repositories.
** As a developer, you can:
*** See the name of the chart repository on the chart card in the catalog to distinguish charts with the same name, but from different chart repositories.
*** Get more insight into the charts at the catalog level on the cards.
*** Filter the catalog by chart repositories if multiple repositories are configured.

[id="ocp-4-7-ibm-z"]
==== IBM Z and LinuxONE

With this release, IBM Z and LinuxONE are now compatible with {product-title} {product-version}. See xref:../installing/installing_ibm_z/installing-ibm-z.adoc#installing-ibm-z[Installing a cluster with z/VM on IBM Z and LinuxONE] or xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc#installing-restricted-networks-ibm-z[Installing a cluster with z/VM on IBM Z and LinuxONE in a restricted network] for installation instructions.

[discrete]
===== Notable Enhancements

The following new features are supported on IBM Z and LinuxONE with {product-title} {product-version}:

* KVM on {op-system-base} 8.3 or later is supported as a hypervisor for user-provisioned installation of {product-title} {product-version} on IBM Z and LinuxONE.  See xref:../installing/installing_ibm_z/installing-ibm-z-kvm.adoc#installing-ibm-z-kvm[Installing a cluster with {op-system-base} KVM on IBM Z and LinuxONE] for installation instructions.
* Multipathing
* OpenShift Pipelines TP
* OpenShift Service Mesh
* OVN-Kubernetes with an initial installation of {product-title} 4.7
* Persistent storage using Fibre Channel
* Persistent storage using Raw Block
* z/VM Emulated FBA devices on SCSI disks

[discrete]
===== Supported features

The following features are also supported on IBM Z and LinuxONE:

* CodeReady Workspaces
* Developer CLI - odo
* Persistent storage using iSCSI
* Persistent storage using local volumes (Local Storage Operator)

[discrete]
===== Restrictions

Note the following restrictions for {product-title} on IBM Z and LinuxONE:

* {product-title} for IBM Z does not include the following Technology Preview features:
** Precision Time Protocol (PTP) hardware
** CSI volume snapshots

* The following {product-title} features are unsupported:
** Log forwarding
** OpenShift Virtualization
** CodeReady Containers (CRC)
** OpenShift Metering
** Multus CNI plug-in
** FIPS cryptography
** Encrypting data stored in etcd
** Automatic repair of damaged machines with machine health checking
** Tang mode disk encryption during {product-title} deployment
** OpenShift Serverless
** Helm command-line interface (CLI) tool
** Controlling overcommit and managing container density on nodes
** etcd cluster Operator
** CSI volume cloning
** NVMe
** 4K FCP block device

* Worker nodes must run {op-system-first}.
* Persistent shared storage must be provisioned by using either NFS or other supported storage protocols
* Persistent non-shared storage must be provisioned using local storage, like iSCSI, FC, or using LSO with DASD, FCP, or EDEV/FBA.
* These features are available only for {product-title} on IBM Z for {product-version}:
** HyperPAV enabled on IBM System Z /LinuxONE for the virtual machines for FICON attached ECKD storage

[id="ocp-4-7-ibm-power"]
==== IBM Power Systems

With this release, IBM Power Systems are now compatible with {product-title} {product-version}. See xref:../installing/installing_ibm_power/installing-ibm-power.adoc#installing-ibm-power_installing-ibm-power[Installing a cluster on IBM Power Systems] or xref:../installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc#installing-restricted-networks-ibm-power_installing-restricted-networks-ibm-power[Installing a cluster on IBM Power Systems in a restricted network] for installation instructions.

[discrete]
===== Notable Enhancements

The following new features are supported on IBM Power Systems with {product-title} {product-version}:

* Multipathing
* OpenShift Pipelines TP
* OpenShift Service Mesh
* OVN-Kubernetes with an initial installation of {product-title} 4.7
* Persistent storage using Fibre Channel
* Persistent storage using Raw Block
* 4K Disk Support

[discrete]
===== Supported Features

The following features are also supported on IBM Power Systems:

* Currently, four Operators are supported:
** Cluster-Logging-Operator
** Cluster-NFD-Operator
** Elastic Search-Operator
** Local Storage Operator

* Developer CLI - odo
* CodeReady Workspaces
* Persistent storage using iSCSI
* HostPath

[discrete]
===== Restrictions

Note the following restrictions for {product-title} on IBM Power Systems:

* The following {product-title} features are unsupported:
** OpenShift Metering
** OpenShift Serverless
** OpenShift Virtualization
** CodeReady Containers (CRC)

* Worker nodes must run {op-system-first}.
* Persistent storage must be of the Filesystem type that uses local volumes, Network File System (NFS), or Container Storage Interface (CSI)

[id="ocp-4-7-security"]
=== Security and compliance

[id="ocp-4-7-security-user-oauth-tokens"]
==== Managing user-owned OAuth access tokens

Users can now manage their own OAuth access tokens. This allows users to review their tokens and delete any tokens that have timed out or are no longer needed.

For more information, see xref:../authentication/managing-oauth-access-tokens.adoc#managing-oauth-access-tokens[Managing user-owned OAuth access tokens].

[id="ocp-4-7-security-cco-deletion-gcp-creds-mint-mode"]
==== Cloud Credential Operator support for deletion of GCP root credentials after installation

You can now remove or rotate the GCP admin-level credential that the xref:../operators/operator-reference.adoc#cloud-credential-operator_red-hat-operators[Cloud Credential Operator] uses in Mint mode. This option requires the presence of the admin-level credential during installation, but the credential is not stored in the cluster permanently and does not need to be long-lived.

[id="ocp-4-7-compliance-cis-benchmark"]
==== CIS Kubernetes Benchmark profile for the Compliance Operator

You can now use the Compliance Operator to perform Center for Internet Security (CIS) Kubernetes Benchmark checks. CIS profiles for {product-title} are based on the CIS Kubernetes checks.

[id="ocp-4-7-security-secure-boot"]
==== Secure Boot support for installer-provisioned clusters

You can now deploy a cluster with Secure Boot when using installer-provisioned infrastructure on bare metal nodes. Deploying a cluster with Secure Boot requires UEFI boot mode and Red Fish Virtual Media. You cannot use self-generated keys with Secure Boot.

[id="ocp-4-7-networking"]
=== Networking

[id="ocp-4-7-ovn-kubernetes-migration"]
==== Expanded platform support for migrating from the OpenShift SDN cluster network provider to the OVN-Kubernetes cluster network provider

A xref:../networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc#migrate-from-openshift-sdn[migration to the OVN-Kubernetes cluster network provider] is now supported on installer-provisioned clusters on the following platforms:

* Bare metal hardware
* Amazon Web Services (AWS)
* Google Cloud Platform (GCP)
* Microsoft Azure
* {rh-openstack-first}
* VMware vSphere

[id="ocp-4-7-network-connection-health-checks"]
==== Network connection health checks for API servers, load balancers, and nodes

To assist you with diagnosing cluster network connectivity issues, the Cluster Network Operator (CNO) now runs a connectivity check controller to perform connection health checks in your cluster. The results of the connection tests are available in `PodNetworkConnectivityCheck` objects in the `openshift-network-diagnostics` namespace. For more information, see xref:../networking/verifying-connectivity-endpoint.adoc#verifying-connectivity-endpoint[Verifying connectivity to an endpoint].

[id="ocp-4-7-ovn-kubernetes-egress-firewall-dns"]
==== OVN-Kubernetes egress firewall support for DNS rules

When configuring an egress firewall rule, you can now use a xref:../networking/ovn_kubernetes_network_provider/configuring-egress-firewall-ovn.adoc#domain-name-server-resolution_configuring-egress-firewall-ovn[DNS domain name] instead of an IP address.
With the addition of DNS support in the OVN-Kubernetes cluster network provider egress firewall implementation, parity is achieved with the OpenShift SDN cluster network provider egress firewall implementation.

[id="ocp-4-7-sr-iov-dpdk-functions"]
==== Library for interacting with SR-IOV virtual functions in DPDK mode within containers

For containers interacting with SR-IOV virtual functions (VFs) in Data Plane Development Kit (DPDK) mode, the `app-netutil` library now provides the following functions: `GetCPUInfo()`, `GetHugepages()`, and `GetInterfaces()`. For more information, see xref:../networking/hardware_networks/about-sriov.adoc#nw-sriov-app-netutil_about-sriov[DPDK library for use with container applications].

[id="ocp-4-7-ovn-kubernetes-egress-router-cni"]
==== Egress router CNI (Technical Preview)

The egress router CNI plug-in is introduced in Technical Preview. You can use the plug-in to deploy an egress router in redirect mode. This egress router provides parity for OVN-Kubernetes compared to OpenShift SDN, but for redirect mode only. The plug-in does not perform in HTTP proxy or DNS proxy modes, and this is a difference with the implementation for OpenShift SDN. For more information, see xref:../networking/ovn_kubernetes_network_provider/deploying-egress-router-ovn-redirection.adoc[Deploying an egress router pod in redirect mode].

[id="ocp-4-7-ovn-kubernetes-ipsec-support"]
==== OVN-Kubernetes IPsec support for encrypted traffic between pods

When you install a cluster, you can configure the OVN-Kubernetes cluster network provider with IPsec enabled. With IPsec enabled, all cluster network traffic between pods is sent over an encrypted IPsec tunnel. You cannot enable or disable IPsec after cluster installation.

The IPsec tunnel is not used for network traffic between pods that are configured to use the host network. However, traffic sent from a pod on the host network and received by a pod that uses the cluster network does use the IPsec tunnel. For more information, see xref:../networking/ovn_kubernetes_network_provider/about-ipsec-ovn.adoc#about-ipsec-ovn[IPsec encryption configuration].

[id="ocp-4-7-sriov-networknodepolicy-netfilter"]
==== SR-IOV network node policy enhancement for deployment with {rh-openstack-first}

The SR-IOV Network Operator is enhanced to support an additional field, `spec.nicSelector.netFilter`, in the custom resource for an SR-IOV network node policy. You can use the new field to specify an {rh-openstack} network by the network ID. For more information, see xref:../networking/hardware_networks/configuring-sriov-device.adoc[Configuring an SR-IOV network device].

[id="ocp-4-7-kuryr-services-without-selectors"]
==== {rh-openstack} Kuryr support for services without pod selectors

Clusters that run on {rh-openstack} and use Kuryr now support services that do not have pod selectors specified.

[id="ocp-4-7-preparation-HAProxy-2.2"]
==== Adjusting HTTP header names

If legacy applications are sensitive to the capitalization of HTTP header names, use the Ingress Controller `spec.httpHeaders.headerNameCaseAdjustments` API field for a solution to accommodate legacy applications until they can be fixed.

{product-title} will update to HAProxy 2.2, which down-cases HTTP header names by default, for example, changing `Host: xyz.com` to `host: xyz.com`. Make sure to add the necessary configuration by using `spec.httpHeaders.headerNameCaseAdjustments` before upgrading {product-title} when HAProxy 2.2 is available.

[id="ocp-4-7-networking-k8s-nmstate"]
==== Kubernetes NMState Operator (Technology Preview)

{product-title} 4.7 provides post-installation state-driven network configuration on the secondary network interfaces of cluster nodes using the Kubernetes NMState Operator as a Technology Preview feature. For more information, see
xref:../networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.adoc#k8s-nmstate-about-the-k8s-nmstate-operator[Using Kubernetes NMState (Technology Preview)].

[NOTE]
====
Configuration must occur before scheduling pods.
====

[id="ocp-4-7-storage"]
=== Storage

[id="ocp-4-7-storage-csi-snapshots"]
==== Persistent storage using CSI volume snapshots is generally available

You can use the Container Storage Interface (CSI) to create, restore, and delete a volume snapshot when using CSI drivers that provide support for volume snapshots. This feature was previously introduced as a Technology Preview feature in {product-title} 4.4 and is now generally available and enabled by default in {product-title} 4.7.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-snapshots.adoc#persistent-storage-csi-snapshots[Using CSI volume snapshots].

[id="ocp-4-5-persistent-storage-csi-gcp-pd"]
==== Persistent storage using the GCP PD CSI Driver Operator (Technology Preview)

The Google Cloud Platform (GCP) persistent disk (PD) CSI driver is automatically deployed and managed on GCP environments, allowing you to dynamically provision these volumes without having to install the driver manually. The GCP PD CSI Driver Operator that manages this driver is in Technology Preview.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-gcp-pd.adoc#persistent-storage-csi-gcp-pd[GCP PD CSI Driver Operator].

[id="ocp-4-5-persistent-storage-csi-cinder"]
==== Persistent storage using the OpenStack Cinder CSI Driver Operator

You can now use CSI to provision a persistent volume using the CSI driver for OpenStack Cinder.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-cinder.adoc#persistent-storage-csi-cinder[OpenStack Cinder CSI Driver Operator].

[id="ocp-4-7-storage-vsphere-problem-detector"]
==== vSphere Problem Detector Operator

The vSphere Problem Detector Operator periodically checks functionality of {product-title} clusters installed in a vSphere environment. The vSphere Problem Detector Operator is installed by default by the Cluster Storage Operator, allowing you to quickly identify and troubleshoot common storage issues, such as configuration and permissions, on vSphere clusters.

[id="ocp-4-7-registry"]
=== Registry

[id="ocp-4-7-registry-oci-support"]
==== Open Container Initiative images support

The {product-title} internal registry and image streams now support Open Container Initiative (OCI) images. You can use OCI images in the same way you would use Docker `schema2` images.

//Add link

[id="ocp-4-7-olm"]
=== Operator lifecycle

[id="ocp-4-7-safe-operator-upgrades"]
==== Safe Operator upgrades

To make upgrades more robust, it is recommend that Operators actively communicate with the service that is about to be updated. If a service is processing a critical operation, such as live migrating virtual machines (VMs) in OpenShift Virtualization or restoring a database, it might be unsafe to upgrade the related Operator at that time.

In {product-title} 4.7, Operators can take advantage of the new `OperatorCondition` resource to communicate a non-upgradeable state to Operator Lifecycle Manager (OLM), such as when a related service is performing a critical operation. The non-upgradeable state delays any pending Operator upgrade, whether automatically or manually approved, until the Operator finishes the operation and reports upgrade readiness.

See xref:../operators/understanding/olm/olm-operatorconditions.adoc#olm-operatorconditions[Operator conditions] for more about how OLM uses this communication channel.

See xref:../operators/admin/olm-managing-operatorconditions.adoc#olm-operatorconditions[Managing Operator conditions] for details on overriding states in OLM as a cluster administrator.

See xref:../operators/operator_sdk/osdk-generating-csvs.adoc#osdk-operatorconditions_osdk-generating-csvs[Enabling Operator conditions] for details on updating your project as an Operator developer to use the communication channel.

[id="ocp-4-7-pull-secrets-cs"]
==== Adding pull secrets to catalog sources

If certain images relevant to Operators managed by Operator Lifecycle Manager (OLM) are hosted in an authenticated container image registry, also known as a private registry, OLM and OperatorHub are unable to pull the images by default. To enable access, you can create a pull secret that contains the authentication credentials for the registry.

By referencing one or more secrets in a catalog source, some of these required images can be pulled for use in OperatorHub, while other images require updates to the global cluster pull secret or namespace-scoped secrets.

See xref:../operators/admin/olm-managing-custom-catalogs.adoc#olm-accessing-images-private-registries_olm-managing-custom-catalogs[Accessing images for Operators from private registries] for more details.

[id="ocp-4-7-osdk"]
=== Operator development

[id="ocp-4-7-osdk-supported"]
==== Operator SDK now fully supported

As of {product-title} 4.7, the Operator SDK is now a fully supported Red Hat offering. With the downstream release of Operator SDK v1.3.0, officially supported and branded Operator SDK tooling is now available for download directly from Red Hat.

The Operator SDK CLI assists Operator developers and independent software vendor (ISV) partners in writing Operators that provide a great user experience and are compatible with OpenShift distributions and Operator Lifecycle Manager (OLM).

The Operator SDK enables Operator authors with cluster administrator access to a Kubernetes-based cluster, such as {product-title}, to develop their own Operators based on Go, Ansible, or Helm. For Go-based Operators, link:https://kubebuilder.io/[Kubebuilder] is embedded into the SDK as the scaffolding solution; this means existing Kubebuilder projects can be used as is with the SDK and continue to work.

The following features highlight some of the capabilities of the Operator SDK:

Native support for Operator Bundle Format:: The Operator SDK includes native support for the xref:../operators/understanding/olm-packaging-format.adoc#olm-bundle-format_olm-packaging-format[Operator Bundle Format] introduced in {product-title} 4.6. All metadata required to package an Operator for OLM is generated automatically. Operator developers can use this functionality to package and test their Operator for OLM and OpenShift distributions directly from their CI pipelines.

Operator Lifecycle Manager integration:: The Operator SDK provides developers with a streamlined experience for quickly testing their Operator with OLM from their workstation. You can use the xref:../operators/operator_sdk/osdk-working-bundle-images.adoc#osdk-bundle-deploy-olm_osdk-working-bundle-images[`run bundle`] subcommand to run Operator on a cluster and test whether the Operator behaves correctly when managed by OLM.

Webhook integration:: The Operator SDK supports xref:../operators/operator_sdk/osdk-generating-csvs.adoc#olm-defining-csv-webhook_osdk-generating-csvs[webhook integration] with OLM, which simplifies installing Operators that have admission or custom resource definition (CRD) conversion webhooks. This feature relieves the cluster administrator of having to manually register the webhooks, add TLS certificates, and set up certificate rotation.

Validation scorecard:: Operator authors should validate that their Operator is packaged correctly and free of syntax errors. To validate an Operator, the xref:../operators/operator_sdk/osdk-scorecard.adoc#osdk-scorecard[scorecard tool] provided by the Operator SDK begins by creating all resources required by any related custom resources (CRs) and the Operator. The scorecard then creates a proxy container in the deployment of the Operator, which is used to record calls to the API server and run some of the tests. The tests performed also examine some of the parameters in the CRs.

Upgrade readiness reporting:: Operator developers can use the Operator SDK to take advantage of code scaffolding support for Operator conditions, including xref:../operators/operator_sdk/osdk-generating-csvs.adoc#osdk-operatorconditions_osdk-generating-csvs[reporting upgrade readiness] to OLM.

Trigger Operator upgrades:: You can quickly test upgrading your Operator by using OLM integration in the Operator SDK, without requiring you to manually manage index images and catalog sources. The xref:../operators/operator_sdk/osdk-working-bundle-images.adoc#osdk-bundle-upgrade-olm_osdk-working-bundle-images[`run bundle-upgrade`] subcommand automates triggering an installed Operator to upgrade to a later version by specifying a bundle image for the later version.

[NOTE]
====
Operator SDK v1.3.0 supports Kubernetes 1.19.
====

See xref:../operators/operator_sdk/osdk-about.adoc#osdk-about[Developing Operators] for full documentation on the Operator SDK.

=== Builds

[discrete]
[id="ocp-4-7-buildah-version"]
// https://bugzilla.redhat.com/show_bug.cgi?id=1907407
==== Print the buildah version to the build log

In the current version, when the {product-title} performs a build and the log level is five or higher, the cluster writes the buildah version information to the build log. This information helps Red Hat Engineering reproduce bug reports. Previously, this version information was not available in the build logs.

[id="ocp-4-7-images"]
=== Images


[id="ocp-4-7-machine-api"]
=== Machine API

[id="ocp-4-7-machine-api-wait-upgrade"]
==== Updates are immediately blocked if a machine config pool is degraded

If a machine config pool (MCP) is in a `degraded` state, the Machine Config Operator (MCO) now reports its *Upgradeable* status as *False*. As a result, you are now prevented from performing an update within a minor version, for example, from 4.7 to 4.8, until all machine config pools are healthy. Previously, with a degraded machine config pool, the Machine Config Operator did not report its *Upgradeable* status as *false*. The update was allowed and would eventually fail when updating the Machine Config Operator because of the degraded machine config pool. There is no change in this behavior for updates within z-stream releases, for example, from 4.7.1 to 4.7.2. As such, you should check the machine config pool status before performing a z-stream update.

[id="ocp-4-7-aws-tenancy-dedicated"]
==== Machine sets running on AWS support Dedicated Instances

Machine sets running on AWS now support Dedicated Instances. Configure Dedicated Instances by specifying a dedicated tenancy under the `providerSpec` field in the machine set YAML file.

For more information, see xref:../machine_management/creating_machinesets/creating-machineset-aws.adoc#machineset-dedicated-instance_creating-machineset-aws[Machine sets that deploy machines as Dedicated Instances].

[id="ocp-4-7-gcp-customer-managed-encryption-keys"]
==== Machine sets running on GCP support customer-managed encryption keys

You can now enable encryption with a customer-managed key for machine sets running on GCP. Users can configure an encryption key under the `providerSpec` field in the machine set YAML file. The key is used to encrypt the data encryption key, not to encrypt the customer's data.

For more information, see xref:../machine_management/creating_machinesets/creating-machineset-gcp.adoc#machineset-enabling-customer-managed-encryption_creating-machineset-gcp[Enabling customer-managed encryption keys for a machine set].

[id="ocp-4-7-machine-api-cluster-wide-proxy-settings"]
==== Machine API components honor cluster-wide proxy settings

The Machine API now honors cluster-wide proxy settings. When a cluster-wide proxy is configured, all Machine API components will route traffic through the configured proxy.

[id="ocp-4-7-nodes"]
=== Nodes

[id="ocp-4-7-nodes-descheduler-ga"]
==== Descheduler is generally available

The descheduler is now generally available. The descheduler provides the ability to evict a running pod so that the pod can be rescheduled onto a more suitable node. You can enable one or more of the following descheduler profiles:

* `AffinityAndTaints`: evicts pods that violate inter-pod anti-affinity, node affinity, and node taints.
* `TopologyAndDuplicates`: evicts pods in an effort to evenly spread similar pods, or pods of the same topology domain, among nodes.
* `LifecycleAndUtilization`: evicts long-running pods and balances resource usage between nodes.

[NOTE]
====
With the GA, you can enable descheduler profiles and configure the descheduler interval. Any other settings that were available during Technology Preview are no longer available.
====

For more information, see xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler[Evicting pods using the descheduler].

[id="ocp-4-7-nodes-scheduler-profiles"]
==== Scheduler profiles (Technology Preview)

You can now specify a scheduler profile to control how pods are scheduled onto nodes. This is a replacement for configuring a scheduler policy. The following scheduler profiles are available:

* `LowNodeUtilization`: This profile attempts to spread pods evenly across nodes to get low resource usage per node.

* `HighNodeUtilization`: This profile attempts to place as many pods as possible onto as few nodes as possible, to minimize node count with high usage per node.

* `NoScoring`: This is a low-latency profile that strives for the quickest scheduling cycle by disabling all score plug-ins. This might sacrifice better scheduling decisions for faster ones.

For more information, see xref:../nodes/scheduling/nodes-scheduler-profiles.adoc#nodes-scheduler-profiles[Scheduling pods using a scheduler profile].

[id="ocp-4-7-nodes-autoscaling-ga"]
==== Autoscaling for memory utilization GA

Autoscaling for memory utilization is now generally available. You can create horizontal pod autoscaler custom resources to automatically scale the pods associated with a deployment config or replication controller to maintain the average memory utilization you specify, either a direct value or a percentage of requested memory. For more information, see xref:../nodes/pods/nodes-pods-autoscaling.html#nodes-pods-autoscaling-creating-memory_nodes-pods-autoscaling[Creating a horizontal pod autoscaler object for memory utilization].

[id="ocp-4-7-machine-autoscaling-zero"]
==== Autoscaling to zero machines for clusters on {rh-openstack}

Clusters that run on {rh-openstack} can now autoscale to zero machines.

[id="ocp-4-7-nodes-non-preempting-priority-classes"]
==== Non-preempting option for priority classes (Technology Preview)

You can now configure a priority class to be non-preempting by setting the `preemptionPolicy` field to `Never`. Pods with this priority class setting are placed in the scheduling queue ahead of lower priority pods, but do not preempt other pods.

For more information, see xref:../nodes/pods/nodes-pods-priority.html#non-preempting-priority-class_nodes-pods-priority[Non-preempting priority classes].

[id="ocp-4-7-logging"]
=== Red Hat OpenShift Logging

[discrete]
[id="ocp-4-7-cluster-logging-renamed-openshift-logging"]
==== Cluster Logging becomes Red Hat OpenShift Logging

With this release, Cluster Logging becomes Red Hat OpenShift Logging, version 5.0.

[discrete]
[id="ocp-4-7-eo-max-five-shards"]
// https://bugzilla.redhat.com/show_bug.cgi?id=1883444
==== Maximum five primary shards per index

With this release, the Elasticsearch Operator (EO) sets the number of primary shards for an index between one and five, depending on the number of data nodes defined for a cluster.

Previously, the EO set the number of shards for an index to the number of data nodes. When an index in Elasticsearch was configured with a number of replicas, it created that many replicas for each primary shard, not per index. Therefore, as the index sharded, a greater number of replica shards existed in the cluster, which created a lot of overhead for the cluster to replicate and keep in sync.

[discrete]
[id="ocp-4-7-updated-eo-name"]
// https://bugzilla.redhat.com/show_bug.cgi?id=1898920
==== Updated Elasticsearch Operator name and maturity level

This release updates the display name of the Elasticsearch Operator and operator maturity level. The new display name and clarified specific use for the Elasticsearch Operator are updated in Operator Hub.

[discrete]
[id="ocp-4-7-es-csv-success"]
// https://bugzilla.redhat.com/show_bug.cgi?id=1913464
==== Elasticsearch Operator reports on CSV success

This release adds reporting metrics to indicate that installing or upgrading the Elasticsearch Operator ClusterServiceVersion (CSV) was successful. Previously, there was no way to determine, or generate an alert, if the CSV installation or upgrade for the Elasticsearch Operator failed. Now, an alert is provided as part of the Elasticsearch Operator.

[discrete]
[id="ocp-4-7-reduced-cert-warnings"]
// https://bugzilla.redhat.com/show_bug.cgi?id=1884812
==== Reduce Elasticsearch pod certificate permission warnings

Previously, when the Elasticsearch pod started, it generated certificate permission warnings, which misled some users to troubleshoot their clusters. The current release fixes these permissions issues to reduce these types of notifications.

[discrete]
[id="ocp-4-7-links-from-alerts"]
// https://bugzilla.redhat.com/show_bug.cgi?id=1913469
==== New links from alerts to explanations and troubleshooting

This release adds a link from the alerts that an Elasticsearch cluster generates to a page of explanations and troubleshooting steps for that alert.

[discrete]
[id="ocp-4-7-curl-conn-timeout"]
// https://bugzilla.redhat.com/show_bug.cgi?id=1881709
==== New connection timeout for deletion jobs

The current release adds a connection timeout for deletion jobs, which helps prevent pods from occasionally hanging when they query Elasticsearch to delete indices. Now, if the underlying 'curl' call does not connect before the timeout period elapses, the timeout terminates the call.

[id="ocp-4-7-monitoring"]
=== Monitoring

[id="ocp-4-7-monitoring-api-performance-dashboard"]
==== New API performance monitoring dashboard

The API performance dashboard is now available from the web console. This dashboard can be used to help troubleshoot performance issues with the Kubernetes API server or the OpenShift API server. You can access the API performance dashboard from the web console by navigating to *Monitoring* -> *Dashboards* and selecting the *API Performance* dashboard.

This dashboard provides API server metrics, such as:

* Request duration
* Request rate
* Request termination
* Requests in flight
* Requests aborted
* etcd request duration
* etcd object count
* Long-running requests
* Response status code
* Response size
* Priority and fairness

// TODO: Link to troubleshooting using this dashboard section once it is available

[id="ocp-4-7-scale"]
=== Scale

[id="ocp-4-7-scale-cluster-maximums"]
==== Cluster maximums

Updated guidance around xref:../scalability_and_performance/planning-your-environment-according-to-object-maximums.adoc#planning-your-environment-according-to-object-maximums[cluster maximums] for {product-title} {product-version} is now available.

Use the link:https://access.redhat.com/labs/ocplimitscalculator/[{product-title} Limit Calculator] to estimate cluster limits for your environment.

[id="ocp-4-7-cnf-latency-test"]
==== Test to determine CPU latency
The latency test, a part of the CNF-test container, provides a way to measure if the isolated CPU latency is below the requested upper bound.

For information about running a latency test, see xref:../scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc#cnf-performing-end-to-end-tests-running-the-latency_tests[Running the latency tests].

[id="ocp-4-7-scale-pao-disable-interrupt-processing"]
==== New globallyDisableIrqLoadBalancing feature in Performance Addon Operator allows global device interrupt processing to be disabled for guaranteed pod CPUs

The Performance Addon Operator manages host CPUs by dividing them into reserved CPUs for cluster and operating system housekeeping duties, and isolated CPUs for workloads. A new performance profile field `globallyDisableIrqLoadBalancing` is available to manage whether or not device interrupts are processed by the isolated CPU set.

New pod annotations `irq-load-balancing.crio.io` and `cpu-quota.crio.io` are used in conjunction with `globallyDisableIrqLoadBalancing` to define whether or not device interrupts are processed for a pod. When configured, CRI-O disables device interrupts only as long as the pod is running.

For more information, see xref:../scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc#managing-device-interrupt-processing-for-guaranteed-pod-isolated-cpus_cnf-master[Managing device interrupt processing for guaranteed pod isolated CPUs].

[id="ocp-4-7-scale-new-vrf-cni-plugin"]
==== New VRF CNI plug-in allows secondary networks to be assigned to VRFs

A new VRF CNI plugin that allows you to assign additional networks to a VRF is now available. When you create a secondary network using a `rawConfig` configuration for the CNO custom resource and configure a VRF for it, the interface created for the pod is associated with the VRF. You can also use the VRF CNI plug-in to assign an SR-IOV network to a VRF.

For more information, see xref:../networking/multiple_networks/assigning-a-secondary-network-to-a-vrf.adoc#cnf-assigning-a-secondary-network-to-a-vrf[Assigning a secondary network to a VRF] and xref:../networking/hardware_networks/configuring-sriov-device.adoc#cnf-assigning-a-sriov-network-to-a-vrf_configuring-sriov-device[Assigning an SR-IOV network to a VRF].

[id="ocp-4-7-scale-xt_32-end-to-end-test"]
==== The xt_u32 end-to-end test is enabled for CNF
xt_u32 is an iptables kernel module that allows packet filtering based on arbitrary content. It can look beyond headers or special protocols that are not covered by other iptables modules.

For more information, see xref:../scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc#cnf-performing-end-to-end-tests-for-platform-verification_cnf-master[Performing end-to-end tests for platform verification].

[id="ocp-4-7-dev-exp"]
=== Developer experience

==== Red Hat OpenShift GitOps (Technology Preview)
The Red Hat OpenShift GitOps 1.0 Technology Preview release introduces a declarative way to implement continuous deployment for cloud native applications. You can use Red Hat OpenShift GitOps to adopt GitOps principles for managing cluster configurations, and for automating secure, predictable, traceable, and repeatable application delivery across hybrid, multi-cluster, Kubernetes environments.
It uses Argo CD as the core, and adds other tooling to enable teams to implement GitOps workflows across clusters. For more information, see, xref:../cicd/gitops/understanding-openshift-gitops.adoc#about-gitops_understanding-openshift-gitops[Understanding OpenShift GitOps].

[id="ocp-4-7-insights-operator"]
=== Insights Operator

[id="ocp-4-7-insights-operator-data-collection-enhancements"]
==== Insights Operator data collection enhancements

In {product-title} 4.7, the Insights Operator collects the following additional information:

* The top 100 `InstallPlan` entries to identify invalid Operator Lifecycle Manager (OLM) installations
* The service accounts from the Kubernetes default namespace and the `openshift*` built-in namespaces
* The `ContainerRuntimeConfig` and `MachineConfigPools` configuration files to verify container storage limits
* The configuration files for all available `operator.openshift.io` control pane resources to identify Operators in unmanaged states
* The `NetNamespaces` names, including their `netID` and egress IP addresses
* A list of all installed Operator Lifecycle Manager Operators, including version information
* The Persistent Volume definition, if used in the `openshift-image-registry` configuration
* Appearances of certain log entries of pods in the `openshift-apiserver-operator` namespace
* Appearances of certain log entries of `sdn` pods in the `openshift-sdn` namespace

With this additional information, Red Hat can provide improved remediation steps in {cloud-redhat-com}.


[id="ocp-4-7-notable-technical-changes"]
== Notable technical changes

{product-title} 4.7 introduces the following notable technical changes.

[discrete]
[id="ocp-4-7-olm-k8s-1-20"]
==== Operator Lifecycle Manager updated to use Kubernetes 1.20

Operator Lifecycle Manager (OLM) strives to keep up to date with Kubernetes releases when they become available. The OLM-provided `ClusterServiceVersion` (CSV) resource is composed of a number of core Kubernetes resources. When OLM increments Kubernetes dependencies, the embedded resources are updated as well.

As of {product-title} 4.7, OLM and its associated components have been updated to use Kubernetes 1.20. Typically, Kubernetes is backwards compatible with a few of its previous versions. Operator authors are encouraged to keep their projects up to date to maintain compatibility and take advantage of updated resources.

See xref:../release_notes/versioning-policy.adoc#ocp-versioning-policy[OpenShift Container Platform Versioning Policy] for more specific details on backwards compatibility guarantees.

See link:https://kubernetes.io/docs/setup/release/version-skew-policy/[Kubernetes documentation] for details about version skew policies in the upstream Kubernetes project.

[id="ocp-4-7-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more fine-grained functionality that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *DEP*: _Deprecated_
* *REM*: _Removed_

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.5 |OCP 4.6 |OCP 4.7

|`OperatorSource` objects
|DEP
|REM
|REM

|Package Manifest Format (Operator Framework)
|DEP
|DEP
|DEP

|`oc adm catalog build`
|DEP
|DEP
|DEP

|`--filter-by-os` flag for `oc adm catalog mirror`
|GA
|GA
|DEP

|v1beta1 CRDs
|DEP
|DEP
|DEP

|Docker Registry v1 API
|GA
|DEP
|

|Metering Operator
|GA
|DEP
|DEP

|Scheduler policy
|GA
|GA
|DEP

|====

[id="ocp-4-7-deprecated-features"]
=== Deprecated features

[id="ocp-4-7-scheduler-policy-deprecated"]
==== Scheduler policy

Using a scheduler policy to control pod placement is deprecated and is planned for removal in a future release. For more information on the Technology Preview alternative, see xref:../nodes/scheduling/nodes-scheduler-profiles.adoc#nodes-scheduler-profiles[Scheduling pods using a scheduler profile].

[id="ocp-4-7-filterbyos-deprecated"]
==== Catalog mirroring using filter-by-os flag

When using the `oc adm catalog mirror` command to mirror catalogs, the `--filter-by-os` flag was previously allowed to filter architectures of mirrored content. This would break references to those images in the catalog that point to the manifest list and not the manifest. The `--filter-by-os` flag now only filters the index image that is pulled and unpacked. To clarify this, the new `--index-filter-by-os` flag is now added and should be used instead.

The `--filter-by-os` flag is also now deprecated.

[id="ocp-4-7-removed-features"]
=== Removed features

[id="ocp-4-7-removed-provisioningHostIP-and-bootstrapProvisioningIP"]
==== Installer-provisioned clusters no longer require provisioningHostIP or bootstrapProvisioningIP

When using installer-provisioned installation on bare metal nodes, {product-title} 4.6 required providing two IP addresses from the `baremetal` network to the `provisioningHostIP` and `bootstrapProvisioningIP` configuration settings when deploying without a `provisioning` network. These IP addresses and configuration settings are no longer required in {product-title} 4.7 when using installer provisioned infrastructure on bare metal nodes and deploying without a `provisioning` network.

[id="ocp-4-7-images-removed-from-samples-imagestreams"]
==== Images removed from samples imagestreams

The following images are no longer included in the samples imagestreams provided with {product-title}:

----
registry.redhat.io/ubi8/go-toolset:1.13.4
registry.redhat.io/rhdm-7/rhdm-decisioncentral-rhel8:7.8.1
registry.redhat.io/rhdm-7/rhdm-decisioncentral-rhel8:7.8.0
registry.redhat.io/rhdm-7/rhdm-kieserver-rhel8:7.8.1
registry.redhat.io/rhdm-7/rhdm-kieserver-rhel8:7.8.0
registry.redhat.io/rhpam-7/rhpam-businesscentral-monitoring-rhel8:7.8.1
registry.redhat.io/rhpam-7/rhpam-businesscentral-monitoring-rhel8:7.8.0
registry.redhat.io/rhpam-7/rhpam-businesscentral-rhel8:7.8.0
registry.redhat.io/rhpam-7/rhpam-kieserver-rhel8:7.8.1
registry.redhat.io/rhpam-7/rhpam-kieserver-rhel8:7.8.0
registry.redhat.io/rhpam-7/rhpam-smartrouter-rhel8:7.8.1
registry.redhat.io/rhpam-7/rhpam-smartrouter-rhel8:7.8.0
----

[id="ocp-4-7-bug-fixes"]
== Bug fixes

*Bare Metal Hardware Provisioning*

* Previously, when trying to enable `baremetal` on assisted installer the `baremetal-operator` errors with `no bmc details`.  The Baseboard Management Controller (BMC) details can now be omitted for hosts in an unmanaged state. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1902653[*BZ#1902653*])

* Previously, when using virtual media on a Dell system, if the virtual media was already attached before the deployment commenced it would fail. Ironic now retries if this occurs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1910739[*BZ#1910739*])

* Previously, master nodes were losing their link-local address on the provisioning interface preventing provisioning from working. A workaround has been added to `toggle addr_gen_mode` to prevent this from occurring. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1909682[*BZ#1909682*])

* Previously on some systems, the installer would communicate with Ironic before it was ready and fail. This is now prevented. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1915295[*BZ#1915295*])

* Previously the `cluster-baremetal-operator` used the incorrect logging library. This issue resulted in command line arguments not being consistent with other Operators and not all Kubernetes library logs were getting logged. Switching the logging library has fixed this issue. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1906143[*BZ#1906143*])

* When using IPv6 on an interface, after a certain amount of time Network Manager removes the link-local IPv6 address. This issue led to PXE boot failures occurring for nodes after the IPv6 link-local address is removed. A workaround has been added to toggle the interface IPv6 addr_gen_mode which will cause the link-local address to be added back. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1901040[*BZ#1901040*])

* Previously Supermicro nodes boot to PXE upon reboot after successful deployment to disk. This issue is now fixed by always setting `BootSourceOverrideEnabled` when setting `BootSourceOverrideTarget`. Supermicro nodes now boot to disk persistently after deployment. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1918558[*BZ#1918558*])

* Service agent images shipped with `baremetal` IPI can now run on systems with UEFI secure boot enabled. Since network boot is not compatible with secure boot, using virtual media is required in this case. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1893648[*BZ#1893648*])

* Node auto-discovery is no longer enabled in `baremetal` IPI. It was not handled correctly and caused duplicate bare metal hosts registration. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1898517[*BZ#1898517*])

*Scale*

* The `nosmt` additional kernel argument which configures hyperthreading was previously undocumented for use with {product-title}. To disable hyperthreading, create a performance profile that is appropriate for your hardware and topology, and then set `nosmt` as an additional kernel argument.
+
For more information, see xref:../scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc#about_hyperthreading_for_low_latency_and_real_time_applications_cnf-master[About hyperthreading for low latency and real-time applications].

*Networking*

* The code in `ovn-kube` that detects the default gateway was not taking into consideration multipath environments. As a result, Kubernetes nodes failed to start because they could not find the default gateway. The logic has been modified to consider the first available gateway if multipath is present. OVN-Kubernetes now works in environments with multipath and multiple default gateways. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1914250[*BZ#1914250*])

* When deploying a cluster in dual stack mode OVN-Kubernetes was using the wrong source of truth.
+
The OVN-Kubernetes master node performs an initial synchronization to keep OVN and Kubernetes system databases in sync. This issue resulted in race conditions on OVN-Kubernetes startup leading to some of the Kubernetes services becoming unreachable. Bootstrap logic deleted these services as they were considered orphans.
+
This bug fix ensures Kubernetes is used as the source of truth. OVN-Kubernetes now starts correctly and keeps both OVN and Kubernetes in sync on startup. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1915295[*BZ#1915295*])

*Installer*

* Previously when virtual-media was used, fast-track mode would not work as expected as nodes were rebooted between operations. This issue is now fixed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1893546[*BZ#1893546*])

* Previously when using dual stack deployments, worker node host names did not match the name inspected before deployment causing nodes to need manual approval. This is now fixed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1895909[*BZ#1895909*])

* Bare metal provisioning now does not fail if there is a small, up to one hour, clock skew between the control plane and a host being deployed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1906448[*BZ#1906448*])

*Red Hat OpenShift Logging*

* Previously, logs were not sent to managed storage when legacy log forwarding was enabled. This happened because the internal generation of the `logforwarding` configuration improperly made a decision for  either `logforwarding` or legacy `logforwarding`. The current release fixes this issue:  Logs are sent to managed storage when the logstore is defined in the `clusterlogging` instance.  Additionally, logs are sent to legacy `logforwarding` when enabled regardless of whether a managed logstore is enabled or not. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1921263[*1921263*])

* Previously, the Fluentd collector pod went into a crash loop when the `ClusterLogForwarder` had an incorrectly-configured secret. The current release fixes this issue. Now, the `ClusterLogForwarder` validates the secrets and reports any errors in its status field. As a result, it does not cause the Fluentd collector pod to crash. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1888943[*1888943*])

* Previously, nodes did not recover from `Pending` status because a software bug did not correctly update their statuses in the Elasticsearch custom resource (CR). The current release fixes this issue, so the nodes can recover when their status is `Pending.` (link:https://bugzilla.redhat.com/show_bug.cgi?id=1887357[*BZ#1887357*])

* Previously, omitting the Storage size of the Elasticsearch node caused panic in the Elasticsearch Operator code. This panic appeared in the logs as: `Observed a panic: "invalid memory address or nil pointer dereference"` The panic happened because although Storage size is a required field, the software didn't check for it. The current release fixes this issue, so there is no panic if the storage size is omitted. Instead, the storage defaults to ephemeral storage and generates a log message for the user. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1899589[*BZ#1899589*])

* Previously, Elasticsearch rejected HTTP requests whose headers exceeded the default max header size, 8 KB. Now, the max header size is 128 KB, and Elasticsearch no longer rejects HTTP requests for exceeding the max header size. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1845293[*BZ#1845293*])

* Previously, when the Cluster Logging Operator (CLO) scaled down the number of Elasticsearch nodes in the `clusterlogging` CR to three nodes, it omitted previously-created nodes that had unique IDs. The Elasticsearch Operator rejected the update because it has safeguards that prevent nodes with unique IDs from being removed. Now, when the CLO scales down the number of nodes and updates the Elasticsearch CR, it marks nodes with unique IDs as count 0 instead of omitting them. As a result, users can scale down their cluster to 3 nodes by using the `clusterlogging` CR. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1879150[*BZ#1879150*])

* Previously, the Elasticsearch rollover pods failed with a `resource_already_exists_exception` error. Within the Elasticsearch rollover API, when the next index was created, the `*-write` alias was not updated to point to it. As a result, the next time the rollover API endpoint was triggered for that particular index, it received an error that the resource already existed.
+
The current release fixes this issue. Now, when a rollover occurs in the `indexmanagement` cronjobs, if a new index was created, it verifies that the alias points to the new index. This behavior prevents the error. If the cluster is already receiving this error, a cronjob fixes the issue so that subsequent runs work as expected. Now, performing rollovers no longer produces the exception. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1893992[*BZ#1893992*])

* Previously, if you updated the Kibana resource configuration in the `clusterlogging` instance to `resource{}`, the resulting nil map caused a panic and changed the status of the Elasticsearch Operator to `CrashLoopBackOff`. The current release fixes this issue by initializing the map. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1889573[*BZ#1889573*])

* Previously, if you deleted a Kibana route, the Cluster Logging Operator (CLO) could not recover or recreate it. Now, the CLO watches the route, and if you delete the route, the Elasticsearch Operator can reconcile or recreate it. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1890825[*BZ#1890825*])

* Previously, `elasticsearch-rollover` and `elasticsearch-delete` pods remained in the `Invalid JSON:` or `ValueError: No JSON object could be decoded` error states. This exception was raised because there was no exception handler for invalid JSON input. The current release fixes this issue by providing a handler for invalid JSON input. As a result, the handler outputs an error message instead of an exception traceback, and the `elasticsearch-rollover` and `elasticsearch-delete` jobs do not remain those error states. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1899905[*BZ#1899905*])

* Previously, in some cases, the Red Hat OpenShift Logging/Elasticsearch dashboard was missing from the {product-title} monitoring dashboard because the dashboard configuration resource referred to a different namespace owner and caused the {product-title} to garbage-collect that resource. Now, the ownership reference is removed from the Elasticsearch Operator reconciler configuration, and the logging dashboard appears in the console. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1910259[*BZ#1910259*])

* Previously, Fluent stopped sending logs even though the logging stack seemed functional. Logs were not shipped to an endpoint for an extended period even when an endpoint came back up. This happened if the max backoff time was too long and the endpoint was down. The current release fixes this issue by lowering the max backoff time, so the logs are shipped sooner. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1894634[*BZ#1894634*])

* Previously, if you deleted the secret, it was not recreated. Even though the certificates were on a disk local to the operator, they weren't rewritten because they hadn't changed. That is, certificates were only written if they changed. The current release fixes this issue. It rewrites the secret if the certificate changes or is not found. Now, if you delete the master certificates, they are replaced. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1901869[*BZ#1901869*])

* Previously, because of a bug, the software did not find some certificates and regenerated them. This triggered the Elasticsearch operator to perform a rolling upgrade on the Elasticsearch cluster, which sometimes produced mismatched certificates. The current release fixes this issue. Now the operator consistently reads and writes certificates to the same working directory and only regenerates the certificates if needed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1905910[*BZ#1905910*])

* Previously, queries to the root endpoint to retrieve the Elasticsearch version received a 403 response. The 403 response broke any services that used this endpoint in prior releases. This error happened because non-administrative users did not have the `monitor` permission required to query the root endpoint and retrieve the Elasticsearch version. Now, non-administrative users can query the root endpoint for the deployed version of Elasticsearch. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1906765[*BZ#1906765*])

*Builds*

* Previously, Dockerfile builds could not change permissions of the `/etc/pki/ca-trust` directory or create files inside it. This issue was caused by fixing link:https://bugzilla.redhat.com/show_bug.cgi?id=1826183[*BZ#1826183*] in version 4.6, which added support for HTTPS proxies with CAs for builds and *always* mounted `/etc/pki/ca-trust`, which prevented builds that included their own CAs or modified the system trust store from working correctly at runtime. The current release fixes this issue by reverting Bug 1826183. Now, builder images that include their own CAs work again. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1891759[*BZ#1891759*])

* Previously, after upgrading from {product-title} version 4.5 to version 4.6, running `git clone` from a private repository failed because builds did not add proxy information to the Git configuration that was used to pull the source code. As a result, the source code could not be pulled if the cluster used a global proxy and the source was pulled from a private Git repository. Now, Git is configured correctly when the cluster uses a global proxy and the `git clone` command can pull source code from a private Git repository if the cluster uses a global proxy. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1896446[*BZ#1896446*])

* Previously, the node pull secret feature did not work: node pull secrets were not used if `forcePull: true` was set in the Source and Docker strategy builds. As a result, builds failed to pull images that required the cluster-wide pull secret. Now, node pull secrets are always merged with user-provided pull secrets. As a result, builds can pull images when `forcePull: true` is set, and the source registry requires the cluster-wide pull secret. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1883803[*BZ#1883803*])

* Previously, {product-title} builds failed on `git clone` when SCP-style SSH locations were specified because of Golang URL parsing, which does not accommodate Git SCP-styled SSH locations. As a result, {product-title} builds and Source-to-Image (S2I) failed when those types of source URLs were supplied. Now, builds and S2I bypass Golang URL parsing and strip the `ssh://` prefix to accommodate Git SCP-styled SSH locations (link:https://bugzilla.redhat.com/show_bug.cgi?id=1884270[*BZ#1884270*])

*Node Tuning Operator*

* When an invalid Tuned profile is created, the `openshift-tuned` supervisor process may ignore future profile updates and fail to apply the updated profile. This bug fix keeps state information about Tuned profile application success or failure. Now `openshift-tuned` recovers from profile application failures on receiving new valid profiles. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1919970[*BZ#1919970*])

*Web console (Developer perspective)*

* Previously, the user was denied access to pull images from other projects, due to insufficient user permissions. This bug fix removes all the user interface checks for role bindings and shows the `oc` command alert to help users use the command line. With this bug fix, the user is no longer blocked from creating images from different namespaces and is now able to deploy images from their other projects. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1894020[*BZ#1894020*])

* The console used a prior version of the `KafkaSource` object that used the `resources` and `service account` fields in their specification. The latest v1beta1 version of the `KafkaSource` object removed these fields, due to which the user was unable to create the `KafkaSource` object with the v1beta1 version. This issue has been fixed now and the user is able to create the `KafkaSource` object with the v1beta1 version. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1892653[*BZ#1892653*])

* Previously, when you created an application using source code from Git repositories with the `.git` suffix, and then clicked the edit source code link, a "page not found" error was displayed. This fix removes the `.git` suffix from the repository URL and transforms the SSH URL to an HTTPS URL. The generated link now leads to the correct repository page. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1896296[*BZ#1896296*])

* Previously, the underlying `SinkBinding` resources were shown in the *Topology* view, along with the actual source created in the case of `Container Source` and `KameletBinding` resources, confusing users. This issue was fixed. Now, only the actual resource created for the event source is displayed in the *Topology* view, and the underlying `SinkBinding` resources, if created, are displayed in the sidebar. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1906685[*BZ#1906685*])

* Previously, when you installed the Serverless Operator, without creating the eventing custom resource, a channel card was displayed. When you clicked the card, a confusing alert message was displayed. This issue has now been fixed. The channel card, with a proper alert message, is now displayed only if the channel custom resource definition is present. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1909092[*BZ#1909092*])

* Previously, when you closed the web terminal connection, all the terminal output from that session, which can be useful to you, disappeared. This issue has been fixed. The terminal output is now retained even after the session is closed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1909067[*BZ#1909067*])

* Technology preview badges were displayed on the Eventing user interface although it had its GA release with {product-title} 4.6. The Technology preview badges are now removed and the changes were back-ported to the {product-title} 4.6.9 version. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1894810[*BZ#1894810*])

* Previously, volume mounts for deployments were not preserved if the deployment was edited using the console edit flows. The modified deployment YAML overwrote or removed the volume mounts in the pod template specification. This issue has been fixed. The volume mounts are now preserved even when the deployment is edited using the console edit flows. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1867965[*BZ#1867965*])

* In case of multiple triggers one subscribing to Knative service and another to In Memory Channel as subscriber, the Knative resources were not displayed on the *Topology* view. This issue has been fixed now, so that the Knative data model returns proper data, and the Knative resources are displayed on the *Topology* view. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1906683[*BZ#1906683*])

* Previously, in a disconnected environment, the Helm charts were not displayed in the *Developer Catalog* due to an invalid configuration while fetching code. This issue has been fixed by ensuring that proxy environment variables are considered and the Helm charts are now displayed on the Developer Catalog. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1918748[*BZ#1918748*])

* While running a Pipeline, the log tab of the `TaskRun` resource displayed the string as `undefined` after the command in the output. This was caused due to some edge cases where some internal string operations printed  `undefined` to the log output. This issue has been fixed now, and the pipeline log output does not drop empty lines from the log stream and does not print the string `undefined` any longer. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1915898[*BZ#1915898*])

* Previously, the *Port* list in the *Add* flow only provided options for exposed ports and did not allow you to specify a custom port. The list has now been replaced by a typeahead select menu, and now it is possible to specify a custom port while creating the application. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1881881[*BZ#1881881*])

* Previously, when conditional tasks failed, the completed pipeline runs showed a permanent pending task for each failed conditional task. This issue has been fixed by disabling the failed conditional tasks and by adding skipped icons to them. This gives a better picture of the state of the pipeline run. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1880389[*BZ#1880389*])

* Previously, the pod scale up or down buttons were available for a single pod resource, and the page crashed when the user pressed the scale button. This issue has been fixed by not showing the scale up or down buttons for a single pod resource. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1909678[*BZ#1909678*])

* Previously, the chart URL for downloading the chart to instantiate a helm release was unreachable. This happened because the `index.yaml` file from the remote repository, referenced in the Helm chart repository, was fetched and used as is. Some of these index files contained relative chart URLs. This issue has now been fixed by translating relative chart URLs to absolute URLs, which makes the chart URL reachable. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1912907[*BZ#1912907*])

*Performance Addon Operator*

* Previously, incorrect wait in the must-gather logic resulted in too early termination of log gathering. This issue resulted in depending on timing the log gathering operation being interrupted prematurely, leading to partial log collection.
This is now fixed by adding the correct wait in the must-gather logic. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1906355[*BZ#1906355*])

* Previously, must-gather collected an unbounded amount of kubelet logs on all nodes. This issue resulted in an excessive amount of data being transferred and collected, with no clear benefit for the user.
+
This issue is fixed by collecting a bounded amount, the last eight hours, of kubelet logs only on worker nodes and not collecting kubelet logs on the control plane nodes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1918691[*BZ#1918691*])


[id="ocp-4-7-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.5 |OCP 4.6 |OCP 4.7

|Precision Time Protocol (PTP)
|TP
|TP
|

|`oc` CLI Plug-ins
|TP
|TP
|

|experimental-qos-reserved
|TP
|TP
|

|Ephemeral Storage Limit/Requests
|TP
|TP
|

|Descheduler
|TP
|TP
|GA

|Podman
|TP
|TP
|

|OVN-Kubernetes Pod network provider
|TP
|GA
|GA

|HPA custom metrics adapter based on Prometheus
|TP
|TP
|

|HPA for memory utilization
|TP
|TP
|GA

|Service Binding
|TP
|TP
|

|Log forwarding
|TP
|GA
|GA

|Monitoring for user-defined projects
|TP
|GA
|GA

|Raw Block with Cinder
|TP
|TP
|TP

|External provisioner for AWS EFS
|TP
|TP
|TP

|CSI volume snapshots
|TP
|TP
|GA

|CSI volume cloning
|TP
|GA
|GA

|CSI volume expansion
|TP
|TP
|TP

|vSphere Problem Detector Operator
|-
|-
|GA

|CSI GCP PD Driver Operator
|-
|-
|TP

|CSI OpenStack Cinder Driver Operator
|-
|-
|TP

|CSI AWS EBS Driver Operator
|TP
|TP
|TP

|Red Hat Virtualization (oVirt) CSI Driver Operator
|-
|GA
|GA

|CSI inline ephemeral volumes
|TP
|TP
|TP

|Automatic device discovery and provisioning with Local Storage Operator
|-
|TP
|TP

|OpenShift Pipelines
|TP
|TP
|TP

|OpenShift GitOps
|-
|TP
|TP

|Vertical Pod Autoscaler
|TP
|TP
|

|Operator API
|TP
|GA
|GA

|Adding kernel modules to nodes
|TP
|TP
|

|Egress router CNI plug-in
|-
|-
|TP

|Scheduler profiles
|-
|-
|TP

|Non-preempting priority classes
|-
|-
|TP

|Kubernetes NMState Operator
|
|
|TP

|====

[id="ocp-4-7-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.7 and beyond!
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to 4.7, you can either revoke or continue to allow unauthenticated access. It is recommended to revoke unauthenticated access unless there is a specific need for it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

* {op-system} real time (RT) kernels are currently only supported on compute nodes, not control plane nodes. Compact clusters are not supported with RT kernels in {product-title} 4.7. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1887007[*BZ#1887007*])

* It is currently not possible to use the AWS Secure Token Service (STS), which is a Technology Preview feature, in a cluster installed into the AWS C2S Secret Region due to current {product-title} limitations. This will be fixed in a future release of {product-title}. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1927157[*BZ#1927157*])

* Installing a cluster into the AWS C2S Secret Region using your own infrastructure based on xref:../installing/installing_aws/installing-aws-user-infra.adoc#installing-aws-user-infra[Red Hat's recommended CloudFormation templates] does not work due to issues with creating the bootstrap nodes during the installation process. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1924080[*BZ#1924080*])

* Upgrading Performance Addon Operator from 4.6 to 4.7 fails with the error:
+
[source,terminal]
----
"Warning  TooManyOperatorGroups  11m   operator-lifecycle-manager  csv created in namespace with multiple operatorgroups, can't pick one automatically"
----
+
Before upgrading, follow the procedure as described in xref:../scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc#upgrading-performance-addon-operator-configured-for-a-specific-namespace_cnf-master[Upgrading Performance Addon Operator when previously installed to a specific namespace].
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1913826[*BZ#1913826*])

* A reboot is sometimes required to enact SR-IOV changes on supported NICs. SR-IOV currently issues the reboot when it is ready. If this reboot coincides with changes in the Machine Config policy, the node can be left in an undetermined state.  The Machine Config Operator assumes that the updated policy has been applied when it has not.
+
[NOTE]
====
This race condition can also be caused by adding a node to a Machine Config Pool that has MCP and SR-IOV changes.
====
+
To avoid this issue, new nodes requiring MCO and SR-IOV changes should be completed sequentially. First, apply all MCO configuration and wait for the nodes to settle. Then, apply the SR-IOV configuration.
+
If a new node is being added to a Machine Config Pool that includes SR-IOV, this issue can be avoided by removing the SR-IOV policy from the Machine Config Pool and then adding the new worker. Then, re-apply the SR-IOV policy.
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1921321[*BZ#1921321*])

* The `stalld` service triggers a bug in the kernel, which results in the node freezing. In order to work around this issue, the Performance Addon Operator disables `stalld` by default. The fix impacts latency associated with DPDK based workloads, however the functionality will be restored once the kernel bug (link:https://bugzilla.redhat.com/show_bug.cgi?id=1912118[*BZ#1912118*]) is fixed.

* Fluentd pods with the `ruby-kafka-1.1.0` and `fluent-plugin-kafka-0.13.1` gems are not compatible with Apache Kafka version 0.10.1.0.
+
As a result, log forwarding to Kafka fails with a message: `error_class=Kafka::DeliveryFailed error="Failed to send messages to flux-openshift-v4/1"`
+
The `ruby-kafka-0.7` gem dropped support for Kafka 0.10 in favor of native support for Kafka 0.11. The `ruby-kafka-1.0.0` gem added support for Kafka 2.3 and 2.4. The current version of OpenShift Logging tests and therefore supports Kafka version 2.4.1.
+
To work around this issue, upgrade to a supported version of Apache Kafka.
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1907370[*BZ#1907370*])

* Precision Time Protocol (PTP) faults are observed on the Mellanox MT27800 Family [ConnectX-5] of adapter cards. In the `ptp4l` log, errors are observed which disturb clock synchronization.
+
These errors result in larger than normal system clock updates due to the NIC hardware clock resetting.
The root cause of this issue is unknown and no workaround currently exists.
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1913279[*BZ#1913279*])

* When attempting an IPI installation on bare metal using the latest Dell firmware (04.40.00.00) nodes will not be deployed and an error will show in their status. This is due to Dell Firmware (4.40.00.00) using eHTML5 as the Virtual Console Plug-in.
+
To work around this issue, change the Virtual Console Plugin to HTML5 and run the deployment again. The nodes should now be successfully deployed. For more information, see xref:../installing/installing_bare_metal_ipi/ipi-install-prerequisites.adoc#ipi-install-firmware-requirements-for-installing-with-virtual-media_ipi-install-prerequisites[Firmware requirements for installing with virtual media].
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1915828[*BZ#1915828*])

* Installing a cluster on {rh-openstack} that uses Kuryr times out with the following messages during bootstrapping:
+
[source,terminal]
----
INFO Waiting up to 20m0s for the Kubernetes API at https://api.ostest.shiftstack.com:6443...
INFO API v1.20.0+ba45583 up
INFO Waiting up to 30m0s for bootstrapping to complete...
ERROR Attempted to gather ClusterOperator status after wait failure: listing ClusterOperator objects: Get "https://api.ostest.shiftstack.com:6443/apis/config.openshift.io/v1/clusteroperators": dial tcp 10.46.44.166:6443: connect: connection refused
INFO Use the following commands to gather logs from the cluster
INFO openshift-install gather bootstrap --help
FATAL failed to wait for bootstrapping to complete: timed out waiting for the condition
----
+
The timeout is caused by changes in how Kuryr detects the {rh-openstack} Networking service (neutron) subnet of the cluster's nodes.
+
As a workaround, do not remove the control plane machine manifests as described by the "Creating the Kubernetes manifest and Ignition config files" section in the installation documentation. When you are instructed to run the following command:
+
[source,terminal]
----
$ rm -f openshift/99_openshift-cluster-api_master-machines-*.yaml openshift/99_openshift-cluster-api_worker-machineset-*.yaml
----
+
Run this command instead:
+
[source,terminal]
----
$ rm -f openshift/99_openshift-cluster-api_worker-machineset-*.yaml
----
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1927244[*BZ#1927244*])

* In {product-title} 4.3 and 4.4, if the user has the console open in multiple tabs, some sidebar links in the *Developer perspective* do not directly link to the project, and there is an unexpected shift in the selected project. This will be resolved in a future release. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1839101[BZ#1839101])
* In {product-title} 4.5, a user with scale permissions cannot scale a deployment or deployment config using the console if they do not have edit rights to the deployment or deployment config. This will be resolved in a future release. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1886888[BZ#1886888])
* In {product-title} 4.5, when there is minimal or no data in the *Developer Console*, most of the monitoring charts or graphs (CPU consumption, memory usage, and bandwidth) show a range of -1 to 1. However, none of these values can ever go below zero. This will be resolved in a future release. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1904106[BZ#1904106])
* Currently, the prerequisites in the web console quick start cards appear as a paragraph instead of a list. This will be resolved in a future release. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1905147[BZ#1905147])
* Currently, in the *Search Page*, the *Pipelines* resources table is not immediately updated after the *Name* filter is applied or removed. However, if you refresh the page or close and expand the *Pipelines* section, the *Name* filter is applied. The same behavior is seen when you remove the *Name* filter. This will be resolved in a future release. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1901207[BZ#1901207]).

[id="ocp-4-7-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} 4.7 are released as asynchronous errata through the Red Hat Network. All {product-title} 4.7 errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified via email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} 4.7. Versioned asynchronous releases, for example with the form {product-title} 4.7.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster.adoc#updating-cluster[updating your cluster] properly.
====

[id="ocp-4-7-0-ga"]
=== RHBA-2021:1234 - {product-title} 4.7 image release and bug fix advisory

Issued: 2021-xx-xx

{product-title} release 4.7 is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2020:1234[RHBA-2020:1234] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2020:5678[RHBA-2020:5678] advisory.

Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:

link:https://access.redhat.com/solutions/<ARTICLE_ID>[{product-title} 4.7.0 container image list]
