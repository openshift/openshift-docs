[id="ocp-4-7-release-notes"]
= {product-title} {product-version} release notes
include::modules/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multi-tenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-7-about-this-release"]
== About this release

// TODO: Update k8s link once there is a version-specific URL for the Kubernetes release
(link:https://access.redhat.com/errata/RHBA-2020:1234[RHBA-2020:1234]) is now available. This release uses link:https://kubernetes.io/docs/setup/release/notes/[Kubernetes 1.20] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

//Red Hat did not publicly release {product-title} 4.6.0 as the GA version and, instead, is releasing {product-title} 4.6.1 as the GA version.

{product-title} {product-version} clusters are available at https://cloud.redhat.com/openshift. The {cloud-redhat-com} application for {product-title} allows you to deploy OpenShift clusters to either on-premise or cloud environments.

{product-title} {product-version} is supported on {op-system-base-full} 7.7 or later, as well as {op-system-first} 4.6.

You must use {op-system} machines for the control plane, which are also known as master machines, and you can use either {op-system} or {op-system-base-full} 7.7 or later for compute machines, which are also known as worker machines.

[IMPORTANT]
====
Because only {op-system-base-full} version 7.7 or later is supported for compute machines, you must not upgrade the {op-system-base} compute machines to version 8.
====

//{product-title} 4.6 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

With the release of {product-title} 4.7, version 4.4 is now end of life. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift Container Platform Life Cycle Policy].

[id="ocp-4-7-inclusive-language"]
== Making open source more inclusive

Red Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases. For more details, see link:https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language[Red Hat CTO Chris Wrightâ€™s message].

[id="ocp-4-7-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-7-rhcos"]
=== {op-system-first}

[id="ocp-4-7-enhanced-disk-provisioning"]
==== Enhanced disk provisioning for LUKS, RAID, and FBA DASD

{product-title} 4.7 includes several improvements to disk provisioning for bare metal deployments. The following features are currently supported for new 4.7 clusters only:

* Native Ignition support for LUKS disk encryption provides additional configurability for encrypted root filesystems, as well as support for encryption of additional data filesystems.
* {op-system} now supports boot disk mirroring, except on s390x, providing redundancy in the case of disk failure.
* {op-system} on s390x can be installed onto fixed-block architecture (FBA)-type direct access storage device (DASD) disks.

* Multipathing is now supported during initial deployment and early boot for storage devices when attached to clusters that are created using {product-title} 4.7 or higher.

[NOTE]
====
On new clusters, LUKS configuration must use the native Ignition mechanism, as provisioning fails if the legacy `/etc/clevis.json` file is included in the machine config. On clusters that are upgrading from {product-title} 4.6 or earlier, LUKS can only be configured by using `/etc/clevis.json`.
====

[id="ocp-4-7-bootupd"]
==== Update the bootloader by using `bootupd`

With `bootupd`, {op-system} users now have access to a cross-distribution, system-agnostic OS update tool that manages firmware and boot updates in UEFI and legacy BIOS boot modes that run on modern architectures.

[id="ocp-4-7-rhcos-rhel-8-3-packages"]
==== {op-system} now supports RHEL 8.3

{op-system} is now using Red Hat Enterprise Linux (RHEL) 8.3 packages. {product-title} 4.6 and below will stay with RHEL 8.2 packages. This enables you to have the latest fixes, features, and enhancements, such as NetworkManager features, as well as the latest hardware support and driver updates.

[id="ocp-4-7-rhcos-kdump"]
==== {op-system} now supports `kdump` service (Technical Preview)
The `kdump` service is introduced in Technical Preview in {op-system} to provide a crash-dumping mechanism for debugging kernel issues. You can use this service to save system memory content for later analysis. The `kdump` service is not managed at the cluster-level and must be enabled and configured manually on a per-node basis.

[id="ocp-4-7-ignition"]
==== Ignition updates
The following Ignition updates are now available:

* {op-system} now supports Ignition config spec 3.2.0. This update provides support for disk partition resizing, LUKS encrypted storage, and `gs://` URLs.
* When executing in non-default AWS partitions, such as GovCloud or AWS China, Ignition now fetches `s3://` resources from the same partition.
* Ignition now supports AWS EC2 Instance Metadata Service Version 2 (IMDSv2).

[id="ocp-4-7-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-7-aws-c2s-secret-region"]
==== Installing a cluster into the AWS C2S Secret Region

You can now install a cluster on Amazon Web Services (AWS) into the Commercial Cloud Services (C2S) Secret Region. Because the C2S region does not have an {op-system} AMI published by Red Hat, you must upload a custom AMI that belongs to that region. You are also required to include the CA certificates for C2S in the `additionalTrustBundle` field of the `install-config.yaml` file during cluster installation. Clusters deployed to the C2S Secret Region do not have access to the Internet; therefore, you must configure a private image registry.

The installation program does not support destroying a cluster deployed to the C2S region; you must manually remove the resources of the cluster.

//Link for more info when available.

[id="ocp-4-7-gcp-disk-encryption"]
==== Installing cluster on GCP with disk encryption using a personal encryption key

You can now install a cluster on Google Cloud Platform (GCP) and use a personal encryption key to encrypt both virtual machines and persistent volumes. This is done by setting the `controlPlane.platform.gcp.osDisk.encryptionKey`, `compute.platform.gcp.osDisk.encryptionKey`, or `gcp.defaultMachinePlatform.osDisk.encryptionKey` field in the `install-config.yaml` file.

//Link for more info when available.

[id="ocp-4-7-web-console"]
=== Web console

[id="ocp-4-7-web-console-localization"]
==== Web console localization

The web console is now localized and provides language support for global users. English, Japanese, and Simplified Chinese are currently supported. The displayed language follows your browser preferences, but you can also select a language to override the browser default. From the *User* drop-down menu, select *Language preferences* to update your language setting. Localized date and time is now also supported.

[id="ocp-4-7-web-console-insights-plugin"]
==== Insights plug-in

The xref:../support/remote_health_monitoring/using-insights-to-identify-issues-with-your-cluster.adoc#displaying-the-insights-status-in-the-web-console_using-insights-to-identify-issues-with-your-cluster[Insights plug-in] is now integrated into the {product-title} web console. Insights provides cluster health data, such as the number of total issues and total risks of the issues. Risks are labeled as *Critical*, *Important*, *Moderate*, or *Low*. You can quickly navigate to {cloud-redhat-com} for further details about the issues and how to fix them.

[id="ocp-4-7-security"]
=== Security and compliance

[id="ocp-4-7-security-user-oauth-tokens"]
==== Managing user-owned OAuth access tokens

Users can now manage their own OAuth access tokens. This allows users to review their tokens and delete any tokens that have timed out or are no longer needed.

For more information, see xref:../authentication/managing-oauth-access-tokens.adoc#managing-oauth-access-tokens[Managing user-owned OAuth access tokens].

[id="ocp-4-7-security-cco-deletion-gcp-creds-mint-mode"]
==== Cloud Credential Operator support for deletion of GCP root credentials after installation

You can now remove or rotate the GCP admin-level credential that the xref:../operators/operator-reference.adoc#cloud-credential-operator_red-hat-operators[Cloud Credential Operator] uses in Mint mode. This option requires the presence of the admin-level credential during installation, but the credential is not stored in the cluster permanently and does not need to be long-lived.

[id="ocp-4-7-compliance-cis-benchmark"]
==== CIS Kubernetes Benchmark profile for the Compliance Operator

You can now use the Compliance Operator to perform Center for Internet Security (CIS) Kubernetes Benchmark checks. CIS profiles for {product-title} are based on the CIS Kubernetes checks.

[id="ocp-4-7-security-secure-boot"]
==== Secure Boot support for installer-provisioned clusters

You can now deploy a cluster with Secure Boot when using installer-provisioned infrastructure on bare metal nodes. Deploying a cluster with Secure Boot requires UEFI boot mode and Red Fish Virtual Media. You cannot use self-generated keys with Secure Boot.

[id="ocp-4-7-networking"]
=== Networking

[id="ocp-4-7-ovn-kubernetes-migration"]
==== Expanded platform support for migrating from the OpenShift SDN cluster network provider to the OVN-Kubernetes cluster network provider

A xref:../networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc#migrate-from-openshift-sdn[migration to the OVN-Kubernetes cluster network provider] is now supported on installer-provisioned clusters on the following platforms:

* Bare metal hardware
* Amazon Web Services (AWS)
* Google Cloud Platform (GCP)
* Microsoft Azure
* {rh-openstack-first}
* VMware vSphere

[id="ocp-4-7-ovn-kubernetes-egress-firewall-dns"]
==== OVN-Kubernetes egress firewall support for DNS rules

When configuring an egress firewall rule, you can now use a xref:../networking/ovn_kubernetes_network_provider/configuring-egress-firewall-ovn.adoc#domain-name-server-resolution_configuring-egress-firewall-ovn[DNS domain name] instead of an IP address.
With the addition of DNS support in the OVN-Kubernetes cluster network provider egress firewall implementation, parity is achieved with the OpenShift SDN cluster network provider egress firewall implementation.

[id="ocp-4-7-sr-iov-dpdk-functions"]
==== Library for interacting with SR-IOV virtual functions in DPDK mode within containers

For containers interacting with SR-IOV virtual functions (VFs) in Data Plane Development Kit (DPDK) mode, the `app-netutil` library now provides the following functions: `GetCPUInfo()`, `GetHugepages()`, and `GetInterfaces()`. For more information, see xref:../networking/hardware_networks/about-sriov.adoc#nw-sriov-app-netutil_about-sriov[DPDK library for use with container applications].

[id="ocp-4-7-networking-k8s-nmstate"]
==== Kubernetes NMState Operator (Technology Preview)

{product-title} 4.7 provides post-installation state-driven network configuration on the secondary network interfaces of cluster nodes using the Kubernetes NMState Operator as a Technology Preview feature. For more information, see
xref:../networking/k8s_nmstate/k8s-nmstate-using-kubernetes-nmstate.adoc[Using Kubernetes NMState (Technology Preview)].

[NOTE]
====
Configuration must occur before scheduling pods.
====

[id="ocp-4-7-storage"]
=== Storage

[id="ocp-4-7-storage-csi-snapshots"]
==== Persistent storage using CSI volume snapshots is generally available

You can use the Container Storage Interface (CSI) to create, restore, and delete a volume snapshot when using CSI drivers that provide support for volume snapshots. This feature was previously introduced as a Technology Preview feature in {product-title} 4.4 and is now generally available and enabled by default in {product-title} 4.7.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-snapshots.adoc#persistent-storage-csi-snapshots[Using CSI volume snapshots].

[id="ocp-4-5-persistent-storage-csi-gcp-pd"]
==== Persistent storage using the GCP PD CSI Driver Operator (Technology Preview)

The Google Cloud Platform (GCP) persistent disk (PD) CSI driver is automatically deployed and managed on GCP environments, allowing you to dynamically provision these volumes without having to install the driver manually. The GCP PD CSI Driver Operator that manages this driver is in Technology Preview.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-gcp-pd.adoc#persistent-storage-csi-gcp-pd[GCP PD CSI Driver Operator].

[id="ocp-4-5-persistent-storage-csi-cinder"]
==== Persistent storage using the OpenStack Cinder CSI Driver Operator

You can now use CSI to provision a persistent volume using the CSI driver for OpenStack Cinder.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-cinder.adoc#persistent-storage-csi-cinder[OpenStack Cinder CSI Driver Operator].

[id="ocp-4-7-storage-vsphere-problem-detector"]
==== vSphere Problem Detector Operator

The vSphere Problem Detector Operator periodically checks functionality of {product-title} clusters installed in a vSphere environment. The vSphere Problem Detector Operator is installed by default by the Cluster Storage Operator, allowing you to quickly identify and troubleshoot common storage issues, such as configuration and permissions, on vSphere clusters.

[id="ocp-4-7-registry"]
=== Registry

[id="ocp-4-7-registry-oci-support"]
==== Open Container Initiative images support

The {product-title} internal registry and image streams now support Open Container Initiative (OCI) images. You can use OCI images in the same way you would use Docker `schema2` images.

//Add link

[id="ocp-4-7-operators"]
=== Operator lifecycle


[id="ocp-4-7-images"]
=== Images


[id="ocp-4-7-machine-api"]
=== Machine API

[id="ocp-4-7-machine-api-wait-upgrade"]
==== Updates are immediately blocked if a machine config pool is degraded

If a machine config pool (MCP) is in a `degraded` state, the Machine Config Operator (MCO) now reports its *Upgradeable* status as *False*. As a result, you are now prevented from performing an update within a minor version, for example, from 4.7 to 4.8, until all machine config pools are healthy. Previously, with a degraded machine config pool, the Machine Config Operator did not report its *Upgradeable* status as *false*. The update was allowed and would eventually fail when updating the Machine Config Operator because of the degraded machine config pool. There is no change in this behavior for updates within z-stream releases, for example, from 4.7.1 to 4.7.2. As such, you should check the machine config pool status before performing a z-stream update.

[id="ocp-4-7-nodes"]
=== Nodes

[id="ocp-4-7-nodes-descheduler-ga"]
==== Descheduler is generally available

The descheduler is now generally available. The descheduler provides the ability to evict a running pod so that the pod can be rescheduled onto a more suitable node. You can enable one or more of the following descheduler profiles:

* `AffinityAndTaints`: evicts pods that violate inter-pod anti-affinity, node affinity, and node taints.
* `TopologyAndDuplicates`: evicts pods in an effort to evenly spread similar pods, or pods of the same topology domain, among nodes.
* `LifecycleAndUtilization`: evicts long-running pods and balances resource usage between nodes.

[NOTE]
====
With the GA, you can enable descheduler profiles and configure the descheduler interval. Any other settings that were available during Technology Preview are no longer available.
====

For more information, see xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler[Evicting pods using the descheduler].

[id="ocp-4-7-nodes-scheduler-profiles"]
==== Scheduler profiles (Technology Preview)

You can now specify a scheduler profile to control how pods are scheduled onto nodes. This is a replacement for configuring a scheduler policy. The following scheduler profiles are available:

* `LowNodeUtilization`: This profile attempts to spread pods evenly across nodes to get low resource usage per node.

* `HighNodeUtilization`: This profile attempts to place as many pods as possible onto as few nodes as possible, to minimize node count with high usage per node.

* `NoScoring`: This is a low-latency profile that strives for the quickest scheduling cycle by disabling all score plug-ins. This might sacrifice better scheduling decisions for faster ones.

For more information, see xref:../nodes/scheduling/nodes-scheduler-profiles.adoc#nodes-scheduler-profiles[Scheduling pods using a scheduler profile].

[id="ocp-4-7-nodes-autoscaling-ga"]
==== Autoscaling for memory utilization GA

Autoscaling for memory utilization is now generally available. You can create horizontal pod autoscaler custom resources to automatically scale the pods associated with a deployment config or replication controller to maintain the average memory utilization you specify, either a direct value or a percentage of requested memory. For more information, see xref:../nodes/pods/nodes-pods-autoscaling.html#nodes-pods-autoscaling-creating-memory_nodes-pods-autoscaling[Creating a horizontal pod autoscaler object for memory utilization].

[id="ocp-4-7-nodes-non-preempting-priority-classes"]
==== Non-preempting option for priority classes (Technology Preview)

You can now configure a priority class to be non-preempting by setting the `preemptionPolicy` field to `Never`. Pods with this priority class setting are placed in the scheduling queue ahead of lower priority pods, but do not preempt other pods.

For more information, see xref:../nodes/pods/nodes-pods-priority.html#non-preempting-priority-class_nodes-pods-priority[Non-preempting priority classes].

[id="ocp-4-7-logging"]
=== Cluster logging


[id="ocp-4-7-monitoring"]
=== Monitoring

[id="ocp-4-7-monitoring-api-performance-dashboard"]
==== New API performance monitoring dashboard

The API performance dashboard is now available from the web console. This dashboard can be used to help troubleshoot performance issues with the Kubernetes API server or the OpenShift API server. You can access the API performance dashboard from the web console by navigating to *Monitoring* -> *Dashboards* and selecting the *API Performance* dashboard.

This dashboard provides API server metrics, such as:

* Request duration
* Request rate
* Request termination
* Requests in flight
* Requests aborted
* etcd request duration
* etcd object count
* Long-running requests
* Response status code
* Response size
* Priority and fairness

// TODO: Link to troubleshooting using this dashboard section once it is available

[id="ocp-4-7-scale"]
=== Scale

[id="ocp-4-7-cnf-latency-test"]
==== Test to determine CPU latency
The latency test, a part of the CNF-test container, provides a way to measure if the isolated CPU latency is below the requested upper bound.

For information about running a latency test, see xref:../scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc#cnf-performing-end-to-end-tests-running-the-latency_tests[Running the latency tests].

[id="ocp-4-7-dev-exp"]
=== Developer experience



[id="ocp-4-7-insights-operator"]
=== Insights Operator

[id="ocp-4-7-insights-operator-data-collection-enhancements"]
==== Insights Operator data collection enhancements

In {product-title} 4.7, the Insights Operator collects the following additional information:

* The top 100 `InstallPlan` entries to identify invalid Operator Lifecycle Manager (OLM) installations
* The service accounts from the Kubernetes default namespace and the `openshift*` built-in namespaces
* The `ContainerRuntimeConfig` and `MachineConfigPools` configuration files to verify container storage limits
* The configuration files for all available `operator.openshift.io` control pane resources to identify Operators in unmanaged states
* The `NetNamespaces` names, including their `netID` and egress IP addresses
* The `StatefulSet` configuration definitions from the {product-title} default namespaces to check the Prometheus monitoring storage type
* Appearances of certain log entries of pods in the `openshift-apiserver-operator` namespace
* Appearances of certain log entries of `sdn` pods in the `openshift-sdn` namespace

With this additional information, Red Hat can provide improved remediation steps in {cloud-redhat-com}.


[id="ocp-4-7-notable-technical-changes"]
== Notable technical changes

{product-title} 4.7 introduces the following notable technical changes.



[id="ocp-4-7-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more fine-grained functionality that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *DEP*: _Deprecated_
* *REM*: _Removed_

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.5 |OCP 4.6 |OCP 4.7

|`OperatorSource` objects
|DEP
|REM
|REM

|Package Manifest Format (Operator Framework)
|DEP
|DEP
|

|`oc adm catalog build`
|DEP
|DEP
|

|v1beta1 CRDs
|DEP
|DEP
|

|Docker Registry v1 API
|GA
|DEP
|

|Metering Operator
|GA
|DEP
|DEP

|Scheduler policy
|GA
|GA
|DEP

|====

[id="ocp-4-7-deprecated-features"]
=== Deprecated features

[id="ocp-4-7-scheduler-policy-deprecated"]
==== Scheduler policy

Using a scheduler policy to control pod placement is deprecated and is planned for removal in a future release. For more information on the Technology Preview alternative, see xref:../nodes/scheduling/nodes-scheduler-profiles.adoc#nodes-scheduler-profiles[Scheduling pods using a scheduler profile].

[id="ocp-4-7-removed-features"]
=== Removed features

[id="ocp-4-7-removed-provisioningHostIP-and-bootstrapProvisioningIP"]
==== Installer-provisioned clusters no longer require provisioningHostIP or bootstrapProvisioningIP

When using installer-provisioned installation on bare metal nodes, {product-title} 4.6 required providing two IP addresses from the `baremetal` network to the `provisioningHostIP` and `bootstrapProvisioningIP` configuration settings when deploying without a `provisioning` network. These IP addresses and configuration settings are no longer required in {product-title} 4.7 when using installer provisioned infrastructure on bare metal nodes and deploying without a `provisioning` network.


[id="ocp-4-7-bug-fixes"]
== Bug fixes



[id="ocp-4-7-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.5 |OCP 4.6 |OCP 4.7

|Precision Time Protocol (PTP)
|TP
|TP
|

|`oc` CLI Plug-ins
|TP
|TP
|

|experimental-qos-reserved
|TP
|TP
|

|Ephemeral Storage Limit/Requests
|TP
|TP
|

|Descheduler
|TP
|TP
|GA

|Podman
|TP
|TP
|

|OVN-Kubernetes Pod network provider
|TP
|GA
|GA

|HPA custom metrics adapter based on Prometheus
|TP
|TP
|

|HPA for memory utilization
|TP
|TP
|GA

|Service Binding
|TP
|TP
|

|Log forwarding
|TP
|GA
|GA

|Monitoring for user-defined projects
|TP
|GA
|GA

|Raw Block with Cinder
|TP
|TP
|TP

|External provisioner for AWS EFS
|TP
|TP
|TP

|CSI volume snapshots
|TP
|TP
|GA

|CSI volume cloning
|TP
|GA
|GA

|CSI volume expansion
|TP
|TP
|TP

|vSphere Problem Detector Operator
|-
|-
|GA

|CSI GCP PD Driver Operator
|-
|-
|TP

|CSI OpenStack Cinder Driver Operator
|-
|-
|TP

|CSI AWS EBS Driver Operator
|TP
|TP
|TP

|Red Hat Virtualization (oVirt) CSI Driver Operator
|-
|GA
|GA

|CSI inline ephemeral volumes
|TP
|TP
|TP

|Automatic device discovery and provisioning with Local Storage Operator
|-
|TP
|TP

|OpenShift Pipelines
|TP
|TP
|

|Vertical Pod Autoscaler
|TP
|TP
|

|Operator API
|TP
|GA
|GA

|Adding kernel modules to nodes
|TP
|TP
|

|Kubernetes NMState Operator
|
|
|TP

|====

[id="ocp-4-7-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.7 and beyond!
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to 4.7, you can either revoke or continue to allow unauthenticated access. It is recommended to revoke unauthenticated access unless there is a specific need for it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

* {op-system} real time (RT) kernels are currently only supported on compute nodes, not control plane nodes. Compact clusters are not supported with RT kernels in {product-title} 4.7. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1887007[*BZ#1887007*])

[id="ocp-4-7-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} 4.7 are released as asynchronous errata through the Red Hat Network. All {product-title} 4.7 errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified via email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} 4.7. Versioned asynchronous releases, for example with the form {product-title} 4.7.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster.adoc#updating-cluster[updating your cluster] properly.
====

[id="ocp-4-7-0-ga"]
=== RHBA-2021:1234 - {product-title} 4.7 image release and bug fix advisory

Issued: 2021-xx-xx

{product-title} release 4.7 is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2020:1234[RHBA-2020:1234] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2020:5678[RHBA-2020:5678] advisory.

Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:

link:https://access.redhat.com/solutions/<ARTICLE_ID>[{product-title} 4.7.0 container image list]
