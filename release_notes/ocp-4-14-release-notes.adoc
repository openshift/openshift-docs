:_content-type: ASSEMBLY
[id="ocp-4-14-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-14-about-this-release"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-2023:1326[RHSA-2023:1326]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.27.md[Kubernetes 1.27] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. With the {cluster-manager-first} application for {product-title}, you can deploy {product-title} clusters to either on-premises or cloud environments.

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.6, 8.7, and 8.8 as well as on {op-system-first} 4.13.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//TODO: Add this for 4.14
Starting with {product-title} 4.12, an additional six months is added to the Extended Update Support (EUS) phase on even numbered releases from 18 months to two years. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

Starting with {product-title} 4.14, Extended Update Support (EUS) is extended to 64-bit ARM, {ibmpowerProductName}(ppc64le), and {ibmzProductName}(s390x) platforms.  For more information, see the link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

//TODO: Add the line below for EUS releases.
{product-title} 4.14 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

//TODO: The line below should be used when it is next appropriate. Revisit in August 2023 timeframe.
Maintenance support ends for version 4.12 on 25 January 2025 and goes to extended life phase. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

// Added in 4.14. Language came directly from Kirsten Newcomer.
{product-title} is designed for FIPS. When running {op-system-base-full} or {op-system-first} booted in FIPS mode, {product-title} core components use the {op-system-base} cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the `x86_64`, `ppc64le`, and `s390x` architectures.

For more information about the NIST validation program, see link:https://csrc.nist.gov/Projects/cryptographic-module-validation-program/validated-modules[Cryptographic Module Validation Program]. For the latest NIST status for the individual versions of {op-system-base} cryptographic libraries that have been submitted for validation, see link:https://access.redhat.com/articles/2918071#fips-140-2-and-fips-140-3-2[Compliance Activities and Government Standards].

[id="ocp-4-14-add-on-support-status"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-14-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-14-rhcos"]
=== {op-system-first}


[id="ocp-4-14-installation-and-update"]
=== Installation and update

[id="ocp-4-14-user-defined-tags-azure"]
==== User-defined tags for Microsoft Azure is now generally available

User-defined tags for Microsoft Azure was previously introduced as Technology Preview in {product-title} 4.13 and is now generally available in {product-title} 4.14. For more information, see xref:../installing/installing_azure/installing-azure-customizations.adoc#installing-azure-user-defined-tags_installing-azure-customizations[Configuring the user-defined tags for Azure].

[id="ocp-4-14-aws-security-groups"]
==== Applying existing AWS security groups to a cluster

By default, the installation program creates and attaches security groups to control plane and compute machines. The rules associated with the default security groups cannot be modified.

With {product-title} {product-version}, if you deploy a cluster to an existing Amazon Virtual Private Cloud (VPC), you can apply additional existing AWS security groups to control plane and compute machines. These security groups must be associated with the VPC that you are deploying the cluster to. Applying custom security groups can help you meet the security needs of your organization, in such cases where you must control the incoming or outgoing traffic of these machines.

For more information, see xref:../installing/installing_aws/installing-aws-vpc.adoc#installation-aws-vpc-security-groups_installing-aws-vpc[Applying existing AWS security groups to the cluster].

[id="ocp-4-14-admin-ack-updating"]
==== Required administrator acknowledgment when updating from {product-title} 4.13 to 4.14

{product-title} 4.14 uses Kubernetes 1.27, which removed a xref:../release_notes/ocp-4-14-release-notes.adoc#ocp-4-14-removed-kube-1-27-apis[deprecated API].

A cluster administrator must provide a manual acknowledgment before the cluster can be updated from {product-title} 4.13 to 4.14. This is to help prevent issues after updating to {product-title} 4.14, where APIs that have been removed are still in use by workloads, tools, or other components running on or interacting with the cluster. Administrators must evaluate their cluster for any APIs in use that will be removed and migrate the affected components to use the appropriate new API version. After this is done, the administrator can provide the administrator acknowledgment.

All {product-title} 4.13 clusters require this administrator acknowledgment before they can be updated to {product-title} 4.14.

For more information, see xref:../updating/preparing_for_updates/updating-cluster-prepare.adoc#updating-cluster-prepare[Preparing to update to {product-title} 4.14].

[id="ocp-4-14-nutanix-three-node"]
==== Three-node cluster support for Nutanix
Deploying a three-node cluster is supported on Nutanix as of {product-title} {product-version}. This type of {product-title} cluster is a more resource efficient cluster. It consists of only three control plane machines, which also act as compute machines.

For more information, see xref:../installing/installing_nutanix/installing-nutanix-three-node.adoc#installing-nutanix-three-node[Installing a three-node cluster on Nutanix].

[id="ocp-4-14-installation-gcp-confidential-vms"]
==== Installing a cluster on GCP using Confidential VMs is generally available
In {product-title} {product-version}, using Confidential VMs when installing your cluster is generally available. For more information, see xref:../installing/installing_gcp/installing-gcp-customizations.adoc#installation-gcp-enabling-confidential-vms_installing-gcp-customizations[Enabling Confidential VMs].

[id="ocp-4-14-dc-build-apis-disabled"]
==== DeploymentConfig and Build APIs can now be disabled

The `DeploymentConfig` and `Build` APIs are now optional cluster capabilities. You can disable them prior to installation, if necessary.

For more information, see xref:../installing/cluster-capabilities.adoc#cluster-capabilities[Cluster capabilities].

[id="ocp-4-14-rootvolume-types-openstack-available"]
==== Root volume types parameter for {rh-openstack} is now available
You can now specify one or more root volume types in {rh-openstack}, by using the `rootVolume.types` parameter. This parameter is available for both control plane and compute machines.

[id="ocp-4-14-static-ip-addresses-vsphere-nodes"]
==== Static IP addresses for vSphere nodes
You can provision bootstrap, control plane, and compute nodes with static IP addresses in environments where Dynamic Host Configuration Protocol (DHCP) does not exist.

:FeatureName: Static IP addresses for vSphere nodes
include::snippets/technology-preview.adoc[]

After you have deployed your cluster to run nodes with static IP addresses, you can scale a machine to use one of these static IP addresses. Additionally, you can use a machine set to configure a machine to use one of the configured static IP addresses.

For more information, see the "Static IP addresses for vSphere nodes" section in the xref:../installing/installing_vsphere/installing-vsphere-installer-provisioned.adoc[Installing a cluster on vSphere] document.

[id="ocp-4-14-bmo-validations"]
==== Additional validation for the Bare Metal Host CR

The Bare Metal Host Custom Resource (CR) now contains the `ValidatingWebhooks` parameter. With this parameter, the Bare Metal Operator now catches any configuration errors before accepting the CR, and returns a message with the configuration errors to the user.
[id="ocp-4-14-quickly-install-cluster-aws-local-zones"]
==== Install a cluster quickly in AWS Local Zones
For {product-title} {product-version}, you can quickly install a cluster on Amazon Web Services (AWS) to extend compute nodes to Local Zone locations. After you add zone names to the installation configuration file, the installation program fully automates the creation of required resources, network and compute, on each Local Zone.

For more information, see xref:../installing/installing_aws/installing-aws-localzone.adoc#installation-cluster-quickly-extend-workers_installing-aws-localzone[Intall a cluster quickly in AWS Local Zones].


[id="ocp-4-14-vsphere-pre-existing-template"]
==== Quickly install {op-system} on vSphere hosts by using a pre-existing {op-system} image template
{product-title} {product-version} includes a new VMware vSphere configuration parameter for use on installer-provisioned infrastructure: `template`. By using this parameter, you can now specify the absolute path to a pre-existing {op-system-first} image template or virtual machine in the installation configuration file. The installation program can then use the image template or virtual machine to quickly install {op-system} on vSphere hosts.

This installation method is an alternative to uploading an {op-system} image on vSphere hosts.

[IMPORTANT]
====
Before you set a path value for the `template` parameter, ensure that the default {op-system} boot image in the {product-title} release matches the {op-system} image template or virtual machine version; otherwise, cluster installation might fail.
====

[id="ocp-4-14-OCP-on-ARM"]
==== {product-title} on 64-bit ARM

{product-title} {product-version} is now supported on 64-bit ARM architecture-based Google Cloud Platform installer-provisioned and user-provisioned infrastructures. You can also now use the `oc mirror` CLI plug-in disconnected environments on 64-bit ARM clusters. For more information about instance availability and installation documentation, see xref:../installing/installing-preparing.adoc#supported-installation-methods-for-different-platforms[Supported installation methods for different platforms].

[id="ocp-4-14-post-installation"]
=== Post-installation configuration

[id="ocp-4-14-OCP-on-multi-arch-clusters"]
==== {product-title} cluster with multi-architecture compute machines
{product-title} {product-version} clusters with multi-architecture compute machines are now supported on Google Cloud Platform (GCP) as a Day 2 operation. {product-title} clusters with multi-architecture compute machines on bare metal installations are now generally available. For more information on clusters with multi-architecture compute machines and supported platforms, see xref:../post_installation_configuration/configuring-multi-arch-compute-machines/multi-architecture-configuration.adoc#multi-architecture-configuration[About clusters with multi-architecture compute machines].

[id="ocp-4-14-web-console"]
=== Web console

[id="ocp-4-14-administrator-perspective"]
==== Administrator Perspective

With this release, there are several updates to the *Administrator* perspective of the web console. You can now perform the following actions:

* Narrow down the list of resources in a list view or search page with exact search capabilities. This will help when you have similarly named resources and fuzzy search does not narrow down your search.
* Provide direct feedback about features and report a bug by clicking the *Help* button on the toolbar and clicking *Share Feedback* from the drop-down list.
* Display and hide tooltips in the YAML editor. The tooltips will persist so you don't have to change it every time you navigate to the page.

[id="supported-os-types-cluster"]
===== Operating system based filtering in OperatorHub

With this update, Operators in OperatorHub are now filtered based on the nodes operating system because clusters can contain heterogenous nodes.

[id="console-supports-installing-specific-operator-versions"]
===== Support for installing specific Operator versions in the web console

With this update, you can now choose from a list of available versions for an Operator based on the selected channel on the *OperatorHub* page in the console. Additionally, you can view the metadata for that channel and version when available. When selecting an older version, a manual approval update strategy is required, otherwise the Operator will immediately update back to the latest version on the channel.

//link to content in Operator book TBD

[id="console-supports-aws-sts-detection"]
===== OperatorHub support for AWS STS

With this release, OperatorHub detects when an Amazon Web Services (AWS) cluster is using the Security Token Service (STS). When detected, a "Cluster in STS Mode" notification displays with additional instructions before installing an Operator to ensure it runs correctly. The *Operator Installation* page is also modified to add the required *role ARN* field.

For more information, see _Token authentication for Operators on cloud providers_.
//xref:../operators/operator_sdk/osdk-token-auth.adoc#osdk-token-auth[Token authentication for Operators on cloud providers].

[id="ocp-4-14-developer-perspective"]
==== Developer Perspective

[id="ocp-4-14-openshift-cli"]
=== OpenShift CLI (oc)

[id="oc-mirror-multi-arch-oci-local-images"]
==== Supporting multi-arch OCI local images for catalogs with oc-mirror

With {product-title} {product-version}, oc-mirror supports multi-arch OCI local images for catalogs.

OCI layouts consist of an `index.json` file that identifies the images held within them on disk. This `index.json` file can reference any number of single or multi-arch images. However, oc-mirror only references a single image at a time in a given OCI layout. The image stored in the OCI layout can be a single-arch image, that is, an image manifest or a multi-arch image, that is, a manifest list.

The `ImageSetConfiguration` stores the OCI images. After processing the catalog, the catalog content adds new layers representing the content of all images in the layout. The ImageBuilder is modified to handle image updates for both single-arch and multi-arch images.

[id="oc-logging-in-browser"]
==== Logging in to the CLI using a web browser

With {product-title} {product-version}, a new `oc` command-line interface (CLI) flag, `--web` is now available for the `oc login` command.

With this enhancement, you can log in using a web browser, which allows you to avoid inserting your access token into the command line.

For more information, see xref:../cli_reference/openshift_cli/getting-started-cli.adoc#cli-logging-in-web_cli-developer-commands[Logging in to the OpenShift CLI using a web browser].

[id="oc-new-build-enhancement"]
==== Enhancement to oc new-build

A new oc command line interface (CLI) flag, `--import-mode`, has been added to the `oc new-build` command. With this enhancement, users can set the `--import-mode` flag to `Legacy` or `PreserverOriginal`, which provides users the option to trigger builds using a single sub-manifest, or all manifests, respectively.

[id="oc-new-app-enhancement"]
==== Enhancement to oc new-app

A new oc command line interface (CLI) flag, `--import-mode`, has been added to the `oc new-app` command. With this enhancement, users can set the `--import-mode` flag to `Legacy` or `PreserverOriginal`, which provides users the option to create new applications using a single sub-manifest, or all manifests, respectively.

For more information, see xref:../applications/creating_applications/creating-applications-using-cli.adoc#setting-the-import-mode[Setting the import mode].

[id="ocp-4-14-ibm-z"]
=== {ibmzProductName} and {linuxoneProductName}

[id="ocp-4-14-ibm-power"]
=== {ibmpowerProductName}

[id="ocp-4-14-auth"]
=== Authentication and authorization

[id="ocp-4-14-auth-required-scc"]
==== SCC preemption prevention

With this release, you can now require your workloads to use a specific security context constraint (SCC). By setting a specific SCC, you can prevent the SCC that you want from being preempted by another SCC in the cluster.

For more information, see xref:../authentication/managing-security-context-constraints.adoc#security-context-constraints-requiring_configuring-internal-oauth[Configuring a workload to require a specific SCC].

[id="ocp-4-14-auth-psa-privileged-namespaces"]
==== Pod security admission privileged namespaces

With this release, the following system namespaces are always set to the `privileged` pod security admission profile:

* `default`
* `kube-public`
* `kube-system`

For more information, see xref:../authentication/understanding-and-managing-pod-security-admission.adoc#psa-privileged-namespaces_understanding-and-managing-pod-security-admission[Privileged namespaces].

[id="ocp-4-14-auth-psa-disable-sync-modified"]
==== Pod security admission synchronization disabled on modified namespaces

With this release, if a user manually modifies a pod security admission label from the automatically labeled value on a label-synchronized namespace, synchronization is disabled for that label. Users can enable synchronization again, if necessary.

For more information, see xref:../authentication/understanding-and-managing-pod-security-admission.adoc#security-context-constraints-psa-sync-exclusions_understanding-and-managing-pod-security-admission[Pod security admission synchronization namespace exclusions].

[id="ocp-4-14-auth-cco-sts"]
==== OLM-based Operator support for AWS STS

With this release, some Operators managed by Operator Lifecycle Manager (OLM) on Amazon Web Services (AWS) clusters can use the Cloud Credential Operator (CCO) in manual mode with the Security Token Service (STS). These Operators authenticate with limited-privilege, short-term credentials that are managed outside the cluster.

For more information, see _Token authentication for Operators on cloud providers_.
//xref:../operators/operator_sdk/osdk-token-auth.adoc#osdk-token-auth[Token authentication for Operators on cloud providers].

[id="ocp-4-14-networking"]
=== Networking

[id="ocp-4-14-ingress-node-firewall-operator-ga"]
==== Ingress Node Firewall Operator is generally available
Ingress Node Firewall Operator was made a technology preview in {product-title} 4.12. With this release, Ingress Node Firewall Operator is generally available. You can now configure firewall rules at the node level. For more information, see xref:../networking/networking-operators-overview.adoc#networking-operators-overview[Ingress Node Firewall Operator].

[id="ocp-4-14-networking-kernal-network-pinning"]
==== Dynamic use of non-reserved CPUs for OVS

With this release, the Open vSwitch (OVS) networking stack can dynamically use non-reserved CPUs.
This dynamic use of non-reserved CPUs occurs by default in nodes in a machine config pool that has a performance profile applied to it.
The dynamic use of available, non-reserved CPUs maximizes compute resources for OVS and minimizes network latency for workloads during periods of high demand.
OVS remains unable to dynamically use isolated CPUs assigned to containers in `Guaranteed` QoS pods. This separation avoids disruption to critical application workloads.

[NOTE]
====
When the Node Tuning Operator recognizes the performance conditions to activate the use of non-reserved CPUs, there is a several second delay while OVN-Kubernetes configures the CPU affinity alignment of OVS daemons running on the CPUs. During this window, if a `Guaranteed` QoS pod starts, it can experience a latency spike.
====

[id="ocp-4-14-networking-sriov-exclude-topology"]
==== Exclude SR-IOV network topology for NUMA-aware scheduling

With this release, you can exclude advertising the Non-Uniform Memory Access (NUMA) node for the SR-IOV network to the Topology Manager. By not advertising the NUMA node for the SR-IOV network, you can permit more flexible SR-IOV network deployments during NUMA-aware pod scheduling.

For example, in some scenarios, it is a priority to maximize CPU and memory resources for a pod on a single NUMA node. By not providing a hint to the Topology Manager about the NUMA node for the podâ€™s SR-IOV network resource, the Topology Manager can deploy the SR-IOV network resource and the pod CPU and memory resources to different NUMA nodes. In earlier {product-title} releases, the Topology Manager attempted to place all resources on the same NUMA node only.

For more information about this more flexible SR-IOV network deployment during NUMA-aware pod scheduling, see xref:../networking/hardware_networks/configuring-sriov-device.adoc#nw-sriov-exclude-topology-manager_configuring-sriov-device[Exclude the SR-IOV network topology for NUMA-aware scheduling].

[id="ocp-4-14-networking-haproxy-update"]
==== Update to HAProxy 2.6
With this release, {product-title} updated to HAProxy 2.6.

[id="ocp-4-14-nmstate-ui-console-update"]
==== NMstate Operator updated in console

With this release, you can access the NMstate Operator and resources such as the `NodeNetworkState` (NNS), `NodeNetworkConfigurationPolicy` (NNCP), and `NodeNetworkConfigurationEnhancement` (NNCE) from the web console. In the *Administrator* perspective of the console from the *Networking* page you can access NNCP as well as NNCE from the *NodeNetworkConfigurationPolicy* page and NNS on the *NodeNetworkState* page. For more information about NMState resources and how to update them in the console, see xref:../networking/k8s_nmstate/k8s-nmstate-updating-node-network-config.adoc#k8s-nmstate-updating-node-network-config[Updating node network configuration].

[id="ocp-4-14-networking-ovn-kubernetes-ipsec-ibm-cloud"]
==== OVN-Kubernetes network plugin support for IPsec on IBM Cloud

IPsec is now supported on the IBM Cloud platform for clusters that use the OVN-Kubernetes network plugin, which is the default in {product-title} 4.14. For more information, see xref:../networking/ovn_kubernetes_network_provider/configuring-ipsec-ovn.adoc#configuring-ipsec-ovn[Configuring IPsec encryption].

[id="ocp-4-14-single-stack-support-nmstate"]
==== Single-stack IPv6 support for Kubernetes NMstate
With this release, you can use Kubernetes NMState Operator in single-stack IPv6 clusters.

[id="ocp-4-14-networking-egress-service"]
==== Egress service resource to manage egress traffic for pods behind a load balancer (Technology Preview)

With this update, you can use an `EgressService` custom resource (CR) to manage egress traffic for pods behind a load balancer service. This is available as a Technology Preview feature.

You can use the `EgressService` CR to manage egress traffic in the following ways:

* Assign the load balancer service's IP address as the source IP address of egress traffic for pods behind the load balancer service.

* Assign the egress traffic for pods behind a load balancer to a different network than the default node network.

For more information, see xref:../networking/ovn_kubernetes_network_provider/configuring-egress-traffic-for-vrf-loadbalancer-services.adoc#configuring-egress-traffic-loadbalancer-services[Configuring an egress service].

[id="ocp-4-14-networking-metallb-vrf"]
==== Support for VRF specification in MetalLB's BGPPeer resource (Technology Preview)

With this update, you can specify a Virtual Routing and Forwarding (VRF) instance in a `BGPPeer` custom resource. MetalLB can advertise services through the interfaces belonging to the VRF. This is available as a Technology Preview feature.

For more information, see xref:../networking/metallb/metallb-configure-bgp-peers.adoc#nw-metallb-bgp-peer-vrf_configure-metallb-bgp-peers[Exposing a service through a network VRF].

[id="ocp-4-14-networking-nmstate-vrf"]
==== Support for VRF specification in NMState's NodeNetworkConfigurationPolicy resource (Technology Preview)

With this update, you can associate a Virtual Routing and Forwarding (VRF) instance with a network interface by using a `NodeNetworkConfigurationPolicy` custom resource. By associating a VRF instance with a network interface, you can support traffic isolation, independent routing decisions, and the logical separation of network resources. This feature is available as a Technology Preview feature.

For more information, see xref:../networking/k8s_nmstate/k8s-nmstate-updating-node-network-config.adoc#virt-example-host-vrf_k8s_nmstate-updating-node-network-config[Example: Network interface with a VRF instance node network configuration policy].

[id="ocp-414-broadcom-bcm57504-support"]
==== Support for Broadcom BCM57504 is now GA

Support for the Broadcom BCM57504 network interface controller is now available for the SR-IOV Network Operator.

For more information, see xref:../networking/hardware_networks/about-sriov.adoc#supported-devices_about-sriov[Supported devices].

[id="ocp-4-14-admin-network-policy"]
==== Admin Network Policy (Technology Preview)

Admin Network Policy is available as a Technology Preview feature.

You can enable `AdminNetworkPolicy` and `BaselineAdminNetworkPolicy` resources, which are part of the Network Policy V2 API, in clusters running the OVN-Kubernetes CNI plugin. Cluster administrators can apply cluster-scoped policies and safeguards for an entire cluster before namespaces are created. Network administrators can secure clusters by enforcing network traffic controls that cannot be overridden by users. Network administrators can enforce optional baseline network traffic controls that can be overridden by users in the cluster, if necessary. Currently, these APIs support only expressing policies for intra-cluster traffic.

[id="ocp-4-14-creating-subinterface"]
==== MAC-VLAN, IP-VLAN, and VLAN subinterface creation for pods

With this release, the ability to create a MAC-VLAN, IP-VLAN, and VLAN subinterface based on a master interface in a container namespace is generally available. You can use this feature to create the master interfaces as part of the pod network configuration in a separate network attachment definition. You can then base the VLAN, MACVLAN or IPVLAN on this interface without knowing the network configuration of the node. For more information, see xref:../networking/multiple_networks/configuring-additional-network.html#nw-about-configuring-master-interface-container_configuring-additional-network[About configuring the master interface in the container network namespace].

[id="ocp-4-14-tap-device-plugin"]
==== Enhance network flexibility by using the TAP device plugin

This release introduces a new Container Network Interface (CNI) network plugin type: the TAP device plugin. You can use this plugin to create TAP devices within containers, which enables user-space programs to handle network frames and act as an interface that receives frames from and that sends frames to user-space applications instead of through traditional network interfaces. For more information, see xref:../networking/multiple_networks/configuring-additional-network.html#nw-multus-tap-object_configuring-additional-network[Configuration for a TAP additional network].

[id="ocp-4-14-non-root-dpdk"]
==== Support for running rootless DPDK workloads with kernel access by using the TAP CNI plugin

In {product-title} version 4.14 and later, DPDK applications that need to inject traffic to the kernel can run in non-privileged pods with the help of the TAP CNI plugin. For more information, see xref:../networking/hardware_networks/using-dpdk-and-rdma.html#nw-running-dpdk-rootless-tap_using-dpdk-and-rdma[Using the TAP CNI to run a rootless DPDK workload with kernel access].

[id="ocp-4-14-storage"]
=== Storage

[id="ocp-4-14-storage-device-selector"]
==== Support for OR logic in LVMS

With this release, the logical volume manager (LVM) cluster custom resource (CR) provides `OR` logic in the `deviceSelector` setting. In previous releases, specifying the `paths` setting for device paths used `AND` logic only. With this release, you can also specify the `optionalPaths` setting, which supports `OR` logic. For more information, see the CR examples in xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#persistent-storage-using-lvms[Persistent storage using logical volume manager storage].

[id="ocp-4-14-storage-lvms-ext4-support"]
==== Support for ext4 in LVMS

With this release, the logical volume manager (LVM) cluster custom resource (CR) provides support for the `ext4` filesystem with the `fstype` setting under `deviceClasses`. The default filesystem is `xfs`. For more information, see the CR examples in xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#persistent-storage-using-lvms[Persistent storage using logical volume manager storage].

[id="ocp-4-14-registry"]
=== Registry

[id="ocp-4-14-optional-image-registry-operator"]
==== Optional Image Registry Operator

With this release, the Image Registry Operator is now an optional component. This feature helps reduce the overall resources footprint of {product-title} in Telco environments when the Image Registry Operator is not needed.

For more information about disabling the Image Registry Operator, see xref:../installing/cluster-capabilities.adoc#selecting-cluster-capabilities_cluster-capabilities[Selecting cluster capabilities].

[id="ocp-4-14-olm"]
=== Operator lifecycle

[id="ocp-4-14-osdk"]
=== Operator development

[id="ocp-4-14-osdk-cco-sts"]
==== Token authentication for Operators on cloud providers: AWS STS

With this release, Operators managed by Operator Lifecycle Manager (OLM) can support token authentication when running on Amazon Web Services (AWS) clusters that use the Security Token Service (STS). The Cloud Credential Operator (CCO) is updated to semi-automate provisioning certain limited-privilege, short-term credentials, provided that the Operator author has enabled their Operator to support AWS STS.

For more information about enabling OLM-based Operators to support CCO-based workflows with AWS STS, see _Token authentication for Operators on cloud providers_.
//xref:../operators/operator_sdk/osdk-token-auth.adoc#osdk-token-auth[Token authentication for Operators on cloud providers].

[id="ocp-4-14-machine-config-operator"]
=== Machine Config Operator

[id="ocp-4-14-additional-prometheus-metrics"]
==== Additional metrics available in Prometheus

With this release, you can query additional metrics to more closely monitor the state of your machines and machine config pools.

For more information on how to use Prometheus, see xref:../monitoring/managing-metrics.adoc#viewing-a-list-of-available-metrics[Viewing a list of available metrics].

[id="ocp-4-14-offline-tang"]
==== Support for offline Tang provisioning

With this release, you can now provision an {product-title} cluster with Tang-enforced, network-bound disk encryption (NBDE) using Tang servers that are unreachable during first boot.

For more information, see xref:../installing/install_config/installing-customizing.adoc#installation-special-config-encryption-threshold_installing-customizing[Configuring an encryption threshold] and xref:../installing/install_config/installing-customizing.adoc#installation-special-config-storage-procedure_installing-customizing[Configuring disk encryption and mirroring].

[id="ocp-4-14-machine-api"]
=== Machine API

[id="ocp-4-14-mapi-cpms-platform-support"]
==== Support for control plane machine sets on Nutanix clusters

With this release, control plane machine sets are supported for Nutanix clusters.

For more information, see xref:../machine_management/control_plane_machine_management/cpmso-getting-started.adoc#cpmso-getting-started[Getting started with the Control Plane Machine Set Operator].

[id="ocp-4-14-mco-ca-distribution"]
==== Handling of registry certificate authorities

The Machine Config Operator now handles distributing certificate authorities for image registries. This change does not affect end users.

[id="ocp-4-14-mapi-aws-placement-groups"]
==== Support for assigning AWS machines to placement groups

With this release, you can configure a machine set to deploy machines within an existing AWS placement group. You can use this feature with Elastic Fabric Adapter (EFA) instances to improve network performance for machines within the specified placement group.

You can use this feature with xref:../machine_management/creating_machinesets/creating-machineset-aws.adoc#machineset-aws-existing-placement-group_creating-machineset-aws[compute] and xref:../machine_management/control_plane_machine_management/cpmso-using.adoc#machineset-aws-existing-placement-group_cpmso-using[control plane] machine sets.

[id="ocp-4-14-nodes"]
=== Nodes

[id="ocp-4-14-descheduler-resource-limits"]
==== Descheduler resource limits for large clusters

With this release, the resource limits for the descheduler operand are removed. This enables the descheduler to be used for large clusters with many nodes and pods without failing due to out-of-memory errors.

[id="ocp-4-14-nodes-pod-topology-constraints-matchlabelkeys"]
==== Pod topology spread constraints matchLabelKeys parameter is now generally available

The `matchLabelKeys` parameter for configuring pod topology spread constraints is now generally available in {product-title} 4.14. Previously, the parameter was available as a Technology Preview feature by enabling the `TechPreviewNoUpgrade` feature set. The `matchLabelKeys` parameter takes a list of pod label keys to select the pods to calculate spreading over.

For more information, see xref:../nodes/scheduling/nodes-scheduler-pod-topology-spread-constraints.adoc#nodes-scheduler-pod-topology-spread-constraints[Controlling pod placement by using pod topology spread constraints].

[id="ocp-4-14-MaxUnavailableStatefulSet"]
==== MaxUnavailableStatefulSet enabled

With this release, the `MaxUnavailableStatefulSet` featureset configuration parameter is enabled by default. This allows users to define the maximum number of `StatefulSet` pods that can be unavailable during updates, and reduces application downtime when upgrading.

[id="ocp-4-14-monitoring"]
=== Monitoring

The monitoring stack for this release includes the following new and modified features.

[id="ocp-4-14-monitoring-updates-to-monitoring-stack-components-and-dependencies"]
==== Updates to monitoring stack components and dependencies
This release includes the following version updates for monitoring stack components and dependencies:

* kube-state-metrics to 2.9.2
* node-exporter to 1.6.1
* prom-label-proxy to 0.7.0
* Prometheus to 2.46.0
* prometheus-operator to 0.67.1

[id="ocp-4-14-monitoring-changes-to-alerting-rules"]
==== Changes to alerting rules

[NOTE]
====
Red{nbsp}Hat does not guarantee backward compatibility for recording rules or alerting rules.
====

* *New*
** Added the `KubeDeploymentRolloutStuck` alert to monitor if the rollout of a deployment has not progressed for 15 minutes.
** Added the `NodeSystemSaturation` alert to monitor resource saturation on a node.
** Added the `NodeSystemdServiceFailed` alert to monitor the systemd service on a node.
** Added the `NodeMemoryMajorPagesFaults` alert to monitor major page faults on a node.
** Added the `PrometheusSDRefreshFailure` alert to monitor failed Prometheus service discoveries.

* *Changed*
** Modified the `KubeAggregatedAPIDown` alert and the `KubeAggregatedAPIErrors` alert to evaluate only metrics from the `apiserver` job.
** Modified the `KubeCPUOvercommit` alert to evaluate only metrics from the `kube-state-metrics` job.
** Modified the `NodeHighNumberConntrackEntriesUsed`, `NodeNetworkReceiveErrs` and `NodeNetworkTransmitErrs` alerts to evaluate only metrics from the `node-exporter` job.

* *Removed*
** Removed the `MultipleContainersOOMKilled` alert for not being actionable. Nodes under memory pressure are covered by other alerts.

[id="ocp-4-14-monitoring-new-option-to-create-alerts-based-on-core-platform-metrics"]
==== New option to create alerts based on core platform metrics

With this release, administrators can create new alerting rules based on core platform metrics.
You can now modify settings for existing platform alerting rules by adjusting thresholds and by changing labels.
You can also define and add new custom alerting rules by constructing a query expression based on core platform metrics in the `openshift-monitoring` namespace.
This feature was included as a technology preview in the {product-title} 4.12 release and is now generally available.
For more information, see xref:../monitoring/managing-alerts.adoc#managing-core-platform-alerting-rules_managing-alerts[Managing alerting rules for core platform monitoring].

[id="ocp-4-14-monitoring-new-option-to-specify-resource-limits-for-all-monitoring-components"]
==== New option to specify resource limits for all monitoring components

With this release, you can now specify resource requests and limits for all monitoring components, including the following:

* Alertmanager
* kube-state-metrics
* monitoring-plugin
* node-exporter
* openshift-state-metrics
* Prometheus
* Prometheus Adapter
* Prometheus Operator and its admission webhook service
* Telemeter Client
* Thanos Querier
* Thanos Ruler

In previous versions of {product-title}, you could only set options for Prometheus, Alertmanager, Thanos Querier, and Thanos Ruler.

[id="ocp-4-14-monitoring-new-options-to-configure-node-exporter-collectors"]
==== New options to configure node-exporter collectors

With this release, you can customize Cluster Monitoring Operator (CMO) config map settings for additional node-exporter collectors.
The following node-exporter collectors are now optional, and you can enable or disable each one individually in the config map settings:

* `ksmd` collector
* `mountstats` collector
* `processes` collector
* `systemd` collector

In addition, you can now exclude network devices from the relevant collector configuration for the `netdev` and `netclass` collectors.
You can also now use the `maxProcs` option to set the maximum number of processes that can run node-exporter.

[id="ocp-4-14-monitoring-new-options-to-deploy-monitoring-web-console-plugin-resources"]
==== New option to deploy monitoring web console plugin resources

With this release, the monitoring pages in the *Observe* section of the {product-title} web console are deployed as a xref:../web_console/dynamic-plugin/overview-dynamic-plugin.adoc[dynamic plugin].
With this change, the Cluster Monitoring Operator (CMO) is now the component that deploys the {product-title} web console monitoring plugin resources.
You can now use CMO settings to configure the following features of the console monitoring plugin resource:

* Node selectors
* Tolerations
* Topology spread constraints
* Resource requests
* Resource limits

[id="ocp-4-14-scalability-and-performance"]
=== Scalability and performance

[id="ocp-4-14-PAO-image-must-gather"]
==== PAO must-gather image added to default must-gather image

With this release, the Performance Addon Operator (PAO) must-gather image is no longer required as an argument for the `must-gather` command to capture debugging data related to low-latency tuning. The functions of the PAO must-gather image are now under the default plugin image used by the `must-gather` command without any image arguments.

For further information about gathering debugging information relating to low-latency tuning, see xref:../scalability_and_performance/cnf-low-latency-tuning.adoc#cnf-collecting-low-latency-tuning-debugging-data-for-red-hat-support_cnf-master[Collecting low latency tuning debugging data for Red Hat Support].

[id="ocp-4-14-NRO-image-must-gather"]
==== Collecting data for the NUMA Resources Operator with the must-gather image of the operator

In this release, the `must-gather` tool is updated to collect the data of the NUMA Resources Operator with the `must-gather` image of the operator.

For further information about gathering debugging information for the NUMA Resources Operator, see xref:../scalability_and_performance/cnf-numa-aware-scheduling.html#cnf-about-collecting-nro-data_numa-aware[Collecting NUMA Resources Operator data].

[id="ocp-4-14-additional-power-savings-control"]
==== Enabling more control over the C-states for each pod

With this release, you have more control over the C-states for your pods. Now, instead of disabling C-states completely, you can specify a maximum latency in microseconds for C-states. You can configure this option in the `cpu-c-states.crio.io` annotation, which helps to optimize power savings in high-priority applications by enabling some of the shallower C-states instead of disabling them completely.

For further information about enabling more control over the C-states, see xref:../scalability_and_performance/cnf-low-latency-tuning.html#node-tuning-operator-pod-power-saving-config_cnf-master[Optional: Power saving configurations].

[id="ocp-4-14-nw-ipv6-spoke-cluster-support"]
==== Support for provisioning IPv6 spoke clusters from dual-stack hub clusters
With this update, you can provision IPv6 address spoke clusters from dual-stack hub clusters. In a zero touch provisioning (ZTP) environment, the HTTP server on the hub cluster that hosts the boot ISO now listens on both IPv4 and IPv6 networks. The provisioning service also checks the baseboard management controller (BMC) address scheme on the target spoke cluster and provides a matching URL for the installation media. These updates offer the ability to provision single-stack, IPv6 spoke clusters from a dual-stack hub cluster.

[id="ocp-4-14-precaching-user-spec-images"]
==== Pre-caching user-specified images with {cgu-operator-full}

With this release, you can pre-cache your application workload images before upgrading your applications on {sno} clusters with {cgu-operator-full}. For more information, see xref:../scalability_and_performance/ztp_far_edge/ztp-talm-updating-managed-policies.adoc#talm-prechache-user-specified-images-concept_ztp-talm[Pre-caching user-specified images with TALM on single-node OpenShift clusters].

[id="ocp-4-14-ztp-siteconfig-disk-cleaning"]
==== Disk cleaning option through SiteConfig and GitOps ZTP

With this release, you can remove the partitioning table before installation by using the `automatedCleaningMode` field in the `SiteConfig` CR. For more information, see xref:../scalability_and_performance/ztp_far_edge/ztp-deploying-far-edge-sites.adoc#ztp-deploying-a-site_ztp-deploying-far-edge-sites[Deploying a managed cluster with SiteConfig and GitOps ZTP].

[id="ocp-4-14-ztp-support-custom-node-labels"]
==== Support for adding custom node labels in the SiteConfig CR through GitOps ZTP

With this update, you can add the `nodeLabels` field in the `SiteConfig` CR to create custom roles for nodes in managed clusters. For more information about how to add custom labels, see xref:../scalability_and_performance/ztp_far_edge/ztp-deploying-far-edge-sites.adoc#ztp-deploying-a-site_ztp-deploying-far-edge-sites[Deploying a managed cluster with SiteConfig and GitOps ZTP] or xref:../scalability_and_performance/ztp_far_edge/ztp-manual-install.adoc#ztp-generating-install-and-config-crs-manually_ztp-manual-install[Generating GitOps ZTP installation and configuration CRs manually].

[id="ocp-4-14-hcp"]
=== Hosted control planes (Technology Preview)

[id="ocp-4-14-insights-operator"]
=== Insights Operator

[id="ocp-4-14-insights-operator-on-demand-data-gathering"]
==== On demand data gathering (Technology Preview)
In {product-title} 4.14, Insights Operator can now run gather operations on demand. For more information about running gather operations on demand, see xref:../support/remote_health_monitoring/using-insights-operator.adoc#running-insights-operator-gather_using-insights-operator[Running an Insights Operator gather operation].

[id="ocp-4-14-insights-operator-individual-pods"]
==== Running gather operations as individual pods (Technology Preview)
In {product-title} 4.14 Technology Preview clusters, Insights Operator runs gather operations in individual pods. This supports the new on demand data gathering feature.

[id="install-sno-requirements-for-installing-on-a-single-node"]
=== Requirements for installing {product-title} on a single node

{product-version} now supports `x86_64` and `arm64` CPU architectures.

[id="ocp-4-14-notable-technical-changes"]
== Notable technical changes

{product-title} {product-version} introduces the following notable technical changes.

// Note: use [discrete] for these sub-headings.

[discrete]
[id="ocp-4-14-cluster-cloud-controller-manager-operator"]
=== Cloud controller managers for additional cloud providers

The Kubernetes community plans to deprecate the use of the Kubernetes controller manager to interact with underlying cloud platforms in favor of using cloud controller managers. As a result, there is no plan to add Kubernetes controller manager support for any new cloud platforms.

This release introduces the General Availability of using cloud controller managers for Amazon Web Services and Microsoft Azure.

To learn more about the cloud controller manager, see the link:https://kubernetes.io/docs/concepts/architecture/cloud-controller/[Kubernetes Cloud Controller Manager documentation].

To manage the cloud controller manager and cloud node manager deployments and lifecycles, use the Cluster Cloud Controller Manager Operator.

For more information, see the xref:../operators/operator-reference.adoc#cluster-cloud-controller-manager-operator_cluster-operators-ref[Cluster Cloud Controller Manager Operator] entry in the _Platform Operators reference_.

[discrete]
[id="ocp-4-14-planned-psa-restricted-enforcement"]
=== Future restricted enforcement for pod security admission

Currently, pod security violations are shown as warnings and logged in the audit logs, but do not cause the pod to be rejected.

Global restricted enforcement for pod security admission is currently planned for the next minor release of {product-title}. When this restricted enforcement is enabled, pods with pod security violations will be rejected.

To prepare for this upcoming change, ensure that your workloads match the pod security admission profile that applies to them. Workloads that are not configured according to the enforced security standards defined globally or at the namespace level will be rejected. The `restricted-v2` SCC admits workloads according to the link:https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted[Restricted] Kubernetes definition.

If you are receiving pod security violations, see the following resources:

* See xref:../authentication/understanding-and-managing-pod-security-admission.adoc#security-context-constraints-psa-alert-eval_understanding-and-managing-pod-security-admission[Identifying pod security violations] for information about how to find which workloads are causing pod security violations.

* See xref:../authentication/understanding-and-managing-pod-security-admission.adoc#security-context-constraints-psa-synchronization_understanding-and-managing-pod-security-admission[Security context constraint synchronization with pod security standards] to understand when pod security admission label synchronization is performed. Pod security admission labels are not synchronized in certain situations, such as the following situations:
** The workload is running in a system-created namespace that is prefixed with `openshift-`.
** The workload is running on a pod that was created directly without a pod controller.

* If necessary, you can set a custom admission profile on the namespace or pod by setting the `pod-security.kubernetes.io/enforce` label.

[discrete]
[id="ocp-4-14-cert-manager-operator-1-11"]
=== cert-manager Operator general availability

The {cert-manager-operator} 1.11 is now generally available in {product-title} 4.14 as well as {product-title} 4.13 and {product-title} 4.12.

[id="ocp-4-14-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* _General Availability_
* _Deprecated_
* _Removed_

[discrete]
=== Operator deprecated and removed features

.Operator deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|SQLite database format for Operator catalogs
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
=== Images deprecated and removed features

.Images deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|`ImageChangesInProgress` condition for Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated

|`MigrationInProgress` condition for Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated

|====

//[discrete]
//=== Monitoring deprecated and removed features

//.Monitoring deprecated and removed tracker
//[cols="4,1,1,1",options="header"]
//|====
//|Feature |4.12 |4.13 |4.14

//|====

[discrete]
=== Installation deprecated and removed features

.Installation deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|vSphere 7.0 Update 1 or earlier
|General Availability
|Removed ^[1]^
|Removed

|VMware ESXi 7.0 Update 1 or earlier
|General Availability
|Removed ^[1]^
|Removed

|CoreDNS wildcard queries for the `cluster.local` domain
|Deprecated
|Deprecated
|Deprecated

|`ingressVIP` and `apiVIP` settings in the `install-config.yaml` file for installer-provisioned infrastructure clusters
|Deprecated
|Deprecated
|Deprecated

|`compute.platform.openstack.rootVolume.type` for {rh-openstack}
|General Availability
|General Availability
|Deprecated

|`controlPlane.platform.openstack.rootVolume.type` for {rh-openstack}
|General Availability
|General Availability
|Deprecated
|====
[.small]
--
1. For {product-title} {product-version}, you must install the {product-title} cluster on a VMware vSphere version 7.0 Update 2 or later instance, including VMware vSphere version 8.0, that meets the requirements for the components that you use.
--
//[discrete]
//=== Updating clusters deprecated and removed features

//.Updating clusters deprecated and removed tracker
//[cols="4,1,1,1",options="header"]
//|====
//|Feature |4.12 |4.13 |4.14

//|====

[discrete]
=== Storage deprecated and removed features

.Storage deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|Persistent storage using FlexVolume
|Deprecated
|Deprecated
|Deprecated

|====

//[discrete]
//=== Authentication and authorization deprecated and removed features

//.Authentication and authorization deprecated and removed tracker
//[cols="4,1,1,1",options="header"]
//|====
//|Feature |4.12 |4.13 |4.14

//|====
//[discrete]
//=== Specialized hardware and driver enablement deprecated and removed features

//.Specialized hardware and driver enablement deprecated and removed tracker
//[cols="4,1,1,1",options="header"]
//|====
//|Feature |4.12 |4.13 |4.14

//|====

[discrete]
=== Multi-architecture deprecated and removed features

.Multi-architecture deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|IBM Power8 all models (`ppc64le`)
|Deprecated
|Removed
|Removed

|{ibmpowerProductName} AC922 (`ppc64le`)
|Deprecated
|Removed
|Removed

|{ibmpowerProductName} IC922 (`ppc64le`)
|Deprecated
|Removed
|Removed

|{ibmpowerProductName} LC922 (`ppc64le`)
|Deprecated
|Removed
|Removed

|IBM z13 all models (`s390x`)
|Deprecated
|Removed
|Removed

|{linuxoneProductName} Emperor (`s390x`)
|Deprecated
|Removed
|Removed

|{linuxoneProductName} Rockhopper (`s390x`)
|Deprecated
|Removed
|Removed

|AMD64 (x86_64) v1 CPU
|Deprecated
|Removed
|Removed

|====

[discrete]
=== Networking deprecated and removed features

.Networking deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|Kuryr on {rh-openstack}
|Deprecated
|Deprecated
|Deprecated
|====

//[discrete]
//=== Web console deprecated and removed features

//.Web console deprecated and removed tracker
//[cols="4,1,1,1",options="header"]
//|====
//|Feature |4.12 |4.13 |4.14

//|====

[discrete]
=== Node deprecated and removed features

.Node deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|`ImageContentSourcePolicy` (ICSP) objects
|General Availability
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/zone`
|General Availability
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/region`
|General Availability
|Deprecated
|Deprecated

|====

[discrete]
=== Workloads deprecated and removed features

.Workloads deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|`DeploymentConfig` objects
|General Availability
|General Availability
|Deprecated

|====

[id="ocp-4-14-deprecated-features"]
=== Deprecated features

[id="ocp-4-14-deployment-config-deprecated"]
=== DeploymentConfig resources are now deprecated

As of {product-title} 4.14, `DeploymentConfig` objects are deprecated. `DeploymentConfig` objects are still supported, but are not recommended for new installations. Only security-related and critical issues will be fixed.

Instead, use `Deployment` objects or another alternative to provide declarative updates for pods.

[id="ocp-4-14-ztp-talm-defaultcatsrc-update"]
=== Operator-specific CatalogSource CRs used in {gitops-shortname} ZTP are deprecated

From {product-title} {product-version}, you must only use the `DefaultCatSrc.yaml` `CatalogSource` CR when updating Operators with {cgu-operator-first}. All other `CatalogSource` CRs are deprecated and are planned to be removed in a future release. Red{nbsp}Hat will provide bug fixes and support for this feature during the current release lifecycle, but this feature will no longer receive enhancements and will be removed. For more information about `DefaultCatSrc` CR, see xref:../scalability_and_performance/ztp_far_edge/ztp-talm-updating-managed-policies.adoc#talo-operator-update_ztp-talm[Performing an Operator update].

[id="ocp-4-14-removed-features"]
=== Removed features

[id="ocp-4-14-removed-kube-1-27-apis"]
==== Beta APIs removed from Kubernetes 1.27

Kubernetes 1.27 removed the following deprecated API, so you must migrate manifests and API clients to use the appropriate API version. For more information about migrating removed APIs, see the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-27[Kubernetes documentation].

.APIs removed from Kubernetes 1.27
[cols="2,2,2",options="header",]
|===
|Resource |Removed API |Migrate to

|`CSIStorageCapacity`
|`storage.k8s.io/v1beta1`
|`storage.k8s.io/v1`

|===

[id="ocp-4-14-openshift-default-registry"]
==== Removal of the OPENSHIFT_DEFAULT_REGISTRY

{product-title} {product-version} has removed support for the `OPENSHIFT_DEFAULT_REGISTRY` variable. This variable was primarily used to enable backwards compatibility of the internal image registry for earlier setups.

[id="ocp-4-14-future-deprecation"]
=== Notice of future deprecation

[id="ocp-4-14-future-deprecation-sdn"]
==== Future deprecation of the OpenShift SDN network plugin

Targeting {product-title} 4.15, the OpenShift SDN CNI network plugin will not be an option for new installations, but will continue to be supported in clusters upgrading to 4.15 and 4.16 from clusters previously installed with the OpenShift SDN network plugin. In a future release, targeting no earlier than 4.17, the OpenShift SDN network plugin will be removed and and will no longer be supported.

[id="ocp-4-14-bug-fixes"]
== Bug fixes

[discrete]
[id="ocp-4-14-api-auth-bug-fixes"]
==== API Server and Authentication

//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / TALO
//Telco Edge / ZTP

[discrete]
[id="ocp-4-14-bare-metal-hardware-bug-fixes"]
==== Bare Metal Hardware Provisioning

[discrete]
[id="ocp-4-14-builds-bug-fixes"]
==== Builds

[discrete]
[id="ocp-4-14-cloud-compute-bug-fixes"]
==== Cloud Compute

[discrete]
[id="ocp-4-14-cloud-cred-operator-bug-fixes"]
==== Cloud Credential Operator

[discrete]
[id="ocp-4-14-dev-console-bug-fixes"]
==== Developer Console

[discrete]
[id="ocp-4-14-cloud-etcd-operator-bug-fixes"]
==== etcd Cluster Operator

[discrete]
[id="ocp-4-14-hosted-control-plane-bug-fixes"]
==== Hosted Control Plane

[discrete]
[id="ocp-4-14-image-registry-bug-fixes"]
==== Image Registry

[discrete]
[id="ocp-4-14-installer-bug-fixes"]
==== Installer

[discrete]
[id="ocp-4-14-kube-controller-bug-fixes"]
==== Kubernetes Controller Manager

[discrete]
[id="ocp-4-14-kube-scheduler-bug-fixes"]
==== Kubernetes Scheduler

[discrete]
[id="ocp-4-14-machine-config-operator-bug-fixes"]
==== Machine Config Operator

[discrete]
[id="ocp-4-14-management-console-bug-fixes"]
==== Management Console

[discrete]
[id="ocp-4-14-monitoring-bug-fixes"]
==== Monitoring

[discrete]
[id="ocp-4-14-networking-bug-fixes"]
==== Networking

[discrete]
[id="ocp-4-14-node-bug-fixes"]
==== Node

[discrete]
[id="ocp-4-14-node-tuning-operator-bug-fixes"]
==== Node Tuning Operator (NTO)

[discrete]
[id="ocp-4-14-openshift-cli-bug-fixes"]
==== OpenShift CLI (oc)

Previously, container image references that have both tag and digest were not correctly interpreted by the oc-mirror plug-in and resulted in the following error:

[source,yaml]
----
"localhost:6000/cp/cpd/postgresql:13.7@sha256" is not a valid image reference: invalid reference format
----

This behavior has been fixed, and the references are now accepted and correctly mirrored.

[discrete]
[id="ocp-4-14-olm-bug-fixes"]
==== Operator Lifecycle Manager (OLM)

[discrete]
[id="ocp-4-14-openshift-operator-sdk-bug-fixes"]
==== Operator SDK

[discrete]
[id="ocp-4-14-file-integrity-operator-bug-fixes"]
==== File Integrity Operator

[discrete]
[id="ocp-4-14-compliance-operator-bug-fixes"]
==== Compliance Operator

[discrete]
[id="ocp-4-14-openshift-api-server-bug-fixes"]
==== OpenShift API server

[discrete]
[id="ocp-4-14-rhcos-bug-fixes"]
==== {op-system-first}

[discrete]
[id="ocp-4-14-security-profiles-operator-bug-fixes"]
==== Security Profiles Operator

[discrete]
[id="ocp-4-14-scalability-and-performance-bug-fixes"]
==== Scalability and performance

[discrete]
[id="ocp-4-14-storage-bug-fixes"]
==== Storage

[discrete]
[id="ocp-4-14-windows-containers-bug-fixes"]
==== Windows containers

[id="ocp-4-14-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the following tables, features are marked with the following statuses:

* _Technology Preview_
* _General Availability_
* _Not Available_
* _Deprecated_

[discrete]
=== Networking Technology Preview features

.Networking Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|PTP dual NIC hardware configured as boundary clock
|Technology Preview
|General Availability
|General Availability

|Ingress Node Firewall Operator
|Technology Preview
|Technology Preview
|General Availability

|Advertise using L2 mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Technology Preview
|Technology Preview
|Technology Preview

|Multi-network policies for SR-IOV networks
|Technology Preview
|Technology Preview
|Technology Preview

|OVN-Kubernetes network plugin as secondary network
|Not Available
|Technology Preview
|Technology Preview

|Updating the interface-specific safe sysctls list
|Technology Preview
|Technology Preview
|Technology Preview

|MT2892 Family [ConnectX-6 Dx] SR-IOV support
|Technology Preview
|General Availability
|General Availability

|MT2894 Family [ConnectX-6 Lx] SR-IOV support
|Technology Preview
|General Availability
|General Availability

|MT42822 BlueField-2 in ConnectX-6 NIC mode SR-IOV support
|Technology Preview
|General Availability
|General Availability

|Silicom STS Family SR-IOV support
|Technology Preview
|General Availability
|General Availability

|MT2892 Family [ConnectX-6 Dx] OvS Hardware Offload support
|Technology Preview
|General Availability
|General Availability

|MT2894 Family [ConnectX-6 Lx] OvS Hardware Offload support
|Technology Preview
|General Availability
|General Availability

|MT42822 BlueField-2 in ConnectX-6 NIC mode OvS Hardware Offload support
|Technology Preview
|General Availability
|General Availability

|Switching Bluefield-2 from DPU to NIC
|Technology Preview
|General Availability
|General Availability

|Intel E810-XXVDA4T
|Not Available
|General Availability
|General Availability

|Egress service custom resource
|Not Available
|Not Available
|Technology Preview

|VRF specification in `BGPPeer` custom resource
|Not Available
|Not Available
|Technology Preview

|VRF specification in `NodeNetworkConfigurationPolicy` custom resource
|Not Available
|Not Available
|Technology Preview

|Admin Network Policy (`AdminNetworkPolicy`)
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Storage Technology Preview features

.Storage Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|Shared Resources CSI Driver and Build CSI Volumes in OpenShift Builds
|Technology Preview
|Technology Preview
|Technology Preview

|CSI Google Filestore Driver Operator
|Technology Preview
|Technology Preview
|Technology Preview

|CSI automatic migration
(Azure file, VMware vSphere)
|Technology Preview
|General Availability
|General Availability

|CSI inline ephemeral volumes
|Technology Preview
|General Availability
|General Availability

|{ibmpowerProductName} Virtual Server Block CSI Driver Operator
|Not Available
|Technology Preview
|Technology Preview

|Automatic device discovery and provisioning with Local Storage Operator
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Installation Technology Preview features

.Installation Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|Adding kernel modules to nodes with kvc
|Technology Preview
|Technology Preview
|Technology Preview

|Selectable Cluster Inventory
|Technology Preview
|Technology Preview
|Technology Preview

|Multi-architecture compute machines
|Technology Preview
|General Availability
|General Availability

|Mount shared entitlements in BuildConfigs in RHEL
|Technology Preview
|Technology Preview
|Technology Preview

|Enabling NIC partitioning for SR-IOV devices
|Not Available
|Technology Preview
|Technology Preview

|Azure Tagging
|Not Available
|Technology Preview
|Technology Preview

|GCP Confidential VMs
|Not Available
|Technology Preview
|General Availability

|====

[discrete]
=== Node Technology Preview features

.Nodes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|Linux Control Group version 2 (cgroup v2)
|Technology Preview
|General Availability
|General Availability

|crun container runtime
|Technology Preview
|General Availability
|General Availability

|Cron job time zones
|Technology Preview
|Technology Preview
|Technology Preview

|`MaxUnavailableStatefulSet` featureset
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Multi-Architecture Technology Preview features

.Multi-Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|`kdump` on `arm64` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `s390x` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `ppc64le` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|IBM Secure Execution on {ibmzProductName} and {linuxoneProductName}
|Technology Preview
|General Availability
|General Availability

|{ibmpowerProductName} Virtual Server using installer-provisioned infrastructure
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Specialized hardware and driver enablement Technology Preview features

.Specialized hardware and driver enablement Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|Driver Toolkit
|Technology Preview
|General Availability
|General Availability

|Hub and spoke cluster support
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Web console Technology Preview features

.Web console Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

//|Multicluster console
//|Technology Preview
//|Technology Preview
//|Technology Preview

|====

[discrete]
[id="ocp-413-scalability-tech-preview"]
=== Scalability and performance Technology Preview features

.Scalability and performance Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|Hyperthreading-aware CPU manager policy
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Technology Preview
|Technology Preview
|Technology Preview

|{factory-prestaging-tool}
|Not Available
|Technology Preview
|Technology Preview

|{sno-caps} cluster expansion with worker nodes
|Technology Preview
|General Availability
|General Availability

|{cgu-operator-first}
|Technology Preview
|General Availability
|General Availability

|Mount namespace encapsulation
|Not Available
|Technology Preview
|Technology Preview

|NUMA-aware scheduling with NUMA Resources Operator
|Technology Preview
|General Availability
|General Availability

|HTTP transport replaces AMQP for PTP and bare-metal events
|Not Available
|Technology Preview
|Technology Preview

|Intel E810 Westport Channel NIC as PTP grandmaster clock
|Not Available
|Technology Preview
|Technology Preview

|Workload partitioning for three-node clusters and standard clusters
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Operator Technology Preview features

.Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|Hybrid Helm Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Java-based Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Not Available
|Technology Preview
|Technology Preview

|Platform Operators
|Technology Preview
|Technology Preview
|Technology Preview

|RukPak
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Monitoring Technology Preview features

.Monitoring Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14


|Alerting rules based on platform monitoring metrics
|Technology Preview
|Technology Preview
|General Availability

|Metrics Collection Profiles
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== {rh-openstack-first} Technology Preview features

.{rh-openstack} Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14


|====

[discrete]
=== Architecture Technology Preview features

.Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|Hosted control planes for {product-title} on bare metal
|Technology Preview
|Technology Preview
|Technology Preview

|Hosted control planes for {product-title} on Amazon Web Services (AWS)
|Technology Preview
|Technology Preview
|Technology Preview

|Hosted control planes for {product-title} on {VirtProductName}
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Machine management Technology Preview features

.Machine management Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|Managing machines with the Cluster API
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Alibaba Cloud
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Amazon Web Services
|Technology Preview
|Technology Preview
|General Availability

|Cloud controller manager for Google Cloud Platform
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Microsoft Azure
|Technology Preview
|Technology Preview
|General Availability

|Cloud controller manager for Nutanix
|Technology Preview
|General Availability
|General Availability

|Cloud controller manager for VMware vSphere
|Technology Preview
|General Availability
|General Availability

|====

[discrete]
=== Authentication and authorization Technology Preview features

.Authentication and authorization Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|Pod security admission restricted enforcement
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Machine Config Operator Technology Preview features

.Machine Config Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.12 |4.13 |4.14

|{op-system-first} image layering
|Technology Preview
|General Availability
|General Availability

|====

[id="ocp-4-14-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.8 and beyond! This needs some SME/QE review before being updated for 4.11. Need to check if KI should be removed or should stay.
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to {product-version}, you can either revoke or continue to allow unauthenticated access. Unless there is a specific need for unauthenticated access, you should revoke it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* If the installation program cannot get all of the projects that are associated with the Google Cloud Platform (GCP) service account, the installation fails with a `context deadline exceeded` error message.
+
This behavior occurs when the following conditions are met:

** The service account has access to an excessive number of projects.
** The installation program is run with one of the following commands:
*** `openshift-install create install-config`
+
.Error message
[source,text]
----
FATAL failed to fetch Install Config: failed to fetch dependency of "Install Config": failed to fetch dependency of "Base Domain": failed to generate asset "Platform": failed to get projects: context deadline exceeded
----
*** `openshift-install create cluster` without an existing installation configuration file (`install-config.yaml`)
+
.Error message
[source,text]
----
FATAL failed to fetch Metadata: failed to fetch dependency of "Metadata": failed to fetch dependency of "Cluster ID": failed to fetch dependency of "Install Config": failed to fetch dependency of "Base Domain": failed to generate asset "Platform": failed to get projects: context deadline exceeded
----
*** `openshift-install create manifests` with or without an existing installation configuration file
+
.Error message
[source,text]
----
ERROR failed to fetch Master Machines: failed to load asset "Install Config": failed to create install config: platform.gcp.project: Internal error: context deadline exceeded
----
+
As a workaround, if you have an installation configuration file, update it with a specific project id to use (`platform.gcp.projectID`). Otherwise, manually create an installation configuration file, and enter a specific project id. Run the installation program again, specifying the file. (link:https://issues.redhat.com/browse/OCPBUGS-15238[*OCPBUGS-15238*])

[id="ocp-4-14-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating_a_cluster/updating-cluster-web-console.adoc#updating-cluster-web-console[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-14-0-ga"]
=== RHSA-2023:1234 - {product-title} 4.14.0 image release, bug fix, and security update advisory

Issued: 2023-10-TBD

{product-title} release 4.13.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2023:1234[RHSA-2023:1234] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2023:1234[RHSA-2023:1234] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.14.0 --pullspecs
----
//replace 4.y.z for the correct values for the release. You do not need to update oc to run this command.
