:_content-type: ASSEMBLY
[id="ocp-4-10-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-10-about-this-release"]
== About this release

{product-title} *(link:https://access.redhat.com/errata/RHSA-2022:0056[RHSA-2022:0056])* 4.10 is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md[Kubernetes 1.23] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

//Red Hat did not publicly release {product-title} 4.10.0 as the GA version and, instead, is releasing {product-title} 4.10.TBD as the GA version.

{product-title} {product-version} clusters are available at https://cloud.redhat.com/openshift. The {console-redhat-com} application for {product-title} allows you to deploy OpenShift clusters to either on-premise or cloud environments.

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.4 and 8.5, as well as on {op-system-first} 4.10.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//TODO: Add the line below for EUS releases.
//{product-title} 4.6 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

//TODO: The line below is not true for 4.9 but should be used when it is next appropriate. Revisit in October 2022 timeframe.
//With the release of {product-title} 4.9, version 4.6 is now end of life. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].


[id="ocp-4-10-add-on-support-status"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-10-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-10-documentation"]
=== Documentation

[id="ocp-4-10-getting-started"]
==== Getting started with {product-title}

{product-title} 4.10 now includes a getting started guide. Getting Started with {product-title} defines basic terminology and provides role-based next steps for developers and administrators.

The tutorials walk new users through the web console and the OpenShift CLI (`oc`) interfaces. New users can accomplish the following tasks through the Getting Started:

* Create a project
* Grant view permissions
* Deploy a container image from Quay
* Examine and scale an application
* Deploy a Python application from GitHub
* Connect to a database from Quay
* Create a secret
* Load and view your application

For more information, see xref:../getting_started/openshift-overview.adoc[Getting Started with {product-title}].

[id="ocp-4-10-rhcos"]
=== {op-system-first}

[id="ocp-4-10-coreos-installer-customize"]
==== Improved customization of bare metal {op-system} installation

The `coreos-installer` utility now has `iso customize` and `pxe customize` subcommands for more flexible customization when installing {op-system} on bare metal from the live ISO and PXE images.

This includes the ability to customize the installation to fetch Ignition configs from HTTPS servers that use a custom certificate authority or self-signed certificate.

[id="ocp-4-10-rhcos-rhel-8-4-packages"]
==== {op-system} now uses {op-system-base} 8.4

{op-system} now uses {op-system-base-full} 8.4 packages in {product-title} 4.10. These packages provide you the latest fixes, features, and enhancements, such as NetworkManager features, as well as the latest hardware support and driver updates. 

[id="ocp-4-10-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-10-AWS-default-components"]
==== New default component types for AWS installations

The {product-title} 4.10 installer uses new default component types for installations on AWS. The installation program uses the following components by default:

* AWS EC2 M6i instances for both control plane and compute nodes, where available
* AWS EBS gp3 storage

[id="ocp-4-10-OCP-on-ARM"]
==== {product-title} on ARM

{product-title} 4.10 is now supported on ARM based AWS EC2 A1 and bare-metal platforms. Instance availability and installation documentation can be found in xref:../installing/installing-preparing.html#supported-installation-methods-for-different-platforms[Supported installation methods for different platforms].

The following features are supported for {product-title} on ARM:

* OpenShift Cluster Monitoring
* RHEL 8 Application Streams
* OVNKube
* Elastic Block Store (EBS) for AWS
* AWS .NET applications
* NFS storage on bare metal

The following Operators are supported for {product-title} on ARM:

* Node Tuning Operator
* Node Feature Discovery Operator
* Cluster Samples Operator
* Cluster Logging Operator
* Elasticsearch Operator
* Service Binding Operator

[id="ocp-4-10-ibm-cloud-installation"]
==== Installing a cluster on IBM Cloud using installer-provisioned infrastructure

{product-title} 4.10 introduces support for installing a cluster on IBM Cloud using installer-provisioned infrastructure.

For more information, see xref:../installing/installing_ibm_cloud_public/preparing-to-install-on-ibm-cloud.adoc#preparing-to-install-on-ibm-cloud[Preparing to install on IBM Cloud].

[id="ocp-4-10-installation-and-upgrade-vsphere-provisioning"]
==== Thin provisioning support for VMware vSphere cluster installation

{product-title} {product-version} introduces support for thin-provisioned disks when you install a cluster using installer-provisioned infrastructure. You can provision disks as `thin`, `thick`, or `eagerZeroedThick`. For more information about disk provisioning modes in VMware vSphere, see xref:../installing/installing_vsphere/installing-vsphere-installer-provisioned-customizations.adoc#installation-configuration-parameters_installing-vsphere-installer-provisioned-customizations[Installation configuration parameters].

[id="ocp-4-10-installation-and-upgrade-aws-ami"]
==== Installing a cluster into an Amazon Web Services GovCloud region

{op-system-first} Amazon Machine Images (AMIs) are now available for AWS GovCloud regions. The availability of these AMIs improves the installation process because you are no longer required to upload a custom RHCOS AMI to deploy a cluster.

For more information, see xref:../installing/installing_aws/installing-aws-government-region.adoc#installing-aws-government-region[Installing a cluster on AWS into a government region].

[id="ocp-4-10-installation-and-upgrade-tagrole"]
==== Using a custom AWS IAM role for instance profiles

Beginning with {product-title} {product-version}, if you configure a cluster with an existing IAM role, the installation program no longer adds the `shared` tag to the role when deploying the cluster. This enhancement improves the installation process for organizations that want to use a custom IAM role, but whose security policies prevent the use of the `shared` tag.

[id="ocp-4-10-installation-vsphere-csi"]
==== CSI driver installation on vSphere clusters

To install a CSI driver on a cluster running on vSphere, you must have the following components installed:

* Virtual hardware version 15 or greater
* vSphere version 6.7 Update 3 or greater
* VMware ESXi version 6.7 Update 3 or greater

Components with versions lower than those above are still supported, but are deprecated. Support for them will be removed in a future version of {product-title}.

[NOTE]
====
If your cluster is deployed on vSphere, and the preceding components are lower than the version mentioned above, upgrading from {product-title} 4.9 to 4.10 on vSphere is supported, but no vSphere CSI driver will be installed. Bug fixes and other upgrades to 4.10 are still supported, however upgrading to 4.11 will be unavailable.
====

[id="ocp-4-10-installation-and-upgrade-alibaba-ipi"]
==== Installing a cluster on Alibaba Cloud using installer-provisioned infrastructure (Technology Preview)

{product-title} {product-version} introduces the ability for installing a cluster on Alibaba Cloud using installer-provisioned infrastructure in Technology Preview. This type of installation lets you use the installation program to deploy a cluster on infrastructure that the installation program provisions and the cluster maintains.
//For more information, see <insert link to help topic>.

[id="ocp-4-10-installation-and-upgrade-ash-ipi"]
==== Installing a cluster on Microsoft Azure Stack Hub using installer-provisioned infrastructure

{product-title} {product-version} introduces support for installing a cluster on Azure Stack Hub using installer-provisioned infrastructure. This type of installation lets you use the installation program to deploy a cluster on infrastructure that the installation program provisions and the cluster maintains.

For more information, see xref:../installing/installing_azure_stack_hub/installing-azure-stack-hub-default.adoc#installing-azure-stack-hub-default[Installing a cluster on Azure Stack Hub with an installer-provisioned infrastructure].

[id="ocp-4-10-conditional-updates"]
==== Conditional updates

{product-title} 4.10 adds support for consuming conditional update paths provided by the OpenShift Update Service.
Conditional update paths convey identified risks and the conditions under which those risks apply to clusters.
The Administrator perspective on the web console only offers recommended upgrade paths for which the cluster does not match known risks.
However, OpenShift CLI (`oc`) 4.10 or later can be used to display additional upgrade paths for {product-title} 4.10 clusters.
Associated risk information including supporting documentation references is displayed with the paths.
The administrator may review the referenced materials and choose to perform the supported, but no longer recommended, upgrade.

For more information, see xref:../updating/understanding-upgrade-channels-release.adoc#conditional-updates-overview_understanding-upgrade-channels-releases[Conditional updates] and xref:../updating/updating-cluster-cli.adoc#update-conditional-upgrade-pathupdating-cluster-cli[Updating along a conditional upgrade path].

[id="ocp-4-10-web-console"]
=== Web console

[id="ocp-4-10-dynamic-plugin-technology-preview"]
==== Dynamic plug-in (Technology Preview)
Starting with {product-title} {product-version}, the ability to create OpenShift console dynamic plug-ins is now available as a Technology Preview feature. You can use this feature to customize your interface at runtime in many ways, including:

* Adding custom pages
* Adding perspectives and updating navigation items
* Adding tabs and actions to resource pages

For more information about the dynamic plug-in, see xref:../web_console/dynamic-plug-ins.adoc#dynamic-plug-ins_dynamic-plug-ins[Adding a dynamic plug-in to the {product-title} web console].

[id="ocp-4-10-multicluster-console-technology-preview"]
==== Multicluster console (Technology Preview)
Starting with {product-title} 4.10, the multicluster console is now available as a Technology Preview feature. By enabling this feature, you can connect to remote clusters' API servers from a single {product-title} console.

[id="pod-debug-mode-web-console"]
==== Running a pod in debug mode
With this update, you can now view debug terminals in the web console. When a pod has a container that is in a `CrashLoopBackOff` state, a debug pod can be launched. A terminal interface is displayed and can be used to debug the crash looping container.

* This feature can be accessed by the pod status pop-up window, which is accessed by clicking on the status of a pod, provides links to debug terminals for each crash looping container within that pod.
* You can also access this feature on the *Logs* tab of the pod details page. A debug terminal link is displayed above the log window when a crash looping container is selected.

Additionally, the pod status pop-up window now provides links to the *Logs* and *Events* tabs of the pod details page.

[id="web-console-customized-workload-notifications"]
==== Customized workload notifications
With this update, you can customize workload notifications on the *User Preferences* page. *User workload notifications* under the *Notifications* tab allows you to hide user workload notifications that appear on the *Cluster Overview* page or in your drawer.

[id="improved-quota-visibility-web-console"]
==== Improved quota visibility
With this update, non-admin users are now able to view their usage of the `AppliedClusterResourceQuota` on the *Project Overview*, *ResourceQuotas*, and *API Explorer* pages to determine the cluster-scoped quota available for use. Additionally, `AppliedClusterResourceQuota` details can now be found on the *Search* page.

[id="ocp-cluster-support-level"]
==== Cluster support level

{product-title} now enables you to view support level information about your cluster on the *Overview* -> *Details* card, in the *Cluster Settings*, in the *About* modal, and adds a notification to your notifications drawer when your cluster is unsupported. From the *Overview* page, you can manage subscription settings under the *Service Level Agreement (SLA)*.

[id="ocp-4-10-ibm-z"]
=== IBM Z and LinuxONE

With this release, IBM Z and LinuxONE are now compatible with {product-title} {product-version}. The installation can be performed with z/VM or {op-system-base} KVM. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_z/installing-ibm-z.adoc#installing-ibm-z[Installing a cluster with z/VM on IBM Z and LinuxONE]
* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc#installing-restricted-networks-ibm-z[Installing a cluster with z/VM on IBM Z and LinuxONE in a restricted network]
* xref:../installing/installing_ibm_z/installing-ibm-z-kvm.adoc#installing-ibm-z-kvm[Installing a cluster with {op-system-base} KVM on IBM Z and LinuxONE]
* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z-kvm.adoc#installing-restricted-networks-ibm-z-kvm[Installing a cluster with RHEL KVM on IBM Z and LinuxONE in a restricted network]

[discrete]
==== Notable enhancements

The following new features are supported on IBM Z and LinuxONE with {product-title} {product-version}:

* Horizontal pod autoscaling
* The following Multus CNI plug-ins are supported:
** Bridge
** Host-device
** IPAM
** IPVLAN

* NMState Operator
* OVN-Kubernetes IPsec encryption
* Vertical Pod Autoscaler Operator

[discrete]
==== Supported features

The following features are also supported on IBM Z and LinuxONE:

* Currently, the following Operators are supported:
** Cluster Logging Operator
** Local Storage Operator
** NFD Operator
** NMState Operator
** OpenShift Elasticsearch Operator
** Service Binding Operator
** Vertical Pod Autoscaler Operator

* Encrypting data stored in etcd
* Helm
* Multipathing
* Persistent storage using iSCSI
* Persistent storage using local volumes (Local Storage Operator)
* Persistent storage using hostPath
* Persistent storage using Fibre Channel
* Persistent storage using Raw Block
* OVN-Kubernetes
* Support for multiple network interfaces
* Three-node cluster support
* z/VM Emulated FBA devices on SCSI disks
* 4K FCP block device

These features are available only for {product-title} on IBM Z and LinuxONE for {product-version}:

* HyperPAV enabled on IBM Z and LinuxONE for the virtual machines for FICON attached ECKD storage

[discrete]
==== Restrictions

The following restrictions impact {product-title} on IBM Z and LinuxONE:

* The following {product-title} Technology Preview features are unsupported:
** Precision Time Protocol (PTP) hardware

* The following {product-title} features are unsupported:
** Automatic repair of damaged machines with machine health checking
** CodeReady Containers (CRC)
** Controlling overcommit and managing container density on nodes
** CSI volume cloning
** CSI volume snapshots
** FIPS cryptography
** NVMe
** OpenShift Metering
** OpenShift Virtualization
** Tang mode disk encryption during {product-title} deployment

* Worker nodes must run {op-system-first}
* Persistent shared storage must be provisioned by using either NFS or other supported storage protocols
* Persistent non-shared storage must be provisioned using local storage, like iSCSI, FC, or using LSO with DASD, FCP, or EDEV/FBA

[id="ocp-4-10-ibm-power"]
=== IBM Power

With this release, IBM Power is now compatible with {product-title} {product-version}. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_power/installing-ibm-power.adoc#installing-ibm-power_installing-ibm-power[Installing a cluster on IBM Power]
* xref:../installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc#installing-restricted-networks-ibm-power_installing-restricted-networks-ibm-power[Installing a cluster on IBM Power in a restricted network]

[discrete]
==== Notable enhancements

The following new features are supported on IBM Power with {product-title} {product-version}:

* Horizontal pod autoscaling
* The following Multus CNI plug-ins are supported:
** Bridge
** Host-device
** IPAM
** IPVLAN

* NMState Operator
* OVN-Kubernetes IPsec encryption
* Vertical Pod Autoscaler Operator

[discrete]
==== Supported features

The following features are also supported on IBM Power:

* Currently, the following Operators are supported:
** Cluster Logging Operator
** Local Storage Operator
** NFD Operator
** NMState Operator
** OpenShift Elasticsearch Operator
** SR-IOV Network Operator
** Service Binding Operator
** Vertical Pod Autoscaler Operator

* Encrypting data stored in etcd
* Helm
* Multipathing
* Multus SR-IOV
* NVMe
* OVN-Kubernetes
* Persistent storage using iSCSI
* Persistent storage using local volumes (Local Storage Operator)
* Persistent storage using hostPath
* Persistent storage using Fibre Channel
* Persistent storage using Raw Block
* Support for multiple network interfaces
* Support for Power10
* Three-node cluster support
* 4K Disk Support

[discrete]
==== Restrictions

The following restrictions impact {product-title} on IBM Power:

* The following {product-title} Technology Preview features are unsupported:
** Precision Time Protocol (PTP) hardware

* The following {product-title} features are unsupported:
** Automatic repair of damaged machines with machine health checking
** CodeReady Containers (CRC)
** Controlling overcommit and managing container density on nodes
** FIPS cryptography
** OpenShift Metering
** OpenShift Virtualization
** Tang mode disk encryption during {product-title} deployment

* Worker nodes must run {op-system-first}
* Persistent storage must be of the Filesystem type that uses local volumes, Network File System (NFS), or Container Storage Interface (CSI)

// Remove after 4.10

[id="ocp-4-10-security"]
=== Security and compliance

Information regarding new features, enhancements, and bug fixes for security and compliance components can be found in the xref:../security/compliance_operator/compliance-operator-release-notes.adoc[Compliance Operator] and xref:../security/file_integrity_operator/file-integrity-operator-release-notes.adoc[File Integrity Operator] release notes.

For more information about security and compliance, see xref:../security/index.adoc[{product-title} security and compliance].

[id="ocp-4-10-networking"]
=== Networking

[id="ocp-4-10-networking-dual-stack-api-change"]
==== Dual-stack services require that ipFamilyPolicy is specified

When you create a service that uses multiple IP address families, you must explicitly specify `ipFamilyPolicy: PreferDualStack` or `ipFamilyPolicy: RequireDualStack` in your Service object definition. This change breaks backwards compatibility with earlier releases of {product-title}.

For more information, see link:https://bugzilla.redhat.com/show_bug.cgi?id=2045576[BZ#2045576].

[id="ocp-4-10-redfish-events"]
==== Low-latency Redfish hardware event delivery (Technology Preview)

{product-title} now provides a hardware event proxy that enables applications running on bare-metal clusters to respond quickly to Redfish hardware events, such as hardware changes and failures.

The hardware event proxy supports a publish-subscribe service that allows relevant applications to consume hardware events detected by Redfish. The proxy must be running on hardware that supports Redfish v1.8 and higher. An Operator manages the lifecycle of the `hw-event-proxy` container.

You can use a REST API to develop applications to consume and respond to events such as breaches of temperature thresholds, fan failure, disk loss, power outages, and memory failure. Reliable end-to-end messaging without persistent stores is based on the Advanced Message Queuing Protocol (AMQP). The latency of the messaging service is in the 10 millisecond range.

[NOTE]
====
This feature is supported for single node OpenShift clusters only.
====

[id="ocp-4-10-networking-ovn-gateway-config"]
==== OVN-Kubernetes support for gateway configuration

The OVN-Kubernetes CNI network provider adds support for configuring how egress traffic is sent to the node gateway.
By default, egress traffic is processed in OVN to exit the cluster and traffic is not affected by specialized routes in the kernel routing table.

This enhancement adds a `gatewayConfig.routingViaHost` field.
With this update, the field can be set at runtime as a post-installation activity and when it is set to `true`, egress traffic is sent from pods to the host networking stack.
This update benefits highly specialized installations and applications that rely on manually configured routes in the kernel routing table.

This enhancement has an interaction with the Open vSwitch hardware offloading feature.
With this update, when the `gatewayConfig.routingViaHost` field is set to `true`, you do not receive the performance benefits of the offloading because egress traffic is processed by the host networking stack.

For configuration information, see xref:../networking/cluster-network-operator.adoc#nw-operator-configuration-parameters-for-ovn-sdn_cluster-network-operator[Configuration for the OVN-Kubernetes CNI cluster network provider].

[id="ocp-4-10-networking-metrics"]
==== Enhancements to networking metrics

The following metrics are now available for clusters.
The metric names that start with `sdn_controller` are unique to the OpenShift SDN CNI network provider.
The metric names that start with `ovn` are unique to the OVN-Kubernetes CNI network provider:

* `network_attachment_definition_instances{networks="egress-router"}`
* `openshift_unidle_events_total`
* `ovn_controller_bfd_run`
* `ovn_controller_ct_zone_commit`
* `ovn_controller_flow_generation`
* `ovn_controller_flow_installation`
* `ovn_controller_if_status_mgr`
* `ovn_controller_if_status_mgr_run`
* `ovn_controller_if_status_mgr_update`
* `ovn_controller_integration_bridge_openflow_total`
* `ovn_controller_ofctrl_seqno_run`
* `ovn_controller_patch_run`
* `ovn_controller_pinctrl_run`
* `ovnkube_master_ipsec_enabled`
* `ovnkube_master_num_egress_firewall_rules`
* `ovnkube_master_num_egress_firewalls`
* `ovnkube_master_num_egress_ips`
* `ovnkube_master_pod_first_seen_lsp_created_duration_seconds`
* `ovnkube_master_pod_lsp_created_port_binding_duration_seconds`
* `ovnkube_master_pod_port_binding_chassis_port_binding_up_duration_seconds`
* `ovnkube_master_pod_port_binding_port_binding_chassis_duration_seconds`
* `sdn_controller_num_egress_firewall_rules`
* `sdn_controller_num_egress_firewalls`
* `sdn_controller_num_egress_ips`

The `ovnkube_master_resource_update_total` metric is removed for the 4.10 release.

[id="ocp-4-10-networkobservability-switch"]
==== Switching between YAML view and a web console form

* Previously, changes were not retained when switching between *YAML view* and *Form view* on the web console. Additionally, after switching to *YAML view*, you could not return to *Form view*. With this update, you can now easily switch between *YAML view* and *Form view* on the web console without losing changes.

[id="ocp-4-10-networkobservability-targeted-pods"]
==== Listing pods targeted by network policies
When using the network policy functionality in the {product-title} web console, the pods affected by a policy are listed. The list changes as the combined namespace and pod selectors in these policy sections are modified:

* Peer definition
* Rule definition
* Ingress
* Egress

The list of impacted pods includes only those pods accessible by the user.

[id="ocp-4-10-networking-must-gather-tcpdump"]
==== Enhancement to must-gather to simplify network tracing

The `oc adm must-gather` command is enhanced in a way that simplifies collecting network packet captures.

Previously, `oc adm must-gather` could start a single debug pod only.
With this enhancement, you can start a debug pod on multiple nodes at the same time.

You can use the enhancement to run packet captures on multiple nodes at the same time to simplify troubleshooting network communication issues.
A new `--node-selector` argument provides a way to identify which nodes you are collecting packet captures for.

For more information, see xref:../support/gathering-cluster-data.adoc#support-network-trace-methods_gathering-cluster-data[Network trace methods] and xref:../support/gathering-cluster-data.adoc#support-collecting-host-network-trace_gathering-cluster-data[Collecting a host network trace].
[id="ocp-4-10-pod-level-bonding"]
==== Pod-level bonding for secondary networks (Technology Preview)
Bonding at the pod level is vital to enable workloads inside pods that require high availability and more throughput. With pod-level bonding, you can create a bond interface from multiple single root I/O virtualization (SR-IOV) virtual function interfaces in kernel mode interface. The SR-IOV virtual functions are passed into the pod and attached to a kernel driver.

Scenarios where pod-level bonding is required include creating a bond interface from multiple SR-IOV virtual functions on different physical functions. Creating a bond interface from two different physical functions on the host can be used to achieve high availability at pod level.

[NOTE]
====
The current functionality of Bond CNI is available only in active-backup mode. For further details, see link:https://bugzilla.redhat.com/show_bug.cgi?id=2037214[*BZ#2037214*].
====

[id="ocp-4-10-egress-ip-support-public-clouds"]
==== Egress IP address support for clusters installed on public clouds

As a cluster administrator, you can associate one or more egress IP addresses with a namespace. An egress IP address ensures that a consistent source IP address is associated with traffic from a particular namespace that is leaving the cluster.

For the OVN-Kubernetes and OpenShift SDN cluster network providers, you can configure an egress IP address on the following public cloud providers:

* Amazon Web Services (AWS)
* Google Cloud Platform (GCP)
* Microsoft Azure

To learn more, refer to the respective documentation for your cluster network provider:

* OVN-Kubernetes: xref:../networking/ovn_kubernetes_network_provider/configuring-egress-ips-ovn.adoc#configuring-egress-ips-ovn[Configuring an egress IP address]
* OpenShift SDN: xref:../networking/openshift_sdn/assigning-egress-ips.adoc#egress-ips[Configuring egress IPs for a project]

[id="ocp-4-10-openshift-sdn-netpol-egress-policies"]
==== OpenShift SDN cluster network provider network policy support for egress policies and ipBlock except

If you use the OpenShift SDN cluster network provider, you can now use egress rules in network policy with `ipBlock` and `ipBlock.except`. You define egress policies in the `egress` array of the `NetworkPolicy` object.

For more information, refer to xref:../networking/network_policy/about-network-policy.adoc#about-network-policy[About network policy].

[id="ocp-4-10-ne-router-compression"]
==== Ingress Controller router compression
This enhancement adds the ability to configure global HTTP traffic compression on the HAProxy Ingress Controller for specific MIME types. This update enables gzip-compression of your ingress workloads when there are large amounts of compressible routed traffic.

For more information, see xref:../networking/ingress-operator.adoc#nw-configuring-router-compression_configuring-ingress[Using router compression].

[id="ocp-4-10-ne-coredns-customization"]
==== Support for CoreDNS customization
A cluster administrator can now configure DNS servers to allow DNS name resolution through the configured servers for the default domain. A DNS forwarding configuration can have both the default servers specified in the `/etc/resolv.conf` file and the upstream DNS servers.

For more information, see xref:../networking/dns-operator.adoc#nw-dns-forward_dns-operator[Using DNS forwarding].

[id="ocp-4-10-ne-syslog-maxlength"]
==== Support for configuring the maximum length of the syslog message in the Ingress Controller
You can now set the maximum length of the syslog message in the Ingress Controller to any value between 480 and 4096 bytes.

For more information, see xref:../networking/ingress-operator.adoc#nw-ingress-controller-configuration-parameters_configuring-ingress[Ingress Controller configuration parameters].

[id="ocp-4-10-ovs-hardware-offloading"]
==== Open vSwitch hardware offloading support for SR-IOV

You can now configure Open vSwitch hardware offloading to increase data processing performance on compatible bare metal nodes.
Hardware offloading is a method for processing data that removes data processing tasks from the CPU and transfers them to the dedicated data processing unit of a network interface controller.
Benefits of this feature include faster data processing, reduced CPU workloads, and lower computing costs.

For more information, see xref:../networking/hardware_networks/configuring-hardware-offloading.adoc#about-hardware-offloading_hardware-offloading[Configuring hardware offloading].

[id="ocp-4-10-networking-ipi-nmstate"]
==== Configuring host network interfaces with NMState on installer-provisioned clusters

{product-title} now provides a `networkConfig` configuration setting for installer-provisioned clusters, which takes an NMState YAML configuration to configure host interfaces. During installer-provisioned installations, you can add the `networkConfig` configuration setting and the NMState YAML configuration to the `install-config.yaml` file. Additionally, you can add the `networkConfig` configuration setting and the NMState YAML configuration to the bare metal host resource when using the Bare Metal Operator.

The most common use case for the `networkConfig` configuration setting is to set static IP addresses on a host's network interface during installation or while expanding the cluster.

For more information, see xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.html#configuring-host-network-interfaces-in-the-install-config.yaml-file_ipi-install-installation-workflow[Configuring host network interfaces in the install-config.yaml file].

[id="ocp-4-10-networking-linuxptp"]
==== Boundary clock and PTP enhancements to linuxptp services

You can now specify multiple network interfaces in a `PtpConfig` profile to allow nodes running RAN vDU applications to serve as a Precision Time Protocol Telecom Boundary Clock (PTP T-BC). Interfaces configured as boundary clocks now also support PTP fast events.

For more information, see xref:../networking/using-ptp.adoc#configuring-linuxptp-services-as-boundary-clock_using-ptp[Configuring linuxptp services as boundary clock].

[id="ocp-4-10-networking-columbiaville"]
==== Support for Intel 800-Series Columbiaville NICs

Intel 800-Series Columbiaville NICs are now fully supported for interfaces configured as boundary clocks or ordinary clocks. Columbiaville NICs are supported in the following configurations:

* Ordinary clock
* Boundary clock synced to the Grandmaster clock
* Boundary clock with one port synchronizing from an upstream source clock, and three ports providing downstream timing to destination clocks

For more information, see xref:../networking/using-ptp.adoc#configuring-ptp-devices[Configuring PTP devices].

[id="ocp-4-10-networking-k8s-nmstate-operator"]
==== Kubernetes NMState Operator is GA for bare-metal, IBM Power, IBM Z, and LinuxONE installations

{product-title} now provides the Kubernetes NMState Operator for bare-metal, IBM Power, IBM Z, and LinuxONE installations. The Kubernetes NMState Operator is still a Technology Preview for all other platforms. See xref:../networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.adoc[About the Kubernetes NMState Operator] for additional details.


[id="ocp-4-10-hardware"]
=== Hardware

[id="ocp-4-10-networking-metallb"]
==== Enhancements to MetalLB load balancing

The following enhancements to MetalLB and the MetalLB Operator are included in this release:

* Support for Border Gateway Protocol (BGP) is added.
* Support for Bidirectional Forwarding Detection (BFD) in combination with BGP is added.
* Support for IPv6 and dual-stack networking is added.
* Support for specifying a node selector on the `speaker` pods is added. You can now control which nodes are used for advertising load balancer service IP addresses. This enhancement applies to layer 2 mode and BGP mode.
* Validating web hooks are added to ensure that address pool and BGP peer custom resources are valid.
* The `v1alpha1` API version for the `AddressPool` and `MetalLB` custom resource definitions that were introduced in the 4.9 release are deprecated. Both custom resources are updated to the `v1beta1` API version.
* Support for speaker pod tolerations in the MetalLB custom resource definition is added.

For more information, see xref:../networking/metallb/about-metallb.adoc#about-metallb[About MetalLB and the MetalLB Operator].

[id="ocp-4-10-storage"]
=== Storage

[id="ocp-4-10-storage-metrics-indication"]
==== Storage metrics indicator

* With this update, workloads can securely share `Secrets` and `ConfigMap` objects across namespaces using inline ephemeral `csi` volumes provided by the Shared Resource CSI Driver. Container Storage Interface (CSI) volumes and the Shared Resource CSI Driver are Technology Preview features. (link:https://issues.redhat.com/browse/BUILD-293[BUILD-293])

[id="ocp-4-10-console-storage-plugin-enhancement"]
==== Console Storage Plug-in enhancement

* A new feature has been added to the Console Storage Plug-in that adds Aria labels throughout the installation flow for screen readers. This provides better accessibility for users that use screen readers to access the console.

* A new feature has been added that provides metrics indicating the amount of used space on volumes used for persistent volume claims (PVCs). This information appears in the PVC list, and in the PVC details in the *Used* column. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1985965[*BZ#1985965*])

[id="ocp-4-10-olm"]
=== Operator Lifecycle Manager

[id="ocp-4-10-copied-csvs"]
==== Disabling copied CSVs to support large clusters

When an Operator is installed by Operator Lifecycle Manager (OLM), a simplified copy of its cluster service version (CSV) is created in every namespace that the Operator is configured to watch. These CSVs are known as copied CSVs; they identify controllers that are actively reconciling resource events in a given namespace.

On large clusters, with namespaces and installed Operators potentially in the hundreds or thousands, copied CSVs can consume an untenable amount of resources, such as OLMâ€™s memory usage, cluster etcd limits, and networking bandwidth. To support these larger clusters, cluster administrators can now disable copied CSVs for Operators that are installed with the `AllNamespaces` mode.

For more details, see xref:../operators/admin/olm-config.adoc#olm-config[Configuring Operator Lifecycle Manager features].

[id="ocp-4-10-olm-dependencies"]
==== Generic and complex constraints for dependencies

Operators with specific dependency requirements can now use complex constraints or requirement expressions. The new `olm.constraint` bundle property holds dependency constraint information. A message field allows Operator authors to convey high-level details about why a particular constraint was used.

For more details, see xref:../operators/understanding/olm/olm-understanding-dependency-resolution.adoc#olm-understanding-dependency-resolution[Operator Lifecycle Manager dependency resolution].

[id="ocp-4-10-olm-hypershift"]
==== Operator Lifecycle Manager support for Hypershift

Operator Lifecycle Manager (OLM) components, including Operator catalogs, can now run entirely on Hypershift-managed control planes. This capability does not incur any cost to tenants on worker nodes.

[id="ocp-4-10-olm-arm-enhancements"]
==== Operator Lifecycle Manager support for ARM

Previously, the default Operator catalogs did not support ARM. With this enhancement, Operator Lifecycle Manager (OLM) adds default Operator catalogs to ARM clusters. As a result, the OperatorHub now includes content by default for Operators that support ARM. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1996928[*BZ#1996928*])

[id="ocp-4-10-osdk"]
=== Operator development

[id="ocp-4-10-hybrid-helm-operator"]
==== Hybrid Helm Operator (Technology Preview)

The standard Helm-based Operator support in the Operator SDK has limited functionality compared to the Go-based and Ansible-based Operator support that has reached the Auto Pilot capability (level V) in the xref:../operators/understanding/olm-what-operators-are.adoc#olm-maturity-model_olm-what-operators-are[Operator maturity model].

Starting in {product-title} 4.10 as a Technology Preview feature, the Operator SDK includes the Hybrid Helm Operator to enhance the existing Helm-based support abilities through Go APIs. Operator authors can generate an Operator project beginning with a Helm chart, and then add advanced, event-based logic to the Helm reconciler in Go language. Authors can use Go to continue adding new APIs and custom resource definitions (CRDs) in the same project.

For more details, see xref:../operators/operator_sdk/helm/osdk-hybrid-helm.adoc#osdk-hybrid-helm[Operator SDK tutorial for Hybrid Helm Operators].

[id="ocp-4-10-ansible-operator-metrics"]
==== Custom metrics for Ansible-based Operators

Operator authors can now use the Ansible-based Operator support in the Operator SDK to expose custom metrics, emit Kubernetes events, and provide better logging.

For more details, see xref:../operators/operator_sdk/osdk-monitoring-prometheus.adoc#osdk-monitoring-prometheus[Exposing custom metrics for Ansible-based Operators].

[id="ocp-4-10-builds"]
=== Builds

* With this update, you can use CSI volumes in OpenShift Builds, which is a Technology Preview feature. This feature relies on the newly introduced Shared Resource CSI Driver and the Insights Operator to import {op-system-base} Simple Content Access (SCA) certificates. For example, by using this feature, you can run entitled builds with `SharedSecret` objects and install entitled RPM packages during builds rather than copying your {op-system-base} subscription credentials and certificates into the builds' namespaces. (link:https://issues.redhat.com/browse/BUILD-274[BUILD-274])
+
[IMPORTANT]
====
The `SharedSecret` objects and OpenShift Shared Resources feature are only available if you enable the `TechPreviewNoUpgrade` feature set. These Technology Preview features are not part of the default features. Enabling this feature set cannot be undone and prevents upgrades. This feature set is not recommended on production clusters. See xref:../post_installation_configuration/cluster-tasks.adoc#post-install-tp-tasks[Enabling Technology Preview features using FeatureGates].
====

* With this update, workloads can securely share `Secrets` and `ConfigMap` objects across namespaces using inline ephemeral `csi` volumes provided by the Shared Resource CSI Driver. Container Storage Interface (CSI) volumes and the Shared Resource CSI Driver are Technology Preview features. (link:https://issues.redhat.com/browse/BUILD-293[BUILD-293])

[id="ocp-4-10-jenkins"]
=== Jenkins

* With this update, you can run Jenkins agents as sidecar containers. You can use this capability to run any container image in a Jenkins pipeline that has a correctly configured pod template and Jenkins file. Now, to compile code, you can run two new pod templates named `java-build` and `nodejs-builder` as sidecar containers with Jenkins. These two pod templates use the latest Java and NodeJS versions provided by the `java` and `nodejs` image streams in the `openshift` namespace. The previous non-sidecar `maven` and `nodejs` pod templates have been deprecated. (link:https://issues.redhat.com/browse/JKNS-132[JKNS-132])

[id="ocp-4-10-machine-api"]
=== Machine API

[id="ocp-4-10-mco-config-drift"]
==== Enhanced configuration drift detection

With this enhancement, the Machine Config Daemon (MCD) now checks nodes for configuration drift if a filesystem write event occurs for any of the files specified in the machine config and before a new machine config is applied, in addition to node bootup. Previously, the MCD checked for configuration drift only at node bootup. This change was made because node reboots do not occur frequently enough to avoid the problems caused by configuration drift until an administrator can correct the issue.

Configuration drift occurs when the on-disk state of a node differs from what is configured in the machine config. The Machine Config Operator (MCO) uses the MCD to check nodes for configuration drift and, if detected, sets that node and machine config pool (MCP) to `degraded`.

For more information about configuration drift, see xref:../post_installation_configuration/machine-configuration-tasks.adoc#machine-config-drift-detection_post-install-machine-configuration-tasks[Understanding configuration drift detection].

[id="ocp-4-10-nodes"]
=== Nodes

[id="ocp-4-10-node-cgroups-v2"]
==== Linux control groups version 2 (Developer Preview)

You can now enable link:https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html[Linux control groups version 2] (cgroups v2) on specific nodes in your cluster. The {product-title} process for enabling cgroups v2 disables all cgroups version 1 controllers and hierarchies. The {product-title} cgroups version 2 feature is in Developer Preview and is not supported by Red Hat at this time. For more information, see xref:../nodes/nodes/nodes-nodes-working.adoc#nodes-nodes-cgroups-2_nodes-nodes-working[Enabling Linux control groups version 2 (cgroups v2)].

[id="ocp-4-10-node-swap-memory"]
==== Support for swap memory use on nodes (Technology preview)

You can enable swap memory use for {product-title} workloads on a per-node basis. For more information, see xref:../nodes/nodes/nodes-nodes-working.adoc#nodes-nodes-swap-memory_nodes-nodes-working[Enabling swap memory use on nodes].

[id="ocp-4-10-node-maintenance-operator"]
==== Place nodes into maintenance mode by using the Node Maintenance Operator

The Node Maintenance Operator (NMO) cordons off nodes from the rest of the cluster and drains all the pods from the nodes. By placing nodes under maintenance, you can investigate problems with a machine, or perform operations on the underlying machine, that might result in a node failure. This is a standalone version of NMO. If you installed OpenShift Virtualization, then you must use the NMO that is bundled with it.

[id="ocp-4-10-node-health-check-operator"]
==== Node Health Check Operator enhancements

The Node Health Check Operator provides these new enhancements:

* Support for running in disconnected mode
* Prevent conflicts with machine health check. For more information, see xref:../nodes/nodes/eco-node-health-check-operator.adoc#how-nhc-prevent-conflict-with-mhc_node-health-check-operator[About how node health checks prevent conflicts with machine health checks]

[id="ocp-4-10-logging"]
=== Red Hat OpenShift Logging

In {product-title} 4.7, _Cluster Logging_ became _Red Hat OpenShift Logging_. For more information, see xref:../logging/cluster-logging-release-notes.adoc[Release notes for Red Hat OpenShift Logging].

[id="ocp-4-10-monitoring"]
=== Monitoring

[id="ocp-4-10-scalability-and-performance"]
=== Scalability and performance

[id="ocp-4-10-SRO-metrics"]
==== New Special Resource Operator metrics

The Special Resource Operator (SRO) now exposes metrics to help you watch the health of your SRO custom resources and objects. For more information, see xref:../hardware_enablement/psap-special-resource-operator.html#special-resource-operator-metrics_special-resource-operator[Prometheus Special Resource Operator metrics].

[id="ocp-4-10-sro-crd-field"]
==== Special Resource Operator custom resource definition fields

Using `oc explain` for  Special Resource Operator (SRO) now provides online documentation for SRO custom resource definitions (CRD). This enhancement provides better specifics for CRD fields. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2031875[*BZ#2031875*])

[id="ocp-4-10-nto-metrics"]
==== New Node Tuning Operator metric added to Telemetry

A Node Tuning Operator (NTO) metric is now added to Telemetry. Follow the procedure in xref:../support/remote_health_monitoring/showing-data-collected-by-remote-health-monitoring.adoc#showing-data-collected-from-the-cluster_showing-data-collected-by-remote-health-monitoring[Showing data collected by Telemetry] to see all the metrics collected by Telemetry.

[id="ocp-4-10-nfd-topology-updater"]
==== NFD Topology Updater is now available

The Node Feature Discovery (NFD) Topology Updater is a daemon responsible for examining allocated resources on a worker node. It accounts for resources that are available to be allocated to new pod on a per-zone basis, where a zone can be a Non-Uniform Memory Access (NUMA) node. See xref:../hardware_enablement/psap-node-feature-discovery-operator.adoc#using-the-nfd-topology-updater_node-feature-discovery-operator[Using the NFD Topology Updater] for more information.

[id="ocp-4-10-hyperthreading-aware-CPU-manager-policy"]
==== Hyperthreading-aware CPU manager policy (Technology Preview)
Hyperthreading-aware CPU manager policy in {product-title} is now available without the need for extra tuning. The cluster administrator can enable this feature if required. Hyperthreads are abstracted by the hardware as logical processors. Hyperthreading allows a single physical processor to execute two heavyweight threads (processes) at the same time, dynamically sharing processor resources.

[id="ocp-4-10-backup-and-restore"]
=== Backup and restore

[id="ocp-4-10-dev-exp"]
=== Developer experience

[id="ocp-4-10-dev-exp-prune-deployments"]
==== Pruning deployment replica sets (Technology Preview)

This release introduces a Technology Preview flag `--replica-sets` to the `oc adm prune deployments` command. By default, only replication controllers are pruned with the `oc adm prune deployments` command. When you set `--replica-sets` to `true`, replica sets are also included in the pruning process.

For more information, see xref:../applications/pruning-objects.adoc#pruning-deployments_pruning-objects[Pruning deployment resources].

[id="ocp-4-10-insights-operator"]
=== Insights Operator

[id="ocp-4-10-insights-operator-sca"]
==== Importing simple content access certificates

In {product-title} 4.10, Insights Operator now imports your simple content access certificates from {console-redhat-com} by default.

For more information, see xref:../support/remote_health_monitoring/insights-operator-simple-access.adoc[Importing simple content access certificates with Insights Operator].

[id="ocp-4-10-insights-operator-data-collection-enhancements"]
==== Insights Operator data collection enhancements

To reduce the amount of data sent to Red Hat, Insights Operator only gathers information when certain conditions are met. For example, Insights Operator only gathers the Alertmanager logs when Alertmanager fails to send alert notifications.

In {product-title} 4.10, the Insights Operator collects the following additional information:

* (Conditional) The logs from pods where the `KubePodCrashlooping` and `KubePodNotReady` alerts are firing
* (Conditional) The Alertmanager logs when the `AlertmanagerClusterFailedToSendAlerts` or `AlertmanagerFailedToSendAlerts` alerts are firing
* Silenced alerts from Alertmanager
* The node logs from the journal unit (kubelet)
* The `CostManagementMetricsConfig` from clusters with `costmanagement-metrics-operator` installed
* The time series database status from the monitoring stack Prometheus instance
* Additional information about the {product-title} scheduler

With this additional information, Red Hat improves {product-title} functionality and enhances Insights Advisor recommendations.

[id="ocp-4-10-auth"]
=== Authentication and authorization

[id="ocp-4-10-auth-sync-group-membership"]
==== Syncing group membership from OpenID Connect identity providers

This release introduces support for synchronizing group membership from an OpenID Connect provider to {product-title} upon user login. You can enable this by configuring the `groups` claim in the {product-title} OpenID Connect identity provider configuration.

For more information, see xref:../authentication/identity_providers/configuring-oidc-identity-provider.adoc#identity-provider-oidc-CR_configuring-oidc-identity-provider[Sample OpenID Connect CRs].

[id="ocp-4-10-auth-oidc-providers"]
==== Additional supported OIDC providers

The Okta and Ping Identity OpenID Connect (OIDC) providers are now tested and supported with {product-title}.

For the full list of OIDC providers, see xref:../authentication/identity_providers/configuring-oidc-identity-provider.adoc#identity-provider-oidc-supported_configuring-oidc-identity-provider[Supported OIDC providers].

[id="ocp-4-10-oc-commands-obtain-credentials-from-podman-config"]
==== oc commands now obtain credentials from Podman configuration locations

Previously, `oc` commands that used the registry configuration, for example `oc registry login` or `oc image` commands, obtained credentials from Docker configuration locations. With {product-title} {product-version}, if a registry entry cannot be found in the default Docker configuration location, `oc` commands obtain the credentials from Podman configuration locations. You can set your preference to either `docker` or `podman` by using the `REGISTRY_AUTH_PREFERENCE` environment variable to prioritize the location.

Users also have the option to use the `REGISTRY_AUTH_FILE` environment variable, which serves as an alternative to the existing `--registry-config` CLI flag. The `REGISTRY_AUTH_FILE` environment variable is also compatible with `podman`.

[id="ocp-4-10-notable-technical-changes"]
== Notable technical changes

{product-title} {product-version} introduces the following notable technical changes.

// Note: use [discrete] for these sub-headings.

[discrete]
[id="ocp-4-10-TLS-subject-alternative-names-required"]
==== TLS X.509 certificates must have a Subject Alternative Name

X.509 certificates must have a properly set the Subject Alternative Name field.
If you update your cluster without this, you risk breaking your cluster or rendering it inaccessible.

In older versions of {product-title}, X.509 certificates worked without a Subject Alternative Name, so long as the Common Name field was set.
link:https://docs.openshift.com/container-platform/4.6/release_notes/ocp-4-6-release-notes.html#ocp-4-6-tls-common-name[This behavior was removed in {product-title} 4.6].

In some cases, certificates without a Subject Alternative Name continued to work in {product-title} 4.6, 4.7, 4.8, and 4.9.
Because it uses Kubernetes 1.23, {product-title} 4.10 does not allow this under any circumstances.

[discrete]
[id="ocp-4-10-cluster-cloud-controller-manager-operator"]
==== Cloud controller managers for additional cloud providers

The Kubernetes community plans to deprecate the Kubernetes controller manager in favor of using cloud controller managers to interact with underlying cloud platforms. As a result, there is no plan to add Kubernetes controller manager support for any new cloud platforms. The IBM Cloud implementation that is added in this release of {product-title} uses cloud controller managers.

In addition, this release supports using cloud controller managers for Google Cloud Platform (GCP), VMware vSphere, and Alibaba Cloud as a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview].

To learn more about the cloud controller manager, see the link:https://kubernetes.io/docs/concepts/architecture/cloud-controller/[Kubernetes Cloud Controller Manager documentation].

To manage the cloud controller manager and cloud node manager deployments and lifecycles, use the Cluster Cloud Controller Manager Operator.

For more information, see the xref:../operators/operator-reference.adoc#cluster-cloud-controller-manager-operator_platform-operators-ref[Cluster Cloud Controller Manager Operator] entry in the _Platform Operators reference_.

[discrete]
[id="ocp-4-10-operator-sdk-v-1-16-0"]
==== Operator SDK v1.16.0

{product-title} 4.10 supports Operator SDK v1.16.0. See xref:../cli_reference/osdk/cli-osdk-install.adoc#cli-osdk-install[Installing the Operator SDK CLI] to install or update to this latest version.

[NOTE]
====
Operator SDK v1.16.0 supports Kubernetes 1.22.
====

Many deprecated `v1beta1` APIs were removed in Kubernetes 1.22, including `sigs.k8s.io/controller-runtime v0.10.0` and `controller-gen v0.7`. This is a breaking change if you need to scaffold `v1beta1` APIs for custom resource definitions (CRDs) or webhooks to publish your project into older cluster versions.

For more information about changes introduced in Kubernetes 1.22, see link:https://docs.openshift.com/container-platform/4.9/release_notes/ocp-4-9-release-notes.html#ocp-4-9-osdk-k8s-api-bundle-validate[Validating bundle manifests for APIs removed from Kubernetes 1.22] and link:https://docs.openshift.com/container-platform/4.9/release_notes/ocp-4-9-release-notes.html#ocp-4-9-removed-kube-1-22-apis[Beta APIs removed from Kubernetes 1.22].

If you have any Operator projects that were previously created or maintained with Operator SDK v1.10.1, see xref:../operators/operator_sdk/osdk-upgrading-projects.adoc#osdk-upgrading-projects[Upgrading projects for newer Operator SDK versions] to ensure your projects are upgraded to maintain compatibility with Operator SDK v1.16.0.

[id="ocp-4-10-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more fine-grained functionality that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *DEP*: _Deprecated_
* *REM*: _Removed_

//TODO: remove anything that has been REM since 4.7?

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.8 |OCP 4.9 | OCP 4.10

|Package manifest format (Operator Framework)
|REM
|REM
|REM

|SQLite database format for Operator catalogs
|GA
|DEP
|DEP

|`oc adm catalog build`
|REM
|REM
|REM

|`--filter-by-os` flag for `oc adm catalog mirror`
|REM
|REM
|REM

|v1beta1 CRDs
|DEP
|REM
|REM

|Docker Registry v1 API
|DEP
|REM
|REM

|Metering Operator
|DEP
|REM
|REM

|Scheduler policy
|DEP
|DEP
|REM

|`ImageChangesInProgress` condition for Cluster Samples Operator
|DEP
|DEP
|DEP

|`MigrationInProgress` condition for Cluster Samples Operator
|DEP
|DEP
|DEP

|Use of `v1` without a group in `apiVersion` for {product-title} resources
|DEP
|REM
|REM

|Use of `dhclient` in {op-system}
|DEP
|REM
|REM

|Cluster Loader
|DEP
|DEP
|REM

|Bring your own {op-system-base} 7 compute machines
|DEP
|DEP
|REM

|`lastTriggeredImageID` field in the `BuildConfig` spec for Builds
|DEP
|REM
|REM

|Jenkins Operator
|DEP
|DEP
|REM

|HPA custom metrics adapter based on Prometheus
|REM
|REM
|REM

|vSphere 6.7 Update 2 or earlier
|GA
|DEP
|DEP

|Virtual hardware version 13
|GA
|DEP
|DEP

|VMware ESXi 6.7 Update 2 or earlier
|GA
|DEP
|DEP

|====

[id="ocp-4-10-deprecated-features"]
=== Deprecated features

[id="ocp-4-10-arch-deprecations"]
==== IBM POWER8, IBM z13 all models, LinuxONE Emperor, LinuxONE Rockhopper, and x86_64 v1 architectures will be deprecated

{op-system} functionality in IBM POWER8, IBM z13 all models, LinuxONE Emperor, LinuxONE Rockhopper, and x86_64 v1 CPU architectures will be deprecated in an upcoming release. Additional details for when support will discontinue for these architectures will be announced in a future release.

[NOTE]
====
AMD and Intel 64-bit architectures (x86-64-v2) will still be supported.
====

[id="ocp-4-10-docker-config-location-deprecation"]
==== Default Docker configuration location deprecation
Previously, `oc` commands that used a registry configuration would obtain credentials from the Docker configuration location, which was `~/.docker/config.json` by default. This has been deprecated and will be replaced by a Podman configuration location in a future version of {product-title}.

[id="ocp-4-10-empty-file-support-deprecation"]
==== Empty file and stdout support deprecation in oc registry login

Support for empty files using the `--registry-config` and `--to` flags in `oc registry login` has been deprecated. Support for `-` (standard output) has also been deprecated as an argument when using `oc registry login`. They will be removed in a future version of {product-title}.

[id="ocp-4-10-jenkins-non-sidecar-podtemplates-deprecation"]
==== Non-sidecar pod templates for Jenkins deprecation

* In {product-title} 4.10, the non-sidecar `maven` and `nodejs` pod templates for Jenkins are deprecated. These pod templates are planned for removal in a future release. Bug fixes and support are provided through the end of that future life cycle, after which no new feature enhancements are made. Instead, with this update, you can run Jenkins agents as sidecar containers. (link:https://issues.redhat.com/browse/JKNS-257[JKNS-257])

[id="ocp-4-10-removed-features"]
=== Removed features

{product-title} 4.10 removes the Jenkins Operator, which was a Technology Preview feature, from the *OperatorHub* page in the {product-title} web console interface. Bug fixes and support are no longer available.
+
Instead, you can continue to deploy Jenkins on {product-title} by using the templates provided by the Samples Operator. Alternatively, you can install the Jenkins Helm Chart from the Developer Catalog by using the *Helm* page in the *Developer* perspective of the web console.

[id="ocp-4-10-oc-commands-removed"]
==== OpenShift CLI (oc) commands removed

The following OpenShift CLI (`oc`) commands were removed with this release:

* `oc adm completion`
* `oc adm config`
* `oc adm options`

[id="ocp-4-10-removed-scheduler-policy"]
==== Scheduler policy removed

Support for configuring a scheduler policy has been removed with this release. Use a xref:../nodes/scheduling/nodes-scheduler-profiles.adoc#nodes-scheduler-profiles[scheduler profile] instead to control how pods are scheduled onto nodes.

[id="ocp-4-10-removed-feature-rhel-7-support"]
==== {op-system-base} 7 support for compute machines removed

Support for running {op-system-base-full} 7 compute machines in {product-title} has been removed. If you prefer using {op-system-base} compute machines, they must run on {op-system-base} 8.

You cannot upgrade {op-system-base} 7 compute machines to {op-system-base} 8. You must deploy new {op-system-base} 8 hosts, and the old {op-system-base} 7 hosts must be removed.

[id="ocp-4-10-bug-fixes"]
== Bug fixes

[discrete]
[id="ocp-4-10-api-server-auth-bug-fixes"]
==== API server and authentication

[discrete]
[id="ocp-4-10-bare-metal-hardware-bug-fixes"]
==== Bare Metal Hardware Provisioning

[discrete]
[id="ocp-4-10-builds-bug-fixes"]
==== Builds

* Before this update, if you created a build configuration containing an image change trigger in {product-title} 4.7.x or earlier, the image change trigger might trigger builds continuously.
+
This issue happened because, with the deprecation and removal of the `lastTriggeredImageID` field from the `BuildConfig` spec for Builds, the image change trigger controller stopped checking that field before instantiating builds. {product-title} 4.8 introduced new fields in the status that the image change trigger controller needed to check, but did not.
+
With this update, the image change trigger controller continuously checks the correct fields in the spec and status for the last triggered image ID. Now, it only triggers a build when necessary. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2004203[BZ#2004203])

* Before this update, image references in Builds needed to specify the Red Hat registry name explicitly. With this update, if an image reference does not contain the registry, the Build searches the Red Hat registries and other well-known registries to locate the image. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2011293[BZ#2011293])

[discrete]
[id="ocp-4-10-jenkins-bug-fixes"]
==== Jenkins

* Before this update, version 1.0.48 of the OpenShift Jenkins Sync Plugin introduced a `NullPointerException` error when Jenkins notified the plug-in of new jobs that were not associated with an OpenShift Jenkins Pipeline Build Strategy Build Config. Ultimately, this error was benign because there was no `BuildConfig` object to associate with the incoming Jenkins Job. Core Jenkins ignored the exception in our plug-in and moved on to the next listener. However, a long stack trace showed up in the Jenkins log that distracted users. With this update, the plug-in resolves the issue by making the proper checks to avoid this error and the subsequent stack trace. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2030692[BZ#2030692])

* Before this update, performance improvements in version 1.0.48 of the OpenShift Sync Jenkins plugin incorrectly specified the labels accepted for `ConfigMap` and `ImageStream` objects intended to map into the Jenkins Kubernetes plugin pod templates. As a result, the plug-in no longer imported pod templates from `ConfigMap` and `ImageStream` objects with a `jenkins-agent` label.
+
This update corrects the accepted label specification so that the plug-in imports pod templates from `ConfigMap` and `ImageStream` objects that have the `jenkins-agent` label. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2034839[2034839])

[discrete]
[id="ocp-4-10-cloud-compute-bug-fixes"]
==== Cloud Compute

* Previously, editing a machine specification on {rh-openstack-first} would cause {product-title} to attempt to delete and recreate the machine. As a result, this caused an unrecoverable loss of the node it was hosting. With this fix, any edits made to the machine specification after creation are ignored. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1962066[*BZ#1962066*])

* Previously, on clusters that run on {rh-openstack-first}, floating IP addresses were not reported for machine objects. As a result, certificate signing requests (CSRs) that the kubelet created were not accepted, preventing nodes from joining the cluster. All IP addresses are now reported for machine objects. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2022627[*BZ#2022627*])

* Previously, the check to ensure that the AWS machine was not updated before requeueing was removed. Consequently, problems arose when the AWS machine's virtual machine had been removed, but its object was still available. If this happens, the AWS machine would requeue in an infinite loop and could not be deleted or updated. This update restores the check that was used to ensure that the AWS machine was not updated before requeueing. As a result, machines no longer requeue if they have been updated. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2007802[*BZ#2007802*])

* Previously, modifying a selector changed the list of machines that a machine set observed. As a result, leaks could occur because the machine set lost track of machines it had already created. This update ensures that the selector is immutable once created. As a result, machine sets now lists the correct machines. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2005052[*BZ#2005052*])

* Previously, if a virtual machine template had snapshots, an incorrect disk size was picked due to an incorrect usage of the `linkedClone` operation. With this update, the default clone operation is changed to `fullClone` for all situations. `linkedClone` must now by specified by the user. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2001008[*BZ#2001008*])

* Previously, the custom resource definition (CRD) schema requirements did not allow numeric values. Consequently, marshaling errors occurred during upgrades. This update corrects the schema requirements to allow both string and numeric values. As a result, marshaling errors are no longer reported by the API server conversion. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1999425[*BZ#1999425*])

* Previously, if the Machine API Operator was moved, or the pods were deployed as a result of a name change, the `MachineNotYetDeleted` metric would reset for each monitored machine. This update changes the metric query to ignore the source pod label. As a result, the `MachineNotYetDeleted` metric now properly alerts in scenarios where the Machine API Operator pod has been renamed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1986237[*BZ#1986237*])

* Previously, egress IPs on vSphere were picked up by the vSphere cloud provider within the kubelet. These were unexpected by the certificate signing requests (CSR) approver. Consequently, nodes with egress IPs would not have their CSR renewals approved. This update allows the CSR approver to account for egress IPs. As a result, nodes with egress IPs on vSphere SDN clusters now continue to function and have valid CSR renewals. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1860774[*BZ#1860774*])

* Previously, worker nodes failed to start, and the installation program failed to generate URL images due to the broken path defaulting for the disk image and incompatible changes in the Google Cloud Platform (GCP) SDK. As a result, the machine controller was unable to create machines. This fix repairs the URL images by changing the base path in the GCP SDK. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2009111[*BZ#2009111*])

* Previously, the machine would freeze during the deletion process due to a lag in the vCenter's `powerOff` task. VMware showed the machine to be powered off, but {product-title} reported it to be powered on, which resulted in the machine freezing during the deletion process. This update improves the `powerOff` task handling on vSphere to be checked before the task to delete from the database is created, which prevents the machine from freezing during the deletion process. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2011668[*BZ#2011668*])

* After installing or updating {product-title}, the value of the metrics showed one pending CSR after the last CSR was reconciled. This resulted in the metrics reporting one pending CSR when there should be no pending CSRs. This fix ensures the pending CSR count is always valid post-approval by updating the metrics at the end of each reconcile loop. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2013528[*BZ#2013528*])

* Previously, AWS checked for credentials when the `cloud-provider` flag was set to empty string. The credentials were checked by making calls to the metadata service, even on non-AWS platforms. This caused latency in the ECR provider startup and AWS credential errors logged in all platforms, including non-AWS. This fix prevents the credentials check from making any requests to the metadata service to ensure that credential errors are no longer being logged. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2015515[*BZ#2015515*])

* Previously, the Cluster Autoscaler Unschedulable pods showed a severity warning alert that suggested it required developer intervention. This alert is informational and does not describe a problematic condition that requires developer intervention. This issue resulted in the Cluster Autosaver Unschedulable pods not meeting the warning alert criteria. This fix changes the Cluster Autosaver Unschedulable pods by lowering its severity. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2025230[*BZ#2025230*])

* Previously, the Machine API sometimes reconciled a machine before AWS had communicated VM creation across its API. As a result, AWS reported the VM does not exist and the Machine API considered it failed. With this release, the Machine API waits until the AWS API has synched before trying to mark the machine as provisioned. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2025767[*BZ#2025767*])

* Previously, a large volume of nodes created simultaneously on UPI clusters could lead to a large number of CSRs being generated. As a result, certificate renewals were not automated because the approver stops approving certificates when there are over 100 pending certificate requests. With this release, existing nodes are accounted for when calculating the approval cut-off and UPI clusters can now benefit from automated certificate renewal even with large scale refresh requests. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2028019[*BZ#2028019*])

* Previously, the generated list of instance types embedded in Machine API controllers was out of date. Some of these instance types were unknown and could not be annotated for scale-from-zero requirements. With this release, the generated list is updated to include support for newer instance types. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2040376[*BZ#2040376*])

* Previously, AWS Machine API controllers did not set the IOPS value for block devices other than the IO1 type, causing IOPS fields for GP3 block devices to be ignored. With this release, the IOPS is set on all supported block device types and users can set IOPS for block devices that are attached to the machine. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2040504[*BZ#2040504*])

[discrete]
[id="ocp-4-10-cloud-credential-operator-bug-fixes"]
==== Cloud Credential Operator

* Previously, when using the Cloud Credential Operator in manual mode on an Azure cluster, the `Upgradeable` status was not set to `False`. This behavior was different for other platforms. With this release, Azure clusters using the Cloud Credential Operator in manual mode have the `Upgradeable` status set to `False`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1976674[*BZ#1976674*])

* Previously, the now unnecessary `controller-manager-service` service resource that was created by the Cloud Credential Operator was still present. With this release, the Cluster Version Operator cleans it up.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1977319[*BZ#1977319*])

* Previously, changes to the log level setting for the Cloud Credential Operator in the `CredentialsRequest` custom resource were ignored. With this release, logging verbosity can be controlled by editing the `CredentialsRequest` custom resource.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1991770[*BZ#1991770*])

* Previously, the Cloud Credential Operator (CCO) pod restarted with a continuous error when AWS was the default secret annotator for {rh-openstack-first}. This update fixes the default setting for the CCO pod and prevents the CCO pod from crashing. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1996624[*BZ#1996624*])

[discrete]
[id="ocp-4-10-cluster-version-operator-bug-fixes"]
==== Cluster Version Operator

* Previously, a pod might fail to start due to an invalid mount request that was not a part of the manifest. With this update, the Cluster Version Operator (CVO) removes any volumes and volume mounts from in-cluster resources that are not included in the manifest. This allows pods to start successfully. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2002834[*BZ#2002834*])

* Previously, when monitoring certificates were rotated, the Cluster Version Operator (CVO) would log errors and monitoring would be unable to query metrics until the CVO pod was manually restarted. With this update, the CVO monitors the certificate files and automatically recreates the metrics connection whenever the certificate files change. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2027342[*BZ#2027342*])

[discrete]
[id="ocp-4-10-console-storage-bug-fixes"]
==== Console Storage Plug-in

* Previously, a loading prompt was not present while the persistent volumes (PVs) were being provisioned and the capacity was `0` TiB which created a confusing scenario. With this update, a loader is added for the loading state which provides details to the user if the PVs are still being provisioned or capacity is to be determined. It will also inform the user of any errors in the process. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1928285[*BZ#1928285*])

* Previously, the grammar was not correct in certain places and there were instances where translators were unable to interpret the context. This had a negative impact on readability. With this update, the grammar in various places is corrected, the storage classes for translators is itemized, and the overall readability is improved. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1961391[*BZ#1961391*])

* Previously, when pressing a pool inside the block pools page, the final `Ready` phase persisted after deletion. Consequently, the pool was in the `Ready` state even after deletion. This update redirects users to the *Pools* page and refreshes the pools after detention. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1981396[*BZ#1981396*])

[discrete]
[id="ocp-4-10-dns-bug-fixes"]
==== Domain Name System (DNS)

* Previously, the DNS Operator did not enable the `prometheus` pug-in in the server blocks for custom upstream resolvers. Consequently, CoreDNS did not report metrics for upstream resolvers and only reported metrics for the default server block. With this update, the DNS Operator was changed to enable the `prometheus` plug-in in all server blocks. CoreDNS now reports Prometheus metrics for custom upstream resolvers. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2020489[*BZ#2020489*])

* Previously, an upstream DNS that provided a response greater than 512 characters caused an application to fail. This occurred because it could not clone the repository from GitHub because to DNS could not be resolved. With this update, the `bufsize` for KNI CoreDNS is set to 521 to avoid name resolutions from GitHub. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1991067[*BZ#1991067*])

* When the DNS Operator reconciles its operands, the Operator gets the cluster DNS service object from the API to determine whether the Operator needs to create or update the service. If the service already exists, the Operator compares it with what the Operator expects to get to determine whether an update is needed. Kubernetes 1.22, on which {product-title} 4.9 is based, introduced a new `spec.internalTrafficPolicy` API field for services. The Operator leaves this field empty when it creates the service, but the API sets a default value for this field. The Operator was observing this default value and trying to update the field back to the empty value. This caused the Operator's update logic to continuously try to revert the default value that the API set for the service's internal traffic policy. With this fix, when comparing services to determine whether an update is required, the Operator now treats the empty value and default value for `spec.internalTrafficPolicy` as equal. As a result, the Operator no longer spuriously tries to update the cluster DNS service when the API sets a default value for the service's `spec.internalTrafficPolicy field`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2002461[*BZ#2002461*])

* Previously, the DNS Operator did not enable the cache plug-in for server blocks in the CoreDNS `Corefile` configuration map corresponding to entries in the `spec.servers` field of the `dnses.operator.openshift.io/default` object. As a result, CoreDNS did not cache responses from upstream resolvers that were configured using `spec.servers`. With this bug fix, the DNS Operator is changed to enable the cache plug-in for all server blocks, using the same parameters that the Operator already configured for the default server block. CoreDNS now caches responses from all upstream resolvers. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2006803[*BZ#2006803*])

[discrete]
[id="ocp-4-10-image-registry-bug-fixes"]
==== Image Registry

* Previously, the registry internally resolved `\docker.io` references into `\registry-1.docker.io` and used it to store credentials. As a result, credentials for `\docker.io` images could not be located. With this update, the `\registry-1.docker.io` hostname has been changed back to `\docker.io` when searching for credentials. As a result, the registry can correctly find credentials for `\docker.io images`. (https://bugzilla.redhat.com/show_bug.cgi?id=2024859[*BZ#2024859*])

* Previously, the image pruner job did not retry upon failure. As a result, a single failure could degrade the Image Registry Operator until the next time it ran. With this fix, the temporary problems with the pruner do not degrade the Image Registry Operator. (https://bugzilla.redhat.com/show_bug.cgi?id=2051692[*BZ#2051692*])

* Previously, the Image Registry Operator was modifying objects from the informer. As a result, these objects could be concurrently modified by the informer and cause race conditions. With this fix, controllers and informers have different copies of the object and do not have race conditions. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2028030[*BZ#2028030*])

* Previously, `TestAWSFinalizerDeleteS3Bucket` would fail because of an issue with the location of the configuration object in the Image Registry Operator. This update ensures that the configuration object is stored in the correct location. As a result, the Image Registry Operator no longer panics when running `TestAWSFinalizerDeleteS3Bucket`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2048443[*BZ#2048443*])

* Previously, error handling caused the `access denied` error to be output as `authentication required`. This bug resulted in incorrect error logs. Through Docker distribution error handling, the error output was changed from `authentication required` to `access denied`. Now the `access denied` error provides more precise error logs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1902456[*BZ#1902456*])

* Previously, the registry was immediately exiting on a shut down request. As a result, the router did not have time to discover that the registry pod was gone and could send requests to it. With this fix, when the pod is deleted it stays active for a few extra seconds to give other components time to discover its deletion. Now, the router does not send requests to non-existent pods during upgrades, which no longer leads to disruptions. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1972827[*BZ#1972827*])

* Previously, the registry proxied response from the first available mirrored registry. When a mirror registry was available but did not have the requested data, pull-through did not try to use other mirrors even if they contained the required data. With this fix, pull-through tries other mirror registries if the first mirror replied with `Not Found`. Now, pull-through can discover data if it exists on any mirror registry. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2008539[*BZ#2008539*])

[discrete]
[id="ocp-4-10-image-streams-bug-fixes"]
==== Image Streams

* Previously, the image policy admission plug-in did not recognize deployment configurations, notably that stateful sets could be updated. As a result, image stream references stayed unresolved in deployment configurations when the `resolve-names` annotation was used. Now, the plug-in is updated so that it resolves annotations in deployment configurations and stateful sets. As a result, image stream tags get resolved in created and edited deployment configurations. (https://bugzilla.redhat.com/show_bug.cgi?id=2000216[*BZ#2000216*])

* Previously, when global pull secrets were updated, existing API server pod pull secrets were not updated. Now, the mount point for the pull secret is changed from the `/var/lib/kubelet/config.json` file to the `/var/lib/kubelet` directory. As a result, the updated pull secret now appears in existing API server pods. (https://bugzilla.redhat.com/show_bug.cgi?id=1984592[*BZ#1984592*])

* Previously, the image admission plug-in did not check annotations inside deployment configuration templates. As a result, annotations inside deployment configuration templates could not be handled in replica controllers, and they were ignored. Now, the image admission plug-in analyzes the template of deployment configurations. With this fix, the image admission plug-in recognizes the annotations on the deployment configurations and on their templates. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2032589[*BZ#2032589*])

[discrete]
[id="ocp-4-10-installer-bug-fixes"]
==== Installer

* Previously, pre-flight checks did not account for {rh-openstack-first} resource utilization. As a result, those checks failed with an incorrect error message when utilization, rather than quota, impeded installation. Pre-flight checks now process both {rh-openstack} quota and utilization. The checks fail with correct error messages if the quota is sufficient but resources are not. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2001317[*BZ#2001317*])

* Before this update, the oVirt Driver could specify ReadOnlyMany (ROX) and ReadWriteMany (RWX) access modes when creating a PVC from a configuration file. This caused an error because the driver does not support shared disks and, as a result, could not support these access modes. With this update, the access mode has been limited to single node access. The system prevents any attempt to specify ROX or RWX when creating PVC and logs an error message. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1882983[*BZ#1882983*])

* Previously, disk uploads in the Terraform provider were not handled properly. As a result, the {product-title} installation program failed. With this update, disk upload handling has been fixed, and disk uploads succeed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917893[*BZ#1917893*])

* Previously, when installing a Microsoft Azure cluster with a special size, the installation program would check if the total number of virtual CPUs (vCPU) met the minimum resource requirement to deploy the cluster. Consequently, this could cause an install error. This update changes the check the installation program makes from the total number of vCPUs to the number of vCPUs available. As a result, a concise error message is given that lets the Operator know that the virtual machine size does not meet the minimum resource requirements. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2025788[*BZ#2025788*])

* Previously, RAM validation for {rh-openstack-first} checked for values using a wrong unit, and as a result the validation accepted flavors that did not meet minimum RAM requirements. With this fix, RAM validation now rejects flavors with insufficient RAM. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2009699[*BZ#2009699*])

* Previously, {product-title} master nodes were missing Ingress security group rules when they were schedulable and deployed on {rh-openstack}. As a result, {product-title} deployments on {rh-openstack} failed for compact clusters with no dedicated workers. This fix adds Ingress security group rules on {rh-openstack-first} when masters are schedulable. Now, you can deploy compact three-node clusters on {rh-openstack}. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1955544[*BZ#1955544*])

* Previously, if you specified an invalid AWS region, the installation program continued to try and validate availability zones. This caused the installation program to become unresponsive for 60 minutes before timing out. The installation program now verifies the AWS region and service endpoints before availability zones, which reduces the amount of time the installer takes to report the error. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2019977[*BZ#2019977*])

* Previously, you could not install a cluster to VMware vSphere if the vCenter host name began with a number. The installation program has been updated and no longer treats this type of host name as invalid. Now, a cluster deploys successfully when the vCenter host name begins with a number.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2021607[*BZ#2021607*])

* Previously, if you specified a custom disk instance type when deploying a cluster on Microsoft Azure, the cluster might not deploy. This occurred because the installation program incorrectly determined that the minimum resource requirements had been met. The installation program has been updated, and now reports an error when the number of vcpus available for the instance type in the selected region does not meet the minimum resource requirements. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2025788[*BZ#2025788*])

* Previously, if you defined custom IAM roles when deploying an AWS cluster, you might have to manually remove bootstrap instance profiles after uninstalling the cluster. Intermittently, the installer did not remove bootstrap instance profiles. The installation program has been updated, and all machine instance profiles are removed when the cluster is uninstalled.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2028695[*BZ#2028695*])

* Previously, the default provisioningIP value was different when the host bits were set in the provisioning network CIDR. This resulted in a different value for the provisioningIP than expected. This difference caused a conflict with the other IP addresses on the provisioning network. This fix adds a validation to ensure that the ProvisioningNetworkCIDR does not have host bits set. As a result, if the ProvisioningNetworkCIDR has the host bits set, the installation program will stop and report the validation error. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2006291[*BZ#2006291*])

* Previously, the BMC driver IPMI was not supported for a secure UEFI boot. This resulted in an unsuccessful boot. This fix adds a validation check to ensure that `UEFISecureBoot` mode is not used with bare-metal drivers. As a result, a secure UEFI boot is successful. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2011893[*BZ#2011893*])

* With this update, the 4.8 UPI template is updated from version 3.1.0 to 3.2.0 to match the Ignition version. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1949672[*BZ#1949672*])

* Previously, when asked to mirror the contents of the base registry, the {product-title} installation program would exit with a validation error, citing incorrect `install-config` file values for `imageContentSources`. With this update, the installation program now allows `imageContentSources` values to specify base registry names and the installation program no longer exits when specifying a base registry name. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1960378[*BZ#1960378*])

* Previously, the UPI ARM templates were attaching an SSH key to the virtual machine (VM) instances created. As a result, the creation of the VMs fails when the SSH key provided by the user is the `ed25519` type. With this update, the creation of the VMs succeeds regardless of the type of the SSH key provided by the user. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1968364[*BZ#1968364*])

* After successfully creating a `aws_vpc_dhcp_options_association` resource, AWS might still report that the resource does not exist. Consequently, the AWS Terraform provider fails the installation. With this update, you can retry the query of the `aws_vpc_dhcp_options_association` resource for a period of time after creation until AWS reports that the resource exists. As a result, installations succeed despite AWS reporting that the `aws_vpc_dhcp_options_association` resource does not exist. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2032521[*BZ#2032521*])

* Previously, when installing {product-title} on AWS with local zones enabled, the installation program could create some resources on a local zone rather than an availability zone. This caused the installation program to fail because load balancers cannot run on local zones. With this fix, the installation program ignores local zones and only considers availability zones when installing cluster components. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1997059[*BZ#1997059*])

* Previously, terraform could attempt to upload the bootstrap ignition configuration file to Azure before it had finished creating the configuration file. If the upload started before the local file was created, the installation would fail. With this fix, terraform uploads the ignition configuration file directly to Azure rather than creating a local copy first. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2004313[*BZ#2004313*])

* Previously, a race condition could occur if the `cluster-bootstrap` and Cluster Version Operator components attempted to write a manifest for the same resource to the Kubernetes API at the same time. This could result in the `Authentication` resource being overwritten by a default copy, which removed any customizations made to that resource. With this fix, the Cluster Version Operator has been blocked from overwriting the manifests that come from the installation program. This prevents any user-specified customizations to the `Authentication` resource from being overwritten. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2008119[*BZ#2008119*])

* Previously, when installing {product-title} on AWS, the installation program created the bootstrap machine using the `m5.large` instance type. This caused the installation to fail in regions where that instance type is not available. With this fix, the bootstrap machine uses the same instance type as the control plane machines. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2016955[*BZ#2016955*])

* Previously, when installing {product-title} on AWS, the installation program did not recognize EC2 G and VT instances and defaulted them to X instances. This caused incorrect instance quotas to be applied to these instances. With this fix, the installation program recognizes EC2 G and VT instances and applies the correct instance quotas. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2017874[*BZ#2017874*])

[discrete]
[id="ocp-4-10-kube-api-server-bug-fixes"]
==== Kubernetes API server

[discrete]
[id="ocp-4-10-kube-scheduler-bug-fixes"]
==== Kubernetes Scheduler

* Before this update, upgrading to the current release did not set the correct weights for the `TaintandToleration`, `NodeAffinity`, and `InterPodAffinity` parameters. This update resolves the issue so that upgrading correctly sets the weights for `TaintandToleration` to `3`, `NodeAffinity` to `2`, and `InterPodAffinity` to `2`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2039414[*BZ#2039414*])

* In {product-title} 4.10, code for serving insecure metrics is removed from the `kube-scheduler` code base. Now, metrics are served only through a secure server. Bug fixes and support are provided through the end of a future life cycle. After which, no new feature enhancements are made. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1889488[*BZ#1889488*])

[discrete]
[id="ocp-4-10-machine-config-operator-bug-fixes"]
==== Machine Config Operator

* Previously, when updating SSH keys, the Machine Config Operator (MCO) changed the owner and group of the `authorized_keys` file to `root`. This update ensures that the MCO preserves `core` as the owner and group when it updates the `authorized_keys` file. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1956739[BZ#1956739])

* Previously, a warning message sent by the `clone_slave_connection` function was incorrectly stored in a `new_uuid` variable, which is intended to store only the connection's UUID. As a result, `nmcli` commands that include the `new_uuid` variable were failing due to the incorrect value being stored in the `new_uuid` variable. With this fix, the `clone_slave_connection` function warning message is redirected to `stderr`. Now, `nmcli` commands that reference the `new_uuid` variable do not fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2022646[*BZ#2022646*])

* Previously, an old version of the Kubernetes client library was present in the `baremetal-runtimecfg` project. When a Virtual IP (VIP) failed, the client connections might not be closed in a timely manner. This could result in long delays for monitor containers that rely on talking to the API. This fix updates the client library. Now, connections are closed as expected on VIP failovers and the monitor containers do not hang for an excessive period of time. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1995021[*BZ#1995021*])

* Before this update, the Machine Config Operator (MCO) stored pending configuration changes to disk before it applied them to {op-system-first}. If a power loss interrupted the MCO from applying the configuration, it treated the configuration as applied and did not validate the changes. If this configuration contained invalid changes, applying them failed. With this update, the MCO saves a configuration to disk only after being applied. This way, if the power is lost while the MCO is applying the configuration, it reapplies the configuration after it restarts. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1916169[*BZ#1916169*])

* Before this update, when you used the Machine Config Operator (MCO) to create or update an SSH key, it set the owner and group of the `authorized_keys` file to `root`. This update resolves the issue. When the MCO creates or updates the `authorized_keys` file, it correctly sets or preserves `core` as the owner and group of the file. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1956739[*BZ#1956739*])

* Previously, in clusters that use Stateless Address AutoConfiguration (SLAAC), the Ironic `addr-gen-mode` parameter was not being persisted to the OVNKubernetes bridge. As a result, the IPv6 addresses could change when the bridge was created. This broke the cluster because node IP changes are unsupported. This fix persists the `addr-gen-mode` parameter when creating the bridge. The IP address is now consistent throughout the deployment process. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1990625[*BZ#1990625*])

* Previously, if a machine config included a compressed file with the `spec.config.storage.files.contents.compression` parameter set to `gzip`, the Machine Config Daemon (MCD) incorrectly wrote the compressed file to disk without extracting it. With this fix, the MCD now extracts a compressed file when the compression parameter is set to `gzip`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1970218[*BZ#1970218*])

* Previously, `systemd` units were cleaned up only when completely removed. As a result, `systemd` units could not be unmasked by using a machine config because the masks were not being removed unless the `systemd` unit was completely removed. With this fix, when you configure a `systemd` unit as `mask: true` in a machine config, any existing masks are removed. As a result, `systemd` units can now be unmasked. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1966445[*BZ#1966445*])

[discrete]
[id="ocp-4-10-management-console-bug-fixes"]
==== Management Console

* Previously, the *OperatorHub* category and card links did not include valid `href` attributes. Consequently, the *OperatorHub* category and card links could not be opened in a new tab. This update adds valid `href` attributes to the *OperatorHub* category and card links. As a result, the *OperatorHub* and its card links can be opened in new tabs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2013127[*BZ#2013127*])

* Previously, on the *Operand Details* page, a special case was created where the conditions table for the `status.conditions` property always rendered before all other tables. Consequently, the `status.conditions` table did not follow the ordering rules of descriptors, which caused unexpected behavior when users tried to change the order of the tables. This update removes the special case for `status.conditions` and only defaults to rendering it first if no descriptor is defined for that property. As a result, the table for `status.condition` is rendered according to descriptor ordering rules when a descriptor is defined on that property. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2014488[*BZ#2014488*])

* Previously, the *Resource Details* page metrics tab was exceeding the cluster-scoped Thanos endpoint. Consequently, users without authorization for this endpoint would receive a `401` response for all queries. With this update, the Thanos tenancy endpoints are updated, and redundant namespace query arguments are removed. As a result, users with the correct role-based access control (RBAC) permissions can now see data in the metrics tab of the *Resource Details* page. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2015806[*BZ#2015806*])

* Previously, when an Operator added an API to an existing API group, it did not trigger API discovery. Consequently, new APIs were not seen by the front end until the page was refreshed. This update makes APIs added by Operators viewable by the front end without a page refresh. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1815189[*BZ#1815189*])

* Previously, in the {console-redhat-com} for {rh-openstack-first}, the control plane was not translated into simplified Chinese. As a result, naming differed from {product-title} documentation. This update fixes the translation issue in the {console-redhat-com}. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1982063[*BZ#1982063*])

* Previously, filtering of virtual tables in the {console-redhat-com} was broken. Consequently, all of the available `nodes` would not appear following a search. This update includes new virtual table logic that fixes the filtering issue in the {console-redhat-com}. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1990255[*BZ#1990255*])

[discrete]
[id="ocp-4-10-networking-bug-fixes"]
==== Networking

* When using the OVN-Kubernetes network provider in {product-title} versions prior to 4.8, the node routing table was used for routing decisions. In newer versions of {product-title}, the host routing table is bypassed. In this release, you can now specify whether you want to use or bypass the host kernel networking stack for traffic routing decisions. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1996108[*BZ#1996108*])

* Previously, when Kuryr was used in a restricted installation with proxy, the Cluster Network Operator was not enforcing usage of the proxy to allow communication with the {rh-openstack-first} API. Consequently, cluster installation did not progress. With this update, the Cluster Network Operator can communicate with the {rh-openstack} API through the proxy. As a result, installation now succeeds. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1985486[*BZ#1985486*])

* Previously, pods that used secondary interfaces with IP addresses provided by the Whereabouts Container Network Interface (CNI) plug-in might get stuck in the `ContainerCreating` state because of IP address exhaustion. Now, Whereabouts properly accounts for released IP addresses from cluster events, such as reboots, that previously were not tracked. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1914053[BZ#1914053])

* Previously, when using the OpenShift SDN cluster network provider, idled services used an increasing amount of CPU to un-idle services. In this release, the idling code for kube-proxy is optimized to reduce CPU utilizing for service idling. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1966521[BZ#1966521])

* Previously, when using the OVN-Kubernetes cluster network provider, the presence of any unknown field in an internal configuration map could cause the OVN-Kubernetes pods to fail to start during a cluster upgrade. Now the presence of unknown fields causes a warning, rather than a failure. As a result, the OVN-Kubernetes pods now successfully start during a cluster upgrade. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1988440[BZ#1988440])

[discrete]
[id="ocp-4-10-node-bug-fixes"]
==== Node

* Previously, when logging into a worker node, messages might appear indicating a `systemd-coredump` services failure. This was due to the unnecessary inclusion of the `system-systemd` namespace for containers. A filter now prevents this namespace from impacting the workflow. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1978528[*BZ#1978528*])

* Previously, when clusters were restarted, the status of terminated pods might have been reset to `Running`, which would result in an error. This has been corrected and now all terminated pods remain terminated and active pods reflect their correct status. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1997478[*BZ#1997478*])

* Previously, certain stop signals were ignored in {product-title}, causing services in the container to continue running. With an update to the signal parsing library, all stop signals are now respected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2000877[*BZ#2000877*])

* Previously, pod namespaces managed by CRI-O, for example network, IPC, and UTS, were not unmounted when the pod was removed. This resulted in leakage, driving the Open vSwitch CPU to 100%, which caused pod latency and other performance impacts. This has been resolved and pod namespaces are unmounted when removed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2003193[*BZ#2003193*])

[discrete]
[id="ocp-4-10-openshift-cli-bug-fixes"]
==== OpenShift CLI (oc)

* Previously, some minor requests did not have the user agent string set correctly, so the default Go user agent string was used instead for `oc`. The user agent string is now set correctly for all mirror requests, and the expected `oc` user agent string is now sent to registries. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1987257[*BZ#1987257*])

* Previously, `oc debug` assumed that it was always targeting Linux-based containers by trying to run a Bash shell, and if Bash was not present in the container, it attempted to debug as a Windows container. The `oc debug` command now uses pod selectors to determine the operating system of the containers and now works properly on both Linux and Windows-based containers. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1990014[*BZ#1990014*])

* Previously, the `--dry-run` flag was not working properly for several `oc set` subcommands, so `--dry-run=server` was performing updates to resources rather than performing a dry run. The `--dry-run` flags are now working properly to perform dry runs on the `oc set` subcommands. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2035393[*BZ#2035393*])

[discrete]
[id="ocp-4-10-container-bug-fixes"]
==== OpenShift containers

* Previously, a container using SELinux could not read `/var/log/containers` log files due to a missing policy. This update makes all log files in `/var/log` accessible including those accessed through symlink. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2005997[*BZ#2005997*])

[discrete]
[id="ocp-4-10-openshift-controller-manage-bug-fixes"]
==== OpenShift Controller Manager

* Previously, the `openshift_apps_deploymentconfigs_last_failed_rollout_time` metric improperly set the `namespace` label as the value of the `exported_namespace` label. The `openshift_apps_deploymentconfigs_last_failed_rollout_time` metric now has the correct `namespace` label set. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2012770[*BZ#2012770*])

[discrete]
[id="ocp-4-10-olm-bug-fixes"]
==== Operator Lifecycle Manager (OLM)

* Before this update, default catalog sources for the `marketplace-operator` did not tolerate tainted nodes and the `CatalogSource` pod would only have the default settings for tolerations, `nodeSelector`, and `priorityClassName`. With this update, the `CatalogSource` specification now includes the optional `spec.grpcPodConfig` field that can override tolerations, `nodeSelector`, and `priorityClassName` for the pod. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1927478[*BZ#1927478*])

* Before this update, the `csv_suceeded` metric would be lost when the OLM Operator was restarted. With this update, the `csv_succeeded` metric is emitted at the beginning of the OLM Operator's startup logic. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1927478[*BZ#1927478*])

* Before this update, the `oc adm catalog mirror` command did not set minimum and maximum values for the `--max-icsp-size` flag. As a result, the field accepted values that were less than zero or were too large. With this update, values are limited to sizes greater than zero and less than 250001. Values outside of this range fail validation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1972962[*BZ#1972962*])

* Before this update, bundled images did not contain the related images needed for Operator deployment in file-based catalogs. As a result, images were not mirrored to disconnected clusters unless specified in the `relatedImages` field of the ClusterServiceVersion (CSV). With this update, the `opm render` command adds the CSV Operator images to the `relatedImages` file when the file-based catalog bundle image is rendered. The images necessary for Operator deployment are now mirrored to disconnected clusters even if they are not listed in the `relatedImages` field of the CSV. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2002075[*BZ#2002075*])

* Before this update, it could take up to 15 minutes for Operators to perform `skipRange` updates. This was a known issue that could be resolved if cluster administrators deleted the `catalog-operator` pod in the `openshift-operator-lifecycle-manager` namespace. This caused the pod to be automatically recreated and triggered the `skipRange` upgrade. With this update, obsolete API calls have been fixed in Operator Lifecycle Manager (OLM), and `skipRange` updates trigger immediately. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2002276[*BZ#2002276*])

* Occasionally, update events on clusters would happen at the same time that Operator Lifecycle Manager (OLM) modified an object from the lister cache. This caused concurrent map writes. This fix updates OLM so it no longer modifies objects retrieved from the lister cache. Instead, OLM modifies a copy of the object where applicable. As a result, OLM no longer experiences concurrent map writes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2003164[*BZ#2003164*])

* Previously, Operator Lifecycle Manager (OLM) could not establish gRPC connections to catalog source pods that were only reachable through a proxy. If a catalog source pod was behind a proxy, OLM could not connect to the proxy and the hosted Operator content was unavailable for installation. This bug fix introduces a `GRPC_PROXY` environment variable that defines a proxy that OLM uses to establish connections to gRPC catalog sources. As a result, OLM can now be configured to use a proxy when connecting to gRPC catalog sources. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2011927[*BZ#2011927*])

* Previously, skipped bundles were not verified to be members of the same package. Bundles could skip across packages, which broke upgrade chains. This bug fix adds validation to ensure skipped bundles are in the same package. As a result, no bundle can skip bundles in another package, and upgrade graphs no longer break across packages. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2017327[*BZ#2017327*])

* In the `CatalogSource` object, the `RegistryServiceStatus` field stores service information that is used to generate an address that Operator Lifecycle Manager (OLM) relies on to establish a connection with the associated pod. If the `RegistryServiceStatus` field is not nil and is missing the namespace, name, and port information for its service, OLM is unable to recover until the associated pod has an invalid image or spec. With this bug fix, when reconciling a catalog source, OLM now ensures that the `RegistryServiceStatus` field of the `CatalogSource` object is valid and updates its status to reflect the change. Additionally, this address is stored within the status of the catalog source in the `status.GRPCConnectionState.Address` field. If the address changes, OLM updates this field to reflect the new address. As a result, the `.status.connectionState.address` field of a catalog source should no longer be nil. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2026343[*BZ#2026343*])

[discrete]
[id="ocp-4-10-openshift-api-server-bug-fixes"]
==== OpenShift API server

[discrete]
[id="ocp-4-10-openshift-update-service-bug-fixes"]
==== OpenShift Update Service

[discrete]
[id="ocp-4-10-rhcos-bug-fixes"]
==== {op-system-first}

* Previously, when the {op-system} live ISO added a UEFI boot entry for itself, it assumed the existing UEFI boot entry IDs were consecutive, thereby causing the live ISO to fail in the UEFI firmware when booting on systems with non-consecutive boot entry IDS. With this fix, the {op-system} live ISO no longer adds a UEFI boot entry for itself and the ISO boots successfully. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2006690[*BZ#2006690*])

* To help you determine whether a user-provided image was already booted, information has been added on the terminal console describing when the machine was provisioned through Ignition and whether a user Ignition configuration was provided. This allows you to verify that Ignition ran when you expected it to. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2016004[*BZ#2016004*])

* Previously, when reusing an existing statically keyed LUKS volume during provisioning, the encryption key was not correctly written to disk and Ignition would fail with a "missing persisted keyfile" error. With this fix, Ignition now correctly writes keys for reused LUKS volumes so that existing statically keyed LUKS volumes can be reused during provisioning. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2043296[*BZ#2043296*])

* Previously, `ostree-finalize-staged.service` failed while upgrading a {op-system-first} node to 4.6.17. To fix this, the sysroot code now ignores any irregular or non-symlink files in `/etc`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1945274[*BZ#1945274*])

* Previously, `initramfs` files were missing udev rules for by-id symlinks of attached SCSI devices. Because of this, Ignition configuration files that referenced these symlinks would result in a failed boot of the installed system. With this update, the `63-scsi-sg3_symlink.rules` for SCSI rules are added in dracut. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1990506[*BZ#1990506*])

* Previously, on bare-metal machines, a race condition occurred between `system-rfkill.service` and `ostree-remount.service`. Consequently, the `ostree-remount` service failed and the node operating system froze during the boot process. With this update, the `/sysroot/` directory is now read-only. As a result, the issue no longer occurs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1992618[*BZ#1992618*])

* Previously, {op-system-first} live ISO boots added a UEFI boot entry, prompting a reboot on systems with a TPM. With this update, the {op-system} live ISO no longer adds a UEFI boot entry so the ISO does not initiate a reboot after first boot. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2004449[*BZ#2004449*])

[discrete]
[id="ocp-4-10-routing-bug-fixes"]
==== Routing

* Previously, if the cluster administrator provided a default ingress certificate that was missing the newline character for the last line, the {product-title} router would write out a corrupt PEM file for HAProxy. Now, it writes out a valid PEM file even if the input is missing a newline character. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1894431[*BZ#1894431*])

* Previously, a route created where the combined name and namespace for the DNS segment was greater than 63 characters long would be rejected. This expected behavior could cause problems integrating with upgraded versions of {product-title}. Now, an annotation allows non-conformant DNS hostnames. With `AllowNonDNSCompliantHostAnnotation` set to `true`, the non-conformant DNS hostname, or one longer than 63 characters, is allowed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1964112[*BZ#1964112*])

* Previously, the Cluster Ingress Operator would not create wildcard DNS records for Ingress Controllers when the cluster's `ControlPlaneTopology` was set to `External`. In Hypershift clusters where the `ControlPlaneTopology` was set to `External` and the Platform was AWS, the Cluster Ingress Operator never became available. This updates limits the disabling of DNS updates when the `ControlPlaneTopology` is `External` and the platform is IBM Cloud. As a result, wildcard DNS entries are created for Hypershift clusters running on AWS. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2011972[*BZ#2011972*])

* Previously, the cluster ingress router was blocked from working because the Ingress Operator failed to configure a wildcard DNS record for the cluster ingress router on Azure Stack Hub IPI. With this fix, the Ingress Operator now uses the configured ARM endpoint to configure DNS on Azure Stack Hub IPI. As a result, the cluster ingress router now works properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2032566[*BZ#2032566*])

* Previously, the cluster-wide prox configuration could not accept IPv6 addresses for the `noProxy` setting. Consequently, it was impossible to install a cluster whose configuration was having `noProxy` with IPv6 addresses. With this update, the Cluster Network Operator is now able to parse IPv6 addresses for the `noProxy` setting of the cluster-wide proxy resource. As a result, it is now possible to exclude IPv6 addresses for the `noProxy` setting. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1939435[*BZ#1939435*])

* Before {product-title} 4.8, the IngressController API did not have any subfields under the `status.endpointPublishingStrategy.hostNetwork` and `status.endpointPublishingStrategy.nodePort` fields. These fields could be null even if the `spec.endpointPublishingStrategy.type` was set to `HostNetwork` or `NodePortService`. In {product-title} 4.8, the `status.endpointPublishingStrategy.hostNetwork.protocol` and `status.endpointPublishingStrategy.nodePort.protocol` subfields were added, and the Ingress Operator set default values for these subfields when the Operator admitted or re-admitted an IngressController that specified the "HostNetwork" or "NodePortService" strategy type. With this bug, however, the Operator ignored updates to these spec fields, and updating `spec.endpointPublishingStrategy.hostNetwork.protocol` or `spec.endpointPublishingStrategy.nodePort.protocol` to `PROXY` to enable proxy protocol on an existing IngressController had no effect. To work around this issue, it was necessary to delete and recreate the IngressController to enable PROXY protocol. With this update, the Ingress Operator is changed so that it correctly updates the status fields when `status.endpointPublishingStrategy.hostNetwork` and `status.endpointPublishingStrategy.nodePort` are null and when the IngressController spec fields specify proxy protocol with the `HostNetwork` or `NodePortService` endpoint publishing strategy type. As a result, setting `spec.endpointPublishingStrategy.hostNetwork.protocol` or `spec.endpointPublishingStrategy.nodePort.protocol` to `PROXY` now takes proper effect on upgraded clusters. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1997226[*BZ#1997226*])

[discrete]
[id="ocp-4-10-samples-bug-fixes"]
==== Samples

* Before this update, if the Cluster Samples Operator encountered an `APIServerConflictError` error, it reported `sample-operator` as having `Degraded status` until it recovered. Momentary errors of this type were not unusual during upgrades but caused undue concern for administrators monitoring the Operator status. With this update, if the Operator encounters a momentary error, it no longer indicates `openshift-samples` as having `Degraded status` and tries again to connect to the API server. Momentary shifts to `Degraded status` no longer occur. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1993840[BZ#1993840])

* Before this update, various allowed and blocked registry configuration options in the cluster image configuration might prevent the Cluster Samples Operator from creating image streams. As a result, the samples operator might mark itself as degraded, which impacted the general {product-title} install and upgrade status.
+
In various circumstances, the management state of the Cluster Samples Operator can make the transition to `Removed`. With this update, these circumstances now include when the image controller configuration parameters prevent the creation of image streams by using either the default image registry or the image registry specified by the `samplesRegistry` setting. The Operator status now also indicates that the cluster image configuration is preventing the creation of the sample image streams. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2002368[BZ#2002368])


[discrete]
[id="ocp-4-10-storage-bug-fixes"]
==== Storage

* Previously, the Local Storage Operator (LSO) took a long time to delete orphaned persistent volumes (PVs) due to the accumulation of a 10-second delay. With this update, the LSO does not use the 10-second delay, PVs are deleted promptly, and local disks are made available for new persistent volume claims sooner. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2001605[*BZ#2001605*])

* Previously, Manila error handling would degrade the Manila Operator, and the cluster. Errors are now treated as non-fatal so that the Manila Operator is disabled, rather than degrading the cluster. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2001620[*BZ#2001620*])

* In slower cloud environments, such as when using Cinder, the cluster might become degraded. Now, {product-title} accommodates slower environments so that the cluster does not become degraded. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2027685[*BZ#2027685*])

[discrete]
[id="ocp-4-10-web-console-admin-perspective-bug-fixes"]
==== Web console (Administrator perspective)

[discrete]
[id="ocp-4-10-web-console-developer-perspective-bug-fixes"]
==== Web console (Developer perspective)

* Before this update, resources in the *Developer* perspective of the web console had invalid links to details about that resource. This update resolves the issue. It creates valid links so that users can access resource details. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2000651[BZ#2000651])

* Before this update, you could only specify a subject in the `SinkBinding` form by label, not by name. With this update, you can use a drop-down list to select whether to specify a subject by name or label. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2002266[BZ#2002266])

* Before this update, the web terminal icon was available in the web console's banner head only if you installed the Web Terminal Operator in the `openShift-operators` namespace. With this update, the terminal icon is available regardless of the namespace where you install the Web Terminal Operator. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2006329[2006329])

* Before this update, the service binding connector did not appear in topology if you used a `resource` property rather than a `kind` property to define a `ServiceBinding` custom resource (CR). This update resolves the issue by reading the CR's `resource` property to display the connector on the topology. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2013545[BZ#2013545])

* Before this update, the name input fields used a complex and recursive regular expression to validate user inputs. This regular expression made name detection very slow and often caused errors. This update resolves the issue by optimizing the regular expression and avoiding recursive matching. Now, name detection is fast and does not cause errors. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2014497[BZ#2014497])

* Before this update, feature flag gating was missing from some extensions contributed by the knative plug-in. Although this issue did not affect what was displayed, these extensions ran unnecessarily, even if the serverless operator was not installed. This update resolves the issue by adding feature flag gating to the extensions where it was missing. Now, the extensions do not run unnecessarily. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2016438[BZ#2016438])

* Before this update, if you repeatedly clicked links to get details for resources such as custom resource definitions or pods and the application encountered multiple code reference errors, it crashed and displayed a `t is not a function` error. This update resolves the issue. When an error occurs, the application resolves a code reference and stores the resolution state so that it can correctly handle additional errors. The application no longer crashes when code reference errors occur. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2017130[BZ#2017130])

* Before this update, users with restricted access could not access their config map in a shared namespace to save their user settings on a cluster and load them in another browser or machine. As a result, user preferences such as pinned navigation items were only saved in the local browser storage and not shared between multiple browsers. This update resolves the issue: The web console Operator automatically creates RBAC rules so that each user can save these settings to a config map in a shared namespace and more easily switch between browsers. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2018234[BZ#2018234])

* Before this update, creating connections between virtual machines (VMs) in the Topology view failed because it relied on a method that did not support custom resource definition (CRDs). This update resolves the issue by adding support for CRDs. Now you can create connections between VMs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2020904[BZ#2020904])

[id="ocp-4-10-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_
* *DEP*: _Deprecated_

//TODO: remove anything that has been GA since 4.7?

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.8 |OCP 4.9 | OCP 4.10

|Precision Time Protocol (PTP)
|TP
|TP
|

|`oc` CLI plug-ins
|GA
|GA
|GA

|CSI Volumes in OpenShift Builds
|-
|-
|TP

|Service Binding
|TP
|TP
|

|Raw Block with Cinder
|GA
|GA
|GA

|CSI volume expansion
|TP
|TP
|

|CSI Azure Disk Driver Operator
|TP
|TP
|

|CSI Azure Stack Hub Driver Operator
|-
|GA
|GA

|CSI GCP PD Driver Operator
|GA
|GA
|GA

|CSI OpenStack Cinder Driver Operator
|TP
|TP
|

|CSI AWS EBS Driver Operator
|TP
|GA
|GA

|CSI AWS EFS Driver Operator
|-
|TP
|

|CSI automatic migration
|TP
|TP
|

|CSI inline ephemeral volumes
|TP
|TP
|TP

|CSI vSphere Driver Operator
|TP
|TP
|

|Shared Resource CSI Driver
|-
|-
|TP

|Automatic device discovery and provisioning with Local Storage Operator
|TP
|TP
|

|OpenShift Pipelines
|GA
|GA
|GA

|OpenShift GitOps
|GA
|GA
|GA

|OpenShift sandboxed containers
|TP
|TP
|GA

|Vertical Pod Autoscaler
|GA
|GA
|GA

|Cron jobs
|GA
|GA
|GA

|PodDisruptionBudget
|GA
|GA
|GA

|Adding kernel modules to nodes with kvc
|TP
|TP
|

|Egress router CNI plug-in
|GA
|GA
|GA

|Scheduler profiles
|TP
|GA
|GA

|Non-preempting priority classes
|TP
|TP
|

|Kubernetes NMState Operator
|TP
|TP
|-

|Assisted Installer
|TP
|TP
|

|AWS Security Token Service (STS)
|GA
|GA
|GA

|Kdump
|TP
|TP
|

|OpenShift Serverless
|GA
|GA
|GA

|Serverless functions
|TP
|TP
|

|Data Plane Development Kit (DPDK) support
|TP
|GA
|GA

|Memory Manager feature
|-
|TP
|

|CNI VRF plug-in
|TP
|GA
|GA

|Cluster Cloud Controller Manager Operator
|-
|GA
|GA

|Cloud controller manager for Alibaba Cloud
|-
|-
|TP

|Cloud controller manager for Amazon Web Services
|-
|TP
|TP

|Cloud controller manager for Google Cloud Platform
|-
|-
|TP

|Cloud controller manager for IBM Cloud
|-
|-
|GA

|Cloud controller manager for Microsoft Azure
|-
|TP
|TP

|Cloud controller manager for Microsoft Azure Stack Hub
|-
|GA
|GA

|Cloud controller manager for {rh-openstack-first}
|-
|TP
|TP

|Cloud controller manager for VMware vSphere
|-
|-
|TP

|Driver Toolkit
|TP
|TP
|

|Special Resource Operator (SRO)
|-
|TP
|

|Node Health Check Operator
|-
|TP
|

|Pod-level bonding for secondary networks
|-
|-
|TP

|Hyperthreading-aware CPU manager policy
|-
|-
|TP

|Dynamic Plug-ins
|-
|-
|TP

|Multicluster console
|-
|-
|TP

|Hybrid Helm Operator
|-
|-
|TP

|====

[id="ocp-4-10-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.8 and beyond! This needs some SME/QE review before being updated for 4.9.
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to 4.8, you can either revoke or continue to allow unauthenticated access. It is recommended to revoke unauthenticated access unless there is a specific need for it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* The assignment of egress IP addresses to control plane nodes with the egress IP feature is not supported on a cluster provisioned on Amazon Web Services (AWS). (link:https://bugzilla.redhat.com/show_bug.cgi?id=2039656[*BZ#2039656*])

* Previously, there was a race condition between {rh-openstack-first} credentials secret creation and `kube-controller-manager` startup. As a result, {rh-openstack-first} cloud provider would not be configured with {rh-openstack} credentials and would break support when creating Octavia load balancers for `LoadBalancer` services. To work around this, you must restart the `kube-controller-manager` pods by deleting the pods manually from the manifests. When you use the workaround, the `kube-controller-manager` pods restart and {rh-openstack} credentials are properly configured. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2004542[*BZ#2004542*])

* The ability to delete operands from the web console using the `delete all operands` option is currently disabled. It will be re-enabled in a future version of {product-title}. For more information, see link:https://bugzilla.redhat.com/show_bug.cgi?id=2012120[BZ#2012120] and link:https://bugzilla.redhat.com/show_bug.cgi?id=2012971[BZ#2012971].

* This release contains a known issue with Jenkins. If you customize the hostname and certificate of the OpenShift OAuth route, Jenkins no longer trusts the OAuth server endpoint. As a result, users cannot log in to the Jenkins console if they rely on the OpenShift OAuth integration to manage identity and access.
+
Workaround: See the Red Hat Knowledge base solution, link:https://access.redhat.com/solutions/6470721[Deploy Jenkins on OpenShift with Custom OAuth Server URL]. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1991448[*BZ#1991448*])

* This release contains a known issue with Jenkins. The `xmlstarlet` command line toolkit, which is required to validate or query XML files, is missing from this {op-system-base}-based image. This issue impacts deployments that do not use OpenShift OAuth for authentication. Although OpenShift OAuth is enabled by default, users can disable it.
+
Workaround: Use OpenShift OAuth for authentication. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2055653[BZ#2055653])

* Google Cloud Platform (GCP) UPI installation fails when the instance group name is longer than the maximum size of 64 characters. You are restricted in the naming process after adding the "-instance-group" suffix. Shorten the suffix to "-ig" to reduce the number of characters. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1921627[*BZ#1921627*])

* If an incorrect network is specified in the vSphere `install-config.yaml` file, then an error message from Terraform is generated after a while. Add a check during the creation of manifests to notify the user if the network is invalid. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1956776[*BZ#1956776*])

* The Special Resource Operator (SRO) might fail to install on Google Cloud Platform due to a software-defined network policy. As a result, the simple-kmod pod is not created. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1996916[*BZ#1996916*])

* Currently, idling a stateful set is unsupported when you run `oc idle` for a service that is mapped to a stateful set. There is no known workaround at this time. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1976894[*BZ#1976894*])

*  The China (Nanjing) and UAE (Dubai) regions of Alibaba Cloud International Portal accounts do not support installer-provisioned infrastructure (IPI) installations. The China (Guangzhou) and China (Ulanqab) regions do not support a Server Load Balancer (SLB) if using Alibaba Cloud International Portal accounts and, therefore, also do not support IPI installations.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2048062[*BZ#2048062*])

[id="ocp-4-10-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster-within-minor.adoc#updating-cluster-within-minor[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-10-0-ga"]
=== RHSA-2022:0056 - {product-title} 4.10.0 image release, bug fix, and security update advisory

Issued: 2022-03-09

{product-title} release 4.10.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2022:0056[RHSA-2022:0056] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2022:0055[RHSA-2022:0055] advisory.

Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:

link:https://access.redhat.com/solutions/6765671[{product-title} 4.10.0 container image list]
