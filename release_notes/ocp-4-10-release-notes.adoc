:_content-type: ASSEMBLY
[id="ocp-4-10-release-notes"]
= {product-title} {product-version} release notes
include::modules/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multi-tenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-10-about-this-release"]
== About this release

{product-title} *(link:https://access.redhat.com/errata/RHSA-2021:3759[RHSA-2021:3759])* is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md[Kubernetes 1.23] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

//Red Hat did not publicly release {product-title} 4.10.0 as the GA version and, instead, is releasing {product-title} 4.10.TBD as the GA version.

{product-title} {product-version} clusters are available at https://cloud.redhat.com/openshift. The {console-redhat-com} application for {product-title} allows you to deploy OpenShift clusters to either on-premise or cloud environments.

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.4 and 8.5, as well as on {op-system-first} 4.10.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//TODO: Add the line below for EUS releases.
//{product-title} 4.6 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

//TODO: The line below is not true for 4.9 but should be used when it is next appropriate. Revisit in October 2022 timeframe.
//With the release of {product-title} 4.9, version 4.6 is now end of life. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].


[id="ocp-4-10-add-on-support-status"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-10-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-10-documentation"]
=== Documentation

[id="ocp-4-10-getting-started"]
==== Getting started with {product-title}

{product-title} 4.10 now includes a getting started guide. Getting Started with {product-title} defines basic terminology and provides role-based next steps for developers and administrators.

The tutorials walk new users through the web console and the OpenShift CLI (`oc`) interfaces. New users can accomplish the following tasks through the Getting Started:

* Create a project
* Grant view permissions
* Deploy a container image from Quay
* Examine and scale an application
* Deploy a Python application from GitHub
* Connect to a database from Quay
* Create a secret
* Load and view your application

For more information, see xref:../getting_started/openshift-overview.adoc[Getting Started with {product-title}].

[id="ocp-4-10-rhcos"]
=== {op-system-first}

[id="ocp-4-10-coreos-installer-customize"]
==== Improved customization of bare metal {op-system} installation

The `coreos-installer` utility now has `iso customize` and `pxe customize` subcommands for more flexible customization when installing {op-system} on bare metal from the live ISO and PXE images.

[id="ocp-4-10-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-10-AWS-default-components"]
==== New default component types for AWS installations

The {product-title} 4.10 installer uses new default component types for installations on AWS. The installer uses the following components by default:

* AWS EC2 M6i instances for both control plane and compute nodes, where available
* AWS EBS gp3 storage

[id="ocp-4-10-OCP-on-ARM"]
==== {product-title} on ARM

{product-title} 4.10 is now supported on ARM based AWS EC2 A1 and bare-metal platforms. Instance availability and installation documentation can be found in xref:../installing/installing-preparing.html#supported-installation-methods-for-different-platforms[Supported installation methods for different platforms].

The following features are supported for {product-title} on ARM:

* OpenShift Cluster Monitoring
* RHEL 8 Application Streams
* OVNKube
* OpenShift Logging
* Elastic Book Store (EBS) for AWS
* AWS Certificate Manager (ACM)
* AWS .NET applications
* NFS storage on bare metal

The following Operators are supported for {product-title} on ARM:

* Node Tuning Operator
* Node Feature Discovery Operator
* Cluster Samples Operator
* Cluster Logging Operator
* Elasticsearch Operator
* Service Binding Operator

[id="ocp-4-10-ibm-cloud-installation"]
==== Installing a cluster on IBM Cloud using installer-provisioned infrastructure

{product-title} 4.10 introduces support for installing a cluster on IBM Cloud using installer-provisioned infrastructure.

//For more information, see ../installing/installing_ibm_cloud_public/preparing-to-install-on-ibm-cloud.adoc#preparing-to-install-on-ibm-cloud[Preparing to install on IBM Cloud].

[id="ocp-4-10-installation-and-upgrade-vsphere-provisioning"]
==== Thin provisioning support for VMware vSphere cluster installation

{product-title} {product-version} introduces support for thin provisioned disks when you install a cluster using installer-provisioned infrastructure. You can provision disks as `thin`, `thick`, or `eagerZeroedThick`. For more information about disk provisioning modes in VMware vSphere, see xref:../installing/installing_vsphere/installing-vsphere-installer-provisioned-customizations.html#installation-configuration-parameters_installing-vsphere-installer-provisioned-customizations[Installation configuration parameters].

[id="ocp-4-10-installation-and-upgrade-aws-ami"]
==== Installing a cluster into an Amazon Web Services GovCloud region

{op-system-first} Amazon Machine Images (AMIs) are now available for AWS GovCloud regions. The availability of these AMIs improves the installation process because you are no longer required to upload a custom RHCOS AMI to deploy a cluster.
//For more information on installing to a AWS GovCloud region, <insert link to topic after it merges>

[id="ocp-4-10-installation-and-upgrade-tagrole"]
==== Using a custom AWS IAM role for instance profiles

Beginning with {product-title} {product-version}, if you configure a cluster with an existing IAM role, the installer no longer adds the `shared` tag to the role when deploying the cluster. This enhancement improves the installation process for organizations that want to use a custom IAM role, but whose security policies prevent the use of the `shared` tag.

[id="ocp-4-10-installation-vsphere-csi"]
==== CSI driver installation on vSphere clusters

To install a CSI driver on a cluster running on vSphere, you must have the following components installed:

* Virtual hardware version 15 or greater
* vSphere version 6.7 Update 3 or greater
* VMware ESXi version 6.7 Update 3 or greater

Components with versions lower than those above are still supported, but are deprecated. Support for them will be removed in a future version of {product-title}.

[NOTE]
====
If your cluster is deployed on vSphere, and the preceding components are lower than the version mentioned above, upgrading from {product-title} 4.9 to 4.10 on vSphere is supported, but no vSphere CSI driver will be installed. Bug fixes and other upgrades to 4.10 are still supported, however upgrading to 4.11 will be unavailable.
====

[id="ocp-4-10-conditional-updates"]
==== Conditional updates

{product-title} 4.10 adds support for consuming conditional update paths provided by the OpenShift Update Service.
Conditional update paths convey identified risks and the conditions under which those risks apply to clusters.
The Administrator perspective on the web console only offers recommended upgrade paths for which the cluster does not match known risks.
However, OpenShift CLI (`oc`) 4.10 or later can be used to display additional upgrade paths for {product-title} 4.10 clusters.
Associated risk information including supporting documentation references is displayed with the paths.
The administrator may review the referenced materials and choose to perform the supported, but no longer recommended, upgrade.

For more information, see xref:../updating/understanding-upgrade-channels-release.adoc#conditional-updates-overview_understanding-upgrade-channels-releases[Conditional updates] and xref:../updating/updating-cluster-cli.adoc#update-conditional-upgrade-pathupdating-cluster-cli[Updating along a conditional upgrade path].

[id="ocp-4-10-web-console"]
=== Web console

[id="pod-debug-mode-web-console"]
==== Running a pod in debug mode
With this update, you can now view debug terminals in the web console. When a pod has a container that is in a `CrashLoopBackOff` state, a debug pod can be launched. A terminal interface is displayed and can be used to debug the crash looping container.

* This feature can be accessed by the pod status pop-up window, which is accessed by clicking on the status of a pod, provides links to debug terminals for each crash looping container within that pod.
* You can also access this feature on the *Logs* tab of the pod details page. A debug terminal link is displayed above the log window when a crash looping container is selected.

Additionally, the pod status pop-up window now provides links to the *Logs* and *Events* tabs of the pod details page.

[id="web-console-customized-workload-notifications"]
==== Customized workload notifications
With this update, you can customize workload notifications on the *User Preferences* page. *User workload notifications* under the *Notifications* tab allows you to hide user workload notifications that appear on the *Cluster Overview* page or in your drawer.

[id="ocp-4-10-ibm-z"]
=== IBM Z and LinuxONE

[id="ocp-4-10-ibm-power"]
=== IBM Power Systems

[id="ocp-4-10-security"]
=== Security and compliance

[id="ocp-4-10-networking"]
=== Networking

[id="ocp-4-10-redfish-events"]
==== Low-latency Redfish hardware event delivery (Technology Preview)

{product-title} now provides a hardware event proxy that enables applications running on bare-metal clusters to respond quickly to Redfish hardware events, such as hardware changes and failures.

The hardware event proxy supports a publish-subscribe service that allows relevant applications to consume hardware events detected by Redfish. The proxy must be running on hardware that supports Redfish v1.8 and higher. An Operator manages the lifecycle of the `hw-event-proxy` container.

You can use a REST API to develop applications to consume and respond to events such as breaches of temperature thresholds, fan failure, disk loss, power outages, and memory failure. Reliable end-to-end messaging without persistent stores is based on the Advanced Message Queuing Protocol (AMQP). The latency of the messaging service is in the 10 millisecond range.

[NOTE]
====
This feature is supported for single node OpenShift clusters only.
====

[id="ocp-4-10-networking-ovn-gateway-config"]
==== OVN-Kubernetes support for gateway configuration

The OVN-Kubernetes CNI network provider adds support for configuring how egress traffic is sent to the node gateway.
By default, egress traffic is processed in OVN to exit the cluster and traffic is not affected by specialized routes in the kernel routing table.

This enhancement adds a `gatewayConfig.routingViaHost` field.
With this update, the field can be set at runtime as a post-installation activity and when it is set to `true`, egress traffic is sent from pods to the host networking stack.
This update benefits highly-specialized installations and applications that rely on manually configured routes in the kernel routing table.

This enhancement has an interaction with the Open vSwitch hardware offloading feature.
With this update, when the `gatewayConfig.routingViaHost` field is set to `true`, you do not receive the performance benefits of the offloading because egress traffic is processed by the host networking stack.

For configuration information, see xref:../networking/cluster-network-operator.adoc#nw-operator-configuration-parameters-for-ovn-sdn_cluster-network-operator[Configuration for the OVN-Kubernetes CNI cluster network provider].

[id="ocp-4-10-networking-metrics"]
==== Enhancements to networking metrics

The following metrics are now available for clusters.
The metric names that start with `sdn_controller` are unique to the OpenShift SDN CNI network provider.
The metric names that start with `ovn` are unique to the OVN-Kubernetes CNI network provider:

* `network_attachment_definition_instances{networks="egress-router"}`
* `openshift_unidle_events_total`
* `ovn_controller_bfd_run`
* `ovn_controller_ct_zone_commit`
* `ovn_controller_flow_generation`
* `ovn_controller_flow_installation`
* `ovn_controller_if_status_mgr`
* `ovn_controller_if_status_mgr_run`
* `ovn_controller_if_status_mgr_update`
* `ovn_controller_integration_bridge_openflow_total`
* `ovn_controller_ofctrl_seqno_run`
* `ovn_controller_patch_run`
* `ovn_controller_pinctrl_run`
* `ovnkube_master_ipsec_enabled`
* `ovnkube_master_num_egress_firewall_rules`
* `ovnkube_master_num_egress_firewalls`
* `ovnkube_master_num_egress_ips`
* `ovnkube_master_pod_first_seen_lsp_created_duration_seconds`
* `ovnkube_master_pod_lsp_created_port_binding_duration_seconds`
* `ovnkube_master_pod_port_binding_chassis_port_binding_up_duration_seconds`
* `ovnkube_master_pod_port_binding_port_binding_chassis_duration_seconds`
* `sdn_controller_num_egress_firewall_rules`
* `sdn_controller_num_egress_firewalls`
* `sdn_controller_num_egress_ips`

The `ovnkube_master_resource_update_total` metric is removed for the 4.10 release.

[id="ocp-4-10-networkobservability-switch"]
==== Switching between YAML view and a web console form

* Previously, changes were not retained when switching between *YAML view* and *Form view* on the web console. Additionally, after switching to *YAML view*, you could not return to *Form view*. With this update, you can now easily switch between *YAML view* and *Form view* on the web console without losing changes.

[id="ocp-4-10-networkobservability-targeted-pods"]
==== Listing pods targeted by network policies
When using the network policy functionality in the {product-title} web console, the pods affected by a policy are listed. The list changes as the combined namespace and pod selectors in these policy sections are modified:

* Peer definition
* Rule definition
* Ingress
* Egress

The list of impacted pods includes only those pods accessible by the user.

[id="ocp-4-10-networking-must-gather-tcpdump"]
==== Enhancement to must-gather to simplify network tracing

The `oc adm must-gather` command is enhanced in a way that simplifies collecting network packet captures.

Previously, `oc adm must-gather` could start a single debug pod only.
With this enhancement, you can start a debug pod on multiple nodes at the same time.

You can use the enhancement to run packet captures on multiple nodes at the same time to simplify troubleshooting network communication issues.
A new `--node-selector` argument provides a way to identify which nodes you are collecting packet captures for.

For more information, see xref:../support/gathering-cluster-data.adoc#support-network-trace-methods_gathering-cluster-data[Network trace methods] and xref:../support/gathering-cluster-data.adoc#support-collecting-host-network-trace_gathering-cluster-data[Collecting a host network trace].

[id="ocp-4-10-egress-ip-support-public-clouds"]
=== Egress IP address support for clusters installed on public clouds

As a cluster administrator, you can associate one or more egress IP addresses with a namespace. An egress IP address ensures that a consistent source IP address is associated with traffic from a particular namespace that is leaving the cluster.

For the OVN-Kubernetes and OpenShift SDN cluster network providers, you can configure an egress IP address on the following public cloud providers:

* Amazon Web Services (AWS)
* Google Cloud Platform (GCP)
* Microsoft Azure

To learn more, refer to the respective documentation for your cluster network provider:

* OVN-Kubernetes: xref:../networking/ovn_kubernetes_network_provider/configuring-egress-ips-ovn.adoc#configuring-egress-ips-ovn[Configuring an egress IP address]
* OpenShift SDN: xref:../networking/openshift_sdn/assigning-egress-ips.adoc#egress-ips[Configuring egress IPs for a project]

[id="ocp-4-10-openshift-sdn-netpol-egress-policies"]
==== OpenShift SDN cluster network provider network policy support for egress policies and ipBlock except

If you use the OpenShift SDN cluster network provider, you can now use egress rules in network policy with `ipBlock` and `ipBlock.except`. You define egress policies in the `egress` array of the `NetworkPolicy` object.

For more information, refer to xref:../networking/network_policy/about-network-policy.adoc#about-network-policy[About network policy].

[id="ocp-4-10-ne-router-compression"]
==== Ingress Controller router compression
This enhancement adds the ability to configure global HTTP traffic compression on the HAProxy Ingress Controller for specific MIME types. This update enables gzip-compression of your ingress workloads when there are large amounts of compressible routed traffic.

For more information, see xref:../networking/ingress-operator.adoc#nw-configuring-router-compression_configuring-ingress[Using router compression].

[id="ocp-4-10-ne-coredns-customization"]
==== Support for CoreDNS customization
A cluster administrator can now configure DNS servers to allow DNS name resolution through the configured servers for the default domain. A DNS forwarding configuration can have both the default servers specified in the `/etc/resolv.conf` file and the upstream DNS servers.

For more information, see xref:../networking/dns-operator.adoc#nw-dns-forward_dns-operator[Using DNS forwarding].

[id="ocp-4-10-ne-syslog-maxlength"]
==== Support for configuring the maximum length of the syslog message in the Ingress Controller
You can now set the maximum length of the syslog message in the Ingress Controller to any value between 480 and 4096 bytes.

For more information, see xref:../networking/ingress-operator.adoc#nw-ingress-controller-configuration-parameters_configuring-ingress[Ingress Controller configuration parameters].


[id="ocp-4-10-hardware"]
=== Hardware

[id="ocp-4-10-networking-metallb"]
==== Enhancements to MetalLB load balancing

The following enhancements to MetalLB and the MetalLB Operator are included in this release:

* Support for Border Gateway Protocol (BGP) is added.
* Support for Bidirectional Forwarding Detection (BFD) in combination with BGP is added.
* Support for IPv6 and dual-stack networking is added.
* Support for specifying a node selector on the `speaker` pods is added. You can now control which nodes are used for advertising load balancer service IP addresses. This enhancement applies to layer 2 mode and BGP mode.
* Validating web hooks are added to ensure that addresss pool and BGP peer custom resources are valid.
* The `v1alpha1` API version for the `AddressPool` and `MetalLB` custom resource definitions that were introduced in the 4.9 release are deprecated. Both custom resources are updated to the `v1beta1` API version.
* Support for speaker pod tolerations in the MetalLB custom resrouce definition is added.

For more information, see xref:../networking/metallb/about-metallb.adoc#about-metallb[About MetalLB and the MetalLB Operator].

[id="ocp-4-10-storage"]
=== Storage

[id="ocp-4-10-registry"]
=== Registry

[id="ocp-4-10-olm"]
=== Operator lifecycle

[id="ocp-4-10-osdk"]
=== Operator development

[id="ocp-4-10-builds"]
=== Builds

[id="ocp-4-10-images"]
=== Images

[id="ocp-4-10-machine-api"]
=== Machine API

[id="ocp-4-10-mco-config-drift"]
==== Enhanced configuration drift detection

With this enhancement, the Machine Config Daemon (MCD) now checks nodes for configuration drift if a filesystem write event occurs for any of the files specified in the machine config and before a new machine config is applied, in addition to node bootup. Previously, the MCD checked for configuration drift only at node bootup. This change was made because node reboots do not occur frequently enough to avoid the problems caused by configuration drift until an administrator can correct the issue.

Configuration drift occurs when the on-disk state of a node differs from what is configured in the machine config. The Machine Config Operator (MCO) uses the MCD to check nodes for configuration drift and, if detected, sets that node and machine config pool (MCP) to `degraded`.

For more information on configuration drift, see xref:../post_installation_configuration/machine-configuration-tasks.adoc#machine-config-drift-detection_post-install-machine-configuration-tasks[Understanding configuration drift detection].

[id="ocp-4-10-nodes"]
=== Nodes

[id="ocp-4-10-node-cgroups-v2"]
==== Linux control groups version 2 (Developer Preview)

You can now enable link:https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html[Linux control groups version 2] (cgroups v2) on specific nodes in your cluster. The {product-title} process for enabling cgroups v2 disables all cgroups version 1 controllers and hierarchies. The {product-title} cgroups version 2 feature is in Developer Preview and is not supported by Red Hat at this time.

[id="ocp-4-10-logging"]
=== Red Hat OpenShift Logging

In {product-title} 4.7, _Cluster Logging_ became _Red Hat OpenShift Logging_. For more information, see xref:../logging/cluster-logging-release-notes.adoc[Release notes for Red Hat OpenShift Logging].

[id="ocp-4-10-monitoring"]
=== Monitoring

[id="ocp-4-10-scalability-and-performance"]
=== Scalability and performance

[id="ocp-4-10-SRO-metrics"]
==== New Special Resource Operator metrics

The Special Resource Operator (SRO) now exposes metrics to help you watch the health of your SRO custom resources and objects. For more information, see xref:../hardware_enablement/psap-special-resource-operator.html#special-resource-operator-metrics_special-resource-operator[Prometheus Special Resource Operator metrics].


[id="ocp-4-10-backup-and-restore"]
=== Backup and restore

[id="ocp-4-10-dev-exp"]
=== Developer experience

[id="ocp-4-10-dev-exp-prune-deployments"]
==== Pruning deployment replica sets (Technology Preview)

This release introduces a Technology Preview flag `--replica-sets` to the `oc adm prune deployments` command. By default, only replication controllers are pruned with the `oc adm prune deployments` command. When you set `--replica-sets` to `true`, replica sets are also included in the pruning process.

For more information, see xref:../applications/pruning-objects.adoc#pruning-deployments_pruning-objects[Pruning deployment resources].

[id="ocp-4-10-insights-operator"]
=== Insights Operator

[id="ocp-4-10-insights-operator-sca"]
==== Importing simple content access certificates

In {product-title} 4.10, Insights Operator now imports your simple content access certificates from {console-redhat-com} by default.

For more information, see xref:../support/remote_health_monitoring/insights-operator-simple-access.adoc[Importing simple content access certificates with Insights Operator].

[id="ocp-4-10-insights-operator-data-collection-enhancements"]
==== Insights Operator data collection enhancements

To reduce the amount of data sent to Red Hat, Insights Operator only gathers information when certain conditions are met. For example, Insights Operator only gathers the Alertmanager logs when Alertmanager fails to send alert notifications.


In {product-title} 4.10, the Insights Operator collects the following additional information:

* (Conditional) The logs from pods where the `KubePodCrashlooping` and `KubePodNotReady` alerts are firing
* (Conditional) The Alertmanager logs when the `AlertmanagerClusterFailedToSendAlerts` or `AlertmanagerFailedToSendAlerts` alerts are firing
* Silenced alerts from Alertmanager
* The node logs from the journal unit (kubelet)
* The `CostManagementMetricsConfig` from clusters with `costmanagement-metrics-operator` installed
* The time series database status from the monitoring stack Prometheus instance
* Additional information about the {product-title} scheduler

With this additional information, Red Hat improves {product-title} functionality and enhances Insights Advisor recommendations.

[id="ocp-4-10-auth"]
=== Authentication and authorization

[id="ocp-4-10-auth-sync-group-membership"]
==== Syncing group membership from OpenID Connect identity providers

This release introduces support for synchronizing group membership from an OpenID Connect provider to {product-title} upon user login. You can enable this by configuring the `groups` claim in the {product-title} OpenID Connect identity provider configuration.

For more information, see xref:../authentication/identity_providers/configuring-oidc-identity-provider.adoc#identity-provider-oidc-CR_configuring-oidc-identity-provider[Sample OpenID Connect CRs].

[id="ocp-4-10-oc-commands-obtain-credentials-from-podman-config"]
==== oc commands now obtain credentials from Podman configuration locations

Previously, `oc` commands that used the registry configuration, for example `oc registry login` or `oc image` commands, obtained credentials from Docker configuration locations. With {product-title} {product-version}, if a registry entry cannot be found in the default Docker configuration location, `oc` commands obtain the credentials from Podman configuration locations. You can set your preference to either `docker` or `podman` by using the `REGISTRY_AUTH_PREFERENCE` environment variable to prioritize the location.

Users also have the option to use the `REGISTRY_AUTH_FILE` environment variable, which serves as an alternative to the existing `--registry-config` CLI flag. The `REGISTRY_AUTH_FILE` environment variable is also compatible with `podman`.


[id="ocp-4-10-sandboxed-containers"]
=== {sandboxed-containers-first}

[id="ocp-4-10-notable-technical-changes"]
== Notable technical changes

{product-title} {product-version} introduces the following notable technical changes.

// Note: use [discrete] for these sub-headings.

[discrete]
[id="ocp-4-10-TLS-subject-alternative-names-required"]
==== TLS X.509 certificates must have a Subject Alternative Name

X.509 certificates must have a properly set the Subject Alternative Name field.
If you update your cluster without this, you risk breaking your cluster or rendering it inaccessible.

In older versions of {product-title}, X.509 certificates worked without a Subject Alternative Name, so long as the Common Name field was set.
link:https://docs.openshift.com/container-platform/4.6/release_notes/ocp-4-6-release-notes.html#ocp-4-6-tls-common-name[This behavior was removed in {product-title} 4.6].

In some cases, certificates without a Subject Alternative Name continued to work in {product-title} 4.6, 4.7, 4.8, and 4.9.
Because it uses Kubernetes 1.23, {product-title} 4.10 does not allow this under any circumstances.

[discrete]
[id="ocp-4-10-cluster-cloud-controller-manager-operator"]
==== Cloud controller managers for additional cloud providers

The Kubernetes community plans to deprecate the Kubernetes controller manager in favor of using cloud controller managers to interact with underlying cloud platforms. As a result, there is no plan to add Kubernetes controller manager support for any new cloud platforms. The Alibaba Cloud and IBM Cloud implementations that are added in this release of {product-title} use cloud controller managers.

In addition, this release supports using cloud controller managers for Google Cloud Platform (GCP) and VMware vSphere as a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview].

To learn more about the cloud controller manager, see the link:https://kubernetes.io/docs/concepts/architecture/cloud-controller/[Kubernetes Cloud Controller Manager documentation].

To manage the cloud controller manager and cloud node manager deployments and lifecycles, use the Cluster Cloud Controller Manager Operator.

For more information, see the xref:../operators/operator-reference.adoc#cluster-cloud-controller-manager-operator_platform-operators-ref[Cluster Cloud Controller Manager Operator] entry in the _Platform Operators reference_.

[id="ocp-4-10-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more fine-grained functionality that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *TP*: _Technology Preview_
* *DEP*: _Deprecated_
* *REM*: _Removed_

//TODO: remove anything that has been REM since 4.7?

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.8 |OCP 4.9 | OCP 4.10

|Package manifest format (Operator Framework)
|REM
|REM
|REM

|SQLite database format for Operator catalogs
|GA
|DEP
|

|`oc adm catalog build`
|REM
|REM
|REM

|`--filter-by-os` flag for `oc adm catalog mirror`
|REM
|REM
|REM

|v1beta1 CRDs
|DEP
|REM
|REM

|Docker Registry v1 API
|DEP
|REM
|REM

|Metering Operator
|DEP
|REM
|REM

|Scheduler policy
|DEP
|DEP
|REM

|`ImageChangesInProgress` condition for Cluster Samples Operator
|DEP
|DEP
|

|`MigrationInProgress` condition for Cluster Samples Operator
|DEP
|DEP
|

|Use of `v1` without a group in `apiVersion` for {product-title} resources
|DEP
|REM
|REM

|Use of `dhclient` in {op-system}
|DEP
|REM
|REM

|Cluster Loader
|DEP
|DEP
|REM

|Bring your own {op-system-base} 7 compute machines
|DEP
|DEP
|REM

|`lastTriggeredImageID` field in the `BuildConfig` spec for Builds
|DEP
|REM
|REM

|Jenkins Operator
|DEP
|DEP
|

|HPA custom metrics adapter based on Prometheus
|REM
|REM
|

|vSphere 6.7 Update 2 or earlier
|GA
|DEP
|DEP

|Virtual hardware version 13
|GA
|DEP
|DEP

|VMware ESXi 6.7 Update 2 or earlier
|GA
|DEP
|DEP

|====

[id="ocp-4-10-deprecated-features"]
=== Deprecated features

[id="ocp-4-10-arch-deprecations"]
==== IBM POWER8, IBM z13 all models, LinuxONE Emperor, LinuxONE Rockhopper, and x86_64 v1 architectures will be deprecated

{op-system} functionality in IBM POWER8, IBM z13 all models, LinuxONE Emperor, LinuxONE Rockhopper, and x86_64 v1 CPU architectures will be deprecated in an upcoming release. Additional details for when support will discontinue for these architectures will be announced in a future release.

[NOTE]
====
AMD and Intel 64-bit architectures (x86-64-v2) will still be supported.
====

[id="ocp-4-10-docker-config-location-deprecation"]
==== Default Docker configuration location deprecation
Previously, `oc` commands that used a registry configuration would obtain credentials from the Docker configuration location, which was `~/.docker/config.json` by default. This has been deprecated and will be replaced by a Podman configuration location in a future version of {product-title}.

[id="ocp-4-10-empty-file-support-deprecation"]
==== Empty file and stdout support deprecation in oc registry login

Support for empty files using the `--registry-config` and `--to` flags in `oc registry login` has been deprecated. Support for `-` (standard output) has also been deprecated as an argument when using `oc registry login`. They will be removed in a future version of {product-title}.

[id="ocp-4-10-removed-features"]
=== Removed features

[id="ocp-4-10-oc-commands-removed"]
==== OpenShift CLI (oc) commands removed

The following OpenShift CLI (`oc`) commands were removed with this release:

* `oc adm completion`
* `oc adm config`
* `oc adm options`

[id="ocp-4-10-removed-scheduler-policy"]
==== Scheduler policy removed

Support for configuring a scheduler policy has been removed with this release. Use a xref:../nodes/scheduling/nodes-scheduler-profiles.adoc#nodes-scheduler-profiles[scheduler profile] instead to control how pods are scheduled onto nodes.

[id="ocp-4-10-removed-feature-rhel-7-support"]
==== {op-system-base} 7 support for compute machines removed

Support for running {op-system-base-full} 7 compute machines in {product-title} has been removed. If you prefer using {op-system-base} compute machines, they must run on {op-system-base} 8.

You cannot upgrade {op-system-base} 7 compute machines to {op-system-base} 8. You must deploy new {op-system-base} 8 hosts, and the old {op-system-base} 7 hosts must be removed.

[id="ocp-4-10-bug-fixes"]
== Bug fixes

[discrete]
[id="ocp-4-10-api-server-auth-bug-fixes"]
==== API server and authentication

[discrete]
[id="ocp-4-10-bare-metal-hardware-bug-fixes"]
==== Bare Metal Hardware Provisioning

* Previously, virtual media based deployments of {product-title} have been observed to intermittently fail on iDRAC hardware types. This occurred when outstanding Lifecycle Controller jobs clashed with virtual media configuration requests. With this update, virtual media deployment failure has been fixed by purging any Lifecycle Controller job while registering iDRAC hardware prior to deployment. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1988879[*BZ#1988879*])

* Previously, the `curl` utility used by the machine downloader image did not support classless inter-domain routing (CIDR) with `no_proxy`. A a result, any CIDR in`noProxy` was ignored when downloading the {op-system-first} image. With this update, proxies are now removed from the environment before calling `curl` when appropriate. As a result, when downloading the machine image, any CIDR in `no_proxy` is no longer ignored.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=1990556[*BZ#1990556*])

* Previously, Ironic failed to attach virtual media images for provisioning SuperMicro X11/X12 servers because these models expect a non-standard deice strong (`UsbCd`) for CD-based virtual media. With this update, provisioning now overrides `UsbCd` on SuperMicro machines provisioned with CD-based virtual media.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=2009555[*BZ#2009555*])

* Previously, Ironic failed to attach virtual media images on SuperMicro X11/X12 servers due to overly restrictive URl validations on the BMCs of these machines. With this update, the `filename` parameter has now been removed from the URL if the virtual media image is backed by a local file. As a result, the parameter still passes if the image is backed by an object store. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2011626[*BZ#2011626*])

* Previously, users had to enter a long form of an IPv6 address in the installation configuration file, for example `2001:0db8:85a3:0000:0000:8a2e:0370:7334`. Ironic could not find an interface matching this IP address causing the installation to fail. With this update, the IPv6 address supplied by the user is converted to a short form address, for example, `2001:db8:85a3::8a2e:370:7334`. As a result, installation is now successful. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2010698[*BZ#2010698*])

[discrete]
[id="ocp-4-10-builds-bug-fixes"]
==== Builds

[discrete]
[id="ocp-4-10-cloud-compute-bug-fixes"]
==== Cloud Compute

* Previously, editing a machine specification on {rh-openstack-first} would cause {product-title} to attempt to delete and recreate the machine. As a result, this caused an unrecoverable loss of the node it was hosting. With this fix, any edits made to the machine specification after creation are ignored. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1962066[*BZ#1962066*])

[discrete]
[id="ocp-4-10-cluster-version-operator-bug-fixes"]
==== Cluster Version Operator

[discrete]
[id="ocp-4-10-console-storage-bug-fixes"]
==== Console Storage Plug-in

[discrete]
[id="ocp-4-10-image-registry-bug-fixes"]
==== Image Registry

[discrete]
[id="ocp-4-10-installer-bug-fixes"]
==== Installer

* Previously, RAM validation for {rh-openstack-first} checked for values using a wrong unit, and as a result the validation accepted flavors that did not meet minimum RAM requirements. With this fix, RAM validation now rejects flavors with insufficient RAM. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2009699[*BZ#2009699*])

* Previously, {product-title} master nodes were missing Ingress security group rules when they were schedulable and deployed on {rh-openstack}. As a result, {product-title} deployments on {rh-openstack} failed for compact clusters with no dedicated workers. This fix adds Ingress security group rules on {rh-openstack-first} when masters are schedulable. Now, you can deploy compact three-node clusters on {rh-openstack}. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1955544[*BZ#1955544*])
* Only some drivers support UEFI secure boot mode. Validation has been added to the installation process to ensure that UEFISecureBoot mode is not used with bare metal drivers that do not support changing secure boot status. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2011893[*BZ#2011893*])

* The {product-title} Baremetal IPI installer previously used the first nodes defined under hosts in `install-config` as master nodes instead of filtering for the hosts with the master role. The role of master and worker nodes is now recognized when defined. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2003113[*BZ#2003113*])


[discrete]
[id="ocp-4-10-kube-api-server-bug-fixes"]
==== Kubernetes API server

[discrete]
[id="ocp-4-10-machine-config-operator"]
==== Machine Config Operator
Previously, the MCO stored pending configuration changes to disk before OS changes were applied. As a result, in situations such as power loss, the MCO assumed OS changes had already been applied on restart, and validation skipped over changes such as kargs and kernel-rt. This was remedied by only storing configuration changes to disk after the OS changes were applied. Now, if power is lost during the configuration application, the MCO knows it must reattempt the configuration application on restart.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1916169[*BZ#1916169*])

[discrete]
[id="ocp-4-10-networking-bug-fixes"]
==== Networking

[discrete]
[id="ocp-4-10-node-bug-fixes"]
==== Node

[discrete]
[id="ocp-4-10-openshift-cli-bug-fixes"]
==== OpenShift CLI (oc)
Previously, due to the increasing number of CRDs installed in the cluster, the requests reaching for API discovery were limited by client code restrictions. Now, both the limit number and QPS have been boosted, and client-side throttling should appear less frequently. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2042059[*BZ#2042059*])

[discrete]
[id="ocp-4-10-olm-bug-fixes"]
==== Operator Lifecycle Manager (OLM)

[discrete]
[id="ocp-4-10-openshift-api-server-bug-fixes"]
==== OpenShift API server

[discrete]
[id="ocp-4-10-openshift-update-service-bug-fixes"]
==== OpenShift Update Service

[discrete]
[id="ocp-4-10-performance-addon-operator-bug-fixes"]
==== Performance Addon Operator

* The spec.cpu.reserved` flag might not be correctly set by default if `spec.cpu.isolated` is the only parameter defined in `PerformanceProfile`. You must set the settings for both `spec.cpu.reserved` and `spec.cpu.isolated` in the `PerformanceProfile`. The sets must not overlap and the sum of all CPUs mentioned must cover all CPUs expected by the workers in the target pool. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1986681[*BZ#1986681*])

*  Previously, the `oc adm must-gather` tool did not collect performance specific data when more than one `--image` argument was supplied. Files, including node and performance related files, were missing when the operation finished. The issue affects {product-title} versions between 4.7 and 4.10. This issue can be resolved by executing the `oc adm must-gather` operation twice, once for each image. As a result, all expected files can be collected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2018159[*BZ#2018159*]).

* Previously, the `oc adm must-gather` tool failed to collect node data if the `gather-sysinfo` binary was missing in the image. This was caused by a missing `COPY` statement in the Dockerfile. To avoid this issue, you must add the necessary `COPY` statements to the Dockerfile to generate and copy the binaries. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2021036[*BZ#2021036*])

[discrete]
[id="ocp-4-10-rhcos-bug-fixes"]
==== {op-system-first}

[discrete]
[id="ocp-4-10-routing-bug-fixes"]
==== Routing

[discrete]
[id="ocp-4-10-samples-bug-fixes"]
==== Samples

[discrete]
[id="ocp-4-10-storage-bug-fixes"]
==== Storage

[discrete]
[id="ocp-4-10-telco-edge"]
==== Telco Edge
* If a generated policy has a complianceType of `mustonlyhave`, OLM updates to metadata are then reverted as the policy engine restores the ‘expected’ state of the CR. As a consequence, OLM and the policy engine continuously overwrite the metadata of the CR under conflict. This results in high CPU usage. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2021036[*BZ#2021036*])

* Previously, user-supplied fields in the `PolicyGenTemplate` overlay were not copied to generated manifests if the field did not exist in the base source CR. As a result, some user content was lost. The `policyGen` tool is now updated to support all user-supplied fields.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=2028881[*BZ#2028881*])

[discrete]
[id="ocp-4-10-web-console-admin-perspective-bug-fixes"]
==== Web console (Administrator perspective)

[discrete]
[id="ocp-4-10-web-console-developer-perspective-bug-fixes"]
==== Web console (Developer perspective)

[id="ocp-4-10-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_
* *DEP*: _Deprecated_

//TODO: remove anything that has been GA since 4.7?

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.8 |OCP 4.9 | OCP 4.10

|Precision Time Protocol (PTP)
|TP
|TP
|

|`oc` CLI plug-ins
|GA
|GA
|GA

|Service Binding
|TP
|TP
|

|Raw Block with Cinder
|GA
|GA
|GA

|CSI volume expansion
|TP
|TP
|

|CSI Azure Disk Driver Operator
|TP
|TP
|

|CSI Azure Stack Hub Driver Operator
|-
|GA
|GA

|CSI GCP PD Driver Operator
|GA
|GA
|GA

|CSI OpenStack Cinder Driver Operator
|TP
|TP
|

|CSI AWS EBS Driver Operator
|TP
|GA
|GA

|CSI AWS EFS Driver Operator
|-
|TP
|

|CSI automatic migration
|TP
|TP
|

|CSI inline ephemeral volumes
|TP
|TP
|

|CSI vSphere Driver Operator
|TP
|TP
|

|Automatic device discovery and provisioning with Local Storage Operator
|TP
|TP
|

|OpenShift Pipelines
|GA
|GA
|GA

|OpenShift GitOps
|GA
|GA
|GA

|OpenShift sandboxed containers
|TP
|TP
|

|Vertical Pod Autoscaler
|GA
|GA
|GA

|Cron jobs
|GA
|GA
|GA

|PodDisruptionBudget
|GA
|GA
|GA

|Adding kernel modules to nodes with kvc
|TP
|TP
|

|Egress router CNI plug-in
|GA
|GA
|GA

|Scheduler profiles
|TP
|GA
|GA

|Non-preempting priority classes
|TP
|TP
|

|Kubernetes NMState Operator
|TP
|TP
|-

|Assisted Installer
|TP
|TP
|

|AWS Security Token Service (STS)
|GA
|GA
|GA

|Kdump
|TP
|TP
|

|OpenShift Serverless
|GA
|GA
|GA

|Serverless functions
|TP
|TP
|

|Data Plane Development Kit (DPDK) support
|TP
|GA
|GA

|Memory Manager feature
|-
|TP
|

|CNI VRF plug-in
|TP
|GA
|GA

|Cluster Cloud Controller Manager Operator
|-
|GA
|GA

|Cloud controller manager for Alibaba Cloud
|-
|-
|GA

|Cloud controller manager for Amazon Web Services
|-
|TP
|TP

|Cloud controller manager for Google Cloud Platform
|-
|-
|TP

|Cloud controller manager for IBM Cloud
|-
|-
|GA

|Cloud controller manager for Microsoft Azure
|-
|TP
|TP

|Cloud controller manager for Microsoft Azure Stack Hub
|-
|GA
|GA

|Cloud controller manager for {rh-openstack-first}
|-
|TP
|TP

|Cloud controller manager for VMware vSphere
|-
|-
|TP

|Driver Toolkit
|TP
|TP
|

|Special Resource Operator (SRO)
|-
|TP
|

|Node Health Check Operator
|-
|TP
|

| Low-latency Redfish hardware events
|-
|-
|TP

|Pruning replica sets using the `--replica-sets` flag
|-
|-
|TP

|====

[id="ocp-4-10-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.8 and beyond! This needs some SME/QE review before being updated for 4.9.
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to 4.8, you can either revoke or continue to allow unauthenticated access. It is recommended to revoke unauthenticated access unless there is a specific need for it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* The assignment of egress IP addresses to control plane nodes with the EgressIP feature is not supported on a cluster provisioned on Amazon Web Services (AWS). (link:https://bugzilla.redhat.com/show_bug.cgi?id=2039656[*BZ#2039656*])

* Previously, there was a race condition between {rh-openstack-first} credentials secret creation and `kube-controller-manager` startup. As a result, {rh-openstack-first} cloud provider would not be configured with {rh-openstack} credentials and would break support when creating Octavia load balancers for `LoadBalancer` services. To work around this, you must restart the `kube-controller-manager` pods by deleting the pods manually from the manifests. When you use the workaround, the `kube-controller-manager` pods restart and {rh-openstack} credentials are properly configured. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2004542[*BZ#2004542*])


[id="ocp-4-10-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified via email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster-within-minor.adoc#updating-cluster-within-minor[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-10-0-ga"]
=== RHSA-2021:3759 - {product-title} 4.10.0 image release, bug fix, and security update advisory

Issued: TBD

{product-title} release 4.10.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2021:3759[RHSA-2021:3759] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2021:3758[RHSA-2021:3758] advisory.

Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:

link:https://access.redhat.com/solutions/6415591[{product-title} 4.10.0 container image list]
