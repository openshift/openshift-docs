[[release-notes-ocp-3-9-release-notes]]
= {product-title} 3.9 Release Notes
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

Red Hat {product-title} is a Platform as a Service (PaaS) that provides
developers and IT organizations with a cloud application platform for deploying
new applications on secure, scalable resources with minimal configuration and
management overhead. {product-title} supports a wide selection of
programming languages and frameworks, such as Java, Ruby, and PHP.

Built on Red Hat Enterprise Linux and Kubernetes, {product-title}
provides a secure and scalable multi-tenant operating system for today’s
enterprise-class applications, while providing integrated application runtimes
and libraries. {product-title} brings the OpenShift PaaS platform to customer
data centers, enabling organizations to implement a private PaaS that meets
security, privacy, compliance, and governance requirements.

[[ocp-39-about-this-release]]
== About This Release

Red Hat {product-title} version 3.9
(link:https://access.redhat.com/errata/RHBA-2018:0489[RHBA-2018:0489]) is now
available. This release is based on
link:https://github.com/openshift/origin/releases/tag/v3.9.0[OpenShift
Origin 3.9]. New features, changes, bug fixes, and known issues that pertain to
{product-title} 3.9 are included in this topic.

To better synchronize versions of {product-title} with Kubernetes, Red Hat did
not publicly release {product-title} 3.8 and, instead, is releasing
{product-title} 3.9 directly after version 3.7. See
xref:ocp-39-installation[Installation] for information on how this impacts
installation and upgrade processes.

{product-title} 3.9 is supported on RHEL 7.3, 7.4, and 7.5 with the latest packages
from Extras, including Docker 1.13. It is also supported on Atomic Host 7.4.5
and newer. The *docker-latest* package is now deprecated.

For initial installations, see the
xref:../install_config/install/planning.adoc#install-config-install-planning[Installing
a Cluster] topics in the
xref:../install_config/index.adoc#install-config-index[Installation and
Configuration] documentation.

To upgrade to this release from a previous version, see the
xref:../upgrading/index.adoc#install-config-upgrading-index[Upgrading Clusters]
topic.

[[ocp-39-new-features-and-enhancements]]
== New Features and Enhancements

This release adds improvements related to the following components and concepts.

[[ocp-39-container-orchestration]]
=== Container Orchestration

[[ocp-39-soft-image-pruning]]
==== Soft Image Pruning

Now, when pruning images, you do not have to remove the actual image, just
update etcd storage.

It is safer to run `--keep-tag-revisions` and `--keep-younger-than`. After this
is run, administrators can choose to run hard prune (which is safe to run as
long as the registry is put in read-only mode).

[[ocp-39-cloudforms]]
==== Red Hat CloudForms Management Engine 4.6 Container Management

The installation playbooks in {product-title} 3.9 have been updated to support
Red Hat CloudForms Management Engine (CFME) 4.6, which is now currently
available. See the new
xref:../install_config/cfme/index.adoc#install-config-cfme-intro[Deploying Red Hat CloudForms on OpenShift Container Platform] topics for further information.

In addition, this release includes the following new features and updates:

* {product-title} template provisioning
* Offline OpenScapScans
* Alert management: You can choose Prometheus (currently in Technology Preview) and use it in CloudForms.
* Reporting enhancements
* Provider updates
* Chargeback enhancements
* UX enhancememts

[[ocp-39-crio]]
==== CRI-O v1.9

CRI-O is a lightweight, native Kubernetes container runtime interface. By
design, it provides only the runtime capabilities needed by the kubelet. CRI-O is
designed to be part of Kubernetes and evolve in lock-step with the platform.

CRI-O brings:

* A minimal and secure architecture.
* Excellent scale and performance.
* The ability to run any Open Container Initiative (OCI) or docker image.
* Familiar operational tooling and commands.

image::crio-3-7.png[CRI-O]

To install and run CRI-O alongside `docker`, set the following in the
`[OSEv3:vars]` section
xref:../install_config/install/advanced_install.adoc#configuring-ansible[Ansible inventory file] during cluster installation:

----
openshift_use_crio=true
openshift_crio_use_rpm=true <1>
----
<1> CRI-O can only be installed as an RPM. The previously-available system container
for CRI-O has been dropped from xref:ocp-39-technology-preview[Technology
Preview] as of {product-title} 3.9.

[NOTE]
====
The `atomic-openshift-node` service must be RPM- or system container-based when
using CRI-O; it cannot be `docker` container-based. The installer protects
against using CRI-O with `docker` container nodes and will halt installation if
detected.
====

When CRI-O use is enabled, it is installed alongside `docker`, which currently
is required to perform build and push operations to the registry. Over time,
temporary `docker` builds can accumulate on nodes. You can optionally set the
following to enable garbage collection, which adds a daemonset to clean out the
builds:

----
openshift_crio_enable_docker_gc=true
----

When enabled, it will run garbage collection on all nodes by default. You can
also limit the running of the daemonset on specific nodes by setting the
following:

----
openshift_crio_docker_gc_node_selector={'runtime': 'cri-o'}
----

For example, the above would ensure it is only run on nodes with the `runtime:
cri-o` label. This can be helpful if you are running CRI-O only on
xref:../install_config/build_defaults_overrides.adoc#ansible-setting-global-build-defaults[some
nodes], and others are only running `docker`.

See the link:http://cri-o.io/[upstream documentation] for more information on
CRI-O.

[[ocp-39-storage]]
=== Storage

[[ocp-39-pv-resize]]
==== PV Resize

You can expand persistent volume claims online from {product-tile} for CNS
glusterFS, Cinder, and GCE PD.

. Create a storage class with `allowVolumeExpansion=true`.
. The PVC uses the storage class and submits a claim.
. The PVC specifies a new increased size.
. The underlying PV is resized.

[[ocp-39-end-to-end-online-expansion-resize-for-cns-glusterfs-pvs]]
==== End-to-end Online Expansion and Resize for Containerized GlusterFS PV

You can expand persistent volume claims online from {product-tile} for CNS
glusterFS volumes.

This can be done online from {product-title}. Previously, this was only
available from the Heketi CLI. You edit the PVC with the new size, triggering a
PV resize. This is fully qualified for glusterFs backed PVs. Gluster-block PV
resize was added with RHEL 7.5.

. Add `allowVolumeExpansion=true` to the storage class.
. Run:
+
----
$ oc edit pvc claim-name
----

. Edit the `spec.resources.requests.storage` field with the new value.

[[ocp-container-native-storage-glusterfs-pv-consumption-metrics-available]]
==== Container Native Storage GlusterFS PV Consumption Metrics Available from {product-title}

Container Native Storage GlusterFS is extended to provide volume metrics
(including consumption) through Prometheus or Query.

Metrics are available from the PVC endpoint. This adds visibility to what is
being allocated and what is being consumed. Previously, you could only see
allocated size of the PVs. Now, you know how much is really consumed so, if
needed, you can expand it before it runs out of space. This also allows
administrators to do billing based on consumption, if needed.

Examples of added metrics include:

* `kubelet_volume_stats_capacity_bytes`
* `kubelet_volume_stats_inodes`
* `kubelet_volume_stats_inodes_free`
* `kubelet_volume_stats_inodes_used`
* `kubelet_volume_stats_used_bytes`

[[ocp-3-9-automated-cns-deployments-with-advanced-installation]]
==== Automated CNS Deployment with {product-title} Advanced Installation

In the {product-title} advanced installer, the CNS block provisioner deployment
is fixed and the CNS Un-install Playbook is added. This resolves the issue of CNS
block deployment with {product-title} and also provides a way to uninstall a failed
installation of CNS.

CNS storage device details are added to the installer’s inventory file. The
advanced installer manages configuration and deployment of CNS, file and block
provisioners, registry, and ready-to-use PVs.

[[ocp-39-scale]]
=== Scale

[[ocp-39-scale-cluster-limits]]
==== Cluster Limits

Updated guidance around
xref:../scaling_performance/cluster_limits.adoc#scaling-performance-cluster-limits[Cluster
Limits] for {product-title} 3.9 is now available.

[[ocp-39-device-plugins]]
==== Device Plug-ins (Technology Preview)

This is a feature currently in
xref:ocp-39-technology-preview[Technology Preview] and not for production
workloads.

Device plug-ins allow you to use a particular device type (GPU, InfiniBand,
or other similar computing resources that require vendor-specific initialization
and setup) in your {product-title} pod without needing to write custom code. The
device plug-in provides a consistent and portable solution to consume hardware
devices across clusters. The device plug-in provides support for these devices
through an extension mechanism, which makes these devices available to
containers, provides health checks of these devices, and securely shares them.

A device plug-in is a gRPC service running on the nodes (external to
`atomic-openshift-node.service`) that is responsible for managing specific
hardware resources.

See the  xref:../dev_guide/device_plugins.adoc#using-device-plugins[Developer
Guide] for further conceptual information about Device Plug-ins.

[[ocp-39-CPU-manager]]
==== CPU Manager (Technology Preview)

CPU Manager is a feature currently in
xref:ocp-39-technology-preview[Technology Preview] and not for production
workloads.

CPU Manager manages groups of CPUs and constrains workloads to specific CPUs.

CPU Manager is useful for workloads that have some of these attributes:

* Require as much CPU time as possible.
* Are sensitive to processor cache misses.
* Are low-latency network applications.
* Coordinate with other processes and benefit from sharing a single processor
cache.

See
xref:../scaling_performance/using_cpu_manager.adoc#scaling-performance-using-cpu-manager[Using
CPU Manager] for more information.

[[ocp-39-device-manager]]
==== Device Manager (Technology Preview)

Device Manager is a feature currently in
xref:ocp-39-technology-preview[Technology Preview] and not for production
workloads.

Some users want to set resource limits for hardware devices within their pod
definition and have the scheduler find the node in the cluster with those
resources.  While at the same time, Kubernetes needed a way for hardware
vendors to advertise their resources to the kubelet without forcing them to
change core code within Kubernetes

The kubelet now houses a device manager that is extensible through plug-ins. You
load the driver support at the node level. Then, you or the vendor writes a
plug-in that listens for requests to stop/start/attach/assign the requested
hardware resources seen by the drivers. This plug-in is deployed to all the
nodes via a daemonSet.

See xref:../dev_guide/device_manager.adoc#using-device-manager[Using Device
Manager] for more information.

[[ocp-39-hugepages]]
==== Huge Pages (Technology Preview)

Huge pages is a feature currently in xref:ocp-39-technology-preview[Technology
Preview] and not for production workloads.

Memory is managed in blocks known as pages. On most systems, a page is 4Ki. 1Mi
of memory is equal to 256 pages; 1Gi of memory is 256,000 pages, and so on. CPUs
have a built-in memory management unit that manages a list of these pages in
hardware. The Translation Lookaside Buffer (TLB) is a small hardware cache of
virtual-to-physical page mappings. If the virtual address passed in a hardware
instruction can be found in the TLB, the mapping can be determined quickly. If
not, a TLB miss occurs, and the system falls back to slower, software-based
address translation, resulting in performance issues. Since the size of the
TLB is fixed, the only way to reduce the chance of a TLB miss is to increase the
page size.

A huge page is a memory page that is larger than 4Ki. On x86_64 architectures,
there are two common huge page sizes: 2Mi and 1Gi. Sizes vary on other
architectures. In order to use huge pages, code must be written so that
applications are aware of them. Transparent Huge Pages (THP) attempt to automate
the management of huge pages without application knowledge, but they have
limitations. In particular, they are limited to 2Mi page sizes. THP can lead to
performance degradation on nodes with high memory utilization or fragmentation
due to defragmenting efforts of THP, which can lock memory pages. For this
reason, some applications may be designed to (or recommend) usage of
pre-allocated huge pages instead of THP.

In {product-title}, applications in a pod can allocate and consume pre-allocated
huge pages.

See xref:../scaling_performance/managing_hugepages.adoc#scaling-performance-managing-huge-pages[Managing
Huge Pages] for more information.

[[ocp-39-networking]]
=== Networking

[[ocp-39-semi-automatic-namespace-wide-egress-IP]]
==== Semi-automatic Namespace-wide Egress IP
All outgoing external connections from a project share a single, fixed source IP
address and send all traffic via that IP, so that external firewalls can
recognize the application associated with a packet.

It is _semi-automatic_ in that in the first half of implementing the automatic
namespace-wide egress IP feature, it implements the "traffic" side. Namespaces
with automatic egress IPs will send all traffic via that IP. However, it does
not implement the "management" side. Nothing automatically assigns egress IPs to
nodes yet. The administrator must do that manually.

See
xref:../admin_guide/managing_networking.adoc#admin-guide-manage-networking[Managing
Networking] for more information.

[[ocp-39-support-our-own-haproxy-rpm-for-consumption-by-the-router]]
==== Support Our Own HAProxy RPM for Consumption by the Router

Route configuration changes and process upgrades performed under heaving load
have typically required a stop and start sequence of certain services, causing
temporary outages.

In {product-title} 3.9, HAProxy 1.8 sees no difference between updates and
upgrades; a new process is used with a new configuration, and the listening
socket’s file descriptor is transferred from the old to the new process so the
connection is never closed.  The change is seamless, and enables our ability to
do things, like HTTP/2, in the future.

[[ocp-39-master]]
=== Master

[[ocp-39-statefulsets-daemonsets-deployments]]
====  StatefulSets, DaemonSets, and Deployments Now Supported

In {product-title}, statefulsets, daemonsets, and deployments are now stable,
supported, and out of Technology Preview.

[[ocp-39-central-audit-capability]]
==== Central Audit Capability

Provides auditing of items that administrators would like to see, including:

* The event timestamp.
* The activity that generated the entry.
* The API endpoint that was called.
* The HTTP output.
* The item changed due to an activity, with details of the change.
* The user name of the user that initiated an activity.
* The name of the namespace the event occurred in, where possible.
* The status of the event, either success or failure.

Provides auditing of items that administrators would like to trace, including:

* User login and logout from (including session timeout) the web interface,
including unauthorized access attempts.
* Account creation, modification, or removal.
* Account role or policy assignment or de-assignment.
* Scaling of pods.
* Creation of new project or application.
* Creation of routes and services.
* Triggers of builds and/or pipelines.
* Addition or removal or claim of persistent volumes.

Set up auditing in the *_master-config file_*, and restart the *master-config*
service:

----
auditConfig:
  auditFilePath: "/var/log/audit-ocp.log"
  enabled: true
  maximumFileRetentionDays: 10
  maximumFileSizeMegabytes: 10
  maximumRetainedFiles: 10
  logFormat: json
  policyConfiguration: null
  policyFile: /etc/origin/master/audit-policy.yaml
  webHookKubeConfig: ""
  webHookMode:
----

Example log output:

----
{"kind":"Event","apiVersion":"audit.k8s.io/v1beta1","metadata":{"creationTimestamp":"2017-09-29T09:46:39Z"},"level":"Metadata","timestamp":"2017-09-29T09:46:39Z","auditID":"72e66a64-c3e5-4201-9a62-6512a220365e","stage":"ResponseComplete","requestURI":"/api/v1/securitycontextconstraints","verb":"create","user":{"username":"system:admin","groups":["system:cluster-admins","system:authenticated"]},"sourceIPs":["10.8.241.75"],"objectRef":{"resource":"securitycontextconstraints","name":"scc-lg","apiVersion":"/v1"},"responseStatus":{"metadata":{},"code":201}}
----

[[ocp-39-add-support-for-deployments-to-oc-status]]
==== Add Support for Deployments to oc status

The `oc status` command provides an overview of the current project. This
provides similar output for upstream deployments as can be seen for downstream
DeploymentConfigs, with a nested deployment set:

----
$ oc status
In project My Project (myproject) on server https://127.0.0.1:8443

svc/ruby-deploy - 172.30.174.234:8080
  deployment/ruby-deploy deploys istag/ruby-deploy:latest <-
    bc/ruby-deploy source builds https://github.com/openshift/ruby-ex.git on istag/ruby-22-centos7:latest
      build #1 failed 5 hours ago - bbb6701: Merge pull request #18 from durandom/master (Joe User <joeuser@users.noreply.github.com>)
    deployment #2 running for 4 hours - 0/1 pods (warning: 53 restarts)
    deployment #1 deployed 5 hours ago
----

Compare this to the output from {product-title} 3.7:

----
$ oc status
In project dc-test on server https://127.0.0.1:8443

svc/ruby-deploy - 172.30.231.16:8080
  pod/ruby-deploy-5c7cc559cc-pvq9l runs test
----

[[ocp-39-dynamic-admission-controller-follow-up]]
==== Dynamic Admission Controller Follow-up (Technology Preview)

Dynamic Admission Controller Follow-up is a feature currently in
xref:ocp-39-technology-preview[Technology Preview] and not for production
workloads.

An admission controller is a piece of code that intercepts requests to the
Kubernetes API server prior to persistence of the object, but after the request
is authenticated and authorized. Example use cases include mutation of pod
resources and security response.

See
xref:../architecture/additional_concepts/dynamic_admission_controllers.adoc#architecture-additional-concepts-dynamic-admission-controllers[Custom
Admission Controllers] for more information.

[[ocp-39-feature-gates]]
==== Feature Gates

Platform administrators now have the ability to turn off specific features to the
entire platform. This assists in the control of access to alpha, beta, or
Technology Preview features in production clusters.

link:https://kubernetes.io/docs/reference/feature-gates/[Feature gates] use a
key=value pair in the master and kubelet configuration files that describe the
feature you want to block.

.Control Plane:  master-config.yaml
----
kubernetesMasterConfig:
  apiServerArguments:
    feature-gates:
    - CPUManager=true
----

.kubelet:  node-config.yaml
----
kubeletArguments:
  feature-gates:
  - DevicePlugin=true
----

[[ocp-39-installation]]
=== Installation

[[ocp-3-9-improved-playbook-performance]]
==== Improved Playbook Performance

{product-title} 3.9 introduces significant refactoring and restructuring of the
playbooks to improve performance. This includes:

* Restructured playbooks to push all fact-gathering and common dependencies up
into the initialization plays so they are only called once rather than each time
a role needs access to their computed values.

* Refactored playbooks to limit the hosts they touch to only those that are truly
relevant to the playbook.

[[ocp-3-9-quick-installation]]
==== Quick Installation (Deprecated)

Quick Installation is now deprecated in {product-title} 3.9 and will be
completely removed in a future release.

Quick installation will only be capable of installing 3.9. It will not be able
to upgrade from 3.7 or 3.8 to 3.9.

[[ocp-3-9-automated-control-plane-upgrade]]
==== Automated 3.7 to 3.9 Control Plane Upgrade

The installer automatically handles stepping the control plane from 3.7 to 3.8
to 3.9 and node upgrade from 3.7 to 3.9.

Control plane components (API, controllers, and nodes on control plane hosts)
are upgraded seamlessly from 3.7 to 3.8 to 3.9. Data migration happens pre- and
post- {product-title} 3.8 and 3.9 control plane upgrades. Other control plane
components (router, registry, service catalog, and brokers) are upgraded from
{product-title} 3.7 to 3.9. Nodes (node, docker, ovs) are upgraded directly from
{product-title} 3.7 to 3.9 with only one drain of nodes. {product-title} 3.7
nodes operate indefinitely against 3.8 masters should the upgrade process need
to pause in this state. Logging and metrics are updated from {product-title} 3.7
to 3.9.

It is recommended that you upgrade the control plane and nodes independently.
You can still perform the upgrade through an all-in-one playbook, but rollback
is more difficult. Playbooks do not allow for a clean installation of
{product-title} 3.8.

See xref:../upgrading/index.adoc#install-config-upgrading-index[Upgrading
Clusters] for more information.

[[ocp-39-metrics-and-logging]]
=== Metrics and Logging

[[ocp-39-syslog-output-plugin-for-fluentd]]
==== syslog Output Plug-in for fluentd (Technology Preview)

syslog Output Plug-in for fluentd is a feature currently in
xref:ocp-39-technology-preview[Technology Preview] and not for production
workloads.

You can send system and container logs from {product-title} nodes to external
endpoints using the syslog protocol. The fluentd syslog output plug-in supports
this.

[IMPORTANT]
====
Logs sent via syslog are not encrypted and, therefore, insecure.
====

See
xref:../install_config/aggregate_logging.adoc#sending-logs-to-external-rsyslog[Sending
Logs to an External Syslog Server] for more information.

[[ocp-39-prometheus]]
==== Prometheus (Technology Preview)

Prometheus remains in xref:ocp-39-technology-preview[Technology Preview] and is
not for production workloads. Prometheus, AlertManager, and AlertBuffer versions
are now updated and node-exporter is now included:

* prometheus 2.1.0
* Alertmanager 0.14.0
* AlertBuffer 0.2
* node_exporter 0.15.2

You can deploy Prometheus on an {product-title} cluster, collect Kubernetes and
infrastructure metrics, and get alerts. You can see and query metrics and alerts
on the Prometheus web dashboard. Alternatively, you can bring your own Grafana
and hook it up to Prometheus.

See xref:../install_config/cluster_metrics.adoc#openshift-prometheus[Prometheus
on OpenShift] for more information.

[[ocp-39-developer-experience]]
=== Developer Experience

[[ocp-39-memory-usage-improvements]]
==== Jenkins Memory Usage Improvements

Previously, Jenkins worker pods would often consume too much or too little
memory. Now, a startup script intelligently looks at pod limits and environment
variables are appropriately set to ensure limits are respected for spawned JVMs.

[[ocp-39-cli-plug-ins]]
==== CLI Plug-ins

CLI plug-ins are now fully supported.

Usually called _plug-ins_ or _binary extensions_, this feature allows you to
extend the default set of `oc` commands available and, therefore, allows you to
perform new tasks.

See xref:../cli_reference/extend_cli.adoc#cli-reference-extend-cli[Extending the
CLI] for information on how to install and write extensions for the CLI.

[[ocp-39-ability-to-specify-tolerations]]
==== Ability to Specify Default Tolerations via the buildconfig Defaulter

Previously, there was not a way to set a default toleration on build pods so
they could be placed on build-specific nodes. The build defaulter is now updated
to allow the specification of a toleration value, which is applied to the build
pod upon creation.

See
xref:../install_config/build_defaults_overrides.adoc#install-config-build-defaults-overrides[Configuring
Global Build Defaults and Overrides] for more information.

[[ocp-39-web-console]]
=== Web Console

[[ocp-39-catalog-from-within-project-view]]
==== Catalog from within Project View

Quickly get to the catalog from within a project by clicking *Catalog* in the
left navigation.

image::3.9-console-catalog-tab.png[Catalog tab]

[[ocp-39-quickly-search-the-catalog]]
==== Quickly Search the Catalog from within Project View

To quickly find services from within project view, type in your search criteria.

image::3.9-console-catalog-search.png[Search the catalog]

[[ocp-39-select-preferred-home-page]]
==== Select Preferred Home Page

You can now jump straight to certain pages after login. Access the menu from
the account dropdown, choose your option, then log out, then log back in.

image::3.9-console-set-custom-home-page.gif[Set preferred home page]

[[ocp-39-configurable-inactivity-timeout]]
==== Configurable Inactivity Timeout

You can now configure the web console to log users out after a set timeout. The
default is `0` (never).
xref:../install_config/install/advanced_install.adoc#configuring-web-console-customization[Set
the Ansible variable] to the number of minutes:

----
openshift_web_console_inactivity_timeout_minutes=n
----

[[ocp-39-console-as-a-separate-pod]]
==== Web Console as a Separate Pod

The web console is now separated out of the API server. The web console is
packaged as a container image and deployed as a pod. Configure via the
ConfigMap. Changes are auto-detected.

Masters are now schedulable and required to be schedulable for the web consoles
deployments to work.

[[ocp-39-notable-technical-changes]]
== Notable Technical Changes

{product-title} 3.9 introduces the following notable technical changes.

[discrete]
[[ocp-39-manual-upgrade-process-now-unsupported]]
=== Manual Upgrade Process Now Unsupported

As of {product-title} 3.9,
xref:../upgrading/manual_upgrades.adoc#install-config-upgrading-manual-upgrades[manual
upgrades] are not supported. In a future release, this process will be removed.

[discrete]
[[ocp-39-schedulable-masters]]
=== Masters Marked as Schedulable Nodes by Default

In previous versions of {product-title}, master hosts were marked as
unschedulable nodes by default by the installer, meaning that new pods could not
be placed on the hosts. Starting with {product-title} 3.9, however, masters are
marked schedulable automatically during installation and upgrade. This change is
mainly so that the web console, which used to run as part of the master itself,
can instead be run as a pod deployed to the master.

[discrete]
[[ocp-39-default-node-selector]]
=== Default Node Selector Set By Default and Automatic Node Labeling

Starting in {product-title} 3.9, masters are now marked as schedulable nodes by
default. As a result, the default node selector (defined in the master
configuration file's `projectConfig.defaultNodeSelector` field to determine
which node that projects will use by default when placing pods, and previously
left blank by default) is now set by default during cluster installations and
upgrades. It is set to `node-role.kubernetes.io/compute=true` unless overridden
using the `osm_default_node_selector` Ansible variable.

In addition, whether `osm_default_node_selector` is set or not, the following
automatic labeling occurs for hosts defined in your inventory file during
installations and upgrades:

- non-master, non-dedicated infrastructure nodes hosts (by default, this means
nodes with a `region=infra` label) are labeled with
`node-role.kubernetes.io/compute=true`, which assigns the `compute` node role.
- master nodes are labeled with `node-role.kubernetes.io/master=true`, which
assigns the `master` node role.

This ensures that the default node selector has available nodes to choose from
when determining pod placement. See
xref:../install_config/install/advanced_install.adoc#configuring-node-host-labels[Configuring Node Host Labels] for more details.

[discrete]
[[ansible-must-be-installed]]
=== Ansible Must Be Installed via the rhel-7-server-ansible-2.4-rpms Channel

Starting in {product-title} 3.9, Ansible must be installed via the
`rhel-7-server-ansible-2.4-rpms` channel, which is included in RHEL
subscriptions.

[discrete]
[[ocp-39-several-oc-secrets-subcommands-now-deprecated]]
=== Several oc secrets Subcommands Now Deprecated

{product-title} 3.9 deprecates the following `oc secrets` subcommands in favor
of `oc create secret`:

* `new`
* `new-basicauth`
* `new-dockercfg`
* `new-sshauth`

[discrete]
[[updated-default-installer-values]]
=== Updated Default Values for template_service_broker_prefix and template_service_broker_image_name in the Installer

Default values for `template_service_broker_prefix` and
`template_service_broker_image_name` in installer have been updated to be
consistent with other settings.

Previous values are:

    * `template_service_broker_prefix="registry.example.com/openshift3/"`
    * `template_service_broker_image_name="ose-template-service-broker"`

New values are:

    * `template_service_broker_prefix="registry.example.com/openshift3/ose-"`
    * `template_service_broker_image_name="template-service-broker"`

[discrete]
[[removed-become-no-instances]]
=== Removed Several Instances of 'become: no' on Certain Tasks and Playbooks Inside of openshift-anisble

In an effort to provide greater flexibility for users, several instances of
`become: no` on certain tasks and playbooks inside of `openshift-anisble` are
now removed. These statements were primarily applied on `local_action` and
`delegate_to: localhost` commands for creating temporary files on the host
running Ansible.

If a user is running Ansible from a host that does not allow password-less
`sudo`, some of these commands may fail if you run the `ansible-playbook` with
the `-b` (`become`) command line switch, or if it has `ansible_become=True`
applied to the local host in the inventory or `group_vars`.

Elevated permissions are not required on the local host when running
`openshift-ansible` plays.

If target hosts (where {product-title} is being deployed) require the use of
`become`, it is recommended that you add `ansible_become=True` for those hosts
or groups in inventory or `group_vars`/`host_vars`.

If the user is running as root on the local host or connection to the root user
on the remote hosts instead of using become, then you should not notice a change.

[discrete]
[[unqualified-image-specs]]
=== Unqualified Image Specifications

Unqualified image specifications now default to `docker.io` and require API
server configuration to resolve to different registries.

[discrete]
[[ScheduledJob-objects-not-supported]]
=== batch/v2alpha1 ScheduledJob Objects Are No Longer Supported

The `batch/v2alpha1 ScheduledJob` objects are no longer supported. Use CronJobs
instead.

[discrete]
[[autoscaling-API-group-removed]]
===  The autoscaling/v2alpha1 API Group Is Removed

The `autoscaling/v2alpha1` API group has been removed

[discrete]
[[start-node-requires-swap-to-be-disabled]]
=== Start Node Requires Swap to be Disabled

For new installations of {product-title} 3.9 , disabling swap is a strong
recommendation. For {product-title} 3.8, the {product-title} start node requires
swap to be disabled. This is already done as part of the Ansible node
installation.

[discrete]
[[oadm-deprecated]]
=== oadm Command Is Deprecated

The `oadm` command is now deprecated. Use `oc adm` instead.

[discrete]
[[statefulsets-daemonsets-seployments-now-fully-supported]]
=== StatefulSets, DaemonSets, and Deployments Now Fully Supported

The core workloads API, which is composed of the `DaemonSet`, `Deployment`,
`ReplicaSet`, and `StatefulSet kinds`, has been promoted to GA stability in the
`apps/v1` group version. As such, the` apps/v1beta2` group version is
deprecated, and all new code should use the kinds in the apps/v1 group version.
For {product-title} this means the statefulsets, daemonsets, and deployments are
now stable and supported.

[discrete]
[[admin-solutions-guide-removed]]
=== Administrator Solutions Guide Removed

In {product-title} 3.9, the Administrator Solutions guide is removed from the
{product-title} documentation. See the
xref:../day_two_guide/index.adoc#day-two-guide-index[Day Two Operations Guide]
instead.

[[ocp-39-bug-fixes]]
== Bug Fixes

This release fixes bugs for the following components:

*Builds*

* Previously, builds selected the secret to be used for pushing the output image
at the time they were started. When a build started before the default service
account secrets for a project were created, the build may not have found a
suitable secret for pushing the image, resulting in the build failing when it
went to push the image. With this fix, the build is held until the default
service account secrets exist, ensuring that if the default secret is suitable
for pushing the image, it can and will be used. As a result, initial builds in a
newly created project are no longer at risk of failing if the build is created
before the default secrets are populated.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1333030[*BZ#1333030*])

*Command Line Interface*

* The `systemd` units for masters changed without the diagnostics being updated.
This caused the diagnostics to silently check for master `systemd` units that
did not exist, and problems were not reported. With this fix, diagnostics check
for correct master unit names and problems with master `systemd` units and logs
may be found.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1378883[*BZ#1378883*])

*Containers*

* If a container shares namespace with another container, then they would share
the namespace path. If you run the `exec` command in the first container, it
only reads the namespace paths stored in the file and joins those namespaces.
So, if the second container has already been stopped, the `exec` command in the
first container will fail. As a result, this fix saves namespace paths no matter
if containers share namespaces.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1510573[*BZ#1510573*])

*Images*

* Docker has a known "zombie process" phenomenon that impacted the OpenShift
Jenkins image, causing operating system-level resources to be exhausted as these
“zombie processes” accumulated. With this fix, the OpenShift Jenkins image now
leverages one of the Docker image `init` implementations to launch Jenkins,
monitor, and handle any “zombie child processes”. As a result, “zombie
processes” no longer accumulate.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1528548[*BZ#1528548*])

* Due to a fault in the scheduler implementation, the
`ScheduledImageImportMinimumIntervalSeconds` setting was not correctly observed,
causing {product-title} to attempt to import scheduled images at the wrong
intervals. This is now resolved.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1543446[*BZ#1543446*])

* Previously, OpenShift would erroneously re-import all tags on an image stream,
regardless if marked as scheduled or not, if any tag on the image stream was
marked as scheduled. This behavior is now resolved.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1515060[*BZ#1515060*])

*Image Registry*

* The signature importer tried to import signatures from the internal registry
without credentials, causing the registry to check if the anonymous user could
get signatures using SAR requests. With this bug fix, the signature importer skips
the internal registry because the internal registry and the signature importer
work with the same storage, resulting in no SAR requests.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1543122[*BZ#1543122*])

* There was no check of the number of components in the path, causing the data to
be placed in the storage but not be written to the database. With this bug fix, an
early check of the path was added.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1528613[*BZ#1528613*])

*Installer*

* The Kubernetes service IP address was not added to `no_proxy` list for the
docker-registry during installation. As a result, internal registry requests
would be forced to use the proxy, preventing logins and pushes to the internal
registry. The installer was changed to add the Kubernetes service IP to the
`no_proxy` list.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1504464[*BZ#1504464*])

* The installer was pulling the incorrect efs-provisioner image, which caused the
installation of the provisioner pod to fail to deploy. The installer was changed
to pull the correct image.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1523534[*BZ#1523534*])

* When installing {product-title} with a custom registry, the installer was using
the default registry. The registry console default image is now defined as a
fully qualified image `registry.access.redhat.com/openshift3/registry-console`
which means that when a custom registry is specified via `oreg_url` and image
streams are modified to use that custom registry the registry console will also
utilize the custom registry.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1523638[*BZ#1523638*])

* Running the *_redeploy-etcd-ca.yml_* playbook did not update the `ca.crt` used
by etcd system container. The code was changed so that the playbook properly
updates the the etcd ca.crt in *_/etc/etcd/ca.crt_* as expected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1466216[*BZ#1466216*])

* Following a successful deployment of CNS/CRS with glusterblock, {product-title}
 logging and metrics can be deployed using glusterblock as their backend storage
 for fault-tolerant, distributed persistent storage.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1480835[*BZ#1480835*])

* When upgrading from 3.6 to 3.7, the user wanted the Hawkular OpenShift Agent
pods deactivated. But, after upgrade, the HOSA pods are still being deployed. A
new playbook, *uninstall_hosa.yaml*, has been created to remove HOSA from a
{product-title} cluster when `openshift_metrics_install_hawkular_agent=false` in
the Ansible inventory file.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1497408[*BZ#1497408*])

* Because registry credentials for the broker were stored in a ConfigMap,
sensitive credentials could be exposed in plain text. A secret is now created to
store the credentials Registry credentials are no longer visible in plaintext.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1509082[*BZ#1509082*])

* Because of incorrect naming, the uninstall playbook did not remove the
*tuned-profiles-atomic-openshift-node* package. The playbook is now corrected
and the package is removed upon uninstallation of {product-title}.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1509129[*BZ#1509129*])

* When running the installer with the
`openshift_hosted_registry_storage_volume_size` parameter configured with Jnja
code, the installation failed during persistent volume creation. The code is now
fixed to properly interpret the Jinja code.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1518386[*BZ#1518386*])

* During disconnected installations, the service catalog was attempting to pull
down images from the configured registry. This caused the installation to fail
as the registry is not available during a disconnected installation. The
`imagePullPolicy` in the installer was changed to `ifNotPresent`. If the image
is present, the service catalog will not attempt to pull it again, and the
disconnected installation of the service catalog will proceed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1524805[*BZ#1524805*])

* When provisioning hosts with an SSH proxy configured, the masters would never
appear marked as up. With this bug fix, the task is changed to use an Ansible
module that respects SSH proxy configuration. As a result, Ansible is able to
connect to the hosts and they are marked as up.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1541946[*BZ#1541946*])

* In an HTTPS environment, the service catalog installation was failing because
the playbook attempted to contact the API server using cURL without the
`--noproxy` option specified. The command in the playbook was changed to include
`--noproxy` and the installer performs as expected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1544645[*BZ#1544645*])

* Previously, the storage type for Elasticsearch data centers was not preserved
when upgrading/rerunning. This caused the existing storage type to be
overwritten. This bug fix preserves the storage type as the default (using an
inventory variable if specified).
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1496758[*BZ#1496758*])

* Previously, the docker daemon was incorrectly restarted when redeploying node
certificates. This caused unnecessary downtime in nodes since
`atomic-openshift-node` was the only component loading the kubeconfig. This bug
fix adds a flag to check if a new Certificate Authority (CA) is being deployed.
If not, then restarting Docker is skipped.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1537726[*BZ#1537726*])

* Previously, the `docker_image_availability` check did not take into account
variables that override specific container images used for containerized
components. This caused the check to incorrectly report failures when looking
for the default images when the overridden images were actually available. As a
result of this bug fix, the check should accurately report whether the necessary
images are available.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1538806[*BZ#1538806*])

* When determining if a persistent volume claim (PVC) should be created for
Elasticsearch, we used a legacy variable, which did not correctly evaluate if a
PVC was necessary when creating a Network File System (NFS)-backed persistent
volume (PV). This bug fix correctly evaluates if a PVC is necessary for the
deployment configuration.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1538995[*BZ#1538995*])

* Previously, when configuring the registry for Azure Blob storage, the realm of
`core.windows.net` was specified by default. This bug fix allows you to change
`openshift_hosted_registry_storage_azure_blob_realm` to the value that you  want
to use. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1491100[*BZ#1491100*])

* A new playbook has been introduced that uninstalls an existing GlusterFS
deployment. This playbook removes all existing resources, including pods and
services. This playbook also, optionally, removes all data and configuration
from the hosts that were running GlusterFS pods.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1497038[*BZ#1497038*])

*Logging*

* Previously, the {product-title} logging system did not support CRI-O. This bug
fix added a parser for CRI-O formatted logs. As a result, both system and
container logs can be collected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1517605[*BZ#1517605*])

* When redeploying logging, we previously attempted to maintain any changes that
were made to the ConfigMaps post-installation. It was difficult to let users
specify the contents of a ConfigMap file while still needing the ability to
provide the configurations required for the different Elasticsearch, Fluentd,
and Kibana (EFK) stack components. This bug fix created a patch based on changes
made post-deployment and applies that patch to the files provided by the
installer.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1519619[*BZ#1519619*])

*Web Console*

* The Kibana page previously displayed *OPENSHIFT ORIGIN* in the upper left-hand
corner of the {product-title} web console. This bug fix replaces the Origin
header image with the {product-title} header image. As a result, the Kibana page
now displays the desired header.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1546311[*BZ#1546311*])

* Both the {product-title} `DeploymentConfig` and Kubernetes extensions/v1beta1
Deployment resources were labeled with deployment on the web console overview,
so you could not differentiate the resources. `DeploymentConfig` resources on
the *Overview* page are now labelled with `DeploymentConfig`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1488380[*BZ#1488380*])

* The web console's pod status filter did not correctly display pod init status
when an error prevented the pod from initializing, including and init status of
error. If a pod has an `Init:Error` status, the pod status correctly displays
*Init Error* instead of *Pod Initializing*.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1512473[*BZ#1512473*])

* Previously, switching tabs in the web console page for a pipeline build
configuration caused some content on the page to no longer be visible while the
page reloaded. Switching tabs no longer reloads the entire page, and content is
correctly displayed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1527346[*BZ#1527346*])

* By default, an old version of the builder image was shown when you added a
builder to a project and selected by default during builder configuration. This
gave the wrong impression that your only choice was an old version of a language
or framework. The version number is no longer shown in the wizard title, and the
newest available version is selected by default.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1542669[*BZ#1542669*])

* If you used some browsers, you could not consistently use the right click menu
to copy and paste text from internal editors that used the ACE editor library,
including the YAML, Jenkinsfile, and Dockerfile editors. This update uses a
newer version of the ACE editor library, so the right click menu options work
throughout the console.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1463617[*BZ#1463617*])

* Previously, browsers would use the default behavior for the Referrer-Policy
because Referrer-Policy header was not sent by the console. Now the console
correctly sends the Referrer-Policy header, which is set to
`strict-origin-when-cross-origin`, and browsers that listen to the
Referrer-Policy header follow the `strict-origin-when-cross-origin policy` for
the web console.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1504571[*BZ#1504571*])

* Previously, users with read access to the project saw webhook secret values
because they were stored as strings in the build. These users could use these
values to trigger builds even though they had only read access to the project.
Now webhook secrets are defined as secret objects in the build instead of
strings. Users with read only access to the project cannot see the secret values
or use them to trigger builds by using the webhook.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1504819[*BZ#1504819*])

* Previously, adding the same persistent volume claim more than once to a
deployment in the web console caused pods for that deployment to fail. The web
console incorrectly created a new volume when it added the second PVC to the
deployment instead of reusing the existing volume from the pod template spec.
Now, the web console reuses the existing volume if the same PVC is listed more
than once. This behavior lets you add the same PVC with different mount paths
and subpaths as needed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1527689[*BZ#1527689*])

* Previously, it was not clear enough that you can not select an *Image Name* from
the Deploy Image window if you are also creating a new project. The help text
that explains that you can only set an *Image Name* for existing projects is
easier to find.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1535917[*BZ#1535917*])

* Previously, the secrets page in the web console did not display labels. You can
now view the labels for a secret like other resources.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1545828[*BZ#1545828*])

* Sometimes the web console displayed a process template page even if you did not
have permissions to process templates. If you tried to process the template, an
error displayed. Now you can no longer view process templates if you cannot
process them.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1510786[*BZ#1510786*])

* Previously, the *Clear Changes* button did not correctly clear edits to the
*Environment From* variables in the web console environment variable editor. The
button now correctly resets edits to *Environment From* variables.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1515527[*BZ#1515527*])

* By default, dialogs in the web console can be dismissed by clicking in the
negative space surrounding the dialog. IAs a result, the warning dialog could be
inadvertently dismissed. With this bug fix, the warning dialog's configuration
was changed so that it can only be dismissed by clicking one of the buttons in
the dialog. The warning dialog can no longer be inadvertently dismissed by the
user, as clicking one of the dialog's buttons is now required in order to close
the dialog.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1525819[*BZ#1525819*])

*Master*

* Due to a fault in the scheduler implementation, the
`ScheduledImageImportMinimumIntervalSeconds` setting was not correctly observed,
causing {product-title} to attempt to import scheduled images at the wrong
intervals. With this bug fix, the issue is now resolved.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1515058[*BZ#1515058*])

*Networking*

* The {product-title} node was not waiting long enough for the VNID while the
master assigns the VNID and it could take a while to propagate to the node. As a
result, pod creation fails. Increase the timeout from 1 to 5 seconds for
fetching VNID on the node. This bug fix allows pod creation to succeed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1509799[*BZ#1509799*])

* It is now possible to specify a subnet length as part of the `EGRESS_SOURCE`
variable passed to an egress router (for example, `192.168.1.100/24` rather than
`192.168.1.100`). In some network configurations (such as if the gateway address
was a virtual IP that might be backed by one of several physical IPs at
different times), ARP traffic between the egress router and its gateway might
not function correctly if the egress router is not able to send traffic to other
hosts on its local subnet. By specifying `EGRESS_SOURCE` with a subnet length,
the egress router setup script will configure the egress pod in a way that will
work with these network setups.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1527602[*BZ#1527602*])

* In some circumstances, iptables rules could become reordered in a way that would
cause the *per-project static IP address* feature to stop working for some IP
addresses. (For most users, egress IP addresses that ended with an even number
would continue to work, but egress IP addresses ending with an odd number would
fail.) Therefore, external traffic from pods in a project that was supposed to
use a per-project static IP address would end up using the normal node IP
address instead. The iptables rules are changed so that they now have the
expected effect even when they get reordered. With this bug fix, the per-project
static egress IP feature now works reliably.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1527642[*BZ#1527642*])

* Previously, the egress IP initialization code was only run when doing a full SDN
setup, and not when OpenShift services were restarted and found any existing
running SDN. This resulted in failure to create new per-project static egress
IPs (`HostSubnet.EgressIPs`). This issue is now fixed and per-project static
egress IPs works correctly after a node restart.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1533153[*BZ#1533153*])

* Previously, OpenShift was setting colliding host-subnet values, which resulted
in pod IP network to became unavailable across the nodes. This was because the
stale OVS rules were not cleared during node startup. This is now fixed and
the stale OVS rules are cleared on node startup.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1539187[*BZ#1539187*])

* With previous version, if an static IP addressed was removed from a project and
then added back to the same project, it did not worked correctly. This is now
fixed, removing and re-adding static egress IPs works.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1547899[*BZ#1547899*])

* Previously, when OpenShift was deployed on OpenStack, there were few required
`iptables` rules that were not created automatically, which resulted in errors
in pop-to-pod communication between pods on different nodes. The Ansible
OpenShift installer now sets the required `iptables` rules automatically.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1493955[*BZ#1493955*])

* There was a race condition in the startup code that relied on the node setup,
setting a field that the userspace proxy needed. When the network plugin was not
used (or if it was fast) the userspace proxy setup ran sooner and resulted in
reading a nil value for the IP address of the node. Later when the proxy (or the
`unidler` which uses it) was enabled, it would crash because of the nil IP
address value. This issue is now fixed. A retry loop is added that waits for the
IP address value to be set and the userspace proxy and `unidler` work as expected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1519991[*BZ#1519991*])

* In some circumstances, nodes were receiving a duplicate out-of-order HostSubnet
`deleted` event from the master. During processing of this duplicate event, the
node ended up deleting OVS flows corresponding to an active node, disrupting
communications between these two nodes. In the latest version. the HostSubnet
event-processing now checks for and ignores duplicate events. Thus, the OVS
flows are not deleted, and pods communicate normally.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1544903[*BZ#1544903*])

* Previously, the `openshift ex dockergc` command to cleanup docker images, failed
occasionally. This issue is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1511852[*BZ#1511852*])

* Previously, nested secrets did not get mounted in pod. This issue is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1516569[*BZ#1516569*])

* HAproxy versions earlier than version 1.9 dropped new connections during a
reload. This issue is now fixed. By using HAproxy's seamless reload feature,
HAproxy now passes open sockets when reloading, fixing reload issues. fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1464657[*BZ#1464657*])

* There was a spurious error in system logs. The error `Stat fs failed. Error: no
such file or directory` appeared in logs frequently. This was because of calling
the `syscall.Statfs` function in code when the path does not exist. This issue
is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1511576[*BZ#1511576*])

* Previously, a reject routes error message showed up when using router shards.
This issue is now fixed and the rejected routes error messages are now
suppressed in HAproxy if router shards are used.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1491717[*BZ#1491717*])

* Previously, if creating a route with the host set to `localhost`, and if the
`ROUTER_USE_PROXY_PROTOCOL` environment variable was not set to `true`, any
route reloads would fail. This is because the hostname being set to the default
resulted in mismatches in route configurations. The `-H` option is now available
when using `curl`, meaning the health check does not pass the hostname when set
to 'localhost', and routes reload successfully.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1542612[*BZ#1542612*])

* Previously, updating TLS certificates was not possible for cluster
administrators. Because it is an expected task of the cluster administrator, the
role has been changed to update TLS certificates.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1524707[*BZ#1524707*])

*Service Broker*

* Previously, the APBs for MariaDB, PostgreSQL, and MySQL were tagged as
"databases" instead of "database". This is corrected with the tag "database"
matching other services which is now properly shown in search results.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1510804[*BZ#1510804*])

* Async bind and unbind is an experimental feature for the OpenShift Ansible
broker (OAB) and is not supported or enabled by default. Red Hat's officially
released APBs (PostgreSQL, MariaDB, MySQL, and Mediawiki) also do not support
async bind and unbind. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1548997[*BZ#1548997*])

* Previously, the etcd server was not accessible when using the `etcdctl` command.
This was caused by the tcp being set to “0.0.0.0” instead of the expected
`--advertise-client-urls` value of the `asb-etcd` deployment configuration. The
command had been updated and the etcd server is now accessible.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1514417[*BZ#1514417*])

* Previously, the `apb push -o` command failed when using it outside the cluster.
This was because the Docker registry service of the desired service was set to
hit only the route used by internal operations. The appropriate Ansible playbook
has been updated to point to the appropriate route instead.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1519193[*BZ#1519193*])

* Previously, when typing `asbd --help` or `asbd -h`, the `--help` argument returned
a code that was being misinterpreted as an error, resulting in errors printing
out twice. The fix corrects errors to only print once and also to interpret the
help command return code as valid. As a result, the help command now only prints
once. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1525817[*BZ#1525817*])

* Previously, setting the `white-list` variable in an RHCC registry would maintain
searching for any options, even after those options are removed from the
configuration. This was caused by an error in the `white-list` code. The error
has been fixed by this bug.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1526887[*BZ#1526887*])

* Previously, if the registry configuration did not have `auth_type` set to
`config` error messages would appear. This bug ensures that registry
configurations work correctly without the `auth_type` setting.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1526949[*BZ#1526949*])

* Previously, the broker would return a 400 status code when the user did not have
the permissions to execute a task instead of the 403 status code. This bug fixes
the error, and the correct status code is now returned.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1510486[*BZ#1510486*])

* Previously, any MariaDB configuration options were displayed with MySQL options.
This is because MariaDB uses MySQL variables upstream. This bug fix ensures
that, in terms of OpenShift, the variables are called out as MariaDB.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1510294[*BZ#1510294*])

*Storage*

* Previously, OpenShift checked mounted NFS volume with root squash. OpenShift
permissions  while running as root were squashed to the 'nobody' user, who did
not have permissions to access mounted NFS volume. This caused any OpenShift
checks to fail, and it did not unmount NFS volumes. Now, OpenShift does not
access mounted NFS volumes, and checks for mounts by parsing /proc filesystem.
NFS volumes with root squash option are unmounted.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1518237[*BZ#1518237*])

* Previously, when a node that had an OpenStack Cinder type of persistent volume
attached was shut down or crashed, the attached volume did not
detach.Consequence: Because the persistent volume was unavailable, the pods did
not migrate from the failed node, and the volumes were inaccessible from other
nodes and pods. Now a node fails, all of its attached volumes are detached after
a time-out.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1523142[*BZ#1523142*])

* Previously, downward API, secrets, ConfigMap, and projected volumes fully
managed their content and did not allow any other volumes to be mounted on top
of them. This meant that users could not mount any volume on top of the
aforementioned volumes. With this bug fix, the  volumes now touch only the files
they create. As a result, users can mount any volume on top of the
aforementioned volumes.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1430322[*BZ#1430322*])

*Upgrade*

* The upgrade playbooks did not previously regenerate the registry certificate
when upgrading from releases prior to 3.6, which lacked the name
'docker-registry.default.svc'. As such, the configuration variables were not
updated to push to the registry via DNS. The 3.9 upgrade playbooks now
regenerate the certificate when needed, ensuring that all environments upgraded
to 3.9 now push to the registry via DNS.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1519060[*BZ#1519060*])

* The etcd host validation now accepts one or more etcd hosts, allowing greater
flexibility in the number of etcd hosts configured. The recommended number of
etcd hosts is still 3.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1506177[*BZ#1506177*])

[[ocp-39-technology-preview]]
== Technology Preview Features

Some features in this release are currently in Technology Preview. These
experimental features are not intended for production use. Please note the
following scope of support on the Red Hat Customer Portal for these features:

https://access.redhat.com/support/offerings/techpreview[Technology Preview
Features Support Scope]

In the table below, features marked *TP* indicate _Technology Preview_ and
features marked *GA* indicate _General Availability_.

.Technology Preview Tracker
[cols="4",options="header"]
|====
|Feature |OCP 3.6 |OCP 3.7 |OCP 3.9

|xref:ocp-39-prometheus[Prometheus Cluster Monitoring]
| -
|TP
|TP

|Local Storage Persistent Volumes
| -
|TP
|TP

|xref:ocp-39-crio[CRI-O] for runtime pods
| -
|TP
|GA* footnoteref:[disclaimer, Features marked with `*` indicate delivery in a z-stream patch.]

|Tenant Driven Snapshotting
| -
|TP
|TP

|xref:ocp-39-cli-plug-ins[`oc` CLI Plug-ins]
| -
|TP
|TP

|Service Catalog
|TP
|GA
|-

|Template Service Broker
|TP
|GA
| -

|OpenShift Automation Broker
|TP
|GA
| -

|Network Policy
|TP
|GA
|-

|Service Catalog Initial Experience
|TP
|GA
|-

|New Add Project Flow
|TP
|GA
|-

|Search Catalog
|TP
|GA
|-

|CFME Installer
|TP
|GA
|-

|xref:../dev_guide/cron_jobs.adoc#dev-guide-cron-jobs[Cron Jobs]
|TP
|TP
|GA

|xref:../dev_guide/deployments/kubernetes_deployments.adoc#dev-guide-kubernetes-deployments-support[Kubernetes Deployments]
|TP
|TP
|GA

|StatefulSets
|TP
|TP
|GA

|xref:../admin_guide/quota.adoc#limited-resources-quota[Explicit Quota]
|TP
|TP
|GA

|xref:../architecture/additional_concepts/storage.adoc#pv-mount-options[Mount Options]
|TP
|TP
|GA

|xref:../install_config/install/advanced_install.adoc#advanced-install-configuring-system-containers[System Containers for docker, CRI-O]
|TP
|TP
|Dropped

|xref:../install_config/install/advanced_install.adoc#running-the-advanced-installation-system-container[System Container for installer and Kubelet]
|TP
|TP
|GA

|Hawkular Agent
|TP
|Dropped
|

|Pod PreSets
|TP
|Dropped
|

|xref:../admin_guide/overcommit.adoc#configuring-reserve-resources[experimental-qos-reserved]
| -
|TP
|TP

|xref:../admin_guide/sysctls.adoc#admin-guide-sysctls[Pod sysctls]
|TP
|TP
|TP

|xref:../install_config/master_node_configuration.adoc#master-node-config-audit-config[Central Audit]
| -
|TP
|GA

|xref:../admin_guide/managing_networking.adoc#enabling-static-ips-for-external-project-traffic[Static IPs for External Project Traffic]
| -
|TP
|GA

|xref:../dev_guide/templates.adoc#waiting-for-template-readiness[Template Completion Detection]
| -
|TP
|GA

|xref:../cli_reference/basic_cli_operations.adoc#object-types[`replicaSet`]
|TP
|TP
|GA

|xref:../install_config/aggregate_logging.adoc#aggregated-fluentd[Mux]
| -
|TP
|TP

|Clustered MongoDB Template
|TP
|Community
| -

|Clustered MySQL Template
|TP
|Community
| -

|xref:../dev_guide/managing_images.adoc#using-is-with-k8s[Image Streams with Kubernetes Resources]
|TP
|TP
|GA

|xref:ocp-39-device-manager[Device Manager]
| -
| -
|TP

|xref:ocp-39-pv-resize[Persistent Volume Resize]
| -
| -
|TP

|xref:ocp-39-hugepages[Huge Pages]
| -
| -
|TP

|xref:ocp-39-CPU-manager[CPU Manager]
| -
| -
|TP

|xref:ocp-39-device-plugins[Device Plug-ins]
| -
| -
|TP

|xref:ocp-39-syslog-output-plugin-for-fluentd[syslog Output Plug-in for fluentd]
| -
| -
|TP
|====

[[ocp-39-known-issues]]
== Known Issues

// tag::BZ1558672[]
- There is a known issue in the initial GA release of {product-title} 3.9  that
causes the installation and upgrade playbooks to consume more memory than
previous releases. The node scale-up and installation Ansible playbooks may have
consumed more memory on the control host (the system where you run the playbooks
from) than expected due to the use of `include_tasks` in several places. This
issue has been addressed with the release of
link:https://access.redhat.com/errata/RHBA-2018:0600[RHBA-2018:0600]; the
majority of these instances have now been converted to `import_tasks` calls,
which do not consume as much memory. After this change, memory consumption on
the control host should be below 100MiB per host; for large environments (100+
hosts), a control host with at least 16GiB of memory is recommended.
link:https://bugzilla.redhat.com/show_bug.cgi?id=1558672[(*BZ#1558672*)]
// end::BZ1558672[]

[[ocp-39-asynchronous-errata-updates]]
== Asynchronous Errata Updates

Security, bug fix, and enhancement updates for {product-title} 3.9 are released
as asynchronous errata through the Red Hat Network. All {product-title} 3.9
errata is https://access.redhat.com/downloads/content/290/[available on the Red
Hat Customer Portal]. See the
https://access.redhat.com/support/policy/updates/openshift[{product-title}
Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account
settings for Red Hat Subscription Management (RHSM). When errata notifications
are enabled, users are notified via email whenever new errata relevant to their
registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming
{product-title} entitlements for {product-title} errata notification
emails to generate.
====

This section will continue to be updated over time to provide notes on
enhancements and bug fixes for future asynchronous errata releases of
{product-title} 3.9. Versioned asynchronous releases, for example with the form
{product-title} 3.9.z, will be detailed in subsections. In addition, releases in
which the errata text cannot fit in the space provided by the advisory will be
detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on
xref:../upgrading/index.adoc#install-config-upgrading-index[upgrading your cluster] properly.
====
