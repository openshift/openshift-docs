[[release-notes-ocp-3-9-release-notes]]
= {product-title} 3.9 Release Notes
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

Red Hat {product-title} is a Platform as a Service (PaaS) that provides
developers and IT organizations with a cloud application platform for deploying
new applications on secure, scalable resources with minimal configuration and
management overhead. {product-title} supports a wide selection of
programming languages and frameworks, such as Java, Ruby, and PHP.

Built on Red Hat Enterprise Linux and Kubernetes, {product-title}
provides a secure and scalable multi-tenant operating system for today’s
enterprise-class applications, while providing integrated application runtimes
and libraries. {product-title} brings the OpenShift PaaS platform to customer
data centers, enabling organizations to implement a private PaaS that meets
security, privacy, compliance, and governance requirements.

[[ocp-39-about-this-release]]
== About This Release

Red Hat {product-title} version 3.9
(link:https://access.redhat.com/errata/RHBA-2018:0489[RHBA-2018:0489) is now
available. This release is based on
link:https://github.com/openshift/origin/releases/tag/v3.9.0-rc.0[OpenShift
Origin 3.9]. New features, changes, bug fixes, and known issues that pertain to
{product-title} 3.9 are included in this topic.

To better synchronize versions of {product-title} with Kubernetes, Red Hat did
not publicly release {product-title} 3.8 and, instead, is releasing
{product-title} 3.9 directly after version 3.7. See
xref:ocp-39-installation[Installation] for information on how this impacts
installation and upgrade processes.

{product-title} 3.9 is supported on RHEL 7.3 and 7.4 with the latest packages
from Extras, including Docker 1.13. It is also supported on Atomic Host 7.4.5
and newer. The *docker-latest* package is now deprecated.

For initial installations, see the
xref:../install_config/install/planning.adoc#install-config-install-planning[Installing
a Cluster] topics in the
xref:../install_config/index.adoc#install-config-index[Installation and
Configuration] documentation.

To upgrade to this release from a previous version, see the
xref:../upgrading/index.adoc#install-config-upgrading-index[Upgrading Clusters]
topic.

[[ocp-39-new-features-and-enhancements]]
== New Features and Enhancements

This release adds improvements related to the following components and concepts.

[[ocp-39-container-orchestration]]
=== Container Orchestration

[[ocp-39-soft-image-pruning]]
==== Soft Image Pruning

Now, when pruning images, you do not have to remove the actual image, just
update etcd storage.

It is safer to run `--keep-tag-revisions` and `--keep-younger-than`. After this
is run, administrators can choose to run hard prune (which is safe to run as
long as the registry is put in read-only mode).

[[ocp-39-cloudforms]]
==== CloudForms 4.6 Container Management

* {product-title} template provisioning
* Offline OpenScapScans
* Alert management: You can choose Prometheus (currently in Technology Preview) and use it in CloudForms.
* Reporting enhancements
* Provider updates
* Chargeback enhancements
* UX enhancememts

[[ocp-39-crio]]
==== CRI-O v1.9

CRI-O is a lightweight, native Kubernetes container runtime interface. By
design, it provides only the runtime capabilities needed by the kubelet. CRI-O is
designed to be part of Kubernetes and evolve in lock-step with the platform.

CRI-O brings:

* A minimal and secure architecture.
* Excellent scale and performance.
* The ability to run any Open Container Initiative (OCI) or docker image.
* Familiar operational tooling and commands.

image::crio-3-7.png[CRI-O]

To install and run CRI-O alongside `docker`, set the following in the
`[OSEv3:vars]` section
xref:../install_config/install/advanced_install.adoc#configuring-ansible[Ansible inventory file] during cluster installation:

----
openshift_use_crio=true
----

This setting pulls the *openshift3/cri-o* system container image from the
link:https://access.redhat.com/containers[Red Hat Registry] by default. If you
want to use an alternative CRI-O system container image from another registry,
you can also override the default using the following variable:

----
openshift_crio_systemcontainer_image_override=<registry>/<repo>/<image>:<tag>
----

[NOTE]
====
The `atomic-openshift-node` service must be RPM- or system container-based when
using CRI-O; it cannot be `docker` container-based. The installer protects again
using CRI-O with `docker` container nodes and will halt installation if
detected.
====

When CRI-O use is enabled, it is installed alongside `docker`, which currently
is required to perform build and push operations to the registry. Over time,
temporary `docker` builds can accumulate on nodes. You can optionally set the
following to enable garbage collection, which adds a daemonset to clean out the
builds:

----
openshift_crio_enable_docker_gc=true
----

When enabled, it will run garbage collection on all nodes by default. You can
also limit the running of the daemonset on specific nodes by setting the
following:

----
openshift_crio_docker_gc_node_selector={'runtime': 'cri-o'}
----

For example, the above would ensure it is only run on nodes with the `runtime:
cri-o` label. This can be helpful if you are running CRI-O only on
xref:../install_config/build_defaults_overrides.adoc#ansible-setting-global-build-defaults[some
nodes], and others are only running `docker`.

See the link:http://cri-o.io/[upstream documentation] for more information on
CRI-O.

[[ocp-39-storage]]
=== Storage

[[ocp-39-pv-resize]]
==== PV Resize

You can expand persistent volume claims online from {product-tile} for CNS
glusterFS, Cinder, and gcePD.

. Create a storage class with `allowVolumeExpansion=true`.
. The PVC uses the storage class and submits a claim.
. The PVC specifies a new increased size.
. The underlying PV is resized.

[[ocp-39-end-to-end-online-expansion-resize-for-cns-glusterfs-pvs]]
==== End-to-end Online Expansion and Resize for CNS Glusterfs PVs

You can expand persistent volume claims online from {product-tile} for CNS
glusterFS volumes.

This can be done online from {product-title}. Previously, this was only
available from the Heketi CLI. You edit the PVC with the new size, triggering a
PV resize. This is fully qualified for glusterFs backed PVs. Gluster-block PV
resize will be added with RHEL 7.5.

. Add `AllowVolumeExpansion=true` to the storage class.
. Run:
+
----
$ oc edit pvc claim-name
----

. Edit the `spec.resources.requests.storage` field with the new value.

[[ocp-container-native-storage-glusterfs-pv-consumption-metrics-available]]
==== Container Native Storage GlusterFS PV Consumption Metrics Available from {product-title}

Container Native Storage GlusterFS is extended to provide volume metrics
(including consumption) through Prometheus or Query.

Metrics are available from the PVC endpoint. This adds visibility to what is
being allocated and what is being consumed. Previously, you could only see
allocated size of the PVs. Now, you know how much is really consumed so, if
needed, you can expand it before it runs out of space. This also allows
administrators to do billing based on consumption, if needed.

Examples of added metrics include:

* `kubelet_volume_stats_capacity_bytes`
* `kubelet_volume_stats_inodes`
* `kubelet_volume_stats_inodes_free`
* `kubelet_volume_stats_inodes_used`
* `kubelet_volume_stats_used_bytes`

[[ocp-3-9-automated-cns-deployments-with-advanced-installation]]
==== Automated CNS Deployment with {product-title} Advanced Installation

In the {product-title} advanced installer, the CNS block provisioner deployment
is fixed and the CNS Un-install Playbook is added. This resolves the issue of CNS
block deployment with {product-title} and also provides a way to uninstall a failed
installation of CNS.

CNS storage device details are added to the installer’s inventory file. The
advanced installer manages configuration and deployment of CNS, file and block
provisioners, registry, and ready-to-use PVs.

[[ocp-39-scale]]
=== Scale

[[ocp-39-scale-cluster-limits]]
==== Cluster Limits

Updated guidance around
xref:../scaling_performance/cluster_limits.adoc#scaling-performance-cluster-limits[Cluster
Limits] for {product-title} 3.9 is now available.

[[ocp-39-device-plugins]]
==== Device Plug-ins (Technology Preview)

This is a feature currently in
xref:ocp-39-technology-preview[Technology Preview] and not for production
workloads.

Device plug-ins allow you to use a particular device type (GPU, InfiniBand,
or other similar computing resources that require vendor-specific initialization
and setup) in your {product-title} pod without needing to write custom code. The
device plug-in provides a consistent and portable solution to consume hardware
devices across clusters. The device plug-in provides support for these devices
through an extension mechanism, which makes these devices available to
containers, provides health checks of these devices, and securely shares them.

A device plug-in is a gRPC service running on the nodes (external to
`atomic-openshift-node.service`) that is responsible for managing specific
hardware resources.

See the  xref:../dev_guide/device_plugins.adoc#using-device-plugins[Developer
Guide] for further conceptual information about Device Plug-ins and the
xref:../scaling_performance/device_plugin.adoc#scaling-performance-using-device-plugin[Scaling
and Performance Guide] for more information about usage.

[[ocp-39-CPU-manager]]
==== CPU Manager (Technology Preview)

CPU Manager is a feature currently in
xref:ocp-39-technology-preview[Technology Preview] and not for production
workloads.

CPU Manager manages groups of CPUs and constrains workloads to specific CPUs.

CPU Manager is useful for workloads that have some of these attributes:

* Require as much CPU time as possible.
* Are sensitive to processor cache misses.
* Are low-latency network applications.
* Coordinate with other processes and benefit from sharing a single processor
cache.

See
xref:../scaling_performance/using_cpu_manager.adoc#scaling-performance-using-cpu-manager[Using
CPU Manager] for more information.

[[ocp-39-device-manager]]
==== Device Manager (Technology Preview)

Device Manager is a feature currently in
xref:ocp-39-technology-preview[Technology Preview] and not for production
workloads.

Some users want to set resource limits for hardware devices within their pod
definition and have the scheduler find the node in the cluster with those
resources.  While at the same time, Kubernetes needed a way for hardware
vendors to advertise their resources to the kubelet without forcing them to
change core code within Kubernetes

The kubelet now houses a device manager that is extensible through plug-ins. You
load the driver support at the node level. Then, you or the vendor writes a
plug-in that listens for requests to stop/start/attach/assign the requested
hardware resources seen by the drivers. This plug-in is deployed to all the
nodes via a daemonSet.

See xref:../dev_guide/device_manager.adoc#using-device-manager[Using Device
Manager] for more information.

[[ocp-39-hugepages]]
==== Huge Pages (Technology Preview)

Huge pages is a feature currently in xref:ocp-39-technology-preview[Technology
Preview] and not for production workloads..

Memory is managed in blocks known as pages. On most systems, a page is 4Ki. 1Mi
of memory is equal to 256 pages; 1Gi of memory is 256,000 pages, and so on. CPUs
have a built-in memory management unit that manages a list of these pages in
hardware. The Translation Lookaside Buffer (TLB) is a small hardware cache of
virtual-to-physical page mappings. If the virtual address passed in a hardware
instruction can be found in the TLB, the mapping can be determined quickly. If
not, a TLB miss occurs, and the system falls back to slower, software-based
address translation, resulting in performance issues. Since the size of the
TLB is fixed, the only way to reduce the chance of a TLB miss is to increase the
page size.

A huge page is a memory page that is larger than 4Ki. On x86_64 architectures,
there are two common huge page sizes: 2Mi and 1Gi. Sizes vary on other
architectures. In order to use huge pages, code must be written so that
applications are aware of them. Transparent Huge Pages (THP) attempt to automate
the management of huge pages without application knowledge, but they have
limitations. In particular, they are limited to 2Mi page sizes. THP can lead to
performance degradation on nodes with high memory utilization or fragmentation
due to defragmenting efforts of THP, which can lock memory pages. For this
reason, some applications may be designed to (or recommend) usage of
pre-allocated huge pages instead of THP.

In {product-title}, applications in a pod can allocate and consume pre-allocated
huge pages.

See xref:../scaling_performance/managing_hugepages.adoc#scaling-performance-managing-huge-pages[Managing
Huge Pages] for more information.

[[ocp-39-networking]]
=== Networking

[[ocp-39-semi-automatic-namespace-wide-egress-IP]]
==== Semi-automatic Namespace-wide Egress IP
All outgoing external connections from a project share a single, fixed source IP
address and send all traffic via that IP, so that external firewalls can
recognize the application associated with a packet.

It is _semi-automatic_ in that in the first half of implementing the automatic
namespace-wide egress IP feature, it implements the "traffic" side. Namespaces
with automatic egress IPs will send all traffic via that IP. However, it does
not implement the "management" side. Nothing automatically assigns egress IPs to
nodes yet. The administrator must do that manually.

See
xref:admin_guide/managing_networking.adoc#admin-guide-manage-networking[Managing
Networking] for more information.

[[ocp-39-support-our-own-haproxy-rpm-for-consumption-by-the-router]]
==== Support Our Own HAProxy RPM for Consumption by the Router

Route configuration changes and process upgrades performed under heaving load
have typically required a stop and start sequence of certain services, causing
temporary outages.

In {product-title} 3.9, HAProxy 1.8 sees no difference between updates and
upgrades; a new process is used with a new configuration, and the listening
socket’s file descriptor is transferred from the old to the new process so the
connection is never closed.  The change is seamless, and enables our ability to
do things, like HTTP/2, in the future.

[[ocp-39-master]]
=== Master

[[ocp-39-statefulsets-daemonsets-deployments]]
====  StatefulSets, DaemonSets, and Deployments Now Supported

In {product-title}, statefulsets, daemonsets, and deployments are now stable,
supported, and out of Technology Preview.

[[ocp-39-central-audit-capability]]
==== Central Audit Capability

Provides auditing of items that administrators would like to see, including:

* The event timestamp.
* The activity that generated the entry.
* The API endpoint that was called.
* The HTTP output.
* The item changed due to an activity, with details of the change.
* The user name of the user that initiated an activity.
* The name of the namespace the event occurred in, where possible.
* The status of the event, either success or failure.

Provides auditing of items that administrators would like to trace, including:

* User login and logout from (including session timeout) the web interface,
including unauthorized access attempts.
* Account creation, modification, or removal.
* Account role or policy assignment or de-assignment.
* Scaling of pods.
* Creation of new project or application.
* Creation of routes and services.
* Triggers of builds and/or pipelines.
* Addition or removal or claim of persistent volumes.

Set up auditing in the *_master-config file_*, and restart the *master-config*
service:

----
auditConfig:
  auditFilePath: "/var/log/audit-ocp.log"
  enabled: true
  maximumFileRetentionDays: 10
  maximumFileSizeMegabytes: 10
  maximumRetainedFiles: 10
  logFormat: json
  policyConfiguration: null
  policyFile: /etc/origin/master/audit-policy.yaml
  webHookKubeConfig: ""
  webHookMode:
----

Example log output:

----
{"kind":"Event","apiVersion":"audit.k8s.io/v1beta1","metadata":{"creationTimestamp":"2017-09-29T09:46:39Z"},"level":"Metadata","timestamp":"2017-09-29T09:46:39Z","auditID":"72e66a64-c3e5-4201-9a62-6512a220365e","stage":"ResponseComplete","requestURI":"/api/v1/securitycontextconstraints","verb":"create","user":{"username":"system:admin","groups":["system:cluster-admins","system:authenticated"]},"sourceIPs":["10.8.241.75"],"objectRef":{"resource":"securitycontextconstraints","name":"scc-lg","apiVersion":"/v1"},"responseStatus":{"metadata":{},"code":201}}
----

[[ocp-39-add-support-for-deployments-to-oc-status]]
==== Add Support for Deployments to oc status

The `oc status` command provides an overview of the current project. This
provides similar output for upstream deployments as can be seen for downstream
DeploymentConfigs, with a nested deployment set:

----
$ oc status
In project My Project (myproject) on server https://127.0.0.1:8443

svc/ruby-deploy - 172.30.174.234:8080
  deployment/ruby-deploy deploys istag/ruby-deploy:latest <-
    bc/ruby-deploy source builds https://github.com/openshift/ruby-ex.git on istag/ruby-22-centos7:latest
      build #1 failed 5 hours ago - bbb6701: Merge pull request #18 from durandom/master (Joe User <joeuser@users.noreply.github.com>)
    deployment #2 running for 4 hours - 0/1 pods (warning: 53 restarts)
    deployment #1 deployed 5 hours ago
----

Compare this to the output from {product-title} 3.7:

----
$ oc status
In project dc-test on server https://127.0.0.1:8443

svc/ruby-deploy - 172.30.231.16:8080
  pod/ruby-deploy-5c7cc559cc-pvq9l runs test
----

[[ocp-39-dynamic-admission-controller-follow-up]]
==== Dynamic Admission Controller Follow-up (Technology Preview)

Dynamic Admission Controller Follow-up is a feature currently in
xref:ocp-37-technology-preview[Technology Preview] and not for production
workloads.

An admission controller is a piece of code that intercepts requests to the
Kubernetes API server prior to persistence of the object, but after the request
is authenticated and authorized. Example use cases include mutation of pod
resources and security response.

See
xref:../architecture/additional_concepts/dynamic_admission_controllers.adoc#architecture-additional-concepts-dynamic-admission-controllers[Custom
Admission Controllers] for more information.

[[ocp-39-feature-gates]]
==== Feature Gates

Platform administrators now have the ability to turn off specific features to the
entire platform. This assists in the control of access to alpha, beta, or
Technology Preview features in production clusters.

link:https://kubernetes.io/docs/reference/feature-gates/[Feature gates] use a
key=value pair in the master and kubelet configuration files that describe the
feature you want to block.

.Control Plane:  master-config.yaml
----
kubernetesMasterConfig:
  apiServerArguments:
    feature-gates:
    - CPUManager=true
----

.kubelet:  node-config.yaml
----
kubeletArguments:
  feature-gates:
  - DevicePlugin=true
----

[[ocp-39-installation]]
=== Installation

[[ocp-3-9-improved-playbook-performance]]
==== Improved Playbook Performance

{product-title} 3.9 introduces significant refactoring and restructuring of the
playbooks to improve performance. This includes:

* Restructured playbooks to push all fact-gathering and common dependencies up
into the initialization plays so they are only called once rather than each time
a role needs access to their computed values.

* Refactored playbooks to limit the hosts they touch to only those that are truly
relevant to the playbook.

[[ocp-3-9-quick-installation]]
==== Quick Installation (Deprecated)

Quick Installation is now deprecated in {product-title} 3.9 and will be
completely removed in a future release.

Quick installation will only be capable of installing 3.9. It will not be able
to upgrade from 3.7 or 3.8 to 3.9.

[[ocp-3-9-automated-control-plane-upgrade]]
==== Automated 3.7 to 3.9 Control Plane Upgrade

The installer automatically handles stepping the control plane from 3.7 to 3.8
to 3.9 and node upgrade from 3.7 to 3.9.

Control plane components (API, controllers, and nodes on control plane hosts)
are upgraded seamlessly from 3.7 to 3.8 to 3.9. Data migration happens pre- and
post- {product-title} 3.8 and 3.9 control plane upgrades. Other control plane
components [router, registry, service catalog, and brokers] are upgraded from
{product-title} 3.7 to 3.9. Nodes [node, docker, ovs] are upgraded directly from
{product-title} 3.7 to 3.9 with only one drain of nodes. {product-title} 3.7
nodes operate indefinitely against 3.8 masters should the upgrade process need
to pause in this state. Logging and metrics are updated from {product-title} 3.7
to 3.9.

It is recommended that you upgrade the control plane and nodes independently.
You can still perform the upgrade through an all-in-one playbook, but rollback
is more difficult. Playbooks do not allow for a clean installation of
{product-title} 3.8.

See xref:../upgrading/index.adoc#install-config-upgrading-index[Upgrading
Clusters] for more information.

[[ocp-39-metrics-and-logging]]
=== Metrics and Logging

[[ocp-39-syslog-output-plugin-for-fluentd]]
==== syslog Output Plug-in for fluentd (Technology Preview)

syslog Output Plug-in for fluentd is a feature currently in
xref:ocp-37-technology-preview[Technology Preview] and not for production
workloads.

You can send system and container logs from {product-title} nodes to external
endpoints using the syslog protocol. The fluentd syslog output plug-in supports
this.

[IMPORTANT]
====
Logs sent via syslog are not encrypted and, therefore, insecure.
====

See
xref:../install_config/aggregate_logging.adoc#sending-logs-to-external-rsyslog[Sending
Logs to an External Syslog Server] for more information.

[[ocp-39-prometheus]]
==== Prometheus (Technology Preview)

Prometheus remains in xref:ocp-39-technology-preview[Technology Preview] and is
not for production workloads. Prometheus, AlertManager, and AlertBuffer versions
are now updated and node-exporter is now included:

* prometheus 2.1.0
* Alertmanager 0.14.0
* AlertBuffer 0.2
* node_exporter 0.15.2

You can deploy Prometheus on an {product-title} cluster, collect Kubernetes and
infrastructure metrics, and get alerts. You can see and query metrics and alerts
on the Prometheus web dashboard. Alternatively, you can bring your own Grafana
and hook it up to Prometheus.

See xref:../install_config/cluster_metrics.adoc#openshift-prometheus[Prometheus
on OpenShift] for more information.

[[ocp-39-developer-experience]]
=== Developer Experience

[[ocp-39-memory-usage-improvements]]
==== Jenkins Memory Usage Improvements

Previously, Jenkins worker pods would often consume too much or too little
memory. Now, a startup script intelligently looks at pod limits and environment
variables are appropriately set to ensure limits are respected for spawned JVMs.

[[ocp-39-cli-plug-ins]]
==== CLI Plug-ins

CLI plug-ins are now fully supported.

Usually called _plug-ins_ or _binary extensions_, this feature allows you to
extend the default set of `oc` commands available and, therefore, allows you to
perform new tasks.

See xref:../cli_reference/extend_cli.adoc#cli-reference-extend-cli[Extending the
CLI] for information on how to install and write extensions for the CLI.

[[ocp-39-ability-to-specify-tolerations]]
==== Ability to Specify Default Tolerations via the buildconfig Defaulter

Previously, there was not a way to set a default toleration on build pods so
they could be placed on build-specific nodes. The build defaulter is now updated
to allow the specification of a toleration value, which is applied to the build
pod upon creation.

See
xref:../nstall_config/build_defaults_overrides.adoc#install-config-build-defaults-overrides[Configuring
Global Build Defaults and Overrides] for more information.

[[ocp-39-web-console]]
=== Web Console

[[ocp-39-catalog-from-within-project-view]]
==== Catalog from within Project View

Quickly get to the catalog from within a project by clicking *Catalog* in the
left navigation.

image::3.9-console-catalog-tab.png[Catalog tab]

[[ocp-39-quickly-search-the-catalog]]
==== Quickly Search the Catalog from within Project View

To quickly find services from within project view, type in your search criteria.

image::3.9-console-catalog-search.png[Search the catalog]

[[ocp-39-select-preferred-home-page]]
==== Select Preferred Home Page

You can now jump straight to certain pages after login. Access the menu from
the account dropdown, choose your option, then log out, then log back in.

image::3.9-console-set-custom-home-page.gif[Set preferred home page]

[[ocp-39-configurable-inactivity-timeout]]
==== Configurable Inactivity Timeout

You can now configure the web console to log users out after a set timeout. The
default is `0` (never).
xref:../install_config/install/advanced_install.adoc#configuring-web-console-customization[Set
the Ansible variable] to the number of minutes:

----
openshift_web_console_inactivity_timeout_minutes=n
----

[[ocp-39-console-as-a-separate-pod]]
==== Console as a Separate Pod

The web console is now separated out of the API server. The web console is
packaged as a container image and deployed as a pod. Configure via the
ConfigMap. Changes are auto-detected.

Masters are now schedulable and required to be schedulable for the web consoles
deployments to work.

[[ocp-39-notable-technical-changes]]
== Notable Technical Changes

{product-title} 3.9 introduces the following notable technical changes.

[discrete]
[[ansible-must-be-installed]]
=== Ansible Must Be Installed via the rhel-7-server-ansible-2.4-rpms Channel

Starting in {product-title} 3.9, Ansible must be installed via the
`rhel-7-server-ansible-2.4-rpms` channel, which is included in RHEL
subscriptions.

[discrete]
[[ocp-39-several-oc-secrets-subcommands-now-deprecated]]
=== Several oc secrets Subcommands Now Deprecated

{product-title} 3.9 deprecates the following `oc secrets` subcommands in favor
of `oc create secret`:

* `new`
* `new-basicauth`
* `new-dockercfg`
* `new-sshauth`

[discrete]
[[updated-default-installer-values]]
=== Updated Default Values for template_service_broker_prefix and template_service_broker_image_name in the Installer

Default values for `template_service_broker_prefix` and
`template_service_broker_image_name` in installer have been updated to be
consistent with other settings.

Previous values are:

    * `template_service_broker_prefix="registry.example.com/"`
    * `template_service_broker_image_name="origin-template-service-broker"`
    * `template_service_broker_image_name="ose-template-service-broker"`

New values are:

    * `template_service_broker_prefix="registry.example.com/ose-"`
    * `template_service_broker_prefix="registry.example.com/origin-"`
    * `template_service_broker_image_name="template-service-broker"`

[discrete]
[[removed-become-no-instances]]
=== Removed Several Instances of 'become: no' on Certain Tasks and Playbooks Inside of openshift-anisble

In an effort to provide greater flexibility for users, several instances of
`become: no` on certain tasks and playbooks inside of `openshift-anisble` are
now removed. These statements were primarily applied on `local_action` and
`delegate_to: localhost` commands for creating temporary files on the host
running Ansible.

If a user is running Ansible from a host that does not allow password-less
`sudo`, some of these commands may fail if you run the `ansible-playbook` with
the `-b` (`become`) command line switch, or if it has `ansible_become=True`
applied to the local host in the inventory or `group_vars`.

Elevated permissions are not required on the local host when running
`openshift-ansible` plays.

If target hosts (where {product-title} is being deployed) require the use of
`become`, it is recommended that you add `ansible_become=True` for those hosts
or groups in inventory or `group_vars`/`host_vars`.

If the user is running as root on the local host or connection to the root user
on the remote hosts instead of using become, then you should not notice a change.

[discrete]
[[unqualified-image-specs]]
=== Unqualified Image Specifications

Unqualified image specifications now default to `docker.io` and require API
server configuration to resolve to different registries.

[discrete]
[[SchedueldJob-objects-not-supported]]
=== batch/v2alpha1 SchedueldJob Objects Are No Longer Supported

The `batch/v2alpha1 SchedueldJob` objects are no longer supported. Use CronJobs
instead.

[discrete]
[[autoscaling-API-group-removed]]
===  The autoscaling/v2alpha1 API Group Is Removed

The `autoscaling/v2alpha1` API group has been removed


[discrete]
[[oadm-deprecated]]
=== oadm Command Is Deprecated

The `oadm` command is now deprecated. Use `oc adm` instead.

[discrete]
[[hawkular-is-deprecated]]
=== Hawkular Is Deprecated

In {product-title} 3.9, Hawkular is deprecated.

[discrete]
[[statefulsets-daemonsets-seployments-now-fully-supported]]
=== StatefulSets, DaemonSets, and Deployments Now Fully Supported

The core workloads API, which is composed of the `DaemonSet`, `Deployment`,
`ReplicaSet`, and `StatefulSet kinds`, has been promoted to GA stability in the
`apps/v1` group version. As such, the` apps/v1beta2` group version is
deprecated, and all new code should use the kinds in the apps/v1 group version.
For {product-title} this means the statefulsets, daemonsets, and deployments are
now stable and supported.

[[ocp-39-bug-fixes]]
== Bug Fixes

This release fixes bugs for the following components:


*Authentication*


*Builds*


*Command Line Interface*


*Containers*


*Deployments*


*Images*

*Image Registry*


*Installer*

*Logging*


*Web Console*


*Master*


*Metrics*


*Networking*

*Pod*


*Routing*


*Service Broker*


*Storage*


*Templates*


*Upgrade*


[[ocp-37-technology-preview]]
== Technology Preview Features

Some features in this release are currently in Technology Preview. These
experimental features are not intended for production use. Please note the
following scope of support on the Red Hat Customer Portal for these features:

https://access.redhat.com/support/offerings/techpreview[Technology Preview
Features Support Scope]

The following new features are now available in Technology Preview:

- xref:ocp-39-device-manager[Device Manager]
- xref:ocp-39-CPU-manager[CPU Manager]
- GPU Support
- xref:ocp-39-pv-resize[Persistent Volume Resize]
- xref:ocp-39-hugepages[Huge pages]
- xref:ocp-39-syslog-output-plugin-for-fluentd[syslog Output Plug-in for fluentd]

The following features that were formerly in Technology Preview from a previous
{product-title} release are now fully supported:

- xref:../dev_guide/cron_jobs.adoc#dev-guide-cron-jobs[Cron Jobs]
- xref:ocp-39-crio[CRI-O]
- xref:ocp-39-cli-plug-ins[CLI Plug-ins]
- xref:../dev_guide/deployments/kubernetes_deployments.adoc#dev-guide-kubernetes-deployments-support[Kubernetes
Deployments Support]
- `StatefulSets`
- xref:../admin_guide/quota.adoc#limited-resources-quota[Require Explicit Quota to Consume a Resource]
- xref:../architecture/additional_concepts/storage.adoc#pv-mount-options[Mount Options]
- xref:../install_config/install/advanced_install.adoc#advanced-install-configuring-system-containers[Installation of etcd, Docker Daemon, and Ansible Installer as System Containers]
- xref:../install_config/install/advanced_install.adoc#running-the-advanced-installation-system-container[Running OpenShift Installer as a System Container]
- xref:../admin_guide/overcommit.adoc#configuring-reserve-resource[`experimental-qos-reserved`]
- xref:../admin_guide/sysctls.adoc#admin-guide-sysctls[pod sysctls]
- xref:../admin_guide/managing_networking.adoc#enabling-static-ips-for-external-project-traffic[Enabling Static IPs for External Project Traffic]
- xref:../install_config/master_node_configuration.adoc#master-node-config-audit-config[Central Audit]
- xref:../dev_guide/templates.adoc#waiting-for-template-readiness[Template Completion Detection]
- xref:../cli_reference/basic_cli_operations.adoc#object-types[`replicaSet`]
- xref:../dev_guide/managing_images.adoc#using-is-with-k8s[Using Image Streams with Kubernetes Resources]

The following features that were formerly in Technology Preview from a previous
{product-title} release remain in Technology Preview:

- xref:ocp-37-prometheus[Prometheus Cluster Monitoring]
- xref:ocp-37-local-persistent-volumes[Local Storage Persistent Volumes]
- xref:ocp-37-tenant-driven-storage-snapshotting[Tenant-driven Storage Snapshotting]
- xref:../install_config/aggregate_logging.adoc#aggregated-fluentd[`mux`]
- Bind in Context


[[ocp-37-asynchronous-errata-updates]]
== Asynchronous Errata Updates

Security, bug fix, and enhancement updates for {product-title} 3.9 are released
as asynchronous errata through the Red Hat Network. All {product-title} 3.9
errata is https://access.redhat.com/downloads/content/290/[available on the Red
Hat Customer Portal]. See the
https://access.redhat.com/support/policy/updates/openshift[{product-title}
Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account
settings for Red Hat Subscription Management (RHSM). When errata notifications
are enabled, users are notified via email whenever new errata relevant to their
registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming
{product-title} entitlements for {product-title} errata notification
emails to generate.
====

This section will continue to be updated over time to provide notes on
enhancements and bug fixes for future asynchronous errata releases of
{product-title} 3.9. Versioned asynchronous releases, for example with the form
{product-title} 3.9.z, will be detailed in subsections. In addition, releases in
which the errata text cannot fit in the space provided by the advisory will be
detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[upgrading your cluster] properly.
====
