:_mod-docs-content-type: ASSEMBLY
[id="ocp-4-15-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-15-about-this-release"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-2023:7198[RHSA-2023:7198]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.28.md[Kubernetes 1.28] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. With the {cluster-manager-first} application for {product-title}, you can deploy {product-title} clusters to either on-premises or cloud environments.

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.8 and a later version of {op-system-base} 8 that is released before End of Life of {product-title} {product-version}. {product-title} {product-version} is also supported on {op-system-first} {product-version}. To understand {op-system-base} versions used by {op-system}, see link:https://access.redhat.com/articles/6907891[{op-system-base} Versions Utilized by {op-system-first} and {product-title}] (Knowledgebase article).

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

For {product-title} 4.12 on `x86_64` architecture, Red Hat has added a 6-month Extended Update Support (EUS) phase that extends the total available lifecycle from 18 months to 24 months. For {product-title} 4.12 running on 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures, the EUS lifecycle remains at 18 months.

Starting with {product-title} 4.14, each EUS phase for even numbered releases on all supported architectures, including `x86_64`, 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures, has a total available lifecycle of 24 months.

Starting with {product-title} 4.14, Red Hat offers a 12-month additional EUS add-on, denoted as _Additional EUS Term 2_, that extends the total available lifecycle from 24 months to 36 months. The Additional EUS Term 2 is available on all architecture variants of {product-title}.

For more information about this support, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

//TODO: The line below should be used when it is next appropriate. Revisit in August 2023 time frame.
Maintenance support ends for version 4.12 on 17 July 2024 and goes to extended update support phase. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

Commencing with the {product-version} release, Red Hat is simplifying the administration and management of Red Hat shipped cluster Operators with the introduction of three new life cycle classifications; Platform Aligned, Platform Agnostic, and Rolling Stream. These life cycle classifications provide additional ease and transparency for cluster administrators to understand the life cycle policies of each Operator and form cluster maintenance and upgrade plans with predictable support boundaries. For more information, see link:https://access.redhat.com/webassets/avalon/j/includes/session/scribe/?redirectTo=https%3A%2F%2Faccess.redhat.com%2Fsupport%2Fpolicy%2Fupdates%2Fopenshift_operators[OpenShift Operator Life Cycles].

// Added in 4.14. Language came directly from Kirsten Newcomer.
{product-title} is designed for FIPS. When running {op-system-base-full} or {op-system-first} booted in FIPS mode, {product-title} core components use the {op-system-base} cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the `x86_64`, `ppc64le`, and `s390x` architectures.

For more information about the NIST validation program, see link:https://csrc.nist.gov/Projects/cryptographic-module-validation-program/validated-modules[Cryptographic Module Validation Program]. For the latest NIST status for the individual versions of {op-system-base} cryptographic libraries that have been submitted for validation, see link:https://access.redhat.com/articles/2918071#fips-140-2-and-fips-140-3-2[Compliance Activities and Government Standards].

[id="ocp-4-15-add-on-support-status"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-15-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-15-rhcos"]
=== {op-system-first}

[id="ocp-4-15-rhcos-rhel-9-2-packages"]
==== {op-system} now uses {op-system-base} 9.2

{op-system} now uses {op-system-base-full} 9.2 packages in {product-title} {product-version}. These packages ensure that your {product-title} instance receives the latest fixes, features, enhancements, hardware support, and driver updates.

[id="ocp-4-15-rhcos-iscsi-support"]
==== Support for iSCSI devices (Technology Preview)

{op-system} now supports the `iscsi_bft` driver, letting you boot directly from iSCSI devices that work with the iSCSI Boot Firmware Table (iBFT), as a Technology Preview. This lets you target iSCSI devices as the root disk for installation.

For more information, see the link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/performing_a_standard_rhel_9_installation/iscsi-disks-in-installation-program_installing-rhel[{op-system-base} documentation].

[id="ocp-4-15-installation-and-update"]
=== Installation and update

[id="ocp-4-15-azure-storage-account-encryption"]
==== Encrypting Azure storage account during installation

You can now encrypt Azure storage accounts during installation by providing the installation program with a customer managed encryption key. See xref:../installing/installing_azure/installation-config-parameters-azure.adoc#installation-configuration-parameters-additional-azure_installation-config-parameters-azure[Installation configuration parameters] for descriptions of the parameters required to encrypt Azure storage accounts.

[id="ocp-4-15-capi-openstack-tech-preview-no-upgrade"]
==== {rh-openstack} integration into the Cluster CAPI Operator (Tech Preview)

If you enable the `TechPreviewNoUpgrade` feature flag, the Cluster CAPI Operator deploys the {cap-openstack-first} and manages its lifecycle.
The Cluster CAPI Operator automatically creates `Cluster` and `OpenStackCluster` resources for the current {product-title} cluster.

It is now possible to configure the Cluster API `Machine` and `OpenStackMachine` resources similarly to how Machine API resources are configured.
It is important to note that while Cluster API resources are functionally equivalent to Machine API resources, structurally they are not identical.

[id="ocp-4-15-installation-and-update-ibm-cloud-user-managed-encryption"]
==== IBM Cloud and user-managed encryption

You can now specify your own {ibm-name} Key Protect for {ibm-cloud-name} root key as part of the installation process. This root key is used to encrypt the root (boot) volume of control plane and compute machines, and the persistent volumes (data volumes) that are provisioned after the cluster is deployed.

For more information, see xref:../installing/installing_ibm_cloud_public/user-managed-encryption-ibm-cloud.adoc#user-managed-encryption-ibm-cloud[User-managed encryption for IBM Cloud].

[id="ocp-4-15-installation-and-update-ibm-cloud-restricted-install"]
==== Installing a cluster on IBM Cloud with limited internet access

You can now install a cluster on {ibm-cloud-name} in an environment with limited internet access, such as a disconnected or restricted network cluster. With this type of installation, you create a registry that mirrors the contents of the {product-title} installation images. You can create this registry on a mirror host, which can access both the internet and your restricted network.

For more information, see xref:../installing/installing_ibm_cloud_public/installing-ibm-cloud-restricted.adoc#installing-ibm-cloud-restricted[Installing a cluster on IBM Cloud in a restricted network].

[id="ocp-4-15-installation-and-update-aws-wavelength-zones"]
==== Installing a cluster on AWS to extend nodes to Wavelength Zones

You can quickly install an {product-title} cluster in Amazon Web Services (AWS) Wavelength Zones by setting the zone names in the edge compute pool of the `install-config.yaml` file, or install a cluster in an existing VPC with Wavelength Zone subnets.

You can also perform postinstallation tasks to extend an existing {product-title} cluster on AWS to use AWS Wavelength Zones.

For more information, see xref:../installing/installing_aws/installing-aws-wavelength-zone.adoc#installing-aws-wavelength-zone[Installing a cluster on AWS with compute nodes on AWS Wavelength Zones] and xref:../post_installation_configuration/aws-compute-edge-zone-tasks.adoc#aws-compute-edge-zone-tasks[Extend existing clusters to use AWS Local Zones or Wavelength Zones].

[id="ocp-4-15-installation-customize-cluster-network-mtu-aws"]
==== Customizing the cluster network MTU on AWS deployments

Before you deploy a cluster on AWS Local Zones infrastructure, you can customize the cluster network maximum transmission unit (MTU) for your cluster network to meet the needs of your infrastructure.

You can customize the MTU for a cluster by specifying the networking.clusterNetworkMTU parameter in the `install-config.yaml` configuration file.

For more information, see xref:../installing/installing_aws/installing-aws-localzone.adoc#installation-aws-cluster-network-mtu_installing-aws-localzone[Customizing the cluster network MTU].

[id="ocp-4-15-installing-aws-outposts"]
==== Installing a cluster on AWS with compute nodes on AWS Outposts

In {product-title} version 4.14, you could install a cluster on AWS with compute nodes running in AWS Outposts as a Technology Preview. In {product-title} 4.15, you can install a cluster on AWS into an existing VPC and provision compute nodes on AWS Outposts as a postinstallation configuration task.

For more information, see xref:../installing/installing_aws/installing-aws-vpc.adoc#installing-aws-vpc[Installing a cluster on AWS into an existing VPC] and xref:../installing/installing_aws/installing-aws-outposts.adoc#installing-aws-outposts[Extending an AWS VPC cluster into an AWS Outpost].

[id="ocp-4-15-installation-and-update-nutanix-failure-domains"]
==== Nutanix and fault tolerant deployments

By default, the installation program installs control plane and compute machines into a single Nutanix Prism Element (cluster). To improve the fault tolerance of your {product-title} cluster, you can now specify that these machines be distributed across multiple Nutanix clusters by configuring failure domains.

For more information, see xref:../installing/installing_nutanix/nutanix-failure-domains.adoc#nutanix-failure-domains[Fault tolerant deployments using multiple Prism Elements].

[id="ocp-4-15-installation-and-update-OCP-on-ARM"]
==== {product-title} on 64-bit ARM

{product-title} {product-version} now supports the ability to enable 64k page sizes in the {op-system} kernel using the Machine Config Operator (MCO). This setting is exclusive to machines with 64-bit ARM architectures. For more information, see the xref:../post_installation_configuration/machine-configuration-tasks.adoc#machine-configuration-tasks[Machine configuration tasks] documentation.

[id="ocp-4-15-installation-and-update-optional-olm"]
==== Optional OLM cluster capability

In {product-title} 4.15, you can disable the Operator Lifecycle Manager (OLM) capability during installation. For further information, see xref:../installing/overview/cluster-capabilities.adoc#olm-overview_cluster-capabilities[Operator Lifecycle Manager capability].

[id="ocp-4-15-deploying-osp-with-root-volume-etcd-on-local-disk"]
==== Deploying {rh-openstack-first} with root volume and etcd on local disk (Technology Preview)

You can now move etcd from a root volume (Cinder) to a dedicated ephemeral local disk as a Day 2 deployment. With this Technology Preview feature, you can resolve and prevent performance issues of your {rh-openstack} installation.

For more information, see xref:../installing/installing_openstack/deploying-openstack-with-rootVolume-etcd-on-local-disk.adoc#deploying-openstack-with-rootvolume-etcd-on-local-disk[Deploying on OpenStack with rootVolume and etcd on local disk].

[id="ocp-4-15-agent-vsphere-credentials"]
==== Configure vSphere integration with the Agent-based Installer

You can now configure your cluster to use vSphere while creating the `install-config.yaml` file for an Agent-based Installation.
For more information, see xref:../installing/installing_with_agent_based_installer/installation-config-parameters-agent.adoc#installation-configuration-parameters-additional-vsphere_installation-config-parameters-agent[Additional VMware vSphere configuration parameters].

[id="ocp-4-15-agent-baremetal-configuration"]
==== Additional bare metal configurations during Agent-based installation

You can now make additional configurations for the bare metal platform while creating the `install-config.yaml` file for an Agent-based Installation.
These new options include host configuration, network configuration, and baseboard management controller (BMC) details.

These fields are not used during the initial provisioning of the cluster, but they eliminate the need to set the fields after installation.
For more information, see xref:../installing/installing_with_agent_based_installer/installation-config-parameters-agent.adoc#installation-configuration-parameters-additional-bare_installation-config-parameters-agent[Additional bare metal configuration parameters for the Agent-based Installer].

[id="ocp-4-15-baremetal-raid-dell"]
==== Use the Dell iDRAC BMC to configure a RAID during installer-provisioned installation

You can now use the Dell iDRAC baseboard management controller (BMC) with the Redfish protocol to configure a redundant array of independent disks (RAID) for the bare metal platform during an installer-provisioned installation.
For more information, see xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#configuring-the-raid_ipi-install-installation-workflow[Optional: Configuring the RAID].

[id="ocp-4.15-postinstallation-configuration"]
=== Postinstallation configuration

[id="ocp-4.15-postinstallation-configuration-multi-arch-compute-machines"]
==== {product-title} clusters with multi-architecture compute machines

On {product-title} {product-version} clusters with multi-architecture compute machines, you can now enable 64k page sizes in the {op-system-first} kernel on the 64-bit ARM compute machines in your cluster. For more information on setting this parameter, see xref:../post_installation_configuration/configuring-multi-arch-compute-machines/multi-architecture-compute-managing.adoc#multi-architecture-enabling-64k-pages_multi-architecture-compute-managing[Enabling 64k pages on the {op-system-first} kernel].

[id="ocp-4-15-web-console"]
=== Web console

[id="ocp-4-15-administrator-perspective"]
==== Administrator perspective

This release introduces the following updates to the *Administrator* perspective of the web console:

* Enable and disable the tailing to Pod log viewer to minimize load time.
* View recommended values for `VerticalPodAutoscaler` on the *Deployment* page.

[id="node-uptime-information"]
===== Node uptime information

With this update, you can enable the ability to view additional node uptime information to track node restarts or failures. Navigate to the *Compute* -> *Nodes* page, click *Manage columns*, and then select *Uptime*.

[id="web-console-dynamic-plugin-enhancements"]
===== Dynamic plugin enhancements

With this update, you can add a new details item to the default resource summary on the *Details* page by using `console.resource/details-item`. The {product-title} release also adds example implementation for annotation, label and the delete modal to the CronTab dynamic plugin.

For more information, see xref:../web_console/dynamic-plugin/dynamic-plugins-reference.adoc#dynamic-plugin-reference[Dynamic plugin reference]

For more information about `console.resource/details-item`, see xref:../web_console/dynamic-plugin/dynamic-plugins-reference.adoc#dynamic-plugin-sdk-extensions_dynamic-plugins-reference[{product-title} console API].

[id="ocp-4-15-console-supports-azure-sts-detection"]
===== OperatorHub support for {entra-first}

With this release, OperatorHub detects when a {product-title} cluster running on Azure is configured for {entra-first}. When detected, a "Cluster in Workload Identity / Federated Identity Mode" notification is displayed with additional instructions before installing an Operator to ensure it runs correctly. The *Operator Installation* page is also modified to add fields for the required Azure credentials information.

For the updated step for the *Install Operator* page, see xref:../operators/admin/olm-adding-operators-to-cluster.adoc#olm-installing-from-operatorhub-using-web-console_olm-adding-operators-to-a-cluster[Installing from OperatorHub using the web console].

[id="ocp-4-15-developer-perspective"]
==== Developer Perspective

This release introduces the following updates to the *Developer* perspective of the web console:

* Pipeline history and logs based on the data from Tekton Results are available in the dashboard without requiring PipelineRun CRs on the cluster.

[id="ocp-4-15-software-supply-chain-enhancements"]
===== Software Supply Chain Enhancements

The *PipelineRun Details* page in the *Developer* or *Administrator* perspective of the web console provides an enhanced visual representation of PipelineRuns within a Project.

For more information, see link:https://docs.openshift.com/pipelines/latest/about/understanding-openshift-pipelines.html[{pipelines-title}].

[id="ocp-4-15-red-hat-developer-hub"]
===== {rh-dev-hub} in the web console

With this update, a quick start is now available for you to learn more about how to install and use the developer hub.

For more information, see link:https://access.redhat.com/documentation/en-us/red_hat_developer_hub/1.0[Product Documentation for {rh-dev-hub}].

[id="ocp-4-15-builds-support"]
===== builds for {product-title} is supported in the web console

With this update, builds for {product-title} 1.0 is supported in the web console. Builds is an extensible build framework based on the link:https://shipwright.io/[Shipwright project]. You can use builds for {product-title} to build container images on an {product-title} cluster.

For more information, see link:https://docs.openshift.com/builds/1.0/about/overview-openshift-builds.html[builds for {product-title}].

//[id="ocp-4-15-openshift-cli"]
//=== OpenShift CLI (oc)
//
[id="ocp-4-15-ibm-z"]
=== {ibm-z-title} and {ibm-linuxone-title}

With this release, {ibm-z-name} and {ibm-linuxone-name} are now compatible with {product-title} {product-version}. You can perform the installation with z/VM, LPAR, or {op-system-base-full} Kernel-based Virtual Machine (KVM). For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_z/preparing-to-install-on-ibm-z.adoc#preparing-to-install-on-ibm-z[Installing a cluster with on {ibm-z-title} and {ibm-linuxone-title}]

[IMPORTANT]
====
Compute nodes must run {op-system-first}.
====

[id="release-notes-ibm-z-enhancements_{context}"]
==== {ibm-z-title} and {ibm-linuxone-title} notable enhancements

The {ibm-z-name} and {ibm-linuxone-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components and concepts.

This release introduces support for the following features on {ibm-z-name} and {ibm-linuxone-name}:

* Agent-based Installer
* {cert-manager-operator}
* `s390x` control plane with `x86_64` multi-architecture compute nodes

[id="release-notes-cluster-lpar_{context}"]
==== Installing a cluster in an LPAR on {ibm-z-title} and {ibm-linuxone-title}

{product-title} now supports user-provisioned installation of {product-title} 4.15 in a logical partition (LPAR) on {ibm-z-title} and {ibm-linuxone-title}.

For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_z/installing-ibm-z-lpar.adoc#installing-ibm-z-lpar[Installing a cluster in an LPAR on {ibm-z-name} and {ibm-linuxone-name}]

* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z-lpar.adoc#installing-restricted-networks-ibm-z-lpar[Installing a cluster in an LPAR on {ibm-z-name} and {ibm-linuxone-name} in a restricted network]

[id="ocp-4-15-ibm-power"]
=== {ibm-power-title}

{ibm-power-name} is now compatible with {product-title} {product-version}. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_power/installing-ibm-power.adoc#installing-ibm-power_installing-ibm-power[Installing a cluster on {ibm-power-name}]
* xref:../installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc#installing-restricted-networks-ibm-power_installing-restricted-networks-ibm-power[Installing a cluster on {ibm-power-name} in a restricted network]

[IMPORTANT]
====
Compute nodes must run {op-system-first}.
====

[id="release-notes-ibm-power-notable-enhancements"]
==== {ibm-power-title} notable enhancements

The {ibm-power-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on {ibm-power-name}:

* Agent-based Installer
* {cert-manager-operator}
* {ibm-power-name} Virtual Server Block CSI Driver Operator
* Installer-provisioned Infrastructure Enablement for {ibm-power-name} Virtual Server
* Multi-architecture {ibm-power-name} control plane with support of Intel and {ibm-power-name} workers
* nx-gzip for Power10 (Hardware Acceleration)
* The `openshift-install` utility to support various SMT levels on {ibm-power-name} (Hardware Acceleration)

[id="release-notes-ibm-support-matrix"]
=== {ibm-power-title}, {ibm-z-title}, and {ibm-linuxone-title} support matrix

Starting in {product-title} 4.14, Extended Update Support (EUS) is extended to the {ibm-power-name} and the {ibm-z-name} platform. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

.{product-title} features
[cols="3,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Alternate authentication providers
|Supported
|Supported

|Agent-based Installer
|Supported
|Supported

|Assisted Installer
|Supported
|Supported

|Automatic Device Discovery with Local Storage Operator
|Unsupported
|Supported

|Automatic repair of damaged machines with machine health checking
|Unsupported
|Unsupported

|Cloud controller manager for {ibm-cloud-name}
|Supported
|Unsupported

|Controlling overcommit and managing container density on nodes
|Unsupported
|Unsupported

|Cron jobs
|Supported
|Supported

|Descheduler
|Supported
|Supported

|Egress IP
|Supported
|Supported

|Encrypting data stored in etcd
|Supported
|Supported

|FIPS cryptography
|Supported
|Supported

|Helm
|Supported
|Supported

|Horizontal pod autoscaling
|Supported
|Supported

|Hosted control planes (Technology Preview)
|Supported
|Supported

|IBM Secure Execution
|Unsupported
|Supported

|{ibm-power-server-name} Block CSI Driver Operator
|Supported
|Unsupported

|Installer-provisioned Infrastructure Enablement for {ibm-power-server-name}
|Supported
|Unsupported

|Installing on a single node
|Supported
|Supported

|IPv6
|Supported
|Supported

|Monitoring for user-defined projects
|Supported
|Supported

|Multi-architecture compute nodes
|Supported
|Supported

|Multipathing
|Supported
|Supported

|Network-Bound Disk Encryption - External Tang Server
|Supported
|Supported

|Non--volatile memory express drives (NVMe)
|Supported
|Unsupported

|oc-mirror plugin
|Supported
|Supported

|OpenShift CLI (`oc`) plugins
|Supported
|Supported

|Operator API
|Supported
|Supported

|OpenShift Virtualization
|Unsupported
|Unsupported

|OVN-Kubernetes, including IPsec encryption
|Supported
|Supported

|PodDisruptionBudget
|Supported
|Supported

|Precision Time Protocol (PTP) hardware
|Unsupported
|Unsupported

|{openshift-local-productname}
|Unsupported
|Unsupported

|Scheduler profiles
|Supported
|Supported

|Stream Control Transmission Protocol (SCTP)
|Supported
|Supported

|Support for multiple network interfaces
|Supported
|Supported

|Three-node cluster support
|Supported
|Supported

|Topology Manager
|Supported
|Unsupported

|z/VM Emulated FBA devices on SCSI disks
|Unsupported
|Supported

|4K FCP block device
|Supported
|Supported
|====

.Persistent storage options
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}
|Persistent storage using iSCSI
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using local volumes (LSO)
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using hostPath
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Fibre Channel
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Raw Block
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using EDEV/FBA
|Supported ^[1]^
|Supported ^[1]^,^[2]^
|====
[.small]
--
1. Persistent shared storage must be provisioned by using either {rh-storage-first} or other supported storage protocols.
2. Persistent non-shared storage must be provisioned by using local storage, such as iSCSI, FC, or by using LSO with DASD, FCP, or EDEV/FBA.
--

.Operators
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|{cert-manager-operator}
|Supported
|Supported

|Cluster Logging Operator
|Supported
|Supported

|Cluster Resource Override Operator
|Supported
|Supported

|Compliance Operator
|Supported
|Supported

|Cost Management Metrics Operator
|Supported
|Supported

|File Integrity Operator
|Supported
|Supported

|HyperShift Operator
|Technology Preview
|Technology Preview

|Local Storage Operator
|Supported
|Supported

|MetalLB Operator
|Supported
|Supported

|Network Observability Operator
|Supported
|Supported

|NFD Operator
|Supported
|Supported

|NMState Operator
|Supported
|Supported

|OpenShift Elasticsearch Operator
|Supported
|Supported

|Vertical Pod Autoscaler Operator
|Supported
|Supported
|====

.Multus CNI plugins
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Bridge
|Supported
|Supported

|Host-device
|Supported
|Supported

|IPAM
|Supported
|Supported

|IPVLAN
|Supported
|Supported
|====

.CSI Volumes
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Cloning
|Supported
|Supported

|Expansion
|Supported
|Supported

|Snapshot
|Supported
|Supported
|====

[id="ocp-4-15-auth"]
=== Authentication and authorization

[id="ocp-4-15-auth-olm-azure-sts"]
==== OLM-based Operator support for {entra-first}

With this release, some Operators managed by Operator Lifecycle Manager (OLM) on Azure clusters can use the Cloud Credential Operator (CCO) in manual mode with {entra-first}. These Operators authenticate with short-term credentials that are managed outside the cluster.

For more information, see xref:../operators/operator_sdk/token_auth/osdk-cco-azure.adoc#osdk-cco-azure[CCO-based workflow for OLM-managed Operators with Azure AD Workload Identity].

[id="ocp-4-15-networking"]
=== Networking

[id="ocp-4-15-networking-ovn-kubernetes-ipsec-support-for-external-traffic"]
==== OVN-Kubernetes network plugin support for IPsec encryption of external traffic general availability (GA)

{product-title} now supports encryption of external traffic, also known as _north-south traffic_. IPsec already supports encryption of network traffic between pods, known as _east-west traffic_. You can use both features together to provide full in-transit encryption for {product-title} clusters.

This feature is supported on the following platforms:

- Bare metal
- {gcp-first}
- {rh-openstack-first}
- {vmw-full}

For more information, see xref:../networking/ovn_kubernetes_network_provider/configuring-ipsec-ovn.adoc#nw-ovn-ipsec-external_configuring-ipsec-ovn[Enabling IPsec encryption for external IPsec endpoints].

[id="ocp-4-15-ipv6-default-macvlan"]
==== IPv6 unsolicited neighbor advertisements now default on macvlan CNI plugin

Previously, if one pod (`Pod X`) was deleted, and a second pod (`Pod Y`) was created with a similar configuration, `Pod Y` might have had the same IPv6 address as `Pod X`, but it would have a different MAC address. In this scenario, the router was unaware of the MAC address change, and it would continue sending traffic to the MAC address for `Pod X`.

With this update, pods created using the macvlan CNI plugin, where the IP address management CNI plugin has assigned IPs, now send IPv6 unsolicited neighbor advertisements by default onto the network. This enhancement notifies the network fabric of the new pod's MAC address for a particular IP to refresh IPv6 neighbor caches.

[id="ocp-4-15-configuring-whereabouts-IP-reconciler-schedule"]
==== Configuring the Whereabouts IP reconciler schedule

The Whereabouts reconciliation schedule was hard-coded to run once per day and could not be reconfigured. With this release, a `ConfigMap` object has enabled the configuration of the Whereabouts cron schedule. For more information, see xref:../networking/multiple_networks/configuring-additional-network.adoc#nw-multus-configuring-whereabouts-ip-reconciler-schedule_configuring-additional-network[Configuring the Whereabouts IP reconciler schedule].

[id="ocp-4-15-egressfirewall-updates-status-management"]
==== Status management updates for EgressFirewall and AdminPolicyBasedExternalRoute CR

The following updates have been made to the status management of `EgressFirewall` and `AdminPolicyBasedExternalRoute` custom resource policy:

** The `status.status` field is set to `failure` if at least one message reports `failure`.

** The `status.status` field is empty if no failures are reported and not all nodes have reported their status.

** The `status.status` field is set to `success` if all nodes report `success`.

** The `status.mesages` field lists messages. The messages are listed by the node name by default and are prefixed with the node name.

[id="ocp-4-15-metallb-metrics"]
==== Additional BGP metrics for MetalLB

With this update, MetalLB exposes additional metrics relating to communication between MetalLB and Border Gateway Protocol (BGP) peers. For more information, see xref:../networking/metallb/metallb-troubleshoot-support.adoc#nw-metallb-metrics_metallb-troubleshoot-support[MetalLB metrics for BGP and BFD].

[id="ocp-4-15-all-multicast"]
==== Supporting all-multicast mode

{product-title} now supports configuring the all-multicast mode by using the tuning CNI plugin. This update eliminates the need to grant the `NET_ADMIN` capability to the pod's Security Context Constraints (SCC), enhancing security by minimizing potential vulnerabilities for your pods.

For more information about all-multicast mode, see xref:../networking/hardware_networks/configuring-interface-sysctl-sriov-device.adoc#nw-about-all-multi-cast-mode_configuring-sysctl-interface-sriov-device[About all-multicast mode].

[id="ocp-4-15-multi-network-policy-ipv6"]
==== Multi-network policy support for IPv6 networks

With this update, you can now create multi-network policies for IPv6 networks. For more information, see xref:../networking/multiple_networks/configuring-multi-network-policy.adoc#nw-multi-network-policy-ipv6-support_configuring-multi-network-policy[Supporting multi-network policies in IPv6 networks].

[id="ocp-4-15-ingress-dashboards"]
==== Ingress Operator metrics dashboard available

With this release, Ingress networking metrics are now viewable from within the {product-title} web console. See xref:../networking/networking-dashboards.adoc#ingress-dashboards[Ingress Operator dashboard] for more information.

[id="ocp-4-15-CoreDNS-filtration-externalname-service-queries"]
==== CoreDNS filtration of ExternalName service queries for subdomains

As of {product-title} {product-version}, CoreDNS has been updated from 1.10.1 to 1.11.1.

This update to CoreDNS resolved an issue where CoreDNS would incorrectly provide a response to a query for an `ExternalName` service that shared its name with a top-level domain, such as `com` or `org`. A query for subdomains of an external service should not resolve to that external service. See the associated link:https://github.com/coredns/coredns/pull/6162[CoreDNS GitHub issue] for more information.

[id="ocp-4-15-CoreDNS-metrics-deprecation-and-removal"]
==== CoreDNS metrics deprecation and removal

As of {product-title} {product-version}, CoreDNS has been updated from 1.10.1 to 1.11.1.

This update to CoreDNS resulted in the deprecation and removal of certain metrics that have been relocated, including the metrics `coredns_forward_healthcheck_failures_total`, `coredns_forward_requests_total`, `coredns_forward_responses_total`, and `coredns_forward_request_duration_seconds`. See link:https://coredns.io/plugins/forward/#metrics[CoreDNS Metrics] for more information.

[id="ocp-4-15-networking-supported-hardware-for-sr-iov"]
==== Supported hardware for SR-IOV (Single Root I/O Virtualization)

{product-title} {product-version} adds support for the following SR-IOV devices:

* Mellanox MT2910 Family [ConnectX&#8209;7]

For more information, see xref:../networking/hardware_networks/about-sriov.adoc#supported-devices_about-sriov[Supported devices].

[id="ocp-4-15-networking-nncp-host-network-settings"]
==== Host network configuration policy for SR-IOV network VFs (Technology Preview)

With this release, you can use the `NodeNetworkConfigurationPolicy` resource to manage host network settings for Single Root I/O Virtualization (SR-IOV) network virtual functions (VF) in an existing cluster.

For example, you can configure a host network Quality of Service (QoS) policy to manage network access to host resources by an attached SR-IOV network VF. For more information, see xref:../networking/k8s_nmstate/k8s-nmstate-updating-node-network-config.adoc#virt-example-vf-host-services_k8s_nmstate-updating-node-network-config[Node network configuration policy for virtual functions].

[id="ocp-4-15-networking-sriov-draining"]
==== Parallel node draining during SR-IOV network policy updates

With this update, you can configure the SR-IOV Network Operator to drain nodes in parallel during network policy updates. The option to drain nodes in parallel enables faster rollouts of SR-IOV network configurations. You can use the `SriovNetworkPoolConfig` custom resource to configure parallel node draining and define the maximum number of nodes in the pool that the Operator can drain in parallel.

For more information, see xref:../networking/hardware_networks/configuring-sriov-device.adoc#configure-sr-iov-operator-parallel-nodes_configuring-sriov-device[Configuring parallel node draining during SR-IOV network policy updates].

[id="ocp-4-15-registry"]
=== Registry

[id="ocp-4-15-private-storage-endpoint-azure"]
==== Support for private storage endpoint on {azure-short}

With this release, the Image Registry Operator can be leveraged to use private storage endpoints on {azure-short}. You can use this feature to seamlessly configure private endpoints for storage accounts when {product-title} is deployed on private {azure-short} clusters, so that users can deploy the image registry without exposing public-facing storage endpoints.

For more information, see the following sections:

* xref:../post_installation_configuration/configuring-private-cluster.adoc#registry-configuring-private-storage-endpoint-azure_configuring-private-cluster[Configuring a private storage endpoint on Azure]
* xref:../installing/installing_azure/installing-azure-private.adoc#installing-private-image-registry-private-azure[Optional: Preparing a private Microsoft Azure cluster for a private image registry]

[id="ocp-4-15-storage"]
=== Storage

[id="ocp-4.15-storage-recovering-vgs-from-prev-installation"]
==== Recovering volume groups from the previous {lvms} installation

With this release, the `LVMCluster` custom resource (CR) provides support for recovering volume groups from the previous {lvms} installation. If the `deviceClasses.name` field is set to the name of a volume group from the previous {lvms} installation, {lvms} recreates the resources related to that volume group in the current {lvms} installation. This simplifies the process of using devices from the previous {lvms} installation through the reinstallation of {lvms}.

For more information, see xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#lvms-creating-lvms-cluster_logical-volume-manager-storage[Creating a Logical Volume Manager cluster on a worker node].

[id="ocp-4.15-storage-support-for-wiping-the-devices"]
==== Support for wiping the devices in {lvms}

This feature provides a new optional field `forceWipeDevicesAndDestroyAllData` in the `LVMCluster` custom resource (CR) to force wipe the selected devices. Before this release, wiping the devices required you to manually access the host. With this release, you can force wipe the disks without manual intervention. This simplifies the process of wiping the disks.

[WARNING]
====
If `forceWipeDevicesAndDestroyAllData` is set to `true`, {lvms} wipes all previous data on the devices. You must use this feature with caution.
====

For more information, see xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#lvms-creating-lvms-cluster_logical-volume-manager-storage[Creating a Logical Volume Manager cluster on a worker node].

[id="ocp-4.15-storage-support-for-lvms-on-multi-node-clusters"]
==== Support for deploying {lvms} on multi-node clusters

This feature provides support for deploying {lvms} on multi-node clusters. Previously, {lvms} only supported single-node configurations. With this release, {lvms} supports all of the {product-title} deployment topologies. This enables provisioning of local storage on multi-node clusters.
[WARNING]
====
{lvms} only supports node local storage on multi-node clusters. It does not support storage data replication mechanism across nodes. When using {lvms} on multi-node clusters, you must ensure storage data replication through active or passive replication mechanisms to avoid a single point of failure.
====

For more information, see xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#lvms-preface-sno-ran_logical-volume-manager-storage[Deploying {lvms}].

[id="ocp-4.15-storage-support-for-integrating-raid-arrays-with-lvms"]
==== Integrating RAID arrays with {lvms}

This feature provides support for integrating RAID arrays that are created using the `mdadm` utility with {lvms}. The `LVMCluster` custom resource (CR) provides support for adding paths to the RAID arrays in the `deviceSelector.paths` field and the `deviceSelector.optionalPaths` field.

For more information, see xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#lvms-integrating-software-raid-arrays_logical-volume-manager-storage[Integrating software RAID arrays with LVM Storage].

[id="ocp-4.15-storage-fips-conpliance-support-for-lvms"]
==== FIPS compliance support for {lvms}

With this release, {lvms} is designed for Federal Information Processing Standards (FIPS). When {lvms} is installed on {product-title} in FIPS mode, {lvms} uses the RHEL cryptographic libraries that have been submitted to NIST for FIPS 140-3 validation only on the x86_64 architecture.

[id="ocp-4-15-storage-retro-sc-assignment"]
==== Retroactive default StorageClass assignment is generally available

Before {product-title} 4.13, if there was no default storage class, persistent volumes claims (PVCs) that were created that requested the default storage class remained stranded in the pending state indefinitely, unless you manually delete and recreate them. Starting with {product-title} 4.14, as a Technology Preview feature, the default storage class is assigned to these PVCs retroactively so that they do not remain in the pending state. After a default storage class is created, or one of the existing storage classes is declared the default, these previously stranded PVCs are assigned to the default storage class. This feature is now generally available.

For more information, see xref:..//storage/container_storage_interface/persistent-storage-csi-sc-manage.adoc#absent-default-storage-class[Absent default storage class].

[id="ocp-4-15-storage-lso-erase-lv-option"]
==== Local Storage Operator option to facilitate removing existing data on local volumes is generally available

This feature provides an optional field, `forceWipeDevicesAndDestroyAllData` defining whether or not to call `wipefs`, which removes partition table signatures (magic strings) making the disk ready to use for Local Storage Operator (LSO) provisioning. No other data besides signatures is erased. This feature is now generally available. Note that this feature does not apply to `LocalVolumeSet` (LVS).

For more information, see xref:..//storage/persistent_storage/persistent_storage_local/persistent-storage-local.adoc#local-volume-cr_persistent-storage-local[Provisioning local volumes by using the Local Storage Operator].

[id="ocp-4-15-storage-non-graceful-node-shutdown"]
==== Detach CSI volumes after non-graceful node shutdown is generally available

Starting with {product-title} 4.13, Container Storage Interface (CSI) drivers can automatically detach volumes when a node goes down non-gracefully as a Technology Preview feature. When a non-graceful node shutdown occurs, you can then manually add an out-of-service taint on the node to allow volumes to automatically detach from the node. This feature is now generally available.

For more information, see xref:..//storage/persistent-storage-csi-vol-detach-non-graceful-shutdown.adoc[Detach CSI volumes after non-graceful node shutdown].

[id="ocp-4-15-storage-gcp-filestore-shared-vpc"]
==== Shared VPC is supported for the GCP Filestore CSI Driver Operator as generally available

Shared virtual private cloud (VPC) for the Google Compute Platform (GCP) Container Storage Interface (CSI) Driver Operator is now supported as a generally available feature. Shared VPC simplifies network management, allows consistent network policies, and provides a centralized view of network resources.

For more information, see xref:..//storage/container_storage_interface/persistent-storage-csi-google-cloud-file.adoc#persistent-storage-csi-google-cloud-file-create-sc_persistent-storage-csi-google-cloud-file[Creating a storage class for GCP Filestore Storage].

[id="ocp-4-15-storage-ibm-vpc-byok"]
==== User-Managed encryption supports IBM VPC Block storage as generally available

The user-managed encryption feature allows you to provide keys during installation that encrypt {product-title} node root volumes, and enables all managed storage classes to use the specified encryption key to encrypt provisioned storage volumes. This feature was introduced in {product-title} 4.13 for Google Cloud Platform (GCP) persistent disk (PD) storage, Microsoft Azure Disk, and Amazon Web Services (AWS) Elastic Block storage (EBS), and is now supported on IBM Virtual Private Cloud (VPC) Block storage.

//TODO: For more information, see xref:..//storage/container_storage_interface/persistent-storage-csi-ibm-vpc-block.adoc#byok_persistent-storage-csi-ibm-vpc-block [User-managed encryption].
//
[id="ocp-4-15-storage-selinux-relabling-mount-options"]
==== SELinux relabeling using mount options (Technology Preview)

Previously, when SELinux was enabled, the persistent volume's (PV's) files were relabeled when attaching the PV to the pod, potentially causing timeouts when the PVs contained many files, as well as overloading the storage backend.

In {product-title} 4.15, for Container Storage Interface (CSI) driver that support this feature, the driver will mount the volume directly with the correct SELinux labels, eliminating the need to recursively relabel the volume, and pod startup can be significantly faster.

This feature is supported with Technology Preview status.

If the following conditions are true, the feature is enabled by default:

* The CSI driver that provides the volume has support for this feature with `seLinuxMountSupported: true` in its CSIDriver instance. The following CSI drivers that are shipped as part of {product-title} announce SELinux mount support:

** AWS Elastic Block Storage (EBS)
** Azure Disk
** Google Compute Platform (GCP) persistent disk (PD)
** IBM Virtual Private Cloud (VPC) Block
** OpenStack Cinder
** VMware vSphere

* The pod that uses the persistent volume has full SELinux label specified in its `spec.securityContext` or `spec.containers[*].securityContext` by using `restricted` SCC.

* Access mode set to `ReadWriteOncePod` for the volume.

[id="ocp-4-15-oci"]
=== Oracle(R) Cloud Infrastructure

[id="ocp-4-15-installation-oci-assisted-installer-vm"]
==== Using the Assisted Installer to install a cluster on {oci}

You can run cluster workloads on {oci-first} infrastructure that supports dedicated, hybrid, public, and multiple cloud environments. Both Red Hat and Oracle test, validate, and support running {oci} in an {product-title} cluster on {oci}.

{oci} provides services that can meet your needs for regulatory compliance, performance, and cost-effectiveness. You can access {oci} Resource Manager configurations to provision and configure {oci} resources.

For more information, see xref:../installing/installing_oci/installing-oci-assisted-installer.adoc[Using the Assisted Installer to install a cluster on {oci}].

[id="ocp-4-15-installation-oci-agent-based-installer-vm"]
==== Using the Agent-based Installer to install a cluster on {oci}

You can use the Agent-based Installer to install a cluster on {oci-first}, so that you can run cluster workloads on infrastructure that supports dedicated, hybrid, public, and multiple cloud environments.

The Agent-based installer provides the ease of use of the Assisted Installation service, but with the capability to install a cluster in either a connected or disconnected environment.

{oci} provides services that can meet your regulatory compliance, performance, and cost-effectiveness needs. {oci} supports 64-bit `x86` instances and 64-bit ARM instances.

For more information, see xref:../installing/installing_oci/installing-oci-agent-based-installer.adoc[Using the Agent-based Installer to install a cluster on {oci}].

[id="ocp-4-15-olm"]
=== Operator lifecycle

[id="ocp-4-15-olmv1"]
==== {olmv1-first} (Technical Preview)

Operator Lifecycle Manager (OLM) has been included with {product-title} 4 since its initial release. {product-title} 4.14 introduced components for a next-generation iteration of OLM as a Technology Preview feature, known during this phase as _{olmv1}_. This updated framework evolves many of the concepts that have been part of previous versions of OLM and adds new capabilities.

During this Technology Preview phase of {olmv1} in {product-title} {product-version}, administrators can explore the following features added to this release:

Support for version ranges::
You can specify a version range by using a comparison string in an Operator or extension's custom resource (CR). If you specify a version range in the CR, {olmv1} installs or updates to the latest version of the Operator that can be resolved within the version range. For more information, see xref:../operators/olm_v1/olmv1-installing-an-operator-from-a-catalog.adoc#olmv1-updating-an-operator_olmv1-installing-operator[Updating an Operator] and xref:../operators/olm_v1/olmv1-installing-an-operator-from-a-catalog.adoc#olmv1-version-range-support_olmv1-installing-operator[Support for version ranges]

Performance improvements in the Catalog API::
The Catalog API now uses an HTTP service to serve catalog content on the cluster. Previously, custom resource definitions (CRDs) were used for this purpose. The change to using an HTTP service to serve catalog content reduces the load on the Kubernetes API server. For more information, see xref:../operators/olm_v1/olmv1-installing-an-operator-from-a-catalog.adoc#olmv1-finding-operators-to-install_olmv1-installing-operator[Finding Operators to install from a catalog].

include::snippets/olmv1-cli-only.adoc[]

For more information, see xref:../operators/olm_v1/index.adoc#olmv1-about[About Operator Lifecycle Manager 1.0].

include::snippets/olmv1-tp-extension-support.adoc[]

//
//[id="ocp-4-15-osdk"]
//=== Operator development

[id="ocp-4-15-deprecation"]
==== Deprecation schema for Operator catalogs

The optional `olm.deprecations` schema defines deprecation information for Operator packages, bundles, and channels in a file-based catalog. Operator authors can use this schema in a `deprecations.yaml` file to provide relevant messages about their Operators, such as support status and recommended upgrade paths, to users running those Operators from a catalog. After the Operator is installed, any specified messages can be viewed as status conditions on the related `Subscription` object.

For information on the `olm.deprecations` schema, see xref:../operators/understanding/olm-packaging-format.adoc#olm-deprecations-schema_olm-packaging-format[Operator Framework packaging format].

[id="ocp-4-15-osdk"]
=== Operator development

[id="ocp-4-15-osdk-cco-azure"]
==== Token authentication for Operators on cloud providers: {entra-first}

With this release, Operators managed by Operator Lifecycle Manager (OLM) can support token authentication when running on Azure clusters configured for {entra-first}. Updates to the Cloud Credential Operator (CCO) enable semi-automated provisioning of certain short-term credentials, provided that the Operator author has enabled their Operator to support {entra-first}.

For more information, see xref:../operators/operator_sdk/token_auth/osdk-cco-azure.adoc#osdk-cco-azure[CCO-based workflow for OLM-managed Operators with Azure AD Workload Identity].

[id="ocp-4-15-builds"]
=== Builds

[id="ocp-4-15-machine-config-operator"]
=== Machine Config Operator

[id="ocp-4-15-mco-mcn-status"]
==== Improved MCO state reporting by node (Technology Preview)

With this release, you can monitor updates for individual nodes as a Technology Preview. For more information, see xref:../post_installation_configuration/machine-configuration-tasks.adoc#checking-mco-node-status[Checking machine config node status].

[id="ocp-4-15-machine-api"]
=== Machine API

[id="ocp-4-15-machine-api-and-update-vsphere-cpms"]
==== Defining a VMware vSphere failure domain for a control plane machine set (Technology Preview)

By using a vSphere failure domain resource, you can use a control plane machine set to deploy control plane machines on hardware that is separate from the primary VMware vSphere infrastructure. A control plane machine set helps balance control plane machines across defined failure domains to provide fault tolerance capabilities to your infrastructure.

For more information, see xref:../machine_management/control_plane_machine_management/cpmso-configuration.adoc#cpmso-yaml-failure-domain-vsphere_cpmso-configuration[Sample VMware vSphere failure domain configuration] and xref:../machine_management/control_plane_machine_management/cpmso-getting-started.adoc#cpmso-platform-matrix_cpmso-getting-started[Supported cloud providers].

[id="ocp-4-15-nodes"]
=== Nodes

[id="ocp-4-15-nodes-dev-fuse"]
==== The /dev/fuse device enables faster builds on unprivileged pods

You can configure unprivileged pods with the `/dev/fuse` device to access faster builds.

For more information, see xref:../nodes/containers/nodes-containers-dev-fuse.adoc#nodes-containers-dev-fuse[Accessing faster builds with /dev/fuse].

[id="ocp-4-15-nodes-log-linking"]
==== Log linking is enabled by default

Beginning with {product-title} 4.15, log linking is enabled by default. Log linking gives you access to the container logs for your pods.

[id="ocp-4-15-nodes-icsp-idms-compatibility"]
==== ICSP, IDMS, and ITMS are now compatible

`ImageContentSourcePolicy` (ICSP), `ImageDigestMirrorSet` (IDMS), and `ImageTagMirrorSet` (ITMS) objects now function in the same cluster at the same time. Previously, to use the newer IDMS or ITMS objects, you needed to delete any ICSP objects. Now, you can use any or all of the three types of objects to configure repository mirroring after the cluster is installed. For more information, see xref:../post_installation_configuration/preparing-for-users.html#images-configuration-registry-mirror_post-install-preparing-for-users[Understanding image registry repository mirroring].

[IMPORTANT]
====
Using an ICSP object to configure repository mirroring is a deprecated feature. Deprecated functionality is still included in {product-title} and continues to be supported. However, it might be removed in a future release of this product. Because it is deprecated functionality, avoid using it for new deployments.
====

[id="ocp-4-15-monitoring"]
=== Monitoring

The in-cluster monitoring stack for this release includes the following new and modified features.

[id="ocp-4-15-monitoring-updates-to-monitoring-stack-components-and-dependencies"]
==== Updates to monitoring stack components and dependencies

This release includes the following version updates for in-cluster monitoring stack components and dependencies:

* Alertmanager to 0.26.0
* kube-state-metrics to 2.10.1
* node-exporter to 1.7.0
* Prometheus to 2.48.0
* Prometheus Adapter to 0.11.2
* Prometheus Operator to 0.70.0
* Thanos Querier to 0.32.5

[id="ocp-4-15-monitoring-changes-to-alerting-rules"]
==== Changes to alerting rules

[NOTE]
====
Red{nbsp}Hat does not guarantee backward compatibility for recording rules or alerting rules.
====

* The `NodeClockNotSynchronising` and `NodeClockSkewDetected` alerting rules are now disabled when the Precision Time Protocol (PTP) is in use.

[id="ocp-4-15-monitoring-new-metrics-server-to-access-metrics-API-technology-preview"]
==== New Metrics Server component to access the Metrics API (Technology Preview)

This release introduces a Technology Preview option to add a Metrics Server component to the in-cluster monitoring stack.
As a Technology Preview feature, Metrics Server is automatically installed instead of Prometheus Adapter if the `FeatureGate` custom resource is configured with the `TechPreviewNoUpgrade` option.
If installed, Metrics Server collects resource metrics and exposes them in the `metrics.k8s.io` Metrics API service for use by other tools and APIs.
Using Metrics Server instead of Prometheus Adapter frees the core platform Prometheus stack from handling this functionality.
For more information, see xref:../observability/monitoring/config-map-reference-for-the-cluster-monitoring-operator.adoc#metricsserverconfig[MetricsServerConfig] in the config map API reference for the {cmo-full} and xref:../nodes/clusters/nodes-cluster-enabling-features.adoc[Enabling features using feature gates].

[id="ocp-4-15-monitoring-new-feature-to-send-exemplar-data-to-remote-write-storage-for-user-defined-projects"]
==== New feature to send exemplar data to remote write storage for user-defined projects

User-defined projects can now use remote write to send exemplar data scraped by Prometheus to remote storage.
To use this feature, configure remote write using the `sendExemplars` option in the `RemoteWriteSpec` resource.
For more information, see xref:../observability/monitoring/config-map-reference-for-the-cluster-monitoring-operator.adoc#remotewritespec[RemoteWriteSpec] in the config map API reference for the {cmo-full}.

[id="ocp-4-15-monitoring-improved-alert-querying-for-user-defined-projects"]
==== Improved alert querying for user-defined projects

Applications in user-defined projects now have API access to query alerts for application namespaces via the rules tenancy port for Thanos Querier.
You can now construct queries that access the `/api/v1/alerts` endpoint via port 9093 for Thanos Querier, provided that the HTTP request contains a `namespace` parameter.
In previous releases, the rules tenancy port for Thanos Querier did not provide API access to the `/api/v1/alerts` endpoint.

[id="ocp-4-15-monitoring-prometheus-updated-to-tolerate-jitters-at-scrape-time"]
==== Prometheus updated to tolerate jitters at scrape time

The default Prometheus configuration in the monitoring stack has been updated so that jitters are tolerated at scrape time.
For monitoring deployments that have shown sub-optimal chunk compression for data storage, this update helps to optimize data compression, thereby reducing the disk space used by the time series database in these deployments.

[id="ocp-4-15-monitoring-improved-staleness-handling-for-the-kubelet-service-monitor"]
==== Improved staleness handling for the kubelet service monitor

Staleness handling for the kubelet service monitor has been improved to ensure that alerts and time aggregations are accurate.
This improved functionality is active by default and makes the dedicated service monitors feature obsolete.
As a result, the dedicated service monitors feature has been disabled and is now deprecated, and setting the `DedicatedServiceMonitors` resource to `enabled` has no effect.

[id="ocp-4-15-monitoring-improved-ability-to-troubleshoot-reports-of-tasks-failing"]
==== Improved ability to troubleshoot reports of tasks failing

The reasons provided when tasks fail in monitoring components are now more granular so that you can more easily pinpoint whether a reported failure originated in components deployed in the `openshift-monitoring` namespace or in the `openshift-user-workload-monitoring` namespace.
If the {cmo-first} reports task failures, the following reasons have been added to identify where the failures originated:

* The `PlatformTasksFailed` reason indicates failures that originated in the `openshift-monitoring` namespace.
* The `UserWorkloadTasksFailed` reason indicates failures that originated in the `openshift-user-workload-monitoring` namespace.

[id="ocp-4-15-network-observability-1-5"]
=== Network Observability Operator

The Network Observability Operator releases updates independently from the {product-title} minor version release stream. Updates are available through a single, Rolling Stream which is supported on all currently supported versions of {product-title} 4. Information regarding new features, enhancements, and bug fixes for the Network Observability Operator is found in the xref:../observability/network_observability/network-observability-operator-release-notes.adoc#network-observability-rn[Network Observability release notes].

[id="ocp-4-15-scalability-and-performance"]
=== Scalability and performance

You can set the control plane hardware speed to one of `"Standard"`, `"Slower"`, or the default, `""`, which allows the system to decide which speed to use. This is a Technology Preview feature. For more information, see xref:../scalability_and_performance/recommended-performance-scale-practices/recommended-etcd-practices.adoc#etcd-tuning-parameters_recommended-etcd-practices[Setting tuning parameters for etcd].

[id="ocp-4-15-hub-side-templating-policygentemplate"]
==== Hub-side templating for PolicyGenTemplate CRs

You can manage the configuration of fleets of clusters by using hub templates to populate the group and site values in the generated policies that get applied to managed clusters.
By using hub templates in group and site `PolicyGenTemplate` (PGT) CRs you can significantly reduce the number of policies on the hub cluster.
For more information, see xref:../edge_computing/ztp-advanced-policy-config.adoc#ztp-specifying-nics-in-pgt-crs-with-hub-cluster-templates_ztp-advanced-policy-config[Specifying group and site configuration in group PolicyGenTemplate CRs with hub templates].

[id="ocp-4-15-nto-latency-testing"]
==== Node Tuning Operator (NTO)

The Cloud-native Network Functions (CNF) tests image for latency tests, `cnf-tests`, has been simplified.
The new image has three tests for latency measurements.
The tests run by default and require a performance profile configured on the cluster.
If no performance profile is configured, the tests do not run.

The following variables are no longer recommended for use:

* `ROLE_WORKER_CNF`
* `NODES_SELECTOR`
* `PERF_TEST_PROFILE`
* `FEATURES`
* `LATENCY_TEST_RUN`
* `DISCOVERY_MODE`

To generate the `junit` report, the `--ginkgo.junit-report` flag replaces `--junit`.

For more information, see xref:../scalability_and_performance/cnf-performing-platform-verification-latency-tests.adoc#cnf-performing-platform-verification-latency-tests[Performing latency tests for platform verification].

[id="ocp-4-15-bare-metal-operator"]
==== Bare Metal Operator

For {product-title} {product-version}, when the Bare Metal Operator removes a host from the cluster it also powers off the host. This enhancement streamlines hardware maintenance and management.

[id="ocp-4-15-hcp"]
=== Hosted control planes

[id="ocp-4-15-non-bare-metal-agent-machines"]
==== Configuring hosted control plane clusters by using non-bare metal agent machines (Technology Preview)

With this release, you can provision a hosted control plane cluster by using non-bare metal agent machines. For more information, see link:https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.10/html/clusters/cluster_mce_overview#configuring-hosting-service-cluster-configure-agent-non-bm[Configuring hosted control plane clusters using non-bare metal agent machines (Technology Preview)].

[id="ocp-4-15-hcp-kubevirt-console"]
==== Creating a hosted cluster by using the {product-title} console

With this release, you can now create a hosted cluster with the KubeVirt platform by using the {product-title} console. The multicluster engine for Kubernetes Operator (MCE) enables the hosted cluster view. For more information, see link:https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.10/html/clusters/cluster_mce_overview#hosted-create-kubevirt-console[Creating a hosted cluster by using the console].

[id="ocp-4-15-namage-nodepools-kubevirt"]
==== Configuring additional networks, guaranteed CPUs, and VM scheduling for node pools

With this release, you can now configure additional networks, request a guaranteed CPU access for Virtual Machines (VMs), and manage scheduling of KubeVirt VMs for node pools. For more information, see link:https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.10/html/clusters/cluster_mce_overview#managing-nodepools-hosted-cluster-kubevirt[Configuring additional networks, guaranteed CPUs, and VM scheduling for node pools].

//[id="ocp-4-15-insights-operator"]
//=== Insights Operator
//
[id="ocp-4-15-notable-technical-changes"]
== Notable technical changes

{product-title} {product-version} introduces the following notable technical changes.

// Note: use [discrete] for these sub-headings.
// example sub-heading below:
//[discrete]
//[id="ocp-4-15-cluster-cloud-controller-manager-operator"]
//=== Cloud controller managers for additional cloud providers
//
[discrete]
[id="ocp-4-15-ports-use-tls"]
=== Cluster metrics ports secured

With this release, the ports that serve metrics for the Cluster Machine Approver Operator and Cluster Cloud Controller Manager Operator use the Transport Layer Security (TLS) protocol for additional security. (link:https://issues.redhat.com/browse/OCPCLOUD-2272[OCPCLOUD-2272], link:https://issues.redhat.com/browse/OCPCLOUD-2271[OCPCLOUD-2271])

[discrete]
[id="ocp-4-15-cluster-cloud-controller-manager-operator"]
=== Cloud controller manager for Google Cloud Platform

The Kubernetes community plans to deprecate the use of the Kubernetes controller manager to interact with underlying cloud platforms in favor of using cloud controller managers. As a result, there is no plan to add Kubernetes controller manager support for any new cloud platforms.

This release introduces the General Availability of using a cloud controller manager for Google Cloud Platform.

To learn more about the cloud controller manager, see the link:https://kubernetes.io/docs/concepts/architecture/cloud-controller/[Kubernetes Cloud Controller Manager documentation].

To manage the cloud controller manager and cloud node manager deployments and lifecycles, use the Cluster Cloud Controller Manager Operator.

For more information, see the xref:../operators/operator-reference.adoc#cluster-cloud-controller-manager-operator_cluster-operators-ref[Cluster Cloud Controller Manager Operator] entry in the _Cluster Operators reference_.

[discrete]
[id="ocp-4-15-planned-psa-restricted-enforcement"]
=== Future restricted enforcement for pod security admission

Currently, pod security violations are shown as warnings in the audit logs without resulting in the rejection of the pod.

Global restricted enforcement for pod security admission is currently planned for the next minor release of {product-title}. When this restricted enforcement is enabled, pods with pod security violations will be rejected.

To prepare for this upcoming change, ensure that your workloads match the pod security admission profile that applies to them. Workloads that are not configured according to the enforced security standards defined globally or at the namespace level will be rejected. The `restricted-v2` SCC admits workloads according to the link:https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted[Restricted] Kubernetes definition.

If you are receiving pod security violations, see the following resources:

* See xref:../authentication/understanding-and-managing-pod-security-admission.adoc#security-context-constraints-psa-alert-eval_understanding-and-managing-pod-security-admission[Identifying pod security violations] for information about how to find which workloads are causing pod security violations.

* See xref:../authentication/understanding-and-managing-pod-security-admission.adoc#security-context-constraints-psa-synchronization_understanding-and-managing-pod-security-admission[About pod security admission synchronization] to understand when pod security admission label synchronization is performed. Pod security admission labels are not synchronized in certain situations, such as the following situations:
** The workload is running in a system-created namespace that is prefixed with `openshift-`.
** The workload is running on a pod that was created directly without a pod controller.

* If necessary, you can set a custom admission profile on the namespace or pod by setting the `pod-security.kubernetes.io/enforce` label.

[discrete]
[id="ocp-4-15-auth-generated-secrets"]
=== Secrets are no longer automatically generated when the integrated {product-registry} is disabled

If you disable the `ImageRegistry` cluster capability or if you disable the integrated {product-registry} in the Cluster Image Registry Operator's configuration, a service account token secret and image pull secret are no longer generated for each service account.

For more information, see xref:../nodes/pods/nodes-pods-secrets.adoc#auto-generated-sa-token-secrets_nodes-pods-secrets[Automatically generated secrets].

[discrete]
[id="ocp-4-15-ovnic-default-range"]
=== Open Virtual Network Infrastructure Controller default range

With this update, the Controller uses `100.88.0.0/16` as the default IP address range for the transit switch subnet. Do not use this IP range in your production infrastructure network. (link:https://issues.redhat.com/browse/OCPBUGS-20178[OCPBUGS-20178])

[discrete]
[id="ocp-4-15-no-strict-limits-variable"]
=== Introduction of HAProxy no strict-limits variable

The transition to HAProxy 2.6 included enforcement for the `strict-limits` configuration, which resulted in unrecoverable errors when `maxConnections` requirements could not be met. The `strict-limits` setting is not configurable by end users and remains under the control of the HAProxy template.

This release introduces a configuration adjustment in response to the migration to the `maxConnections` issues. Now, the HAProxy configuration switches to using `no strict-limits`. As a result, HAProxy no longer fatally exits when the `maxConnection` configuration cannot be satisfied. Instead, it emits warnings and continues running. When `maxConnection` limitations cannot be met, warnings such as the following examples might be returned:

* `[WARNING] (50) : [/usr/sbin/haproxy.main()] Cannot raise FD limit to 4000237, limit is 1048576.`
* `[ALERT] (50) : [/usr/sbin/haproxy.main()] FD limit (1048576) too low for maxconn=2000000/maxsock=4000237. Please raise 'ulimit-n' to 4000237 or more to avoid any trouble.`

To resolve these warnings, we recommend specifying `-1` or `auto` for the `maxConnections` field when tuning an IngressController. This choice allows HAProxy to dynamically calculate the maximum value based on the available resource limitations in the running container, which eliminates these warnings. (link:https://issues.redhat.com/browse/OCPBUGS-21803[OCPBUGS-21803])

[discrete]
[id="ocp-4-15-auth-deployer-sa"]
=== The deployer service account is no longer created if the DeploymentConfig cluster capability is disabled

If you disable the `DeploymentConfig` cluster capability, the `deployer` service account and its corresponding secrets are no longer created.

For more information, see xref:../installing/overview/cluster-capabilities.adoc#deployment-config-capability_cluster-capabilities[DeploymentConfig capability].

[discrete]
[id="ocp-4-15-must-gather-storage-default"]
=== Must-gather storage limit default

A default limit of 30% of the storage capacity of the node for the container has been added for data collected by the `oc adm must-gather` command. If necessary, you can use the `--volume-percentage` flag to adjust the default storage limit.

For more information, see xref:../support/gathering-cluster-data.adoc#must-gather-storage-limit_gathering-cluster-data[Changing the must-gather storage limit].

[discrete]
[id="ocp-4-15-agent-tui-serial-console"]
=== Agent-based Installer interactive network configuration displays on the serial console

With this update, when an Agent ISO is booted on a server with no graphical console, interactive network configuration is possible on the serial console.
Status displays are paused on all other consoles while the interactive network configuration is active.
Previously, the displays could be shown only on a graphical console.
(link:https://issues.redhat.com/browse/OCPBUGS-19688[OCPBUGS-19688])

[id="ocp-4-15-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* _General Availability_
* _Deprecated_
* _Removed_

[discrete]
[id="ocp-4-15-operators-dep-rem"]
=== Operator lifecycle and development deprecated and removed features

.Operator lifecycle and development deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|SQLite database format for Operator catalogs
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
=== Images deprecated and removed features

.Images deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|`ImageChangesInProgress` condition for Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated

|`MigrationInProgress` condition for Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
=== Monitoring deprecated and removed features

.Monitoring deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|`dedicatedServiceMonitors` setting that enables dedicated service monitors for core platform monitoring
|General Availability
|General Availability
|Deprecated

|`prometheus-adapter` component that queries resource metrics from Prometheus and exposes them in the metrics API.
|General Availability
|General Availability
|Deprecated

|====

[discrete]
=== Installation deprecated and removed features

.Installation deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|OpenShift SDN network plugin
|General Availability
|Deprecated
|Removed ^[1]^

|`--cloud` parameter for `oc adm release extract`
|General Availability
|Deprecated
|Deprecated

|CoreDNS wildcard queries for the `cluster.local` domain
|Removed
|Removed
|Removed

|`compute.platform.openstack.rootVolume.type` for {rh-openstack}
|General Availability
|Deprecated
|Deprecated

|`controlPlane.platform.openstack.rootVolume.type` for {rh-openstack}
|General Availability
|Deprecated
|Deprecated

|`ingressVIP` and `apiVIP` settings in the `install-config.yaml` file for installer-provisioned infrastructure clusters
|Deprecated
|Deprecated
|Deprecated

|`platform.gcp.licenses` for Google Cloud Provider
|Deprecated
|Removed
|Removed

|====
[.small]
--
1. While the OpenShift SDN network plugin is no longer supported by the installation program in version 4.15, you can upgrade a cluster that uses the OpenShift SDN plugin from version 4.14 to version 4.15.
--

[discrete]
=== Storage deprecated and removed features

.Storage deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Persistent storage using FlexVolume
|Deprecated
|Deprecated
|Deprecated

|====

// [discrete]
// === Authentication and authorization deprecated and removed features
//
// .Authentication and authorization deprecated and removed tracker
// [cols="4,1,1,1",options="header"]
// |====
// |Feature |4.13 |4.14 |4.15
//
// |====
// [discrete]
// === Specialized hardware and driver enablement deprecated and removed features
//
// .Specialized hardware and driver enablement deprecated and removed tracker
// [cols="4,1,1,1",options="header"]
// |====
// |Feature |4.13 |4.14 |4.15
//
// |====
// There are no deprecated or removed features jbrigman per Brett Thurber

[discrete]
=== Networking deprecated and removed features

.Networking deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Kuryr on {rh-openstack}
|Deprecated
|Deprecated
|Removed

|OpenShift SDN network plugin
|General Availability
|Deprecated
|Deprecated

|====

[discrete]
=== Building applications deprecated and removed features

.Service Binding Operator deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Service Binding Operator
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
=== Node deprecated and removed features

.Node deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|`ImageContentSourcePolicy` (ICSP) objects
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/zone`
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/region`
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
=== OpenShift CLI (oc) deprecated and removed features
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|`--include-local-oci-catalogs` parameter for `oc-mirror`
|General Availability
|Removed
|Removed

|`--use-oci-feature` parameter for `oc-mirror`
|Deprecated
|Removed
|Removed

|====

[discrete]
=== Workloads deprecated and removed features

.Workloads deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|`DeploymentConfig` objects
|General Availability
|Deprecated
|Deprecated

|====

[discrete]
=== Bare metal monitoring

.Bare Metal Event Relay Operator tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Bare Metal Event Relay Operator
|Technology Preview
|Technology Preview
|Deprecated

|====

[id="ocp-4-15-deprecated-features"]
=== Deprecated features

[id="ocp-4-15-deprecation-sdn"]
==== Deprecation of the OpenShift SDN network plugin

OpenShift SDN CNI is deprecated as of {product-title} 4.14. As of {product-title} {product-version}, the network plugin is not an option for new installations. In a subsequent future release, the OpenShift SDN network plugin is planned to be removed and no longer supported. Red{nbsp}Hat will provide bug fixes and support for this feature until it is removed, but this feature will no longer receive enhancements. As an alternative to OpenShift SDN CNI, you can use OVN Kubernetes CNI instead.

[id="ocp-4-15-bmer"]
==== Bare Metal Event Relay Operator

The Bare Metal Event Relay Operator is deprecated. The ability to monitor bare-metal hosts by using the Bare Metal Event Relay Operator will be removed in a future {product-title} release.

[id="ocp-4-15-deprecation-sbo"]
==== Service Binding Operator

The Service Binding Operator is deprecated and will be removed with the {product-title} 4.16 release. Red Hat will provide critical bug fixes and support for this component during the current release lifecycle, but this component will no longer receive feature enhancements.

[id="ocp-4-15-dedicated-service-monitors-for-core-platform-monitoring"]
==== Dedicated service monitors for core platform monitoring
With this release, the dedicated service monitors feature for core platform monitoring is deprecated.
The ability to enable dedicated service monitors by configuring the `dedicatedServiceMonitors` setting in the `cluster-monitoring-config` config map object in the `openshift-monitoring` namespace will be removed in a future {product-title} release.
To replace this feature, Prometheus functionality has been improved to ensure that alerts and time aggregations are accurate.
This improved functionality is active by default and makes the dedicated service monitors feature obsolete.

[id="ocp-4-15-prometheus-adapter-for-core-platform-monitoring"]
==== Prometheus Adapter for core platform monitoring
With this release, the Prometheus Adapter component for core platform monitoring is deprecated and is planned to be removed in a future release.
Red{nbsp}Hat will provide bug fixes and support for this component during the current release lifecycle, but this component will no longer receive enhancements and will be removed.
As a replacement, a new Metrics Server component has been added to the monitoring stack.
Metrics Server is a simpler and more lightweight solution because it does not rely on Prometheus for its functionality.
Metrics Server also ensures scalability and a more accurate tracking of resource metrics.
With this release, the improved functionality of Metrics Server is available by default if you enable the `TechPreviewNoUpgrade` option in the `FeatureGate` custom resource.

[id="ocp-4-15-deprecated-oc-registry-info"]
==== oc registry info command is deprecated

With this release, the experimental `oc registry info` command is deprecated.

To view information about the integrated {product-registry}, run `oc get imagestream -n openshift` and check the `IMAGE REPOSITORY` column.

[id="ocp-4-15-removed-features"]
=== Removed features

[id="ocp-4-15-openshift-default-registry"]
==== Removal of the OPENSHIFT_DEFAULT_REGISTRY

{product-title} {product-version} has removed support for the `OPENSHIFT_DEFAULT_REGISTRY` variable. This variable was primarily used to enable backwards compatibility of the internal image registry for earlier setups. The `REGISTRY_OPENSHIFT_SERVER_ADDR` variable can be used in its place.

//.APIs removed from Kubernetes 1.27
//[cols="2,2,2",options="header",]
//|===
//|Resource |Removed API |Migrate to

//|`CSIStorageCapacity`
//|`storage.k8s.io/v1beta1`
//|`storage.k8s.io/v1`

//|===

[id="ocp-4-15-rhosp-kuryr-eol"]
==== Installing clusters on {rh-openstack-first} with Kuryr is removed

As of {product-title} {product-version}, support for installing clusters on {rh-openstack} with kuryr is removed.

//[id="ocp-4-15-future-deprecation"]
//=== Notice of future deprecation
//
[id="ocp-4-15-future-removals"]
=== Future Kubernetes API removals

The next minor release of {product-title} is expected to use Kubernetes 1.29. Kubernetes 1.29 has removed a deprecated API.

See the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-29[Deprecated API Migration Guide] in the upstream Kubernetes documentation for the list of Kubernetes API removals.

See link:https://access.redhat.com/articles/6955985[Navigating Kubernetes API deprecations and removals] for information about how to check your cluster for Kubernetes APIs that are planned for removal.

[id="ocp-4-15-bug-fixes"]
== Bug fixes
//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / TALO
//Telco Edge / ZTP
[discrete]
[id="ocp-4-15-api-auth-bug-fixes"]
==== API Server and Authentication

* Previously, the `termination.log` in the kube-apiserver log folder had invalid permissions due to settings in the upstream library. With this release, the upstream library was updated and the `terminate.log` now has the expected permissions. (link:https://issues.redhat.com/browse/OCPBUGS-11856[OCPBUGS-11856])

* Previously, the Cluster Version Operator (CVO) enabled a capability if the existing manifest got the capability annotation after an upgrade. This caused the console to be enabled after upgrading to {product-title} 4.14 for users who had previously disabled the console capability. With this release, the unnecessary console capability was removed from the existing manifest and the console capability is no longer implicitly enabled. (link:https://issues.redhat.com/browse/OCPBUGS-20331[OCPBUGS-20331])

* Previously, when the `openshift-kube-controller-manager` namespace was deleted, the following error was logged repeatedly: `failed to synchronize namespace`. With this release, the error is no longer logged when the `openshift-kube-controller-manager` namespace is deleted. (link:https://issues.redhat.com/browse/OCPBUGS-17458[OCPBUGS-17458])

[discrete]
[id="ocp-4-15-bare-metal-hardware-bug-fixes"]
==== Bare Metal Hardware Provisioning

* Previously, deploying IPv6-only hosts from a dual-stack {ztp} hub prevented the correct callback URL from being passed to the baseboard management controller (BMC). Consequently, an IPv4 URL was passed unconditionally. This issue has been resolved, and the IP version of the URL now depends on the IP version of the BMC address. (link:https://issues.redhat.com/browse/OCPBUGS-23759[OCPBUGS-23759])

* Previously, the Bare Metal Operator (BMO) container had a `hostPort` specified as `60000`, but the `hostPort` was not actually in use despite the specification. As a result, other services could not use port 60000. This fix removes the `hostPort` specification from the container configuration. Now, port 60000 is available for use by other services. (link:https://issues.redhat.com/browse/OCPBUGS-18788[OCPBUGS-18788])

* Previously, the Cluster Baremetal Operator (CBO) failed when it checked the infrastructure `platformStatus` field and returned `nil`. With {product-title} {product-version}, the CBO has been updated so that it checks and returns a blank value when `apiServerInternalIPs` returns `nil`, which resolves this issue. (link:https://issues.redhat.com/browse/OCPBUGS-17589[OCPBUGS-17589])

* Previously, the `inspector.ipxe` configuration used the `IRONIC_IP` variable, which did not account for IPv6 addresses because they have brackets. Consequently, when the user supplied an incorrect `boot_mac_address`, iPXE fell back to the `inspector.ipxe` configuration, which supplied a malformed IPv6 host header since it did not contain brackets.
+
With {product-title} {product-version}, the `inspector.ipxe` configuration has been updated to use the `IRONIC_URL_HOST` variable, which accounts for IPv6 addresses and resolves the issue. (link:https://issues.redhat.com/browse/OCPBUGS-27060[OCPBUGS-27060])

* Previously, there was a bug when attempting to deploy {product-title} on a new bare metal host using RedFish Virtual Media with Cisco UCS hardware. This bug blocked bare metal hosts from new provisions, because Ironic was unable to find a suitable virtual media device. With this update, Ironic does more checks in all available virtual media devices. As a result, Cisco UCS hardware can now be provisioned when using RedFish Virtual Media. (link:https://issues.redhat.com/browse/OCPBUGS-23105[OCPBUGS-23105])

* Previously, when installing {product-title} with the `bootMode` field set to `UEFISecureBoot` on a node where the `secureBoot` field was set to `disabled`, the installation program failed to start. With this update, Ironic has been updated so that you can install {product-title} with `secureBoot` set to `enabled`. (link:https://issues.redhat.com/browse/OCPBUGS-9303[OCPBUGS-9303])

[discrete]
[id="ocp-4-15-builds-bug-fixes"]
==== Builds

* Previously, timestamps were not preserved when copying contents between containers. With this release, the `-p` flag is added to the `cp` command to allow timestamps to be preserved. (link:https://issues.redhat.com/browse/OCPBUGS-22497[OCPBUGS-22497])

[discrete]
[id="ocp-4-15-cloud-compute-bug-fixes"]
==== Cloud Compute

* Previously, an error in the parsing of taints from the `MachineSet` spec meant that the autoscaler could not account for any taint set directly on the spec. Consequently, when relying on the `MachineSet` taints for scaling from zero, the taints from the spec were not considered, which could cause incorrect scaling decisions. With this update, parsing issues within the scale from zero logic have been resolved. As a result, auto scaler can now scale up correctly and identify taints that would prevent workloads from scheduling. (link:https://issues.redhat.com/browse/OCPBUGS-27750[OCPBUGS-27750])

* Previously, an Amazon Web Services (AWS) code that provided image credentials was removed from the kubelet in {product-title} 4.14. Consequently, pulling images from Amazon Elastic Container Registry (ECR) failed without a specified pull secret, because the kubelet could no longer authenticate itself and pass credentials to the container runtime. With this update, a separate credential provider has been configured, which is now responsible for providing ECR credentials for the kubelet. As a result, the kubelet can now pull private images from ECR. (link:https://issues.redhat.com/browse/OCPBUGS-27486[OCPBUGS-27486])

* Previously when deploying a hosted control plane (HCP) KubeVirt cluster the `--node-selector` command, the node selector was not applied to the `kubevirt-cloud-controller-manager` pods within the HCP namespace. Consequentially, you could not pin the entire HCP pods to specific nodes. With this update, this issue has been fixed. (link:https://issues.redhat.com/browse/OCPBUGS-27071[OCPBUGS-27071])

* Previously, the default virtual machine (VM) type for the Microsoft Azure load balancer was changed from `Standard` to `VMSS`. Consequently, the service type load balancer could not attach standard VMs to load balancers. This update reverts these changes to the previous configuration to maintain compatibility with {product-title} deployments. As a result, load balancer attachments are now more consistent. (link:https://issues.redhat.com/browse/OCPBUGS-26210[OCPBUGS-26210])

* Previously, deployments on {rh-openstack} nodes with additional ports with the `enable_port_security` field set to `false` were prevented from creating `LoadBalancer` services. With this update, this issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-22246[OCPBUGS-22246])

* Previously worker nodes on {rh-openstack-first} were named with domain components if the Nova metadata service was unavailable the first time the worker nodes booted. {product-title} expects the node names to be the same as the Nova instance. The name discrepancy caused the nodes' certificate request to be rejected and the nodes could not join the cluster. With this update, the worker nodes will wait and retry the metadata service indefinitely on first boot ensuring the nodes are correctly named. (link:https://issues.redhat.com/browse/OCPBUGS-22200[OCPBUGS-22200])

* Previously, the cluster autoscaler crashed when used with nodes that have Container Storage Interface (CSI) storage. The issue is resolved in this release. (link:https://issues.redhat.com/browse/OCPBUGS-23096[OCPBUGS-23096])

* Previously, in certain proxied environments, the Amazon Web Services (AWS) metadata service might not have been present on initial startup, and might have only been available shortly after startup. The kubelet hostname fetching did not account for this delay and, consequently, the node would fail to boot because it would not have a valid hostname. This update ensures that the hostname fetching script retries on failure for some time. As a result, inaccessibility of the metadata service is tolerated for a short period of time. (link:https://issues.redhat.com/browse/OCPBUGS-20369[OCPBUGS-20369])

* In {product-title} version 4.14 and later, there is a known issue that causes installation of Microsoft Azure Stack Hub to fail. Microsoft Azure Stack Hub clusters that are upgraded to 4.14 or later might encounter load balancer configuration issues as nodes scale up or down. Installing or upgrading to 4.14 in Microsoft Azure Stack Hub environments is not recommended until this issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-20213[OCPBUGS-20213])

* Previously, some conditions during the startup process of the Cluster Autoscaler Operator caused a lock that prevented the Operator from successfully starting and marking itself available. As a result, the cluster became degraded. The issue is resolved with this release. (link:https://issues.redhat.com/browse/OCPBUGS-18954[OCPBUGS-18954])

* Previously, attempting to perform a Google Cloud Platform XPN internal cluster installation failed when control nodes were added to a second internal instance group. This bug has been fixed. (link:https://issues.redhat.com/browse/OCPBUGS-5755[OCPBUGS-5755])

* Previously, the termination handler prematurely exited before marking a node for termination. This condition occurred based on the timing of when the termination signal was received by the controller. With this release, the possibility of early termination is accounted for by introducing an additional check for termination. (link:https://issues.redhat.com/browse/OCPBUGS-2117[OCPBUGS-2117])

* Previously, when the `Build` cluster capability was not enabled, the cluster version Operator (CVO) failed to synchronize the build informer, and did not start successfully. With this release, the CVO successfully starts when the `Build` capability is not enabled. (link:https://issues.redhat.com/browse/OCPBUGS-22956[OCPBUGS-22956])

[discrete]
[id="ocp-4-15-cloud-cred-operator-bug-fixes"]
==== Cloud Credential Operator

* Previously, the Cloud Credential Operator utility (`ccoctl`) created custom GCP roles at the cluster level, so each cluster contributed to the quota limit on the number of allowed custom roles. Because of GCP deletion policies, deleted custom roles continue to contribute to the quota limit for many days after they are deleted. With this release, custom roles are added at the project level instead of the cluster level to reduce the total number of custom roles created. Additionally, an option to clean up custom roles is now available when deleting the GCP resources that the `ccoctl` utility creates during installation. These changes can help avoid reaching the quota limit on the number of allowed custom roles. (link:https://issues.redhat.com/browse/OCPBUGS-28850[OCPBUGS-28850])

* Previously, when the `Build` cluster capability was not enabled, the Cluster Version Operator (CVO) failed to synchronize the build informer and did not start successfully. With this release, the CVO successfully starts when the `Build` capability is not enabled. (link:https://issues.redhat.com/browse/OCPBUGS-26510[OCPBUGS-26510])

* Previously, buckets created by running the `ccoctl azure create` command were prohibited from allowing public blob access due to a change in the default behavior of Microsoft Azure buckets. With this release, buckets created by running the `ccoctl azure create` command are explicitly set to allow public blob access. (link:https://issues.redhat.com/browse/OCPBUGS-22369[OCPBUGS-22369])

* Previously, an Azure Managed Identity role was omitted from the Cloud Controller Manager service account. As a result, the Cloud Controller Manager could not manage service type load balancers in environments deployed to existing VNets with a private publishing method. With this release, the missing role was added to the Cloud Credential Operator utility (`ccoctl`) and Azure Managed Identity installations into an existing VNet with private publishing is possible. (link:https://issues.redhat.com/browse/OCPBUGS-21745[OCPBUGS-21745])

* Previously, the Cloud Credential Operator did not support updating the vCenter server value in the root secret `vshpere-creds` that is stored in the `kube-system` namespace. As a result, attempting to update this value caused both the old and new values to exist because the component secrets were not synchronized correctly. With this release, the Cloud Credential Operator resets the secret data during synchronization so that updating the vCenter server value is supported. (link:https://issues.redhat.com/browse/OCPBUGS-20478[OCPBUGS-20478])

* Previously, the Cloud Credential Operator utility (`ccoctl`) failed to create AWS Security Token Service (STS) resources in China regions because the China region DNS suffix `.amazonaws.com.cn` differs from the suffix `.amazonaws.com` that is used in other regions. With this release, `ccoctl` can detect the correct DNS suffix and use it to create the required resources. (link:https://issues.redhat.com/browse/OCPBUGS-13597[OCPBUGS-13597])

[discrete]
[id="ocp-4-15-cluster-version-operator-bug-fixes"]
==== Cluster Version Operator

* The Cluster Version Operator (CVO) continually retrieves update recommendations and evaluates known conditional update risks against the current cluster state. Previously, failing risk evaluations blocked the CVO from fetching new update recommendations. When the risk evaluations were failing because the update recommendation service served a poorly-defined update risk, this issue could prevent the CVO from noticing the update recommendation service serving an improved risk declaration. With this release, the CVO continues to poll the update recommendation service regardless of whether update risks are successfully evaluated or not. (link:https://issues.redhat.com/browse/OCPBUGS-25949[OCPBUGS-25949])

[discrete]
[id="ocp-4-15-dev-console-bug-fixes"]
==== Developer Console

* Previously, `BuildRun` logs were not visible in the *Logs* Tab of the BuildRun due to the recent update in the API version of the specified resources. With this update, the Logs of the `TaskRuns` were added back into the *Logs* tab of the BuildRun for both v1alpha1 and v1beta1 versions of the builds Operator. (link:https://issues.redhat.com/browse/OCPBUGS-29283[OCPBUGS-29283])

* Previously, the console UI failed when a *Task* in the Pipeline Builder that was previously installed from the *ArtifactHub* was selected and an error page displayed. With this update, the console UI no longer expects optional data and the console UI no longer fails. (link:https://issues.redhat.com/browse/OCPBUGS-24001[OCPBUGS-24001])

* Previously, the *Edit Build* and *BuildRun* options in the *Actions* menu of the Shipwright Plugin did not allow you to edit in the YAML tab. With this update, you can edit in the YAML tab. (link:https://issues.redhat.com/browse/OCPBUGS-23164[OCPBUGS-23164])

* Previously, the console searched only for the file name `Dockerfile` in a repository to identify the repository suitable for the `Container` strategy in the Import Flows. Since other containerization tools are available, support for the `Containerfile` file name is now suitable for the `Container` strategy. (link:https://issues.redhat.com/browse/OCPBUGS-22976[OCPBUGS-22976])

* Previously, when an unauthorized user opened a link to the console that contains path and query parameters, and they were redirected to a login page, the query parameters did not restore after the login was successful. As a result, the user needed to restore the search or click the link to the console again. With this update, the latest version saves and restores the query parameters similar to the path. (link:https://issues.redhat.com/browse/OCPBUGS-22199[OCPBUGS-22199])

* Previously, when navigating to the *Create Channel* page from the *Add* or *Topology* view, the default name as `Channel` is present, but the *Create* button is disabled with `Required` showing under the name field. With this update, if the default channel name is added then the `Required` message will not display when clicking the *Create* button. (link:https://issues.redhat.com/browse/OCPBUGS-19783[OCPBUGS-19783])

* Previously, there were similar options to choose from when using the quick search function. With this update, the *Source-to-image* option is differentiated from the *Samples* option in the *Topology* quick search. (link:https://issues.redhat.com/browse/OCPBUGS-18371[OCPBUGS-18371])

* Previously, when {serverless-product-name} Operator was installed and the Knative (Kn) serving instance had not been created, then when navigating to the *Global configuration* page from *Administration* -> *Cluster Settings* and clicking *Knative-serving* a `404 page not found` error was displayed. With this update, before adding *Knative-serving* to the *Global configuration*, a check is in place to determine if a Knative serving instance is created. (link:https://issues.redhat.com/browse/OCPBUGS-18267[OCPBUGS-18267])

* Previously there was an issue with the *Edit Knative Service* form that prevented users from editing the Knative service they previously created. With this update, you can edit a Knative service that was previously created. (link:https://issues.redhat.com/browse/OCPBUGS-6513[OCPBUGS-6513])

[discrete]
[id="ocp-4-15-cloud-etcd-operator-bug-fixes"]
==== etcd Cluster Operator

* Previously, the `cluster-backup.sh` script cached the `etcdctl` binary on the local machine indefinitely, making updates impossible. With this update, the `cluster-backup.sh` script pulls the latest `etcdctl` binary each time it is run. (link:https://issues.redhat.com/browse/OCPBUGS-19052[OCPBUGS-19052])

[discrete]
[id="ocp-4-15-hosted-control-plane-bug-fixes"]
==== Hosted Control Plane

* Previously, when using a custom Container Network Interface (CNI) plugin in a hosted cluster, role-based access control (RBAC) rules were configured only when you set the `hostedcluster.spec.networking.networkType` field to `Calico`. Role-based access control (RBAC) rules were not configured when you set the `hostedcluster.spec.networking.networkType` field to `Other`. With this release, RBAC rules are configured properly, when you set the `hostedcluster.spec.networking.networkType` field to `Other`. (link:https://issues.redhat.com/browse/OCPBUGS-28235[OCPBUGS-28235])

* Previously, a node port failed to expose properly because the `ipFamilyPolicy` field was set to `SingleStack` for the `kube-apiserver` resource. With this update, if the `ipFamilyPolicy` is set to `PreferredDualStack`, node port is exposed properly. (link:https://issues.redhat.com/browse/OCPBUGS-23350[OCPBUGS-23350])

* Previously, after configuring the Open Virtual Network (OVN) for a hosted cluster, the `cloud-network-config-controller`, `multus-admission-controller`, and `ovnkube-control-plane` resources were missing the `hypershift.openshift.io/hosted-control-plane:{hostedcluster resource namespace}-{cluster-name}` label. With this update, after configuring the Open Virtual Network (OVN) for a hosted cluster, the `cloud-network-config-controller`, `multus-admission-controller`, and `ovnkube-control-plane` resources contain the `hypershift.openshift.io/hosted-control-plane:{hostedcluster resource namespace}-{cluster-name}` label. (link:https://issues.redhat.com/browse/OCPBUGS-19370[OCPBUGS-19370])

* Previously, after creating a hosted cluster, to create a config map, if you used a name other than `user-ca-bundle`, the deployment if the Control Plane Operator (CPO) failed. With this update, you can use unique names to create a config map. The CPO is deployed successfully. (link:https://issues.redhat.com/browse/OCPBUGS-19419[OCPBUGS-19419])

* Previously, hosted clusters with `.status.controlPlaneEndpoint.port: 443` would mistakenly expose port 6443 for public and private routers. With this update, hosted clusters with `.status.controlPlaneEndpoint.port: 443` only expose the port 443. (link:https://issues.redhat.com/browse/OCPBUGS-20161[OCPBUGS-20161])

* Previously, if the Kube API server is exposed by using IPv4 and IPv6, and the IP address is set in the `HostedCluster` resource, the IPv6 environment did not work properly. With this update, when the Kube API server is exposed by using IPv4 and IPv6, the IPv6 environment works properly. (link:https://issues.redhat.com/browse/OCPBUGS-20246[OCPBUGS-20246])

* Previously, if the console Operator and Ingress pods were located on the same node, the console Operator would fail and mark the console cluster Operator as unavailable. With this release, if the console Operator and Ingress pods are located on the same node, the console Operator no longer fails. (link:https://issues.redhat.com/browse/OCPBUGS-23300[OCPBUGS-23300])

* Previously, if uninstallation of a hosted cluster is stuck, status of the Control Plane Operator (CPO) was reported incorrectly. With this update, the status of the CPO is reported correctly. (link:https://issues.redhat.com/browse/OCPBUGS-26412[OCPBUGS-26412])

* Previously, if you tried to override the {product-title} version while the initial upgrade was in progress, a hosted cluster upgrade would fail. With this update, if you override the current upgrade with a new {product-title} version, the upgrade completes successfully. (link:https://issues.redhat.com/browse/OCPBUGS-18122[OCPBUGS-18122])

* Previously, if you update the pull secret for the hosted control planes, it did not reflect on the worker nodes immediately. With this update, when you change the pull secret, reconciliation is triggered and worker nodes are updated with a new pull secret immediately. (link:https://issues.redhat.com/browse/OCPBUGS-19834[OCPBUGS-19834])

* Previously, the Hypershift Operator would report time series for node pools that no longer existed. With this release, the Hypershift Operator reports time series for node pools correctly. (link:https://issues.redhat.com/browse/OCPBUGS-20179[OCPBUGS-20179])

* Previously, the `--enable-uwm-telemetry-remote-write` flag was enabled by default. This setting blocked the telemetry reconciliation. With this update, you can disable the `--enable-uwm-telemetry-remote-write` flag to allow telemetry reconciliation. (link:https://issues.redhat.com/browse/OCPBUGS-26410[OCPBUGS-26410])

* Previously, the control Plane Operator (CPO) failed to update the VPC endpoint service when an IAM role path ARN was provided as the additional allowed principal: `arn:aws:iam::${ACCOUNT_ID}:role/${PATH}/name` With this update, The CPO updates the VPC endpoint service with the `arn:aws:iam::${ACCOUNT_ID}:role/${PATH}/name` allowed principal successfully. (link:https://issues.redhat.com/browse/OCPBUGS-23511[OCPBUGS-23511])

* Previously, to customize OAuth templates,
if you configured the `HostedCluster.spec.configuration.oauth` field, this setting did not reflect in a hosted cluster. With this update, you can configure the `HostedCluster.spec.configuration.oauth` field in a hosted cluster successfully. (link:https://issues.redhat.com/browse/OCPBUGS-15215[OCPBUGS-15215])

* Previously, when deploying a hosted cluster by using a dual stack networking, by default, the `clusterIP` field was set to an IPv6 network instead of an IPv4 network. With this update, when deploying a hosted cluster by using a dual stack networking, the `clusterIP` field is set to IPv4 network by default. (link:https://issues.redhat.com/browse/OCPBUGS-16189[OCPBUGS-16189])

* Previously, when deploying a hosted cluster, if you configure the `advertiseAddress` field in the `HostedCluster` resource, the hosted cluster deployment would fail. With this release, you can deploy a hosted cluster successfully after configuring the `advertiseAddress` field in the `HostedCluster` resource. (link:https://issues.redhat.com/browse/OCPBUGS-19746[OCPBUGS-19746])

* Previously, when you set the `hostedcluster.spec.networking.networkType` field to `Calico` in a hosted cluster, the Cluster Network Operator did not have enough role-based access control (RBAC) permissions to deploy the `network-node-identity` resource. With this update, the `network-node-identity` resource is deployed successfully. (link:https://issues.redhat.com/browse/OCPBUGS-23083[OCPBUGS-23083])

* Previously, you could not update the default configuration for audit logs in a hosted cluster. Therefore, components of a hosted cluster could not generate audit logs. With this update, you can generate audit logs for components of a hosted cluster by updating the default configuration. (link:https://issues.redhat.com/browse/OCPBUGS-13348[OCPBUGS-13348])

[discrete]
[id="ocp-4-15-image-registry-bug-fixes"]
==== Image Registry

* Previously, the Image Registry pruner relied on a cluster role that was managed by the OpenShift API server. This could cause the pruner job to intermittently fail during an upgrade. Now, the Image Registry Operator is responsible for creating the pruner cluster role, which resolves the issue. (link:https://issues.redhat.com/browse/OCPBUGS-18969[OCPBUGS-18969])

* The Image Registry Operator makes API calls to the storage account list endpoint as part of obtaining access keys. In projects with several {product-title} clusters, this might lead to API limits being reached. As a result, `429` errors were returned when attempting to create new clusters. With this update, the time between calls has been increased from 5 minutes to 20 minutes, and API limits are no longer reached. (link:https://issues.redhat.com/browse/OCPBUGS-18469[OCPBUGS-18469])

* Previously, the default low settings for QPS and Burst caused the image registry to return with a gateway timeout error when API server requests were not returned in an appropriate time. To resolve this issue, users had to restart the image registry. With this update, the default settings for QPS and Burst have been increased, and this issue no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-18999[OCPBUGS-18999])

* Previously, when creating the deployment resource for the Cluster Image Registry Operator, error handling used a pointer variable without checking if the value was `nil` first. Consequently, when the pointer value was `nil`, a panic was reported in the logs. With this update, a nil check was added so that the panic is no longer reported in the logs. (link:https://issues.redhat.com/browse/OCPBUGS-18103[OCPBUGS-18103])

* Previously, the {product-title} 4.14 release introduced a change that gave users the perception that their images were lost when updating from {product-title} version 4.13 to 4.14. A change to the default internal registry caused the registry to use an incorrect path when using the Microsoft Azure object storage. With this release, the correct path is used and a job has been added to the registry operator that moves any blobs pushed to the registry that used the wrong storage path into the correct storage path, which effectively merges the two distinct storage paths into a single path.
+
[NOTE]
====
This fix does *not* work on Azure Stack Hub (ASH). ASH users who used OCP versions 4.14.0 through 4.14.13 when upgrading to 4.14.14+ will need to execute manual steps to move their blobs to the correct storage path.
====
+
(link:https://issues.redhat.com/browse/OCPBUGS-29525[OCPBUGS-29525])

[discrete]
[id="ocp-4-15-installer-bug-fixes"]
==== Installer

* Previously, installing a cluster on AWS might fail in some cases due to a validation error. With this update, the installation program produces the necessary cloud configuration object to satisfy the machine config operator. This results in the installation succeeding. (link:https://issues.redhat.com/browse/OCPBUGS-12707[OCPBUGS-12707])

* Previously, installing a cluster on GCP using a service account attached to a VM for authentication might fail due to an internal data validation bug. With this release, the installation program has been updated to correctly validate the authentication parameters when using a service account attached to a VM. (link:https://issues.redhat.com/browse/OCPBUGS-19376[OCPBUGS-19376])

* Previously, the vSphere connection configuration interface showed the network name instead of the cluster name in the "vCenter cluster" field. With this update, the "vCenter cluster" field has been updated to display the cluster name. (link:https://issues.redhat.com/browse/OCPBUGS-23347[OCPBUGS-23347])

* Previously, when you authenticated with the `credentialsMode` parameter not set to `Manual` and you used the `gcloud cli` tool, the installation program retrieved Google Cloud Platform (GCP) credentials from the `osServiceAccount.json` file. This operation caused the GCP cluster installation to fail. Now, a validation check scans the `install-config.yaml` file and prompts you with a message if you did not set `credentialsMode` to `Manual`. Note that in `Manual` mode, you must edit the manifests and provide the credentials. (link:https://issues.redhat.com/browse/OCPBUGS-17757[OCPBUGS-17757])

* Previously when you attempted to install an {product-title} on VMware vSphere by using installer-provisioned infrastructure, a resource pool object would include a double backslash. This format caused the installation program to generate an incorrect path to network resources that in turn caused the installation operation to fail. After the installation program processed this resource pool object, the program outputted a "network not found" error message. Now, the installation program retrieves the cluster object for the purposes of joining the InventoryPath with the network name so that the program specifies the correct path to the resource pool object. (link:https://issues.redhat.com/browse/OCPBUGS-23376[OCPBUGS-23376])

* Previously, after installing an Azure Red Hat OpenShift cluster, some cluster Operators were unavailable. This was the result of one of the cluster’s load balancers not being created as part of the installation process. With this update, the load balancer is correctly created. After installing a cluster, all cluster Operators are available. (link:https://issues.redhat.com/browse/OCPBUGS-24191[OCPBUGS-24191])

* Previously, if the VMware vSphere cluster contained an ESXi host that was offline, the installation failed with a "panic: runtime error: invalid memory address or nil pointer dereference" message. With this update, the error message states that the ESXi host is unavailable. (link:https://issues.redhat.com/browse/OCPBUGS-20350[OCPBUGS-20350])

* Previously, if you only used the default machine configuration to specify existing AWS security groups when installing a cluster on AWS (`platform.aws.defaultMachinePlatform.additonalSecurityGroupsIDs`), the security groups were not applied to control plane machines. With this update, existing AWS security groups are correctly applied to control planes when they are specified using the default machine configuration. (link:https://issues.redhat.com/browse/OCPBUGS-20525[OCPBUGS-20525])

* Previously, installing a cluster on AWS failed when the specified machine instance type (`platform.aws.type`) did not support the machine architecture that was specified for control plane or compute machines (`controlPlane.architecture` and `compute.architecture`). With this update, the installation program now checks to determine if the machine instance type supports the specified architecture and displays an error message if it does not. (link:https://issues.redhat.com/browse/OCPBUGS-26051[OCPBUGS-26051])

* Previously, the installation program did not validate some configuration settings before installing the cluster. This behavior occurred when these settings were only specified in the default machine configuration (`platform.azure.defaultMachinePlatform`). As a result, the installation would succeed even if the following conditions were met:
+
** An unsupported machine instance type was specified.
** Additional functionality, such as accelerated networking or the use of Azure ultra disks, was not supported by the specified machine instance type.

+
With this fix, the installation program now displays an error message that specifies the unsupported configuration. (link:https://issues.redhat.com/browse/OCPBUGS-20364[OCPBUGS-20364])

* Previously, when installing an AWS cluster to the Secret Commercial Cloud Services (SC2S) region and specifying existing AWS security groups, the installation failed with an error that stated that the functionality was not available in the region. With this fix, the installation succeeds. (link:https://issues.redhat.com/browse/OCPBUGS-18830[OCPBUGS-18830])

* Previously, when you specified Key Management Service (KMS) encryption keys in the `kmsKeyARN` section of the `install-config.yaml` configuration file for installing a cluster on Amazon Web Services (AWS), permission roles were not added during the cluster installation operation. With this update, after you specify the keys in the configuration file, an additional set of keys are added to the cluster so that the cluster successfully installs. If you specify the `credentialsMode` parameter in the configuration file, all KMS encryption keys are ignored. (link:https://issues.redhat.com/browse/OCPBUGS-13664[OCPBUGS-13664])

* Previously, Agent-based installations on {oci-first} did not show a console displaying installation progress to users, making it more difficult to track installation progress. With this update, Agent-based installations on {oci} now display installation progress on the console. (link:https://issues.redhat.com/browse/OCPBUGS-19092[OCPBUGS-19092])

* Previously, if static networking was defined in the `install-config.yaml` or `agent-config.yaml` files for the Agent-based Installer, and an interface name was over 15 characters long, the network manager did not allow the interface to come up. With this update, interface names longer than 15 characters are truncated and the installation can proceed. (link:https://issues.redhat.com/browse/OCPBUGS-18552[OCPBUGS-18552])

* Previously, if the user did not specify the `rendezevousIP` field in the `agent-config.yaml` file and hosts were defined in the same file with static network configuration, then the first host was designated as a rendezvous node regardless of its role. This caused the installation to fail.
With this update, the Agent-based Installer prioritizes the rendezvous node search by first looking among the hosts with a `master` role and a static IP defined. If none is found, then a potential candidate is searched for through the hosts that do not have a role defined. Hosts with a static network configuration that are explicitly configured with a `worker` role are ignored. (link:https://issues.redhat.com/browse/OCPBUGS-5471[OCPBUGS-5471])

* Previously, the Agent console application was shown during the boot process of all Agent-based installations, enabling network customizations before proceeding with the installation. Because network configuration is rarely needed during cloud installations, this would unnecessarily slow down installations on {oci-first}.
+
With this update, Agent-based installations on {oci} no longer show the Agent console application and proceed more quickly. (link:https://issues.redhat.com/browse/OCPBUGS-19093[OCPBUGS-19093])

* Previously, the Agent-based Installer enabled an external Cloud Controller Manager (CCM) by default when the platform was defined as `external`. This prevented users from disabling the external CCM when performing installations on cloud platforms that do not require one. With this update, users are required to enable an external CCM only when performing an Agent-based installation on {oci-first}. (link:https://issues.redhat.com/browse/OCPBUGS-18455[OCPBUGS-18455])

* Previously, the `agent wait-for` command failed to record logs in the `.openshift_install.log` file. With this update, logs are recorded in the `.openshift_install.log` file when you use the `agent wait-for` command. (link:https://issues.redhat.com/browse/OCPBUGS-5728[OCPBUGS-5728])

* Previously, the `assisted-service` on the bootstrap machine became unavailable after the bootstrap node rebooted, preventing any communication from the `assisted-installer-controller`. This stopped the `assisted-installer-controller` from removing uninitialized taints from worker nodes, causing the cluster installation to hang waiting on cluster Operators.
+
With this update, the `assisted-installer-controller` can remove the uninitialized taints even if `assisted-service` becomes unavailable, and the installation can proceed. (link:https://issues.redhat.com/browse/OCPBUGS-20049[OCPBUGS-20049])

* Previously, the platform type was erroneously required to be lowercase in the `AgentClusterInstall` cluster manifest used by the Agent-based Installer. With this update, mixed case values are required, but the original lowercase values are now accepted and correctly translated. (link:https://issues.redhat.com/browse/OCPBUGS-19444[OCPBUGS-19444])

* Previously, the `manila-csi-driver-controller-metrics` service had empty endpoints due to an incorrect name for the app selector. With this release the app selector name is changed to `openstack-manila-csi` and the issue is fixed. (link:https://issues.redhat.com/browse/OCPBUGS-9331[OCPBUGS-9331])

* Previously, the assisted installer removed the uninitialized taints for all vSphere nodes which prevented the vSphere CCM from initializing the nodes properly. This caused the vSphere CSI operator to degrade during the initial cluster installation because the node's provider ID was missing. With this release, the assisted installer checks if vSphere credentials were provided in the `install-config.yaml`. If credentials were provided, the OpenShift version is greater or equal to 4.15, and the agent installer was used, the assisted-installer and assisted-installer-controller do not remove the uninitialized taints. This means that the node's providerID and VM's UUID are properly set and the vSphere CSI operator is installed. (link:https://issues.redhat.com/browse/OCPBUGS-29485[OCPBUGS-29485])

[discrete]
[id="ocp-4-15-kube-controller-bug-fixes"]
==== Kubernetes Controller Manager

* Previously, when the `maxSurge` field was set for a daemon set and the toleration was updated, pods failed to scale down, which resulted in a failed rollout due to a different set of nodes being used for scheduling. With this release, nodes are properly excluded if scheduling constraints are not met, and rollouts can complete successfully. (link:https://issues.redhat.com/browse/OCPBUGS-19452[OCPBUGS-19452])
//
//[discrete]
//[id="ocp-4-15-kube-scheduler-bug-fixes"]
//==== Kubernetes Scheduler

[discrete]
[id="ocp-4-15-machine-config-operator-bug-fixes"]
==== Machine Config Operator

* Previously, a misspelled environment variable prevented a script from detecting that the `node.env` file was present. This caused the contents of the `node.env` file to be overwritten after each boot, and the kubelet hostname could not be changed. With this update, the environment variable spelling is corrected and edits to the `node.env` file persist across reboots. (link:https://issues.redhat.com/browse/OCPBUGS-27307[OCPBUGS-27307])

* Previously, the Machine Config Operator allowed user-provided certificate authority updates to be made without requiring a new machine config to trigger. Because the new write method for these updates was missing a newline character, it caused validation errors for the contents of the CA file on-disk and the Machine Config Daemon became degraded. With this release, the CA file contents are fixed, and updates proceed as expected. (link:https://issues.redhat.com/browse/OCPBUGS-25424[OCPBUGS-25424])

* Previously, the Machine Config Operator allowed user-provided certificate authority bundle changes to be applied to the cluster without needing a machine config, to prevent disruption. Because of this, the `user-ca` bundle was not propagating to applications running on the cluster and required a reboot to see the changes take effect. With this update, the MCO now runs the `update-ca-trust` command and restarts the CRI-O service so that the new CA properly applies. (link:https://issues.redhat.com/browse/OCPBUGS-24035[OCPBUGS-24035])

* Previously, the initial mechanism used by the Machine Config Operator to handle image registry certs would delete and create new config maps rather than patching existing ones. This caused a significant increase in API usage from the MCO. With this update, the mechanism has been updated so that it uses a JSON patch instead, thereby resolving the issue. (link:https://issues.redhat.com/browse/OCPBUGS-18800[OCPBUGS-18800])

* Previously, the Machine Config Operator was pulling the `baremetalRuntimeCfgImage` container image multiple times: the first time to obtain node details and subsequent times to verify that the image is available. This caused issues during certificate rotation in situations where the mirror server or Quay was not available, and subsequent image pulls would fail. However, if the image is already on the nodes due to the first image pull then the nodes should start the kubelet regardless. With this update, the `baremetalRuntimeCfgImage` image is only pulled one time, thereby resolving the issue. (https://issues.redhat.com/browse/OCPBUGS-18772[OCPBUGS-18772])

// Jira required a login so this note should not have been documented.
//* Previously, the `nmstatectl` command failed to retrieve the correct permanent MAC address during {product-title} updates for some network environments. This caused the interface to be renamed and the bond connection on the node to break during the update. With this release, patches were applied to the `nmstate` package and MCO to prevent renaming, and updates proceed as expected. (https://issues.redhat.com/browse/OCPBUGS-17877[OCPBUGS-17877])

* Previously, the Machine Config Operator became the default provider of image registry certificates and the `node-ca` daemon was removed. This caused issues with the HyperShift Operator, because removing the `node-ca` daemon also removed the image registry path in the Machine Config Server (MCS), which HyperShift uses to get the Ignition configuration and start the bootstrap process. With this update, a flag containing the MCS image registry data is provided, which Ignition can use during the bootstrap process, thereby resolving the issue. (https://issues.redhat.com/browse/OCPBUGS-17811[OCPBUGS-17811])

* Previously, older {op-system} boot images contained a race condition between services on boot that prevented the node from running the `rhcos-growpart` command before it pulled images, preventing the node from starting up. This caused node scaling to sometimes fail on clusters that use old boot images because it was determined there was no room left on the disk. With this update, processes were added to the Machine Config Operator for stricter ordering of services so that nodes boot correctly.
+
[NOTE]
====
In these situations, updating to newer boot images prevents similar issues from occurring.
====
+
(https://issues.redhat.com/browse/OCPBUGS-15087[OCPBUGS-15087])

* Previously, the Machine Config Operator (MCO) leveraged the `oc image extract` command to pull images during updates but the `ImageContentSourcePolicy` (ICSP) object was not respected when pulling those images. With this update, the MCO now uses the `podman pull` command internally and images are pulled from the location as configured in the ICSP. (https://issues.redhat.com/browse/OCPBUGS-13044[OCPBUGS-13044])

[discrete]
[id="ocp-4-15-management-console-bug-fixes"]
==== Management Console

* Previously, the *Expand PVC* modal assumed the existing PVC had a `spec.resources.requests.storage` value that includes a unit. As a result, when the *Expand PVC* modal was used to expand a PVC that had a `requests.storage` value without a unit, the console would display an incorrect value in the modal. With this update, the console was updated to handle storage values with and without a unit. (link:https://issues.redhat.com/browse/OCPBUGS-27909[OCPBUGS-27909])

* Previously, the console check to determine if a file is binary was not robust enough. As a result, XML files were misidentified as binary and not displaying in the console. With this update, an additional check was added to more precisely check if a file is binary. (link:https://issues.redhat.com/browse/OCPBUGS-26591[OCPBUGS-26591])

* Previously, the *Node Overview* page failed to render when a `MachineHealthCheck` without `spec.unhealthyConditions` existed on a cluster. With this update, the *Node Overview* page was updated to allow for `MachineHealthChecks` without `spec.unhealthyConditions`. Now, the *Node Overview* page renders even if `MachineHealthChecks` without `spec.unhealthyConditions` are present on the cluster. (link:https://issues.redhat.com/browse/OCPBUGS-25140[OCPBUGS-25140])

* Previously, the console was not up-to-date with the newest matchers key for alert notification receivers, and the alert manager receivers created by the console utilized the older match key. With this update, the console uses matchers instead, and converts any existing match instances to matchers when modifying an existing alert manager receiver. (link:https://issues.redhat.com/browse/OCPBUGS-23248[OCPBUGS-23248])

* Previously, impersonation access was incorrectly applied. With this update, the console correctly applies impersonation access. (link:https://issues.redhat.com/browse/OCPBUGS-23125[OCPBUGS-23125])

* Previously, when the Advanced Cluster Management for Kubernetes (ACM) and multicluster engine for Kubernetes (MCE) Operators are installed and their plugins are enabled, the YAML code Monaco editor failed to load.
With this update, optional resource chaining was added to prevent a failed resource call, and the YAML editor no longer fails to load when the ACM and MCE Operators are installed and their plugins enabled. (link:https://issues.redhat.com/browse/OCPBUGS-22778[OCPBUGS-22778])

[discrete]
[id="ocp-4-15-monitoring-bug-fixes"]
==== Monitoring

* Previously, the monitoring-plugin component did not start if IPv6 was disabled for a cluster. This release updates the component to support the following internet protocol configurations in a cluster: IPv4 only, IPv6 only, and both IPv4 and IPv6 simultaneously. This change resolves the issue, and the monitoring-plugin component now starts up if the cluster is configured to support only IPv6. (link:https://issues.redhat.com/browse/OCPBUGS-21610[OCPBUGS-21610])

* Previously, instances of Alertmanager for core platform monitoring and for user-defined projects could inadvertently become peered during an upgrade. This issue could occur when multiple Alertmanager instances were deployed in the same cluster. This release fixes the issue by adding a `--cluster.label` flag to Alertmanager that helps to block any traffic that is not intended for the cluster. (link:https://issues.redhat.com/browse/OCPBUGS-18707[OCPBUGS-18707])

* Previously, it was not possible to use text-only email templates in an Alertmanager configuration to send text-only email alerts. With this update, you can configure Alertmanager to send text-only email alerts by setting the `html` field of the email receiver to an empty string. (link:https://issues.redhat.com/browse/OCPBUGS-11713[OCPBUGS-11713])

* Previously, Thanos Querier was unable to query pod metrics because the supporting `kube-rbac-proxy` instance disallowed `metrics.k8s.io/v1beta1/pods`. With this update, the `kube-rbac-proxy` configuration for Thanos Querier is fixed and you can now successfully query pod metrics. (link:https://issues.redhat.com/browse/OCPBUGS-17035[OCPBUGS-17035])

[discrete]
[id="ocp-4-15-networking-bug-fixes"]
==== Networking

* Previously, when creating an IngressController with an empty spec, the IngressController's status showed `Invalid`. However, the `route_controller_metrics_routes_per_shard` metric would still get created. When the invalid IngressController was deleted, the `route_controller_metrics_routes_per_shard` metric would not clear, and it would show information for that metric. With this update, metrics are only created for IngressControllers that are admitted, which resolves this issue. (link:https://issues.redhat.com/browse/OCPBUGS-3541[OCPBUGS-3541])

* Previously, timeout values larger than what Go programming language could parse were not properly validated. Consequently, timeout values larger than what HAProxy could parse caused issues with HAProxy. With this update, if the timeout specifies a value larger than what can be parsed, it is capped at the maximum that HAProxy can parse. As a result, issues are no longer caused for HAProxy. (link:https://issues.redhat.com/browse/OCPBUGS-6959[OCPBUGS-6959])

* Previously, an external neighbor could have its MAC address changed while the cluster was shutting down or hibernating. Although a Gratuitous Address Resolution Protocol (GARP) should notify other neighbors about this change, the cluster would not process the GARP because it was not running. When the cluster was brought back up, that neighbor might not be reachable from the OVN-Kubernetes cluster network because the stale MAC address was being used. This update enables an aging mechanism so that a neighbor's MAC address is periodically refreshed every 300 seconds. (link:https://issues.redhat.com/browse/OCPBUGS-11710[OCPBUGS-11710])

* Previously, when an IngressController was configured with SSL/TLS, but did not have the `clientca-configmap` finalizer, the Ingress Operator would try to add the finalizer without checking whether the IngressController was marked for deletion. Consequently, if an IngressController was configured with SSL/TLS and was subsequently deleted, the Operator would correctly remove the finalizer. It would then repeatedly, and erroneously, try and fail to update the IngressController to add the finalizer back, resulting in error messages in the Operator's logs.
+
With this update, the Ingress Operator no longer adds the `clientca-configmap` finalizer to an IngressController that is marked for deletion. As a result, the Ingress Operator no longer tries to perform erroneous updates, and no longer logs the associated errors. (link:https://issues.redhat.com/browse/OCPBUGS-14994[OCPBUGS-14994])

* Previously, a race condition occurred between the handling of pods that had been scheduled and the pods that had been completed on a node when OVN-Kubernetes started. This condition often occurred when nodes rebooted. Consequently, the same IP was erroneously assigned to multiple pods. This update fixes the race condition so that the same IP is not assigned to multiple pods in those circumstances. (link:https://issues.redhat.com/browse/OCPBUGS-16634[OCPBUGS-16634])

* Previously, there was an error that caused a route to be rejected due to a duplicate host claim. When this occurred, the system would mistakenly select the first route it encountered, which was not always the conflicting route. With this update, all routes for the conflicting host are first retrieved and then sorted based on their submission time. This allows the system to accurately determine and select the newest conflicting route. (link:https://issues.redhat.com/browse/OCPBUGS-16707[OCPBUGS-16707])

* Previously, when a new `ipspec-host` pod was started, it would clear or remove the existing `XFRM` state. Consequently, it would remove existing north-south traffic policies. This issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-19817[OCPBUGS-19817])

* Previously, the `ovn-k8s-cni-overlay, topology:layer2` NetworkAttachmentDefinition did not work in a hosted pod when using the Kubevirt provider. Consequently, the pod did not start. This issue has been resolved, and pods can now start with an `ovn-k8s-cni-overlay` NetworkAttachmentDefinition. (link:https://issues.redhat.com/browse/OCPBUGS-22869[OCPBUGS-22869])

* Previously, the Azure upstream DNS did not comply with non-EDNS DNS queries because it returned a payload larger than 512 bytes. Because CoreDNS 1.10.1 no longer uses EDNS for upstream queries and only uses EDNS when the original client query uses EDNS, the combination would result in an overflow `servfail` error when the upstream returned a payload larger than 512 bytes for non-EDNS queries using CoreDNS 1.10.1. Consequently, upgrading from {product-title} 4.12 to 4.13 led to some DNS queries failing that previously worked.
+
With this release, instead of returning an overflow `servfail` error, the CoreDNS now truncates the response, indicating that the client can try again in TCP. As a result, clusters with a noncompliant upstream now retry with TCP when experiencing overflow errors. This prevents any disruption of functionality between {product-title} 4.12 and 4.13. (link:https://issues.redhat.com/browse/OCPBUGS-27904[OCPBUGS-27904]), (link:https://issues.redhat.com/browse/OCPBUGS-28205[OCPBUGS-28205])

* Previously, there was a limitation in private Microsoft Azure clusters where secondary IP addresses designated as egress IP addresses lacked outbound connectivity. This meant that pods associated with these IP addresses were unable to access the internet. However, they could still reach external servers within the infrastructure network, which is the intended use case for egress IP addresses. This update enables egress IP addresses for Microsoft Azure clusters, allowing outbound connectivity to be achieved through outbound rules. (link:https://issues.redhat.com/browse/OCPBUGS-5491[OCPBUGS-5491])

* Previously, when using multiple NICS, egress IP addresses were not correctly reassigned to the correct egress node when labeled or unlabeled. This bug has been fixed, and egress IP addresses are now reassigned to the correct egress node. (link:https://issues.redhat.com/browse/OCPBUGS-18162[OCPBUGS-18162])

* Previously, a new logic introduced for determining where to run the Keepalived process did not consider the ingress VIP or VIPs. As a result, the Keepalived pods might not have ran on ingress nodes, which could break the cluster. With this fix, the logic now includes the ingress VIP or VIPs, and the Keepalived pods should always be available. (link:https://issues.redhat.com/browse/OCPBUGS-18771[OCPBUGS-18771])

* Previously on Hypershift clusters, pods were not always being scheduled on separate zones. With this update, the `multus-admission-controller` deployment now uses a `PodAntiAffinity` spec for Hypershift to operate in the proper zone. (link:https://issues.redhat.com/browse/OCPBUGS-15220[OCPBUGS-15220])

* Previously, a certificate that existed for 10 minutes was used to implement Multus. With this update, a per node certificate is used for the Multus CNI plugin and the certificate's existence is increased to a 24 hour duration. (link:https://issues.redhat.com/browse/OCPBUGS-19861[OCPBUGS-19861]), (link:https://issues.redhat.com/browse/OCPBUGS-19859[OCPBUGS-19859])

* Previously, the `spec.desiredState.ovn.bridge-mappings` API configuration deleted all the external IDs in Open vSwitch (OVS) local tables on each Kubernetes node. As a result, the OVN chassis configuration was deleted, breaking the default cluster network. With this fix, you can use the `ovn.bridge-mappings` configuration without affecting the OVS configuration. (link:https://issues.redhat.com/browse/OCPBUGS-18869[OCPBUGS-18869])

* Previously, if NMEA sentences were lost on their way to the E810 controller, the T-GM would not be able to synchronize the devices in the network synchronization chain. If these conditions were met, the PTP operator reported an error. With this release, a fix is implemented to report 'FREERUN' in case of a loss of the NMEA string. (link:https://issues.redhat.com/browse/OCPBUGS-20514[OCPBUGS-20514])

* Previously, pods assigned an IP from the pool created by the Whereabouts CNI plugin persisted in the `ContainerCreating` state after a node force reboot. With this release, the Whereabouts CNI plugin issue associated with the IP allocation after a node force reboot is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-18893[OCPBUGS-18893])

* Previously, when using the assisted installer, OVN-Kubernetes took a long time to bootstrap. This issue occurred because there were three `ovnkube-control-plane` nodes. The first two started up normally, but the third delayed the installation time. The issue would only resolve after a timeout expiration; afterwards, installation would continue.
+
With this update, the third `ovnkube-control-plane` node has been removed. As a result, the installation time has been reduced. (link:https://issues.redhat.com/browse/OCPBUGS-29480[OCPBUGS-29480])

[discrete]
[id="ocp-4-15-node-bug-fixes"]
==== Node

* Due to how the Machine Config Operator (MCO) handles machine configurations for worker pools and custom pools, the MCO might apply an incorrect cgroup version argument for custom pools. As a consequence, nodes in the custom pool might feature an incorrect cgroup kernel argument that causes unpredictable behavior. As a workaround, specify cgroup version kernel arguments for worker and control plane pools only.(link:https://issues.redhat.com/browse/OCPBUGS-19352[OCPBUGS-19352])

* Previously, CRI-O was not configuring the cgroup hierarchy correctly to account for the unique way that `crun` creates cgroups. As a consequence, disabling the CPU quota with a PerformanceProfile did not work. With this fix, using a PerformanceProfile to disable CPU quota works as expected. (link:https://issues.redhat.com/browse/OCPBUGS-20492[OCPBUGS-20492])

* Previously, because of a default setting (`container_use_dri_devices, true`), containers were unable to use dri devices. With this fix, containers can use dri devices as expected. (link:https://issues.redhat.com/browse/OCPBUGS-24042[OCPBUGS-24042])

* Previously, the kubelet was running with the `unconfined_service_t` SELinux type. As a consequence, all our plugins failed to deploy due to an Selinux denial. With this fix, the kubelet now runs with the `kubelet_exec_t` SELinux type. As a result, plugins deploy as expected. (link:https://issues.redhat.com/browse/OCPBUGS-20022[OCPBUGS-20022])

* Previously, the `CRI-O` would automatically remove container images on an upgrade. This caused issues in pre-pulling images. With this release, when {product-title} performs a minor upgrade, the container images will not be automatically removed and instead are subject to kubelet's image garbage collection, which will trigger based on disk usage. (link:https://issues.redhat.com/browse/OCPBUGS-25228[OCPBUGS-25228])

* Previously, when adding RHCOS machines to an existing cluster using ansible playbooks, machines were installed with openvswitch version 2.7. With this update, RHCOS machines added to existing clusters using ansible playbooks are installed with openvswitch version 3.1. This openvswitch version increases network performance. (link:https://issues.redhat.com/browse/OCPBUGS-18595[OCPBUGS-18595])

[discrete]
[id="ocp-4-15-node-tuning-operator-bug-fixes"]
==== Node Tuning Operator (NTO)

* Previously, the Tuned profile reports `Degraded` condition after applying a PerformanceProfile. The generated Tuned profile was trying to set a `sysctl` value for the default Receive Packet Steering (RPS) mask when it already configured the same value using an `/etc/sysctl.d` file. Tuned warns about that and the Node Tuning Operator (NTO) treats that as a degradation with the following message `The TuneD daemon issued one or more error message(s) when applying the profile profile. TuneD stderr: net.core.rps_default_mask`. With this update, the duplication was solved by not setting the default RPS mask using Tuned. The `sysctl.d` file was left in place as it applies early during boot. (link:https://issues.redhat.com/browse/OCPBUGS-25092[OCPBUGS-25092])

* Previously, the Node Tuning Operator (NTO) did not set the `UserAgent` and used a default one. With this update, the NTO sets the `UserAgent` appropriately, which makes debugging the cluster easier. (link:https://issues.redhat.com/browse/OCPBUGS-19785[OCPBUGS-19785])

* Previously, when the Node Tuning Operator (NTO) pod restarted while there were a large number of CSVs in the cluster, the NTO pod would fail and entered into `CrashBackLoop` state. With this update, pagination has been added to the list CSVs requests and this avoids the `api-server` timeout issue that resulted in the `CrashBackLoop` state. (link:https://issues.redhat.com/browse/OCPBUGS-14241[OCPBUGS-14241])

[discrete]
[id="ocp-4-15-openshift-cli-bug-fixes"]
==== OpenShift CLI (oc)

* Previously, to filter operator packages by channel, for example, `mirror.operators.catalog.packages.channels`, you had to specify the default channel for the package, even if you did not intend to use the packages from that channel. Based on this information, the resulting catalog is considered invalid if the `imageSetConfig` does not contain the default channel for the package.
+
This update introduces the `defaultChannel` field in the `mirror.operators.catalog.packages` section. You can now select a default channel. This action enables `oc-mirror` to build a new catalog that defines the selected channel in the `defaultChannel` field as the default for the package. (link:https://issues.redhat.com/browse/OCPBUGS-385[OCPBUGS-385])

* Previously, using `eus-` channels for mirroring in `oc-mirror` resulted in failure. This was due to the restriction of `eus-` channels to mirror only even-numbered releases. With this update, `oc-mirror` can now effectively use `eus-` channels for mirroring releases.
(link:https://issues.redhat.com/browse/OCPBUGS-26065[OCPBUGS-26065])

* Previously, while using `oc-mirror` for mirroring local OCI operator catalogs from a hidden folder resulted in the following error:
`error: ".hidden_folder/data/publish/latest/catalog-oci/manifest-list/kubebuilder/kube-rbac-proxy@sha256:<SHASUM>" is not a valid image reference: invalid reference format`.
With this update, the image references are adjusted in the local OCI catalog to prevent any errors during mirroring.
(link:https://issues.redhat.com/browse/OCPBUGS-25077[OCPBUGS-25077])

* Previously, the {product-title} CLI (`oc`) version was not printed when running the `must-gather` tool. With this release, the `oc` version is now listed in the summary section when running `must-gather`. (link:https://issues.redhat.com/browse/OCPBUGS-24199[OCPBUGS-24199])

* Previously, if you ran a command in `oc debug`. such as `oc debug node/worker -- sleep 5; exit 1`, without attaching to the terminal, a `0` exit code was always returned regardless of the command's exit code. With this release, the exit code is now properly returned from the command. (link:https://issues.redhat.com/browse/OCPBUGS-20342[OCPBUGS-20342])

* Previously, when mirroring, `HTTP401` errors were observed due to expired authentication tokens. These errors occurred during the catalog introspection phase or the image mirroring phase. This issue has been fixed for catalog introspection. Additionally, fixing the Network Time Protocol (NTP) resolves the problem seen during the mirroring phase. For more information, see the article on "Access to the requested resource" error when mirroring images.
(link:https://issues.redhat.com/browse/OCPBUGS-7465[OCPBUGS-7465])

[discrete]
[id="ocp-4-15-olm-bug-fixes"]
==== Operator Lifecycle Manager (OLM)

* After you install an Operator, if the catalog becomes unavailable, the subscription for the Operator is updated with a `ResolutionFailed` status condition. Before this update, when the catalog became available again, the `ResolutionFailed` status was not cleared. With this update, this status is now cleared from the subscription after the catalog becomes available, as expected. (link:https://issues.redhat.com/browse/OCPBUGS-29116[OCPBUGS-29116])

* With this update, OLM performs a best-effort verification that existing custom resources (CRs) are not invalidated when you install an updated custom resource definition (CRD). (link:https://issues.redhat.com/browse/OCPBUGS-18948[OCPBUGS-18948])

* Before this update, the install plan for an Operator displayed duplicate values in the `clusterSeviceVersionNames` field. This update removes the duplicate values. (link:https://issues.redhat.com/browse/OCPBUGS-17408[OCPBUGS-17408])

* Before this update, if you created an Operator group with same name as a previously existing cluster role, Operator Lifecycle Manager (OLM) overwrote the cluster role. With this fix, OLM generates a unique cluster role name for every Operator group by using the following syntax:
+
.Naming syntax
+
[source,text]
----
olm.og.<operator_group_name>.<admin_edit_or_view>-<hash_value>
----
+
For more information, see xref:../operators/understanding/olm/olm-understanding-operatorgroups.adoc#olm-operatorgroups-rbac_olm-understanding-operatorgroups[Operator groups]. (link:https://issues.redhat.com/browse/OCPBUGS-14698[OCPBUGS-14698])

* Previously, if an Operator installation or upgrade took longer than 10 minutes, the operation could fail with the following error:
+
[source,text]
----
Bundle unpacking failed. Reason: DeadlineExceeded, Message: Job was active longer than specified deadline
----
+
This issue occurred because Operator Lifecycle Manager (OLM) had a bundle unpacking job that was configured with a timeout of 600 seconds. Bundle unpacking jobs could fail because of network or configuration issues in the cluster that might be transient or resolved with user intervention. With this bug fix, OLM automates the re-creation of failed unpack jobs indefinitely by default.
+
This update adds the optional `operatorframework.io/bundle-unpack-min-retry-interval` annotation for Operator groups. This annotation sets a minimum interval to wait before attempting to re-create the failed job. (link:https://issues.redhat.com/browse/OCPBUGS-6771[OCPBUGS-6771])

* In Operator Lifecycle Manager (OLM), the Catalog Operator was logging many errors regarding missing `OperatorGroup` objects in namespaces that had no Operators installed. With this fix, if a namespace has no `Subscription` objects in it, OLM no longer checks if an `OperatorGroup` object is present in the namespace. (link:https://issues.redhat.com/browse/OCPBUGS-25330[OCPBUGS-25330])

* With the security context constraint (SCC) API, users are able to configure security contexts for scheduling workloads on their cluster. Because parts of core {product-title} components run as pods that are scheduled on control plane nodes, it is possible to create a SCC that prevents those core components from being properly scheduled in `openshift-*` namespaces.
+
This bug fix reduces the role-based access control (RBAC) scope for the `openshift-operator-lifecycle-manager` service account used to run the `package-server-manager` core component. With this update, it is now significantly less likely that an SCC can be applied to the cluster that causes unexpected scheduling issues with the `package-server-manager` component.
+
[WARNING]
====
The SCC API can globally affect scheduling on an {product-title} cluster. When applying such constraints to workloads on the cluster, carefully read the xref:../authentication/managing-security-context-constraints.adoc#managing-pod-security-policies[SCC documentation].
====
+
(link:https://issues.redhat.com/browse/OCPBUGS-20347[OCPBUGS-20347])
//
//[discrete]
//[id="ocp-4-15-openshift-api-server-bug-fixes"]
//==== OpenShift API server

//[discrete]
//[id="ocp-4-15-rhcos-bug-fixes"]
//==== {op-system-first}
//
[discrete]
[id="ocp-4-15-scalability-and-performance-bug-fixes"]
==== Scalability and performance

* Previously, a race condition between `udev` events and the creation queues associated with physical devices led to some of the queues being configured with the wrong Receive Packet Steering (RPS) mask when they should be reset to zero. This resulted in the RPS mask being configured on the queues of the physical devices, meaning they were using RPS instead of Receive Side Scaling (RSS), which could impact the performance. With this fix, the event was changed to be triggered per queue creation instead of at device creation. This guarantees that no queue will be missing. The queues of all physical devices are now set up with the correct RPS mask which is empty. (link:https://issues.redhat.com/browse/OCPBUGS-18662[OCPBUGS-18662])

* Previously, due to differences in setting up a container’s `cgroup` hierarchy, containers that use the `crun` OCI runtime along with a `PerformanceProfile` configuration encountered performance degradation. With this release, when handling a `PerformanceProfile` request, CRI-O accounts for the differences in `crun` and correctly configures the CPU quota to ensure performance. (link:https://issues.redhat.com/browse/OCPBUGS-20492[OCPBUGS-20492])

[discrete]
[id="ocp-4-15-storage-bug-fixes"]
==== Storage

* Previously, {lvms} did not support disabling over-provisioning, and the minimum value for the `thinPoolConfig.overprovisionRatio` field in the `LVMCluster` CR was 2. With this release, you can disable over-provisioning by setting the value of the `thinPoolConfig.overprovisionRatio` field to 1. (link:https://issues.redhat.com/browse/OCPBUGS-24396[OCPBUGS-24396])

* Previously, if the `LVMCluster` CR was created with an invalid device path in the `deviceSelector.optionalPaths` field, the `LVMCluster` CR was in `Progressing` state. With this release, if the `deviceSelector.optionalPaths` field contains an invalid device path, {lvms} updates the `LVMCluster` CR state to `Failed`. (link:https://issues.redhat.com/browse/OCPBUGS-23995[OCPBUGS-23995])

* Previously, the {lvms} resource pods were preempted while the cluster was congested. With this release, upon updating {product-title}, {lvms} configures the `priorityClassName` parameter to ensure proper scheduling and preemption behavior while the cluster is congested. (link:https://issues.redhat.com/browse/OCPBUGS-23375[OCPBUGS-23375])

* Previously, upon creating the `LVMCluster` CR, {lvms} skipped the counting of volume groups. This resulted in the `LVMCluster` CR moving to `Progressing` state even when the volume groups were valid. With this release, upon creating the `LVMCluster` CR, {lvms} counts all the volume groups, and updates the `LVMCluster` CR state to `Ready` if the volume groups are valid. (link:https://issues.redhat.com/browse/OCPBUGS-23191[OCPBUGS-23191])

* Previously, if the default device class was not present on all selected nodes, {lvms} failed to set up the `LVMCluster` CR. With this release, {lvms} detects all the default device classes even if the default device class is present only on one of the selected nodes. With this update, you can define the default device class only on one of the selected nodes. (link:https://issues.redhat.com/browse/OCPBUGS-23181[OCPBUGS-23181])

* Previously, upon deleting the worker node in the single-node OpenShift (SNO) and worker node topology, the `LVMCluster` CR still included the configuration of the deleted worker node. This resulted in the `LVMCluster` CR remaining in `Progressing` state. With this release, upon deleting the worker node in the SNO and worker node topology, {lvms} deletes the worker node configuration in the `LVMCluster` CR, and updates the `LVMCluster` CR state to `Ready`. (link:https://issues.redhat.com/browse/OCPBUGS-13558[OCPBUGS-13558])

* Previously, CPU limits for the AWS EFS CSI driver container could cause performance degradation of volumes managed by the AWS EFS CSI Driver Operator. With this release, the CPU limits from the AWS EFS CSI driver container have been removed to help prevent potential performance degradation. (link:https://issues.redhat.com/browse/OCPBUGS-28645[OCPBUGS-28645])

* Previously, if you used the `performancePlus` parameter in the Azure Disk CSI driver and provisioned volumes 512 GiB or smaller, you would receive an error from the driver that you need a disk size of at least 512 GiB. With this release, if you use the `performancePlus` parameter and provision volumes 512 GiB or smaller, the Azure Disk CSI driver automatically resizes volumes to be 513 GiB. (link:https://issues.redhat.com/browse/OCPBUGS-17542[OCPBUGS-17542])
//
//[discrete]
//[id="ocp-4-15-windows-containers-bug-fixes"]
//==== Windows containers

[id="ocp-4-15-technology-preview-tables"]
== Technology Preview features status

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the following tables, features are marked with the following statuses:

* _Technology Preview_
* _General Availability_
* _Not Available_
* _Deprecated_

[discrete]
=== Networking Technology Preview features

.Networking Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Ingress Node Firewall Operator
|Technology Preview
|General Availability
|General Availability

|Advertise using L2 mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Technology Preview
|Technology Preview
|Technology Preview

|Multi-network policies for SR-IOV networks
|Technology Preview
|Technology Preview
|General Availability

|OVN-Kubernetes network plugin as secondary network
|Technology Preview
|General Availability
|General Availability

|Updating the interface-specific safe sysctls list
|Technology Preview
|Technology Preview
|Technology Preview

|Egress service custom resource
|Not Available
|Technology Preview
|Technology Preview

|VRF specification in `BGPPeer` custom resource
|Not Available
|Technology Preview
|Technology Preview

|VRF specification in `NodeNetworkConfigurationPolicy` custom resource
|Not Available
|Technology Preview
|Technology Preview

|Admin Network Policy (`AdminNetworkPolicy`)
|Not Available
|Technology Preview
|Technology Preview

|IPsec external traffic (north-south)
|Not Available
|Technology Preview
|General Availability

|Host network settings for SR-IOV VFs
|Not Available
|Not Available
|Technology Preview

|Dual-NIC hardware as PTP boundary clock
|General Availability
|General Availability
|General Availability

|Egress IPs on additional network interfaces
|Not Available
|General Availability
|General Availability

|Intel E810 Westport Channel NIC as PTP grandmaster clock
|Technology Preview
|Technology Preview
|Technology Preview

|Dual Intel E810 Westport Channel NICs as PTP grandmaster clock
|Not Available
|Technology Preview
|Technology Preview
|====

[discrete]
=== Storage Technology Preview features

.Storage Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Automatic device discovery and provisioning with Local Storage Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Google Filestore CSI Driver Operator
|Technology Preview
|General Availability
|General Availability

|{ibm-power-server-name} Block CSI Driver Operator
|Technology Preview
|Technology Preview
|General Availability

|Read Write Once Pod access mod
|Not available
|Technology Preview
|Technology Preview

|Build CSI Volumes in OpenShift Builds
|Technology Preview
|General Availability
|General Availability

|Shared Resources CSI Driver in OpenShift Builds
|Technology Preview
|Technology Preview
|Technology Preview

|{secrets-store-operator}
|Not available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Installation Technology Preview features

.Installation Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Installing {product-title} on {oci-first} with VMs
|N/A
|General Availability
|General Availability

|Installing {product-title} on {oci-first} on bare metal
|N/A
|Developer Preview
|Developer Preview

|Adding kernel modules to nodes with kvc
|Technology Preview
|Technology Preview
|Technology Preview

|Azure Tagging
|Technology Preview
|General Availability
|General Availability

|Enabling NIC partitioning for SR-IOV devices
|Technology Preview
|Technology Preview
|Technology Preview

|GCP Confidential VMs
|Technology Preview
|General Availability
|General Availability

|User-defined labels and tags for Google Cloud Platform (GCP)
|Not Available
|Technology Preview
|Technology Preview

|Installing a cluster on Alibaba Cloud by using installer-provisioned infrastructure
|Technology Preview
|Technology Preview
|Technology Preview

|Mount shared entitlements in BuildConfigs in RHEL
|Technology Preview
|Technology Preview
|Technology Preview

|Selectable Cluster Inventory
|Technology Preview
|Technology Preview
|Technology Preview

|Static IP addresses with vSphere (IPI only)
|Not Available
|Technology Preview
|Technology Preview

|Support for iSCSI devices in {op-system}
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Node Technology Preview features

.Nodes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Cron job time zones
|Technology Preview
|General Availability
|General Availability

|`MaxUnavailableStatefulSet` featureset
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Multi-Architecture Technology Preview features

.Multi-Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|{ibm-power-server-name} using installer-provisioned infrastructure
|Technology Preview
|Technology Preview
|General Availability

|`kdump` on `arm64` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `s390x` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `ppc64le` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Specialized hardware and driver enablement Technology Preview features

.Specialized hardware and driver enablement Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Driver Toolkit
|General Availability
|General Availability
|General Availability

|Hub and spoke cluster support
|General Availability
|General Availability
|General Availability

|====

[discrete]
[id="ocp-413-scalability-tech-preview"]
=== Scalability and performance Technology Preview features

.Scalability and performance Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|{factory-prestaging-tool}
|Technology Preview
|Technology Preview
|Technology Preview

|Hyperthreading-aware CPU manager policy
|Technology Preview
|Technology Preview
|Technology Preview

|HTTP transport replaces AMQP for PTP and bare-metal events
|Technology Preview
|Technology Preview
|Technology Preview

|Mount namespace encapsulation
|Technology Preview
|Technology Preview
|Technology Preview

|NUMA-aware scheduling with NUMA Resources Operator
|General Availability
|General Availability
|General Availability

|Node Observability Operator
|Technology Preview
|Technology Preview
|Technology Preview

|{sno-caps} cluster expansion with worker nodes
|General Availability
|General Availability
|General Availability

|{cgu-operator-first}
|General Availability
|General Availability
|General Availability

|Tuning etcd latency tolerances
|Not Available
|Technology Preview
|Technology Preview

|Workload partitioning for three-node clusters and standard clusters
|Technology Preview
|General Availability
|General Availability

|====

[discrete]
[id="ocp-4-15-operators-tech-preview"]
=== Operator lifecycle and development Technology Preview features

.Operator lifecycle and development Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Operator Lifecycle Manager (OLM) v1
|Not Available
|Technology Preview
|Technology Preview

|RukPak
|Technology Preview
|Technology Preview
|Technology Preview

|Platform Operators
|Technology Preview
|Technology Preview
|Technology Preview

|Hybrid Helm Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Java-based Operator
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Monitoring Technology Preview features

.Monitoring Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Alerting rules based on platform monitoring metrics
|Technology Preview
|General Availability
|General Availability

|Metrics Collection Profiles
|Technology Preview
|Technology Preview
|Technology Preview

|Metrics Server
|Not Available
|Not Available
|Technology Preview

|====


[discrete]
=== {rh-openstack-first} Technology Preview features

.{rh-openstack} Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|External load balancers with installer-provisioned infrastructure
|Technology Preview
|General Availability
|General Availability

|Dual-stack networking with installer-provisioned infrastructure
|Not Available
|Technology Preview
|General Availability

|Dual-stack networking with user-provisioned infrastructure
|Not Available
|Not Available
|General Availability

|OpenStack integration into the Cluster CAPI Operator ^[1]^
|Not Available
|Not Available
|Technology Preview

|Control Plane with `rootVolumes` and `etcd` on local disk
|Not Available
|Not Available
|Technology Preview
|====
[.small]
--
1. For more information, see xref:../release_notes/ocp-4-15-release-notes.adoc#ocp-4-15-capi-openstack-tech-preview-no-upgrade[OpenStack integration into the Cluster CAPI Operator].
--

[discrete]
=== Architecture Technology Preview features

.Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Hosted control planes for {product-title} on Amazon Web Services (AWS)
|Technology Preview
|Technology Preview
|Technology Preview
// Needs to move to GA after Nov 15, 2023
|Hosted control planes for {product-title} on bare metal
|Technology Preview
|General Availability
|General Availability
// Needs to move to GA after Nov 15, 2023
|Hosted control planes for {product-title} on {VirtProductName}
|Not Available
|General Availability
|General Availability

|Hosted control planes for {product-title} using non-bare metal agent machines
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Machine management Technology Preview features

.Machine management Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Managing machines with the Cluster API for {aws-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {gcp-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Defining a vSphere failure domain for a control plane machine set
|Not Available
|Not Available
|Technology Preview

|Cloud controller manager for Alibaba Cloud
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Amazon Web Services
|Technology Preview
|General Availability
|General Availability

|Cloud controller manager for Google Cloud Platform
|Technology Preview
|Technology Preview
|General Availability

|Cloud controller manager for {ibm-power-name} VS
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Microsoft Azure
|Technology Preview
|General Availability
|General Availability

|====

[discrete]
=== Authentication and authorization Technology Preview features

.Authentication and authorization Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Pod security admission restricted enforcement
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Machine Config Operator Technology Preview features

.Machine Config Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Improved MCO state reporting
|Not Available
|Not Available
|Technology Preview

|====

[id="ocp-4-15-known-issues"]
== Known issues

* A regression in the behaviour of `libreswan` caused some nodes with IPsec enabled to lose communication with pods on other nodes in the same cluster. To resolve this issue, consider disabling IPsec for your cluster. (link:https://issues.redhat.com/browse/OCPBUGS-44670[OCPBUGS-44670])

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* {run-once-operator} (RODOO) cannot be installed on clusters managed by the HyperShift Operator. (link:https://issues.redhat.com/browse/OCPBUGS-17533[OCPBUGS-17533])

* When installing a cluster on VMware vSphere with static IP addresses (Tech Preview), the installation program can apply an incorrect configuration to the control plane machine sets (CPMS). This can result in control plane machines being recreated without static IP addresses defined. (link:https://issues.redhat.com/browse/OCPBUGS-29236[OCPBUGS-28236])

* Specifying a standard Ebdsv5 or Ebsv5 family machine type instance is not supported when installing an Azure cluster. This limitation is the result of the Azure terraform provider not supporting these machine types. (link:https://issues.redhat.com/browse/OCPBUGS-18690[OCPBUGS-18690])

* When running a cluster with FIPS enabled, you might receive the following error when running the OpenShift CLI (`oc`) on a {op-system-base} 9 system: `FIPS mode is enabled, but the required OpenSSL backend is unavailable`. As a workaround, use the `oc` binary provided with the {product-title} cluster. (link:https://issues.redhat.com/browse/OCPBUGS-23386[OCPBUGS-23386])

* In {product-version} with IPv6 networking running on {rh-openstack-first} environments, `IngressController` objects configured with the `endpointPublishingStrategy.type=LoadBalancerService` YAML attribute will not function correctly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2263550[*BZ#2263550*], link:https://bugzilla.redhat.com/show_bug.cgi?id=2263552[*BZ#2263552*])

* In {product-version} with IPv6 networking running on {rh-openstack-first} environments, health monitors created with IPv6 `ovn-octavia` load balancers will not function correctly. (link:https://issues.redhat.com/browse/OCPBUGS-29603[OCPBUGS-29603])

* In {product-version} with IPv6 networking running on {rh-openstack-first} environments, sharing a IPv6 load balancer with multiple services is not allowed because of an issue that mistakenly marks IPv6 load balancer as internal to the cluster.(link:https://issues.redhat.com/browse/OCPBUGS-29605[OCPBUGS-29605])

* When installing an {product-title} cluster with static IP addressing and Tang encryption, nodes start without network settings. This condition prevents nodes from accessing the Tang server, causing installation to fail. To address this condition, you must set the network settings for each node as `ip` installer arguments.
+
. For installer-provisioned infrastructure, before installation provide the network settings as `ip` installer arguments for each node by executing the following steps.

.. Create the manifests.

.. For each node, modify the `BareMetalHost` custom resource with annotations to include the network settings. For example:
+
[source,terminal]
----
$ cd ~/clusterconfigs/openshift
$ vim openshift-worker-0.yaml
----
+
[source,yaml]
----
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  annotations:
    bmac.agent-install.openshift.io/installer-args: '["--append-karg", "ip=<static_ip>::<gateway>:<netmask>:<hostname_1>:<interface>:none", "--save-partindex", "1", "-n"]' <1> <2> <3> <4> <5>
    inspect.metal3.io: disabled
    bmac.agent-install.openshift.io/hostname: <fqdn> <6>
    bmac.agent-install.openshift.io/role: <role> <7>

  generation: 1
  name: openshift-worker-0
  namespace: mynamespace
spec:
  automatedCleaningMode: disabled
  bmc:
    address: idrac-virtualmedia://<bmc_ip>/redfish/v1/Systems/System.Embedded.1 <8>
    credentialsName: bmc-secret-openshift-worker-0
    disableCertificateVerification: true
  bootMACAddress: 94:6D:AE:AB:EE:E8
  bootMode: "UEFI"
  rootDeviceHints:
    deviceName: /dev/sda
----
For the `ip` settings, replace:
<1> `<static_ip>` with the static IP address for the node, for example, `192.168.1.100`
<2> `<gateway>` with the IP address of your network's gateway, for example, `192.168.1.1`
<3> `<netmask>` with the network mask, for example, `255.255.255.0`
<4> `<hostname_1>` with the node's hostname, for example, `node1.example.com`
<5> `<interface>` with the name of the network interface, for example, `eth0`
<6> `<fqdn>` with the fully qualified domain name of the node
<7> `<role>` with `worker` or `master` to reflect the node's role
<8> `<bmc_ip>` with the BMC IP address and the protocol and path of the BMC, as needed.

.. Save the file to the `clusterconfigs/openshift` directory.

.. Create the cluster.

. When installing with the {ai-full}, before installation modify each node's installer arguments using the API to append the network settings as `ip` installer arguments. For example:
+
[source,bash]
----
$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs/${infra_env_id}/hosts/${host_id}/installer-args \
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d '
    {
      "args": [
        "--append-karg",
        "ip=<static_ip>::<gateway>:<netmask>:<hostname_1>:<interface>:none", <1> <2> <3> <4> <5>
        "--save-partindex",
        "1",
        "-n"
      ]
    }
  ' | jq
----
For the previous network settings, replace:
<1> `<static_ip>` with the static IP address for the node, for example, `192.168.1.100`
<2> `<gateway>` with the IP address of your network's gateway, for example, `192.168.1.1`
<3> `<netmask>` with the network mask, for example, `255.255.255.0`
<4> `<hostname_1>` with the node's hostname, for example, `node1.example.com`
<5> `<interface>` with the name of the network interface, for example, `eth0`.

Contact Red Hat Support for additional details and assistance.

(link:https://issues.redhat.com/browse/OCPBUGS-23119[OCPBUGS-23119])

* In {product-title} {product-version}, all nodes use Linux control group version 2 (cgroup v2) for internal resource management in alignment with the default {op-system-base} 9 configuration. However, if you apply a performance profile in your cluster, the low-latency tuning features associated with the performance profile do not support cgroup v2.
+
As a result, if you apply a performance profile, all nodes in the cluster reboot to switch back to the cgroup v1 configuration. This reboot includes control plane nodes and worker nodes that were not targeted by the performance profile.
+
To revert all nodes in the cluster to the cgroups v2 configuration, you must edit the `Node` resource. For more information, see xref:../nodes/clusters/nodes-cluster-cgroups-2.adoc#nodes-clusters-cgroups-2_nodes-cluster-cgroups-2[Configuring Linux cgroup v2]. You cannot revert the cluster to the cgroups v2 configuration by removing the last performance profile. (link:https://issues.redhat.com/browse/OCPBUGS-16976[OCPBUGS-16976])

* Currently, an error might occur when deleting a pod that uses an SR-IOV network device. This error is caused by a change in {op-system-base} 9 where the previous name of a network interface is added to its alternative names list when it is renamed. As a consequence, when a pod attached to an SR-IOV virtual function (VF) is deleted, the VF returns to the pool with a new unexpected name, such as `dev69`, instead of its original name, such as `ensf0v2`. Although this error is not severe, the Multus and SR-IOV logs might show the error while the system recovers on its own. Deleting the pod might take a few seconds longer due to this error. (link:https://issues.redhat.com/browse/OCPBUGS-11281[OCPBUGS-11281], link:https://issues.redhat.com/browse/OCPBUGS-18822[OCPBUGS-18822], link:https://issues.redhat.com/browse/RHEL-5988[RHEL-5988])
* When you run Cloud-native Network Functions (CNF) latency tests on an {product-title} cluster, the `oslat` test can sometimes return results greater than 20 microseconds. This results in an `oslat` test failure.
(link:https://issues.redhat.com/browse/RHEL-9279[RHEL-9279])

* When you use `preempt-rt` patches with the real time kernel and you update the SMP affinity of a network interrupt, the corresponding Interrupt Request (IRQ) thread does not immediately receive the update.
Instead, the update takes effect when the next interrupt is received, and the thread is subsequently migrated to the correct core.
(link:https://issues.redhat.com/browse/RHEL-9148[RHEL-9148])

* The global navigation satellite system (GNSS) module in an Intel Westport Channel e810 NIC that is configured as a grandmaster clock (T-GM) can report the GPS `FIX` state and the GNSS offset between the GNSS module and the GNSS constellation satellites.
+
The current T-GM implementation does not use the `ubxtool` CLI to probe the `ublox` module for reading the GNSS offset and GPS `FIX` values.
Instead, it uses the `gpsd` service to read the GPS `FIX` information.
This is because the current implementation of the `ubxtool` CLI takes 2 seconds to receive a response, and with every call, it increases CPU usage threefold.
(link:https://issues.redhat.com/browse/OCPBUGS-17422[OCPBUGS-17422])

* The current grandmaster clock (T-GM) implementation has a single NMEA sentence generator sourced from the GNSS without a backup NMEA sentence generator.
If NMEA sentences are lost on their way to the e810 NIC, the T-GM cannot synchronize the devices in the network synchronization chain and the PTP Operator reports an error.
A proposed fix is to report a `FREERUN` event when the NMEA string is lost.
(link:https://issues.redhat.com/browse/OCPBUGS-19838[OCPBUGS-19838])

* Currently, the *YAML* tab of some pages in the web console stops unexpectedly on some browsers when the multicluster engine for Kubernetes operator (MCE) is installed. The following message is displayed: "Oh no! Something went wrong." (link:https://issues.redhat.com/browse/OCPBUGS-29812[OCPBUGS-29812])

* If you have IPsec enabled on your cluster, you must disable it prior to upgrading to {product-title} 4.15. There is a known issue where pod-to-pod communication might be interrupted or lost when updating to 4.15 without disabling IPsec. For information on disabling IPsec, see xref:../networking/ovn_kubernetes_network_provider/configuring-ipsec-ovn.adoc#configuring-ipsec-ovn[Configuring IPsec encryption]. (link:https://issues.redhat.com/browse/OCPBUGS-43323[OCPBUGS-43323])

* If you have IPsec enabled on the cluster and IPsec encryption is configured between the cluster and an external node, stopping the IPsec connection on the external node causes a loss of connectivity to the external node. This connectivity loss occurs because on the {product-title} side of the connection, the IPsec tunnel shutdown is not recognized. (link:https://issues.redhat.com/browse/RHEL-24802[RHEL-24802])

* If you have IPsec enabled on the cluster, and your cluster is a hosted control planes for {product-title} cluster, the MTU adjustment to account for the IPsec tunnel for pod-to-pod traffic does not happen automatically. (link:https://issues.redhat.com/browse/OCPBUGS-28757[OCPBUGS-28757])

* If you have IPsec enabled on the cluster, you cannot modify existing IPsec tunnels to external hosts that you have created. Modifying an existing NMState Operator `NodeNetworkConfigurationPolicy` object to adjust an existing IPsec configuration to encrypt traffic to external hosts is not recognized by {product-title}. (link:https://issues.redhat.com/browse/RHEL-22720[RHEL-22720])

* If you have IPsec enabled on the cluster, on the node hosting the north-south IPsec connection, restarting the `ipsec.service` systemd unit or restarting the `ovn-ipsec-host` pod causes a loss of the IPsec connection. (link:https://issues.redhat.com/browse/RHEL-26878[RHEL-26878])

* Currently, there is a known issue with the mirroring of operator catalogs. The `oc-mirror` rebuilds the catalogs and regenerates their internal cache according to the `imagesetconfig` catalog filtering specifications. This operation requires the use of the `opm` binary contained in the catalogs. In OpenShift Container Platform 4.15, the operator catalogs contain the `opm` RHEL 9 binary, which causes the mirroring process to fail on RHEL 8 systems. (link:https://issues.redhat.com/browse/OCPBUGS-31536[OCPBUGS-31536])

// https://docs.asciidoctor.org/asciidoc/latest/directives/include-list-item-content/
* {empty}
+
--
include::snippets/opm-rhel8-known-issue.adoc[]
--

//[id="ocp-telco-ran-4-15-known-issues"]
//

* There is a known issue in this release preventing the creation of web terminals when logged into the cluster as `kubeadmin`. The terminal will return the message: `Error Loading OpenShift command line terminal: User is not a owner of the requested workspace.` This issue will be fixed in a future {product-title} release. (link:https://issues.redhat.com/browse/WTO-262[WTO-262])

[id="ocp-telco-core-4-15-known-issues"]

* Currently, defining a `sysctl` value for a setting with a slash in its name, such as for bond devices, in the `profile` field of a Tuned resource might not work. Values with a slash in the `sysctl` option name are not mapped correctly to the `/proc` filesystem. As a workaround, create a `MachineConfig` resource that places a configuration file with the required values in the `/etc/sysctl.d` node directory. (link:https://issues.redhat.com/browse/RHEL-3707[RHEL-3707])

* Due to an issue with Kubernetes, the CPU Manager is unable to return CPU resources from the last pod admitted to a node to the pool of available CPU resources. These resources are allocatable if a subsequent pod is admitted to the node. However, this in turn becomes the last pod, and again, the CPU manager cannot return this pod's resources to the available pool.
+
This issue affects CPU load balancing features because these features depend on the CPU Manager releasing CPUs to the available pool. Consequently, non-guaranteed pods might run with a reduced number of CPUs. As a workaround, schedule a pod with a `best-effort` CPU Manager policy on the affected node. This pod will be the last admitted pod and this ensures the resources will be correctly released to the available pool.(link:https://issues.redhat.com/browse/OCPBUGS-17792[OCPBUGS-17792])

* When a node reboot occurs all pods are restarted in a random order. In this scenario it is possible that `tuned` pod started after the workload pods. This means the workload pods start with partial tuning, which can affect performance or even cause the workload to fail. (link:https://issues.redhat.com/browse/OCPBUGS-26400[OCPBUGS-26400])

* The installation of {product-title} might fail when a performance profile is present in the extra manifests folder and targets the primary or worker pools. This is caused by the internal install ordering that processes the performance profile before the default primary and worker `MachineConfigPools` are created. It is possible to workaround this issue by including a copy of the stock primary or worker `MachineConfigPools` in the extra manifests folder. (link:https://issues.redhat.com/browse/OCPBUGS-27948[OCPBUGS-27948]) (link:https://issues.redhat.com/browse/OCPBUGS-18640[OCPBUGS-18640])

* In hosted control planes for {product-title}, the HyperShift Operator extracts the release metadata only once during Operator initialization. When you make changes in the management cluster or create a hosted cluster, the HyperShift Operator does not refresh the release metadata. As a workaround, restart the HyperShift Operator by deleting its pod deployment. (link:https://issues.redhat.com/browse/OCPBUGS-29110[OCPBUGS-29110])

* In hosted control planes for {product-title}, when you create the custom resource definition (CRD) for `ImageDigestMirrorSet` and `ImageContentSourcePolicy` objects at the same time in a disconnected environment, the HyperShift Operator creates the object only for the `ImageDigestMirrorSet` CRD, ignoring the `ImageContentSourcePolicy` CRD. As a workaround, copy the `ImageContentSourcePolicies` object configuration in the `ImageDigestMirrorSet` CRD. (link:https://issues.redhat.com/browse/OCPBUGS-29466[OCPBUGS-29466])

* In hosted control planes for {product-title}, when creating a hosted cluster in a disconnected environment, if you do not set the `hypershift.openshift.io/control-plane-operator-image` annotation explicitly in the `HostedCluster` resource, the hosted cluster deployment fails with an error. (link:https://issues.redhat.com/browse/OCPBUGS-29494[OCPBUGS-29494])

[id="ocp-4-15-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating_a_cluster/updating-cluster-web-console.adoc#updating-cluster-web-console[updating your cluster] properly.
====

// 4.15.50
[id="ocp-4-15-50_{context}"]
=== RHSA-2025:4422 - {product-title} 4.15.50 bug fix and security update

Issued: 8 May 2025

{product-title} release 4.15.50, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2025:4422[RHSA-2025:4422 advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2025:4424[RHBA-2025:4424] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.50 --pullspecs
----

[id="ocp-4-15-50-bug-fixes_{context}"]
==== Bug fixes

* Previously, when the openshift-host-networknamespace was modified by the user or by an upgrade, the network policy incorrectly set VNID≠0. With this release, the VNID parameter is correctly set after the namespace is modified. (link:https://issues.redhat.com/browse/OCPBUGS-54868[OCPBUGS-54868]) 

* Previously, a User Datagram Protocol (UDP) packet that was larger than the maximum transmission unit (MTU) value set for the cluster, could not be sent to the endpoint of the packet by using a service. With this release, the pod IP address is used instead of the service IP address regardless of the packet size, so that the UDP packet can be sent to the endpoint. (link:https://issues.redhat.com/browse/OCPBUGS-50583[*OCPBUGS-50583*]) 

// 4.15.49
[id="ocp-4-15-49_{context}"]
=== RHSA-2025:3790 - {product-title} 4.15.49 bug fix and security update

Issued: 16 April 2025

{product-title} release 4.15.49, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2025:3790[RHSA-2025:3790] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2025:3792[RHBA-2025:3792] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.49 --pullspecs
----

[id="ocp-4-15-49-known-issues_{context}"]
==== Known issues

* IPsec is not supported on {op-system-base-full} compute nodes because of a `libreswan` incompatiblility issue between a host and an `ovn-ipsec` container that exist in each compute node.  (link:https://issues.redhat.com/browse/OCPBUGS-36688[OCPBUGS-36688]) 

[id="ocp-4-15-49-bug-fixes_{context}"]
==== Bug fixes

* Previously, an update to the {ibm-cloud-name} Cloud Internet Services (CIS) implementation impacted the upstream Terraform plugin. If you attempted to create an external-facing cluster on {ibm-cloud-name}, the following error occurred:
+
[source,terminal]
----
ERROR Error: Plugin did not respond
ERROR
ERROR with module.cis.ibm_cis_dns_record.kubernetes_api_internal[0],
ERROR on cis/main.tf line 27, in resource "ibm_cis_dns_record" "kubernetes_api_internal":
ERROR 27: resource "ibm_cis_dns_record" "kubernetes_api_internal"
----
+
With this release, you can use the installation program to create an external cluster on {product-title} without the plugin issue. (link:https://issues.redhat.com/browse/OCPBUGS-54367[OCPBUGS-54367]) 

* Previously, when installing a cluster on {aws-first} in existing subnets that were located in edge zones, such as a Local Zone or a Wavelength Zone, the `kubernetes.io/cluster/<InfraID>:shared` tag was missing in the subnet resources of the edge zone. With this release, a fix ensures that all subnets that are used in the `install-config.yaml` configuration file have the required tag. (link:https://issues.redhat.com/browse/OCPBUGS-54353[OCPBUGS-54353]) 

* Previously, iSCSI and Fibre Channel devices attached by multipath did not resolve correctly when partitioned. This was caused by improper handling of multipath devices. With this release, the partitioned multipath storage is now correctly recognized. (https://issues.redhat.com/browse/OCPBUGS-53139[OCPBUGS-53139]) 

* Previously, the *Cluster Settings* page would not properly render during a cluster update if the Cluster Version Operator (CVO) did not receive a `Completed` update. With this release, the *Cluster Setting* page properly renders even if the CVO has not received a `Completed` update. (link:https://issues.redhat.com/browse/OCPBUGS-53138[OCPBUGS-53138]) 

* Previously, the *Observe* section on the web console did not show items contributed from plugins unless certain flags related to monitoring were set. However, these flags prevented other plugins, such as logging, distributed tracing, network observability, and so on, from adding items to the *Observe* section. With this release, the monitoring flags are removed so that other plugins can add items to the *Observe* section. (link:https://issues.redhat.com/browse/OCPBUGS-53055[OCPBUGS-53055]) 

* Previously, in the `condition.Status` field, the ignition-server controller overloaded the Kubernetes agent server (KAS) by updating that condition with the same message in every reconcile loop. The updates caused an overload of the KAS. With this release, the controller checks the message and validates whether it is the existing message so that the KAS is not overloaded. (link:https://issues.redhat.com/browse/OCPBUGS-50867[OCPBUGS-50867]) 

* Previously, a custom Security Context Constraint (SCC) impacted pods that the Cluster Version Operator generated from receiving a cluster version upgrade. With this release, {product-title} now sets a default SCC to each pod, so that any custom SCC created does not impact a pod. (link:https://issues.redhat.com/browse/OCPBUGS-50591[OCPBUGS-50591]) 

[id="ocp-4-15-49-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.48
[id="ocp-4-15-48_{context}"]
=== RHSA-2025:3055 - {product-title} 4.15.48 bug fix and security update

Issued: 26 March 2025

{product-title} release 4.15.48, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2025:3055[RHSA-2025:3055] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2025:3057[RHBA-2025:3057] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.48 --pullspecs
----

[id="ocp-4-15-48-bug-fixes_{context}"]
==== Bug fixes

* Previously, the availability set fault domain count was hardcoded to `2`. This value works in most regions on {azure-first} because the fault domain counts are typically at least `2`, but failed in the `centraluseuap` and `eastusstg` regions. With this release, the availability set fault domain count in a region is set dynamically so that this issue no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-53226[OCPBUGS-53226])

* Previously, the `trusted-ca-bundle-managed` ConfigMap component was a mandatory component. If you attempted to use a custom Public Key Infrastructure (PKI), the deployment would fail because the OpenShift API server expected the presence of the `trusted-ca-bundle-managed` ConfigMap component. With this release, this component is optional so that you can deploy clusters without the `trusted-ca-bundle-managed` config map component when you use a custom PKI. (link:https://issues.redhat.com/browse/OCPBUGS-52896[OCPBUGS-52896])

* Previously, the URL for the *Alert Rules* page on the web console was incorrect. With this release, the URL is fixed and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-52344[OCPBUGS-52344])

* Previously, Operator Lifecycle Manager (OLM) sometimes concurrently resolved the same namespace in a cluster. As a consequence, subscriptions reached a terminal state of `ConstraintsNotSatisfiable` because two concurrent processes interacted with a subscription, which caused a CSV file to become unassociated. With this release, OLM no longer concurrently resolves namespaces, so that OLM correctly processes a subscription without leaving a CSV file in an unassociated state. (link:https://issues.redhat.com/browse/OCPBUGS-48662[OCPBUGS-48662])

[id="ocp-4-15-48-updating_{context}"]
==== Updating
To update an {product-title} 4.17 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.47
[id="ocp-4-15-47_{context}"]
=== RHSA-2025:2454 - {product-title} 4.15.47 bug fix and security update

Issued: 12 March 2025

{product-title} release 4.15.47, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2025:2454[RHSA-2025:2454] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2025:2456[RHSA-2025:2456] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.47 --pullspecs
----

[id="ocp-4-15-47-bug-fixes_{context}"]
==== Bug fixes

* Previously, an extra `name` prop was passed into the resource list page extensions used to list related operands on the *CSV details* page. This caused the operand list to be filtered by the `CSV` name, which often caused it to be an empty list. With this update, the operands are listed as expected. (link:https://issues.redhat.com/browse/OCPBUGS-51332[OCPBUGS-51332])

* Previously, incorrect addresses were passed to the Kubernetes EndpointSlice on a cluster. This issue prevented the installation of the MetalLB Operator on an Agent-based cluster in an IPv6 disconnected environment. With this release, a fix modifies the address evaluation method. Red{nbsp}Hat Marketplace pods can successfully connect to the cluster API server. As a result, the installation of the MetalLB Operator and the handling of ingress traffic in IPv6 disconnected environments can occur. (link:https://issues.redhat.com/browse/OCPBUGS-51253[OCPBUGS-51253])

* Previously,`konnectivity-https-proxy` did not have the additional trust bundles that were applied in the `configuration.proxy.trustCA` certificate. This caused hosted clusters to fail the provisioning process. With this release, the specified certificates are added to `Konnectivity` and propagate the proxy environment variables, allowing hosted clusters with secure proxies and custom certificates to successfully complete their provisioning. (link:https://issues.redhat.com/browse/OCPBUGS-52172[OCPBUGS-52172])

* Previously, in the Red{nbsp}Hat {product-title} web console *Notifications* section, silenced alerts were visible in the notification drawer because the alerts did not include external labels. With this release, the alerts include external labels so that silenced alerts are not visible on the notification drawer. (link:https://issues.redhat.com/browse/OCPBUGS-49849[OCPBUGS-49849])

[id="ocp-4-15-47-updating_{context}"]
==== Updating
To update an {product-title} 4.17 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.46
[id="ocp-4-15-46_{context}"]
=== RHSA-2025:1711 - {product-title} 4.15.46 bug fix and security update

Issued: 27 February 2025

{product-title} release 4.15.46, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2025:1711[RHSA-2025:1711] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2025:1713[RHSA-2025:1713] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.46 --pullspecs
----

[id="ocp-4-15-46-bug-fixes_{context}"]
==== Bug fixes

* Previously, if you tried to rerun a resolver-based `PipelineRun` from the {product-title} console, the `Invalid PipelineRun configuration, unable to start Pipeline` UI message was displayed. With this release, you can rerun a resolver-based `PipelineRun` with no problem. (link:https://issues.redhat.com/browse/OCPBUGS-48593[OCPBUGS-48593])

* Previously, a bug caused requests to update the `deploymentconfigs/scale` sub resource to fail when a matching admission webhook was configured. With this release, you can update to continue without an error. (link:https://issues.redhat.com/browse/OCPBUGS-47766[OCPBUGS-47766])

* Previously, the installation program did not validate the maximum transmission unit (MTU) for custom networks on Red{nbsp}Hat OpenStack platforms, which led to an installation failure when the MTU was too small. For IPv6, the minimum MTU is 1280 and 100 for OVN-Kubernetes. With this release, the installation program validates the MTU of Red{nbsp}Hat OpenStack custom networks. (link:https://issues.redhat.com/browse/OCPBUGS-41815[OCPBUGS-41815])

[id="ocp-4-15-46-updating_{context}"]
==== Updating
To update an {product-title} 4.16 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.45
[id="ocp-4-15-45_{context}"]
=== RHSA-2025:1128 - {product-title} 4.15.45 bug fix and security update

Issued: 12 February 2025

{product-title} release 4.15.45, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2025:1128[RHSA-2025:1128] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2025:1130[RHSA-2025:1130] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.45 --pullspecs
----

[id="ocp-4-15-45-bug-fixes_{context}"]
==== Bug fixes

* Previously, crun failed to stop a container if you opened a terminal session and then disconnected from it. With this release, the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-48751[OCPBUGS-48751])

* Previously, every time a subcription was reconciled, the OLM catalog Operator requested a full view of the catalog metadata from the catalog source pod of the subscription. These requests caused performance issues for the catalog pods. With this release, the OLM catalog Operator now uses a local cache that is refreshed periodically and reused by all subscription reconciliations, so that the performance issue for the catalog pods no longer persists. (link:https://issues.redhat.com/browse/OCPBUGS-48697[OCPBUGS-48697])

* Previously, when you used the *Form View* to edit `Deployment` or `DeploymentConfig` API objects on the {product-title} web console, duplicate `ImagePullSecrets` parameters existed in the YAML configuration for either object. With this release, a fix ensures that duplicate `ImagePullSecrets` parameters do not get automatically added for either object. (link:https://issues.redhat.com/browse/OCPBUGS-48592[OCPBUGS-48592])

[id="ocp-4-15-45-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

//4.15.44
[id="ocp-4-15-44_{context}"]
=== RHSA-2025:0646 - {product-title} 4.15.44 bug fix and security update

Issued: 29 January 2025

{product-title} release 4.15.44, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2025:0646[RHSA-2025:0646] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2025:0648[RHSA-2025:0648] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.44 --pullspecs
----

[id="ocp-4-15-44-bug-fixes_{context}"]
==== Bug fixes

* Previously, East to West pod traffic over the Geneve overlay could stop working between one or multiple nodes, which prevented pods from reaching pods on other nodes. With this release, the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-47799[OCPBUGS-47799])

* Previously, when installing a cluster on {ibm-cloud-name} into an existing VPC, the installation program retrieved an unsupported VPC region. Attempting to install into a supported VPC region that follows the unsupported VPC region alphabetically caused the installation program to crash. With this release, the installation program is updated to ignore any VPC regions that are not fully available during resource lookups. (link:https://issues.redhat.com/browse/OCPBUGS-44259[OCPBUGS-44259])

[id="ocp-4-15-44-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

//4.15.43
[id="ocp-4-15-43_{context}"]
=== RHSA-2025:0121 - {product-title} 4.15.43 bug fix and security update

Issued: 15 January 2025

{product-title} release 4.15.43, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2025:0121[RHSA-2025:0121] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2025:0125[RHBA-2025:0125] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.43 --pullspecs
----

[id="ocp-4-15-43-bug-fixes_{context}"]
==== Bug fixes

* Previously, a machine controller failed to save the {vmw-full} task ID of an instance template clone operation. This caused the machine to go into the `Provisioning` state and to power off. With this release, the {vmw-full} machine controller can detect and recover from this state. (link:https://issues.redhat.com/browse/OCPBUGS-48105[OCPBUGS-48105])

* Previously, installation of an {aws-short} cluster failed in certain environments on existing subnets when the `MachineSet` object's parameter `publicIp` was explicitly set to `false`. With this release, a fix ensures that a configuration value set for `publicIp` no longer causes issues when the installation program provisions machines for your {aws-short} cluster in certain environments. (link:https://issues.redhat.com/browse/OCPBUGS-47680[OCPBUGS-47680])

* Previously, the IDs used to determine the number of rows in a Dashboard table were not unique and some rows would be combined if their IDs were the same. With this release, the ID uses more information to prevent duplicate IDs and the table can display each expected row. (link:https://issues.redhat.com/browse/OCPBUGS-47646[OCPBUGS-47646])

* Previously, the algorithm for calculating the priority of machine removal equated Machines over a specific age to Machines annotated as preferred for removal. With this release, the priority of unmarked Machines sorted by age is reduced to avoid conflict with those explicitly marked, and the algorithm has been updated to ensure age order is guaranteed for Machines up to ten years old. (link:https://issues.redhat.com/browse/OCPBUGS-46080[OCPBUGS-46080])

* Previously, in managed services, audit logs are sent to a local webhook service. Control plane deployments sent traffic through `konnectivity` and attempted to send the audit webhook traffic through the `konnectivity` proxies - `openshift-apiserver` and `oauth-openshift`.  With this release, the audit-webhook is in the list of no_proxy hosts for the affected pods, and the audit log traffic that is sent to the audit-webhook is successfully sent. (link:https://issues.redhat.com/browse/OCPBUGS-46075[OCPBUGS-46075])

[id="ocp-4-15-43-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

//4.15.42
[id="ocp-4-15-42_{context}"]
=== RHSA-2024:11562 - {product-title} 4.15.42 bug fix and security update

Issued: 02 January 2025

{product-title} release 4.15.42, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:11562[RHSA-2024:11562] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2024:11565[RHBA-2024:11565] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.42 --pullspecs
----

[id="ocp-4-15-42-bug-fixes_{context}"]
==== Bug fixes

* Previously, when the webhook token authenticator was enabled and had the authorization type set to `None`, the {product-title} web console would consistently crash. With this release, the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-46482[OCPBUGS-46482])

* Previously, when you attempted to use the Operator Lifecycle Manager (OLM) to upgrade an Operator, the upgrade was blocked and an `error validating existing CRs against new CRD's schema` message was generated. An issue existed with OLM, whereby OLM erroneously identified incompatibility issues validating existing custom resources (CRs) against the new Operator version's custom resource definitions (CRDs). With this release, the validation is corrected so that Operator upgrades are no longer blocked. (link:https://issues.redhat.com/browse/OCPBUGS-46479[OCPBUGS-46479])

* Previously, the images for custom OS layering were not present when the OS was on Red Hat Enterprise Linux CoreOS (RHCOS) 4.15, preventing some customers from upgrading from RHCOS 4.15 to RHCOS 4.16. This release adds Azure Container Registry (ACR) and Google Container Registry (GCR) image credential provider RPMs to RHCOS 4.15. (link:https://issues.redhat.com/browse/OCPBUGS-46063[OCPBUGS-46063])

* Previously, you could not configure your Amazon Web Services DHCP option set with a custom domain name containing a period (`.`) as the final character, as trailing periods were not allowed in a Kubernetes object name. With this release, trailing periods are allowed in a domain name in a DHCP option set. (link:https://issues.redhat.com/browse/OCPBUGS-46034[OCPBUGS-46034])

* Previously, when `openshift-sdn` pods were deployed during the {product-title} upgrading process, the Open vSwitch (OVS) storage table was cleared. This issue occurred on {product-title} {product-version}.19 and later versions. Ports for existing pods had to be re-created and this disrupted numerous services. With this release, a fix ensures that the OVS tables do not get cleared and pods do not get disconnected during a cluster upgrade operation. (link:https://issues.redhat.com/browse/OCPBUGS-45955[OCPBUGS-45955])

* Previously, you could not remove a `finally` pipeline task from the *edit Pipeline* form if you created a pipeline with only one `finally` task. With this release, you can remove the `finally` task from the *edit Pipeline* form and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-45950[OCPBUGS-45950])

* Previously, the `aws-sdk-go-v2` software development kit (SDK) failed to authenticate an `AssumeRoleWithWebIdentity` API operation on an {aws-short} {sts-first} cluster. With this release, the pod identity webhook now includes a default region, and this issue no longer persists. (link:https://issues.redhat.com/browse/OCPBUGS-45940[OCPBUGS-45940])

* Previously, the installation program populated the `network.devices`, `template` and `workspace` fields in the `spec.template.spec.providerSpec.value` section of the {vmw-full} control plane machine set custom resource (CR).
These fields should be set in the {vmw-short} failure domain, and the installation program populating them caused unintended behaviors.
Updating these fields did not trigger an update to the control plane machines, and these fields were cleared when the control plane machine set was deleted.
With this release, the installation program is updated to no longer populate values that are included in the failure domain configuration.
If these values are not defined in a failure domain configuration, for instance on a cluster that is updated to {product-title} {product-version} from an earlier version, the values defined by the installation program are used.
(link:https://issues.redhat.com/browse/OCPBUGS-45839[OCPBUGS-45839], link:https://issues.redhat.com/browse/OCPBUGS-37064[OCPBUGS-37064])

* Previously, the ClusterTasks were listed on the *Pipelines builder* and *ClusterTask list* pages in the *Tasks* navigation menu. With this release, the `ClusterTask` functionality is deprecated from Pipelines 1.17 and the `ClusterTask` dependency is removed from static plug-in. On the Pipelines builder page, you will only see the present task in the Namespace and Community tasks. (link:https://issues.redhat.com/browse/OCPBUGS-45248[OCPBUGS-45248])

[id="ocp-4-15-42-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

//4.15.41
[id="ocp-4-15-41_{context}"]
=== RHSA-2024:10839 - {product-title} 4.15.41 bug fix and security update

Issued: 12 December 2024

{product-title} release 4.15.41, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:10839[RHSA-2024:10839] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2024:10842[RHBA-2024:10842] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.41 --pullspecs
----

[id="ocp-4-15-41-bug-fixes_{context}"]
==== Bug fixes

* Previously, the Single-Root I/O Virtualization (SR-IOV) Operator did not expire the acquired lease during the Operator's shutdown. This impacted a new instance of the Operator, because the new instance had to wait for the lease to expire before the new instance could work. With this release, an update to the Operator shutdown logic ensures that the Operator expires the lease when the Operator is shutting down. (link:https://issues.redhat.com/browse/OCPBUGS-43361[OCPBUGS-43361])

* Previously, when you used the Agent-based Installer to install a cluster on a node that had an incorrect date, the cluster installation failed. With this release, a patch is applied to the Agent-based Installer live ISO time synchronization. The patch fixes the date issue and configures the `/etc/chrony.conf` file with the list of additional Network Time Protocol (NTP) servers, so that you can set any of these additional NTP servers in the `agent-config.yaml` without experiencing a cluster installation issue. (link:https://issues.redhat.com/browse/OCPBUGS-45207[OCPBUGS-45207])

* Previously, {hcp}-based clusters were unable to authenticate through the `oc login` command. The web browser displayed an error when it attepted to retrieve the the token after selecting *Display Token*. With this release, `cloud.ibm.com` and other cloud-based endpoints are no longer proxied and authentication is successful. (link:https://issues.redhat.com/browse/OCPBUGS-44278[OCPBUGS-44278])

* Previously, the Messaging Application Programming Interface (MAPI) for {ibm-cloud-title} currently checked the first group of subnets (50) when searching for subnet details by name. With this release, the search provides pagination support to search all subnets. (link:https://issues.redhat.com/browse/OCPBUGS-43675[OCPBUGS-43675])

[id="ocp-4-15-41-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

//4.15.39
[id="ocp-4-15-39_{context}"]
=== RHSA-2024:10142 - {product-title} 4.15.39 bug fix and security update

Issued: 26 November 2024

{product-title} release 4.15.39, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:10142[RHSA-2024:10142] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2024:10145[RHSA-2024:10145] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.39 --pullspecs
----

[id="ocp-4-15-39-bug-fixes_{context}"]
==== Bug fixes

* Previously, the approval process for a certificate signing request (CSR) failed because the node name and internal DNS entry for a CSR did not match the case-sensitive check. With this release, an update to the approval process for CSRs avoids the case-sensitive check and a CSR with a matching node name and internal DNS entry does not fail the matching-pair check. (link:https://issues.redhat.com/browse/OCPBUGS-44705[OCPBUGS-44705])

* Previously, when the Cluster Resource Override Operator was unable to completely deploy its operand controller, the Operator would restart the process. Each time the Operator attempted the deployment process, the Operator created a new set of secrets. This resulted in a large number of secrets created in the namespace where the Cluster Resource Override Operator was deployed. With this release, the fixed version correctly processes the service account annotations and only one set of secrets is created. (link:https://issues.redhat.com/browse/OCPBUGS-44378[OCPBUGS-44378])

* Previously, when the Cluster Version Operator (CVO) pod restarted while it was initializing the synchronization work, the Operator interrupted the guard of the blocked upgrade request. The blocked request was unexpectedly accepted. With this release, the guard of the blocked upgrade request continues after the CVO restarts. (link:https://issues.redhat.com/browse/OCPBUGS-44328[OCPBUGS-44328])

* Previously, enabling Encapsulated Security Payload (ESP) hardware offload by using IPSec on attached interfaces in Open vSwitch (OVS) interrupted connectivity because of a bug in OVS. With this release, {product-title} automatically disables ESP hardware offload on the OVS-attached interfaces so that the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-44240[OCPBUGS-44240])

[id="ocp-4-15-39-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

//4.15.38
[id="ocp-4-15-38_{context}"]
=== RHSA-2024:8991 - {product-title} 4.15.38 bug fix and security update

Issued: 13 November 2024

{product-title} release 4.15.38, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:8991[RHSA-2024:8991] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2024:8994[RHSA-2024:8994] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.38 --pullspecs
----

[id="ocp-4-15-38-bug-fixes_{context}"]
==== Bug fixes

* Previously, an invalid or unreachable identity provider (IDP) blocked updates to {hcp}. With this release, the `ValidIDPConfiguration` condition in the `HostedCluster` object now reports any IDP errors and these errors do not block updates of {hcp}. (link:https://issues.redhat.com/browse/OCPBUGS-44201[OCPBUGS-44201])

* Previously, the Machine Config Operator (MCO) vSphere `resolv-prepender` script used systemd directives that were not compatible with old boot image versions of {product-title} 4. With this release, these {product-title} nodes are compatible with old boot images with one of the following solutions: scaling with a boot image 4.13 or later, by using manual intervention, or upgrading to a release with this fix. (link:https://issues.redhat.com/browse/OCPBUGS-42110[OCPBUGS-42110])

* Previously, when the Image Registry Operator was configured in {azure-short} with `networkAccess:Internal`, you could not successfully set `managementState` to `Removed` in the Operator configuration. This issue was caused by an authorization error that occurred when the Operator started to delete the storage. With this release, the Operator successfully deletes the storage account, which automatically deletes the storage container. The `managementState` status in the Operator configuration is updated to the `Removed` state. (link:https://issues.redhat.com/browse/OCPBUGS-43656[OCPBUGS-43656])

[id="ocp-4-15-38-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

//4.15.37
[id="ocp-4-15-37_{context}"]
=== RHSA-2024:8425 - {product-title} 4.15.37 bug fix and security update

Issued: 30 October 2024

{product-title} release 4.15.37, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:8425[RHSA-2024:8425] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2024:8428[RHSA-2024:8428] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.37 --pullspecs
----

[id="ocp-4-15-37-enhancements_{context}"]
==== Enhancements

* Previously, you could not control log levels for the internal component that selects IP addresses for cluster nodes. With this release, you can now enable debug log levels so that you can either increase or decrease log levels on demand. To adjust log levels, you must create a config map manifest file with a configuration analogous to the following:
+
[source,yaml]
----
apiVersion: v1
data:
  enable-nodeip-debug: "true"
kind: ConfigMap
metadata:
  name: logging
  namespace: openshift-vsphere-infra
# ...
----
+
(link:https://issues.redhat.com/browse/OCPBUGS-37704[OCPBUGS-37704])

[id="ocp-4-15-37-bug-fixes_{context}"]
==== Bug fixes

* Previously, when you attempted to use the `oc import-image` command to import an image in a {hcp} cluster, the command failed because of access issues with a private image registry. With this release, an update to `openshift-apiserver` pods in a {hcp} cluster resolves names that use the data plane so that the `oc import-image` command now works as expected with private image registries. (link:https://issues.redhat.com/browse/OCPBUGS-43468[OCPBUGS-43468])

* Previously, when you used the `must-gather` tool, a Multus Container Network Interface (CNI) log file, `multus.log`, was stored in a node's file system. This situation caused the tool to generate unnecessary debug pods in a node. With this release, the Multus CNI no longer creates a `multus.log` file, and instead uses a CNI plugin pattern to inspect any logs for Multus DaemonSet pods in the `openshift-multus` namespace. (link:https://issues.redhat.com/browse/OCPBUGS-43057[OCPBUGS-43057])

* Previously, the Ingress and DNS operators failed to start correctly because of rotating root certificates. With this release, the Ingress and DNS operator Kubeconfigs are conditionally managed by using the annotation that defines when the PKI requires management and the issue is resolved.. (link:https://issues.redhat.com/browse/OCPBUGS-42992[OCPBUGS-42992])

* Previously, when you configured the image registry to use an {azure-first} storage account that was located in a resource group other than the cluster's resource group, the Image Registry Operator would become degraded. This occurred because of a validation error. With this release, an update to the Operator allows for authentication only by using a storage account key. Validation of other authentication requirements is not required. (link:https://issues.redhat.com/browse/OCPBUGS-42934[OCPBUGS-42934])

* Previously for hosted control planes (HCP), a cluster that used mirroring release images might result in existing node pools to use the hosted cluster's operating system version instead of the `NodePool` version. With this release, a fix ensures that node pools use their own versions. (link:https://issues.redhat.com/browse/OCPBUGS-42881[OCPBUGS-42881])

* Previously, creating cron jobs to create pods for your cluster caused the component that fetches the pods to fail. Because of this issue, the *Topology* page on the {product-title} web console failed. With this release, a `3` second delay is configured for the component that fetches pods that are generated from the cron job so that this issue no longer exists. (link:https://issues.redhat.com/browse/OCPBUGS-42611[OCPBUGS-42611])

* Previously, the Ingress Operator prevented upgrades from {product-title} 4.15 to 4.16 if any certificate type in the default certificate chain used the SHA-1 hashing algorithm. With this release, the Ingress Operator now only checks default leaf certificates for SHA-1 hash values, so that intermediate and root certificates in the default chain can continue to use SHA-1 hash values without blocking cluster upgrades. (link:https://issues.redhat.com/browse/OCPBUGS-42480[OCPBUGS-42480])

* Previously, when installing a cluster on bare metal using installer provisioned infrastructure, the installation could time out if the network to the bootstrap virtual machine is slow. With this update, the timeout duration has been increased to cover a wider range of network performance scenarios. (link:https://issues.redhat.com/browse/OCPBUGS-42335[OCPBUGS-42335])

* Previously, Ironic inspection failed if special or invalid characters existed in the serial number of a block device. This occurred because the `lsblk` command failed to escape the characters. With this release, the command escapes the characters so this issue no longer persists. (link:https://issues.redhat.com/browse/OCPBUGS-39018[OCPBUGS-39018])

[id="ocp-4-15-37-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].


//4.15.36
[id="ocp-4-15-36_{context}"]
=== RHSA-2024:7594 - {product-title} 4.15.36 bug fix and security update

Issued: 09 October 2024

{product-title} release 4.15.36, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:7594[RHSA-2024:7594] advisory. There are no RPM packages for this release.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.36 --pullspecs
----

[id="ocp-4-15-36-bug-fixes_{context}"]
==== Bug fixes

* Previously, if a connection on port 9637 for Windows nodes was refused, the Kubelet Service Monitor threw a `target down` alert because CRIO doesn't run on Windows nodes. With this release, Windows nodes are excluded from the Kubelet Service Monitor. (link:https://issues.redhat.com/browse/OCPBUGS-42586[OCPBUGS-42586])

* Previously, a change in the ordering of the `TextInput` parameters for PatternFly v4 and v5 caused the `until` field to be improperly filled and you could not edit it. With this release, the `until` field is editable so you can input the correct information. (link:https://issues.redhat.com/browse/OCPBUGS-42384[OCPBUGS-42384])

* Previously, when the Node Tuning Operator (NTO) was configured using performance profiles, it created the `ocp-tuned-one-shot systemd` service. The service ran before the kubelet and blocked the service execution. The `systemd` service invoked Podman, which used the NTO image. If the NTO image was not present, Podman tried to fetch the image.
+
With this release, support is added for cluster-wide proxy environment variables that are defined in the `/etc/mco/proxy.env` environment. This allows Podman to pull NTO images in environments that need the HTTP/HTTPS proxy for out-of-cluster connections. (link:https://issues.redhat.com/browse/OCPBUGS-42284[OCPBUGS-42284])

* Previously, a node registration issue prevented you from using Redfish Virtual Media to add an xFusion bare-metal node to your cluster. The issue occurred because the hardware was not fully compliant with Redfish. With this release, you can add xFusion bare-metal nodes to your cluster. (link:https://issues.redhat.com/browse/OCPBUGS-38798[OCPBUGS-38798])

[id="ocp-4-15-36-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-35_{context}"]
=== RHSA-2024:7179 - {product-title} 4.15.35 bug fix and security update

Issued: 02 October 2024

{product-title} release 4.15.35, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:7179[RHSA-2024:7179] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2024:7182[RHSA-2024:7182] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.35 --pullspecs
----

[id="ocp-4-15-35-bug-fixes_{context}"]
==== Bug fixes

* Previously, when you upgraded an {product-title} cluster from 4.14 to 4.15, the `vCenterCluster` parameter was not populated with a value in the `use-connection-form.ts` configuration file. As a result, the {vmw-full} GUI did not display {vmw-full} vCenter information. With this release, an update to the Infrastructure custom resource (CR) ensures it has that the GUI checks the `cloud-provider-config` ConfigMap for the `vCenterCluster` value. (link:https://issues.redhat.com/browse/OCPBUGS-42144[OCPBUGS-42144])

* Previously, deploying a self-managed private hosted cluster on {aws-first} fails because the `bootstrap-kubeconfig` file uses an incorrect `kube-apiserver` port. As a result, the {aws-short} instances are provisioned but cannot join the hosted cluster as nodes. With this release, the issue is fixed so this issue no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-42214[OCPBUGS-42214])

* Previously, when the hosted cluster `controllerAvailabilityPolicy` was set to `SingleReplica`, the `podAntiAffinity` property on networking components blocked the availability of the components to a cluster. With this release, the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-42020[OCPBUGS-42020])

* Previously, adding IPv6 support for user-provisioned installation platforms caused an issue with naming {rh-openstack-first} resources, especially when you run two user-provisioned installation clusters on the same {rh-openstack} platform. This happened because the two clusters share the same names for network, subnets, and router resources. With this release, all the resources names for a cluster remain unique for that cluster so no interfere occurs. (link:https://issues.redhat.com/browse/OCPBUGS-42011[OCPBUGS-42011])

* Previously, when you configured a hosted cluster to use an identity provider (IdP) that has either an `http` or `https` endpoint, the IdP hostname did not resolve when sent through the proxy. With this release, a DNS lookup operation checks the IdP before IdP traffic is sent through a proxy, so that IdPs with hostnames can only be resolved by the data plane and verified by the Control Plane Operator (CPO). (link:https://issues.redhat.com/browse/OCPBUGS-41373[OCPBUGS-41373])

* Previously, a group ID was not added to the `/etc/group` within the container when the `spec.securityContext.runAsGroup` attribute was set in the `Pod` resource. With this release, this issue is fixed. (link:https://issues.redhat.com/browse/OCPBUGS-41245[OCPBUGS-41245])

* Previously, the order of an Ansible playbook was modified to run before the `metadata.json` file was created, which caused issues with older versions of Ansible. With this release, the playbook is more tolerant of missing files and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-39288[OCPBUGS-39288])

* Previously, dynamic plugins using PatternFly 4 referenced variables that are not available in {product-title} 4.15 and later. This caused contrast issues for {rh-rhacm-first} in dark mode. With this update, older chart styles are now available to support PatternFly 4 charts used by dynamic plugins. (link:https://issues.redhat.com/browse/OCPBUGS-38537[OCPBUGS-38537])

* Previously, proxying for identity provider (IdP) communication occurred in the Konnectivity agent. By the time traffic reached Konnectivity, its protocol and hostname was no longer available. As a consequence, proxying was not done correctly for the OAUTH server pod. It did not distinguish between protocols that require proxying (`http` or `https`) and protocols that do not (LDAP). In addition, it did not honor the `no_proxy` variable that is configured in the `HostedCluster.spec.configuration.proxy` spec. With this release, you can configure the proxy on the Konnectivity sidecar of the OAUTH server so that traffic is routed appropriately, honoring your `no_proxy` settings. As a result, the OAUTH server can communicate properly with IdPs when a proxy is configured for the hosted cluster. (link:https://issues.redhat.com/browse/OCPBUGS-38058[OCPBUGS-38058])

* Previously, the {ai-full} did not reload new data from the Assisted Service when the {ai-full} checked control plane nodes for readiness and a conflict existed with a write operation from the {ai-full} controller. This conflict prevented the {ai-full} from detecting a node that was marked by the {ai-full} controller as `Ready` because the {ai-full} relied on older information. With this release, the {ai-full} can receive the newest information from the Assisted Service, so that it the {ai-full} can accurately detect the status of each node. (link:https://issues.redhat.com/browse/OCPBUGS-38003[OCPBUGS-38003])

[id="ocp-4-15-35-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-34_{context}"]
=== RHSA-2024:6818 - {product-title} 4.15.34 bug fix and security update

Issued: 25 September 2024

{product-title} release 4.15.34, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:6818[RHSA-2024:6818] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2024:6821[RHBA-2024:6821] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.34 --pullspecs
----

[id="ocp-4-15-34-enhancements_{context}"]
==== Enhancements

The following enhancement is included in this z-stream release:

[id="ocp-4-15-34-IO-new-metric_{context}"]
===== Insights Operator collects new metric

* The Insights Operator (IO) can now collect data from the `haproxy_exporter_server_threshold` metric.
(link:https://issues.redhat.com/browse/OCPBUGS-38593[OCPBUGS-38593])

[id="ocp-4-15-34-bug-fixes_{context}"]
==== Bug fixes

* Previously, when the Operator Lifecycle Manager (OLM) evaluated a potential update, it used the dynamic client list for all custom resource (CR) instances in the cluster.  For clusters with a large number of CRs, that could result in timeouts from the apiserver and stranded updates. With this release, the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-41819[OCPBUGS-41819])

* Previously, if you created a hosted cluster by using a proxy for the purposes of making the cluster reach a control plane from a compute node, the compute node would be unavailable to the cluster. With this release, the proxy settings are updated for the node so that the node can use a proxy to successfully communicate with the control plane. (link:https://issues.redhat.com/browse/OCPBUGS-41947[OCPBUGS-41947])

* Previously, an `AdditionalTrustedCA` field that was specified in the Hosted Cluster image configuration was not reconciled into the `openshift-config` namespace as expected and the component was not available. With this release, the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-41809[OCPBUGS-41809])

* Previously, when you created a `LoadBalancer` service for the Ingress Operator, a log message was generated that stated the change was not effective. This log message should only trigger for a change to an Infra custom resource. With this release, this log message is no longer generated when you create a `LoadBalancer` service for the Ingress Operator. (link:https://issues.redhat.com/browse/OCPBUGS-41635[OCPBUGS-41635])

* Previously, if an IP address was assigned to an egress node and was deleted, then pods selected by that egress IP address might have had incorrect routing information to that egress node. With this release, the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-41340[OCPBUGS-41340])

* Previously, proxying for Operators that run in the control plane of a hosted cluster was done through proxy settings on the Konnectivity agent pod that runs in the data plane. As a consequence, it was not possible to distinguish whether proxying was needed based on application protocol. For parity with {product-title}, IDP communication through HTTPS or HTTP should be proxied, but LDAP communication should not be proxied. This type of proxying also ignores `NO_PROXY` entries that rely on host names because by the time traffic reaches the Konnectivity agent, only the destination IP address is available. With this release, in hosted clusters, proxy is invoked in the control plane by `konnectivity-https-proxy` and `konnectivity-socks5-proxy`, and proxying traffic is stopped from the Konnectivity agent. As a result, traffic that is destined for LDAP servers is no longer proxied. Other HTTPS or HTTPS traffic is proxied correctly. The `NO_PROXY` setting is honored when you specify hostnames. (link:https://issues.redhat.com/browse/OCPBUGS-38065[OCPBUGS-38065])

[id="ocp-4-15-34-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-33_{context}"]
=== RHSA-2024:6685 - {product-title} 4.15.33 bug fix and security update

Issued: 19 September 2024

{product-title} release 4.15.33, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:6685[RHSA-2024:6685] advisory. There are no RPM packages in this release.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.33 --pullspecs
----

[id="ocp-4-15-33-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-32_{context}"]
=== RHSA-2024:6637 - {product-title} 4.15.32 bug fix and security update

Issued: 18 September 2024

{product-title} release 4.15.32, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:6637[RHSA-2024:6637] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2024:6640[RHBA-2024:6640] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.32 --pullspecs
----

[id="ocp-4-15-32-bug-fixes_{context}"]
==== Bug fixes

* Previously, the Cloud Credential Operator (CCO) would produce an error when starting or restarting when there were a large number of secrets in the cluster fetched at once. With this release, the CCO fetches the secrets in batches of 100 and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-41235[OCPBUGS-41235])

* Previously, the `ControlPlaneMachineSet` (CPMS) checked templates and the resource pool based on the full vCenter path. This caused the CPMS to start when it was not needed. With this release, CPMS also checks the file name and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-24632[OCPBUGS-24632])

[id="ocp-4-15-32-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-31_{context}"]
=== RHSA-2024:6409 - {product-title} 4.15.31 bug fix and security update

Issued: 11 September 2024

{product-title} release 4.15.31, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:6409[RHSA-2024:6409] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2024:6414[RHBA-2024:6414] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.31 --pullspecs
----

[id="ocp-4-15-31-bug-fixes_{context}"]
==== Bug fixes

* Previously, the `spec.noProxy` field from the cluster-wide proxy was not considered when the {cmo-first} configured proxy capabilities for Prometheus remote write endpoints. With this release, the {cmo-short} no longer configures proxy capabilities for any remote write endpoints whose URL would bypass the proxy according to the `noProxy` field. (link:https://issues.redhat.com/browse/OCPBUGS-39172[OCPBUGS-39172])

* Previously, utlization cards displayed limit in a way that incorrectly implied a relationship between capacity and limits. With this release, the position of limit is changed to remove this implication. (link:https://issues.redhat.com/browse/OCPBUGS-39085[OCPBUGS-39085])

* Previously, the `openvswitch` service used older cluster configurations after a cluster upgrade and this caused the `openvswitch` service to stop. With this release, the `openvswitch` service is now restarted after a cluster upgrade so that the service uses the newer cluster configurations. (link:https://issues.redhat.com/browse/OCPBUGS-34842[OCPBUGS-34842])

* Previously, after you submitted the same value into the {vmw-first} configuration dialog, cluster nodes unintentionally rebooted. With this release, nodes reboot after you enter new values into the dialog and not the same values.(link:https://issues.redhat.com/browse/OCPBUGS-33938[OCPBUGS-33938])

* Previously, if a virtual machine (VM) was deleted and the network interface controller (NIC) still existed for that VM, the {azure-first} VM verification check failed. With this release, the verification check can now handle this situation by gracefully processing the issue without failing. (link:https://issues.redhat.com/browse/OCPBUGS-31467[OCPBUGS-31467])

* Previously, Red{nbsp}Hat HyperShift periodic conformance jobs failed because of changes to the core operating system. These failed jobs caused the OpenShift API deployment to fail. With this release, an update recursively copies individual trusted certificate authority (CA) certificates instead of copying a single file, so that the periodic conformance jobs succeed and the OpenShift API runs as expected link:https://issues.redhat.com/browse/OCPBUGS-38943[OCPBUGS-38943])

* Previously, the grace period for a node to become ready was not aligned with upstream behavior. This grace period sometimes caused a node to cycle between `Ready` and `Not ready` states. With this release, the issue is fixed so that the grace period does not cause a node to cycle between the two states. (link:https://issues.redhat.com/browse/OCPBUGS-39077[OCPBUGS-39077])

[id="ocp-4-15-31-known-issues_{context}"]
==== Known issues

* On {product-rosa}, node pools might stop scaling workloads or updating configurations for a cluster that uses the {product-version}.23 or later version of the {hcp-capital} service. Depending on the version of components that interact with your cluster, you can resolve this issue be completing the steps in one of the following Red{nbsp}Hat Knowledgebase articles:
+
** link:https://access.redhat.com/articles/7085089[ROSA HCP clusters fail to add new nodes in MachinePool version older than {product-version}.23]
** link:https://access.redhat.com/articles/7085666[ROSA upgrade issue mitigation for HOSTEDCP-1941]
+
(link:https://issues.redhat.com/browse/OCPBUGS-39463[OCPBUGS-39463])

[id="ocp-4-15-31-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-30_{context}"]
=== RHSA-2024:6013 - {product-title} 4.15.30 bug fix and security update

Issued: 5 September 2024

{product-title} release 4.15.30, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:6013[RHSA-2024:6013] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2024:6016[RHSA-2024:6016] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.30 --pullspecs
----

[id="ocp-4-15-30-bug-fixes_{context}"]
==== Bug fixes

* Previously, when running the `oc logs -f <pod>` command, the logs would not output anything after the log file was rotated. With this release, the kubelet outputs a log file after it has been rotated, and as a result the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-38861[OCPBUGS-38861])

* Previously, an internal timeout would occur when the service account had short lived credentials. With this release, that timeout is removed, and the timeout is now controlled by the parent context. (link:https://issues.redhat.com/browse/OCPBUGS-38198[OCPBUGS-38198])

* Previously, setting an invalid `.spec.endpoints.proxyUrl` attribute in the `ServiceMonitor` resource would result in breaking, reloading, and restarting Prometheus. This update fixes the issue by validating the `proxyUrl` attribute against invalid syntax. (link:https://issues.redhat.com/browse/OCPBUGS-36719[OCPBUGS-36719])

* Previously, there was an error when adding parameters to the Pipeline when the resource field was added to the payload, and as a result resources were deprecated. With this update, the resource fields have been removed from the payload, and you can add parameters to the Pipeline without getting an error. (link:https://issues.redhat.com/browse/OCPBUGS-33076[OCPBUGS-33076])

[id="ocp-4-15-30-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-29_{context}"]
=== RHSA-2024:5439 - {product-title} 4.15.29 bug fix and security update

Issued: 28 August 2024

{product-title} release 4.15.29, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHBA-2024:5751[RHBA-2024:5751] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2024:5754[RHSA-2024:5754] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.29 --pullspecs
----

[id="ocp-4-15-29-bug-fixes_{context}"]
==== Bug fixes

* Previously, the ordering of Network Interface Controllers (NICs) in the cloud provider was non-deterministic, which could result in the node using the wrong NIC for communication with the cluster. With this update, the ordering is now consistent. This fix prevents the random ordering that was causing the node networking fail. (link:https://issues.redhat.com/browse/OCPBUGS-38577[OCPBUGS-38577])

[id="ocp-4-15-29-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.28
[id="ocp-4-15-28_{context}"]
=== RHSA-2024:5439 - {product-title} 4.15.28 bug fix and security update

Issued: 2024-08-22

{product-title} release 4.15.28, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:5439[RHSA-2024:5439] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2024:5442[RHSA-2024:5442] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.28 --pullspecs
----

[id="ocp-4-15-28-bug-fixes_{context}"]
==== Bug fixes

* Previously, the {product-title} web console failed to restart a bare metal node. This release fixes this issue so that you can restart a bare metal node by using the {product-title} web console. (link:https://issues.redhat.com/browse/OCPBUGS-37099[OCPBUGS-37099])

[id="ocp-4-15-28-known-issues_{context}"]
==== Known issues

* An error might occur when deleting a pod that uses an SR-IOV network device. This error is caused by a change in {op-system-base} 9 where the previous name of a network interface is added to its alternative names list when it is renamed. As a consequence, when a pod attached to an SR-IOV virtual function (VF) is deleted, the VF returns to the pool with a new unexpected name; for example, `dev69`, instead of its original name,`ensf0v2`. Although this error is non-fatal, Multus and SR-IOV logs might show the error while the system reboots. Deleting the pod might take a few extra seconds due to this error. (link:https://issues.redhat.com/browse/OCPBUGS-11281[OCPBUGS-11281],link:https://issues.redhat.com/browse/OCPBUGS-18822[OCPBUGS-18822], link:https://issues.redhat.com/browse/RHEL-5988[RHEL-5988])

[id="ocp-4-15-28-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].


// 4.15.27
[id="ocp-4-15-27_{context}"]
=== RHSA-2024:5160 - {product-title} 4.15.27 bug fix and security update

Issued: 15 August 2024

{product-title} release 4.15.27, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:5160[RHSA-2024:5160] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2024:5163[RHBA-2024:5163] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.27 --pullspecs
----

[id="ocp-4-15-27-enhancements_{context}"]
==== Enhancements

The following enhancement is included in this z-stream release:

[id="ocp-4-15-27-collecting-ingress-dates_{context}"]
===== Collecting Ingress controller certificate information

* The Insights Operator now collects `NotBefore` and `NotAfter` date information about all Ingress Controller certificates and aggregates the information into a JSON file in the `aggregated/ingress_controllers_certs.json` path. (link:https://issues.redhat.com/browse/OCPBUGS-37672[OCPBUGS-37672])

[id="ocp-4-15-27-bug-fixes_{context}"]
==== Bug fixes

* Previously, when installing a cluster using the Agent-based installation program, generating a large number of manifests prior to installation could fill the Ignition storage, causing the installation to fail. With this update, the Ignition storage has been increased to allow for a much greater number of installation manifests. (link:https://issues.redhat.com/browse/OCPBUGS-33402[OCPBUGS-33402])

* Previously, when using the Agent-based installation program in a disconnected environment, unnecessary certificates were added to the CA trust bundle. With this update, the CA bundle ConfigMap only contains CAs explicitly specified by the user. (link:https://issues.redhat.com/browse/OCPBUGS-34721[OCPBUGS-34721])

* Previously, `HostedClusterConfigOperator` did not delete the `ImageDigestMirrorSet` (IDMS) object after a user removed the `ImageContentSources` field from the `HostedCluster` object. This caused the IDMS object to remain in the `HostedCluster` object. With this release, `HostedClusterConfigOperator` removes all IDMS resources in the `HostedCluster` object so that this issue no longer exists. (link:https://issues.redhat.com/browse/OCPBUGS-37174[OCPBUGS-37174])

* Previously, when the Cloud Credential Operator checked if passthrough mode permissions were correct, the Operator sometimes received a response from the {gcp-first} API about an invalid permission for a project. This bug caused the Operator to enter a degraded state that in turn impacted the installation of the cluster. With this release, the Cloud Credential Operator checks specifically for this error so that it diagnoses it separately without impacting the installation of the cluster. (link:https://issues.redhat.com/browse/OCPBUGS-37288[OCPBUGS-37288])

* Previously, the `OVNKubernetesNorthdInactive` alert did not fire as expected. With this release, the `OVNKubernetesNorthdInactive` alert works as expected and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-36821[OCPBUGS-36821])

[id="ocp-4-15-27-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.25
[id="ocp-4-15-25_{context}"]
=== RHSA-2024:4955 - {product-title} 4.15.25 bug fix and security update

Issued: 7 August 2024

{product-title} release 4.15.25, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:4955[RHSA-2024:4955] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2024:4958[RHSA-2024:4958] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.25 --pullspecs
----

[id="ocp-4-15-25-enhancements"]
==== Enhancements

The following enhancements are included in this z-stream release:

[id="ocp-4-15-25-enhancements-azure-res-cap_{context}"]
===== Configuring Capacity Reservation by using machine sets

* {product-title} release {product-version}.25 introduces support for on-demand Capacity Reservation with Capacity Reservation groups on {azure-full} clusters.
For more information, see _Configuring Capacity Reservation by using machine sets_ for xref:../machine_management/creating_machinesets/creating-machineset-azure.adoc#machineset-azure-capacity-reservation_creating-machineset-azure[compute] or xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-azure.adoc#machineset-azure-capacity-reservation_cpmso-config-options-azure[control plane] machine sets. (link:https://issues.redhat.com/browse/OCPCLOUD-1646[OCPCLOUD-1646])

[id="ocp-4-15-25-bug-fixes_{context}"]
==== Bug fixes

* Previously, a machine config pool (MCP) with a higher `maxUnavailable` value than the amount of unavailable nodes would cause cordoned nodes to receive updates if they were in a certain position on the node list. With this release, a fix ensures that cordoned nodes are not added to a queue to receive updates. (link:https://issues.redhat.com/browse/OCPBUGS-37629[OCPBUGS-37629])

* Prevously, an errant code change resulted in a duplicated `oauth.config.openshift.io` item on the Global Configuration page. With this release, the duplicated item is removed. (link:https://issues.redhat.com/browse/OCPBUGS-37458[OCPBUGS-37458])

* Previously, the Cluster Network Operator (CNO) IPsec mechanism incorrectly behaved on a cluster that had multiple worker machine config pools. With this release, CNO Ipsec mechanism works as intended for a cluster with multiple worker machine config pools. This fix does not apply to updating an IPsec-enabled cluster with multiple paused machine config pools. (link:https://issues.redhat.com/browse/OCPBUGS-37205[OCPBUGS-37205])

* Previously, the Open vSwitch (OVS) pinning procedure set the CPU affinity of the main thread, but other CPU threads did not pick up this affinity if they had already been created. As a consequence, some OVS threads did not run on the correct CPU set, which might interfere with the performance of pods with a Quality of Service (QoS) class of `Guaranteed`. With this release, the OVS pinning procedure updates the affinity of all the OVS threads, ensuring that all OVS threads run on the correct CPU set. (link:https://issues.redhat.com/browse/OCPBUGS-37196[OCPBUGS-37196])

* Previously, when you created or deleted large volumes of service objects simultaneously, the service controller's ability to process each service sequentially would slow down. This caused short timeout issues for the service controller and backlog issues for the objects. With this release, the service controller can now process up to 10 service objects simultaneously to reduce the backlog and timeout issues. (link:https://issues.redhat.com/browse/OCPBUGS-36821[OCPBUGS-36821])

[id="ocp-4-15-25-known-issue_{context}"]
==== Known issues

* On clusters with the SR-IOV Network Operator installed and configured, pods with a secondary interface of SRI-OV VF fail to create a pod sandbox and do not function. (link:https://issues.redhat.com/browse/OCPBUGS-38090[OCPBUGS-38090])

[id="ocp-4-15-25-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-24_{context}"]
=== RHSA-2024:4850 - {product-title} 4.15.24 bug fix and security update

Issued: 31 July 2024

{product-title} release 4.15.24, which includes security updates, is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:4850[RHSA-2024:4850] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2024:4853[RHSA-2024:4853] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.24 --pullspecs
----

[id="ocp-4-15-24-bug-fixes_{context}"]
==== Bug fixes

* Previously, the TuneD daemon could unnecessarily reload an additional time after a Tuned custom resource (CR) update. With this release, the Tuned object has been removed and the TuneD (daemon) profiles are carried directly in the Tuned Profile Kubernetes objects. As a result, the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-36870[OCPBUGS-36870])

[id="ocp-4-15-24-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-23_{context}"]
=== RHSA-2024:4699 - {product-title} 4.15.23 bug fix and security update

Issued: 25 July 2024

{product-title} release 4.15.23 is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:4699[RHSA-2024:4699] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2024:4702[RHSA-2024:4702] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.23 --pullspecs
----

[id="ocp-4-15-23-enhancements_{context}"]
==== Enhancements

The following enhancement is included in this z-stream release:

[id="ocp-4-15-23-timeout-ingress_{context}"]
===== Adding connectTimeout tuning option to Ingress Controller API

* The IngressController API is updated with a new tuning option, `ingresscontroller.spec.tuningOptions.connectTimeout`, which defines how long the router will wait for a response when establishing a connection to a backend server. (link:https://issues.redhat.com/browse/OCPBUGS-36208[OCPBUGS-36208])

[id="ocp-4-15-23-bug-fixes_{context}"]
==== Bug fixes

* Previously, the OpenSSL versions for the Machine Config Operator and the hosted control plane were not the same. With this release, the FIPS cluster `NodePool` resource creation for {product-title} 4.14 and {product-title} 4.15 has been fixed and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-37266[OCPBUGS-37266])

* Previously, the operand details displayed information for the first custom resource definition (CRD) that matched by name. With this release, the operand details page displays information for the CRD that matches by name and the version of the operand. (link:https://issues.redhat.com/browse/OCPBUGS-36971[OCPBUGS-36971])

* Previously, the HyperShift hosted control plane (HCP) would fail to generate ignition because of mismatched versions of {op-system-base-full} OpenSSL versions used by the HyperShift Control Plane Operator and the Machine Config Operator. With this release, the versions of {op-system-base-full} OpenSSL match correctly and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-36863[OCPBUGS-36863])

* Previously, the Ingress Operator could not successfully update the canary route because the Operator did not have permission to update `spec.host` or `spec.subdomain` on an existing route. With this release, the required permission is added to the cluster role for the Operator's `ServiceAccount` and the Ingress Operator can update the canary route. (link:https://issues.redhat.com/browse/OCPBUGS-36466[OCPBUGS-36466])

* Previously, installing an Operator could sometimes fail if the same Operator had been previously installed and uninstalled. This was due to a caching issue. This bug fix updates Operator Lifecycle Manager (OLM) to correctly install the Operator in this scenario, and as a result this issue no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-36451[OCPBUGS-36451])

* Previously, after installing the Pipelines Operator, Pipeline templates took some time to become available in the cluster, but users were still able to create the Deployment. With this update, the *Create* button on the *Import from Git* page is disabled if there is no pipeline template present for the resource selected. (link:https://issues.redhat.com/browse/OCPBUGS-34477[OCPBUGS-34477])

[id="ocp-4-15-23-updating_{context}"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.22
[id="ocp-4-15-22"]
=== RHSA-2024:4474 - {product-title} 4.15.22 bug fix and security update

Issued: 18 July 2024

{product-title} release 4.15.22 is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:4474[RHSA-2024:4474] advisory. There are no RPM packages in this release.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.22 --pullspecs
----

[id="ocp-4-15-22-enhancements"]
==== Enhancements

The following enhancement is included in this z-stream release:

[id="ocp-4-15-22-taskrun-status"]
===== Introducing TaskRun status

* Previously, the `TaskRun` status was not displayed near the `TaskRun` name on the *TaskRun details* page. With this update, the `TaskRun` status is located beside the name of the `TaskRun` in the page heading. (link:https://issues.redhat.com/browse/OCPBUGS-32156[OCPBUGS-32156])

[id="ocp-4-15-22-bug-fixes"]
==== Bug fixes

* Previously, the `HighOverallControlPlaneCPU` alert triggered warnings based on criteria for multi-node clusters with high availability. As a result, misleading alerts were triggered in {sno} clusters because the configuration did not match the environment criteria. This update refines the alert logic to use {sno}-specific queries and thresholds and account for workload partitioning settings. As a result, CPU utilization alerts in {sno} clusters are accurate and relevant to single-node configurations. (link:https://issues.redhat.com/browse/OCPBUGS-35832[OCPBUGS-35832])

* In an AWS STS cluster, the Cloud Credential Operator (CCO) checks `awsSTSIAMRoleARN` in `CredentialsRequest` to create the secret. Previously, CCO logged an error if `awsSTSIAMRoleARN` was not present, which resulted in multiple errors per second. With this release, CCO does not log the error and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-36291[OCPBUGS-36291])

* Previously, if a new deployment was completed at the OSTree level on a host that was identical to the current deployment but on a different stateroot, OSTree treated them as equal. With this release, the OSTree logic is modified and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-36436[OCPBUGS-36436])

* Previously, a change of dependency targets introduced in {product-title} 4.14 prevented {azure-first} {product-title} installations from scaling up new nodes after upgrading to later versions. With this release, the issue is resolved for {product-title} 4.15. (link:https://issues.redhat.com/browse/OCPBUGS-36550[OCPBUGS-36550])

[id="ocp-4-15-22-known-issue_{context}"]
==== Known issue

* If the `ConfigMap` object maximum transmission unit (MTU) is absent in the `openshift-network-operator` namespace, you must create the `ConfigMap` object manually with the machine MTU value before you start the live migration. Otherwise, the live migration fails. (link:https://issues.redhat.com/browse/OCPBUGS-35829[OCPBUGS-35829])

[id="ocp-4-15-22-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.21
[id="ocp-4-15-21"]
=== RHSA-2024:4321 - {product-title} 4.15.21 bug fix and security update

Issued: 10 July 2024

{product-title} release 4.15.21 is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:4321[RHSA-2024:4321] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2024:4324[RHBA-2024:4324] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.21 --pullspecs
----

[id="ocp-4-15-21-bug-fixes"]
==== Bug fixes

* Previously, the `alertmanager-trusted-ca-bundle` config map was not injected into the user-defined Alertmanager container, which prevented the verification of HTTPS web servers receiving alert notifications. With this update, the trusted CA bundle config map is mounted into the Alertmanager container at the `/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem` path. (link:https://issues.redhat.com/browse/OCPBUGS-36312[OCPBUGS-36312])

* Previously, the internal image registry did not correctly authenticate users on clusters configured for `externalAWS` IAM OpenID Connect (OIDC) users. This causes issues for users when pushing or pulling images to and from the internal image registry. With this release, the internal image registry starts by using the `SelfSubjectReview` API instead of the OpenShift-specific user API. The OpenShift-specific user API is not compatible with external OIDC users. (link:https://issues.redhat.com/browse/OCPBUGS-36287[OCPBUGS-36287])

* Previously, for clusters upgraded from earlier versions of {product-title}, enabling `kdump` on an OVN-enabled cluster sometimes prevented the node from rejoining the cluster or returning to the `Ready` state. With this release, stale data from earlier {product-title} versions are removed, so that nodes can now correctly start and rejoin the cluster. (link:https://issues.redhat.com/browse/OCPBUGS-36258[OCPBUGS-36258])

* Previously, the {product-title} installation program included a pair of slashes (`//`) in a path to a resource pool for a cluster installed on {vmw-first}. This issue caused the ControlPlaneMachineSet (CPMS) Operator to create additional contol plane machines. With this release, the pair of slashes is removed to prevent this issue from occuring. (link:https://issues.redhat.com/browse/OCPBUGS-36225[OCPBUGS-36225])

* Previously, the GrowPart tool locked a device. This impacted Linux Unified Key Setup-on-disk-format (LUKS) devices from being opened and caused the operating system to boot into emergency mode. With this release, the call to the GrowPart tool is removed, so that LUKS devices are not unintentionally locked and the operating system can successfully boot. (link:https://issues.redhat.com/browse/OCPBUGS-35988[OCPBUGS-35988])

* Previously, a bug in systemd might cause the `coreos-multipath-trigger.service` unit to hang forever. As a result, the system would never finish booting. With this release, the systemd unit was removed and the issue is fixed. (link:https://issues.redhat.com/browse/OCPBUGS-35749[OCPBUGS-35749])

* Previously, a transient failure to fetch bootstrap data during machine creation, such as a transient failure to connect to the API server, caused the machine to enter a terminal failed state. With this release, failure to fetch bootstrap data during machine creation is retried until it eventually succeeds. (link:https://issues.redhat.com/browse/OCPBUGS-34665[OCPBUGS-34665])

[id="ocp-4-15-21-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.20
[id="ocp-4-15-20"]
=== RHSA-2024:4151 - {product-title} 4.15.20 bug fix and security update

Issued: 2 July 2024

{product-title} release 4.15.20 is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:4151[RHSA-2024:4151] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2024:4154[RHBA-2024:4154] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.20 --pullspecs
----

[id="ocp-4-15-20-enhancements"]
==== Enhancements

The following enhancements are included in this z-stream release:

[id="ocp-4-15-20-imagestream-disconnected-environments"]
===== Enabling imagestream builds in disconnected environments

* This release adds additional TrustedBundles to the OpenShift API server (OAS) container and enables imagestream builds in disconnected environments. (link:https://issues.redhat.com/browse/OCPBUGS-34579[OCPBUGS-34579])

[id="ocp-4-15-20-insights-operator-new-instances"]
===== Collecting the Prometheus and Alert Manager instances by the Insights Operator
* The Insights Operator (IO) now collects the `Prometheus` and `AlertManager` resources in addition to the `openshift-monitoring` custom resource. (link:https://issues.redhat.com/browse/OCPBUGS-35865[OCPBUGS-35865])

[id="ocp-4-15-20-bug-fixes"]
==== Bug fixes

* Previously, when an optional internal function of the cluster autoscaler was not implemented, the function caused repeated log entries. With this release, the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-33885[OCPBUGS-33885])

* Previously, default Operator Lifecycle Manager (OLM) catalog pods remained in a termination state when there was an outage of the node that was being used. With this release, the OLM catalog pods that are backed by a `CatalogSource` correctly recover from planned and unplanned node maintenance. (link:https://issues.redhat.com/browse/OCPBUGS-35305[OCPBUGS-35305]).

// Jira requires a login so this Jira should not have been documented
//* Previously, what the Azure API returns for a subnet caused the Installer to terminate unexpectedly. With this release, the code has been updated to handle the old and new data for subnets, as well as to return an error in case the expected information is not found. (link:https://issues.redhat.com/browse/OCPBUGS-35502[OCPBUGS-35502]).

* Previously, AWS HyperShift clusters leveraged their VPC's primary CIDR range to generate security group rules on the data plane. As a consequence, installing AWS HyperShift clusters into an AWS VPC with multiple CIDR ranges caused the generated security group rules to be insufficient. With this release, security group rules are generated based on the provided Machine CIDR range instead to resolve this issue. (link:https://issues.redhat.com/browse/OCPBUGS-35714[OCPBUGS-35714])

* Previously, in User Provisioned Infrastructure (UPI) or clusters that were upgraded from older versions, `failureDomains` may be missing in Infrastructure objects which caused certain checks to fail. With this release, a fallback `failureDomains` is synthesized from `cloudConfig` if none are available in `infrastructures.config.openshift.io`. (link:https://issues.redhat.com/browse/OCPBUGS-35732[OCPBUGS-35732])

* Previously, a rare timing issue could prevent all control plane nodes from being added to an Agent-based cluster during installation. With this update, all control plane nodes are successfully rebooted and added to the cluster during installation. (link:https://issues.redhat.com/browse/OCPBUGS-35894[OCPBUGS-35894])

[id="ocp-4-15-20-known-issues"]
==== Known issue

* Compact clusters with 3 masters that are configured to run customer workloads are supported with OpenShift IPI installs on GCP, but not on AWS or Azure. (link:https://issues.redhat.com/browse/OCPBUGS-35359[OCPBUGS-35359])

[id="ocp-4-15-20-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.19
[id="ocp-4-15-19"]
=== RHSA-2024:4041 - {product-title} 4.15.19 bug fix and security update

Issued: 27 June 2024

{product-title} release 4.15.19 is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:4041[RHSA-2024:4041] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2024:4044[RHBA-2024:4044] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.19 --pullspecs
----

[id="ocp-4-15-19-bug-fixes"]
==== Bug fixes

* Previously, when a new version of a Custom Resource Definition (CRD) specified a new conversion strategy, this conversion strategy was expected to successfully convert resources. This was not the case because Operator Lifecycle Manager (OLM) cannot run the new conversion strategies for CRD validation without actually performing the update operation. With this release, the OLM implementation generates a warning message during the update process when CRD validations fail with the existing conversion strategy and the new conversion strategy is specified in the new version of the CRD. (link:https://issues.redhat.com/browse/OCPBUGS-35720[OCPBUGS-35720]).

* Previously, during node reboots, especially during update operations, the node that interacts with the rebooting machine entered a `Ready=Unknown` state for a short amount of time. This situation caused the Control Plane Machine Set Operator to enter an `UnavailableReplicas` condition and then an `Available=false` state. The `Available=false` state triggers alerts that demand urgent action, but in this case, intervention was only required for a short period of time until the node rebooted. With this release, a grace period for node unreadiness is provided where if a node enters an unready state, the Control Plane Machine Set Operator does not instantly enter an `UnavailableReplicas` condition or an `Available=false` state. (link:https://issues.redhat.com/browse/OCPBUGS-34971[OCPBUGS-34971]).

* Previously, the OpenShift Cluster Manager container did not have the right TLS Certificates. As a result, image streams could not be used in disconnected deployments. With this update, the TLS Certificates are added as projected volumes. (link:https://issues.redhat.com/browse/OCPBUGS-34580[OCPBUGS-34580])

* Previously, when a serverless function was created in the create serverless form, `BuilldConfig` was not created. With this update, if the Pipelines Operator is not installed, or if the pipeline resource is not created for particular resource, or if the pipeline is not added while creating a serverless function, the `BuildConfig` is created as expected. (link:https://issues.redhat.com/browse/OCPBUGS-34350[OCPBUGS-34350])

* Previously, the reduction of the network queue did not work as expected for inverted rules such as `!ens0`. This happened because the exclamation mark symbol was duplicated in the generated tuned profile. With this release, the duplication no longer occurs so that inverted rules apply as intended. (link:https://issues.redhat.com/browse/OCPBUGS-33929[OCPBUGS-33929]).

* Previously, registry overrides configured by a cluster administrator on the management side applied to non-relevant data-plane components. With this release, registry overrides no longer apply to these components. (link:https://issues.redhat.com/browse/OCPBUGS-33627[OCPBUGS-33627]).

* Previously, when installing a cluster on {vmw-first}, the installation failed if an ESXi host was in maintenance mode because the installation program could not retrieve version information from the host. With this update, the installation program does not attempt to retrieve version information from ESXi hosts that are in maintenance mode, allowing the installation to proceed. (link:https://issues.redhat.com/browse/OCPBUGS-31387[OCPBUGS-31387])

[id="ocp-4-15-19-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.18
[id="ocp-4-15-18"]
=== RHSA-2024:3889 - {product-title} 4.15.18 bug fix and security update

Issued: 18 June 2024

{product-title} release 4.15.18 is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHSA-2024:3889[RHSA-2024:3889] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2024:3892[RHBA-2024:3892] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.18 --pullspecs
----

[id="ocp-4-15-18-enhancements"]
==== Enhancements

The following enhancements are included in this z-stream release:

[id="ocp-4-15-18-unservable-future-versions-status-condition"]
===== Blocking upgrades for SHA1 serving certificates on routes and Ingress Controllers

* {product-title} {product-version} supports SHA1 certificates on routes, but sets `Upgradeable=False`. Consequently, upgrades from 4.15 to 4.16 cause routes and Ingress Controllers with SHA1 certificates to be rejected.
+
Only serving certificates are affected. For routes, this is the certificate specified in `spec.tls.certificate`. For Ingress Controllers, this is the serving certificate in the secret specified in `spec.defaultCertificate`. CA certificates using SHA1 are not impacted. However, due to link:https://issues.redhat.com/browse/OCPBUGS-42480[OCPBUGS-42480], some Ingress Controllers were incorrectly blocking upgrades if the serving certificate was not using SHA1, but the CA certificate was. To resolve this, upgrade to version 4.15.37 to receive this bug fix.
+
Additionally, this update introduces a new `UnservableInFutureVersions` status condition to routes that contain a SHA1 certificate. It also adds an `admin-gate` to block upgrades if this new status is present on any route. As a result, if cluster administrators have routes that use SHA1 certificates in {product-title} 4.15, they must either upgrade these certificates to a supported algorithm or provide an `admin-ack` for the created `admin-gate`. This `admin-ack` allows administrators to proceed with the upgrade without resolving the SHA1 certificate issues, even though the routes will be rejected. The full `admin-ack` command is:
+
[source,terminal]
----
$ oc -n openshift-config patch cm admin-acks --patch '{"data":{"ack-4.15-route-config-not-supported-in-4.16":"true"}}' --type=merge
----
+
(link:https://issues.redhat.com/browse/OCPBUGS-28928[OCPBUGS-28928]).

[id="ocp-4-15-18-password-colon-character"]
===== Allowing pull secret passwords to contain a colon character

* This release introduces the ability to include a colon character in your password for the Openshift Assisted Installer. (link:https://issues.redhat.com/browse/OCPBUGS-34641[OCPBUGS-34641])

[id="ocp-4-15-18-defragment-etcd"]
===== Introducing an etcd defragmentation controller for Hypershift

* This release introduces an etcd degragmentation controller for hosted clusters on Hypershift. (link:https://issues.redhat.com/browse/OCPBUGS-35002[OCPBUGS-35002])

[id="ocp-4-15-18-bug-fixes"]
==== Bug fixes

* Previously, the {product-title} web console terminated unexpectedly if authentication discovery failed on the first attempt. With this update, authentication initialization was updated to retry up to 5 minutes before failing. (link:https://issues.redhat.com/browse/OCPBUGS-30208[OCPBUGS-30208])

* Previously, the `metal3-ironic` and `metal3-ironic-inspector` pods failed when upgrading to {product-title} 4.15.11 from 4.15.8 due to an install failure related to FIPS mode enablement. With this release, the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-33736[OCPBUGS-33736])

* Previously, the OpenShift Agent Installer reported installed SATA SDDs as removable and refused to use any of them as installation targets. With this release removable disks are eligible for installation and the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-34732[OCPBUGS-34732])

* Previously, the AWS EFS driver controller returned a runtime error when it provisioned a new volume on EFS filesystem if pre-existing access points without a POSIX user were present. With this release, the driver has been fixed and the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-34843[OCPBUGS-34843])

* Previously, the secrets-store CSI driver on Hypershift was failing to mount secrets due to an issue with the Hypershift CLI. With this release, the driver is able to mount volumes and the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-34997[OCPBUGS-34997])

* Previously, in disconnected environments, the HyperShift Operator ignored registry overrides. As a consequence, changes to node pools were ignored, and node pools encountered errors. With this update, the metadata inspector works as expected during the HyperShift Operator reconciliation, and the override images are properly populated. (link:https://issues.redhat.com/browse/OCPBUGS-35074[OCPBUGS-35074])

[id="ocp-4-15-18-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-17"]
=== RHBA-2024:3673 - {product-title} 4.15.17 bug fix and security update

Issued: 11 June 2024

{product-title} release 4.15.17 is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHBA-2024:3673[RHBA-2024:3673] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHSA-2024:3676[RHSA-2024:3676] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.17 --pullspecs
----

[id="ocp-4-15-17-enhancements"]
==== Enhancements

The following enhancement is included in this z-stream release:

[id="ocp-4-15-17-kube-api-server-v1alpha1-removal-prep"]
===== Storage migration for cluster versions 4.8 or earlier

* This release introduces a storage migration that supports a safe cluster update from version 4.8 or earlier to the latest supported release. If you created a cluster at version 4.7 or earlier, your stored objects remain accessible when updating your cluster to the latest supported release. (link:https://issues.redhat.com/browse/OCPBUGS-31445[OCPBUGS-31445])

[id="ocp-4-15-17-bug-fixes"]
==== Bug fixes

* Previously, when multiple domains were configured for an Amazon Virtual Private Cloud (VPC) DHCP option, the hostname could return multiple values. However, the logic did not account for multiple values and crashed when it returned a node name containing a space. With this release, the logic has been updated to use the first returned hostname as the node name and the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-33847[OCPBUGS-33847])

* Previously, when enabling `virtualHostedStyle` with `regionEndpoint` set in the Image Registry Operator configuration, the image registry ignored the virtual hosted style configuration and failed to start. With this release, the image registry uses a new upstream distribution configuration and the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-34539[OCPBUGS-34539])

* Previously, the OperatorHub incorrectly excluded the Amazon Resource Name (ARN) role information for ROSA Hosted Control Plane (HCP) clusters. With this update, the OperatorHub correctly displays ARN information and the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-34550[OCPBUGS-34550])

* Previously, when attempting to delete a cluster or BareMetalHost (BMH) resource before installation, the metal3-operator tried to unnecessarily generate a pre-provisioning image. With this release, an exception has been created to prevent the creation of a pre-provisioning image during a BMH deletion and the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-34682[OCPBUGS-34682])

* Previously, some text areas were no longer resizable when editing a config map in the *Form View* of the web console in {product-title} 4.15. With this release, those text areas are now resizable. (link:https://issues.redhat.com/browse/OCPBUGS-34703[OCPBUGS-34703])

[id="ocp-4-15-17-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-16"]
=== RHBA-2024:3488 - {product-title} 4.15.16 bug fix update

Issued: 5 June 2024

{product-title} release 4.15.16 is now available. The list of bug fixes that are included in this update is documented in the link:https://access.redhat.com/errata/RHBA-2024:3488[RHBA-2024:3488] advisory. The RPM packages that are included in this update are provided by the link:https://access.redhat.com/errata/RHBA-2024:3491[RHBA-2024:3491] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.16 --pullspecs
----

[id="ocp-4-15-16-bug-fixes"]
==== Bug fixes

* Previously, in HAProxy 2.6 deployments on {product-title}, shutting down HAproxy could result in a race condition. The main thread `(tid=0)` would wait for other threads to complete, but some threads would enter an infinite loop, consuming 100% CPU. With this release, the variable controlling the loop's termination is now properly reset, preventing non-main threads from looping indefinitely. This ensures that the thread's poll loop can terminate correctly. (link:https://issues.redhat.com/browse/OCPBUGS-33883[OCPBUGS-33883])

* Previously, the console Operator health check controller had a missing return statement, which caused the Operator to crash unexpectedly in some cases. With this release, the issue has been fixed. (link:https://issues.redhat.com/browse/OCPBUGS-33720[OCPBUGS-33720])

* Previously, the `wait-for-ceo` command used during the bootstrapping process to verify etcd rollout did not report errors for some failure modes. With this release, those error messages are visible on the `bootkube` script if the `wait-for-ceo` command exits in an error case. (link:https://issues.redhat.com/browse/OCPBUGS-33564[OCPBUGS-33564])

[id="ocp-4-15-16-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-15"]
=== RHSA-2024:3327 - {product-title} 4.15.15 bug fix and security update

Issued: 29 May 2024

{product-title} release 4.15.15, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:3327[RHSA-2024:3327] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2024:3332[RHBA-2024:3332] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.15 --pullspecs
----

[id="ocp-4-15-15-bug-fixes"]
==== Bug fixes

* Previously, certain situations caused transfer of an Egress IP address from one node to a different node to fail, and this failure impacted the OVN-Kubernetes network. The network failed to send gratuitous Address Resolution Protocol (ARP) requests to peers to inform them of the new node's medium access control (MAC) address. As a result, peers would temporarily send reply traffic to an old node and this traffic led to failover issues. With this release, the OVN-Kubernetes network correctly sends a gratuitous ARP to peers to inform them of the new Egress IP node MAC address, so that each peer can send reply traffic to the new node without causing failover time issues. (link:https://issues.redhat.com/browse/OCPBUGS-33960[OCPBUGS-33960])

* Previously, when mirroring Operator catalogs, the `oc-mirror` CLI plugin rebuilt the catalogs and regenerated the catalog's internal cache according to the `imagesetconfig` catalog filtering specifications. This operation required the use of the `opm` binary found within the catalogs. In {product-title} 4.15, Operator catalogs include the `opm` {op-system-base-full} 9 binary, and this caused the mirroring process to fail when running on {op-system-base} 8 systems. With this release, `oc-mirror` no longer builds catalogs by default. Instead, catalogs are mirrored directly to their destination registries. (link:https://issues.redhat.com/browse/OCPBUGS-33575[OCPBUGS-33575])

* Previously, the image registry did not support Amazon Web Services (AWS) region `ca-west-1`. With this release, the image registry can now be deployed in this region. (link:https://issues.redhat.com/browse/OCPBUGS-33672[OCPBUGS-33672])

* Previously, service accounts (SAs) could not be used as OAuth2 clients because there were no tokens associated with the SAs. With this release, the OAuth registry client has been modified to anticipate this case and the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-33210[OCPBUGS-33210])

* Previously, the proxy information set in the `install-config.yaml` file was not applied to the bootstrap process. With this release, the proxy information is applied to the bootstrap Ignition data which is applied to the bootstrap machine and the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-33205[OCPBUGS-33205])

* Previously, the information from the `imageRegistryOverrides` setting was only extracted once on the HyperShift Operator initialization and did not refresh. With this release, the Hypershift Operator retrieves the new `ImageContentSourcePolicy` files from the management cluster and adds them to the Hypershift Operator and Control Plane Operator in every reconciliation loop. (link:https://issues.redhat.com/browse/OCPBUGS-33117[OCPBUGS-33117])

* Previously, the Hypershift Operator was not using the `RegistryOverrides` mechanism to inspect the image from the internal registry. With this release, the metadata inspector works as expected during the Hypershift Operator reconciliation, and the `OverrideImages` are properly populated. (link:https://issues.redhat.com/browse/OCPBUGS-32220[OCPBUGS-32220])

* Previously, attempting to update the {vmw-first} connection configuration for {product-title} failed if the configuration included `””` characters. With this release, the characters are stored correctly and the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-31863[OCPBUGS-31863])

[id="ocp-4-15-15-known-issues"]
==== Known issue

* Previously, after upgrading to {product-title} 4.15.6, attempting to use the `oc-mirror` CLI plugin within a cluster failed. With this release, there is now a FIPS-compliant version of `oc-mirror` for RHEL 9 and a version of `oc-mirror` for RHEL 8 that is not FIPS-compliant. (link:https://issues.redhat.com/browse/OCPBUGS-31609[OCPBUGS-31609])

[id="ocp-4-15-15-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-14"]
=== RHSA-2024:2865 - {product-title} 4.15.14 bug fix and security update

Issued: 21 May 2024

{product-title} release 4.15.14, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:2865[RHSA-2024:2865] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2024:2870[RHBA-2024:2870] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.14 --pullspecs
----

[id="ocp-4-15-14-bug-fixes"]
==== Bug fixes

* Previously, if traffic was forwarded to terminating endpoints that were not functioning, communications problems occured unless the readiness probes on these endpoints were configured to quickly flag the endpoints as not serving. This occurred because the the endpoint selection for services partially implemented KEP-1669 `ProxyTerminatingEndpoints` traffic to services inside the {product-title} cluster. As a result, this traffic was forwarded to all endpoints that were either ready, such as `ready=true`, `serving=true`, `terminating=false`,  or terminating and serving, such as `ready=false`, `serving=true`, `terminating=true`. This caused communication issues when traffic was forwarded to terminating endpoints and the readiness probes on these endpoints were not configured to quickly flag the endpoints as not serving, `serving=false`, when they were no longer functional. With this release, the endpoints selection logic now fully implements KEP-1669 `ProxyTerminatingEndpoints` for any given service so that all ready endpoints are selected. If no ready endpoints are found, functional terminating and serving endpoints are used.(link:https://issues.redhat.com/browse/OCPBUGS-27852[OCPBUGS-27852])

* Previously, if you configured an {product-title} cluster with a high number of internal services or user-managed load balancer IP addresses, you experienced a delayed startup time for the OVN-Kubernetes service. This delay occurred when the OVN-Kubernetes service attempted to install `iptables` rules on a node. With this release, the OVN-Kubernetes service can process a large number of services in a few seconds. Additionally, you can access a new log to view the status of installing `iptables` rules on a node. (link:https://issues.redhat.com/browse/OCPBUGS-32426[OCPBUGS-32426])

* Previously, some container processes created by using the `exec` command persisted even when CRI-O stopped the container. Consequently, lingering processes led to tracking issues, causing process leaks and defunct statuses. With this release, CRI-O tracks the `exec` calls processed for a container and ensures that the processes created as part of the `exec` calls are terminated when the container is stopped. (link:https://issues.redhat.com/browse/OCPBUGS-32481[OCPBUGS-32481])

* Previously, the *Topology* view in the {product-title} web console did not show the visual connector between a virtual machine (VM) node and other non-VM components. With this release, the visual connector shows interaction activity of a component. (link:https://issues.redhat.com/browse/OCPBUGS-32505[OCPBUGS-32505])

* Previously, a logo in the masthead element of the {product-title} web console could grow beyond 60 pixels in height. This caused the masthead to increase in height. With this release, the masthead logo is constrained to a `max-height` of 60 pixels. (link:https://issues.redhat.com/browse/OCPBUGS-33548[OCPBUGS-33548])

* Previously, if you need the *Form* view in the {product-title} web console to remove an alternate service from a `Route` resource, the alternate service remained in the cluster. With this release, if you delete an alternate service in this way, the alternate service is fully removed from the cluster. (link:https://issues.redhat.com/browse/OCPBUGS-33058[OCPBUGS-33058])

* Previously, {product-title} cluster connections to the {azure-first} API were delayed because of an issue with the API's codebase. With this release, a timeout schedule is set for any calls to the {azure-short} API, so that an API call that hangs for a period of time is terminated. (link:https://issues.redhat.com/browse/OCPBUGS-33127[OCPBUGS-33127])

* Previously, a kernel regression that was introduced in {product-title} 4.15.0 caused kernel issues, such as nodes crashing and rebooting, in nodes that mounted to CephFS storage. In this release, the regression issue is fixed so that the kernel regression issue no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-33250[OCPBUGS-33250])

* Previously, the {vmw-first} Problem Detector Operator did not have HTTP and HTTPS proxies configured for it. This resulted in invalid cluster configuration error messages because connection issues between the Operator and the {vmw-full} vCenter server. With this release, the {vmw-short} Problem Detector Operator uses the same HTTP and HTTPS proxies as other {product-title} cluster Operators so that the {vmw-short} Problem Detector Operator can connect to the {vmw-full} vCenter. (link:https://issues.redhat.com/browse/OCPBUGS-33466[OCPBUGS-33466])

* Previously, Alertmanager would send notification emails that contained a backlink to the Thanos Querier web interface. This web interface is an unreachable web service. With this release, monitoring alert notification emails contain a backlink to the {product-title} web console's *Alerts* page. (link:https://issues.redhat.com/browse/OCPBUGS-33512[OCPBUGS-33512])

[id="ocp-4-15-14-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-13"]
=== RHSA-2024:2773 - {product-title} 4.15.13 bug fix and security update

Issued: 15 May 2024

{product-title} release 4.15.13, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:2773[RHSA-2024:2773] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2024:2776[RHSA-2024:2776] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.13 --pullspecs
----

[id="ocp-4-15-13-bug-fixes"]
==== Bug fixes

* Previously, the name of the Security Context Constraint (SCC) was incorrect so there was no functioning built-in cluster role. With this release, the name was changed to `hostmount-anyuid` and the SCC now has a functioning built-in cluster role. (link:https://issues.redhat.com/browse/OCPBUGS-33277[OCPBUGS-33277])

* Previously, the Ironic Python Agent (IPA) failed when trying to wipe disks because it expected the wrong byte sector size, which caused the node provisioning to fail. With this release, the IPA checks the disk sector size and node provisioning succeeds. (link:https://issues.redhat.com/browse/OCPBUGS-33133[OCPBUGS-33133])

* Previously, Static Persistent Volumes in Azure File on Workload Identity clusters could not be configured due to a bug in the driver causing volume mounts to fail. With this release, the driver has now been fixed, and Static Persistent Volumes mount correctly. (link:https://issues.redhat.com/browse/OCPBUGS-33038[OCPBUGS-33038])

* Previously, during {product-title} updates in performance-tuned clusters, resuming a `MachineConfigPool` resource resulted in additional restarts for nodes in the pool. This was due to the performance profile controller reconciling against outdated machine configurations while the pool was paused. With this update, the controller reconciles against the latest planned machine configurations before the pool resumes, preventing additional node reboots. (link:https://issues.redhat.com/browse/OCPBUGS-32978[OCPBUGS-32978])

* Previously, the load balancing algorithm did not differentiate between active and inactive services when determining weights, and it employed the `random` algorithm excessively in environments with many inactive services or environments routing backends with weight 0. This led to increased memory usage and a higher risk of excessive memory consumption. With this release, changes are made to optimize traffic direction towards active services only and prevent unnecessary use of the `random` algorithm with higher weights, reducing the potential for excessive memory consumption. (link:https://issues.redhat.com/browse/OCPBUGS-32977[OCPBUGS-32977])

* Previously, If a user created a `ContainerRuntimeConfig` resource as an extra manifest for a single-node {product-title} cluster (SNO) installation, the boostrap process failed with the error: `more than one ContainerRuntimeConfig found that matches MCP labels`. With this release, the incorrect processing of `ContainerRuntimeConfig` resources is fixed, which resolves the issue. (link:https://issues.redhat.com/browse/OCPBUGS-30152[OCPBUGS-30152])

* Previously, the Helm Plugin index view did not display the same number of charts as the Helm CLI if the chart names varied. With this release, the Helm catalog now looks for `charts.openshift.io/name` and `charts.openshift.io/provider` so that all versions are grouped together in a single catalog title. (link:https://issues.redhat.com/browse/OCPBUGS-32716[OCPBUGS-32716])

* Previously, the description of the hosted control plane CLI flag `api-server-address` was unclear. With this release, the description has been updated for clarity and completeness. (link:https://issues.redhat.com/browse/OCPBUGS-25858[OCPBUGS-25858])

[id="ocp-4-15-13-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-12"]
=== RHSA-2024:2664 - {product-title} 4.15.12 bug fix and security update

Issued: 9 May 2024

{product-title} release 4.15.12, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:2664[RHSA-2024:2664] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2024:2669[RHSA-2024:2669] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.12 --pullspecs
----

[id="ocp-4-15-12-enhancements"]
==== Enhancements

The following enhancements are included in this z-stream release:

[id="ocp-4-15-12-apiversion-upgrade-v1beta1"]
===== API version for the ClusterTriggerBinding, TriggerTemplate, and EventListener CRDs upgraded from v1alpha1 to v1beta1

* Previously, the API version for the `ClusterTriggerBinding`, `TriggerTemplate`, and `EventListener` CRDs was `v1alpha1`. With this release, the API version is upgraded to `v1beta1` so that the pipelines plugin supports the latest Pipeline Trigger API version for the `ClusterTriggerBinding`, `TriggerTemplate`, and `EventListener` CRDs. (link:https://issues.redhat.com/browse/OCPBUGS-31445[OCPBUGS-31445])

[id="ocp-4-15-12-pipeline-list-page-improvement"]
===== PipelineRun list view performance improvement

* Previously, in the `PipelineRun` list page, all of the `TaskRun` objects were fetched and separated based on their `PipelineRun` name. With this release, `TaskRun` objects are only fetched for failed and cancelled `PipelineRun` objects, and a caching mechanism is added to fetch the `PipelineRun` and `TaskRun` objects that are associated with the failed and cancelled `PipelineRun` objects. (link:https://issues.redhat.com/browse/OCPBUGS-31799[OCPBUGS-31799])

[id="ocp-4-15-12-installer-handling-new-character"]
===== Installer handles the escaping of the % character

* Previously, if a cluster was installed using a proxy, and the proxy information contained escaped characters in the format `%XX`, the installation failed. With this release, the installer now handles the escaping of the `%` character. (link:https://issues.redhat.com/browse/OCPBUGS-32259[OCPBUGS-32259])

[id="ocp-4-15-12-cfe-evaluation-status"]
===== Cluster Fleet Evaluation status information added to the Machine Config Operator

* Previously, the Machine Config Operator (MCO) did not include the Cluster Fleet Evaluation (CFE) status. With this release, the CFE status information is added to the MCO and available to customers. (link:https://issues.redhat.com/browse/OCPBUGS-32922[OCPBUGS-32922])

[id="ocp-4-15-12-operatorhub-filter-rename-FIPS"]
===== OperatorHub filter renamed from FIPS Mode to Designed for FIPS

* Previously, OperatorHub included a filter named *FIPS Mode*. With this release, that filter is named *Designed for FIPS*. (link:https://issues.redhat.com/browse/OCPBUGS-32933[OCPBUGS-32933])

[id="ocp-4-15-12-bug-fixes"]
==== Bug fixes

* Previously, containers had an incorrect view of the pids limit in their `cgroup` hierarchy and reported as a random number instead of `max`. The containers do not have max PIDs and are limited by the pod PID limit, which is set outside of the container's `cgroup` hierarchy and not visible from within the container. With this release, the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-28926[OCPBUGS-28926])

* Previously, for {product-title} deployments on {rh-openstack-first}, the `MachineSet` object did not correctly apply the value for the `Port Security` parameter. With this release, the `MachineSet` object applies the `port_security_enabled` flag as expected. (link:https://issues.redhat.com/browse/OCPBUGS-30857[OCPBUGS-30857])

* Previously, the installation program erroneously attempted to verify the libvirt network interfaces when an agent-based installation was configured with the `openshift-baremetal-install` binary. With this release, the agent installation method does not require libvirt and this validation is disabled. (link:https://issues.redhat.com/browse/OCPBUGS-30944[OCPBUGS-30944])

* Previously, the `cpuset-configure.sh` script could run before all of the system processes were created. With this release, the script is only triggered to run when CRI-O is initialized and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-31692[OCPBUGS-31692])

* Previously, an incorrect `dnsPolicy` was used for the `konnectivity-agent` daemon set in the data plane. As a result, when CoreDNS was down, `konnectivity-agent` pods on the data plane could not resolve the `proxy-server-address` and could fail the `konnectivity-server` in the control plane. With this release, `konnectivity-agent` uses the host system DNS service to lookup the `proxy-server-address` and no longer depends on CoreDNS. (link:https://issues.redhat.com/browse/OCPBUGS-31826[OCPBUGS-31826])

* Previously, if gathering logs from the bootstrap node failed during the `gather bootstrap` execution, the virtual machine (VM) serial console logs were not included in the gather output even if they were collected. With this release, serial logs are always included if they are collected. (link:https://issues.redhat.com/browse/OCPBUGS-32264[OCPBUGS-32264])

* Previously, port 22 was missing from the compute node's security group in AWS SDK installations, therefore connecting to the compute nodes with SSH failed when users used AWS SDK provisioning. With this release, port 22 is added to the compute node's security group and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-32383[OCPBUGS-32383])

* Previously, the installation program required the `s3:HeadBucket` permission for AWS, even though it does not exist. The correct permission for the `HeadBucket` action is `s3:ListBucket`. With this release, `s3:HeadBucket` is removed from the list of required permissions and only `s3:ListBucket` is required, as expected. (link:https://issues.redhat.com/browse/OCPBUGS-32690[OCPBUGS-32690])

* Previously, there was an issue with {product-title} Ansible upgrades because the IPsec configuration was not idempotent. With this release, changes are made to the {product-title} Ansible playbooks, ensuring that all IPsec configurations are idempotent. (link:https://issues.redhat.com/browse/OCPBUGS-33102[OCPBUGS-33102])

[id="ocp-4-15-12-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-11"]
=== RHSA-2024:2068 - {product-title} 4.15.11 bug fix and security update

Issued: 2 May 2024

{product-title} release 4.15.11, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:2068[RHSA-2024:2068] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2024:2071[RHSA-2024:2071] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.11 --pullspecs
----

[id="ocp-4-15-11-enhancements"]
==== Enhancements

The following enhancements are included in this z-stream release:

[id="ocp-4-15-11-topology-page-node-limit-increase"]
===== Increased the number of supported nodes on the *Topology* view

* Previously, the {product-title} web console *Topology* view could only display a maximum of 100 nodes. If you attempted to view more than 100 nodes, the web console would output a `Loading is taking longer than expected.` error message. With this release, the `MAX_NODES_LIMIT` parameter for the web console is set to `200`, so that the web console can display a maximum of 200 nodes. (link:https://issues.redhat.com/browse/OCPBUGS-32340[OCPBUGS-32340])

[id="ocp-4-15-11-grc-acr-rhel-providers"]
===== Added `gcr` and `acr` {op-system-base} credential providers

* {product-title} {product-version} includes `gcr` and `acr` {op-system-base-full} credential providers so that future upgrades to later versions of {product-title} that require {op-system-base} compute nodes deployed on a cluster do not result in a failed installation. (link:https://issues.redhat.com/browse/OCPBUGS-30970[OCPBUGS-30970])

[id="ocp-4-15-11-featuregates-rbac-dns-operator"]
===== Added permission for reading the `featureGates` resource to RBAC rule

* {product-title} {product-version} adds a permission to the role-based access control (RBAC) rule so that the DNS Operator can read the `featureGates` resource. Without this permission, an upgrade operation to a later version of {product-title} could fail. (link:https://issues.redhat.com/browse/OCPBUGS-32093[OCPBUGS-32093])

[id="ocp-4-15-11-bug-fixes"]
==== Bug fixes

* The installation of {product-title} failed when a performance profile was located in the extra manifests folder and targeted *master* or *worker* node roles. This was caused by the internal installation that processes the performance profile before the default *master* or *worker* node roles were created. With this release, the internal installation processes the performance profile after the node roles are created so that this issue no longer exists. (link:https://issues.redhat.com/browse/OCPBUGS-27948[OCPBUGS-27948])

* Previously, the image registry did not support {aws-first} region `ca-west-1`. With this release, the image registry can now be deployed in this region. (link:https://issues.redhat.com/browse/OCPBUGS-31641[OCPBUGS-31641])

* Previously, a cluster upgraded to {product-title} 4.14 or later experienced router pods unexpectedly closing `keep-alive` connections that caused traffic degradation issues for Apache HTTP clients. This issue was caused by router pods using a version of an HAProxy router that closed idle connections after the HAProxy router was restarted. With this release, the pods use a version of an HAProxy router that includes an `idle-close-on-response` option. The HAProxy router now waits for the last request and response transaction before the idle connection is closed. (link:https://issues.redhat.com/browse/OCPBUGS-32435[OCPBUGS-32435])

* Previously, a Redfish virtual media Hewlett Packard Enterprise (HPE) integrated Lights Out (iLO) 5 bare-metal machine's compression was forcibly disabled to workaround other unrelated issues in different hardware models. This caused the `FirmwareSchema` resource to be missing from each iLO 5 bare-metal machine. Each machine needs compression to fetch message registries from their Redfish Baseboard Management Controller (BMC) endpoints. With this release, each iLO 5 bare-metal machine that needs the `FirmwareSchema` resource does not have compression forcibly disabled. (link:https://issues.redhat.com/browse/OCPBUGS-31686[OCPBUGS-31686])

* Previously, nodes of paused `MachineConfigPools` might have their pause status dropped when performing a cluster update. With this release, nodes of paused `MachineConfigPools` correctly stay paused when performing a cluster update. (link:https://issues.redhat.com/browse/OCPBUGS-31839[OCPBUGS-31839])

* Previously, newer versions of Redfish used Manager resources to deprecate the Uniform Resource Identifier (URI) for the RedFish Virtual Media API. This caused any hardware that used the newer Redfish URI for Virtual Media to not be provisioned. With this release, the Ironic API identifies the correct Redfish URI to deploy for the RedFish Virtual Media API so that hardware relying on either the deprecated or the newer URI could be provisioned.  (link:https://issues.redhat.com/browse/OCPBUGS-31830[OCPBUGS-31830])

* Previously, the Cloud Credential Operator (CCO) checked for a non-existent `s3:HeadBucket` permission during the validation checks in mint mode that resulted in a failed cluster installation. With this release, CCO removes the validation check for this non-existing permission so that validation checks pass in mint mode and the cluster installation does not fail. (link:https://issues.redhat.com/browse/OCPBUGS-31924[OCPBUGS-31924])

* Previously, a new Operator Lifecycle Manager (OLM) Operator that upgraded to {product-title} 4.15.3 resulted in failure because important resources were not injected into the upgrade operation. With this release, these resources are now cached so that newer OLM Operator upgrades can succeed. (link:https://issues.redhat.com/browse/OCPBUGS-32311[OCPBUGS-32311])

* Previously, the Red{nbsp} {product-title} web console did not require the `Creator` field as a mandatory field. API changes specified an empty value for this field, but a user profile could still create silent alerts. With this release, the API marks the `Creator` field as a mandatory field for a user profile that needs to create silent alerts. (link:https://issues.redhat.com/browse/OCPBUGS-32097[OCPBUGS-32097])

* Previously, in hosted control planes for {product-title}, when you created the custom resource definition (CRD) for `ImageDigestMirrorSet` and `ImageContentSourcePolicy` objects at the same time in a disconnected environment, the HyperShift Operator created the object only for the `ImageDigestMirrorSet` CRD, ignoring the `ImageContentSourcePolicy` CRD. With this release, the HyperShift Operator can create objects at the same time for the `ImageDigestMirrorSet` and `ImageContentSourcePolicy` CRDs. (link:https://issues.redhat.com/browse/OCPBUGS-32164[OCPBUGS-32164])

* Previously, IPv6 networking services that operated in {rh-openstack-first} environments could not share an IPv6 load balancer that was configured with multiple services because of an issue that mistakenly marks an IPv6 load balancer as `Internal` to the cluster. With this release, IPv6 load balancers are no longer marked as `Internal` so that an IPv6 load balancer with multiple services can be shared among IPv6 networking services. (link:https://issues.redhat.com/browse/OCPBUGS-32246[OCPBUGS-32246])

* Previously, the control plane machine sets (CPMS) did not allow template names for vSphere in a CPMS definition. With this release, a CPMS Operator fix allows template names for vSphere in the CPMS definition so that this issue no longer persists. (link:https://issues.redhat.com/browse/OCPBUGS-32357[OCPBUGS-32357])

* Previously, the control plane machine sets (CPMS) Operator was not correctly handling older {product-title} version configurations that had a vSphere definition in the infrastructure custom resource. This would cause cluster upgrade operations to fail and the CPMS Operator to remain in a `crashloopback` state. With this release, the cluster upgrade operations do not fail because of this issue. (link:https://issues.redhat.com/browse/OCPBUGS-32414[OCPBUGS-32414])

* Previously, the image registry's Azure path fix job incorrectly required the presence of `AZURE_CLIENT_ID` and `TENANT_CLIENT_ID` parameters to function. This caused a valid configuration to throw an error message. With this release, adds a check to the Identity and Access Management (IAM) service account key to validate if these parameters are needed, so that a cluster upgrade operation no longer fails. (link:https://issues.redhat.com/browse/OCPBUGS-32396[OCPBUGS-32396])

* Previously, a build pod that failed because of a memory limitation would have its pod status changed to `Error` instead of `OOMKilled`. This caused these pods to not be reported correctly. The issue would only occur on cgroup v2 nodes. With this release, a pod with a status of `OOMKilled` is correctly detected and reported. (link:https://issues.redhat.com/browse/OCPBUGS-32498[OCPBUGS-32498])

[id="ocp-4-15-11-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].


[id="ocp-4-15-10"]
=== RHSA-2024:1887 - {product-title} 4.15.10 bug fix and security update

Issued: 26 April 2024

{product-title} release 4.15.10, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:1887[RHSA-2024:1887] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2024:1892[RHSA-2024:1892] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.10 --pullspecs
----

[id="ocp-4-15-10-bug-fixes"]
==== Bug fixes

* Previously, clusters created before {product-title} 4.7 will have had signer keys for api-int endpoint updated unexpectedly when users upgraded to {product-title} 4.15 due to the installer deleting the SecretTypeTLS and then recreating the secret with `kubernetes.io/tls` type.  With this release, the issue is resolved by the installer changing the secret type without deleting the secret. (link:https://issues.redhat.com/browse/OCPBUGS-31807[OCPBUGS-31807])

* Previously, when users imported image stream tags, `ImageContentSourcePolicy` (ICSP) was not allowed to co-exist with `ImageDigestMirrorSet` (IDMS) and `ImageTagMirrorSet` (ITMS). {product-title} ignored any IDMS/ITMS created by the user and favored ICSP. With this release, they are allowed to co-exist since importing image stream tags will now respect IDMS/ITMS when ICSP is also present. (link:https://issues.redhat.com/browse/OCPBUGS-31469[OCPBUGS-31469])

* Previously, Terraform would create the compute server group with the policy set for the control plane. As a consequence, the 'serverGroupPolicy' property of the `install-config.yaml` file was ignored for the compute server group. With this release, the server group policy set in the `install-config.yaml` file for the compute MachinePool is correctly applied at install-time in the Terraform flow.  (link:https://issues.redhat.com/browse/OCPBUGS-31335[OCPBUGS-31335])

* Previously, projects that specified a non-intersecting openshift.io/node-selector project selector with pods `.spec.nodeName` could cause runaway Pod creation in Deployments. With this release, pods with non-intersecting `.spec.nodeName` are not admitted by the API server which resolves the issue. (link:https://issues.redhat.com/browse/OCPBUGS-29922[OCPBUGS-29922])

* Previously, a remote attacker with basic login credentials could check the pod manifest to discover a repository pull secret. With this release, the vulnerability has been fixed. (link:https://issues.redhat.com/browse/OCPBUGS-28769[OCPBUGS-28769])

[id="ocp-4-15-10-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-9"]
=== RHSA-2024:1770 - {product-title} 4.15.9 bug fix and security update

Issued: 16 April 2024

{product-title} release 4.15.9, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:1770[RHSA-2024:1770] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2024:1773[RHBA-2024:1773] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.9 --pullspecs
----

[id="ocp-4-15-6-enhancements"]
==== Enhancements

The following enhancements are included in this z-stream release:

[id="ocp-4-15-9-validate-control-plane-replicas"]
===== Number of configured control plane replicas validated

* Previously, the number of control plane replicas could be set to an invalid value, such as 2. With this release, a validation is added to prevent any misconfiguration of the control plane replicas at the ISO generation time. (link:https://issues.redhat.com/browse/OCPBUGS-30822[OCPBUGS-30822])

[id="ocp-4-15-9-bug-fixes"]
==== Bug fixes

* Previously, saving kdump logs to an SSH target was failing in Open Virtual Network (OVN) deployments. The kdump crash logs were not created to the SSH remote when OVN was configured. With this release, OVS-configurations are no longer run before kdump. (link:https://issues.redhat.com/browse/OCPBUGS-30884[OCPBUGS-30884])

* Previously, the `coreos-installer` CLI tool did not correctly modify, reset, or show the kernel arguments for an ISO generated by the `openshift-install agent create image` command. With this release, the `coreos-installer iso kargs modify <iso>`, `coreos-installer iso kargs reset <iso>`, and `coreos-installer iso kargs show <iso>` commands all work as expected. (link:https://issues.redhat.com/browse/OCPBUGS-30922[OCPBUGS-30922])

* Previously, the services secondary IP family test was failing with dual-stack clusters. With this release, the 30000:32767 traffic range is enabled and the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-31284[OCPBUGS-31284])

[id="ocp-4-15-9-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.8
[id="ocp-4-15-8"]
=== RHSA-2024:1668 - {product-title} 4.15.8 bug fix and security update

Issued: 8 April 2024

{product-title} release 4.15.8, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:1668[RHSA-2024:1668] advisory. There are no RPM packages for this update.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.8 --pullspecs
----

[id="ocp-4-15-8-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

// 4.15.7 was skipped.
[id="ocp-4-15-6"]
=== RHSA-2024:1559 - {product-title} 4.15.6 bug fix and security update

Issued: 2 April 2024

{product-title} release 4.15.6, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:1559[RHSA-2024:1559] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2024:1563[RHSA-2024:1563] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.6 --pullspecs
----

[id="ocp-4-15-6-known-issues"]
==== Known issue

* There is a known issue in this release which causes the `oc-mirror` binary to fail on Red Hat Enterprise Linux (RHEL) 8 systems.
Workaround: Use the Red Hat OpenShift Container Platform 4.15.5 `oc-mirror` binary or extract `oc-mirror.rhel8`. (link:https://issues.redhat.com/browse/OCPBUGS-31609[OCPBUGS-31609])

[id="ocp-4-15-6-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-5"]
=== RHSA-2024:1449 - {product-title} 4.15.5 bug fix and security update

Issued: 27 March 2024

{product-title} release 4.15.5, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:1449[RHSA-2024:1449] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2024:1452[RHBA-2024:1452] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.5 --pullspecs
----

[id="ocp-4-15-5-bug-fixes"]
==== Bug fixes

* Previously, the OpenShift Installer could fail to retrieve instance type information from {azure-full} in the allotted time, even when the type existed when verified with the {azure-short} CLI. With this release, the timeout duration has increased to wait for an {azure-short} response, and the error message includes the correct reason for the failure. (link:https://issues.redhat.com/browse/OCPBUGS-29964[OCPBUGS-29964])

* Previously, when creating clusters through OpenShift Cluster Manager (OCM) using the Hive provisioner, which uses OpenShift Installer, the installer failed to delete AWS IAM instance profiles after deleting the cluster. This issue led to an accumulation of instance profiles. With this release, the installer tags the instance profiles and deletes the appropriately tagged profiles. (link:https://issues.redhat.com/browse/OCPBUGS-18986[OCPBUGS-18986])

[id="ocp-4-15-5-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-3"]
=== RHSA-2024:1255 - {product-title} 4.15.3 bug fix and security update

Issued: 19 March 2024

{product-title} release 4.15.3, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:1255[RHSA-2024:1255] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2024:1258[RHBA-2024:1258] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.3 --pullspecs
----

[id="ocp-4-15-3-bug-fixes"]
==== Bug fixes

* Previously, if the root credentials were removed from a Google Cloud Platform (GCP) cluster that was in mint mode, the Cloud Credential Operator (CCO) would go into a degraded state after approximately 1 hour. This issue means that CCO could not manage the credentials root secret for a component. With this update, mint mode supports custom roles, so that removing root credentials from a GCP cluster does not cause the CCO to go into a degraded state. (link:https://issues.redhat.com/browse/OCPBUGS-30412[OCPBUGS-30412])

[id="ocp-4-15-3-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].

[id="ocp-4-15-2"]
=== RHSA-2024:1210 - {product-title} 4.15.2 bug fix and security update

Issued: 13 March 2024

{product-title} release {product-version}.2, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:1210[RHSA-2024:1210] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2024:1213[RHBA-2024:1213] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.2 --pullspecs
----

[id="ocp-4-15-2-known-issues"]
==== Known issues

* Providing a performance profile as an extra manifest at Day 0 did not work in {product-title} 4.15.0, but it is now possible in 4.15.2 with the following limitation:
+
The installation of {product-title} might fail when a performance profile is present in the extra manifests folder and targets the primary or worker pools. This is caused by the internal installation ordering that processes the performance profile before the default primary and worker `MachineConfigPools` are created. It is possible to workaround this issue by including a copy of the stock primary or worker `MachineConfigPools` in the extra manifests folder. (link:https://issues.redhat.com/browse/OCPBUGS-27948[OCPBUGS-27948], link:https://issues.redhat.com/browse/OCPBUGS-29752[OCPBUGS-29752])

[id="ocp-4-15-2-bug-fixes"]
==== Bug fixes

* Previously, when updating to {product-title} 4.15, `CatalogSource` objects never refreshed, which caused the optional Operator catalogs to fail to update. With this release, the image pull policy is changed to `Always`, which enables the optional Operator catalogs to update correctly. (link:https://issues.redhat.com/browse/OCPBUGS-30193[OCPBUGS-30193])

* Previously, the `nodeStatusReportFrequency` setting was linked to the `nodeStatusUpdateFrequency` setting. With this release, the `nodeStatusReportFrequency` setting is set to 5 minutes. (link:https://issues.redhat.com/browse/OCPBUGS-29797[OCPBUGS-29797])

* Previously, under certain conditions, the installer would fail with the error message `unexpected end of JSON input`. With this release, the error message is clarified and suggests users set the `serviceAccount` field in the `install-config.yaml` file to fix the issue. (link:https://issues.redhat.com/browse/OCPBUGS-29495[OCPBUGS-29495])

* Previously, the `oauthMetadata` property provided in the `HostedCluster` object was not honored. With this release, the `oauthMetadata` property is honored by the `HostedCluster` object. (link:https://issues.redhat.com/browse/OCPBUGS-29025[OCPBUGS-29025])

[id="ocp-4-15-2-updating"]
==== Updating
To update an {product-title} 4.15 cluster to this latest release, see xref:../updating/updating_a_cluster/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster by using the CLI].
