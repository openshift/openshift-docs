:_mod-docs-content-type: ASSEMBLY
[id="ocp-4-15-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-15-about-this-release"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-2023:7198[RHSA-2023:7198]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.28.md[Kubernetes 1.28] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. With the {cluster-manager-first} application for {product-title}, you can deploy {product-title} clusters to either on-premises or cloud environments.

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.8 and 8.9, as well as on {op-system-first} 4.15.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//TODO: Add this for 4.14
Starting with {product-title} 4.12, an additional six months is added to the Extended Update Support (EUS) phase on even numbered releases from 18 months to two years. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

Starting with {product-title} 4.14, Extended Update Support (EUS) is extended to 64-bit ARM, {ibm-power-name} (ppc64le), and {ibm-z-name} (s390x) platforms. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

//TODO: Add the line below for EUS releases and comment it out for non-EUS releases.
// {product-title} {product-version} is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

//TODO: The line below should be used when it is next appropriate. Revisit in August 2023 time frame.
Maintenance support ends for version 4.12 on 25 January 2025 and goes to extended life phase. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

Commencing with the {product-version} release, Red Hat is simplifying the administration and management of Red Hat shipped cluster Operators with the introduction of three new life cycle classifications; Platform Aligned, Platform Agnostic, and Rolling Stream. These life cycle classifications provide additional ease and transparency for cluster administrators to understand the life cycle policies of each Operator and form cluster maintenance and upgrade plans with predictable support boundaries. For more information, see link:https://access.redhat.com/webassets/avalon/j/includes/session/scribe/?redirectTo=https%3A%2F%2Faccess.redhat.com%2Fsupport%2Fpolicy%2Fupdates%2Fopenshift_operators[OpenShift Operator Life Cycles].

// Added in 4.14. Language came directly from Kirsten Newcomer.
{product-title} is designed for FIPS. When running {op-system-base-full} or {op-system-first} booted in FIPS mode, {product-title} core components use the {op-system-base} cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the `x86_64`, `ppc64le`, and `s390x` architectures.

For more information about the NIST validation program, see link:https://csrc.nist.gov/Projects/cryptographic-module-validation-program/validated-modules[Cryptographic Module Validation Program]. For the latest NIST status for the individual versions of {op-system-base} cryptographic libraries that have been submitted for validation, see link:https://access.redhat.com/articles/2918071#fips-140-2-and-fips-140-3-2[Compliance Activities and Government Standards].

[id="ocp-4-15-add-on-support-status"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-15-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-15-rhcos"]
=== {op-system-first}

[id="ocp-4-15-installation-and-update"]
=== Installation and update

[id="ocp-4-15-CAPO-tech-preview-no-upgrade"]
==== CAPO integration into the cluster CAPI Operator (Tech Preview)

If you enable the `TechPreviewNoUpgrade` feature flag, the Cluster API (CAPI) Operator deploys the Cluster API Provider for OpenStack (CAPO) and manages its lifecycle. The CAPI Operator automatically creates `Cluster` and `OpenStackCluster` resources for the current {product-title} cluster.

It is now possible to configure the CAPI `Machine` and CAPO `OpenStackMachine` resources in similar fashion to how Machine API (MAPI) resources are configured. It is important to note that the CAPI resources are equivalent to MAPI resources but not identical.

[id="ocp-4-15-installation-and-update-ibm-cloud-user-managed-encryption"]
==== IBM Cloud and user-managed encryption

You can now specify your own {ibm-name} Key Protect for {ibm-cloud-name} root key as part of the installation process. This root key is used to encrypt the root (boot) volume of control plane and compute machines, as well as the persistent volumes (data volumes) that are provisioned after the cluster is deployed.

For more information, see xref:../installing/installing_ibm_cloud_public/user-managed-encryption-ibm-cloud.adoc#user-managed-encryption-ibm-cloud[User-managed encryption for IBM Cloud].

[id="ocp-4-15-installation-and-update-ibm-cloud-restricted-install"]
==== Installing a cluster on IBM Cloud with limited internet access

You can now install a cluster on {ibm-cloud-name} in an environment with limited internet access, such as a disconnected or restricted network cluster. With this type of installation, you create a registry that mirrors the contents of the {product-title} installation images. You can create this registry on a mirror host, which can access both the internet and your restricted network.

For more information, see xref:../installing/installing_ibm_cloud_public/installing-ibm-cloud-restricted.adoc#installing-ibm-cloud-restricted[Installing a cluster on IBM Cloud in a restricted network].

[id="ocp-4-15-installation-and-update-nutanix-failure-domains"]
==== Nutanix and fault tolerant deployments

By default, the installation program installs control plane and compute machines into a single Nutanix Prism Element (cluster). To improve the fault tolerance of your {product-title} cluster, you can now specify that these machines be distributed across multiple Nutanix clusters by configuring failure domains.

For more information, see xref:../installing/installing_nutanix/nutanix-failure-domains.adoc#nutanix-failure-domains[Fault tolerant deployments using multiple Prism Elements].

[id="ocp-4-15-installation-and-update-OCP-on-ARM"]

==== {product-title} on 64-bit ARM

{product-title} {product-version} now supports the ability to enable 64k page sizes in the {op-system} kernel using the Machine Config Operator (MCO). This setting is exclusive to machines with 64-bit ARM architectures. For more information, see the xref:../post_installation_configuration/machine-configuration-tasks.adoc#machine-configuration-tasks[Machine configuration tasks] documentation.

[id="ocp-4-15-installation-and-update-optional-olm"]
==== Optional OLM cluster capability
In {product-title} 4.15, you can disable the Operator Lifecycle Manager (OLM) capability during installation. For further information, see xref:../installing/cluster-capabilities.adoc#olm-overview_cluster-capabilities[Operator Lifecycle Manager capability].

[id="ocp-4-15-deploying-osp-with-root-volume-etcd-on-local-disk"]
==== Deploying {rh-openstack-first} with root volume and etcd on local disk (Technology Preview)

You can now move etcd from a root volume (Cinder) to a dedicated ephemeral local disk as a day 2 deployment. With this Technology Preview feature, you can resolve and prevent performance issues of your {rh-openstack} installation.

//For more information, see < insert link to Deploying OpenStack with rootVolume and etcd on local disk when docs merge>.

[id="ocp-4-15-web-console"]
=== Web console

[id="ocp-4-15-openshift-cli"]
=== OpenShift CLI (oc)

[id="ocp-4-15-ibm-z"]
=== {ibm-z-title} and {ibm-linuxone-title}

[id="ocp-4-15-ibm-power"]
=== {ibm-power-title}

[discrete]
==== {ibm-power-title} notable enhancements

[discrete]
=== {ibm-power-title}, {ibm-z-title}, and {ibm-linuxone-title} support matrix

.{product-title} features
[cols="3,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Alternate authentication providers
|Supported
|Supported

|Automatic Device Discovery with Local Storage Operator
|Unsupported
|Supported

|Automatic repair of damaged machines with machine health checking
|Unsupported
|Unsupported

|Cloud controller manager for {ibm-cloud-name}
|Supported
|Unsupported

|Controlling overcommit and managing container density on nodes
|Unsupported
|Unsupported

|Cron jobs
|Supported
|Supported

|Descheduler
|Supported
|Supported

|Egress IP
|Supported
|Supported

|Encrypting data stored in etcd
|Supported
|Supported

|FIPS cryptography
|Supported
|Supported

|Helm
|Supported
|Supported

|Horizontal pod autoscaling
|Supported
|Supported

|IBM Secure Execution
|Unsupported
|Supported

|{ibm-power-server-name} Block CSI Driver Operator (Technology Preview)
|Supported
|Unsupported

|Installer-provisioned Infrastructure Enablement for {ibm-power-server-name} (Technology Preview)
|Supported
|Unsupported

|Installing on a single node
|Supported
|Supported

|IPv6
|Supported
|Supported

|Monitoring for user-defined projects
|Supported
|Supported

|Multi-architecture compute nodes
|Supported
|Supported

|Multipathing
|Supported
|Supported

|Network-Bound Disk Encryption - External Tang Server
|Supported
|Supported

|Non--volatile memory express drives (NVMe)
|Supported
|Unsupported

|oc-mirror plugin
|Supported
|Supported

|OpenShift CLI (`oc`) plugins
|Supported
|Supported

|Operator API
|Supported
|Supported

|OpenShift Virtualization
|Unsupported
|Unsupported

|OVN-Kubernetes, including IPsec encryption
|Supported
|Supported

|PodDisruptionBudget
|Supported
|Supported

|Precision Time Protocol (PTP) hardware
|Unsupported
|Unsupported

|{openshift-local-productname}
|Unsupported
|Unsupported

|Scheduler profiles
|Supported
|Supported

|Stream Control Transmission Protocol (SCTP)
|Supported
|Supported

|Support for multiple network interfaces
|Supported
|Supported

|Three-node cluster support
|Supported
|Supported

|Topology Manager
|Supported
|Unsupported

|z/VM Emulated FBA devices on SCSI disks
|Unsupported
|Supported

|4K FCP block device
|Supported
|Supported
|====

.Persistent storage options
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}
|Persistent storage using iSCSI
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using local volumes (LSO)
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using hostPath
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Fibre Channel
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Raw Block
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using EDEV/FBA
|Supported ^[1]^
|Supported ^[1]^,^[2]^
|====
[.small]
--
1. Persistent shared storage must be provisioned by using either {rh-storage-first} or other supported storage protocols.
2. Persistent non-shared storage must be provisioned by using local storage, such as iSCSI, FC, or by using LSO with DASD, FCP, or EDEV/FBA.
--

.Operators
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Cluster Logging Operator
|Supported
|Supported

|Cluster Resource Override Operator
|Supported
|Supported

|Compliance Operator
|Supported
|Supported

|File Integrity Operator
|Supported
|Supported

|HyperShift Operator
|Technology Preview
|Technology Preview

|Local Storage Operator
|Supported
|Supported

|MetalLB Operator
|Supported
|Supported

|NFD Operator
|Supported
|Supported

|NMState Operator
|Supported
|Supported

|OpenShift Elasticsearch Operator
|Supported
|Supported

|Service Binding Operator
|Supported
|Supported

|Vertical Pod Autoscaler Operator
|Supported
|Supported
|====

.Multus CNI plugins
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Bridge
|Supported
|Supported

|Host-device
|Supported
|Supported

|IPAM
|Supported
|Supported

|IPVLAN
|Supported
|Supported
|====

.CSI Volumes
[cols="2,1,1",options="header"]
|====
|Feature |{ibm-power-name} |{ibm-z-name} and {ibm-linuxone-name}

|Cloning
|Supported
|Supported

|Expansion
|Supported
|Supported

|Snapshot
|Supported
|Supported
|====

[id="ocp-4-15-auth"]
=== Authentication and authorization

[id="ocp-4-15-networking"]
=== Networking

[id="ocp-4-15-ipv6-default-macvlan"]
==== IPv6 unsolicited neighbor advertisements now default on macvlan CNI plugin

Previously, if one pod (`Pod X`) was deleted, and a second pod (`Pod Y`) was created with a similar configuration, `Pod Y` might have had the same IPv6 address as `Pod X`, but it would have a different MAC address.  In this scenario, the router was unaware of the MAC address address change, and it would continue sending traffic to the MAC address for `Pod X`.

With this update, pods created using the macvlan CNI plugin, where the IP address management CNI plugin has assigned IPs, now send IPv6 unsolicited neighbor advertisements by default onto the network. This enhancement notifies the network fabric of the new pod's MAC address for a particular IP to refresh IPv6 neighbor caches.


[id="ocp-4-15-configuring-whereabouts-IP-reconciler-schedule"]
==== Configuring the Whereabouts IP reconciler schedule

The Whereabouts reconciliation schedule was hard-coded to run once per day and could not be reconfigured. With this release, a `ConfigMap` object has enabled the configuration of the Whereabouts cron schedule. For more information, see xref:../networking/multiple_networks/configuring-additional-network.adoc#nw-multus-configuring-whereabouts-ip-reconciler-schedule_configuring-additional-network[Configuring the Whereabouts IP reconciler schedule].

[id="ocp-4-15-egressfirewall-updates-status-management"]
==== Status management updates for EgressFirewall and AdminPolicyBasedExternalRoute CR

The following updates have been made to the status management of `EgressFirewall` and `AdminPolicyBasedExternalRoute` custom resource policy:

** The `status.status` field is set to `failure` if at least one message reports `failure`.

** The `status.status` field is empty if no failures are reported and not all nodes have reported their status.

** The `status.status` field is set to `success` if all nodes report `success`.

** The `status.mesages` field lists messages. The messages are listed by the node name by default and are prefixed with the node name.

[id="ocp-4-15-metallb-metrics"]
==== Additional BGP metrics for MetalLB

With this update, MetalLB exposes additional metrics relating to communication between MetalLB and Border Gateway Protocol (BGP) peers. For more information, see xref:../networking/metallb/metallb-troubleshoot-support.adoc#nw-metallb-metrics_metallb-troubleshoot-support[MetalLB metrics for BGP and BFD].

[id="ocp-4-15-all-multicast"]
==== Supporting all-multicast mode

{product-title} now supports configuring the all-multicast mode by using the tuning CNI plugin. This update eliminates the need to grant the `NET_ADMIN` capability to the pod's Security Context Constraints (SCC), enhancing security by minimizing potential vulnerabilities for your pods.

For more information about all-multicast mode, see xref:../networking/hardware_networks/configuring-interface-sysctl-sriov-device.adoc#nw-about-all-multi-cast-mode_configuring-sysctl-interface-sriov-device[About all-multicast mode].

[id="ocp-4-15-multi-network-policy-ipv6"]
==== Multi-network policy support for IPv6 networks
With this update, you can now create multi-network policies for IPv6 networks. For more information, see xref:../networking/multiple_networks/configuring-multi-network-policy.adoc#nw-multi-network-policy-ipv6-support_configuring-multi-network-policy[Supporting multi-network policies in IPv6 networks].

[id="ocp-4-15-ingress-dashboards"]
==== Ingress Operator metrics dashboard available
With this release, Ingress networking metrics are now viewable from within the {product-title} web console. See xref:../networking/networking-dashboards.adoc#ingress-dashboards[Ingress Operator dashboard] for more information.

[id="ocp-4-15-CoreDNS-filtration-externalname-service-queries"]
==== CoreDNS filtration of ExternalName service queries for subdomains
As of {product-title} {product-version}, CoreDNS has been updated from 1.10.1 to 1.11.1.

This update to CoreDNS resolved an issue where CoreDNS would incorrectly provide a response to a query for an `ExternalName` service that shared its name with a top-level domain, such as `com` or `org`. A query for subdomains of an external service should not resolve to that external service. See the associated link:https://github.com/coredns/coredns/pull/6162[CoreDNS GitHub issue] for more information.

[id="ocp-4-15-CoreDNS-metrics-deprecation-and-removal"]
==== CoreDNS metrics deprecation and removal
As of {product-title} {product-version}, CoreDNS has been updated from 1.10.1 to 1.11.1.

This update to CoreDNS resulted in the deprecation and removal of certain metrics that have been relocated, including the metrics `coredns_forward_healthcheck_failures_total`, `coredns_forward_requests_total`, `coredns_forward_responses_total`, and `coredns_forward_request_duration_seconds`. See link:https://coredns.io/plugins/forward/#metrics[CoreDNS Metrics] for more information.

[id="ocp-4-15-registry"]
=== Registry

[id="ocp-4-15-private-storage-endpoint-azure"]
==== Support for private storage endpoint on {azure-short}

With this release, the Image Registry Operator can be leveraged to use private storage endpoints on {azure-short}. You can use this feature to seamlessly configure private endpoints for storage accounts when {product-title} is deployed on private {azure-short} clusters, so that users can deploy the image registry without exposing public-facing storage endpoints.

For more information, see the following sections:

* xref:../post_installation_configuration/configuring-private-cluster.adoc#registry-configuring-private-storage-endpoint-azure_configuring-private-cluster[Configuring a private storage endpoint on Azure]
* xref:../installing/installing_azure/installing-azure-private.adoc#installing-private-image-registry-private-azure[Optional: Preparing a private Microsoft Azure cluster for a private image registry]

[id="ocp-4-15-storage"]
=== Storage

[id="ocp-4.15-storage-recovering-vgs-from-prev-installation"]
==== Recovering volume groups from the previous {lvms} installation
With this release, the `LVMCluster` custom resource (CR) provides support for recovering volume groups from the previous {lvms} installation. If the `deviceClasses.name` field is set to the name of a volume group from the previous {lvms} installation, {lvms} recreates the resources related to that volume group in the current {lvms} installation. This simplifies the process of using devices from the previous {lvms} installation through the reinstallation of {lvms}.

For more information, see xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#lvms-creating-lvms-cluster_logical-volume-manager-storage[Creating a Logical Volume Manager cluster on a worker node].

[id="ocp-4.15-storage-support-for-wiping-the-devices"]
==== Support for wiping the devices in {lvms}

This feature provides a new optional field `forceWipeDevicesAndDestroyAllData` in the `LVMCluster` custom resource (CR) to force wipe the selected devices. Prior to this release, wiping the devices required you to manually access the host. With this release, you can force wipe the disks without manual intervention. This simplifies the process of wiping the disks.
[WARNING]
====
If `forceWipeDevicesAndDestroyAllData` is set to `true`, {lvms} wipes all previous data on the devices. You must use this feature with caution.
====

For more information, see xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#lvms-creating-lvms-cluster_logical-volume-manager-storage[Creating a Logical Volume Manager cluster on a worker node].

[id="ocp-4.15-storage-support-for-lvms-on-multi-node-clusters"]
==== Support for deploying {lvms} on multi-node clusters

This feature provides support for deploying {lvms} on multi-node clusters. Prior to this release, {lvms} only supported single-node configurations. With this release, {lvms} supports all of the {product-title} deployment topologies. This enables provisioning of local storage on multi-node clusters.
[WARNING]
====
{lvms} only supports node local storage on multi-node clusters. It does not support storage data replication mechanism across nodes. When using {lvms} on multi-node clusters, you must ensure storage data replication through active or passive replication mechanisms to avoid a single point of failure.
====

For more information, see xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#lvms-preface-sno-ran_logical-volume-manager-storage[Deploying {lvms}].

[id="ocp-4.15-storage-support-for-integrating-raid-arrays-with-lvms"]
==== Integrating RAID arrays with {lvms}

This feature provides support for integrating RAID arrays that are created using the `mdadm` utility with {lvms}.  The `LVMCluster` custom resource (CR) provides support for adding paths to the RAID arrays in the `deviceSelector.paths` field and the `deviceSelector.optionalPaths` field.

For more information, see xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#lvms-integrating-software-raid-arrays_logical-volume-manager-storage[Integrating software RAID arrays with LVM Storage].

[id="ocp-4.14-storage-fips-conpliance-support-for-lvms"]
==== FIPS compliance support for {lvms}

With this release, {lvms} is designed for Federal Information Processing Standards (FIPS). When {lvms} is installed on {product-title} in FIPS mode, {lvms} uses the RHEL cryptographic libraries that have been submitted to NIST for FIPS 140-3 validation only on the x86_64 architecture.

[id="ocp-4-15-storage-retro-sc-assignment"]
==== Retroactive default StorageClass assignment is generally available
Prior to {product-title} 4.13, if there was no default storage class, persistent volumes claims (PVCs) that were created that requested the default storage class remained stranded in the pending state indefinitely, unless you manually delete and recreate them. Starting with {product-title} 4.14, as a Technology Preview feature, the default storage class is assigned to these PVCs retroactively so that they do not remain in the pending state. After a default storage class is created, or one of the existing storage classes is declared the default, these previously stranded PVCs are assigned to the default storage class. This feature is now generally available.

For more information, see xref:..//storage/container_storage_interface/persistent-storage-csi-sc-manage.adoc#absent-default-storage-class[Absent default storage class].

[id="ocp-4-15-storage-lso-erase-lv-option"]
==== Local Storage Operator option to facilitate removing existing data on local volumes is generally available
This feature provides an optional field, `forceWipeDevicesAndDestroyAllData` defining whether or not to call `wipefs`, which removes partition table signatures (magic strings) making the disk ready to use for Local Storage Operator (LSO) provisioning. No other data besides signatures is erased. This feature is now generally available. Note that this feature does not apply to `LocalVolumeSet` (LVS).

For more information, see xref:..//storage/persistent_storage/persistent_storage_local/persistent-storage-local.adoc#local-volume-cr_persistent-storage-local[Provisioning local volumes by using the Local Storage Operator].

[id="ocp-4-15-storage-non-graceful-node-shutdown"]
==== Detach CSI volumes after non-graceful node shutdown is generally available
Starting with {product-title} 4.13, Container Storage Interface (CSI) drivers can automatically detach volumes when a node goes down non-gracefully as a Technology Preview feature. When a non-graceful node shutdown occurs, you can then manually add an out-of-service taint on the node to allow volumes to automatically detach from the node. This feature is now generally available.

For more information, see xref:..//storage/container_storage_interface/persistent-storage-csi-vol-detach-non-graceful-shutdown.adoc[Detach CSI volumes after non-graceful node shutdown].

[id="ocp-4-15-storage-gcp-filestore-shared-vpc"]
==== Shared VPC is supported for the GCP Filestore CSI Driver Operator as generally available
Shared virtual private cloud (VPC) for the Google Compute Platform (GCP) Container Storage Interface (CSI) Driver Operator is now supported as a generally available feature. Shared VPC simplifies network management, allows consistent network policies, and provides a centralized view of network resources.

For more information, see xref:..//storage/container_storage_interface/persistent-storage-csi-google-cloud-file.adoc#persistent-storage-csi-google-cloud-file-create-sc_persistent-storage-csi-google-cloud-file[Creating a storage class for GCP Filestore Storage].

[id="ocp-4-15-storage-ibm-vpc-byok"]
==== User-Managed encryption supports IBM VPC Block storage as generally available
The user-managed encryption feature allows you to provide keys during installation that encrypt {product-title} node root volumes, and enables all managed storage classes to use the specified encryption key to encrypt provisioned storage volumes. This feature was introduced in {product-title} 4.13 for Google Cloud Platform (GCP) persistent disk (PD) storage, Microsoft Azure Disk, and Amazon Web Services (AWS) Elastic Block storage (EBS), and is now supported on IBM Virtual Private Cloud (VPC) Block storage.

//TODO: For more information, see xref:..//storage/container_storage_interface/persistent-storage-csi-ibm-vpc-block.adoc#byok_persistent-storage-csi-ibm-vpc-block [User-managed encryption].

[id="ocp-4-15-storage-selinux-relabling-mount-options"]
==== SELinux relabeling using mount options (Technology Preview)
Previously, when SELinux was enabled, the persistent volume's (PV's) files were relabeled when attaching the PV to the pod, potentially causing timeouts when the PVs contained a lot of files, as well as overloading the storage backend.

In {product-title} 4.15, for Container Storage Interface (CSI) driver that support this feature, the driver will mount the volume directly with the correct SELinux labels, eliminating the need to recursively relabel the volume, and pod startup can be significantly faster.

This feature is supported with Technology Preview status.

If the following conditions are true, the feature is enabled by default:

* The CSI driver that provides the volume has support for this feature with `seLinuxMountSupported: true` in its CSIDriver instance. The following CSI drivers that are shipped as part of {product-title} announce SELinux mount support:

** AWS Elastic Block Storage (EBS)
** Azure Disk
** Google Compute Platform (GCP) persistent disk (PD)
** IBM Virtual Private Cloud (VPC) Block
** OpenStack Cinder
** VMware vSphere

* The pod that uses the persistent volume has full SELinux label specified in its `spec.securityContext` or `spec.containers[*].securityContext` by using `restricted` SCC.

* Access  mode set to `ReadWriteOncePod` for the volume.

[id="ocp-4-15-oci"]
=== Oracle(R) Cloud Infrastructure

[id="ocp-4-15-olm"]
=== Operator lifecycle

[id="ocp-4-15-osdk"]
=== Operator development

[id="ocp-4-15-builds"]
=== Builds

[id="ocp-4-15-machine-config-operator"]
=== Machine Config Operator

[id="ocp-4-15-mco-mcn-status"]
==== Improved MCO state reporting by node (Technology Preview)

With this release, you can monitor updates for individual nodes as a Technology Preview. For more information, see xref:../post_installation_configuration/machine-configuration-tasks.adoc#checking-mco-node-status[Checking machine config node status].

[id="ocp-4-15-machine-api"]
=== Machine API

[id="ocp-4-15-nodes"]
=== Nodes

[id="ocp-4-15-nodes-icsp-idms-compatibility"]
==== ICSP, IDMS, and ITMS are now compatible

`ImageContentSourcePolicy` (ICSP), `ImageDigestMirrorSet` (IDMS), and `ImageTagMirrorSet` (ITMS) objects now function in the same cluster at the same time. Previously, to use the newer IDMS or ITMS objects, you needed to delete any ICSP objects. Now, you can use any or all of the three types of objects to configure repository mirroring after the cluster is installed. For more information, see xref:../post_installation_configuration/preparing-for-users.html#images-configuration-registry-mirror_post-install-preparing-for-users[Understanding image registry repository mirroring].

[IMPORTANT]
====
Using an ICSP object to configure repository mirroring is a deprecated feature. Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.
====

[id="ocp-4-15-monitoring"]
=== Monitoring

The in-cluster monitoring stack for this release includes the following new and modified features.

[id="ocp-4-15-monitoring-updates-to-monitoring-stack-components-and-dependencies"]
==== Updates to monitoring stack components and dependencies
This release includes the following version updates for in-cluster monitoring stack components and dependencies:

* Alertmanager to 0.26.0
* kube-state-metrics to 2.10.1
* node-exporter to 1.7.0
* Prometheus to 2.48.0
* Prometheus Adapter to 0.11.2
* Prometheus Operator to 0.70.0
* Thanos Querier to 0.32.5

[id="ocp-4-15-monitoring-changes-to-alerting-rules"]
==== Changes to alerting rules

[NOTE]
====
Red{nbsp}Hat does not guarantee backward compatibility for recording rules or alerting rules.
====

* The `NodeClockNotSynchronising` and `NodeClockSkewDetected` alerting rules are now disabled when the Precision Time Protocol (PTP) is in use.

[id="ocp-4-15-monitoring-new-metrics-server-to-access-metrics-API-technology-preview"]
==== New Metrics Server component to access the Metrics API (Technology Preview)
This release introduces a Technology Preview option to add a Metrics Server component to the in-cluster monitoring stack.
As a Technology Preview feature, Metrics Server is automatically installed instead of Prometheus Adapter if the `FeatureGate` custom resource is configured with the `TechPreviewNoUpgrade` option.
If installed, Metrics Server collects resource metrics and exposes them in the `metrics.k8s.io` Metrics API service for use by other tools and APIs.
Using Metrics Server instead of Prometheus Adapter frees the core platform Prometheus stack from handling this functionality.
For more information, see xref:../monitoring/config-map-reference-for-the-cluster-monitoring-operator.adoc#metricsserverconfig[MetricsServerConfig] in the config map API reference for the Cluster Monitoring Operator and xref:../nodes/clusters/nodes-cluster-enabling-features.adoc[Enabling features using feature gates].

[id="ocp-4-15-monitoring-new-feature-to-send-exemplar-data-to-remote-write-storage-for-user-defined-projects"]
==== New feature to send exemplar data to remote write storage for user-defined projects
User-defined projects can now use remote write to send exemplar data scraped by Prometheus to remote storage.
To use this feature, configure remote write using the `sendExemplars` option in the `RemoteWriteSpec` resource.
For more information, see xref:../monitoring/config-map-reference-for-the-cluster-monitoring-operator.adoc#remotewritespec[RemoteWriteSpec] in the config map API reference for the Cluster Monitoring Operator.

[id="ocp-4-15-monitoring-improved-alert-querying-for-user-defined-projects"]
==== Improved alert querying for user-defined projects
Applications in user-defined projects now have API access to query alerts for application namespaces via the rules tenancy port for Thanos Querier.
You can now construct queries that access the `/api/v1/alerts` endpoint via port 9093 for Thanos Querier, provided that the HTTP request contains a `namespace` parameter.
In previous releases, the rules tenancy port for Thanos Querier did not provide API access to the `/api/v1/alerts` endpoint.

[id="ocp-4-15-monitoring-prometheus-updated-to-tolerate-jitters-at-scrape-time"]
==== Prometheus updated to tolerate jitters at scrape time
The default Prometheus configuration in the monitoring stack has been updated so that jitters are tolerated at scrape time.
For monitoring deployments that have shown sub-optimal chunk compression for data storage, this update helps to optimize data compression, thereby reducing the disk space used by the time series database in these deployments.

[id="ocp-4-15-monitoring-improved-staleness-handling-for-the-kubelet-service-monitor"]
==== Improved staleness handling for the kubelet service monitor
Staleness handling for the kubelet service monitor has been improved in order to ensure that alerts and time aggregations are accurate.
This improved functionality is active by default and makes the dedicated service monitors feature obsolete.
As a result, the dedicated service monitors feature has been disabled and is now deprecated, and setting the `DedicatedServiceMonitors` resource to `enabled` has no effect.

[id="ocp-4-15-monitoring-improved-ability-to-troubleshoot-reports-of-tasks-failing"]
==== Improved ability to troubleshoot reports of tasks failing
The reasons provided when tasks fail in monitoring components are now more granular so that you can more easily pinpoint whether a reported failure originated in components deployed in the `openshift-monitoring` namespace or in the `openshift-user-workload-monitoring` namespace.
If the Cluster Monitoring Operator (CMO) reports task failures, the following reasons have been added to identify where the failures originated:

* The `PlatformTasksFailed` reason indicates failures that originated in the `openshift-monitoring` namespace.
* The `UserWorkloadTasksFailed` reason indicates failures that originated in the `openshift-user-workload-monitoring` namespace.

[id="ocp-4-15-network-observability-1-5"]
=== Network Observability Operator
The Network Observability Operator releases updates independently from the {product-title} minor version release stream. Updates are available through a single, rolling stream which is supported on all currently supported versions of {product-title} 4. Information regarding new features, enhancements, and bug fixes for the Network Observability Operator is found in the xref:../network_observability/network-observability-operator-release-notes.adoc[Network Observability release notes].

[id="ocp-4-15-scalability-and-performance"]
=== Scalability and performance

[id="ocp-4-15-hub-side-templating-policygentemplate"]
==== Hub-side templating for PolicyGenTemplate CRs
You can manage the configuration of fleets of clusters by using hub templates to populate the group and site values in the generated policies that get applied to managed clusters.
By using hub templates in group and site `PolicyGenTemplate` (PGT) CRs you can significantly reduce the number of policies on the hub cluster.
For more information, see xref:../scalability_and_performance/ztp_far_edge/ztp-advanced-policy-config.adoc#ztp-specifying-nics-in-pgt-crs-with-hub-cluster-templates_ztp-advanced-policy-config[Specifying group and site configuration in group PolicyGenTemplate CRs with hub templates]. 

[id="ocp-4-15-nto-latency-testing"]
==== Node Tuning Operator (NTO)
The Cloud-native Network Functions (CNF) tests image for latency tests, `cnf-tests`, has been simplified. 
The new image has three tests for latency measurements.
The tests run by default and require a performance profile configured on the cluster. 
If no performance profile is configured, the tests do not run. 

The following variables are no longer recommended for use:

* `ROLE_WORKER_CNF`
* `NODES_SELECTOR`
* `PERF_TEST_PROFILE`
* `FEATURES`
* `LATENCY_TEST_RUN` 
* `DISCOVERY_MODE` 

To generate the `junit` report, the `--ginkgo.junit-report` flag replaces `--junit`.

For more information, see xref:../scalability_and_performance/cnf-performing-platform-verification-latency-tests.adoc[Performing latency tests for platform verification]. 


[id="ocp-4-15-hcp"]
=== Hosted control planes


[id="ocp-4-15-insights-operator"]
=== Insights Operator

[id="ocp-4-15-notable-technical-changes"]
== Notable technical changes

{product-title} {product-version} introduces the following notable technical changes.

// Note: use [discrete] for these sub-headings.
// example sub-heading below:
//[discrete]
//[id="ocp-4-15-cluster-cloud-controller-manager-operator"]
//=== Cloud controller managers for additional cloud providers

[discrete]
[id="ocp-4-15-ports-use-tls"]
=== Cluster metrics ports secured

With this release, the ports that serve metrics for the Cluster Machine Approver Operator and Cluster Cloud Controller Manager Operator use the Transport Layer Security (TLS) protocol for additional security. (link:https://issues.redhat.com/browse/OCPCLOUD-2272[*OCPCLOUD-2272*], link:https://issues.redhat.com/browse/OCPCLOUD-2271[*OCPCLOUD-2271*])

[discrete]
[id="ocp-4-15-cluster-cloud-controller-manager-operator"]
=== Cloud controller manager for Google Cloud Platform

The Kubernetes community plans to deprecate the use of the Kubernetes controller manager to interact with underlying cloud platforms in favor of using cloud controller managers. As a result, there is no plan to add Kubernetes controller manager support for any new cloud platforms.

This release introduces the General Availability of using a cloud controller manager for Google Cloud Platform.

To learn more about the cloud controller manager, see the link:https://kubernetes.io/docs/concepts/architecture/cloud-controller/[Kubernetes Cloud Controller Manager documentation].

To manage the cloud controller manager and cloud node manager deployments and lifecycles, use the Cluster Cloud Controller Manager Operator.

For more information, see the xref:../operators/operator-reference.adoc#cluster-cloud-controller-manager-operator_cluster-operators-ref[Cluster Cloud Controller Manager Operator] entry in the _Cluster Operators reference_.

[discrete]
[id="ocp-4-15-planned-psa-restricted-enforcement"]
=== Future restricted enforcement for pod security admission

Currently, pod security violations are shown as warnings in the audit logs without resulting in the rejection of the pod.

Global restricted enforcement for pod security admission is currently planned for the next minor release of {product-title}. When this restricted enforcement is enabled, pods with pod security violations will be rejected.

To prepare for this upcoming change, ensure that your workloads match the pod security admission profile that applies to them. Workloads that are not configured according to the enforced security standards defined globally or at the namespace level will be rejected. The `restricted-v2` SCC admits workloads according to the link:https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted[Restricted] Kubernetes definition.

If you are receiving pod security violations, see the following resources:

* See xref:../authentication/understanding-and-managing-pod-security-admission.adoc#security-context-constraints-psa-alert-eval_understanding-and-managing-pod-security-admission[Identifying pod security violations] for information about how to find which workloads are causing pod security violations.

* See xref:../authentication/understanding-and-managing-pod-security-admission.adoc#security-context-constraints-psa-synchronization_understanding-and-managing-pod-security-admission[About pod security admission synchronization] to understand when pod security admission label synchronization is performed. Pod security admission labels are not synchronized in certain situations, such as the following situations:
** The workload is running in a system-created namespace that is prefixed with `openshift-`.
** The workload is running on a pod that was created directly without a pod controller.

* If necessary, you can set a custom admission profile on the namespace or pod by setting the `pod-security.kubernetes.io/enforce` label.

[discrete]
[id="ocp-4-15-auth-generated-secrets"]
=== Secrets are no longer automatically generated when the integrated {product-registry} is disabled

If you disable the `ImageRegistry` cluster capability or if you disable the integrated {product-registry} in the Cluster Image Registry Operator's configuration, a service account token secret and image pull secret are no longer generated for each service account.

For more information, see xref:../nodes/pods/nodes-pods-secrets.adoc#auto-generated-sa-token-secrets_nodes-pods-secrets[Automatically generated secrets].

[discrete]
[id="ocp-4-15-ovnic-default-range"]
=== Open Virtual Network Infrastructure Controller default range

Previously, the IP address range `168.254.0.0/16` was the default IP address range that the Open Virtual Network Infrastructure Controller used for the transit switch subnet. With this update, the Controller uses `100.88.0.0/16` as the default IP address range. Do not use this IP range in your production infrastructure network. (link:https://issues.redhat.com/browse/OCPBUGS-20178[*OCPBUGS-20178*])

[discrete]
[id="ocp-4-15-no-strict-limits-variable"]
=== Introduction of HAProxy no strict-limits variable

The transition to HAProxy 2.6 included enforcement for the `strict-limits` configuration, which resulted in fatal errors when `maxConnections` requirements could not be met. The `strict-limits` setting is not configurable by end users and remains under the control of the HAProxy template.

This release introduces a configuration adjustment in response to the transition to the `maxConnections` issues. Now, the HAProxy configuration switches to using `no strict-limits`. As a result, HAProxy no longer fatally exits when the `maxConnection` configuration cannot be satisfied. Instead, it emits warnings and continues running. When `maxConnection` limitations cannot be met, warning like the following examples might be returned:

* `[WARNING] (50) : [/usr/sbin/haproxy.main()] Cannot raise FD limit to 4000237, limit is 1048576.`
* `[ALERT] (50) : [/usr/sbin/haproxy.main()] FD limit (1048576) too low for maxconn=2000000/maxsock=4000237. Please raise 'ulimit-n' to 4000237 or more to avoid any trouble.`

To resolve these warnings, we recommend specifying `-1` or `auto` for the `maxConnections` field when tuning an IngressController. This choice allows HAProxy to dynamically calculate the maximum value based on the available resource limitations in the running container, which eliminates these warnings. (https://issues.redhat.com/browse/OCPBUGS-21803[*OCPBUGS-21803*])

[discrete]
[id="ocp-4-15-auth-deployer-sa"]
=== The deployer service account is no longer created if the DeploymentConfig cluster capability is disabled

If you disable the `DeploymentConfig` cluster capability, the `deployer` service account and its corresponding secrets are no longer created.

For more information, see xref:../installing/cluster-capabilities.adoc#deployment-config-capability_cluster-capabilities[DeploymentConfig capability].

[id="ocp-4-15-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* _General Availability_
* _Deprecated_
* _Removed_

[discrete]
[id="ocp-4-15-operators-dep-rem"]
=== Operator lifecycle and development deprecated and removed features

.Operator lifecycle and development deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|SQLite database format for Operator catalogs
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
=== Images deprecated and removed features

.Images deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|`ImageChangesInProgress` condition for Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated

|`MigrationInProgress` condition for Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
=== Monitoring deprecated and removed features

.Monitoring deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|`dedicatedServiceMonitors` setting that enables dedicated service monitors for core platform monitoring
|General Availability
|General Availability
|Deprecated

|====

[discrete]
=== Installation deprecated and removed features

.Installation deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|`--cloud` parameter for `oc adm release extract`
|General Availability
|Deprecated
|Deprecated

|CoreDNS wildcard queries for the `cluster.local` domain
|Deprecated
|Deprecated
|Deprecated

|`compute.platform.openstack.rootVolume.type` for {rh-openstack}
|General Availability
|Deprecated
|Deprecated

|`controlPlane.platform.openstack.rootVolume.type` for {rh-openstack}
|General Availability
|Deprecated
|Deprecated

|`ingressVIP` and `apiVIP` settings in the `install-config.yaml` file for installer-provisioned infrastructure clusters
|Deprecated
|Deprecated
|Deprecated

|`platform.gcp.licenses` for Google Cloud Provider
|Deprecated
|Removed
|Removed

|====
[.small]
--
1. For {product-title} {product-version}, you must install the {product-title} cluster on a VMware vSphere version 7.0 Update 2 or later instance, including VMware vSphere version 8.0, that meets the requirements for the components that you use.
--
[discrete]
=== Updating clusters deprecated and removed features

.Updating clusters deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|====

[discrete]
=== Storage deprecated and removed features

.Storage deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Persistent storage using FlexVolume
|Deprecated
|Deprecated
|Deprecated

|====

// [discrete]
// === Authentication and authorization deprecated and removed features
//
// .Authentication and authorization deprecated and removed tracker
// [cols="4,1,1,1",options="header"]
// |====
// |Feature |4.13 |4.14 |4.15
//
// |====
// [discrete]
// === Specialized hardware and driver enablement deprecated and removed features
//
// .Specialized hardware and driver enablement deprecated and removed tracker
// [cols="4,1,1,1",options="header"]
// |====
// |Feature |4.13 |4.14 |4.15
//
// |====
// There are no deprecated or removed features jbrigman per Brett Thurber

[discrete]
=== Networking deprecated and removed features

.Networking deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Kuryr on {rh-openstack}
|Deprecated
|Deprecated
|Removed

|====

[discrete]
=== Web console deprecated and removed features

.Web console deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|====

[discrete]
=== Node deprecated and removed features

.Node deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|`ImageContentSourcePolicy` (ICSP) objects
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/zone`
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/region`
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
=== OpenShift CLI (oc) deprecated and removed features
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|`--include-local-oci-catalogs` parameter for `oc-mirror`
|General Availability
|Removed
|Removed

|`--use-oci-feature` parameter for `oc-mirror`
|Deprecated
|Removed
|Removed

|====

[discrete]
=== Workloads deprecated and removed features

.Workloads deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|`DeploymentConfig` objects
|General Availability
|Deprecated
|Deprecated

|====

[discrete]
=== Bare metal monitoring

.Bare Metal Event Relay Operator tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Bare Metal Event Relay Operator
|Technology Preview
|Technology Preview
|Deprecated

|====

[id="ocp-4-15-deprecated-features"]
=== Deprecated features

[id="ocp-4-15-bmer"]
==== Bare Metal Event Relay Operator

The Bare Metal Event Relay Operator is deprecated. The ability to monitor bare-metal hosts by using the Bare Metal Event Relay Operator will be removed in a future {product-title} release.

[id="ocp-4-15-dedicated-service-monitors-for-core-platform-monitoring"]
==== Dedicated service monitors for core platform monitoring
With this release, the dedicated service monitors feature for core platform monitoring is deprecated.
The ability to enable dedicated service monitors by configuring the `dedicatedServiceMonitors` setting in the `cluster-monitoring-config` config map object in the `openshift-monitoring` namespace will be removed in a future {product-title} release.
To replace this feature, Prometheus functionality has been improved in order to ensure that alerts and time aggregations are accurate.
This improved functionality is active by default and makes the dedicated service monitors feature obsolete.

[id="ocp-4-15-deprecated-oc-registry-info"]
==== oc registry info command is deprecated

With this release, the experimental `oc registry info` command is deprecated.

To view information about the integrated {product-registry}, run  `oc get imagestream -n openshift` and check the `IMAGE REPOSITORY` column.

[id="ocp-4-15-removed-features"]
=== Removed features

[id="ocp-4-15-openshift-default-registry"]
==== Removal of the OPENSHIFT_DEFAULT_REGISTRY

{product-title} {product-version} has removed support for the `OPENSHIFT_DEFAULT_REGISTRY` variable. This variable was primarily used to enable backwards compatibility of the internal image registry for earlier setups. The `REGISTRY_OPENSHIFT_SERVER_ADDR` variable can be used in its place.

//.APIs removed from Kubernetes 1.27
//[cols="2,2,2",options="header",]
//|===
//|Resource |Removed API |Migrate to

//|`CSIStorageCapacity`
//|`storage.k8s.io/v1beta1`
//|`storage.k8s.io/v1`

//|===

[id="ocp-4-15-rhosp-kuryr-eol"]
==== Installing clusters on {rh-openstack-first} with Kuryr is removed

As of {product-title} {product-version}, support for installing clusters on {rh-openstack} with kuryr is removed.

[id="ocp-4-15-future-deprecation"]
=== Notice of future deprecation

[id="ocp-4-15-future-removals"]
=== Future Kubernetes API removals

The next minor release of {product-title} is expected to use Kubernetes 1.29. Kubernetes 1.29 has removed a deprecated API.

See the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-29[Deprecated API Migration Guide] in the upstream Kubernetes documentation for the list of Kubernetes API removals.

See link:https://access.redhat.com/articles/6955985[Navigating Kubernetes API deprecations and removals] for information about how to check your cluster for Kubernetes APIs that are planned for removal.

[id="ocp-4-15-bug-fixes"]
== Bug fixes
//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / TALO
//Telco Edge / ZTP
[discrete]
[id="ocp-4-15-api-auth-bug-fixes"]
==== API Server and Authentication

[discrete]
[id="ocp-4-15-bare-metal-hardware-bug-fixes"]
==== Bare Metal Hardware Provisioning

[discrete]
[id="ocp-4-15-builds-bug-fixes"]
==== Builds

[discrete]
[id="ocp-4-15-cloud-compute-bug-fixes"]
==== Cloud Compute

[discrete]
[id="ocp-4-15-cloud-cred-operator-bug-fixes"]
==== Cloud Credential Operator

[discrete]
[id="ocp-4-15-cluster-version-operator-bug-fixes"]
==== Cluster Version Operator

[discrete]
[id="ocp-4-15-dev-console-bug-fixes"]
==== Developer Console

[discrete]
[id="ocp-4-15-cloud-etcd-operator-bug-fixes"]
==== etcd Cluster Operator


[discrete]
[id="ocp-4-15-hosted-control-plane-bug-fixes"]
==== Hosted Control Plane

[discrete]
[id="ocp-4-15-image-registry-bug-fixes"]
==== Image Registry

* Previously, the Image Registry pruner relied on a cluster role that was managed by the OpenShift API server. This could cause the pruner job to intermittently fail during an upgrade. Now, the Image Registry Operator is responsible for creating the pruner cluster role, which resolves the issue. (link:https://issues.redhat.com/browse/OCPBUGS-18969[*OCPBUGS-18969*])

* The Image Registry Operator makes API calls to the storage account list endpoint as part of obtaining access keys. In projects with several {product-title} clusters, this might lead to API limits being reached. As a result, `429` errors were returned when attempting to create new clusters. With this update, the time between calls has been increased from 5 minutes to 20 minutes, and API limits are no longer reached. (link:https://issues.redhat.com/browse/OCPBUGS-18469[*OCPBUGS-18469*])

* Previously, the default low settings for QPS and Burst caused the image registry to return with a gateway timeout error when API server requests were not returned in an appropriate time. To resolve this issue, users had to restart the image registry. With this update, the default settings for QPS and Burst have been increased, and this issue no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-18999[*OCPBUGS-18999*])

* Previously, when creating the deployment resource for the Cluster Image Registry Operator, error handling used a pointer variable without checking if the value was `nil` first. Consequently, when the pointer value was `nil`, a panic was reported in the logs. With this update, a nil check was added so that the panic is no longer reported in the logs. (link:https://issues.redhat.com/browse/OCPBUGS-18103[*OCPBUGS-18103*])

[discrete]
[id="ocp-4-15-installer-bug-fixes"]
==== Installer

[discrete]
[id="ocp-4-15-kube-controller-bug-fixes"]
==== Kubernetes Controller Manager

[discrete]
[id="ocp-4-15-kube-scheduler-bug-fixes"]
==== Kubernetes Scheduler


[discrete]
[id="ocp-4-15-machine-config-operator-bug-fixes"]
==== Machine Config Operator

[discrete]
[id="ocp-4-15-management-console-bug-fixes"]
==== Management Console

[discrete]
[id="ocp-4-15-monitoring-bug-fixes"]
==== Monitoring

[discrete]
[id="ocp-4-15-networking-bug-fixes"]
==== Networking

* Previously, when creating an IngressController with an empty spec, the IngressController's status showed `Invalid`. However, the `route_controller_metrics_routes_per_shard` metric would still get created. When the invalid IngressController was deleted, the `route_controller_metrics_routes_per_shard` metric would not clear, and it would show information for that metric. With this update, metrics are only created for IngressControllers that are admitted, which resolves this issue. (link:https://issues.redhat.com/browse/OCPBUGS-3541[*OCPBUGS-3541*])

* Previously, timeout values larger than what Go programming language could parse were not properly validated. Consequently, timeout values larger than what HAProxy could parse caused issues with HAProxy. With this update, if the timeout specifies a value larger than what can be parsed, it is capped at the maximum that HAProxy can parse. As a result, issues are no longer caused for HAProxy. (link:https://issues.redhat.com/browse/OCPBUGS-6959[*OCPBUGS-6959*])

* Previously, an external neighbor could have its MAC address changed while the cluster was shutting down or hibernating. Although a Gratuitous Address Resolution Protocol (GARP) should notify other neighbors about this change, the cluster would not process the GARP because it was not running. When the cluster was brought back up, that neighbor might not be reachable from the OVN-Kubernetes cluster network because the stale MAC address was being used. This update enables an aging mechanism so that a neighbor's MAC address is periodically refreshed every 300 seconds. (link:https://issues.redhat.com/browse/OCPBUGS-11710[*OCPBUGS-11710*])

* Previously, when an IngressController was configured with SSL/TLS, but did not have the `clientca-configmap` finalizer, the Ingress Operator would try to add the finalizer without checking whether the IngressController was marked for deletion. Consequently, if an IngressController was configured with SSL/TLS and was subsequently deleted, the Operator would correctly remove the finalizer. It would then repeatedly, and erroneously, try and fail to update the IngressController to add the finalizer back, resulting in error messages in the Operator's logs.
+
With this update, the Ingress Operator no longer adds the `clientca-configmap` finalizer to an IngressController that is marked for deletion. As a result, the Ingress Operator no longer tries to perform erroneous updates, and no longer logs the associated errors. (link:https://issues.redhat.com/browse/OCPBUGS-14994[*OCPBUGS-14994*])

* Previously, a race condition occurred between the handling of pods that had been scheduled and the pods that had been completed on a node when OVN-Kubernetes started. This condition often occurred when nodes rebooted. Consequently, the same IP was erroneously assigned to multiple pods. This update fixes the race condition so that the same IP is not assigned to multiple pods in those circumstances. (link:https://issues.redhat.com/browse/OCPBUGS-16634[*OCPBUGS-16634*])

* Previously, there was an error that caused a route to be rejected due to a duplicate host claim. When this occurred, the system would mistakenly select the first route it encountered, which was not always the conflicting route. With this update, all routes for the conflicting host are first retrieved and then sorted based on their submission time. This allows the system to accurately determine and select the newest conflicting route. (link:https://issues.redhat.com/browse/OCPBUGS-16707[*OCPBUGS-16707*])

* Previously, when a new `ipspec-host` pod was started, it would clear or remove the existing `XFRM` state. Consequently, it would remove existing north-south traffic policies. This issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-19817[*OCPBUGS-19817*])

* Previously, the `ovn-k8s-cni-overlay, topology:layer2` NetworkAttachmentDefinition did not work in a hosted pod when using the Kubevirt provider. Consequently, the pod did not start. This issue has been resolved, and pods can now start with an `ovn-k8s-cni-overlay` NetworkAttachmentDefinition. (link:https://issues.redhat.com/browse/OCPBUGS-22869[*OCPBUGS-22869*])

* Previously, the Azure upstream DNS did not comply with non-EDNS DNS queries because it returned a payload larger than 512 bytes. Because CoreDNS 1.10.1 no longer uses EDNS for upstream queries and only uses EDNS when the original client query uses EDNS, the combination would result in an overflow `servfail` error when the upstream returned a payload larger than 512 bytes for non-EDNS queries using CoreDNS 1.10.1. Consequently, upgrading from {product-title} 4.12 to 4.13 led to some DNS queries failing that previously worked.
+
With this release, instead of returning an overflow `servfail` error, the CoreDNS now truncates the response, indicating that the client can try again in TCP. As a result, clusters with a non-compliant upstream now retry with TCP when experiencing overflow errors. This prevents any disruption of functionality between {product-title} 4.12 and 4.13. (link:https://issues.redhat.com/browse/OCPBUGS-27904[*OCPBUGS-27904*]), (link:https://issues.redhat.com/browse/OCPBUGS-28205[*OCPBUGS-28205*])

* Previously, there was a limitation in private Microsoft Azure clusters where secondary IP addresses designated as egress IP addresses lacked outbound connectivity. This meant that pods associated with these IP addresses were unable to access the internet. However, they could still reach external servers within the infrastructure network, which is the intended use case for egress IP addresses. This update enables egress IP addresses for Microsoft Azure clusters, allowing outbound connectivity to be achieved through outbound rules. (link:https://issues.redhat.com/browse/OCPBUGS-5491[*OCPBUGS-5491*])

* Previously, when using multiple NICS, egress IP addresses were not correctly reassigned to the correct egress node when labeled or unlabeled. This bug has been fixed, and egress IP addresses are now reassigned to the correct egress node. (link:https://issues.redhat.com/browse/OCPBUGS-18162[(*OCPBUGS-17162*])

* Previously, a new logic introduced for determining where to run the Keepalived process did not consider the ingress VIP or VIPs. As a result, the Keepalived pods might not have ran on ingress nodes, which could break the cluster. With this fix, the logic now includes the ingress VIP or VIPs, and the Keepalived pods should always be available. (link:https://issues.redhat.com/browse/OCPBUGS-18771[*OCPBUGS-18771*])

[discrete]
[id="ocp-4-15-node-bug-fixes"]
==== Node

[discrete]
[id="ocp-4-15-node-tuning-operator-bug-fixes"]
==== Node Tuning Operator (NTO)

[discrete]
[id="ocp-4-15-openshift-cli-bug-fixes"]
==== OpenShift CLI (oc)

[discrete]
[id="ocp-4-15-olm-bug-fixes"]
==== Operator Lifecycle Manager (OLM)

[discrete]
[id="ocp-4-15-openshift-api-server-bug-fixes"]
==== OpenShift API server

[discrete]
[id="ocp-4-15-rhcos-bug-fixes"]
==== {op-system-first}

[discrete]
[id="ocp-4-15-scalability-and-performance-bug-fixes"]
==== Scalability and performance

[discrete]
[id="ocp-4-15-storage-bug-fixes"]
==== Storage

* Prior to this release, {lvms} did not support disabling over-provisioning, and the minimum value for the `thinPoolConfig.overprovisionRatio` field in the `LVMCluster` CR was 2. With this release, you can disable over-provisioning by setting the value of the `thinPoolConfig.overprovisionRatio` field to 1. (link:https://issues.redhat.com/browse/OCPBUGS-24396[*OCPBUGS-24396*])

* Prior to this release, if the `LVMCluster` CR was created with an invalid device path in the `deviceSelector.optionalPaths` field, the `LVMCluster` CR was in `Progressing` state. With this release, if the `deviceSelector.optionalPaths` field contains an invalid device path, {lvms} updates the `LVMCluster` CR state to `Failed`. (link:https://issues.redhat.com/browse/OCPBUGS-23995[*OCPBUGS-23995*])

* Prior to this release, the {lvms} resource pods were preempted while the cluster was congested. With this release, upon updating {product-title}, {lvms} configures the `priorityClassName` parameter to ensure proper scheduling and preemption behavior while the cluster is congested. (link:https://issues.redhat.com/browse/OCPBUGS-23375[*OCPBUGS-23375*])

* Prior to this release, upon creating the `LVMCluster` CR, {lvms} skipped the counting of volume groups. This resulted in the `LVMCluster` CR moving to `Progressing` state even when the volume groups were valid. With this release, upon creating the `LVMCluster` CR, {lvms} counts all the volume groups, and updates the `LVMCluster` CR state to `Ready` if the volume groups are valid. (link:https://issues.redhat.com/browse/OCPBUGS-23191[*OCPBUGS-23191*])

* Prior to this release, if the default device class was not present on all selected nodes, {lvms} failed to set up the `LVMCluster` CR. With this release, {lvms} detects all the default device classes even if the default device class is present only on one of the selected nodes. With this update, you can define the default device class only on one of the selected nodes. (link:https://issues.redhat.com/browse/OCPBUGS-23181[*OCPBUGS-23181*])

* Prior to this release, upon deleting the worker node in the single-node OpenShift (SNO) and worker node topology, the `LVMCluster` CR still included the configuration of the deleted worker node. This resulted in the `LVMCluster` CR remaining in `Progressing` state. With this release, upon deleting the worker node in the SNO and worker node topology, {lvms} deletes the worker node configuration in the `LVMCluster` CR, and updates the `LVMCluster` CR state to `Ready`. (link:https://issues.redhat.com/browse/OCPBUGS-13558[*OCPBUGS-13558*])

[discrete]
[id="ocp-4-15-windows-containers-bug-fixes"]
==== Windows containers
// Added after OCP GA

[id="ocp-4-15-technology-preview-tables"]
== Technology Preview features status

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the following tables, features are marked with the following statuses:

* _Technology Preview_
* _General Availability_
* _Not Available_
* _Deprecated_

[discrete]
=== Networking Technology Preview features

.Networking Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Ingress Node Firewall Operator
|Technology Preview
|General Availability
|General Availability

|Advertise using L2 mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Technology Preview
|Technology Preview
|Technology Preview

|Multi-network policies for SR-IOV networks
|Technology Preview
|Technology Preview
|Technology Preview

|OVN-Kubernetes network plugin as secondary network
|Technology Preview
|General Availability
|General Availability

|Updating the interface-specific safe sysctls list
|Technology Preview
|Technology Preview
|Technology Preview

|Egress service custom resource
|Not Available
|Technology Preview
|Technology Preview

|VRF specification in `BGPPeer` custom resource
|Not Available
|Technology Preview
|Technology Preview

|VRF specification in `NodeNetworkConfigurationPolicy` custom resource
|Not Available
|Technology Preview
|Technology Preview

|Admin Network Policy (`AdminNetworkPolicy`)
|Not Available
|Technology Preview
|Technology Preview

|IPsec external traffic (north-south)
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Storage Technology Preview features

.Storage Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Automatic device discovery and provisioning with Local Storage Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Google Filestore CSI Driver Operator
|Technology Preview
|General Availability
|General Availability

|{ibm-power-server-name} Block CSI Driver Operator
|Technology Preview
|Technology Preview
|General Availability

|Read Write Once Pod access mod
|Not available
|Technology Preview
|Technology Preview

|Build CSI Volumes in OpenShift Builds
|Technology Preview
|General Availability
|General Availability

|Shared Resources CSI Driver in OpenShift Builds
|Technology Preview
|Technology Preview
|Technology Preview

|{secrets-store-operator}
|Not available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Installation Technology Preview features

.Installation Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Adding kernel modules to nodes with kvc
|Technology Preview
|Technology Preview
|Technology Preview

|Azure Tagging
|Technology Preview
|General Availability
|General Availability

|Enabling NIC partitioning for SR-IOV devices
|Technology Preview
|Technology Preview
|Technology Preview

|GCP Confidential VMs
|Technology Preview
|General Availability
|General Availability

|User-defined labels and tags for Google Cloud Platform (GCP)
|Not Available
|Technology Preview
|Technology Preview

|Installing a cluster on Alibaba Cloud by using installer-provisioned infrastructure
|Technology Preview
|Technology Preview
|Technology Preview

|Mount shared entitlements in BuildConfigs in RHEL
|Technology Preview
|Technology Preview
|Technology Preview

|{product-title} on Oracle Cloud Infrastructure (OCI)
|Not Available
|Developer Preview
|Developer Preview

|Selectable Cluster Inventory
|Technology Preview
|Technology Preview
|Technology Preview

|Static IP addresses with vSphere (IPI only)
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Node Technology Preview features

.Nodes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Cron job time zones
|Technology Preview
|General Availability
|General Availability

|`MaxUnavailableStatefulSet` featureset
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Multi-Architecture Technology Preview features

.Multi-Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|{ibm-power-server-name} using installer-provisioned infrastructure
|Technology Preview
|Technology Preview
|General Availability

|`kdump` on `arm64` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `s390x` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `ppc64le` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Specialized hardware and driver enablement Technology Preview features

.Specialized hardware and driver enablement Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Driver Toolkit
|General Availability
|General Availability
|General Availability

|Hub and spoke cluster support
|General Availability
|General Availability
|General Availability

|====

[discrete]
=== Web console Technology Preview features

.Web console Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Multicluster console
|Technology Preview
|Technology Preview
|Technology Preview

|====


[discrete]
[id="ocp-413-scalability-tech-preview"]
=== Scalability and performance Technology Preview features

.Scalability and performance Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Hyperthreading-aware CPU manager policy
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Technology Preview
|Technology Preview
|Technology Preview

|{factory-prestaging-tool}
|Technology Preview
|Technology Preview
|Technology Preview

|Mount namespace encapsulation
|Technology Preview
|Technology Preview
|Technology Preview

|HTTP transport replaces AMQP for PTP and bare-metal events
|Technology Preview
|Technology Preview
|Technology Preview

|Intel E810 Westport Channel NIC as PTP grandmaster clock
|Technology Preview
|Technology Preview
|Technology Preview

|Workload partitioning for three-node clusters and standard clusters
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
[id="ocp-4-15-operators-tech-preview"]
=== Operator lifecycle and development Technology Preview features

.Operator lifecycle and development Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Operator Lifecycle Manager (OLM) v1
|Not Available
|Technology Preview
|Technology Preview

|RukPak
|Technology Preview
|Technology Preview
|Technology Preview

|Platform Operators
|Technology Preview
|Technology Preview
|Technology Preview

|Hybrid Helm Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Java-based Operator
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Monitoring Technology Preview features

.Monitoring Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15


|Alerting rules based on platform monitoring metrics
|Technology Preview
|General Availability
|General Availability

|Metrics Collection Profiles
|Technology Preview
|Technology Preview
|Technology Preview

|Metrics Server
|Not Available
|Not Available
|Technology Preview

|====


[discrete]
=== {rh-openstack-first} Technology Preview features

.{rh-openstack} Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|External load balancers with installer-provisioned infrastructure
|Technology Preview
|General Availability
|General Availability

|Dual-stack networking with installer-provisioned infrastructure
|Not Available
|Technology Preview
|General Availability

|Dual-stack networking with user-provisioned infrastructure
|Not Available
|Not Available
|General Availability

|CAPO integration into the cluster CAPI Operator ^[1]^
|Not Available
|Not Available
|Technology Preview

|Control Plane with `rootVolumes` and `etcd` on local disk
|Not Available
|Not Available
|Technology Preview
|====
[.small]
--
1. For more information, see xref:../release_notes/ocp-4-15-release-notes.adoc#ocp-4-15-CAPO-tech-preview-no-upgrade[CAPO integration into the cluster CAPI Operator].
--

[discrete]
=== Architecture Technology Preview features

.Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Hosted control planes for {product-title} on Amazon Web Services (AWS)
|Technology Preview
|Technology Preview
|Technology Preview
// Needs to move to GA after Nov 15, 2023
|Hosted control planes for {product-title} on bare metal
|Technology Preview
|Technology Preview
|General Availability
// Needs to move to GA after Nov 15, 2023
|Hosted control planes for {product-title} on {VirtProductName}
|Not Available
|Technology Preview
|General Availability

|====

[discrete]
=== Machine management Technology Preview features

.Machine management Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Managing machines with the Cluster API
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Alibaba Cloud
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Amazon Web Services
|Technology Preview
|General Availability
|General Availability

|Cloud controller manager for Google Cloud Platform
|Technology Preview
|Technology Preview
|General Availability

|Cloud controller manager for {ibm-power-name} VS
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Microsoft Azure
|Technology Preview
|General Availability
|General Availability

|====

[discrete]
=== Authentication and authorization Technology Preview features

.Authentication and authorization Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Pod security admission restricted enforcement
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Machine Config Operator Technology Preview features

.Machine Config Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.13 |4.14 |4.15

|Improved MCO state reporting
|Not Available
|Not Available
|Technology Preview

|====

[id="ocp-4-15-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* Specifying a standard Ebdsv5 or Ebsv5 family machine type instance is not supported when installing an Azure cluster. This limitation is the result of the Azure terraform provider not supporting these machine types. (link:https://issues.redhat.com/browse/OCPBUGS-18690[*OCPBUGS-18690*])

* When running a cluster with FIPS enabled, you might receive the following error when running an older version of the OpenShift CLI (`oc`) on a {op-system-base} 9 system: `FIPS mode is enabled, but the required OpenSSL backend is unavailable`. As a workaround, use the `oc` binary provided with the {product-title} cluster. (link:https://issues.redhat.com/browse/OCPBUGS-23386[*OCPBUGS-23386*])

* In {product-version} with IPv6 networking running on {rh-openstack-first} environments, `IngressController` objects configured with the `endpointPublishingStrategy.type=LoadBalancerService` YAML attribute will not function correctly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2263550[*BZ#2263550*], link:https://bugzilla.redhat.com/show_bug.cgi?id=2263552[*BZ#2263552*])

[id="ocp-telco-ran-4-15-known-issues"]

[id="ocp-4-15-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating_a_cluster/updating-cluster-web-console.adoc#updating-cluster-web-console[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-15-0-ga"]
=== RHSA-2024:XXXX - {product-title} {product-version}.0 image release, bug fix, and security update advisory

Issued: TBD

{product-title} release {product-version}.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2024:XXXX[RHSA-2024:XXXX] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2024:XXXX[RHSA-2024:XXXX] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.15.0 --pullspecs
----
//replace 4.y.z for the correct values for the release. You do not need to update oc to run this command.
