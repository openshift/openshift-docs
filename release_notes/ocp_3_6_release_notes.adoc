[[release-notes-ocp-3-6-release-notes]]
= {product-title} 3.6 Release Notes
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

Red Hat {product-title} is a Platform as a Service (PaaS) that provides
developers and IT organizations with a cloud application platform for deploying
new applications on secure, scalable resources with minimal configuration and
management overhead. {product-title} supports a wide selection of
programming languages and frameworks, such as Java, Ruby, and PHP.

Built on Red Hat Enterprise Linux and Google Kubernetes, {product-title}
provides a secure and scalable multi-tenant operating system for today’s
enterprise-class applications, while providing integrated application runtimes
and libraries. {product-title} brings the OpenShift PaaS platform to customer
data centers, enabling organizations to implement a private PaaS that meets
security, privacy, compliance, and governance requirements.

[[ocp-36-about-this-release]]
== About This Release

Red Hat {product-title} version 3.6
(link:https://access.redhat.com/errata/RHBA-2017:1716[RHBA-2017:1716]) is now
available. This release is based on
link:https://github.com/openshift/origin/releases/tag/v3.6.0-rc.0[OpenShift
Origin 3.6]. New features, changes, bug fixes, and known issues that pertain to
{product-title} 3.6 are included in this topic.

{product-title} 3.6 is supported on RHEL 7.3 and newer with the latest packages
from Extras, including Docker 1.12.

For initial installations, see the
xref:../install_config/install/planning.adoc#install-config-install-planning[Installing
a Cluster] topics in the
xref:../install_config/index.adoc#install-config-index[Installation and
Configuration] documentation.

To upgrade to this release from a previous version, see the
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[Upgrading
a Cluster] topics in the
xref:../install_config/index.adoc#install-config-index[Installation and
Configuration] documentation.

[[ocp-36-new-features-and-enhancements]]
== New Features and Enhancements

This release adds improvements related to the following components and concepts.

[[ocp-36-container-orchestration]]
=== Container Orchestration

[[ocp-36-kubernetes-upstream]]
==== Kubernetes Upstream

Many core features announced in March for Kubernetes 1.6 were the result of
{product-title} engineering. Red Hat continues to influence the product in the
areas of storage, networking, resource management, authentication and
authorization, multi-tenancy, security, service deployments and templating, and
controller functionality.

[[ocp-36-cri-interface]]
==== CRI Interface for Kublet-to-Docker Interaction

{product-title} now uses the CRI interface for kublet-to-Docker interaction.

As the container space matures and choices become more available,
{product-title} needs an agnostic interface in Kubernetes for container runtime
interactions. {product-title} 3.6 switches the default configuration to use the
Kubernetes Docker CRI interface.

There is a `enable-cri` setting in the *_node-config.yaml_* configuration file.  A
value of `true` enables the use of the interface. Change it by editing the
file and stopping or starting the `atomic-openshift-node.service`.

----
$ cat /etc/origin/node/node-config.yaml
enable-cri:
  - 'true'
----

[NOTE]
====
Although the Docker CRI is stable and the default, the overall CRI interface in
Kubernetes is still under development. Red Hat does not support crio, rkt, or
frakti in this {product-title} 3.6 release.
====

[[ocp-36-cluster-capacity-utility]]
==== Cluster Capacity Utility for Checking True Allocatable Space

Just like a disk drive, a cluster can become fragmented over time. When you ask
the cluster how much space is left, the addition of all the free space does not
indicate how many actual workloads can run. For example, it might say there is
10 GB left, but it could be that no single node can take more than 512 MB.

{product-title} 3.6 introduces a new container that you can launch as a command line
or a job.  The container allows you to supply a popular workload (image) with a
commonly requested CPU and MEM limit and request.  The logs from the container
will tell you how many of that workload can be deployed.

See
xref:../admin_guide/cluster_capacity.adoc#admin-guide-cluster-capacity[Analyzing
Cluster Capacity] for more information.

[[ocp-36-quota-remote-storage]]
==== Quota on How Much (Size and Type) Remote Storage a Project Can Use

You can now control what classes of storage projects are allowed to access, how
much (total size) of that class, as well as how many claims.

This feature leverages the `ResourceQuota` object and allows you to call out
storage classes by name for size and claim settings.

----
$ oc create quota my-quota-1 --hard=slow.storageclass.storage.k8s.io/requests.storage=20Gi,slow.storageclass.storage.k8s.io/persistentvolumeclaims=15

$ oc describe quota my-quota-1
Name:                                                     my-quota-1
Namespace:                                                 default
Resource                                                 Used   Hard
--------                                                     ----        ---
slow.storageclass.storage.k8s.io/persistentvolumeclaims    0    15
slow.storageclass.storage.k8s.io/requests.storage                0    20Gi
----

See xref:..admin_guide/quota.adoc#limited-resources-quota[Require Explicit Quota
to Consume a Resource] for more information.

[[ocp-36-scope-PVC-quotas-by-storage-class]]
==== Ability to Scope PVC Quotas by Storage Class

In {product-title} 3.6, administrators now have the ability to specify a
separate quota for persistent volume claims (PVCs) and `requests.storage` per
storage class.

See xref:../admin_guide/quota.adoc#admin-guide-quota[Setting Quotas] for more
information.

[[ocp-36-project-configmaps-secrets-downward-api-in-same-directory]]
==== Project ConfigMaps, Secrets, and Downward API In the Same Directory

When you mount a memory backed volume into a container, it leverages a
directory. Now, you can place all sources of the configuration for your
application (`configMaps`, secrets, and downward API) into the same directory
path.

The new projected line in the volume definition allows you to tell multiple
volumes to leverage the same mount point while guarding for path collisions.

----
volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: test-secret
          items:
            - key: data-1
              path: mysecret/my-username
            - key: data-2
              path: mysecret/my-passwd

      - downwardAPI:
          items:
            - path: mydapi/labels
              fieldRef:
                fieldPath: metadata.labels
            - path: mydapi/name
              fieldRef:
                fieldPath: metadata.name
            - path: mydapi/cpu_limit
              resourceFieldRef:
                containerName: allinone-normal
                resource: limits.cpu
                divisor: "1m"

                - configMap:
                    name: special-config
                    items:
                      - key: special.how
                        path: myconfigmap/shared-config
                      - key: special.type
                        path: myconfigmap/private-config
----

[[ocp-36-init-containers]]
==== Init Containers

You run
xref:../architecture/core_concepts/containers_and_images.adoc#init-containers[init
containers] in the same pod as your application container to create the
environment your application requires or to satisfy any preconditions the
application might have. You can run utilities that you would otherwise need to
place into your application image. You can run them in different file system
namespaces (view of the same file system) and offer them different secrets than
your application container.

Init containers run to completion and each container must finish before the next
one starts. The init containers will honor the restart policy. Leverage
`initContainers` in the `podspec`.

----
$ cat init-containers.yaml
apiVersion: v1
kind: Pod
metadata:
  name: init-loop
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - name: workdir
      mountPath: /usr/share/nginx/html
  initContainers:
  - name: init
    image: centos:centos7
    command:
    - /bin/bash
    - "-c"
    - "while :; do sleep 2; echo hello init container; done"
  volumes:
  - name: workdir
    emptyDir: {}
----

----
$ oc get -f init-containers.yaml
NAME        READY     STATUS     RESTARTS   AGE
nginx       0/1       Init:0/1   0          6m
----

[[ocp-36-multiple-schedulers-at-the-same-time]]
====  Multiple Schedulers at the Same Time

Kubernetes now supports extending the default scheduler implementation with
custom schedulers.

After
link:https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/[configuring
and deploying] your new scheduler, you can call it by name from the `podspec`
via `schedulerName`. These new schedulers are packaged into container images and
run as pods inside the cluster.

----
$ cat pod-custom-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  name: custom-scheduler
spec:
  schedulerName: custom-scheduler
  containers:
  - name: hello
    image: docker.io/ocpqe/hello-pod
----

See xref:../
admin_guide/scheduling/index.adoc#admin-guide-scheduling-index[Scheduling] for
more information.

[[ocp-36-turn-configmap-content-into-environment-variables]]
==== Turn ConfigMap Content into Environment Variables within the Container

Instead of individually declaring environment variables in a pod definition, a
`configMap` can be imported and all of its content can be dynamically turned
into environment variables.

In the pod specification, leverage the `envFrom` object and reference the
desired `configMap`:

----
env:
- name: duplicate_key
  value: FROM_ENV
- name: expansion
  value: $(REPLACE_ME)
envFrom:
- configMapRef:
    name: env-config
----

See xref:../dev_guide/configmaps.adoc#dev-guide-configmaps[`ConfigMaps`] for more
information.

[[ocp-36-node-affinity-and-anti-affinity]]
==== Node Affinity and Anti-affinity

Control which nodes your workload will land on in a more generic and powerful
way as compared to `nodeSelector`.

`NodeSelectors` provide a powerful way for a user to specify which node a
workload should land on. However, If the selectors are not available or are
conflicted, the workload will not be scheduled at all. They also require a user
to have specific knowledge of node label keys and values. Operators provide a
more flexible way to select nodes during scheduling.

Now, you can
link:http://blog.kubernetes.io/2017/03/advanced-scheduling-in-kubernetes.html[select
the label value] you would like the operator to compare against (for example,
`In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt`, and `Lt`).  You can choose to
make satisfying the operator required or preferred. Preferred means search for
the match, but, if you can not find one, ignore it.

----
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: "failure-domain.beta.kubernetes.io/zone"
            operator: In
            values: ["us-central1-a"]
----

----
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: "failure-domain.beta.kubernetes.io/zone"
            operator: NotIn
            values: ["us-central1-a"]
----


See
xref:../admin_guide/scheduling/node_affinity.adoc#admin-guide-sched-affinity[Advanced
Scheduling and Node Affinity] for more information.

[[ocp-36-pod-affinity-and-anti-affinity]]
==== Pod Affinity and Anti-Affinity

Pod affinity and anti-affinity is helpful if you want to allow Kubernetes the
freedom to select which zone an application lands in, but whichever it chooses
you would like to make sure another component of that application lands in the
same zone.

Another use case is if you have two application components that, due to security
reasons, cannot be on the same physical box. However, you do not want to lock
them into labels on nodes. You want them to land anywhere, but still honor
anti-affinity.

Many of the same high-level concepts mentioned in the node affinity and
anti-affinity hold true here. For pods, you declare a
link:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature[`topologyKey`],
which will be used as the boundary object for the placement logic.

----
affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: service
            operator: In
            values: [“S1”]
        topologyKey: failure-domain.beta.kubernetes.io/zone


affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: service
            operator: In
            values: [“S1”]
        topologyKey: kubernetes.io/hostname
----

See
xref:../admin_guide/scheduling/pod_affinity.adoc#admin-guide-sched-pod-affinity[Advanced
Scheduling and Pod Affinity and Anti-affinity] for more information.

[[ocp-36-taints-and-tolerations]]
==== Taints and Tolerations

xref:../admin_guide/scheduling/taints_tolerations.adoc#admin-guide-taints[Taints
and tolerations] allow the *node* to control which *pods* should (or should
not) be scheduled on them.

A _taint_ allows a node to refuse pod to be scheduled unless that pod has a
matching _toleration_.

You apply taints to a node through the node specification (`NodeSpec`) and apply
tolerations to a pod through the pod specification (`PodSpec`). A taint on a
node instructs the node to repel all pods that do not tolerate the taint.

Taints and tolerations consist of a key, value, and effect. An operator allows
you to leave one of these parameters empty.

In {product-title} 3.6, daemon pods do respect taints and tolerations, but they
are created with `NoExecute` tolerations for the
`node.alpha.kubernetes.io/notReady` and `node.alpha.kubernetes.io/unreachable`
taints with no `tolerationSeconds`. This ensures that when the
`TaintBasedEvictions` alpha feature is enabled, they will not be evicted when
there are node problems such as a network partition. (When the
`TaintBasedEvictions` feature is not enabled, they are also not evicted in these
scenarios, but due to hard-coded behavior of the `NodeController` rather than
due to tolerations).

Set the taint from the command line:

----
$ oc taint nodes node1 key=value:NoSchedule
----

Set toleration in the `PodSpec`:

----
tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"
----

[[ocp-36-using-image-streams-with-kubernets-resources]]
==== Using Image Streams with Kubernetes Resources (Technology Preview)

This feature is currently in xref:ocp-36-technology-preview[Technology Preview]
and not for production workloads.

{product-title} has long offered easy integration between continuous integration
pipelines that create deployable Docker images and automatic redeployment and
rollout with `DeploymentConfigs`. This makes it easy to define a standard
process for continuous deployment that keeps your application always running. As
new, higher level constructs like deployments and `StatefulSets` have reached
maturity in Kubernetes, there was no easy way to leverage them and still
preserve automatic CI/CD.

In addition, the image stream concept in {product-title} makes it easy to
centralize and manage images that may come from many different locations, but to
leverage those images in Kubernetes resources you had to provide the full
registry (an internal service IP), the namespace, and the tag of the image,
which meant that you did not get the ease of use that `BuildConfigs` and
`DeploymentConfigs` offer by allowing direct reference of an image stream tag.

Starting in {product-title} 3.6, we aim to close that gap both by making it as
easy to trigger redeployment of Kubernetes Deployments and `StatefulSets`, and
also by allowing Kubernetes resources to easily reference {product-title} image
stream tags directly.

See xref:../dev_guide/managing_images.adoc#using-is-with-k8s[Using Image Streams
with Kubernetes Resources] for more information.

[[ocp-36-registry]]
=== Registry

[[ocp-36-validating-image-signatures-show-appropriate-metadata]]
==== Validating Image Signatures Show Appropriate Metadata

When working with image signatures as the `image-admin` role, you can now see
the status of the images in terms of their signatures.

You can now use the `oc adm verify-image-signature` command to save or remove
signatures. The resulting `oc describe istag` displays additional metadata about
the signature’s status.

----
$ oc describe istag origin-pod:latest
Image Signatures:
  Name: sha256:c13060b74c0348577cbe07dedcdb698f7d893ea6f74847154e5ef3c8c9369b2c@f66d720cfaced1b33e8141a844e793be
  Type: atomic
  Status: Unverified

# Verify the image and save the result back to image stream
$ oadm verify-image-signature sha256:c13060b74c0348577cbe07dedcdb698f7d893ea6f74847154e5ef3c8c9369b2c \
  --expected-identity=172.30.204.70:5000/test/origin-pod:latest --save --as=system:admin
sha256:c13060b74c0348577cbe07dedcdb698f7d893ea6f74847154e5ef3c8c9369b2c signature 0 is verified (signed by key: "172B61E538AAC0EE")

# Check the image status
$ oc describe istag origin-pod:latest
Image Signatures:
  Name:   sha256:c13060b74c0348577cbe07dedcdb698f7d893ea6f74847154e5ef3c8c9369b2c@f66d720cfaced1b33e8141a844e793be
  Type:   atomic
  Status:   Verified
  Issued By:  172B61E538AAC0EE
  Signature is Trusted (verified by user "system:admin" on 2017-04-28 12:32:25 +0200 CEST)
  Signature is ForImage ( on 2017-04-28 12:32:25 +0200 CEST)
----

See xref:../admin_guide/image_signatures.adoc#admin-guide-image-signatures[Image
Signatures] and
xref:../install_config/install/host_preparation.adoc#enabling-image-signature-support[Enabling
Image Signature Support] for more information.

[[ocp-36-registry-rest-endpoint-for-reading-writing-image-signatures]]
==== Registry REST Endpoint for Reading and Writing Image Signatures

 There is now a programmable way to read and write signatures using only the
docker registry API.

To read, you must be authenticated to the registry.

----
PUT /extensions/v2/{namespace}/{name}/signatures/{digest}
$ curl http://<user>:<token>@<registry-endpoint>:5000/extensions/v2/<namespace>/<name>/signatures/sha256:<digest>

JSON:
{
  "version": 2,
  "type":    "atomic",
  "name":    "sha256:4028782c08eae4a8c9a28bf661c0a8d1c2fc8e19dbaae2b018b21011197e1484@cddeb7006d914716e2728000746a0b23",
  "content": "<base64 encoded signature>",
}
----

To write, you must have the `image-signer` role.

----
GET /extensions/v2/{namespace}/{name}/signatures/{digest}
$ curl http://<user>:<token>@<registry-endpoint>:5000/extensions/v2/<namespace>/<name>/signatures/sha256:<digest>


{
  "signatures": [
  {
    "version": 2,
    "type":    "atomic",
    "name":    "sha256:4028782c08eae4a8c9a28bf661c0a8d1c2fc8e19dbaae2b018b21011197e1484@cddeb7006d914716e2728000746a0b23",
    "content": "<base64 encoded signature>",
  }
  ]
}
----

[[ocp-36-platform-management]]
=== Platform Management

[[ocp-36-require-explicit-quota-to-consume-a-resource]]
==== Require Explicit Quota to Consume a Resource (Technology Preview)

This feature is currently in xref:ocp-36-technology-preview[Technology Preview]
and not for production workloads.

If a resource is not managed by quota, a user has no restriction on the amount
of resource that can be consumed. For example, if there is no quota on storage
related to the gold storage class, the amount of gold storage a project can
create is unbounded.

See xref:../admin_guide/quota.adoc#limited-resources-quota[Setting Quotas] for
more information.

[[ocp-36-storage]]
=== Storage

[[ocp-36-aws-efs-provisioner]]
==== AWS EFS Provisioner

The AWS EFS provisioner allows you to dynamically use the AWS EFS endpoint to
get NFS remote persistent volumes on AWS.

It leverages the
link:https://kubernetes.io/docs/concepts/storage/persistent-volumes/#provisioner[external
dynamic provisioner interface]. It is provided as a `docker` image that you
configure with a `configMap` and deploy on {product-title}. Then, you can use a
storage class with the appropriate configuration.

.Storage Class Example
----
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: slow
provisioner: foobar.io/aws-efs
parameters:
  gidMin: "40000"
  gidMax: "50000"
----
`gidMin` and `gidMax` are the minimum and maximum values, respectively, of the
GID range for the storage class. A unique value (GID) in this range (`gidMin` to
`gidMax`) is used for dynamically provisioned volumes.

[[ocp-36-vmware-vsphere-storage]]
==== VMware vSphere Storage

VMware vSphere storage allows you to dynamically use the VMware vSphere storage
options ranging from VSANDatastore, ext3, vmdk, and VSAN while honoring vSphere
Storage Policy (SPBM) mappings.

VMware vSphere storage leverages the cloud provider interface in Kubernetes to
trigger this in-tree dynamic storage provisioner. Once the cloud provider has
the correct credential information, tenants can leverage storage class to select
the desired storage.

.Storage Class Example
----
kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
  name: fast
provisioner: kubernetes.io/vsphere-volume
parameters:
    diskformat: zeroedthick
----

See xref:../install_config/configuring_vsphere.adoc#install-config-configuring-vsphere[Configuring for VMWare vSphere] and xref:../install_config/persistent_storage/persistent_storage_vsphere.adoc#install-config-persistent-storage-persistent-storage-vsphere[Persistent Storage Using VMWare vSphere Volume] for more information.

[[ocp-36-increased-security-with-iscsi-chap-mount-operations]]
==== Increased Security with iSCSI CHAP and Mount Operations

You can now use CHAP authentication for your iSCSI remote persistent volumes (PVs).
Also, you can annotate your PVs to leverage any mount options that are supported
by that underlying storage technology.

The tenant supplies the correct user name and password for the CHAP
authentication as a secret in their `podspec`. For mount options, you supply the
annotation in the PV.

----
volumes:
  - name: iscsivol
    iscsi:
      targetPortal: 127.0.0.1
      iqn: iqn.2015-02.example.com:test
      lun: 0
      fsType: ext4
      readOnly: true
      chapAuthDiscovery: true
      chapAuthSession: true
      secretRef:
         name: chap-secret
----

Set `volume.beta.kubernetes.io/mount-options` to
`volume.beta.kubernetes.io/mount-options: rw,nfsvers=4,noexec`.

See xref:../architecture/additional_concepts/storage.adoc#pv-mount-options[Mount
Options] for more information.

[[ocp-36-mount-options]]
==== Mount Options (Technology Preview)

Mount Options are currently in xref:ocp-36-technology-preview[Technology
Preview] and not for production workloads.

You can now specify mount options while mounting a persistent volume by using
the annotation `volume.beta.kubernetes.io/mount-options`

See
xref:../architecture/additional_concepts/storage.adoc#pv-mount-options[Persistent
Storage] for more information.

[[ocp-36-improved-automated-support-for-cns-backed-ocp-hosted-registry]]
====  Improved and Fully Automated Support for CNS-backed OCP Hosted Registry

Previously, only a few supported storage options existed for a scaled,
highly-available integrated {product-title} (OCP) registry. Automated container
native storage (CNS) 3.6 and the {product-title} installer now include an option
to automatically deploy a scale-out registry based on highly available storage,
out of the box. When enabled in the installer’s inventory file, CNS will be
deployed on a desired set of nodes (for instance, infrastructure nodes). Then,
the required underlying storage constructs will automatically be created and
configured for use with the deployed registry. Moving an existing registry
deployment from NFS to CNS is also supported, and requires additional steps for
data migration.

Backing the {product-title} registry with CNS enables users to take advantage of
the globally available storage capacity, strong read/write consistency,
three-way replica, and RHGS data management features.

The feature is provided through integrations in the {product-title}
xref:../install_config/install/advanced_install.adoc#advanced-install-containerized-glusterfs-backed-registry[advanced
installation] process. A few dedicated storage devices and a simple change to
the inventory file is all that is required.

[[ocp-36-ocp-commerical-evaluation-subscription-includes-cns-crs]]
==== {product-title} Commercial Evaluation Subscription Includes CNS and CRS

The OpenShift Commercial Evaluation subscription includes container native
storage (CNS), container ready storage (CRS) solutions.

The OpenShift Commercial Evaluation subscription SKU bundles the CNS and CRS
features, with additional entitlements to evaluate {product-title} with CNS/CRS.

[IMPORTANT]
====
Evaluation SKUs are not bundled with {product-title}'s SKUs or entitlements.
Consult your Red Hat account representative for subscription guidance.
====

[[ocp-36-scale]]
=== Scale

[[ocp-36-updated-etcd-performance-guidance]]
==== Updated etcd Performance Guidance

See
xref:../scaling_performance/host_practices.adoc#scaling-performance-capacity-host-practices[Recommended
Host Practices] for updated etcd performance guidance.

[[ocp-36-updated-sizing-guidance]]
==== Updated Sizing Guidance

In {product-title} 3.6 , the
xref:../install_config/install/planning.adoc#sizing[maximum number of nodes per
cluster] is 2000.

[[ocp-36-networking]]
=== Networking

[[ocp-36-multiple-destinations-in-egress-router]]
==== Multiple Destinations in egress-router

{product-title} 3.6 introduces the ability to connect to multiple destinations
from a project without needing to reserve a separate source IP for each of them.
Also, there is now an optional fallback IP. Old syntax continues to behave the
same and there is no change to `EGRESS_SOURCE` and `EGRESS_GATEWAY` definitions.

Old way:

----
- name: EGRESS_DESTINATION
  value: 203.0.113.25
----

New way:

----
- name: EGRESS_DESTINATION
  value: |
    80 tcp 1.2.3.4
    8080 tcp 5.6.7.8 80
    8443 tcp 9.10.11.12 443
    13.14.15.16
----

----
localport  udp|tcp  dest-ip [dest-port]
----

See
xref:../admin_guide/managing_networking.adoc#admin-guide-manage-networking[Managing
Networking] for more information.

[[ocp-26-added-http-proxy-mode-for-egress-router]]
==== Added HTTP Proxy Mode for the Egress Router

TLS connections (certificate validations) do not easily work because the client
needs to connect to the egress router's IP (or name) rather than to the
destination server's IP/name. Now, the egress router can be run as a proxy
rather than just redirecting packets.

How it works:

. Create a new project and pod.

. Create the `egress-router-http-proxy` pod.

. Create the service for `egress-router-http-proxy`.

. Set up `http_proxy` in the pod:
+
----
# export http_proxy=http://my-egress-router-service-name:8080
# export https_proxy=http://my-egress-router-service-name:8080
----

. Test and check squid headers in response:
+
----
$ curl  -ILs http://www.redhat.com
$ curl  -ILs https://rover.redhat.com
    HTTP/1.1 403 Forbidden
    Via: 1.1 egress-http-proxy (squid/x.x.x)
$ curl  -ILs http://www.google.com
    HTTP/1.1 200 OK
    Via: 1.1 egress-http-proxy (squid/x.x.x)
$ curl  -ILs https://www.google.com
    HTTP/1.1 200 Connection established
    HTTP/1.1 200 OK
----

See
xref:../admin_guide/managing_networking.adoc#admin-guide-manage-networking[Managing
Networking] for more information.

[[ocp-36-use-dns-names-with-egress-firewall]]
==== Use DNS Names with Egress Firewall

There are several benefits of using DNS names versus IP addresses:

- It tracks DNS mapping changes.
- Human-readable, easily remembered naming.
- Potentially backed by multiple IP addresses.

How it works:

. Create the project and pod.
. Deploy egress network policy with DNS names.
. Validate the firewall.

.Egress Policy Example
----
{
    "kind": "EgressNetworkPolicy",
    "apiVersion": "v1",
    "metadata": {
        "name": "policy-test"
    },
    "spec": {
        "egress": [
            {
                "type": "Allow",
                "to": {
                    "dnsName": "stopdisablingselinux.com"
                }
            },
            {
                "type": "Deny",
                "to": {
                  "cidrSelector": "0.0.0.0/0"
                }
            }
        ]
    }
}
----

[NOTE]
====
Exposing services by creating routes will ignore the Egress Network Policy.
Egress Network policy Service endpoint filtering is performed on the `kubeproxy`
node. When the router is involved, `kubeproxy` is bypassed and Egress Network
Policy enforcement is not applied. Administrators can prevent this bypass by
limiting access and the ability to create routes.
====

See xref:../admin_guide/managing_pods.adoc#admin-guide-manage-pods[Managing
Pods] for more information.

[[ocp-36-network-policy]]
==== Network Policy (Technology Preview)

Network Policy (currently in xref:ocp-36-technology-preview[Technology Preview]
and not for production workloads) is an optional plug-in specification of how
selections of pods are allowed to communicate with each other and other network
endpoints. It provides fine-grained network namespace isolation using labels and
port specifications.

After installing the Network Policy plug-in, an annotation that flips the
namespace from `allow all traffic` to `deny all traffic` must first be set on
the namespace. At that point, `NetworkPolicies` can be created that define what
traffic to allow. The annotation is as follows:

----
$ oc annotate namespace ${ns} 'net.beta.kubernetes.io/network-policy={"ingress":{"isolation":"DefaultDeny"}}'
----

The allow-to-red policy specifies "all red pods in namespace `project-a` allow
traffic from any pods in any namespace." This does not apply to the red pod in
namespace `project-b` because `podSelector` only applies to the namespace in
which it was applied.

.Policy applied to project
----
kind: NetworkPolicy
apiVersion: extensions/v1beta1
metadata:
  name: allow-to-red
spec:
  podSelector:
    matchLabels:
      type: red
  ingress:
  - {}
----

See
xref:../admin_guide/managing_networking.adoc#admin-guide-manage-networking[Managing
Networking] for more information.

[[ocp-36-router-template-format]]
==== Router Template Format

{product-title} 3.6 introduces improved router customization documentation. Many
RFEs could be solved with better documentation around the HAProxy
features and functions which are now added, and their customizable fields via
annotations and environment variables. For example, router annotations to do
per-route operations.

For example, to change the behavior of HAProxy (round-robin load balancing)
through annotating a route:

----
$ oc annotate route/ab haproxy.router.openshift.io/balance=roundrobin
----

For more information, see
xref:../install_config/router/customized_haproxy_router.adoc#install-config-router-customized-haproxy[Deploying
a Customized HAProxy Router].

[[use-a-different-f5-partition]]
==== Use a Different F5 Partition Other than /Common

With {product-title} 3.6, there is now the added ability to use custom F5
partitions for properly securing and isolating {product-title} route
synchronization and configuration.

The default is still `/Common` or global partition if not specified. Also,
behavior is unchanged if the partition path is not specified.  This new feature
ensures all the referenced objects are in the same partition, including virtual
servers (`http` or `https`).

[[ocp-36-support-ipv6-terminated-at-the-router-with-internal-ipv4]]
==== Support IPv6 Terminated at the Router with Internal IPv4

The router container is able to terminate IPv6 traffic and pass HTTP[S] through
to the back-end pod.

The IPv6 interfaces on the router must be enabled, with IPv6 addresses listening
(`::80`, `::443`). The client needs to reach the router node using IPv6.
IPv4 should be unaffected and continue to work, even if IPv6 is disabled.

[NOTE]
====
HAProxy can only terminate IPv6 traffic when the router uses the network stack
of the host (default). When using the container network stack (`oadm router
--service-account=router --host-network=false`), there is no global IPv6 address
for the pod.
====

[[ocp-36-installation]]
=== Installation

[[ocp-36-ansible-service-broker]]
==== Ansible Service Broker (Technology Preview)

The Ansible service broker is currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads. This feature includes:

- Implementation of the open service broker API that enables users to leverage Ansible
for provisioning and managing of services via the service catalog on {product-title}.
- Standardized approach for delivering simple to complex multi-container
{product-title} services.
- Works in conjunction with Ansible playbook bundles (APB), which is a lightweight
meta container comprised of a few named playbooks for each open service broker
API operations.

Service catalog and Ansible service broker must be configured during
{product-title} installation. Once enabled, APB services can be deployed right
from Service Catalog UI.

[IMPORTANT]
====
In {product-title} In OCP 3.6.0, the Ansible Service Broker exposes an
unprotected route, which allows unauthenticated users to provision resources in
the cluster, namely Mediawiki and Postgres Ansible Playbook Bundles.
====

See
xref:../install_config/install/advanced_install.adoc#configuring-ansible-service-broker[Configuring
the Ansible Service Broker] for more information.

[[ocp-36-ansible-playbook-bundles]]
==== Ansible Playbook Bundles (APB) (Technology Preview)

Ansible playbook bundles (APB) (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) is a short-lived, lightweight container image consisting of:

* Simple directory structure with named action playbooks
* Metadata consisting of:
** required/optional parameters
** dependencies (provision versus bind)
* Ansible runtime environment
* Leverages existing investment in Ansible playbooks and roles
* Developer tooling available for guided approach
* Easily modified or extended
* Example APB services included with {product-title} 3.6:
** MediaWiki, PostgreSQL

When a user orders an application from the service catalog, the Ansible service
broker will download the associated APB image from the registry and run it. Once
the named operation has been performed on the service, the APB image will then
terminate.

[[ocp-36-automated-installation-of-cloudforms]]
==== Automated installation of CloudForms 4.5 Inside OpenShift (Technology Preview)

The installation of containerized CloudForms inside {product-title} is now part
of the main installer (currently in xref:ocp-36-technology-preview[Technology
Preview] and not for production workloads). It is now treated like other common
components (metrics, logging, and so on).

After the {product-title} cluster is provisioned, there is an additional
playbook you can run to deploy CloudForms into the environment (using the
`openshift_cfme_install_app` flag in the hosts file).

----
$ ansible-playbook -v -i <INVENTORY_FILE> playbooks/byo/openshift-cfme/config.yml
----

Requirements:

[cols="4*", options="header"]
|===
|Type
|Size
|CPUs
|Memory

|Masters
|1+
|8
|12 GB

|Nodes
|2+
|4
|8 GB

|PV Storage
|25 GB
|N/A
|N/A
|===

[NOTE]
====
NFS is the only storage option for the Postgres database at this time.

The NFS server should be on the first master host. The persistent volume backing
the NFS storage volume is mounted on exports.
====

[[ocp-36-automated-cns-deployment-with-ocp-ansible-advanced-installation]]
==== Automated CNS Deployment with OCP Ansible Advanced Installation

{product-title} (OCP) 3.6 now includes an integrated and simplified installation
of container native storage (CNS) through the advanced installer. The
installer’s inventory file is simply configured. The end result is an automated,
supportable, best practice installation of CNS, providing ready-to-use
persistent storage with a pre-created storage class. The advanced installer now
includes automated and integrated support for deployment of CNS, correctly
configured and highly available out-of-the-box.

CNS storage device details are added to the installer’s inventory file. Examples
provided in {product-title}
xref:../install_config/install/advanced_install.adoc#advanced-install-containerized-glusterfs-persistent-storage[advanced
installation documentation]. The installer manages configuration and deployment
of CNS, its dynamic provisioner, and other pertinent details.

[[ocp-36-installation-of-etcd-docker-daemon-and-ansible-installer-as-system-containers]]
==== Installation of etcd, Docker Daemon, and Ansible Installer as System Containers (Technology Preview)

This feature is currently in xref:ocp-36-technology-preview[Technology Preview]
and not for production workloads.

RHEL System Containers offer more control over the life cycle of the services
that do not run inside {product-title} or Kubernetes. Additional system
containers will be offered over time.

System Containers leverage the OSTree on RHEL or Atomic Host. They are
controlled by the kernel init system and therefore can be leveraged earlier in
the boot sequence. This feature is enabled in the installer configuration.

For more information, see
xref:../install_config/install/advanced_install.adoc#advanced-install-configuring-system-containers[Configuring
System Containers].

[[ocp-36-running-openshift-installer-as-a-system-container]]
==== Running OpenShift Installer as a System Container (Technology Preview)

This feature is currently in xref:ocp-36-technology-preview[Technology Preview]
and not for production workloads.

To run the {product-title} installer as a system container:

----
$ atomic install --system --set INVENTORY_FILE=$(pwd)/inventory registry:port/openshift3/ose-ansible:v3.6

$ systemctl start ose-ansible-v3.6
----

[[ocp-36-etcd3-model-for-new-installations]]
==== etcd3 Data Model for New Installations

Starting with new installations of {product-title} 3.6, the etcd3 v3 data model
is the default. By moving to the etcd3 v3 data model, there is now:

- Larger memory space to enable larger cluster sizes.
- Increased stability in adding and removing nodes in general life cycle actions.
- A significant performance boost.

A migration playbook will be provided in the near future allowing
upgraded environments to migrate to the v3 data model.

[[ocp-36-cluster-wide-control-of-ca]]
==== Cluster-wide Control of CA

You now have the ability to change the certificate expiration date en mass
across the cluster for the various framework components that use TLS.

We offer new cluster variables per framework area so that you can use different
time-frames for different framework components. Once set, issue the new
`redeploy-openshift-ca` playbook. This playbook only works for redeploying the
root CA certificate of {product-title}. Once you set the following options, they
will be effective in a new installation, or they can be used when redeploying
certificates against an existing cluster.

.New Cluster Variables
----
# CA, node and master certificate expiry
openshift_ca_cert_expire_days=1825
openshift_node_cert_expire_days=730
openshift_master_cert_expire_days=730

# Registry certificate expiry
openshift_hosted_registry_cert_expire_days=730

# Etcd CA, peer, server and client certificate expiry
etcd_ca_default_days=1825
----

[[ocp-36-general-stability]]
==== General Stability

{product-title} engineering and the OpenShift Online operations teams have been
working closely together to refactor and enhance the installer. The
{product-title} 3.6 release includes the culmination of those efforts, including:

- Upgrading from {product-title} 3.5 to 3.6
- Idempotency refactoring of the configuration role
- Swap handling during installation
- All BYO playbooks pull from a normalized group source
- A final port of operation’s Ansible modules
- A refactoring of excluder roles

[[ocp-36-metrics-and-logging]]
=== Metrics and Logging

[[ocp-36-removing-metrics-deployer-and-removing-logging-deployer]]
==== Removing Metrics Deployer and Removing Logging Deployer

The metrics and logging deployers were replaced with `playbook2image` for `oc
cluster up` so that `openshift-ansible` is used to install logging and metrics:

----
$ oc cluster up --logging --metrics
----

Check metrics and logging pod status:

----
$ oc get pod -n openshift-infra
$ oc get pod -n logging
----

[[ocp-36-expose-elasticsearch-as-a-route]]
====  Expose Elasticsearch as a Route

By default, the Elasticsearch instance deployed with {product-title} aggregated
logging is not accessible from outside the deployed {product-title} cluster. You
can now enable an external route for accessing the Elasticsearch instance
via its native APIs to enable external access to data via various supported
tools.

Direct access to the Elasticsearch instance is enabled using your OpenShift
token. You have the ability to provide the external Elasticsearch and
Elasticsearch Operations host names when creating the server certificate
(similar to Kibana). The provided Ansible tasks simplify route deployment.

[[ocp-36-mux]]
==== Mux (Technology Preview)

`mux` is a new xref:ocp-36-technology-preview[Technology Preview] feature for
{product-title} 3.6.0 designed to facilitate better scaling of aggregated
logging. It uses a smaller set of from Fluentd instances (called _muxes_) kept
near the Elasticsearch instance pod to improve the efficiency of indexing log
records into Elasticsearch.

See xref:../install_config/aggregate_logging.adoc#aggregated-fluentd[Aggregating
Container Logs] for more information.

[[ocp-36-developer-experience]]
=== Developer Experience

[[ocp-36-service-catalog-experience]]
==== Service Catalog Experience in the CLI (Technology Preview)

This feature (currently in xref:ocp-36-technology-preview[Technology Preview]
and not for production workloads) brings the Service Catalog experience to the
CLI.

You can run `oc cluster up --version=latest --service-catalog=true` to get the
Service Catalog experience in {product-title} 3.6.

[[ocp-36-template-service-broker]]
==== Template Service Broker (Technology Preview)

The template service broker (currently in
xref:ocp-36-technology-preview[Technology Preview]) exposes OpenShift templates
through a open service broker API to the Service Catalog.

The template service broker (TSB) matches the lifecycles of provision,
deprovision, bind, unbind with existing templates. No changes are required to
templates, unless you expose bind. Your application will get injected with
configuration details (bind).

[IMPORTANT]
====
The TSB is currently a Technology Preview feature and should not be used in
production clusters. Enabling the TSB currently requires opening unauthenticated
access to the cluster; this security issue will be resolved before exiting the
Technology Preview phase.
====

See
xref:../install_config/install/advanced_install.adoc#configuring-template-service-broker[Configuring
the Template Service Broker] for more information.

[[ocp-36-automicatic-build-pruning]]
==== Automatic Build Pruning

Previously, only `oc adm prune` could be used. Now, you can define how much
build history you want to keep per build configuration. Also, you can set
`successful` versus `failed` history limits separately.

See
xref:../dev_guide/builds/advanced_build_operations.adoc#build-pruning[Advanced
Build Operations] for more information.

[[ocp-36-easier-custom-slave-configuration-for-jenkins]]
==== Easier Custom Slave Configuration for Jenkins

In {product-title} 3.6, it is now easier to make images available as slave pod
templates.

Slaves are defined as image-streams or image-stream tags with the appropriate
label. Slaves can also be specified via a `ConfigMap` with the appropriate
label.

See
xref:../using_images/other_images/jenkins.adoc#using-the-jenkins-kubernetes-plug-in-to-run-jobs[Using
the Jenkins Kubernetes Plug-in to Run Jobs] for more information.

[[ocp-36-detailed-build-timing]]
==== Detailed Build Timing

Builds now record timing information based on more granular steps.

Information such as how long it took to pull the base image, clone the source,
build the source, and push the image are provided. For example:

----
$ oc describe build nodejs-ex-1
Name:        nodejs-ex-1
Namespace:    myproject
Created:    2 minutes ago

Status:            Complete
Started:        Fri, 07 Jul 2017 17:49:37 EDT
Duration:        2m23s
  FetchInputs:       2s
  CommitContainer:   6s
  Assemble:           36s
  PostCommit:            0s
  PushImage:          1m0s
----

[[ocp-36-other-developer-experience-changes]]
==== Other Developer Experience Changes

* xref:../dev_guide/builds/triggering_builds.adoc#webhook-triggers[Webhook triggers] for Github and Bitbucket.
* HTTPD 2.4 s2i support.
* Separate build events for `start`, `canceled`, `success`, and `fail`.
* Support for xref:../dev_guide/builds/build_strategies.adoc#docker-strategy-build-args[arguments in Docker files].
* xref:../dev_guide/builds/build_strategies.adoc#jenkins-pipeline-strategy-environment[Environment variables in pipeline builds].
* Credential support for Jenkins Sync plug-in for ease of working external Jenkins instance.
* xref:../dev_guide/builds/build_environment.adoc#overview[`ValueFrom` Support] in build environment variables.
* Deprecated Jenkins v1 image.
* `oc cluster up`: support launching service catalog
* Switch to nip.io from xip.io, with improved stability

[[ocp-36-web-console]]
=== Web Console

[[ocp-36-service-catalog]]
==== Service Catalog (Technology Preview)

You can now opt into the service catalog (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) during installation or upgrade.

When developing microservices-based applications to run on cloud native
platforms, there are many ways to provision different resources and share their
coordinates, credentials, and configuration, depending on the service
provider and the platform.

To give developers a more seamless experience, {product-title} includes a
xref:../architecture/service_catalog/index.adoc#architecture-additional-concepts-service-catalog[Service
Catalog], an implementation of the link:https://openservicebrokerapi.org/[open
service broker API] (OSB API) for Kubernetes. This allows users to connect any
of their applications deployed in {product-title} to a wide variety of service
brokers.

The service catalog allows cluster administrators to integrate multiple
platforms using a single API specification. The {product-title} web console
displays the service classes offered by brokers in the service catalog, allowing
users to discover and instantiate those services for use with their
applications.

As a result, service users benefit from ease and consistency of use across
different types of services from different providers, while service providers
benefit from having one integration point that gives them access to multiple
platforms.

This feature consists of:

- The Service Consumer: The individual, application , or service that uses a service enabled by the broker and catalog.
- The Catalog: Where services are published for consumption.
- Service Broker: Publishes services and intermediates service creation and credential configuration with a provider.
- Service Provider: The technology delivering the service.
- Open Service Broker API: Lists services, provisions and deprovisions, binds, and unbinds.

See
xref:../install_config/install/advanced_install.adoc#enabling-service-catalog[Enabling
the Service Catalog] for more information.

[[ocp-36-initial-experience]]
==== Initial Experience (Technology Preview)

In {product-title} 3.6, a better initial user experience (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) is introduced, motivated by service catalog. This includes:

- A task-focused interface.
- Key call-outs.
- Unified search.
- Streamlined navigation.

[[ocp-36-search-catalog]]
==== Search Catalog (Technology Preview)

The search catalog feature (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) provides a single, simple way to quickly get what you want.

image::ocp36-search-catalog.gif[search catalog]

[[ocp-36-add-from-catalog]]
==== Add from Catalog (Technology Preview)

The add from catalog feature (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) allows you to provision a service from the catalog.

Select the desired service, then follow prompts for your desired project and
configuration details.

image::ocp36-add-from-catalog.gif[add from catalog]

[[ocp-36-project-overview-redesign]]
==== Project Overview Redesign

In {product-title} 3.6, the Project Overview was resigned based on feedback from
customers.

In this redesign, there are three focused views:

- Applications
- Pipelines
- Resource types

There are now more contextual actions and rolled up metrics across multiple
pods.

image::ocp36-redesigned-project-overview.gif[Redesigned Project Overview]

[[ocp-36-add-to-project]]
==== Add to Project (Technology Preview)

The add to project feature (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) allows you to provision a service without having to leave the Project
Overview.

When you go directly to the catalog from project, the context is preserved. You
can directly provision, then bind.

image::ocp36-add-to-project.gif[add to project]

[[ocp-36-bind-in-context]]
==== Bind in Context (Technology Preview)

The bind in context feature (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) allows you to provision a service and bind without having to leave
the Project Overview.

- Select deployment and initiate a bind.
- Select from bindable services.
- Binding is created and the user stays in context
- See relationships between bound applications and services in the Project
Overview section.

image::ocp36-bind-in-context.gif[bind in context]

[[ocp-36-image-stream-details]]
==== Image Stream Details

In {product-title} 3.6, additional details are provided about image streams and
their tags.

This feature leverages Cockpit views from image streams. It details tags and
provide information about each.

image::ocp36-image-stream-details.png[bind in context]

[[ocp-36-better-messages-for-syntax-errors]]
==== Better Messages for Syntax Errors in JSON and YAML Files

With {product-title} 3.6, better messages for syntax errors in JSON and YAML
files are provided. This includes details of the syntax problem and the line
number containing the issue.

This feature validates input on commands such as `oc create -f foo.json` and
`oc new-app -f template.yaml`. For example:

----
$ oc create -f dc.json
error: json: line 27: invalid character 'y' looking for beginning of value
----

[[ocp-36-cascading-deletes]]
==== Cascading Deletes

When deleting a resource, this feature ensures that all generated or dependent
resources are also deleted.

For example, when selecting a deployment configuration and deleting will delete
the deployment configuration, deployment history, and any running pods.

image::ocp36-cascading-deletes.png[cascading deletes]

[[ocp-36-other-user-interface-changes]]
==== Other User Interface Changes

- Pod details now should show information about
xref:../architecture/core_concepts/containers_and_images.adoc#init-containers[init
containers].
- You can now add or edit environment variables that are populated by data in
secrets or configuration maps.
- You can now create cluster-wide resources from JSON and YAML files.
- There is now an alignment of notification designs.

[[ocp-36-notable-technical-changes]]
== Notable Technical Changes

{product-title} 3.6 introduces the following notable technical changes.

[discrete]
[[ocp-pci-dss-compliance]]
=== Payment Card Industry Data Security Standard (PCI DSS) Compliance

Red Hat has worked with a PCI DSS Qualified Assessor (QSA) and has determined
that {product-title} running on either Red Hat Enterprise Linux or Red Hat
Enterprise Linux Atomic Host could be deployed in a way that it would pass a PCI
assessment. Ultimately, compliance and validation is the responsibility of the
organization deploying {product-title} and their assessor. Implementation of
proper configuration, rules, and policies is paramount to compliance, and
link:https://access.redhat.com/support/offerings/production/soc[Red Hat makes no
claims or guarantees] regarding PCI assessment.

[discrete]
[[ocp-36-federation-decision-deliberation]]
===  Federation Decision Deliberation

In the upstream federation special interest group (SIG), there are two primary
ideas being  discussed. The current control plane model is an intelligent
controller that duplicates API features and functions at a high level. The
client is agnostic and the controller handles the inter-cluster relationships,
policy, and so on. The control plane model may be difficult to maintain.

In the client model, multiple controllers would exist for various features and
functions, and the client would maintain the intelligence to understand how to
affect change across clusters. Red Hat is currently soliciting feedback on these
two models. Customers, partners, and community members are encouraged to
participate in the upstream SIGs.

[discrete]
[[ocp-36-dns-changes]]
=== DNS Changes

Prior to {product-title} 3.6, cluster DNS was provided by the API server running
on the master and the use of *dnsmasq* could be disabled by setting
`openshift_use_dnsmasq=false`. Starting with {product-title} 3.6, the use of
*dnsmasq* is now mandatory and upgrades will be blocked if this variable is set
to false.

Also, when upgrading to version 3.6, the playbooks will configure the node
service to serve DNS requests on `127.0.0.1:53` and *dnsmasq* will be
reconfigured to route queries for `cluster.local` and `in-addr.arpa` to
`127.0.0.1:53` rather than to the Kubernetes service IP. Your node must not run
other services on port 53. Firewall rules exposing port 53 are not necessary, as
all queries will originate from the local network.

[discrete]
[[ocp-36-deprecated-api-types]]
=== Deprecated API Types

The `ClusterPolicy`, `Policy`, `ClusterPolicyBinding` and `PolicyBinding` API
types are deprecated. Users will need to switch any interactions with these
types to instead use `ClusterRole`, `Role`, `ClusterRoleBinding`, or
`RoleBinding` as appropriate. The following `oadm policy` commands can be used
to help with this process:

----
add-cluster-role-to-group
add-cluster-role-to-user
add-role-to-group
add-role-to-user
remove-cluster-role-from-group
remove-cluster-role-from-user
remove-role-from-group
remove-role-from-user
----

The following `oc create` commands can also help:

----
clusterrole
clusterrolebinding
role
rolebinding
----

The use of `oc create policybinding` is also deprecated and no longer a
perquisite for creating a `RoleBinding` to a `Role`.

[discrete]
[[ocp-36-resources-registered-to-api-groups]]
=== OpenShift Resources Registered to API groups

Custom roles that reference OpenShift resources should be updated to include the
appropriate API groups.

[discrete]
[[ocp-36-ambiguous-CIDR-values-rejected]]
=== Ambiguous CIDR Values Rejected

{product-title} will now reject `EgressNetworkPolicy`, `ClusterNetwork`,
`HostSubnet`, and `NetNamespace` objects with ambiguous CIDR values. Before, an
`EgressNetworkPolicyRule` such as the following would be interpreted as "allow
to `192.168.1.*0/24*`".

----
type: Allow
to:
  cidrSelector: 192.168.1.15/24
----

However, the user most likely meant "allow to 192.168.1.*15/32*". In
{product-title} 3.6, trying to create such a rule (or to modify an existing rule
without fixing it) will result in an error.

The same validation is also now performed on CIDR-valued fields in
`ClusterNetwork`, `HostSubnet`, and `NetNamespace` objects, but these are
normally only created or modified by {product-title} itself.

[discrete]
[[ocp-36-volumes-removed-at-pod-termination]]
=== Volumes Removed at Pod Termination

In prior versions, pod volumes remained attached until the pod resource was
deleted from the master. This prevented local disk and memory resources from
being reclaimed as a result of pod eviction. In {product-title} 3.6, the volume
is removed when the pod is terminated.

[discrete]
[[ocp-36-init-containers]]
=== Init Containers

Pod authors can now use
xref:../architecture/core_concepts/containers_and_images.adoc#init-containers[init
containers] to share volumes, perform network operations, and perform
computation prior to the start of the remaining containers.

An init container is a container in a pod that is started before the pod’s
application containers are started. Init containers can also block or delay the
startup of application containers until some precondition is met.

[discrete]
[[ocp-36-init-containers]]
=== Pod Tolerations and Node Taints No Longer Defined in Annotations

xref:../admin_guide/scheduling/taints_tolerations.adoc#admin-guide-taints[Pod
tolerations and node taints] have moved from annotations to API fields in pod
specifications (PodSpec) and node specification (NodeSpec) files, respectively.
Pod tolerations and node taints that are defined in the annotations will be
ignored. The annotation keys `scheduler.alpha.kubernetes.io/tolerations` and
`scheduler.alpha.kubernetes.io/taints` are now removed.

[discrete]
[[ocp-36-router-does-not-allow-SSLv3]]
=== Router Does Not Allow SSLv3

The OpenShift router will no longer allow SSLv3 (to prevent the POODLE attack).
No modern web browser should require this.

[discrete]
[[ocp-36-router-cipher-list-updates]]
=== Router Cipher List Updates
The router cipher list has changed to reflect the current _intermediate_ cipher
suite recommendations from Mozilla. It is now also possible to set the
cipher suite explicitly, or choose from a list of named preset security levels.

[discrete]
[[ocp-36-networkpolicy-objects-v1-semantics]]
=== NetworkPolicy Objects Have NetworkPolicy v1 Semantics from Kubernetes 1.7

When using the `redhat/openshift-ovs-networkpolicy` plug-in, which is still in
Technology Preview,
xref:../admin_guide/managing_networking.html#admin-guide-networking-networkpolicy[`NetworkPolicy`]
objects now have the `NetworkPolicy` v1 semantics from Kubernetes 1.7. They are
still in the `extensions/v1beta1` API group; the new `networking.k8s.io/v1` API
group is not yet available.

In particular, the `net.beta.kubernetes.io/network-policy` annotation on
namespaces to opt in to isolation has been removed. Instead, isolation is now
determined at a per-pod level, with pods being isolated if there is any
`NetworkPolicy` whose `spec.podSelector` targets them. Pods that are targeted by
`NetworkPolicies` accept traffic that is accepted by any of the `NetworkPolicies`
(and nothing else), and pods that are not targeted by any `NetworkPolicy` accept
all traffic by default.

To preserve compatibility when upgrading:

. In namespaces that previously had the `DefaultDeny` annotation, you can
create equivalent v1 semantics by creating a `NetworkPolicy` that matches all
pods but does not allow any traffic:
+
----
kind: NetworkPolicy
apiVersion: extensions/v1beta1
metadata:
  name: default-deny
spec:
  podSelector:
----
+
This will ensure that pods that are not matched by any other `NetworkPolicy`
will continue to be fully-isolated, as they were before.

. In namespaces that previously did not have the `DefaultDeny` annotation, you
should delete any existing `NetworkPolicy` objects. These would have had no
effect before, but with v1 semantics they might cause some traffic to be blocked
that you did not intend to be blocked.

[discrete]
[[ocp-36-deprecated-metadata-volumesource]]
=== Metadata volumeSource Now Deprecated

The
link:https://github.com/openshift/origin/blob/master/vendor/k8s.io/kubernetes/pkg/api/v1/types.go#L338-L341[metadata
`volumeSource`] is now deprecated for multiple releases and will be removed in
{product-title} 3.7.

[discrete]
[[ocp-36-breaking-api-change]]
=== Breaking API Change

Unless explicitly documented otherwise, API fields containing lists of items no
longer distinguish between null and `[]`, and may return either null or `[]`
regardless of the original value submitted.

[discrete]
[[ocp-36-atomic-command-on-hosts]]
=== Atomic Command on Hosts
When using system containers with {product-title}, the `atomic` command on hosts
must be `1.17.2` or later.

[discrete]
[[ocp-36-containers-run-under-build-pods-parent-cgroup]]
=== Containers Run Under Build Pod's Parent cgroup

Containers launched by the build pod (the s2i assemble container or the `docker
build` process) now run under the build pod's parent cgroup.

Previously, the containers had their own cgroup and the memory and CPU limits were
mirrored from the pod's cgroup limits. With this change, the secondary
containers will now be sharing the memory limit that is consumed by the build
pod, meaning the secondary containers will have slightly less memory available
to them.

[discrete]
[[ocp-36-SecurityContextConstraints-vailable-via-groupified-API]]
=== SecurityContextConstraints Available via Groupified API

`SecurityContextConstraints` are now also available via a groupified API at
*_/apis/security.openshift.io/v1/securitycontextconstraints_*. They are still
available at *_/api/v1/securitycontextconstraints_*, but using the groupified API
will provide better integration with tooling.

[discrete]
[[ocp-36-volume-recycler-now-deprecated]]
=== Openshift Volume Recycler Now Deprecated

Openshift Volume Recycler is being deprecated. Anyone using recycler should use
dynamic provision and volume deletion instead.

[[ocp-36-bug-fixes]]
== Bug Fixes

This release fixes bugs for the following components:

*Authentication*

* Nested groups now sync between {product-title} and Active Directory. It is
common to have nested groups in Active Directory.  Users wanted to be able to
sync such groups with {product-title}. This feature was always supported, but
lacked any formal documentation and was difficult to discover.
xref:../install_config/syncing_groups_with_ldap.adoc#sync-ldap-nested-example[Documentation
is now added].
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1437324[*BZ#1437324*])

*Builds*

* When a build is started from a webhook, the server response does not contain a
body Therefore, the CLI cannot easily determine the generation of the created
build, and cannot report it to the user. Change webhook response to contain the
created build object in the body. The CLI can now report the correct build
generation when created.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1373441[*BZ#1373441*])

* Build durations are recorded as part of a storage hook. Build duration is
sometimes calculated incorrectly and reported with an invalid value. Calculate
build duration when recording build time of build completion. As a result, build
durations are reported correctly and align with the build start and completion
times. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1443687[*BZ#1443687*])

* The code was not setting the status reason and status message for certain
failures. Therefore, there were missing status reasons and status messages for
certain failures. With this bug fix, code was added that sets the status reason
and status message and the status reason and message are now set.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1436391[*BZ#1436391*])

* A debug object type is used when high levels of logging are requested. Client
code did not anticipate the alternative object type and experienced a typecast
error. With this bug fix, the client code is updated to handle the debug object
type. The typecast error will not occur and builds now proceed as expected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1441929[*BZ#1441929*])

* When resources were specified in the build default configuration, the resource
values were not applied to the build pods. They were only applied to the build
object. Builds ran without the default resource limits being applied to them
because the pod was created before the build was updated with the default
resource limits. With this bug fix, the build resource defaults are applied to
the build pod. Build pods now have the default resource limits applied, if they
do not already specify resource limits.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1443187[*BZ#1443187*])

* The `new-app` circular dependency code did not account for `BuildConfig` sources
pointing to the `ImageStreamImage` type. As a result, an unnecessary warning was
logged about not being able to follow the reference type `ImageStreamImage`.
This bug fix enhances the `new-app` circular dependency code to account for the
`ImageStreamImage` type. The unnecessary warning no longer appears.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1422378[*BZ#1422378*])

*Command Line Interface*

* Previously, pod headers were only being printed once for all sets of pods when
listing pods from multiple nodes. Executing `oadm manage-node <node-1> <node-2> ...
--evacuate --dry-run` with multiple nodes would print the same output multiple
times (once per each specified node). Therefore, users would see inconsistent or
duplicate pod information. This bug fix resolves the issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1390900[*BZ#1390900*])

* The `--sort-by` in the `oc get` command fails when any object in the list
contains an empty value in the field used to sort, causing a failure. With this
bug fix, empty fields in `--sort-by` are now correctly handled. The output of
`oc get` is printed correctly and empty fields are considered in sorting.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1409878[*BZ#1409878*])

* A Golang issue (in versions up to 1.7.4) adds an overhead of around four seconds
to the TLS handshake on macOS. Therefore, the `oc` calls time out
intermittently on macOS. This bug fix backported the existing fix to 1.7.5 and
upgraded the Golang that we use to build `oc` to that version. The TLS handshake
time is now reduced by about four seconds on macOS.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1435261[*BZ#1435261*])

* When the master configuration specified a default `nodeSelector` for the
cluster, test projects created by `oadm diagnostics` NetworkCheck got this
`nodeSelector` and, therefore, the test pods were also confined to this
`nodeSelector`. NetworkCheck test pods could only be scheduled on a subset of
nodes, preventing the diagnostic covering the entire cluster; in some clusters,
this might even result in too few pods running for the diagnostic to succeed
even if the cluster health is fine. NetworkCheck now creates the test projects
with an empty `nodeSelector` so they can land on any schedulable node. The
diagnostic should now be more robust and meaningful.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1459241[*BZ#1459241*])

*Installer*

* OpenShift Ansible facts were splitting a configuration parameter incorrectly.
Therefore, invalid `NO_PROXY` strings were generated and inserted into user
*_sysconfig/docker_*  files. The logic that generates the NO_PROXY strings was
reviewed and fixed. Valid Docker `NO_PROXY` settings are enerated and inserted
into the *_sysconfig/docker_*  file now.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1414748[*1414748*])

* The OpenShift CA redeployment playbook
(*_playbooks/byo/openshift-cluster/redeploy-openshift-ca.yml_*) would fail to
restart services if certificates were previously expired. Service restarts are
now skipped within the OpenShift CA redeployment playbook when expired
certificates are detected. Expired cluster certificates may be replaced with the
certificate redeployment playbook
(*_playbooks/byo/openshift-cluster/redeploy-certificates.yml_*) once the
OpenShift CA certificate has been replaced via the OpenShift CA redeployment
playbook.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=1452367[*1452367*])

* Previously, installation would fail in multi-master environments in which the
load balanced API was listening on a different port than that of the OpenShift
API and console. This difference is now accounted for and and the master
loopback client configuration is configured to interact with the local master.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1454321[*1454321*])

* A readiness probe is introduced with {product-title} 3.6, but the timeout
threshold was not high enough. This bug fix increases the timeout threshold.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1456139[*1456139*])

* Elasticsearch heap dump should not be written to the root partition. Specify a
location to write a heap dump other than the root partition.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1369914[*1369914*])

* Previously, the upgrade playbooks would use the default `kubeconfig`, which may
have been modified since creation to use a non-admin user. Now the upgrade
playbooks use the admin `kubeconfig`, which avoids this problem.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1468572[*1468572*])

* A fix for a separate PROXY related issue was merged. Therefore, various proxy
related operations began to fail.A correct fix for the original PROXY-related
issue was merged and functionality is now restored.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1470165[*1470165*])

* `NO_PROXY` setting logic was incorrectly indented in the openshift-ansible facts
module, causing `NO_PROXY` settings to always be generated and added to service
configuration files. The logic indentation was moved into the correct
conditional.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1468424[*BZ#1468424*])

* Image streams now reference the DNS hostname of
 `docker-registry.default.svc:5000`, which allows the installer to ensure that
 the hostname is appended to `NO_PROXY` environment variables so image pushes
 work properly in an environment that requires a proxy.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1414749[*BZ#1414749*])

* Starting with {product-title} 3.4, the software-defined networking (SDN)
plug-ins no longer reconfigure the docker bridge maximum transmission unit
(MTU), rather pods are configured properly on creation. Because of this change,
non-OpenShift containers may have a MTU configured that is too large to allow
access to hosts on the SDN. The installer has been updated to align the MTU
setting for the docker bridge with the MTU used inside the cluster, thus
avoiding the problem.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1457062[*BZ#1457062*])

* As part of the RFE to be able to label `PersistentVolume` (PV) for
`PersistentVolumeClaim` (PVC) selectors, the default PVC selector was set to
null but should have been an empty string. This caused the playbook to fail if
the user did not provide a label. This fix leaves the default label blank,
allowing the playbook to run to completion if the user does not provide a PV
label. (https://bugzilla.redhat.com/show_bug.cgi?id=1462352[*BZ#1462352*])

* Metrics were not consistently able to install correctly when using a non-root
user. This caused the playbook to fail due to lack of permissions, or files not
visible due to permissions. With this fix, any local action within the metrics
role added a `become: false` so it ensured it was using the local actions as the
same user running the playbook. The playbook no longer fails to complete due to
permissions. (https://bugzilla.redhat.com/show_bug.cgi?id=1464977[*BZ#1464977*])

* This feature grants the ability to provide `PersistentVolume` (PV) selectors for
PVs created during installation. Previously when installing logging and metrics
with the installer, a PV created for logging could be bound to a metrics PVC,
creating confusion. Now you can provide a PV selector in your inventory when
installing logging and metrics and the PVs created will contain the appropriate
label so that the generated PVCs will correctly bind.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1442277[*BZ#1442277*])

* Hosts missing an OpenSSL python library caused large serial numbers to not be
 parsed using the existing manual parser workaround for missing OpenSSL
 libraries. This bug fix updates the manual parser to account for certificate
 formats with large serial numbers. As a result, certificates with large serials
 on hosts missing the OpenSSL python library can now be parsed, such as during
 certificate expiration checking or certificate redeployment.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1464240[*BZ#1464240*])

* The master configuration parameter `serviceAccountConfig.limitSecretReferences`
may now be set via the installation playbooks by setting the variable
`openshift_master_saconfig_limitsecretreferences` to `true` or `false`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1442332[*BZ#1442332*])

* Older logic was missing a condition in which the `systemd` unit files should be
reloaded, causing updated or changed service unit files to not be identified.
This bug fix updates the Ansible installer master and node roles to ensure the
`reload system units` action is triggered. As a result, updated service unit
files are correctly detected and users no longer receive a “Could not find the
requested service” error anymore.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1451693[*BZ#1451693*])

* An incorrect check for python libraries was used for the metrics role, causing
 playbooks to fail when checking whether *python2-passlib* was installed. This
 bug fix updates the query for checking the availability of the library. As a
 result, the playbook no longer incorrectly fails when *python2-passlib* is
 installed.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1455310[*BZ#1455310*])

* The default persistent volume (PV) selector for the logging persistent volume
claim (PVC) generation was `None` and was being interpreted as a variable. This
caused the playbook to fail because it could not find a variable of the name
`None`. This bug fix updates the default to be `’’`. As a result, the playbook
is able to correctly run to completion when not providing a PV selector.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1463055[*BZ#1463055*])

* The installer now creates a default `StorageClass` whenever AWS or GCE cloud
providers are configured, allowing for out-of-the-box dynamic volume creation.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1393207[*BZ#1393207*])

* The example inventory files have been amended to illustrate all available audit
logging configuration options.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1447774[*BZ#1447774*])

* The default templates have been updated to the latest available for OpenShift
Container Platform 3.6.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1463553[*BZ#1463553*])

* Previously, all certificates for an OpenShift cluster have a validity of one
year. This was not practical for enterprise-level installations. The installer
tool was modified to allow configuration of certificates, meaning the validity
period can be extended.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1275176[*BZ#1275176*])

* The service accounts that belonged in the `openshift-infra` namespace were being
created in `default` after a different fix to create them before role bindings.
Therefore, pods were not able to find their SA for running. With this bug fix,
SAs are created in the correct namespace and pods are able to start.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1477440[*BZ#1477440*])

*Image*

* When Kubernetes settings are updated, Jenkins is restarted and reloaded. This
causes all of the configurations to be reloaded, including {product-title}
settings. Therefore, `credentialsId` becomes null and causes NPE's to be thrown,
stopping the watchers, which can not recover. When Kubernetes is updated,
synchronization with {product-title} is stopped. With this bug fix, the getter
for `credentialsId ` check for null, and returns `""` if found. Kubernetes can
now be updated without NPE.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1451223[*BZ#1451223*])

* Proxy values are logged during builds. Previously, proxy values that contained
user credentials were exposed to anyone who can view build logs. With this bug
fix, credentials that are part of proxy values (for example,
`\http://user:password@proxy.com`) will be redacted from the proxy value being
logged. Proxy credentials are now no longer exposed in build logs.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1366795[*BZ1366795*])

* Previously, the PHP `latest` image stream tag did not point to the latest
available PHP image (7.0). Therefore, users of the `latest` image stream tag did
not get the most recent PHP image available. With this bug fix, the `latest` tag
is updated to point to the most recent image stream tag for PHP. Now, users who
select the `latest` tag will get the PHP 7.0 image. (BZ#1421982)
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1421982[*BZ1421982*])

*Image Registry*

* There was a logic error in how weak and strong references were identified when
searching images eligible for pruning. Therefore, some images having both strong
and weak references in pruning graph could be removed during pruning. The logic
responsible for finding which images have strong references is now fixed.
Pruning now correctly recognizes and prunes images.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1440177[*BZ440177*])

* Only aliases within single Image streams were being resolved. If an update was
done to the source image, cross-image-stream aliases were not resolved properly,
pointing to the old image. This bug fix forbids the creation of
cross-image-stream aliases. Users creating a cross-image-stream alias now get an
error. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1435588[*1435588*])

*Kubernetes*

* Previously, if the pod restarted due to exceeding `failureThreshold` on a probe,
the restarted pod was only allowed a single probe failure before being
restarted, regardless of the `failureThreshold` value. This caused restarted
pods not to get the expected number of probe attempts before being restarted.
This fix allows the reset the failure counter when the pod is restarted,
therefore the restarted pod gets `failureThreshold` attempts for the probe to
succeed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1455056[*BZ#1455056*])

* When attempting to connect to  an `etcd` server to acquire a leader lease, the
master controllers process only tried to reach a single `etcd` cluster member
even if multiple are specified. If the selected `etcd` cluster member is
unavailable, the master controllers process is not able to acquire the leader
lease, which means it will not start up and run properly. This fix enables
attempts to connect to all of the specified `etcd` cluster members until a
successful connection is made, and as a result the master controllers process
can acquire the leader lease and start up properly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1426183[*BZ#1426183*])

* Previously, the same error message was being output for each node in a cluster.
With this fix, the error will include its message and its repeat count.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1462345[BZ#1462345])

*Logging*

* A change in the `authproxy` was keeping it from finding dependent files, causing
the `authproxy` to terminate. With this fix, environment variables were added to
the `deploymentconfig` with the correct path to the files. As a result, the
`openshift-auth-proxy` finds dependent files and starts correctly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1439451[BZ#1439451])

* The Aggregated Logging diagnostic was not updated to reflect updates made to
logging deployment. Therefore, the diagnostic incorrectly reported errors for an
unnecessary Service Account and (if present) the `mux` deployment. With this bug
fix, these errors are no longer reported. In addition, warnings about missing
optional components were all downgraded to Info level. The diagnostic no longer
needlessly alarms the user for these issues.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1421623[*1421623*])

*Web Console*

* Previously, there were issues viewing logs for pods with multiple containers
caused, especially when switching between containers. You should now be able to
switch between container logs without issue and the Follow link should work as
expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1421287[*1421287*])

* It was difficult to find the underlying reason for a failed deployment from the
project overview. The overview will now link to the Events page in these
scenarios, which typically contains useful information about what went wrong.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1365525[*1365525*])

* Previously, the OpenShift namespace appeared at the top of the list of
namespaces for the image stream tag picker, which was confusing in long lists of
namespaces if the user was expecting to find it alphabetically in the drop-down
menu. This happened because the image stream tag picker was adding the OpenShift
namespace to the list after the list was already sorted. The list of namespaces
the user can pick from is now sorted after the OpenShift namespace is added to
the list. Now the list of namespaces a user can pick from, when selecting an
image stream tag for build configuration, options have OpenShift sorted
alphabetically with the other namespaces the user can access.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1436819[*BZ#1436819*])

* The web console now better uses the screen space when displaying services.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1401134[*BZ#1401134*])

*Metrics*

* Previously, partitions in the `metrics_idx` table cause Cassandra to write into
the table packets that are as large as 496 MB and even 700 MB, causing client
requests to Hawkular Metrics to fail. A workaround of changing the compaction
strategy for the `metrics_idx` table from `LCS` to `STCS` was created, leading
to a new, persisting Hawkular image.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1422271[*BZ#1422271*])

* The internal metadata around the Cassandra schema was out of date, leading to
the data being a mix of old and new schema information. The version has been
updated.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1466086[*BZ#1466086*])

*Networking*

* Previously, the {product-title} node proxy did not support using a specified IP
address. This prevented correct operation on hosts with multiple network
interface cards. The {product-title} node process already accepts a
`--bind-address=<ip address>:<port>` command-line flag and `bindAddress:`
configuration file option for the multiple network interface card case. The
proxy functionality is now fixed to respect these options. When `--bind-address`
or `bindAddress` are used, the {product-title} node proxy should work correctly
when the {product-title} node host has multiple network interface cards.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1462428[*1462428*])

* Previously, when an IP address was re-used, it would be generated with a random
MAC address that would be different from the previous one, causing any node with
an ARP cache that still held the old entry for the IP to not communicate with
the node. Now, generating the MAC address deterministically from the IP address
now results in a re-used IP address always having the same MAC address, so the
ARP cache can not be out of sync. This ensures the traffic will now flow.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1451854[*BZ#1451854*])

* Previously, the VNID allow rules were removed before they were really unused.
This meant that if there were still pods in that namespace on the node, they
could not communicate with one another. The way that the tracking is done was
changed so to avoid the edge cases around pod creation or deletion failures.
This meant that the VNID tracking does not fail, so traffic flows.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1454948[*BZ#1454948*])

* Previously, running `oadm diagnostics NetworkCheck` would result in a timeout
error. Changing the script to run from the pod definition fixed the issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1421643[*BZ#1421643*])

* Previously, using an F5 router did not work with re-encrypt routes. Adding the
re-encrypt routes to the same vserver fixed the problem.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1431655[*BZ#1431655*])

* Previously, there was a missing `iptables` rule to block `INVALID` packets,
causing packets to escape cluster. The missing rule was added missing rule
resulting in no more leaks.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1438762[*BZ#1438762*])

* Minor enhancements have been made to the `iptables` proxier to reduce node CPU
usage when many pods and services exist.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1387149[*BZ#1387149*])

* Previously, some fragmented IP packets were mistakenly dropped by
`openshift-node` instead of being delivered to pods, causing large UDP and TCP
packets to have some or all fragments dropped instead of being delivered. The
relevant fragments are now correctly evaluated and sent to their destination,
meaning large UDP and TCP packets should now be delivered to the intended pods
in the cluster.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1419692[*BZ#1419692*])

* Previously, the ARP cache was not compatible with OpenShift clusters with a
large number of routes (more than the default value of `1024`). The default has
been changed to `65536`, meaning clusters with many routes will function.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1425388[*BZ#1425388*])

* Previously, using `oc expose svc` picked up the service port instead of the
target port, meaning the route would not work. The command is now picked up from
the port number.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1431781[*BZ#1431781*])

* Previously, the hybrid proxy was not correctly protecting access to internal
data. This meant that, when it was enabled, it could terminate the
`openshift-node` process with a runtime panic due to concurrent data accesses.
As a fix, all internal data is correctly protected against concurrent access,
meaning the `openshift-node` process should no longer panic with concurrent data
access failures when the hybrid proxy is enabled.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1444125[*BZ#1444125*])

* Previously, after adding the
`netnamespace.network.openshift.io/multicast-enabled=true` annotation to
`netnamespace`, it will create one open-flow rule in table 110, but the
annotation is still there after deletion. The problem has now been fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1449058[*BZ#1449058*])

* Previously, the CLI help text was not clear about what worked on the F5 versus
the HAProxy routers. The CLI help text has been updated with clearer
expectations.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1427774[*BZ#1427774*])

* Previously, having multiple node IP addresses reported in random order by node
status. This led to the SDN controller picking up a random IP each time. IP
stickiness is now maintained, meaning the IP is valid when chosen.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1438402[*BZ#1438402*])

* Previously, cluster-external traffic was handled incorrectly when using the
Technology Preview `NetworkPolicy` plug-in. Pods could not connect to IP
addresses outside the cluster. The issue has been resolved and external traffic now works
correctly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1443765[*BZ#1443765*])

* Previously, the code to set up multicast was not run when only one node was in
the cluster, leading to multicast traffic dropping when on a single-node
cluster. The rules have been changed so the multicast setup is performed for a
single-node.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1445501[*BZ#1445501*])

* Previously, the initialization order of the SDN plug-in set up the event handler
too late, causing early events to have no handler, so the SDN would panic. The
SDN initialization has been re-ordered so that the event handler is in place
before it can be called.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1445504[*BZ#1445504*])

* Previously, the iptables rules were logged at too low of a log level, causing
the logs to fill with iptables noise. The level at which they are logged has
changed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1455655[*BZ#1455655*])

* Previously, the `NetworkPolicy` plug-in (currently in Tech Review) in
{product-title} 3.5 did not implement all features of `NetworkPolicy`. When
using certain `NetworkPolicy` resources that used `PodSelectors`, pods would be
accessible by pod IP, but not by service IP. These issues have been addressed.
All connections that should be allowed by a `NetworkPolicy` are now allowed
whether made directly (pod-to-pod) or indirectly via a service IP.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1419430[*BZ#1419430*])

*REST API*

* `maxScheduledImageImportsPerMinute` was previously documented as accepting `-1`
as a value to allow unlimited imports. This would cause the cluster to panic.
`maxScheduledImageImportsPerMinute` now correctly accepts `-1` as an unlimited
value.  Administrators who have set `maxScheduledImageImportsPerMinute` to an
extremely high number as a workaround may leave the existing setting or now use
`-1`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1388319[*BZ#1388319*])

* Previously, deleting created resources from a project failed to delete the route
and an error message was shown on the web console. The issue has been resolved.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1452569[*BZ#1452569*])

*Routing*

* This enhancement strips HTTP `Proxy` headers to prevent the `httpoxy`
(`\https://httpoxy.org/`) vulnerability. Applications behind the router are now
protected from `httpoxy`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1469633[*1469633*])

* Previously, when quickly adding then deleting a route using the CLI, routes are
queued up to be processed, saving the request data in a store, then acts on
them. The problem is the store is empty when the last request is popped, causing
an issue. This bug fix resolves the issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1447928[*BZ#1447928*])

* This bug fixes the matching logic change, which made the trailing slashed
inadvertently break, meaning that subpaths with trailing `/`s no longer worked.
The code that matches them has been corrected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1448944[*BZ#1448944*])

* Previously, the logic in the HAProxy router template did not account for `Allow`
as `InsecureEdgeTerminationPolicy` for re-encrypt routes, because the cookie
object was set as secure. Logic has been added to correctly tag the cookie as
insecure when `InsecureEdgeTerminationPolicy` is `Allow` for re-encrypt routes.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1428720[*BZ#1428720*])

* Previously, the command to create a list of routes was incorrect, meaning the
route statuses did not get deleted. The logic enumerating routes has been
improved.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1429364[*BZ#1429364*])

* Previously, the script did not check the version of `jq` and does not populate
its array of routes correctly, leading to the script failing when using `-r`.
The fix was to check to make sure the user has an appropriate version of `jq`
and populate the array of target routes properly. Then, the script correctly
clears the routes specified of status messages.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1429398[*BZ#1429398*])

* Previously, the router template did not add `option forwardfor` to re-encrypt
type routes, causing the `X-Forwarded-For` section of *_http header_* file to go
missing. This bug fix adds `option forwardfor` in the router template for the
re-encrypt type routes. Now the `X-Forwarded-For` section of the *_http header_*
file will correctly populate.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1449022[*BZ#1449022*])

* Version 3.6 router introduced a new port named `router-stats`. This bug created
an option for `oadm router` command to allow a user to specify customized a
router-stats port, such as `--stats-port=1936`, so that user could easily create
an customized router.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1452019[*BZ#1452019*])

* This bug tracks the changing matching logic leading to trailing slashed
inadvertently breaking, leading to subpaths with trailing `/`s no longer working.
The code that matches them has been corrected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1446627[*BZ#1446627*])

* This bug added the feature that using `ROUTER_BACKEND_PROCESS_ENDPOINTS=shuffle`
will randomize the order of back-ends in the HAProxy configuration. With long
running sessions and a router that reloads regularly, the first endpoint in the
configuration may receive significantly more load than other back-ends. Setting
the environment variable will randomize the order of the back-ends on every
reload and, thus, help spread the load.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1447115[*BZ#1447115*])

*Storage*

* When an OpenShift node crashed before unmapping a RBD volume, the advisory lock
held on the RBD volume was not released. This prevented other nodes from using
the RBD volume till the advisory lock is manually removed. Now, if no RBD client
is using the RBD volume, the advisory lock is removed automatically. Thus, the
RBD volume can be used by other nodes without manually removing the lock.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1365867[*BZ#1365867*])

* Attach operations on AWS were slow because of duplicate API calls and frequent
polling of volume information. In the latest version, the duplicate API calls
are removed from the code and bulk polling of AWS volumes is implemented, to
avoid API quota problems.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1392357[*BZ#1392357*])

* For persistent volumes, the default mount options provided by OpenShift were not
customizable. Users can now tweak mount options for persistent volumes
(including NFS and other volume types that support it) depending on their
storage configuration.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1405307[*BZ#1405307*])

* The `recycle` reclaim policy is deprecated in favor of dynamic provisioning and
it will be removed in future releases.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1432281[*BZ#1432281*])

* OpenStack Cinder v1 API got deprecated in recent OpenStack release. OpenShift
now supports OpenStack v2 API.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1427701[*BZ#1427701*])

* In kubelet logs, a running pod was sometimes reported as _'cannot start, time
out waiting for the volume'_. Because the kubelet's volumemanager reconstructor
for actual state of world was running before the desired state of world was
populated, which caused the pods in the actual state of world, to have incorrect
volume information. This issue is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1444096[*BZ#1444096*])

* The OpenStack Cinder StorageClass ignored availability zones because of an issue
in the `gophercloud/gophercloud` library. OpenStack Cinder StorageClass now
provisions volumes in the specified availability zone and fails if the specified
availability zone does not exist.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1444828[*BZ#1444828*])

* When mounting volumes using a subpath, the subpath did not receive correct
permissions. This issue is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1445526[*BZ#1445526*])

* Volumes failed to detach after unmounting them from the node. Because Openshift
did not attempt detach operation for pods that were completed (or terminated)
but were not deleted from API server. Thus preventing reuse of volume in other
pods. This bug is fixed and volumes for terminated or completed pods are
detached automatically.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1446788[*BZ#1446788*])

* If the availability optional parameter was not provided for the OpenStack Cinder
StorageClass, all Persistent Volumes provisioned for the Persistent Volume
Claims that used the specified StorageClass were provisioned in the `nova` zone.
Now, such Persistent Volumes are provisioned in an active zone where OpenShift
has a node.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1447568[*BZ#1447568*])

* Pods failed to start, if they specified a file as a volume subPath to mount.
This is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1451008[*BZ#1451008*])

* OpenShift failed to attach disks to the Azure F-Series VMs. This issue is now
fixed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1451039[*BZ#1451039*])

* Previously, when a node stopped (or rebooted) the ones using EBS volumes were
failing because the volume was not detached from the stopped node. Now the
volume gets successfully unmounted and detached from node.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1457510[*BZ#1457510*])

* High OpenShift process CPU utilization is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1460280[*BZ#1460280*])

* Previously, the `AccessModes` field of a PVC was ignored when a PV was
dynamically provisioned for it. This caused users to receive a PV with
inaccurate `AccessModes`. Now the dynamic provisioning of PVs with inaccurate
`AccessModes` are not provisioned when PVCs ask for `AccessModes` that can't be
satisfied by the PVs' underlying volume plugin.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1462275[*BZ#1462275*])

* Dynamically created Azure blob containers were accessible on public internet.
 This happened because the default access permissions for Persistent Volumes
 (PVs) were set to `container` which exposed a publically accessible URI. The
 container permissions are now set to `private` for provisioned Azure Volumes.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1462551[*BZ#1462551*])

* Sometimes, even after the PV and volume are proivisioned successfully, there was
a failed volume creation event in the logs. This issue is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1395547[*BZ#1395547*])

* This bug made it possible to specify multiple `targetPortals` to make use of
 iSCSI multipath, which is the de-facto standard in environments that use iSCSI.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1376022[*BZ#1376022*])

*Upgrades*

* Previously, when 3.1 version of `etcd` was available, the `etcd` RPM did not get
upgraded to the version during the control plane upgrade. The playbook
responsible for `etcd` upgrading is now extended and the `etcd` RPM (and `etcd`
docker images) are properly upgraded to `3.1.*`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1421587[*BZ#1421587*])

* To minimize the attack surface for containers escaping namespace isolation, the
label `svirt_sandbox_file_t` on `/var/lib/origin/openshift.local.volumes/` was
removed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1450167[*BZ#1450167*])

* Previously, when named certificates were added to ansible hosts file, and
 certificate redeploy playbook was run, certificates were not added to
 `master-config.yaml`. This issue is now fixed.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1454478[*BZ#1454478*])

* Previously, the certificate redeployment playbook would not update master
configuration when named certificates were provided. Named certificates will now
be replaced and master configuration will be updated during certificate
redeployment.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1455485[*BZ#1455485*])

* The OpenShift upgrade got applied to all nodes if
`openshift_upgrade_nodes_label` fits no label. Now the installer verifies the
provided label and matches a set of hosts prior to upgrading.  If the label does
not match hosts, the upgrade would silently proceed with upgrading all nodes
given the logic for creating the `oo_nodes_to_upgrade` group.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1462992[*BZ#1462992*])

* If the version of etcd used to produce the etcd backup was version 3.x the
backup can only be loaded by etcd 3.x. This occurs when running etcd in a
containerized install and the version of the rpm installed on the host differs
from that running inside the container. We have updated the backup playbooks to
use the version of etcd from within the container which ensures that a matching
version of etcd is used.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1402771[*BZ#1402771*])

* Previously, the registry-console would use an older image version even after
upgrade. Since registry-console was installed by default, the upgrade playbook
did not update the registry-console deployment configuration to use the same
version as docker-registry. This issue is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1421987[*BZ#1421987*])

[[ocp-36-technology-preview]]
== Technology Preview Features

Some features in this release are currently in Technology Preview. These
experimental features are not intended for production use. Please note the
following scope of support on the Red Hat Customer Portal for these features:

https://access.redhat.com/support/offerings/techpreview[Technology Preview
Features Support Scope]

The following new features are now available in Technology Preview:

- xref:../admin_guide/quota.adoc#limited-resources-quota[Require Explicit Quota to Consume a Resource]
- xref:../architecture/additional_concepts/storage.adoc#pv-mount-options[Mount Options]
- xref:ocp-36-automated-installation-of-cloudforms[Automated installation of CloudForms 4.5 Inside OpenShift]
- xref:../install_config/install/advanced_install.adoc#advanced-install-configuring-system-containers[Installation of etcd, Docker Daemon, and Ansible Installer as System Containers]
- xref:ocp-36-running-openshift-installer-as-a-system-container[Running OpenShift Installer as a System Container]
- xref:../architecture/service_catalog/index.adoc#architecture-additional-concepts-service-catalog[Service
Catalog]
- xref:ocp-36-ansible-service-broker[Ansible Service Broker]
- xref:ocp-36-ansible-playbook-bundles[Ansible Playbook Bundles (APB)]
- xref:ocp-36-initial-experience[Initial Experience]
- xref:ocp-36-search-catalog[Search Catalog]
- xref:ocp-36-add-from-catalog[Add from Catalog]
- xref:ocp-36-add-to-project[Add to Project]
- xref:ocp-36-bind-in-context[Bind in Context]
- xref:ocp-36-template-service-broker[Template Service Broker]
- xref:ocp-36-service-catalog-experience[Service Catalog Experience in the CLI]
- xref:ocp-36-mux[`mux`]

The following features that were formerly in Technology Preview from a previous
{product-title} release are now fully supported:

- xref:../architecture/core_concepts/containers_and_images.adoc#init-containers[Init
containers]

The following features that were formerly in Technology Preview from a previous
{product-title} release remain in Technology Preview:

- xref:../dev_guide/cron_jobs.adoc#dev-guide-cron-jobs[Cron Jobs (formerly called Scheduled Jobs)]
- xref:../admin_guide/managing_networking.adoc#admin-guide-manage-networking[Network Policy]
- xref:../dev_guide/deployments/kubernetes_deployments.adoc#dev-guide-kubernetes-deployments-support[Kubernetes
Deployments Support]
- xref:../admin_guide/managing_pods.adoc#managing-pods-poddisruptionbudget[Pod Distribution Budgets]
- xref:..//release_notes/ocp_3_5_release_notes.adoc#ocp-35-statefulsets[`StatefulSets` formerly known as `PetSets`]

[[ocp-36-known-issues]]
== Known Issues

* When running an upgrade with ``--tags pre_upgrade`, the upgrade failed with:
+
----
"the file_name '/usr/share/ansible/openshift-ansible/playbooks/common/openshift-cluster/upgrades/etcd/noop.yml' does not exist, or is not readable"
----
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1464025[*BZ#1464025*])

* When de-provisioning an Ansible Playbook Bundle (APB) via the
 `service-catalog` and `ansible-service-broker`, the *Provisioned Service* entry
 will linger for longer than the assets created by the APB. The service itself
 will have correctly been de-provisioned. This is due to the service catalog
 eventually confirming with the Ansible Service Broker that the service is
 actually gone. It was
 link:https://github.com/kubernetes-incubator/service-catalog/pull/1067[patched
 upstream].
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1475251[*BZ#1475251*])

* The Ansible playbooks for running OpenShift pre-installation and health checks
may have unexpected side-effects, as they have dependencies on code from the
installer for configuring hosts. This may result in changes to configuration for
yum repositories, Docker, or the firewall for hosts where configuration differs
from the settings specified by the Ansible inventory. Therefore, users should
avoid running these playbooks with an inventory configuration that could result
in changing the cluster in these areas.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1476890[*BZ#1476890*])

* When upgrading from a release of {product-title} less than 3.5.5.18, the upgrade
process may remove data on persistent volumes that fail to unmount correctly. If
you are running a version less than 3.5.5.18, perform the following steps prior
to performing the normal upgrade process:
+
----
# atomic-openshift-excluder unexclude
# yum upgrade atomic-openshift-node
# systemctl restart atomic-openshift-node
----
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1463393[*BZ#1463393*])

* In {product-title} 3.5 and earlier, the Fluentd image included
`fluent-plugin-elasticsearch` version 1.9.2 and earlier. This version will
silently drop records sent in a bulk index request when the queue size is
link:https://github.com/uken/fluent-plugin-elasticsearch/blob/v1.9.2/lib/fluent/plugin/out_elasticsearch.rb#L353[full].
In {product-title} 3.6, which uses version 1.9.5, an error log message was
added, which is why the `Error: status=429` message in the Fluentd logs
link:https://github.com/uken/fluent-plugin-elasticsearch/blob/v1.9.5/lib/fluent/plugin/out_elasticsearch.rb#L355[occurs].
+
To reduce the frequency of this problem, you can increase the Fluentd buffer
chunk size. However, testing does not give consistent results.  You will need to
stop, configure, and restart Fluentd running on all of your nodes.

. Edit the daemonset:
+
----
# oc edit -n logging daemonset logging-fluentd
----

. In the `env:` section, look for `BUFFER_SIZE_LIMIT`. If the value is less than
`8Mi` (8 megabytes), change the value to `8Mi`. Otherwise, use a value of `16Mi`
or `32Mi`. This will roughly increase the size of each bulk index request, which
should decrease the number of such requests made to Elasticsearch, thereby
allowing Elasticsearch to process them more efficiently.

. Once the edit is saved, the Fluentd daemonset trigger should cause a restart of
all of the Fluentd pods running in the cluster.
+
You can monitor the Elasticsearch bulk index thread pool to see how many bulk
index requests it processes and rejects.

. Get the name of an Elasticsearch pod:
+
----
# oc get -n logging pods -l component=es

# espod=$name_of_es_pod
----

. Run the following command:
+
----
# oc exec -n logging $espod -- \
  curl -s -k --cert /etc/elasticsearch/secret/admin-cert \
  --key /etc/elasticsearch/secret/admin-key \
https://localhost:9200/_cat/thread_pool?v\&h=host,bulk.completed,bulk.rejected,bulk.queue,bulk.active,bulk.queueSize
----
+
The output looks like this:
+
----
host       bulk.completed bulk.rejected bulk.queue bulk.active bulk.queueSize
10.128.0.6           2262             0          0           0             50
----
+
The term `completed` means the number of bulk indexing operations that have been
completed. There will be many (hundreds or thousands of) log records per bulk
index request.
+
The term `queue` is the number of pending requests that have been queued up for
the server to process. Once this queue is full, additional operations are
rejected.
+
Note the number of `bulk.rejected` operations. These correspond to `error
status=429` in your Fluentd pod logs. Rejected operations means that Fluentd
dropped these records, and you might need to increase the chunk size again.
+
If you have multiple nodes running Elasticsearch, they will each be listed in
the `curl` output.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1470862[*BZ#1470862*])

* When performing builds using an image source input, directories within the input
content are injected with permissions of 0700 with the default image user as the
owner. This means the content is unlikely to be accessible when the application
image is run under a random UID. This can be worked around by performing a
`chmod` operation in either the assemble script (for S2I builds) or the
Dockerfile (for Docker builds). Most {product-title} S2I builder images already
perform this `chmod` operation, but custom built S2I builder images or builds
using custom assemble scripts may not.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1479130[*BZ#1479130*])

* There is a known issue affecting logging behavior when using the non-default
`json-file` log driver. As a workaround, remove line
`/var/lib/docker/containers` from *_/etc/oci-umount.conf_*, then restart
`docker`, OpenShift services, and the Fluentd  pod.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1477787[*BZ#1477787*])

[[ocp-36-asynchronous-errata-updates]]
== Asynchronous Errata Updates

Security, bug fix, and enhancement updates for {product-title} 3.6 are released
as asynchronous errata through the Red Hat Network. All {product-title} 3.6
errata is https://access.redhat.com/downloads/content/290/[available on the Red
Hat Customer Portal]. See the
https://access.redhat.com/support/policy/updates/openshift[{product-title}
Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account
settings for Red Hat Subscription Management (RHSM). When errata notifications
are enabled, users are notified via email whenever new errata relevant to their
registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming
{product-title} entitlements for {product-title} errata notification
emails to generate.
====

This section will continue to be updated over time to provide notes on
enhancements and bug fixes for future asynchronous errata releases of
{product-title} 3.6. Versioned asynchronous releases, for example with the form
{product-title} 3.6.z, will be detailed in subsections. In addition, releases in
which the errata text cannot fit in the space provided by the advisory will be
detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[upgrading your cluster] properly.
====
