[[release-notes-ocp-3-6-release-notes]]
= {product-title} 3.6 Release Notes
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

Red Hat {product-title} is a Platform as a Service (PaaS) that provides
developers and IT organizations with a cloud application platform for deploying
new applications on secure, scalable resources with minimal configuration and
management overhead. {product-title} supports a wide selection of
programming languages and frameworks, such as Java, Ruby, and PHP.

Built on Red Hat Enterprise Linux and Google Kubernetes, {product-title}
provides a secure and scalable multi-tenant operating system for today’s
enterprise-class applications, while providing integrated application runtimes
and libraries. {product-title} brings the OpenShift PaaS platform to customer
data centers, enabling organizations to implement a private PaaS that meets
security, privacy, compliance, and governance requirements.

[[ocp-36-about-this-release]]
== About This Release

Red Hat {product-title} version 3.6
(link:https://access.redhat.com/errata/RHBA-2017:1716[RHBA-2017:1716]) is now
available. This release is based on
link:https://github.com/openshift/origin/releases/tag/v3.6.0-rc.0[OpenShift
Origin 3.6]. New features, changes, bug fixes, and known issues that pertain to
{product-title} 3.6 are included in this topic.

{product-title} 3.6 is supported on RHEL 7.3 and newer with the latest packages
from Extras, including Docker 1.12.

For initial installations, see the
xref:../install_config/install/planning.adoc#install-config-install-planning[Installing
a Cluster] topics in the
xref:../install_config/index.adoc#install-config-index[Installation and
Configuration] documentation.

To upgrade to this release from a previous version, see the
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[Upgrading
a Cluster] topics in the
xref:../install_config/index.adoc#install-config-index[Installation and
Configuration] documentation.

[[ocp-36-new-features-and-enhancements]]
== New Features and Enhancements

This release adds improvements related to the following components and concepts.

[[ocp-36-container-orchestration]]
=== Container Orchestration

[[ocp-36-kubernetes-upstream]]
==== Kubernetes Upstream

Many core features announced in March for Kubernetes 1.6 were the result of
{product-title} engineering. Red Hat continues to influence the product in the
areas of storage, networking, resource management, authentication and
authorization, multi-tenancy, security, service deployments and templating, and
controller functionality.

[[ocp-36-cri-interface]]
==== CRI Interface for Kublet-to-Docker Interaction

{product-title} now uses the CRI interface for kublet-to-Docker interaction.

As the container space matures and choices become more available,
{product-title} needs an agnostic interface in Kubernetes for container runtime
interactions. {product-title} 3.6 switches the default configuration to use the
Kubernetes Docker CRI interface.

There is a `enable-cri` setting in the *_node-config.yaml_* configuration file.  A
value of `true` enables the use of the interface. Change it by editing the
file and stopping or starting the `atomic-openshift-node.service`.

----
$ cat /etc/origin/node/node-config.yaml
enable-cri:
  - 'true'
----

[NOTE]
====
Although the Docker CRI is stable and the default, the overall CRI interface in
Kubernetes is still under development. Red Hat does not support crio, rkt, or
frakti in this {product-title} 3.6 release.
====

[[ocp-36-cluster-capacity-utility]]
==== Cluster Capacity Utility for Checking True Allocatable Space

Just like a disk drive, a cluster can become fragmented over time. When you ask
the cluster how much space is left, the addition of all the free space does not
indicate how many actual workloads can run. For example, it might say there is
10 GB left, but it could be that no single node can take more than 512 MB.

{product-title} 3.6 introduces a new container that you can launch as a command line
or a job.  The container allows you to supply a popular workload (image) with a
commonly requested CPU and MEM limit and request.  The logs from the container
will tell you how many of that workload can be deployed.

See
xref:../admin_guide/cluster_capacity.adoc#admin-guide-cluster-capacity[Analyzing
Cluster Capacity] for more information.

[[ocp-36-quota-remote-storage]]
==== Quota on How Much (Size and Type) Remote Storage a Project Can Use

You can now control what classes of storage projects are allowed to access, how
much (total size) of that class, as well as how many claims.

This feature leverages the `ResourceQuota` object and allows you to call out
storage classes by name for size and claim settings.

----
$ oc create quota my-quota-1 --hard=slow.storageclass.storage.k8s.io/requests.storage=20Gi,slow.storageclass.storage.k8s.io/persistentvolumeclaims=15

$ oc describe quota my-quota-1
Name:                                                     my-quota-1
Namespace:                                                 default
Resource                                                 Used   Hard
--------                                                     ----        ---
slow.storageclass.storage.k8s.io/persistentvolumeclaims    0    15
slow.storageclass.storage.k8s.io/requests.storage                0    20Gi
----

See xref:../admin_guide/quota.adoc#limited-resources-quota[Require Explicit Quota
to Consume a Resource] for more information.

[[ocp-36-scope-PVC-quotas-by-storage-class]]
==== Ability to Scope PVC Quotas by Storage Class

In {product-title} 3.6, administrators now have the ability to specify a
separate quota for persistent volume claims (PVCs) and `requests.storage` per
storage class.

See xref:../admin_guide/quota.adoc#admin-guide-quota[Setting Quotas] for more
information.

[[ocp-36-project-configmaps-secrets-downward-api-in-same-directory]]
==== Project ConfigMaps, Secrets, and Downward API In the Same Directory

When you mount a memory backed volume into a container, it leverages a
directory. Now, you can place all sources of the configuration for your
application (`configMaps`, secrets, and downward API) into the same directory
path.

The new projected line in the volume definition allows you to tell multiple
volumes to leverage the same mount point while guarding for path collisions.

----
volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: test-secret
          items:
            - key: data-1
              path: mysecret/my-username
            - key: data-2
              path: mysecret/my-passwd

      - downwardAPI:
          items:
            - path: mydapi/labels
              fieldRef:
                fieldPath: metadata.labels
            - path: mydapi/name
              fieldRef:
                fieldPath: metadata.name
            - path: mydapi/cpu_limit
              resourceFieldRef:
                containerName: allinone-normal
                resource: limits.cpu
                divisor: "1m"

                - configMap:
                    name: special-config
                    items:
                      - key: special.how
                        path: myconfigmap/shared-config
                      - key: special.type
                        path: myconfigmap/private-config
----

[[ocp-36-init-containers]]
==== Init Containers

You run
xref:../architecture/core_concepts/containers_and_images.adoc#init-containers[init
containers] in the same pod as your application container to create the
environment your application requires or to satisfy any preconditions the
application might have. You can run utilities that you would otherwise need to
place into your application image. You can run them in different file system
namespaces (view of the same file system) and offer them different secrets than
your application container.

Init containers run to completion and each container must finish before the next
one starts. The init containers will honor the restart policy. Leverage
`initContainers` in the `podspec`.

----
$ cat init-containers.yaml
apiVersion: v1
kind: Pod
metadata:
  name: init-loop
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - name: workdir
      mountPath: /usr/share/nginx/html
  initContainers:
  - name: init
    image: centos:centos7
    command:
    - /bin/bash
    - "-c"
    - "while :; do sleep 2; echo hello init container; done"
  volumes:
  - name: workdir
    emptyDir: {}
----

----
$ oc get -f init-containers.yaml
NAME        READY     STATUS     RESTARTS   AGE
nginx       0/1       Init:0/1   0          6m
----

[[ocp-36-multiple-schedulers-at-the-same-time]]
====  Multiple Schedulers at the Same Time

Kubernetes now supports extending the default scheduler implementation with
custom schedulers.

After
link:https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/[configuring
and deploying] your new scheduler, you can call it by name from the `podspec`
via `schedulerName`. These new schedulers are packaged into container images and
run as pods inside the cluster.

----
$ cat pod-custom-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  name: custom-scheduler
spec:
  schedulerName: custom-scheduler
  containers:
  - name: hello
    image: docker.io/ocpqe/hello-pod
----

See xref:../admin_guide/scheduling/index.adoc#admin-guide-scheduling-index[Scheduling] for
more information.

[[ocp-36-turn-configmap-content-into-environment-variables]]
==== Turn ConfigMap Content into Environment Variables within the Container

Instead of individually declaring environment variables in a pod definition, a
`configMap` can be imported and all of its content can be dynamically turned
into environment variables.

In the pod specification, leverage the `envFrom` object and reference the
desired `configMap`:

----
env:
- name: duplicate_key
  value: FROM_ENV
- name: expansion
  value: $(REPLACE_ME)
envFrom:
- configMapRef:
    name: env-config
----

See xref:../dev_guide/configmaps.adoc#dev-guide-configmaps[`ConfigMaps`] for more
information.

[[ocp-36-node-affinity-and-anti-affinity]]
==== Node Affinity and Anti-affinity

Control which nodes your workload will land on in a more generic and powerful
way as compared to `nodeSelector`.

`NodeSelectors` provide a powerful way for a user to specify which node a
workload should land on. However, If the selectors are not available or are
conflicted, the workload will not be scheduled at all. They also require a user
to have specific knowledge of node label keys and values. Operators provide a
more flexible way to select nodes during scheduling.

Now, you can
link:http://blog.kubernetes.io/2017/03/advanced-scheduling-in-kubernetes.html[select
the label value] you would like the operator to compare against (for example,
`In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt`, and `Lt`).  You can choose to
make satisfying the operator required or preferred. Preferred means search for
the match, but, if you can not find one, ignore it.

----
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: "failure-domain.beta.kubernetes.io/zone"
            operator: In
            values: ["us-central1-a"]
----

----
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: "failure-domain.beta.kubernetes.io/zone"
            operator: NotIn
            values: ["us-central1-a"]
----


See
xref:../admin_guide/scheduling/node_affinity.adoc#admin-guide-sched-affinity[Advanced
Scheduling and Node Affinity] for more information.

[[ocp-36-pod-affinity-and-anti-affinity]]
==== Pod Affinity and Anti-Affinity

Pod affinity and anti-affinity is helpful if you want to allow Kubernetes the
freedom to select which zone an application lands in, but whichever it chooses
you would like to make sure another component of that application lands in the
same zone.

Another use case is if you have two application components that, due to security
reasons, cannot be on the same physical box. However, you do not want to lock
them into labels on nodes. You want them to land anywhere, but still honor
anti-affinity.

Many of the same high-level concepts mentioned in the node affinity and
anti-affinity hold true here. For pods, you declare a
link:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature[`topologyKey`],
which will be used as the boundary object for the placement logic.

----
affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: service
            operator: In
            values: [“S1”]
        topologyKey: failure-domain.beta.kubernetes.io/zone


affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: service
            operator: In
            values: [“S1”]
        topologyKey: kubernetes.io/hostname
----

See
xref:../admin_guide/scheduling/pod_affinity.adoc#admin-guide-sched-pod-affinity[Advanced
Scheduling and Pod Affinity and Anti-affinity] for more information.

[[ocp-36-taints-and-tolerations]]
==== Taints and Tolerations

xref:../admin_guide/scheduling/taints_tolerations.adoc#admin-guide-taints[Taints
and tolerations] allow the *node* to control which *pods* should (or should
not) be scheduled on them.

A _taint_ allows a node to refuse pod to be scheduled unless that pod has a
matching _toleration_.

You apply taints to a node through the node specification (`NodeSpec`) and apply
tolerations to a pod through the pod specification (`PodSpec`). A taint on a
node instructs the node to repel all pods that do not tolerate the taint.

Taints and tolerations consist of a key, value, and effect. An operator allows
you to leave one of these parameters empty.

In {product-title} 3.6, daemon pods do respect taints and tolerations, but they
are created with `NoExecute` tolerations for the
`node.alpha.kubernetes.io/notReady` and `node.alpha.kubernetes.io/unreachable`
taints with no `tolerationSeconds`. This ensures that when the
`TaintBasedEvictions` alpha feature is enabled, they will not be evicted when
there are node problems such as a network partition. (When the
`TaintBasedEvictions` feature is not enabled, they are also not evicted in these
scenarios, but due to hard-coded behavior of the `NodeController` rather than
due to tolerations).

Set the taint from the command line:

----
$ oc taint nodes node1 key=value:NoSchedule
----

Set toleration in the `PodSpec`:

----
tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"
----

[[ocp-36-using-image-streams-with-kubernets-resources]]
==== Using Image Streams with Kubernetes Resources (Technology Preview)

This feature is currently in xref:ocp-36-technology-preview[Technology Preview]
and not for production workloads.

{product-title} has long offered easy integration between continuous integration
pipelines that create deployable Docker images and automatic redeployment and
rollout with `DeploymentConfigs`. This makes it easy to define a standard
process for continuous deployment that keeps your application always running. As
new, higher level constructs like deployments and `StatefulSets` have reached
maturity in Kubernetes, there was no easy way to leverage them and still
preserve automatic CI/CD.

In addition, the image stream concept in {product-title} makes it easy to
centralize and manage images that may come from many different locations, but to
leverage those images in Kubernetes resources you had to provide the full
registry (an internal service IP), the namespace, and the tag of the image,
which meant that you did not get the ease of use that `BuildConfigs` and
`DeploymentConfigs` offer by allowing direct reference of an image stream tag.

Starting in {product-title} 3.6, we aim to close that gap both by making it as
easy to trigger redeployment of Kubernetes Deployments and `StatefulSets`, and
also by allowing Kubernetes resources to easily reference {product-title} image
stream tags directly.

See xref:../dev_guide/managing_images.adoc#using-is-with-k8s[Using Image Streams
with Kubernetes Resources] for more information.

[[ocp-36-registry]]
=== Registry

[[ocp-36-validating-image-signatures-show-appropriate-metadata]]
==== Validating Image Signatures Show Appropriate Metadata

When working with image signatures as the `image-admin` role, you can now see
the status of the images in terms of their signatures.

You can now use the `oc adm verify-image-signature` command to save or remove
signatures. The resulting `oc describe istag` displays additional metadata about
the signature’s status.

----
$ oc describe istag origin-pod:latest
Image Signatures:
  Name: sha256:c13060b74c0348577cbe07dedcdb698f7d893ea6f74847154e5ef3c8c9369b2c@f66d720cfaced1b33e8141a844e793be
  Type: atomic
  Status: Unverified

# Verify the image and save the result back to image stream
$ oc adm verify-image-signature sha256:c13060b74c0348577cbe07dedcdb698f7d893ea6f74847154e5ef3c8c9369b2c \
  --expected-identity=172.30.204.70:5000/test/origin-pod:latest --save --as=system:admin
sha256:c13060b74c0348577cbe07dedcdb698f7d893ea6f74847154e5ef3c8c9369b2c signature 0 is verified (signed by key: "172B61E538AAC0EE")

# Check the image status
$ oc describe istag origin-pod:latest
Image Signatures:
  Name:   sha256:c13060b74c0348577cbe07dedcdb698f7d893ea6f74847154e5ef3c8c9369b2c@f66d720cfaced1b33e8141a844e793be
  Type:   atomic
  Status:   Verified
  Issued By:  172B61E538AAC0EE
  Signature is Trusted (verified by user "system:admin" on 2017-04-28 12:32:25 +0200 CEST)
  Signature is ForImage ( on 2017-04-28 12:32:25 +0200 CEST)
----

See xref:../admin_guide/image_signatures.adoc#admin-guide-image-signatures[Image
Signatures] and
xref:../install_config/install/host_preparation.adoc#enabling-image-signature-support[Enabling
Image Signature Support] for more information.

[[ocp-36-registry-rest-endpoint-for-reading-writing-image-signatures]]
==== Registry REST Endpoint for Reading and Writing Image Signatures

There is now a programmable way to read and write signatures using only the
docker registry API.

To read, you must be authenticated to the registry.

----
PUT /extensions/v2/{namespace}/{name}/signatures/{digest}
$ curl http://<user>:<token>@<registry-endpoint>:5000/extensions/v2/<namespace>/<name>/signatures/sha256:<digest>

JSON:
{
  "version": 2,
  "type":    "atomic",
  "name":    "sha256:4028782c08eae4a8c9a28bf661c0a8d1c2fc8e19dbaae2b018b21011197e1484@cddeb7006d914716e2728000746a0b23",
  "content": "<base64 encoded signature>",
}
----

To write, you must have the `image-signer` role.

----
GET /extensions/v2/{namespace}/{name}/signatures/{digest}
$ curl http://<user>:<token>@<registry-endpoint>:5000/extensions/v2/<namespace>/<name>/signatures/sha256:<digest>


{
  "signatures": [
  {
    "version": 2,
    "type":    "atomic",
    "name":    "sha256:4028782c08eae4a8c9a28bf661c0a8d1c2fc8e19dbaae2b018b21011197e1484@cddeb7006d914716e2728000746a0b23",
    "content": "<base64 encoded signature>",
  }
  ]
}
----

[[ocp-36-platform-management]]
=== Platform Management

[[ocp-36-require-explicit-quota-to-consume-a-resource]]
==== Require Explicit Quota to Consume a Resource (Technology Preview)

This feature is currently in xref:ocp-36-technology-preview[Technology Preview]
and not for production workloads.

If a resource is not managed by quota, a user has no restriction on the amount
of resource that can be consumed. For example, if there is no quota on storage
related to the gold storage class, the amount of gold storage a project can
create is unbounded.

See xref:../admin_guide/quota.adoc#limited-resources-quota[Setting Quotas] for
more information.

[[ocp-36-storage]]
=== Storage

[[ocp-36-aws-efs-provisioner]]
==== AWS EFS Provisioner

The AWS EFS provisioner allows you to dynamically use the AWS EFS endpoint to
get NFS remote persistent volumes on AWS.

It leverages the
link:https://kubernetes.io/docs/concepts/storage/persistent-volumes/#provisioner[external
dynamic provisioner interface]. It is provided as a `docker` image that you
configure with a `configMap` and deploy on {product-title}. Then, you can use a
storage class with the appropriate configuration.

.Storage Class Example
----
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: slow
provisioner: foobar.io/aws-efs
parameters:
  gidMin: "40000"
  gidMax: "50000"
----
`gidMin` and `gidMax` are the minimum and maximum values, respectively, of the
GID range for the storage class. A unique value (GID) in this range (`gidMin` to
`gidMax`) is used for dynamically provisioned volumes.

[[ocp-36-vmware-vsphere-storage]]
==== VMware vSphere Storage

VMware vSphere storage allows you to dynamically use the VMware vSphere storage
options ranging from VSANDatastore, ext3, vmdk, and VSAN while honoring vSphere
Storage Policy (SPBM) mappings.

VMware vSphere storage leverages the cloud provider interface in Kubernetes to
trigger this in-tree dynamic storage provisioner. Once the cloud provider has
the correct credential information, tenants can leverage storage class to select
the desired storage.

.Storage Class Example
----
kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
  name: fast
provisioner: kubernetes.io/vsphere-volume
parameters:
    diskformat: zeroedthick
----

See xref:../install_config/configuring_vsphere.adoc#install-config-configuring-vsphere[Configuring for VMWare vSphere] and xref:../install_config/persistent_storage/persistent_storage_vsphere.adoc#install-config-persistent-storage-persistent-storage-vsphere[Persistent Storage Using VMWare vSphere Volume] for more information.

[[ocp-36-increased-security-with-iscsi-chap-mount-operations]]
==== Increased Security with iSCSI CHAP and Mount Operations

You can now use CHAP authentication for your iSCSI remote persistent volumes (PVs).
Also, you can annotate your PVs to leverage any mount options that are supported
by that underlying storage technology.

The tenant supplies the correct user name and password for the CHAP
authentication as a secret in their `podspec`. For mount options, you supply the
annotation in the PV.

----
volumes:
  - name: iscsivol
    iscsi:
      targetPortal: 127.0.0.1
      iqn: iqn.2015-02.example.com:test
      lun: 0
      fsType: ext4
      readOnly: true
      chapAuthDiscovery: true
      chapAuthSession: true
      secretRef:
         name: chap-secret
----

Set `volume.beta.kubernetes.io/mount-options` to
`volume.beta.kubernetes.io/mount-options: rw,nfsvers=4,noexec`.

See xref:../architecture/additional_concepts/storage.adoc#pv-mount-options[Mount
Options] for more information.

[[ocp-36-mount-options]]
==== Mount Options (Technology Preview)

Mount Options are currently in xref:ocp-36-technology-preview[Technology
Preview] and not for production workloads.

You can now specify mount options while mounting a persistent volume by using
the annotation `volume.beta.kubernetes.io/mount-options`

See
xref:../architecture/additional_concepts/storage.adoc#pv-mount-options[Persistent
Storage] for more information.

[[ocp-36-improved-automated-support-for-cns-backed-ocp-hosted-registry]]
====  Improved and Fully Automated Support for CNS-backed OCP Hosted Registry

Previously, only a few supported storage options existed for a scaled,
highly-available integrated {product-title} (OCP) registry. Automated container
native storage (CNS) 3.6 and the {product-title} installer now include an option
to automatically deploy a scale-out registry based on highly available storage,
out of the box. When enabled in the installer’s inventory file, CNS will be
deployed on a desired set of nodes (for instance, infrastructure nodes). Then,
the required underlying storage constructs will automatically be created and
configured for use with the deployed registry. Moving an existing registry
deployment from NFS to CNS is also supported, and requires additional steps for
data migration.

Backing the {product-title} registry with CNS enables users to take advantage of
the globally available storage capacity, strong read/write consistency,
three-way replica, and RHGS data management features.

The feature is provided through integrations in the {product-title}
xref:../install_config/install/advanced_install.adoc#advanced-install-containerized-glusterfs-backed-registry[advanced
installation] process. A few dedicated storage devices and a simple change to
the inventory file is all that is required.

[[ocp-36-ocp-commerical-evaluation-subscription-includes-cns-crs]]
==== {product-title} Commercial Evaluation Subscription Includes CNS and CRS

The OpenShift Commercial Evaluation subscription includes container native
storage (CNS), container ready storage (CRS) solutions.

The OpenShift Commercial Evaluation subscription SKU bundles the CNS and CRS
features, with additional entitlements to evaluate {product-title} with CNS/CRS.

[IMPORTANT]
====
Evaluation SKUs are not bundled with {product-title}'s SKUs or entitlements.
Consult your Red Hat account representative for subscription guidance.
====

[[ocp-36-scale]]
=== Scale

[[ocp-36-updated-etcd-performance-guidance]]
==== Updated etcd Performance Guidance

See
xref:../scaling_performance/host_practices.adoc#scaling-performance-capacity-host-practices[Recommended
Host Practices] for updated etcd performance guidance.

[[ocp-36-updated-sizing-guidance]]
==== Updated Sizing Guidance

In {product-title} 3.6 , the
xref:../install_config/install/planning.adoc#sizing[maximum number of nodes per
cluster] is 2000.

[[ocp-36-networking]]
=== Networking

[[ocp-36-multiple-destinations-in-egress-router]]
==== Multiple Destinations in egress-router

{product-title} 3.6 introduces the ability to connect to multiple destinations
from a project without needing to reserve a separate source IP for each of them.
Also, there is now an optional fallback IP. Old syntax continues to behave the
same and there is no change to `EGRESS_SOURCE` and `EGRESS_GATEWAY` definitions.

Old way:

----
- name: EGRESS_DESTINATION
  value: 203.0.113.25
----

New way:

----
- name: EGRESS_DESTINATION
  value: |
    80 tcp 1.2.3.4
    8080 tcp 5.6.7.8 80
    8443 tcp 9.10.11.12 443
    13.14.15.16
----

----
localport  udp|tcp  dest-ip [dest-port]
----

See
xref:../admin_guide/managing_networking.adoc#admin-guide-manage-networking[Managing
Networking] for more information.

[[ocp-26-added-http-proxy-mode-for-egress-router]]
==== Added HTTP Proxy Mode for the Egress Router

TLS connections (certificate validations) do not easily work because the client
needs to connect to the egress router's IP (or name) rather than to the
destination server's IP/name. Now, the egress router can be run as a proxy
rather than just redirecting packets.

How it works:

. Create a new project and pod.

. Create the `egress-router-http-proxy` pod.

. Create the service for `egress-router-http-proxy`.

. Set up `http_proxy` in the pod:
+
----
# export http_proxy=http://my-egress-router-service-name:8080
# export https_proxy=http://my-egress-router-service-name:8080
----

. Test and check squid headers in response:
+
----
$ curl  -ILs http://www.redhat.com
$ curl  -ILs https://rover.redhat.com
    HTTP/1.1 403 Forbidden
    Via: 1.1 egress-http-proxy (squid/x.x.x)
$ curl  -ILs http://www.google.com
    HTTP/1.1 200 OK
    Via: 1.1 egress-http-proxy (squid/x.x.x)
$ curl  -ILs https://www.google.com
    HTTP/1.1 200 Connection established
    HTTP/1.1 200 OK
----

See
xref:../admin_guide/managing_networking.adoc#admin-guide-manage-networking[Managing
Networking] for more information.

[[ocp-36-use-dns-names-with-egress-firewall]]
==== Use DNS Names with Egress Firewall

There are several benefits of using DNS names versus IP addresses:

- It tracks DNS mapping changes.
- Human-readable, easily remembered naming.
- Potentially backed by multiple IP addresses.

How it works:

. Create the project and pod.
. Deploy egress network policy with DNS names.
. Validate the firewall.

.Egress Policy Example
----
{
    "kind": "EgressNetworkPolicy",
    "apiVersion": "v1",
    "metadata": {
        "name": "policy-test"
    },
    "spec": {
        "egress": [
            {
                "type": "Allow",
                "to": {
                    "dnsName": "stopdisablingselinux.com"
                }
            },
            {
                "type": "Deny",
                "to": {
                  "cidrSelector": "0.0.0.0/0"
                }
            }
        ]
    }
}
----

[NOTE]
====
Exposing services by creating routes will ignore the Egress Network Policy.
Egress Network policy Service endpoint filtering is performed on the `kubeproxy`
node. When the router is involved, `kubeproxy` is bypassed and Egress Network
Policy enforcement is not applied. Administrators can prevent this bypass by
limiting access and the ability to create routes.
====

See xref:../admin_guide/managing_pods.adoc#admin-guide-manage-pods[Managing
Pods] for more information.

[[ocp-36-network-policy]]
==== Network Policy (Technology Preview)

Network Policy (currently in xref:ocp-36-technology-preview[Technology Preview]
and not for production workloads) is an optional plug-in specification of how
selections of pods are allowed to communicate with each other and other network
endpoints. It provides fine-grained network namespace isolation using labels and
port specifications.

After installing the Network Policy plug-in, an annotation that flips the
namespace from `allow all traffic` to `deny all traffic` must first be set on
the namespace. At that point, `NetworkPolicies` can be created that define what
traffic to allow. The annotation is as follows:

----
$ oc annotate namespace ${ns} 'net.beta.kubernetes.io/network-policy={"ingress":{"isolation":"DefaultDeny"}}'
----

The allow-to-red policy specifies "all red pods in namespace `project-a` allow
traffic from any pods in any namespace." This does not apply to the red pod in
namespace `project-b` because `podSelector` only applies to the namespace in
which it was applied.

.Policy applied to project
----
kind: NetworkPolicy
apiVersion: extensions/v1beta1
metadata:
  name: allow-to-red
spec:
  podSelector:
    matchLabels:
      type: red
  ingress:
  - {}
----

See
xref:../admin_guide/managing_networking.adoc#admin-guide-manage-networking[Managing
Networking] for more information.

[[ocp-36-router-template-format]]
==== Router Template Format

{product-title} 3.6 introduces improved router customization documentation. Many
RFEs could be solved with better documentation around the HAProxy
features and functions which are now added, and their customizable fields via
annotations and environment variables. For example, router annotations to do
per-route operations.

For example, to change the behavior of HAProxy (round-robin load balancing)
through annotating a route:

----
$ oc annotate route/ab haproxy.router.openshift.io/balance=roundrobin
----

For more information, see
xref:../install_config/router/customized_haproxy_router.adoc#install-config-router-customized-haproxy[Deploying
a Customized HAProxy Router].

[[use-a-different-f5-partition]]
==== Use a Different F5 Partition Other than /Common

With {product-title} 3.6, there is now the added ability to use custom F5
partitions for properly securing and isolating {product-title} route
synchronization and configuration.

The default is still `/Common` or global partition if not specified. Also,
behavior is unchanged if the partition path is not specified.  This new feature
ensures all the referenced objects are in the same partition, including virtual
servers (`http` or `https`).

[[ocp-36-support-ipv6-terminated-at-the-router-with-internal-ipv4]]
==== Support IPv6 Terminated at the Router with Internal IPv4

The router container is able to terminate IPv6 traffic and pass HTTP[S] through
to the back-end pod.

The IPv6 interfaces on the router must be enabled, with IPv6 addresses listening
(`::80`, `::443`). The client needs to reach the router node using IPv6.
IPv4 should be unaffected and continue to work, even if IPv6 is disabled.

[NOTE]
====
HAProxy can only terminate IPv6 traffic when the router uses the network stack
of the host (default). When using the container network stack (`oc adm router
--service-account=router --host-network=false`), there is no global IPv6 address
for the pod.
====

[[ocp-36-installation]]
=== Installation

[[ocp-36-ansible-service-broker]]
==== Ansible Service Broker (Technology Preview)

The Ansible service broker is currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads. This feature includes:

- Implementation of the open service broker API that enables users to leverage Ansible
for provisioning and managing of services via the service catalog on {product-title}.
- Standardized approach for delivering simple to complex multi-container
{product-title} services.
- Works in conjunction with Ansible playbook bundles (APB), which is a lightweight
meta container comprised of a few named playbooks for each open service broker
API operations.

Service catalog and Ansible service broker must be configured during
{product-title} installation. Once enabled, APB services can be deployed right
from Service Catalog UI.

[IMPORTANT]
====
In {product-title} In OCP 3.6.0, the Ansible Service Broker exposes an
unprotected route, which allows unauthenticated users to provision resources in
the cluster, namely Mediawiki and Postgres Ansible Playbook Bundles.
====

See
xref:../install_config/install/advanced_install.adoc#configuring-ansible-service-broker[Configuring
the Ansible Service Broker] for more information.

[[ocp-36-ansible-playbook-bundles]]
==== Ansible Playbook Bundles (APB) (Technology Preview)

Ansible playbook bundles (APB) (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) is a short-lived, lightweight container image consisting of:

* Simple directory structure with named action playbooks
* Metadata consisting of:
** required/optional parameters
** dependencies (provision versus bind)
* Ansible runtime environment
* Leverages existing investment in Ansible playbooks and roles
* Developer tooling available for guided approach
* Easily modified or extended
* Example APB services included with {product-title} 3.6:
** MediaWiki, PostgreSQL

When a user orders an application from the service catalog, the Ansible service
broker will download the associated APB image from the registry and run it. Once
the named operation has been performed on the service, the APB image will then
terminate.

[[ocp-36-automated-installation-of-cloudforms]]
==== Automated installation of CloudForms 4.5 Inside OpenShift (Technology Preview)

The installation of containerized CloudForms inside {product-title} is now part
of the main installer (currently in xref:ocp-36-technology-preview[Technology
Preview] and not for production workloads). It is now treated like other common
components (metrics, logging, and so on).

After the {product-title} cluster is provisioned, there is an additional
playbook you can run to deploy CloudForms into the environment (using the
`openshift_cfme_install_app` flag in the hosts file).

----
$ ansible-playbook -v -i <INVENTORY_FILE> playbooks/byo/openshift-cfme/config.yml
----

Requirements:

[cols="4*", options="header"]
|===
|Type
|Size
|CPUs
|Memory

|Masters
|1+
|8
|12 GB

|Nodes
|2+
|4
|8 GB

|PV Storage
|25 GB
|N/A
|N/A
|===

[NOTE]
====
NFS is the only storage option for the Postgres database at this time.

The NFS server should be on the first master host. The persistent volume backing
the NFS storage volume is mounted on exports.
====

[[ocp-36-automated-cns-deployment-with-ocp-ansible-advanced-installation]]
==== Automated CNS Deployment with OCP Ansible Advanced Installation

{product-title} (OCP) 3.6 now includes an integrated and simplified installation
of container native storage (CNS) through the advanced installer. The
installer’s inventory file is simply configured. The end result is an automated,
supportable, best practice installation of CNS, providing ready-to-use
persistent storage with a pre-created storage class. The advanced installer now
includes automated and integrated support for deployment of CNS, correctly
configured and highly available out-of-the-box.

CNS storage device details are added to the installer’s inventory file. Examples
provided in {product-title}
xref:../install_config/install/advanced_install.adoc#advanced-install-containerized-glusterfs-persistent-storage[advanced
installation documentation]. The installer manages configuration and deployment
of CNS, its dynamic provisioner, and other pertinent details.

[[ocp-36-installation-of-etcd-docker-daemon-and-ansible-installer-as-system-containers]]
==== Installation of etcd, Docker Daemon, and Ansible Installer as System Containers (Technology Preview)

This feature is currently in xref:ocp-36-technology-preview[Technology Preview]
and not for production workloads.

RHEL System Containers offer more control over the life cycle of the services
that do not run inside {product-title} or Kubernetes. Additional system
containers will be offered over time.

System Containers leverage the OSTree on RHEL or Atomic Host. They are
controlled by the kernel init system and therefore can be leveraged earlier in
the boot sequence. This feature is enabled in the installer configuration.

For more information, see
xref:../install_config/install/advanced_install.adoc#advanced-install-configuring-system-containers[Configuring
System Containers].

[[ocp-36-running-openshift-installer-as-a-system-container]]
==== Running OpenShift Installer as a System Container (Technology Preview)

This feature is currently in xref:ocp-36-technology-preview[Technology Preview]
and not for production workloads.

To run the {product-title} installer as a system container:

----
$ atomic install --system --set INVENTORY_FILE=$(pwd)/inventory registry:port/openshift3/ose-ansible:v3.6

$ systemctl start ose-ansible-v3.6
----

[[ocp-36-etcd3-model-for-new-installations]]
==== etcd3 Data Model for New Installations

Starting with new installations of {product-title} 3.6, the etcd3 v3 data model
is the default. By moving to the etcd3 v3 data model, there is now:

- Larger memory space to enable larger cluster sizes.
- Increased stability in adding and removing nodes in general life cycle actions.
- A significant performance boost.

A migration playbook will be provided in the near future allowing
upgraded environments to migrate to the v3 data model.

[[ocp-36-cluster-wide-control-of-ca]]
==== Cluster-wide Control of CA

You now have the ability to change the certificate expiration date en mass
across the cluster for the various framework components that use TLS.

We offer new cluster variables per framework area so that you can use different
time-frames for different framework components. Once set, issue the new
`redeploy-openshift-ca` playbook. This playbook only works for redeploying the
root CA certificate of {product-title}. Once you set the following options, they
will be effective in a new installation, or they can be used when redeploying
certificates against an existing cluster.

.New Cluster Variables
----
# CA, node and master certificate expiry
openshift_ca_cert_expire_days=1825
openshift_node_cert_expire_days=730
openshift_master_cert_expire_days=730

# Registry certificate expiry
openshift_hosted_registry_cert_expire_days=730

# Etcd CA, peer, server and client certificate expiry
etcd_ca_default_days=1825
----

[[ocp-36-general-stability]]
==== General Stability

{product-title} engineering and the OpenShift Online operations teams have been
working closely together to refactor and enhance the installer. The
{product-title} 3.6 release includes the culmination of those efforts, including:

- Upgrading from {product-title} 3.5 to 3.6
- Idempotency refactoring of the configuration role
- Swap handling during installation
- All BYO playbooks pull from a normalized group source
- A final port of operation’s Ansible modules
- A refactoring of excluder roles

[[ocp-36-metrics-and-logging]]
=== Metrics and Logging

[[ocp-36-removing-metrics-deployer-and-removing-logging-deployer]]
==== Removing Metrics Deployer and Removing Logging Deployer

The metrics and logging deployers were replaced with `playbook2image` for `oc
cluster up` so that `openshift-ansible` is used to install logging and metrics:

----
$ oc cluster up --logging --metrics
----

Check metrics and logging pod status:

----
$ oc get pod -n openshift-infra
$ oc get pod -n logging
----

[[ocp-36-expose-elasticsearch-as-a-route]]
====  Expose Elasticsearch as a Route

By default, the Elasticsearch instance deployed with {product-title} aggregated
logging is not accessible from outside the deployed {product-title} cluster. You
can now enable an external route for accessing the Elasticsearch instance
via its native APIs to enable external access to data via various supported
tools.

Direct access to the Elasticsearch instance is enabled using your OpenShift
token. You have the ability to provide the external Elasticsearch and
Elasticsearch Operations host names when creating the server certificate
(similar to Kibana). The provided Ansible tasks simplify route deployment.

[[ocp-36-mux]]
==== Mux (Technology Preview)

`mux` is a new xref:ocp-36-technology-preview[Technology Preview] feature for
{product-title} 3.6.0 designed to facilitate better scaling of aggregated
logging. It uses a smaller set of from Fluentd instances (called _muxes_) kept
near the Elasticsearch instance pod to improve the efficiency of indexing log
records into Elasticsearch.

See xref:../install_config/aggregate_logging.adoc#aggregated-fluentd[Aggregating
Container Logs] for more information.

[[ocp-36-developer-experience]]
=== Developer Experience

[[ocp-36-service-catalog-experience]]
==== Service Catalog Experience in the CLI (Technology Preview)

This feature (currently in xref:ocp-36-technology-preview[Technology Preview]
and not for production workloads) brings the Service Catalog experience to the
CLI.

You can run `oc cluster up --version=latest --service-catalog=true` to get the
Service Catalog experience in {product-title} 3.6.

[[ocp-36-template-service-broker]]
==== Template Service Broker (Technology Preview)

The template service broker (currently in
xref:ocp-36-technology-preview[Technology Preview]) exposes OpenShift templates
through a open service broker API to the Service Catalog.

The template service broker (TSB) matches the lifecycles of provision,
deprovision, bind, unbind with existing templates. No changes are required to
templates, unless you expose bind. Your application will get injected with
configuration details (bind).

[IMPORTANT]
====
The TSB is currently a Technology Preview feature and should not be used in
production clusters. Enabling the TSB currently requires opening unauthenticated
access to the cluster; this security issue will be resolved before exiting the
Technology Preview phase.
====

See
xref:../install_config/install/advanced_install.adoc#configuring-template-service-broker[Configuring
the Template Service Broker] for more information.

[[ocp-36-automicatic-build-pruning]]
==== Automatic Build Pruning

Previously, only `oc adm prune` could be used. Now, you can define how much
build history you want to keep per build configuration. Also, you can set
`successful` versus `failed` history limits separately.

See
xref:../dev_guide/builds/advanced_build_operations.adoc#build-pruning[Advanced
Build Operations] for more information.

[[ocp-36-easier-custom-slave-configuration-for-jenkins]]
==== Easier Custom Slave Configuration for Jenkins

In {product-title} 3.6, it is now easier to make images available as slave pod
templates.

Slaves are defined as image-streams or image-stream tags with the appropriate
label. Slaves can also be specified via a `ConfigMap` with the appropriate
label.

See
xref:../using_images/other_images/jenkins.adoc#using-the-jenkins-kubernetes-plug-in-to-run-jobs[Using
the Jenkins Kubernetes Plug-in to Run Jobs] for more information.

[[ocp-36-detailed-build-timing]]
==== Detailed Build Timing

Builds now record timing information based on more granular steps.

Information such as how long it took to pull the base image, clone the source,
build the source, and push the image are provided. For example:

----
$ oc describe build nodejs-ex-1
Name:        nodejs-ex-1
Namespace:    myproject
Created:    2 minutes ago

Status:            Complete
Started:        Fri, 07 Jul 2017 17:49:37 EDT
Duration:        2m23s
  FetchInputs:       2s
  CommitContainer:   6s
  Assemble:           36s
  PostCommit:            0s
  PushImage:          1m0s
----

[[ocp-36-other-developer-experience-changes]]
==== Other Developer Experience Changes

* xref:../dev_guide/builds/triggering_builds.adoc#webhook-triggers[Webhook triggers] for Github and Bitbucket.
* HTTPD 2.4 s2i support.
* Separate build events for `start`, `canceled`, `success`, and `fail`.
* Support for xref:../dev_guide/builds/build_strategies.adoc#docker-strategy-build-args[arguments in Docker files].
* xref:../dev_guide/builds/build_strategies.adoc#jenkins-pipeline-strategy-environment[Environment variables in pipeline builds].
* Credential support for Jenkins Sync plug-in for ease of working external Jenkins instance.
* xref:../dev_guide/builds/build_environment.adoc#overview[`ValueFrom` Support] in build environment variables.
* Deprecated Jenkins v1 image.
* `oc cluster up`: support launching service catalog
* Switch to nip.io from xip.io, with improved stability

[[ocp-36-web-console]]
=== Web Console

[[ocp-36-service-catalog]]
==== Service Catalog (Technology Preview)

You can now opt into the service catalog (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) during installation or upgrade.

When developing microservices-based applications to run on cloud native
platforms, there are many ways to provision different resources and share their
coordinates, credentials, and configuration, depending on the service
provider and the platform.

To give developers a more seamless experience, {product-title} includes a
xref:../architecture/service_catalog/index.adoc#architecture-additional-concepts-service-catalog[Service
Catalog], an implementation of the link:https://openservicebrokerapi.org/[open
service broker API] (OSB API) for Kubernetes. This allows users to connect any
of their applications deployed in {product-title} to a wide variety of service
brokers.

The service catalog allows cluster administrators to integrate multiple
platforms using a single API specification. The {product-title} web console
displays the service classes offered by brokers in the service catalog, allowing
users to discover and instantiate those services for use with their
applications.

As a result, service users benefit from ease and consistency of use across
different types of services from different providers, while service providers
benefit from having one integration point that gives them access to multiple
platforms.

This feature consists of:

- The Service Consumer: The individual, application , or service that uses a service enabled by the broker and catalog.
- The Catalog: Where services are published for consumption.
- Service Broker: Publishes services and intermediates service creation and credential configuration with a provider.
- Service Provider: The technology delivering the service.
- Open Service Broker API: Lists services, provisions and deprovisions, binds, and unbinds.

See
xref:../install_config/install/advanced_install.adoc#enabling-service-catalog[Enabling
the Service Catalog] for more information.

[[ocp-36-initial-experience]]
==== Initial Experience (Technology Preview)

In {product-title} 3.6, a better initial user experience (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) is introduced, motivated by service catalog. This includes:

- A task-focused interface.
- Key call-outs.
- Unified search.
- Streamlined navigation.

[[ocp-36-search-catalog]]
==== Search Catalog (Technology Preview)

The search catalog feature (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) provides a single, simple way to quickly get what you want.

image::ocp36-search-catalog.gif[search catalog]

[[ocp-36-add-from-catalog]]
==== Add from Catalog (Technology Preview)

The add from catalog feature (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) allows you to provision a service from the catalog.

Select the desired service, then follow prompts for your desired project and
configuration details.

image::ocp36-add-from-catalog.gif[add from catalog]

[[ocp-36-project-overview-redesign]]
==== Project Overview Redesign

In {product-title} 3.6, the Project Overview was resigned based on feedback from
customers.

In this redesign, there are three focused views:

- Applications
- Pipelines
- Resource types

There are now more contextual actions and rolled up metrics across multiple
pods.

image::ocp36-redesigned-project-overview.gif[Redesigned Project Overview]

[[ocp-36-add-to-project]]
==== Add to Project (Technology Preview)

The add to project feature (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) allows you to provision a service without having to leave the Project
Overview.

When you go directly to the catalog from project, the context is preserved. You
can directly provision, then bind.

image::ocp36-add-to-project.gif[add to project]

[[ocp-36-bind-in-context]]
==== Bind in Context (Technology Preview)

The bind in context feature (currently in
xref:ocp-36-technology-preview[Technology Preview] and not for production
workloads) allows you to provision a service and bind without having to leave
the Project Overview.

- Select deployment and initiate a bind.
- Select from bindable services.
- Binding is created and the user stays in context
- See relationships between bound applications and services in the Project
Overview section.

image::ocp36-bind-in-context.gif[bind in context]

[[ocp-36-image-stream-details]]
==== Image Stream Details

In {product-title} 3.6, additional details are provided about image streams and
their tags.

This feature leverages Cockpit views from image streams. It details tags and
provide information about each.

image::ocp36-image-stream-details.png[bind in context]

[[ocp-36-better-messages-for-syntax-errors]]
==== Better Messages for Syntax Errors in JSON and YAML Files

With {product-title} 3.6, better messages for syntax errors in JSON and YAML
files are provided. This includes details of the syntax problem and the line
number containing the issue.

This feature validates input on commands such as `oc create -f foo.json` and
`oc new-app -f template.yaml`. For example:

----
$ oc create -f dc.json
error: json: line 27: invalid character 'y' looking for beginning of value
----

[[ocp-36-cascading-deletes]]
==== Cascading Deletes

When deleting a resource, this feature ensures that all generated or dependent
resources are also deleted.

For example, when selecting a deployment configuration and deleting will delete
the deployment configuration, deployment history, and any running pods.

image::ocp36-cascading-deletes.png[cascading deletes]

[[ocp-36-other-user-interface-changes]]
==== Other User Interface Changes

- Pod details now should show information about
xref:../architecture/core_concepts/containers_and_images.adoc#init-containers[init
containers].
- You can now add or edit environment variables that are populated by data in
secrets or configuration maps.
- You can now create cluster-wide resources from JSON and YAML files.
- There is now an alignment of notification designs.

[[ocp-36-notable-technical-changes]]
== Notable Technical Changes

{product-title} 3.6 introduces the following notable technical changes.

[discrete]
[[ocp-pci-dss-compliance]]
=== Payment Card Industry Data Security Standard (PCI DSS) Compliance

Red Hat has worked with a
link:https://www.redhat.com/en/resources/openshift-pci-product-applicability-guide-datasheet[PCI
DSS Qualified Assessor] (QSA) and has determined that {product-title} running on
either Red Hat Enterprise Linux or Red Hat Enterprise Linux Atomic Host could be
deployed in a way that it would pass a PCI assessment. Ultimately, compliance
and validation is the responsibility of the organization deploying
{product-title} and their assessor. Implementation of proper configuration,
rules, and policies is paramount to compliance, and
link:https://access.redhat.com/support/offerings/production/soc[Red Hat makes no
claims or guarantees] regarding PCI assessment.

[discrete]
[[ocp-36-federation-decision-deliberation]]
===  Federation Decision Deliberation

In the upstream federation special interest group (SIG), there are two primary
ideas being  discussed. The current control plane model is an intelligent
controller that duplicates API features and functions at a high level. The
client is agnostic and the controller handles the inter-cluster relationships,
policy, and so on. The control plane model may be difficult to maintain.

In the client model, multiple controllers would exist for various features and
functions, and the client would maintain the intelligence to understand how to
affect change across clusters. Red Hat is currently soliciting feedback on these
two models. Customers, partners, and community members are encouraged to
participate in the upstream SIGs.

[discrete]
[[ocp-36-dns-changes]]
=== DNS Changes

Prior to {product-title} 3.6, cluster DNS was provided by the API server running
on the master and the use of *dnsmasq* could be disabled by setting
`openshift_use_dnsmasq=false`. Starting with {product-title} 3.6, the use of
*dnsmasq* is now mandatory and upgrades will be blocked if this variable is set
to false.

Also, when upgrading to version 3.6, the playbooks will configure the node
service to serve DNS requests on `127.0.0.1:53` and *dnsmasq* will be
reconfigured to route queries for `cluster.local` and `in-addr.arpa` to
`127.0.0.1:53` rather than to the Kubernetes service IP. Your node must not run
other services on port 53. Firewall rules exposing port 53 are not necessary, as
all queries will originate from the local network.

[discrete]
[[ocp-36-deprecated-api-types]]
=== Deprecated API Types

The `ClusterPolicy`, `Policy`, `ClusterPolicyBinding` and `PolicyBinding` API
types are deprecated. Users will need to switch any interactions with these
types to instead use `ClusterRole`, `Role`, `ClusterRoleBinding`, or
`RoleBinding` as appropriate. The following `oc adm policy` commands can be used
to help with this process:

----
add-cluster-role-to-group
add-cluster-role-to-user
add-role-to-group
add-role-to-user
remove-cluster-role-from-group
remove-cluster-role-from-user
remove-role-from-group
remove-role-from-user
----

The following `oc create` commands can also help:

----
clusterrole
clusterrolebinding
role
rolebinding
----

The use of `oc create policybinding` is also deprecated and no longer a
perquisite for creating a `RoleBinding` to a `Role`.

[discrete]
[[ocp-36-resources-registered-to-api-groups]]
=== OpenShift Resources Registered to API groups

Custom roles that reference OpenShift resources should be updated to include the
appropriate API groups.

[discrete]
[[ocp-36-ambiguous-CIDR-values-rejected]]
=== Ambiguous CIDR Values Rejected

{product-title} will now reject `EgressNetworkPolicy`, `ClusterNetwork`,
`HostSubnet`, and `NetNamespace` objects with ambiguous CIDR values. Before, an
`EgressNetworkPolicyRule` such as the following would be interpreted as "allow
to `192.168.1.*0/24*`".

----
type: Allow
to:
  cidrSelector: 192.168.1.15/24
----

However, the user most likely meant "allow to 192.168.1.*15/32*". In
{product-title} 3.6, trying to create such a rule (or to modify an existing rule
without fixing it) will result in an error.

The same validation is also now performed on CIDR-valued fields in
`ClusterNetwork`, `HostSubnet`, and `NetNamespace` objects, but these are
normally only created or modified by {product-title} itself.

[discrete]
[[ocp-36-volumes-removed-at-pod-termination]]
=== Volumes Removed at Pod Termination

In prior versions, pod volumes remained attached until the pod resource was
deleted from the master. This prevented local disk and memory resources from
being reclaimed as a result of pod eviction. In {product-title} 3.6, the volume
is removed when the pod is terminated.

[discrete]
[[ocp-36-init-containers-2]]
=== Init Containers

Pod authors can now use
xref:../architecture/core_concepts/containers_and_images.adoc#init-containers[init
containers] to share volumes, perform network operations, and perform
computation prior to the start of the remaining containers.

An init container is a container in a pod that is started before the pod’s
application containers are started. Init containers can also block or delay the
startup of application containers until some precondition is met.

[discrete]
[[ocp-36-pods-tolerations]]
=== Pod Tolerations and Node Taints No Longer Defined in Annotations

xref:../admin_guide/scheduling/taints_tolerations.adoc#admin-guide-taints[Pod
tolerations and node taints] have moved from annotations to API fields in pod
specifications (PodSpec) and node specification (NodeSpec) files, respectively.
Pod tolerations and node taints that are defined in the annotations will be
ignored. The annotation keys `scheduler.alpha.kubernetes.io/tolerations` and
`scheduler.alpha.kubernetes.io/taints` are now removed.

[discrete]
[[ocp-36-router-does-not-allow-SSLv3]]
=== Router Does Not Allow SSLv3

The OpenShift router will no longer allow SSLv3 (to prevent the POODLE attack).
No modern web browser should require this.

[discrete]
[[ocp-36-router-cipher-list-updates]]
=== Router Cipher List Updates
The router cipher list has changed to reflect the current _intermediate_ cipher
suite recommendations from Mozilla. It is now also possible to set the
cipher suite explicitly, or choose from a list of named preset security levels.

[discrete]
[[ocp-36-networkpolicy-objects-v1-semantics]]
=== NetworkPolicy Objects Have NetworkPolicy v1 Semantics from Kubernetes 1.7

When using the `redhat/openshift-ovs-networkpolicy` plug-in, which is still in
Technology Preview,
xref:../admin_guide/managing_networking.html#admin-guide-networking-networkpolicy[`NetworkPolicy`]
objects now have the `NetworkPolicy` v1 semantics from Kubernetes 1.7. They are
still in the `extensions/v1beta1` API group; the new `networking.k8s.io/v1` API
group is not yet available.

In particular, the `net.beta.kubernetes.io/network-policy` annotation on
namespaces to opt in to isolation has been removed. Instead, isolation is now
determined at a per-pod level, with pods being isolated if there is any
`NetworkPolicy` whose `spec.podSelector` targets them. Pods that are targeted by
`NetworkPolicies` accept traffic that is accepted by any of the `NetworkPolicies`
(and nothing else), and pods that are not targeted by any `NetworkPolicy` accept
all traffic by default.

To preserve compatibility when upgrading:

. In namespaces that previously had the `DefaultDeny` annotation, you can
create equivalent v1 semantics by creating a `NetworkPolicy` that matches all
pods but does not allow any traffic:
+
----
kind: NetworkPolicy
apiVersion: extensions/v1beta1
metadata:
  name: default-deny
spec:
  podSelector:
----
+
This will ensure that pods that are not matched by any other `NetworkPolicy`
will continue to be fully-isolated, as they were before.

. In namespaces that previously did not have the `DefaultDeny` annotation, you
should delete any existing `NetworkPolicy` objects. These would have had no
effect before, but with v1 semantics they might cause some traffic to be blocked
that you did not intend to be blocked.

[discrete]
[[ocp-36-deprecated-metadata-volumesource]]
=== Metadata volumeSource Now Deprecated

The
link:https://github.com/openshift/origin/blob/master/vendor/k8s.io/kubernetes/pkg/api/v1/types.go#L338-L341[metadata
`volumeSource`] is now deprecated for multiple releases and will be removed in
{product-title} 3.7.

[discrete]
[[ocp-36-breaking-api-change]]
=== Breaking API Change

Unless explicitly documented otherwise, API fields containing lists of items no
longer distinguish between null and `[]`, and may return either null or `[]`
regardless of the original value submitted.

[discrete]
[[ocp-36-atomic-command-on-hosts]]
=== Atomic Command on Hosts
When using system containers with {product-title}, the `atomic` command on hosts
must be `1.17.2` or later.

[discrete]
[[ocp-36-containers-run-under-build-pods-parent-cgroup]]
=== Containers Run Under Build Pod's Parent cgroup

Containers launched by the build pod (the s2i assemble container or the `docker
build` process) now run under the build pod's parent cgroup.

Previously, the containers had their own cgroup and the memory and CPU limits were
mirrored from the pod's cgroup limits. With this change, the secondary
containers will now be sharing the memory limit that is consumed by the build
pod, meaning the secondary containers will have slightly less memory available
to them.

[discrete]
[[ocp-36-SecurityContextConstraints-vailable-via-groupified-API]]
=== SecurityContextConstraints Available via Groupified API

`SecurityContextConstraints` are now also available via a groupified API at
*_/apis/security.openshift.io/v1/securitycontextconstraints_*. They are still
available at *_/api/v1/securitycontextconstraints_*, but using the groupified API
will provide better integration with tooling.

[discrete]
[[ocp-36-volume-recycler-now-deprecated]]
=== Openshift Volume Recycler Now Deprecated

Openshift Volume Recycler is being deprecated. Anyone using recycler should use
dynamic provision and volume deletion instead.

[[ocp-36-bug-fixes]]
== Bug Fixes

This release fixes bugs for the following components:

*Authentication*

* Nested groups now sync between {product-title} and Active Directory. It is
common to have nested groups in Active Directory.  Users wanted to be able to
sync such groups with {product-title}. This feature was always supported, but
lacked any formal documentation and was difficult to discover.
xref:../install_config/syncing_groups_with_ldap.adoc#sync-ldap-nested-example[Documentation
is now added].
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1437324[*BZ#1437324*])

*Builds*

* When a build is started from a webhook, the server response does not contain a
body Therefore, the CLI cannot easily determine the generation of the created
build, and cannot report it to the user. Change webhook response to contain the
created build object in the body. The CLI can now report the correct build
generation when created.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1373441[*BZ#1373441*])

* Build durations are recorded as part of a storage hook. Build duration is
sometimes calculated incorrectly and reported with an invalid value. Calculate
build duration when recording build time of build completion. As a result, build
durations are reported correctly and align with the build start and completion
times. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1443687[*BZ#1443687*])

* The code was not setting the status reason and status message for certain
failures. Therefore, there were missing status reasons and status messages for
certain failures. With this bug fix, code was added that sets the status reason
and status message and the status reason and message are now set.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1436391[*BZ#1436391*])

* A debug object type is used when high levels of logging are requested. Client
code did not anticipate the alternative object type and experienced a typecast
error. With this bug fix, the client code is updated to handle the debug object
type. The typecast error will not occur and builds now proceed as expected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1441929[*BZ#1441929*])

* When resources were specified in the build default configuration, the resource
values were not applied to the build pods. They were only applied to the build
object. Builds ran without the default resource limits being applied to them
because the pod was created before the build was updated with the default
resource limits. With this bug fix, the build resource defaults are applied to
the build pod. Build pods now have the default resource limits applied, if they
do not already specify resource limits.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1443187[*BZ#1443187*])

* The `new-app` circular dependency code did not account for `BuildConfig` sources
pointing to the `ImageStreamImage` type. As a result, an unnecessary warning was
logged about not being able to follow the reference type `ImageStreamImage`.
This bug fix enhances the `new-app` circular dependency code to account for the
`ImageStreamImage` type. The unnecessary warning no longer appears.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1422378[*BZ#1422378*])

*Command Line Interface*

* Previously, pod headers were only being printed once for all sets of pods when
listing pods from multiple nodes. Executing `oc adm manage-node <node-1> <node-2> ...
--evacuate --dry-run` with multiple nodes would print the same output multiple
times (once per each specified node). Therefore, users would see inconsistent or
duplicate pod information. This bug fix resolves the issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1390900[*BZ#1390900*])

* The `--sort-by` in the `oc get` command fails when any object in the list
contains an empty value in the field used to sort, causing a failure. With this
bug fix, empty fields in `--sort-by` are now correctly handled. The output of
`oc get` is printed correctly and empty fields are considered in sorting.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1409878[*BZ#1409878*])

* A Golang issue (in versions up to 1.7.4) adds an overhead of around four seconds
to the TLS handshake on macOS. Therefore, the `oc` calls time out
intermittently on macOS. This bug fix backported the existing fix to 1.7.5 and
upgraded the Golang that we use to build `oc` to that version. The TLS handshake
time is now reduced by about four seconds on macOS.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1435261[*BZ#1435261*])

* When the master configuration specified a default `nodeSelector` for the
cluster, test projects created by `oc adm diagnostics` NetworkCheck got this
`nodeSelector` and, therefore, the test pods were also confined to this
`nodeSelector`. NetworkCheck test pods could only be scheduled on a subset of
nodes, preventing the diagnostic covering the entire cluster; in some clusters,
this might even result in too few pods running for the diagnostic to succeed
even if the cluster health is fine. NetworkCheck now creates the test projects
with an empty `nodeSelector` so they can land on any schedulable node. The
diagnostic should now be more robust and meaningful.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1459241[*BZ#1459241*])

*Installer*

* OpenShift Ansible facts were splitting a configuration parameter incorrectly.
Therefore, invalid `NO_PROXY` strings were generated and inserted into user
*_sysconfig/docker_*  files. The logic that generates the NO_PROXY strings was
reviewed and fixed. Valid Docker `NO_PROXY` settings are enerated and inserted
into the *_sysconfig/docker_*  file now.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1414748[*1414748*])

* The OpenShift CA redeployment playbook
(*_playbooks/byo/openshift-cluster/redeploy-openshift-ca.yml_*) would fail to
restart services if certificates were previously expired. Service restarts are
now skipped within the OpenShift CA redeployment playbook when expired
certificates are detected. Expired cluster certificates may be replaced with the
certificate redeployment playbook
(*_playbooks/byo/openshift-cluster/redeploy-certificates.yml_*) once the
OpenShift CA certificate has been replaced via the OpenShift CA redeployment
playbook.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=1452367[*1452367*])

* Previously, installation would fail in multi-master environments in which the
load balanced API was listening on a different port than that of the OpenShift
API and console. This difference is now accounted for and and the master
loopback client configuration is configured to interact with the local master.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1454321[*1454321*])

* A readiness probe is introduced with {product-title} 3.6, but the timeout
threshold was not high enough. This bug fix increases the timeout threshold.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1456139[*1456139*])

* Elasticsearch heap dump should not be written to the root partition. Specify a
location to write a heap dump other than the root partition.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1369914[*1369914*])

* Previously, the upgrade playbooks would use the default `kubeconfig`, which may
have been modified since creation to use a non-admin user. Now the upgrade
playbooks use the admin `kubeconfig`, which avoids this problem.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1468572[*1468572*])

* A fix for a separate PROXY related issue was merged. Therefore, various proxy
related operations began to fail.A correct fix for the original PROXY-related
issue was merged and functionality is now restored.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1470165[*1470165*])

* `NO_PROXY` setting logic was incorrectly indented in the openshift-ansible facts
module, causing `NO_PROXY` settings to always be generated and added to service
configuration files. The logic indentation was moved into the correct
conditional.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1468424[*BZ#1468424*])

* Image streams now reference the DNS hostname of
 `docker-registry.default.svc:5000`, which allows the installer to ensure that
 the hostname is appended to `NO_PROXY` environment variables so image pushes
 work properly in an environment that requires a proxy.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1414749[*BZ#1414749*])

* Starting with {product-title} 3.4, the software-defined networking (SDN)
plug-ins no longer reconfigure the docker bridge maximum transmission unit
(MTU), rather pods are configured properly on creation. Because of this change,
non-OpenShift containers may have a MTU configured that is too large to allow
access to hosts on the SDN. The installer has been updated to align the MTU
setting for the docker bridge with the MTU used inside the cluster, thus
avoiding the problem.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1457062[*BZ#1457062*])

* As part of the RFE to be able to label `PersistentVolume` (PV) for
`PersistentVolumeClaim` (PVC) selectors, the default PVC selector was set to
null but should have been an empty string. This caused the playbook to fail if
the user did not provide a label. This fix leaves the default label blank,
allowing the playbook to run to completion if the user does not provide a PV
label. (https://bugzilla.redhat.com/show_bug.cgi?id=1462352[*BZ#1462352*])

* Metrics were not consistently able to install correctly when using a non-root
user. This caused the playbook to fail due to lack of permissions, or files not
visible due to permissions. With this fix, any local action within the metrics
role added a `become: false` so it ensured it was using the local actions as the
same user running the playbook. The playbook no longer fails to complete due to
permissions. (https://bugzilla.redhat.com/show_bug.cgi?id=1464977[*BZ#1464977*])

* This feature grants the ability to provide `PersistentVolume` (PV) selectors for
PVs created during installation. Previously when installing logging and metrics
with the installer, a PV created for logging could be bound to a metrics PVC,
creating confusion. Now you can provide a PV selector in your inventory when
installing logging and metrics and the PVs created will contain the appropriate
label so that the generated PVCs will correctly bind.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1442277[*BZ#1442277*])

* Hosts missing an OpenSSL python library caused large serial numbers to not be
 parsed using the existing manual parser workaround for missing OpenSSL
 libraries. This bug fix updates the manual parser to account for certificate
 formats with large serial numbers. As a result, certificates with large serials
 on hosts missing the OpenSSL python library can now be parsed, such as during
 certificate expiration checking or certificate redeployment.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1464240[*BZ#1464240*])

* The master configuration parameter `serviceAccountConfig.limitSecretReferences`
may now be set via the installation playbooks by setting the variable
`openshift_master_saconfig_limitsecretreferences` to `true` or `false`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1442332[*BZ#1442332*])

* Older logic was missing a condition in which the `systemd` unit files should be
reloaded, causing updated or changed service unit files to not be identified.
This bug fix updates the Ansible installer master and node roles to ensure the
`reload system units` action is triggered. As a result, updated service unit
files are correctly detected and users no longer receive a “Could not find the
requested service” error anymore.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1451693[*BZ#1451693*])

* An incorrect check for python libraries was used for the metrics role, causing
 playbooks to fail when checking whether *python2-passlib* was installed. This
 bug fix updates the query for checking the availability of the library. As a
 result, the playbook no longer incorrectly fails when *python2-passlib* is
 installed.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1455310[*BZ#1455310*])

* The default persistent volume (PV) selector for the logging persistent volume
claim (PVC) generation was `None` and was being interpreted as a variable. This
caused the playbook to fail because it could not find a variable of the name
`None`. This bug fix updates the default to be `’’`. As a result, the playbook
is able to correctly run to completion when not providing a PV selector.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1463055[*BZ#1463055*])

* The installer now creates a default `StorageClass` whenever AWS or GCE cloud
providers are configured, allowing for out-of-the-box dynamic volume creation.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1393207[*BZ#1393207*])

* The example inventory files have been amended to illustrate all available audit
logging configuration options.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1447774[*BZ#1447774*])

* The default templates have been updated to the latest available for OpenShift
Container Platform 3.6.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1463553[*BZ#1463553*])

* Previously, all certificates for an OpenShift cluster have a validity of one
year. This was not practical for enterprise-level installations. The installer
tool was modified to allow configuration of certificates, meaning the validity
period can be extended.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1275176[*BZ#1275176*])

* The service accounts that belonged in the `openshift-infra` namespace were being
created in `default` after a different fix to create them before role bindings.
Therefore, pods were not able to find their SA for running. With this bug fix,
SAs are created in the correct namespace and pods are able to start.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1477440[*BZ#1477440*])

*Image*

* When Kubernetes settings are updated, Jenkins is restarted and reloaded. This
causes all of the configurations to be reloaded, including {product-title}
settings. Therefore, `credentialsId` becomes null and causes NPE's to be thrown,
stopping the watchers, which can not recover. When Kubernetes is updated,
synchronization with {product-title} is stopped. With this bug fix, the getter
for `credentialsId ` check for null, and returns `""` if found. Kubernetes can
now be updated without NPE.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1451223[*BZ#1451223*])

* Proxy values are logged during builds. Previously, proxy values that contained
user credentials were exposed to anyone who can view build logs. With this bug
fix, credentials that are part of proxy values (for example,
`\http://user:password@proxy.com`) will be redacted from the proxy value being
logged. Proxy credentials are now no longer exposed in build logs.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1366795[*BZ1366795*])

* Previously, the PHP `latest` image stream tag did not point to the latest
available PHP image (7.0). Therefore, users of the `latest` image stream tag did
not get the most recent PHP image available. With this bug fix, the `latest` tag
is updated to point to the most recent image stream tag for PHP. Now, users who
select the `latest` tag will get the PHP 7.0 image. (BZ#1421982)
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1421982[*BZ1421982*])

*Image Registry*

* There was a logic error in how weak and strong references were identified when
searching images eligible for pruning. Therefore, some images having both strong
and weak references in pruning graph could be removed during pruning. The logic
responsible for finding which images have strong references is now fixed.
Pruning now correctly recognizes and prunes images.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1440177[*BZ440177*])

* Only aliases within single Image streams were being resolved. If an update was
done to the source image, cross-image-stream aliases were not resolved properly,
pointing to the old image. This bug fix forbids the creation of
cross-image-stream aliases. Users creating a cross-image-stream alias now get an
error. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1435588[*1435588*])

*Kubernetes*

* Previously, if the pod restarted due to exceeding `failureThreshold` on a probe,
the restarted pod was only allowed a single probe failure before being
restarted, regardless of the `failureThreshold` value. This caused restarted
pods not to get the expected number of probe attempts before being restarted.
This fix allows the reset the failure counter when the pod is restarted,
therefore the restarted pod gets `failureThreshold` attempts for the probe to
succeed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1455056[*BZ#1455056*])

* When attempting to connect to  an `etcd` server to acquire a leader lease, the
master controllers process only tried to reach a single `etcd` cluster member
even if multiple are specified. If the selected `etcd` cluster member is
unavailable, the master controllers process is not able to acquire the leader
lease, which means it will not start up and run properly. This fix enables
attempts to connect to all of the specified `etcd` cluster members until a
successful connection is made, and as a result the master controllers process
can acquire the leader lease and start up properly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1426183[*BZ#1426183*])

* Previously, the same error message was being output for each node in a cluster.
With this fix, the error will include its message and its repeat count.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1462345[BZ#1462345])

*Logging*

* A change in the `authproxy` was keeping it from finding dependent files, causing
the `authproxy` to terminate. With this fix, environment variables were added to
the `deploymentconfig` with the correct path to the files. As a result, the
`openshift-auth-proxy` finds dependent files and starts correctly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1439451[BZ#1439451])

* The Aggregated Logging diagnostic was not updated to reflect updates made to
logging deployment. Therefore, the diagnostic incorrectly reported errors for an
unnecessary Service Account and (if present) the `mux` deployment. With this bug
fix, these errors are no longer reported. In addition, warnings about missing
optional components were all downgraded to Info level. The diagnostic no longer
needlessly alarms the user for these issues.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1421623[*1421623*])

*Web Console*

* Previously, there were issues viewing logs for pods with multiple containers
caused, especially when switching between containers. You should now be able to
switch between container logs without issue and the Follow link should work as
expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1421287[*1421287*])

* It was difficult to find the underlying reason for a failed deployment from the
project overview. The overview will now link to the Events page in these
scenarios, which typically contains useful information about what went wrong.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1365525[*1365525*])

* Previously, the OpenShift namespace appeared at the top of the list of
namespaces for the image stream tag picker, which was confusing in long lists of
namespaces if the user was expecting to find it alphabetically in the drop-down
menu. This happened because the image stream tag picker was adding the OpenShift
namespace to the list after the list was already sorted. The list of namespaces
the user can pick from is now sorted after the OpenShift namespace is added to
the list. Now the list of namespaces a user can pick from, when selecting an
image stream tag for build configuration, options have OpenShift sorted
alphabetically with the other namespaces the user can access.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1436819[*BZ#1436819*])

* The web console now better uses the screen space when displaying services.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1401134[*BZ#1401134*])

*Metrics*

* Previously, partitions in the `metrics_idx` table cause Cassandra to write into
the table packets that are as large as 496 MB and even 700 MB, causing client
requests to Hawkular Metrics to fail. A workaround of changing the compaction
strategy for the `metrics_idx` table from `LCS` to `STCS` was created, leading
to a new, persisting Hawkular image.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1422271[*BZ#1422271*])

* The internal metadata around the Cassandra schema was out of date, leading to
the data being a mix of old and new schema information. The version has been
updated.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1466086[*BZ#1466086*])

*Networking*

* Previously, the {product-title} node proxy did not support using a specified IP
address. This prevented correct operation on hosts with multiple network
interface cards. The {product-title} node process already accepts a
`--bind-address=<ip address>:<port>` command-line flag and `bindAddress:`
configuration file option for the multiple network interface card case. The
proxy functionality is now fixed to respect these options. When `--bind-address`
or `bindAddress` are used, the {product-title} node proxy should work correctly
when the {product-title} node host has multiple network interface cards.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1462428[*1462428*])

* Previously, when an IP address was re-used, it would be generated with a random
MAC address that would be different from the previous one, causing any node with
an ARP cache that still held the old entry for the IP to not communicate with
the node. Now, generating the MAC address deterministically from the IP address
now results in a re-used IP address always having the same MAC address, so the
ARP cache can not be out of sync. This ensures the traffic will now flow.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1451854[*BZ#1451854*])

* Previously, the VNID allow rules were removed before they were really unused.
This meant that if there were still pods in that namespace on the node, they
could not communicate with one another. The way that the tracking is done was
changed so to avoid the edge cases around pod creation or deletion failures.
This meant that the VNID tracking does not fail, so traffic flows.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1454948[*BZ#1454948*])

* Previously, running `oc adm diagnostics NetworkCheck` would result in a timeout
error. Changing the script to run from the pod definition fixed the issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1421643[*BZ#1421643*])

* Previously, using an F5 router did not work with re-encrypt routes. Adding the
re-encrypt routes to the same vserver fixed the problem.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1431655[*BZ#1431655*])

* Previously, there was a missing `iptables` rule to block `INVALID` packets,
causing packets to escape cluster. The missing rule was added missing rule
resulting in no more leaks.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1438762[*BZ#1438762*])

* Minor enhancements have been made to the `iptables` proxier to reduce node CPU
usage when many pods and services exist.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1387149[*BZ#1387149*])

* Previously, some fragmented IP packets were mistakenly dropped by
`openshift-node` instead of being delivered to pods, causing large UDP and TCP
packets to have some or all fragments dropped instead of being delivered. The
relevant fragments are now correctly evaluated and sent to their destination,
meaning large UDP and TCP packets should now be delivered to the intended pods
in the cluster.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1419692[*BZ#1419692*])

* Previously, the ARP cache was not compatible with OpenShift clusters with a
large number of routes (more than the default value of `1024`). The default has
been changed to `65536`, meaning clusters with many routes will function.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1425388[*BZ#1425388*])

* Previously, using `oc expose svc` picked up the service port instead of the
target port, meaning the route would not work. The command is now picked up from
the port number.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1431781[*BZ#1431781*])

* Previously, the hybrid proxy was not correctly protecting access to internal
data. This meant that, when it was enabled, it could terminate the
`openshift-node` process with a runtime panic due to concurrent data accesses.
As a fix, all internal data is correctly protected against concurrent access,
meaning the `openshift-node` process should no longer panic with concurrent data
access failures when the hybrid proxy is enabled.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1444125[*BZ#1444125*])

* Previously, after adding the
`netnamespace.network.openshift.io/multicast-enabled=true` annotation to
`netnamespace`, it will create one open-flow rule in table 110, but the
annotation is still there after deletion. The problem has now been fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1449058[*BZ#1449058*])

* Previously, the CLI help text was not clear about what worked on the F5 versus
the HAProxy routers. The CLI help text has been updated with clearer
expectations.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1427774[*BZ#1427774*])

* Previously, having multiple node IP addresses reported in random order by node
status. This led to the SDN controller picking up a random IP each time. IP
stickiness is now maintained, meaning the IP is valid when chosen.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1438402[*BZ#1438402*])

* Previously, cluster-external traffic was handled incorrectly when using the
Technology Preview `NetworkPolicy` plug-in. Pods could not connect to IP
addresses outside the cluster. The issue has been resolved and external traffic now works
correctly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1443765[*BZ#1443765*])

* Previously, the code to set up multicast was not run when only one node was in
the cluster, leading to multicast traffic dropping when on a single-node
cluster. The rules have been changed so the multicast setup is performed for a
single-node.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1445501[*BZ#1445501*])

* Previously, the initialization order of the SDN plug-in set up the event handler
too late, causing early events to have no handler, so the SDN would panic. The
SDN initialization has been re-ordered so that the event handler is in place
before it can be called.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1445504[*BZ#1445504*])

* Previously, the iptables rules were logged at too low of a log level, causing
the logs to fill with iptables noise. The level at which they are logged has
changed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1455655[*BZ#1455655*])

* Previously, the `NetworkPolicy` plug-in (currently in Tech Review) in
{product-title} 3.5 did not implement all features of `NetworkPolicy`. When
using certain `NetworkPolicy` resources that used `PodSelectors`, pods would be
accessible by pod IP, but not by service IP. These issues have been addressed.
All connections that should be allowed by a `NetworkPolicy` are now allowed
whether made directly (pod-to-pod) or indirectly via a service IP.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1419430[*BZ#1419430*])

*REST API*

* `maxScheduledImageImportsPerMinute` was previously documented as accepting `-1`
as a value to allow unlimited imports. This would cause the cluster to panic.
`maxScheduledImageImportsPerMinute` now correctly accepts `-1` as an unlimited
value.  Administrators who have set `maxScheduledImageImportsPerMinute` to an
extremely high number as a workaround may leave the existing setting or now use
`-1`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1388319[*BZ#1388319*])

* Previously, deleting created resources from a project failed to delete the route
and an error message was shown on the web console. The issue has been resolved.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1452569[*BZ#1452569*])

*Routing*

* This enhancement strips HTTP `Proxy` headers to prevent the `httpoxy`
(`\https://httpoxy.org/`) vulnerability. Applications behind the router are now
protected from `httpoxy`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1469633[*1469633*])

* Previously, when quickly adding then deleting a route using the CLI, routes are
queued up to be processed, saving the request data in a store, then acts on
them. The problem is the store is empty when the last request is popped, causing
an issue. This bug fix resolves the issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1447928[*BZ#1447928*])

* This bug fixes the matching logic change, which made the trailing slashed
inadvertently break, meaning that subpaths with trailing `/`s no longer worked.
The code that matches them has been corrected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1448944[*BZ#1448944*])

* Previously, the logic in the HAProxy router template did not account for `Allow`
as `InsecureEdgeTerminationPolicy` for re-encrypt routes, because the cookie
object was set as secure. Logic has been added to correctly tag the cookie as
insecure when `InsecureEdgeTerminationPolicy` is `Allow` for re-encrypt routes.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1428720[*BZ#1428720*])

* Previously, the command to create a list of routes was incorrect, meaning the
route statuses did not get deleted. The logic enumerating routes has been
improved.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1429364[*BZ#1429364*])

* Previously, the script did not check the version of `jq` and does not populate
its array of routes correctly, leading to the script failing when using `-r`.
The fix was to check to make sure the user has an appropriate version of `jq`
and populate the array of target routes properly. Then, the script correctly
clears the routes specified of status messages.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1429398[*BZ#1429398*])

* Previously, the router template did not add `option forwardfor` to re-encrypt
type routes, causing the `X-Forwarded-For` section of *_http header_* file to go
missing. This bug fix adds `option forwardfor` in the router template for the
re-encrypt type routes. Now the `X-Forwarded-For` section of the *_http header_*
file will correctly populate.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1449022[*BZ#1449022*])

* Version 3.6 router introduced a new port named `router-stats`. This bug created
an option for `oc adm router` command to allow a user to specify customized a
router-stats port, such as `--stats-port=1936`, so that user could easily create
an customized router.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1452019[*BZ#1452019*])

* This bug tracks the changing matching logic leading to trailing slashed
inadvertently breaking, leading to subpaths with trailing `/`s no longer working.
The code that matches them has been corrected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1446627[*BZ#1446627*])

* This bug added the feature that using `ROUTER_BACKEND_PROCESS_ENDPOINTS=shuffle`
will randomize the order of back-ends in the HAProxy configuration. With long
running sessions and a router that reloads regularly, the first endpoint in the
configuration may receive significantly more load than other back-ends. Setting
the environment variable will randomize the order of the back-ends on every
reload and, thus, help spread the load.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1447115[*BZ#1447115*])

*Storage*

* When an OpenShift node crashed before unmapping a RBD volume, the advisory lock
held on the RBD volume was not released. This prevented other nodes from using
the RBD volume till the advisory lock is manually removed. Now, if no RBD client
is using the RBD volume, the advisory lock is removed automatically. Thus, the
RBD volume can be used by other nodes without manually removing the lock.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1365867[*BZ#1365867*])

* Attach operations on AWS were slow because of duplicate API calls and frequent
polling of volume information. In the latest version, the duplicate API calls
are removed from the code and bulk polling of AWS volumes is implemented, to
avoid API quota problems.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1392357[*BZ#1392357*])

* For persistent volumes, the default mount options provided by OpenShift were not
customizable. Users can now tweak mount options for persistent volumes
(including NFS and other volume types that support it) depending on their
storage configuration.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1405307[*BZ#1405307*])

* The `recycle` reclaim policy is deprecated in favor of dynamic provisioning and
it will be removed in future releases.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1432281[*BZ#1432281*])

* OpenStack Cinder v1 API got deprecated in recent OpenStack release. OpenShift
now supports OpenStack v2 API.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1427701[*BZ#1427701*])

* In kubelet logs, a running pod was sometimes reported as _'cannot start, time
out waiting for the volume'_. Because the kubelet's volumemanager reconstructor
for actual state of world was running before the desired state of world was
populated, which caused the pods in the actual state of world, to have incorrect
volume information. This issue is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1444096[*BZ#1444096*])

* The OpenStack Cinder StorageClass ignored availability zones because of an issue
in the `gophercloud/gophercloud` library. OpenStack Cinder StorageClass now
provisions volumes in the specified availability zone and fails if the specified
availability zone does not exist.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1444828[*BZ#1444828*])

* When mounting volumes using a subpath, the subpath did not receive correct
permissions. This issue is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1445526[*BZ#1445526*])

* Volumes failed to detach after unmounting them from the node. Because Openshift
did not attempt detach operation for pods that were completed (or terminated)
but were not deleted from API server. Thus preventing reuse of volume in other
pods. This bug is fixed and volumes for terminated or completed pods are
detached automatically.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1446788[*BZ#1446788*])

* If the availability optional parameter was not provided for the OpenStack Cinder
StorageClass, all Persistent Volumes provisioned for the Persistent Volume
Claims that used the specified StorageClass were provisioned in the `nova` zone.
Now, such Persistent Volumes are provisioned in an active zone where OpenShift
has a node.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1447568[*BZ#1447568*])

* Pods failed to start, if they specified a file as a volume subPath to mount.
This is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1451008[*BZ#1451008*])

* OpenShift failed to attach disks to the Azure F-Series VMs. This issue is now
fixed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1451039[*BZ#1451039*])

* Previously, when a node stopped (or rebooted) the ones using EBS volumes were
failing because the volume was not detached from the stopped node. Now the
volume gets successfully unmounted and detached from node.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1457510[*BZ#1457510*])

* High OpenShift process CPU utilization is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1460280[*BZ#1460280*])

* Previously, the `AccessModes` field of a PVC was ignored when a PV was
dynamically provisioned for it. This caused users to receive a PV with
inaccurate `AccessModes`. Now the dynamic provisioning of PVs with inaccurate
`AccessModes` are not provisioned when PVCs ask for `AccessModes` that can't be
satisfied by the PVs' underlying volume plugin.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1462275[*BZ#1462275*])

* Dynamically created Azure blob containers were accessible on public internet.
 This happened because the default access permissions for Persistent Volumes
 (PVs) were set to `container` which exposed a publically accessible URI. The
 container permissions are now set to `private` for provisioned Azure Volumes.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1462551[*BZ#1462551*])

* Sometimes, even after the PV and volume are proivisioned successfully, there was
a failed volume creation event in the logs. This issue is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1395547[*BZ#1395547*])

* This bug made it possible to specify multiple `targetPortals` to make use of
 iSCSI multipath, which is the de-facto standard in environments that use iSCSI.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1376022[*BZ#1376022*])

*Upgrades*

* Previously, when 3.1 version of `etcd` was available, the `etcd` RPM did not get
upgraded to the version during the control plane upgrade. The playbook
responsible for `etcd` upgrading is now extended and the `etcd` RPM (and `etcd`
docker images) are properly upgraded to `3.1.{asterisk}`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1421587[*BZ#1421587*])

* To minimize the attack surface for containers escaping namespace isolation, the
label `svirt_sandbox_file_t` on `/var/lib/origin/openshift.local.volumes/` was
removed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1450167[*BZ#1450167*])

* Previously, when named certificates were added to ansible hosts file, and
 certificate redeploy playbook was run, certificates were not added to
 `master-config.yaml`. This issue is now fixed.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1454478[*BZ#1454478*])

* Previously, the certificate redeployment playbook would not update master
configuration when named certificates were provided. Named certificates will now
be replaced and master configuration will be updated during certificate
redeployment.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1455485[*BZ#1455485*])

* The OpenShift upgrade got applied to all nodes if
`openshift_upgrade_nodes_label` fits no label. Now the installer verifies the
provided label and matches a set of hosts prior to upgrading.  If the label does
not match hosts, the upgrade would silently proceed with upgrading all nodes
given the logic for creating the `oo_nodes_to_upgrade` group.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1462992[*BZ#1462992*])

* If the version of etcd used to produce the etcd backup was version 3.x the
backup can only be loaded by etcd 3.x. This occurs when running etcd in a
containerized install and the version of the rpm installed on the host differs
from that running inside the container. We have updated the backup playbooks to
use the version of etcd from within the container which ensures that a matching
version of etcd is used.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1402771[*BZ#1402771*])

* Previously, the registry-console would use an older image version even after
upgrade. Since registry-console was installed by default, the upgrade playbook
did not update the registry-console deployment configuration to use the same
version as docker-registry. This issue is now fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1421987[*BZ#1421987*])

[[ocp-36-technology-preview]]
== Technology Preview Features

Some features in this release are currently in Technology Preview. These
experimental features are not intended for production use. Please note the
following scope of support on the Red Hat Customer Portal for these features:

https://access.redhat.com/support/offerings/techpreview[Technology Preview
Features Support Scope]

The following new features are now available in Technology Preview:

- xref:../admin_guide/quota.adoc#limited-resources-quota[Require Explicit Quota to Consume a Resource]
- xref:../architecture/additional_concepts/storage.adoc#pv-mount-options[Mount Options]
- xref:ocp-36-automated-installation-of-cloudforms[Automated installation of CloudForms 4.5 Inside OpenShift]
- xref:../install_config/install/advanced_install.adoc#advanced-install-configuring-system-containers[Installation of etcd, Docker Daemon, and Ansible Installer as System Containers]
- xref:ocp-36-running-openshift-installer-as-a-system-container[Running OpenShift Installer as a System Container]
- xref:../architecture/service_catalog/index.adoc#architecture-additional-concepts-service-catalog[Service Catalog]
- xref:ocp-36-ansible-service-broker[Ansible Service Broker]
- xref:ocp-36-ansible-playbook-bundles[Ansible Playbook Bundles (APB)]
- xref:ocp-36-initial-experience[Initial Experience]
- xref:ocp-36-search-catalog[Search Catalog]
- xref:ocp-36-add-from-catalog[Add from Catalog]
- xref:ocp-36-add-to-project[Add to Project]
- xref:ocp-36-bind-in-context[Bind in Context]
- xref:ocp-36-template-service-broker[Template Service Broker]
- xref:ocp-36-service-catalog-experience[Service Catalog Experience in the CLI]
- xref:ocp-36-mux[`mux`]

The following features that were formerly in Technology Preview from a previous
{product-title} release are now fully supported:

- xref:../architecture/core_concepts/containers_and_images.adoc#init-containers[Init
containers]

The following features that were formerly in Technology Preview from a previous
{product-title} release remain in Technology Preview:

- xref:../dev_guide/cron_jobs.adoc#dev-guide-cron-jobs[Cron Jobs (formerly called Scheduled Jobs)]
- xref:../admin_guide/managing_networking.adoc#admin-guide-manage-networking[Network Policy]
- xref:../dev_guide/deployments/kubernetes_deployments.adoc#dev-guide-kubernetes-deployments-support[Kubernetes
Deployments Support]
- xref:../admin_guide/managing_pods.adoc#managing-pods-poddisruptionbudget[Pod Distribution Budgets]

- `StatefulSets` formerly known as `PetSets`

[[ocp-36-known-issues]]
== Known Issues

* When running an upgrade with ``--tags pre_upgrade`, the upgrade failed with:
+
----
"the file_name '/usr/share/ansible/openshift-ansible/playbooks/common/openshift-cluster/upgrades/etcd/noop.yml' does not exist, or is not readable"
----
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1464025[*BZ#1464025*])

* When de-provisioning an Ansible Playbook Bundle (APB) via the
 `service-catalog` and `ansible-service-broker`, the *Provisioned Service* entry
 will linger for longer than the assets created by the APB. The service itself
 will have correctly been de-provisioned. This is due to the service catalog
 eventually confirming with the Ansible Service Broker that the service is
 actually gone. It was
 link:https://github.com/kubernetes-incubator/service-catalog/pull/1067[patched
 upstream].
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1475251[*BZ#1475251*])

* The Ansible playbooks for running OpenShift pre-installation and health checks
may have unexpected side-effects, as they have dependencies on code from the
installer for configuring hosts. This may result in changes to configuration for
yum repositories, Docker, or the firewall for hosts where configuration differs
from the settings specified by the Ansible inventory. Therefore, users should
avoid running these playbooks with an inventory configuration that could result
in changing the cluster in these areas.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1476890[*BZ#1476890*])

* When upgrading from a release of {product-title} less than 3.5.5.18, the upgrade
process may remove data on persistent volumes that fail to unmount correctly. If
you are running a version less than 3.5.5.18, perform the following steps prior
to performing the normal upgrade process:
+
----
# atomic-openshift-excluder unexclude
# yum upgrade atomic-openshift-node
# systemctl restart atomic-openshift-node
----
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1463393[*BZ#1463393*])

* In {product-title} 3.5 and earlier, the Fluentd image included
`fluent-plugin-elasticsearch` version 1.9.2 and earlier. This version will
silently drop records sent in a bulk index request when the queue size is
link:https://github.com/uken/fluent-plugin-elasticsearch/blob/v1.9.2/lib/fluent/plugin/out_elasticsearch.rb#L353[full].
In {product-title} 3.6, which uses version 1.9.5, an error log message was
added, which is why the `Error: status=429` message in the Fluentd logs
link:https://github.com/uken/fluent-plugin-elasticsearch/blob/v1.9.5/lib/fluent/plugin/out_elasticsearch.rb#L355[occurs].
+
To reduce the frequency of this problem, you can increase the Fluentd buffer
chunk size. However, testing does not give consistent results.  You will need to
stop, configure, and restart Fluentd running on all of your nodes.

. Edit the daemonset:
+
----
# oc edit -n logging daemonset logging-fluentd
----

. In the `env:` section, look for `BUFFER_SIZE_LIMIT`. If the value is less than
`8Mi` (8 megabytes), change the value to `8Mi`. Otherwise, use a value of `16Mi`
or `32Mi`. This will roughly increase the size of each bulk index request, which
should decrease the number of such requests made to Elasticsearch, thereby
allowing Elasticsearch to process them more efficiently.

. Once the edit is saved, the Fluentd daemonset trigger should cause a restart of
all of the Fluentd pods running in the cluster.
+
You can monitor the Elasticsearch bulk index thread pool to see how many bulk
index requests it processes and rejects.

. Get the name of an Elasticsearch pod:
+
----
# oc get -n logging pods -l component=es

# espod=$name_of_es_pod
----

. Run the following command:
+
----
# oc exec -n logging $espod -- \
  curl -s -k --cert /etc/elasticsearch/secret/admin-cert \
  --key /etc/elasticsearch/secret/admin-key \
https://localhost:9200/_cat/thread_pool?v\&h=host,bulk.completed,bulk.rejected,bulk.queue,bulk.active,bulk.queueSize
----
+
The output looks like this:
+
----
host       bulk.completed bulk.rejected bulk.queue bulk.active bulk.queueSize
10.128.0.6           2262             0          0           0             50
----
+
The term `completed` means the number of bulk indexing operations that have been
completed. There will be many (hundreds or thousands of) log records per bulk
index request.
+
The term `queue` is the number of pending requests that have been queued up for
the server to process. Once this queue is full, additional operations are
rejected.
+
Note the number of `bulk.rejected` operations. These correspond to `error
status=429` in your Fluentd pod logs. Rejected operations means that Fluentd
dropped these records, and you might need to increase the chunk size again.
+
If you have multiple nodes running Elasticsearch, they will each be listed in
the `curl` output.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1470862[*BZ#1470862*])

* When performing builds using an image source input, directories within the input
content are injected with permissions of 0700 with the default image user as the
owner. This means the content is unlikely to be accessible when the application
image is run under a random UID. This can be worked around by performing a
`chmod` operation in either the assemble script (for S2I builds) or the
Dockerfile (for Docker builds). Most {product-title} S2I builder images already
perform this `chmod` operation, but custom built S2I builder images or builds
using custom assemble scripts may not.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1479130[*BZ#1479130*])

* There is a known issue affecting logging behavior when using the non-default
`json-file` log driver. As a workaround, remove line
`/var/lib/docker/containers` from *_/etc/oci-umount.conf_*, then restart
`docker`, OpenShift services, and the Fluentd  pod.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1477787[*BZ#1477787*])

[[ocp-36-asynchronous-errata-updates]]
== Asynchronous Errata Updates

Security, bug fix, and enhancement updates for {product-title} 3.6 are released
as asynchronous errata through the Red Hat Network. All {product-title} 3.6
errata is https://access.redhat.com/downloads/content/290/[available on the Red
Hat Customer Portal]. See the
https://access.redhat.com/support/policy/updates/openshift[{product-title}
Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account
settings for Red Hat Subscription Management (RHSM). When errata notifications
are enabled, users are notified via email whenever new errata relevant to their
registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming
{product-title} entitlements for {product-title} errata notification
emails to generate.
====

This section will continue to be updated over time to provide notes on
enhancements and bug fixes for future asynchronous errata releases of
{product-title} 3.6. Versioned asynchronous releases, for example with the form
{product-title} 3.6.z, will be detailed in subsections. In addition, releases in
which the errata text cannot fit in the space provided by the advisory will be
detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[upgrading your cluster] properly.
====

[[ocp-3-6-173-0-5-4]]
=== RHEA-2017:2475 - {product-title} 3.6.173.0.5-4 Images Update

Issued: 2017-08-15

{product-title} release 3.6.173.0.5-4 is now available. The list of container
images included in the update are documented in the
link:https://access.redhat.com/errata/RHEA-2017:2475[RHEA-2017:2475] advisory.

The container images in this release have been updated using the latest base
images.

[[ocp-3-6-173-0-5-4-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.5 or 3.6 cluster to this latest release, use the
automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-6-rhba-2017-1829]]
=== RHBA-2017:1829 - {product-title} 3.6.173.0.5 Bug Fix Update

Issued: 2017-08-31

{product-title} release 3.6.173.0.5 is now available. The container images
included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2017:1829[RHBA-2017:1829] advisory
and listed in xref:ocp-3-6-rhba-2017-1829-images[Images].

Space precluded documenting all of the images for this release in the advisory.
See the following sections for notes on upgrading and details on the images
included in this release.

[[ocp-3-6-rhba-2017-1829-images]]
==== Images

This release updates the Red Hat Container Registry
(`registry.access.redhat.com`) with the following images:

----
openshift3/ose-pod:v3.6.173.0.5-5
rhel7/pod-infrastructure:v3.6.173.0.5-5
openshift3/ose-ansible:v3.6.173.0.5-5
openshift3/ose:v3.6.173.0.5-5
openshift3/ose-docker-registry:v3.6.173.0.5-5
openshift3/ose-egress-router:v3.6.173.0.5-5
openshift3/ose-keepalived-ipfailover:v3.6.173.0.5-5
openshift3/ose-f5-router:v3.6.173.0.5-5
openshift3/ose-deployer:v3.6.173.0.5-5
openshift3/ose-haproxy-router:v3.6.173.0.5-5
openshift3/node:v3.6.173.0.5-5
openshift3/ose-sti-builder:v3.6.173.0.5-5
openshift3/ose-docker-builder:v3.6.173.0.5-5
openshift3/logging-deployer:v3.6.173.0.5-5
openshift3/logging-curator:v3.6.173.0.5-5
openshift3/metrics-deployer:v3.6.173.0.5-5
openshift3/logging-auth-proxy:v3.6.173.0.5-5
openshift3/logging-elasticsearch:v3.6.173.0.5-5
openshift3/logging-fluentd:v3.6.173.0.5-9
openshift3/logging-kibana:v3.6.173.0.5-7
openshift3/metrics-cassandra:v3.6.173.0.5-5
openshift3/metrics-hawkular-metrics:v3.6.173.0.5-5
openshift3/metrics-hawkular-openshift-agent:v3.6.173.0.5-5
openshift3/metrics-heapster:v3.6.173.0.5-5
openshift3/jenkins-1-rhel7:v3.6.173.0.5-5
openshift3/jenkins-5-rhel7:v3.6.173.0.5-5
openshift3/jenkins-slave-base-rhel7:v3.6.173.0.10-5                                                                                                                                                                ​
openshift3/jenkins-slave-maven-rhel7:v3.6.173.0.10-5
openshift3/jenkins-slave-nodejs-rhel7:v3.6.173.0.10-5
openshift3/registry-console:v3.6.173.0.5-5
openshift3/mediawiki-123:v3.6.173.0.10-5
openshift3/apb-base:v3.6.173.0.10-5
openshift3/ose-ansible-service-broker:v3.6.173.0.10-5
openshift3/mediawiki-apb:v3.6.173.0.10-5
openshift3/postgresql-apb:v3.6.173.0.10-5
openshift3/ose-federation:v3.6.173.0.5-5
openshift3/openvswitch:v3.6.173.0.5-5
----

[[ocp-3-6-rhba-2017-1829-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.5 or 3.6 cluster to this latest
release, use the automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-6-rhba-2017-2639]]
=== RHBA-2017:2639 - atomic-openshift-utils Bug Fix and Enhancement Update

Issued: 2017-09-05

{product-title} bug fix and enhancement advisory
link:https://access.redhat.com/errata/RHBA-2017:2639[RHBA-2017:2639], providing
updated *atomic-openshift-utils*, *ansible*, and *openshift-ansible* packages
that fix several bugs and add enhancements, is now available.

[[ocp-3-6-rhba-2017-2639-upgrading]]
==== Upgrading

To apply this update, run the following on all hosts where you intend to
initiate Ansible-based installation or upgrade procedures:

----
# yum update atomic-openshift-utils
----

[[ocp-3-6-rhba-2017-2642]]
=== RHBA-2017:2642 - {product-title} 3.6.1 Bug Fix and Enhancement Update

Issued: 2017-09-08

{product-title} release 3.6.1 is now available. The packages and bug fixes
included in the update are documented in the
https://access.redhat.com/errata/RHBA-2017:2642[RHBA-2017:2642] advisory. The
list of container images included in the update are documented in the
link:https://access.redhat.com/errata/RHEA-2017:2644[RHEA-2017:2644] advisory.

The container images in this release have been updated using the latest base
images.

[[ocp-3-6-rhba-2017-2642-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.5 or 3.6 cluster to this latest
release, use the automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-6-rhba-2017-2847]]
=== RHBA-2017:2847 - {product-title} 3.6.173.0.21 Images Update

Issued: 2017-10-03

{product-title} release 3.6.173.0.21 is now available. The
list of container images included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:2847[RHBA-2017:2847] advisory.

The container images in this release have been updated using the latest base
images.

[[ocp-3-6-rhba-2017-2847-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.5 or 3.6 cluster to this latest
release, use the automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-6-rhba-2017-3049]]
=== RHBA-2017:3049 - {product-title} 3.6.173.0.49 Bug Fix and Enhancement Update

Issued: 2017-10-25

{product-title} release 3.6.173.0.49 is now available. The list of packages
included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:3049[RHBA-2017:3049] advisory.
The container images included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2017:3050[RHBA-2017:3050] advisory.

Space precluded documenting all of the bug fixes, enhancements, and images for
this release in the advisories. See the following sections for notes on
upgrading and details on the bug fixes, enhancements, and images included in
this release.

[[ocp-3-6-rhba-2017-3049-bug-fixes]]
==== Bug Fixes

[discrete]
===== Image Registry

* There was no way to prune orphaned blobs from the OpenShift Container Registry's storage. Orphaned blobs could pile up and consume a considerable amount of free space. This bug fix provides a new low-level utility that is run inside of registry's container and removes the orphaned blobs. As a result, administrators are now able to remove orphaned blobs to retrieve storage space. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1479340[*BZ#1479340*])

* The OpenShift Container Registry used to append the forwarded target port to redirected location URLs. The registry client would get confused by the received location containing a superfluous port, and could not match it against the original host. This happened when exposed with TLS-termination other than passthrough. The client's new request to the target location lacked credentials, and as a consequence, the image push failed due to authorization error. This bug fix rebases the registry to a newer version, which fixes forwarding processing logic. As a result, the registry no longer confuses its clients; clients can push images successfully to the exposed registry using arbitrary TLS-termination. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1489042[*BZ#1489042*])

* Images younger than the threshold were not added to the dependency graph. Blobs used by a young image and by a prunable image were deleted because they had no references in the graph. This bug fix adds young images to the graph and marks them as non-prunable. As a result, blobs now have references and are not deleted. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1498124[*BZ#1498124*])

* Neither documentation nor CLI help talked about insecure connections to the secured registry. Errors used to be hard to decipher when users attempted to prune the secured registry with a bad CA certificate. This bug fix ensures that errors are now printed with hints, CLI help has been updated, and new flags have been provided to allow for insecure fallback. As a result, users can now easily enforce both secure and insecure connections and understand any HTTPS errors and how to resolve them. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1476779[*BZ#1476779*])

* The ClusterRegistry diagnostic checks the registry given to ImageStreams by default with the known registry service. It compares the IP. However,  with OpenShift Container Platform 3.6, the ImageStream now gets a cluster hostname for the registry instead of an IP. Therefore, the diagnostic reports a false error condition because the IP is not the same as the hostname. With this bug fix, the diagnostic now checks if either of the hostname and IP version of the registry matches. The diagnostic now reports correctly against either old or new deployments. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1488059[*BZ#1488059*])

[discrete]
===== Logging

* Messages were previously read into Fluentd's memory buffer and were lost if the pod was restarted. Because Fluentd considers them read even though they have not been pushed to storage, any message not stored but already read by Fluentd was lost. This bug fix replaces the memory buffer with a file-based buffer. As a result, file-buffered messages are pushed to storage once Fluentd restarts. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1483114[*BZ#1483114*])

* The pattern for container logs in the journal field `CONTAINER_NAME` changed. The pattern was not matching for logs from pods in the `default`, `openshift`, or `openshift-infra` namespaces. Logs from these namespaces were being stored in indices matching `project.default.++*++`, for example rather than `.operations.++*++`. This bug fix updates the pattern matcher to match the correct pattern. As a result, logs from pods in the affected namespaces are correctly written to the `.operations.++*++` indices. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1493188[*BZ#1493188*])

* Fluentd could not write the files it uses for buffering records due to a problem converting values from ASCII-8BIT to UTF-8, causing Fluentd to emit numerous errors and be unable to add records to Elasticsearch. This bug fix removes the patch that forced the UTF-8 conversion. As a result, Fluentd can write ASCII-8BIT encoded files for its buffer. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1482002[*BZ#1482002*])

* Non-operations users were not given the proper access to scroll data. This caused users to see a 403 action denied error in Kibana. This bug fix provides cluster-level permissions for the failed action for ordinary users. As a result, users are now able to export UI objects. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1463436[*BZ#1463436*])

* Fluentd was not removing the Kubernetes metadata filter configuration when being used as a mux client. This caused Fluentd to continue opening connections to the OpenShift API server. This bug fix ensures that the Kubernetes metadata filter configuration file is removed when Fluentd is being used as a mux client. As a result, there is no longer a connection from Fluentd running as a mux client to the OpenShift API server. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1464024[*BZ#1464024*])

* In OpenShift Container Platform 3.5 and earlier, the Fluentd image included *fluent-plugin-elasticsearch* version 1.9.2 and earlier. This version will silently drop records sent in a bulk index request when the queue size is full. In OpenShift Container Platform 3.6, which uses version 1.9.5, an error log message was added, causing the “Error: status=429” messages in the Fluentd logs when this occurs. With this bug fix (*fluent-plugin-elasticsearch* version 1.9.5.1), when Fluentd gets an error response from Elasticsearch, Fluentd will retry the operation until it succeeds. However, it will also retry successful operations too in some cases, which will lead to duplicate records. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1491401[BZ#1491401] for more details. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1470862[*BZ#1470862*])

* Fluentd does not stop reading from the journal when the output queue is full. Records are dropped until Fluentd can empty the queue by sending records to Elasticsearch. This bug fix introduces a new configuration parameter, `buffer_queue_full_action`, to all output plug-ins. If using the journal as input, Fluentd will use a value of `block` for this parameter, which will cause Fluentd to stop reading from the journal until Fluentd is able to flush the queue. As a result, records are no longer dropped in this scenario. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1473788[*BZ#1473788*])

* The collectd schema was using `int` instead of `long` for the field values, causing long integer values not to be stored. This bug fix uses `long` instead of `int` in the schema for those fields. As a result, all long integer values can now be stored. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1479645[*BZ#1479645*])

* There was previously no configuration to distinguish which Kibana instance to use, and users were always routed to the same instance. This bug fix adds an annotation to the operations namespace which has the Kibana host name. As a result, users are routed to the Kibana instance specified in the annotation if it exists on the project. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1480988[*BZ#1480988*])

* The Fluentd processing pipeline to format journald records (system and container logs) into the viaq data model format was using dozens of embedded ruby evaluations per record. The record processing was very slow, with excessive CPU usage. This bug fix moves the processing and formatting into dedicated ruby code in the viaq filter plug-in. As a result, the record processing is much faster, with less CPU usage. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1489976[*BZ#1489976*])

* Previously, the `k8s_meta_filter_for_mux_client` ruby code was missing from Flentd's Dockerfile, causing Fluentd to complain about the missing filter. This bug fix adds the file to the Dockerfile, and the errors no longer occur. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1490395[*BZ#1490395*])

* Previously, the openshift-elasticsearch-plugin improperly handled user names with backslashes. This caused users to be unable to access Elasticsearch. This bug fix modifies requests to convert backslashes to forwardslashes. As a result, users are able to access Elasticsearch with user names that contain backslashes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1491227[*BZ#1491227*])

* Fluentd logs previously filled up with warnings about "log unreadable. It is excluded and would be examined next time." when using the json-file log driver. This bug fix adds an exclude pattern to configuration to exclude messages with this priority level. As a result, the messages are no longer visible. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1478821[*BZ#1478821*])

[discrete]
===== Master

* In some failure cases, the etcd client used by OpenShift will not rotate through all the available etcd cluster members. The client will end up repeatedly trying the same server. If that server is down, then requests will fail for an extended time until the client finds the server invalid. If the etcd leader goes away when it is attempted to be contacted for something like authentication, then the authentication fails and the etcd client is stuck trying to communicate with the etcd member that does not exist. User authentication would fail for an extended period of time. With this bug fix, the etcd client now rotates to other cluster members even on failure. If the etcd leader goes away, the worst that should happen is a failure of that one authentication attempt. The next attempt will succeed because a different etcd member will be used. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1490427[*BZ#1490427*])

[discrete]
===== Networking

* Previously, the iptables proxy was not properly locking its use of iptables. Therefore, the iptables proxy could conflict with `docker` and the `openshift-node` process and cause a failure to start containers. The iptables proxy now locks its use of iptables. Pod creation failures due to improper locking of iptables no longer occur. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1417234[*BZ#1417234*])

* There was a locking issue with the egress DNS policy. This prevented the syncing of egress policies, which could lead to invalid egress access from the pods. This bug fix addresses the egress locking issue. Egress policies are now synced and work as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1445694[*BZ#1445694*])

* Conntrack entries for UDP traffic were not cleared when an endpoint was added for a service that previously had no endpoints. The system could end up incorrectly caching a rule that would cause traffic to that service to be dropped rather than being sent to the new endpoint. With this bug fix, the relevant conntrack entries are now deleted at the right time and UDP services work correctly when endpoints are added and removed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1497767[*BZ#1497767*])

[discrete]
===== Pod

* After an `atomic-openshift-node` restart, pods with init containers may have had their status reset when an attempt was made to reread the init container status. If the container was deleted, this would fail, resetting the status. This bug fix prevents rereading the init container status if the current status on the pod resource indicates the init container is terminated. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1491040[*BZ#1491040*])

* Pod anti-affinity is respected across projects. As a result, pod A from project 1 will not land on a node where pod B from project 2 is running, if pod anti-affinity is enabled when scheduling pod A. While scheduling pod A, check for pod anti-affinity only within the project of pod A. Pod anti-affinity will not be respected across projects. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1483119[*BZ#1483119*])

[discrete]
===== Routing

* HAProxy was sending IPv6 addresses in X-Forwarded headers when in ipv6 mode. The behavior changed, so clients that did not expect ipv6 would break. IPv6 mode must be enabled manually rather than defaulting to on. Now, the headers do not change unexpectedly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1472976[*BZ#1472976*])

[[ocp-3-6-rhba-2017-3049-enhancements]]
==== Enhancements

* Using Fluentd as a component for aggregating logs from Fluentd node agents, called _mux_, is a Technology Preview feature for OpenShift Container Platform 3.6. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1469859[*BZ#1469859*])

[[ocp-3-6-rhba-2017-3049-images]]
==== Images

This release updates the Red Hat Container Registry
(`registry.access.redhat.com`) with the following images:

----
openshift3/ose-f5-router:v3.6.173.0.49-4
openshift3/container-engine:v3.6.173.0.49-4
openshift3/jenkins-slave-maven-rhel7:v3.6.173.0.49-5
openshift3/logging-auth-proxy:v3.6.173.0.49-4
openshift3/logging-deployer:v3.6.173.0.49-5
openshift3/logging-fluentd:v3.6.173.0.49-4
openshift3/metrics-cassandra:v3.6.173.0.49-5
openshift3/metrics-hawkular-metrics:v3.6.173.0.49-5
openshift3/metrics-hawkular-openshift-agent:v3.6.173.0.49-4
openshift3/apb-base:v3.6.173.0.49-4
openshift3/ose-cluster-capacity:v3.6.173.0.49-4
openshift3/ose-docker-builder:v3.6.173.0.49-4
openshift3/ose-docker-registry:v3.6.173.0.49-4
openshift3/ose-federation:v3.6.173.0.49-4
openshift3/ose-keepalived-ipfailover:v3.6.173.0.49-4
openshift3/node:v3.6.173.0.49-5
openshift3/ose-pod:v3.6.173.0.49-4
openshift3/ose-service-catalog:v3.6.173.0.49-4
openshift3/jenkins-2-rhel7:v3.6.173.0.49-5
openshift3/ose-egress-http-proxy:v3.6.173.0.49-4
openshift3/ose-ansible:v3.6.173.0.49-5
openshift3/jenkins-slave-base-rhel7:v3.6.173.0.49-5
openshift3/jenkins-slave-nodejs-rhel7:v3.6.173.0.49-5
openshift3/logging-curator:v3.6.173.0.49-4
openshift3/logging-elasticsearch:v3.6.173.0.49-5
openshift3/logging-kibana:v3.6.173.0.49-5
openshift3/metrics-deployer:v3.6.173.0.49-5
openshift3/metrics-heapster:v3.6.173.0.49-4
openshift3/ose-ansible-service-broker:v3.6.173.0.49-4
openshift3/ose-deployer:v3.6.173.0.49-4
openshift3/ose:v3.6.173.0.49-4
openshift3/ose-egress-router:v3.6.173.0.49-4
openshift3/ose-haproxy-router:v3.6.173.0.49-4
openshift3/mediawiki-123:v3.6.173.0.49-4
openshift3/openvswitch:v3.6.173.0.49-5
openshift3/ose-recycler:v3.6.173.0.49-4
openshift3/ose-sti-builder:v3.6.173.0.49-4
openshift3/jenkins-1-rhel7:v3.6.173.0.49-5
openshift3/registry-console:v3.6.173.0.49-4
----

[[ocp-3-6-rhba-2017-3049-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.5 or 3.6 cluster to this latest
release, use the automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-6-rhsa-2017-3389]]
=== RHSA-2017:3389 - Moderate: {product-title} 3.6.173.0.63 Security, Bug Fix, and Enhancement Update

Issued: 2017-12-06

{product-title} release 3.6.173.0.63 is now available. The list of packages
included in the update are documented in the
link:https://access.redhat.com/errata/RHSA-2017:3389[RHSA-2017:3389] advisory.
The container images included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2017:3390[RHBA-2017:3390] advisory.

Space precluded documenting all of the bug fixes, enhancements, and images for
this release in the advisories. See the following sections for notes on
upgrading and details on the bug fixes and images included in this release.

[[ocp-3-6-rhsa-2017-3389-images]]
==== Images

This release updates the Red Hat Container Registry
(`registry.access.redhat.com`) with the following images:

----
openshift3/apb-base:v3.6.173.0.63-10
openshift3/container-engine:v3.6.173.0.63-11
openshift3/logging-auth-proxy:v3.6.173.0.63-11
openshift3/logging-curator:v3.6.173.0.63-11
openshift3/logging-deployer:v3.6.173.0.63-11
openshift3/logging-elasticsearch:v3.6.173.0.63-11
openshift3/logging-fluentd:v3.6.173.0.63-11
openshift3/logging-kibana:v3.6.173.0.63-11
openshift3/mediawiki-123:v3.6.173.0.63-11
openshift3/mediawiki-apb:v3.6.173.0.63-10
openshift3/metrics-cassandra:v3.6.173.0.63-11
openshift3/metrics-deployer:v3.6.173.0.63-11
openshift3/metrics-hawkular-metrics:v3.6.173.0.63-11
openshift3/metrics-hawkular-openshift-agent:v3.6.173.0.63-11
openshift3/metrics-heapster:v3.6.173.0.63-11
openshift3/node:v3.6.173.0.63-11
openshift3/openvswitch:v3.6.173.0.63-11
openshift3/ose-ansible-service-broker:v3.6.173.0.63-11
openshift3/ose-ansible:v3.6.173.0.63-11
openshift3/ose-base:v3.6.173.0.63-11
openshift3/ose-cluster-capacity:v3.6.173.0.63-11
openshift3/ose-deployer:v3.6.173.0.63-11
openshift3/ose-docker-builder:v3.6.173.0.63-11
openshift3/ose-docker-registry:v3.6.173.0.63-11
openshift3/ose-egress-http-proxy:v3.6.173.0.63-11
openshift3/ose-egress-router:v3.6.173.0.63-11
openshift3/ose-f5-router:v3.6.173.0.63-11
openshift3/ose-federation:v3.6.173.0.63-11
openshift3/ose-haproxy-router:v3.6.173.0.63-11
openshift3/ose-keepalived-ipfailover:v3.6.173.0.63-11
openshift3/ose-pod:v3.6.173.0.63-11
openshift3/ose-recycler:v3.6.173.0.63-11
openshift3/ose-service-catalog:v3.6.173.0.63-11
openshift3/ose-sti-builder:v3.6.173.0.63-11
openshift3/ose:v3.6.173.0.63-11
openshift3/postgresql-apb:v3.6.173.0.63-10
openshift3/registry-console:v3.6.173.0.63-11
----

[[ocp-3-6-rhsa-2017-3389-bug-fixes]]
==== Bug Fixes

[discrete]
===== Authentication

* During upgrades, reconciliation happens only for cluster roles automatically,
but this role needs to be adjusted in 3.6 due to enablement of API groups in
this release. The Ansible upgrade code has been changed to address this role
upgrade.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1493213[*BZ#1493213*])

[discrete]
===== Image Registry

* The size of a cached layer did not get counted. Therefore, the layer size for
cached layers was zero. Counting the size for cached layers now allows images to
have proper layer sizes.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1457042[*BZ#1457042*])

[discrete]
===== Logging

* `openshift-elasticsearch-plugin` was creating ACL roles based on the provided
name, which could include slashes and commas. This caused the dependent library
to not properly evaluate roles. With this bug fix, hash the name when creating
ACL roles so they no longer contain the invalid characters.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1494239[*BZ#1494239*])

* If the logging system is under a heavy load, it may take longer than the
five-second timeout for Elasticsearch to respond, or it may respond with an
error indicating that Fluentd needs to back off. In the former case, Fluentd
will retry to send the records again, which can lead to having duplicate
records. In the latter case, if Fluentd is unable to retry, it will drop
records, leading to data loss. For the former case, the fix is to set the
`request_timeout` to 10 minutes, so that Fluentd will wait up to 10 minutes for
the reply from Elasticsearch before retrying the request. In the latter case,
Fluentd will block attempting to read more input, until the output queues and
buffers have enough room to write more data. This bug fix greatly reduces
chances of duplicate data (though it is not entirely eliminated). Also, there is
no data loss due to back pressure.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1497836[*BZ#1497836*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1501948[*BZ#1501948*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1506854[*BZ#1506854*])

[discrete]
===== Management Console

* The management console was defaulting to the legacy API group `extensions` for
jobs. As a result, the legacy API group appeared in the UI in places such as
*Edit YAML*. With this bug fix, the console snow uses the new `batch` API group
as the default for job resources. The API group and version on a job resource
now appear as `batch/v1` wherever it is visible in the console.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1506233[*BZ#1506233*])

[discrete]
===== Metrics

* Extra, unnecessary queries were being performed on each request. The GET
`/hawkular/metrics/metrics` endpoint could fail with timeouts. With this bug
fix, the extra queries are only performed when explicitly requested. By default,
do not execute the extra queries that provide optional data. The endpoint is now
more stable and not as susceptible to timeouts.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1458186[*BZ#1458186*])

* When either a certificate within the chain at `serviceaccount/ca.crt` or any of
the certificates within the provided truststore file contained a white space
after the `BEGIN CERTIFICATE` declaration, the Java keytool rejected the
certificate with an error, causing Origin Metrics to fail to start. As a
workaround, Origin Metrics will now attempt to remove the spaces before feeding
the certificate to the Keytool, but administrators should ensure their
certificates do not contain such spaces.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1471251[*BZ#1471251*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1500464[*BZ#1500464*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1500471[*BZ#1500471*])

[discrete]
===== Networking

* A slow image pull made the network diagnostics fail. With this bug fix, the
 timeout for the image pull was increased. The diagnostics now run in slow
 environments.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1481550[*BZ#1481550*])

* The OpenShift node proxy previously did not support using a specified IP
address. This could prevent correct operation on hosts with multiple network
interface cards. The OpenShift node process already accepts a
`--bind-address=<ip address>:<port>` command-line flag and `bindAddress:`
configuration file option for the multiple network interface card case. The
proxy functionality has been fixed to respect these options. When
`--bind-address` or `bindAddress` are used, the OpenShift node proxy should work
correctly when the OpenShift node host has multiple network interface cards.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1489023[*BZ#1489023*],
link:https://bugzilla.redhat.com/show_bug.cgi?id=1489024[*BZ#1489024*])

* Iptables called too often and unnecessarily. Therefore, time-outs would wait for
iptables operations to finish. This bug fix changes the code so that it skips
reloads when the iptables rules are unchanged. There are now fewer calls to
iptables and, therefore, less time-outs.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1501517[*BZ#1501517*])

[discrete]
===== Pod

* There was a symbolic link error for the log file of every pod started when the
docker log driver was journald. Log symlink creation that fails when using
journald logging driver was skipped. This bug fix resolves the issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1434942[*BZ#1434942*])


* Currently, pod anti-affinity is respected across projects. Pod A from Project 1
will not land on node where Pod B from Project 2 is running, if pod
anti-affinity is enabled when scheduling Pod A. While scheduling Pod A, check
for pod anti-affinity only within the project of Pod A. Pod anti-affinity will
not be respected across projects.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1492194[*BZ#1492194*])

[discrete]
===== Storage

* The volumePath that included the datastore name was parsed incorrectly. The same
applies to volumePath that included datacluster and datastore names. It is not
possible to attach persistent volumes that have the above described volumePath
values. volumePath is now parsed correctly. Persistent volumes that have the
above described volumePath values are attached
correctly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1497042[*BZ#1497042*])

[discrete]
===== Security

* An attacker with knowledge of the given name used to authenticate and access
Elasticsearch can later access it without the token, bypassing authentication.
This attack also requires that the Elasticsearch be configured with an external
route, and the data accessed is limited to the indices.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1501986[*BZ#1501986*])

[[ocp-3-6-rhsa-2017-3389-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.5 or 3.6 cluster to this latest
release, use the automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-6-rhba-2017-3438]]
=== RHBA-2017:3438 - {product-title} 3.6.173.0.83 Bug Fix and Enhancement Update

Issued: 2017-12-14

{product-title} release 3.6.173.0.83 is now available. The list of packages
included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2017:3438[RHBA-2017:3438] advisory.
The container images included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2017:3439[RHBA-2017:3439] advisory.

Space precluded documenting all of the bug fixes, enhancements, and images for
this release in the advisories. See the following sections for notes on
upgrading and details on the bug fixes and images included in this release.

[[ocp-3-6-rhba-2017-3438-images]]
==== Images

This release updates the Red Hat Container Registry
(`registry.access.redhat.com`) with the following images:

----
openshift3/apb-base:v3.6.173.0.83-2
openshift3/container-engine:v3.6.173.0.83-2
openshift3/jenkins-2-rhel7:v3.6.173.0.83-2
openshift3/jenkins-slave-base-rhel7:v3.6.173.0.83-2
openshift3/jenkins-slave-maven-rhel7:v3.6.173.0.83-2
openshift3/jenkins-slave-nodejs-rhel7:v3.6.173.0.83-2
openshift3/logging-curator:v3.6.173.0.83-2
openshift3/logging-elasticsearch:v3.6.173.0.83-2
openshift3/logging-fluentd:v3.6.173.0.83-2
openshift3/logging-kibana:v3.6.173.0.83-2
openshift3/mediawiki-123:v3.6.173.0.83-2
openshift3/mediawiki-apb:v3.6.173.0.83-2
openshift3/metrics-cassandra:v3.6.173.0.83-2
openshift3/metrics-deployer:v3.6.173.0.83-2
openshift3/metrics-hawkular-metrics:v3.6.173.0.83-2
openshift3/metrics-hawkular-openshift-agent:v3.6.173.0.83-2
openshift3/metrics-heapster:v3.6.173.0.83-2
openshift3/node:v3.6.173.0.83-2
openshift3/openvswitch:v3.6.173.0.83-4
openshift3/ose-ansible:v3.6.173.0.83-2
openshift3/ose-cluster-capacity:v3.6.173.0.83-2
openshift3/ose-deployer:v3.6.173.0.83-4
openshift3/ose-docker-builder:v3.6.173.0.83-4
openshift3/ose-docker-registry:v3.6.173.0.83-2
openshift3/ose-egress-http-proxy:v3.6.173.0.83-2
openshift3/ose-f5-router:v3.6.173.0.83-4
openshift3/ose-federation:v3.6.173.0.83-2
openshift3/ose-haproxy-router:v3.6.173.0.83-4
openshift3/ose-keepalived-ipfailover:v3.6.173.0.83-2
openshift3/ose-pod:v3.6.173.0.83-2
openshift3/ose-recycler:v3.6.173.0.83-4
openshift3/ose-service-catalog:v3.6.173.0.83-2
openshift3/ose:v3.6.173.0.83-2
openshift3/postgresql-apb:v3.6.173.0.83-2
openshift3/registry-console:v3.6.173.0.83-2
----

[[ocp-3-6-rhba-2017-3438-bug-fixes]]
==== Bug Fixes

* The *imagetrigger-controller* was missing the permission to create custom `Build` objects. Builds with `customStrategy` and `ImageStreamTag` triggers were not started whenever the `ImageStreamTag` was updated. This bug fix adds the permission to the *imagetrigger-controller*, and as a result builds with `customStrategy` and `ImageStreamTag` triggers are now started when the `ImageStreamTag` is updated. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1519296[*BZ#1519296*])

* Container Native Storage (CNS) installations would fail with an error when `groups.glusterfs_registry` was undefined. This bug has been fixed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1462143[*BZ#1462143*])

* CNS installations would fail if the namespace for CNS pods was different from the namespace `default`. This bug fix ensures that proper namespaces are used for the `heketi` command and service account, and the failure no longer occurs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1464393[*BZ#1464393*])

* The GlusterFS example inventory files had unsupported parameters. This bug fix cleans up these examples. As a result, unsupported parameters are no longer suggested. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1474692[*BZ#1474692*])

* In verifying sufficient disk space available under the *_/var_* directory, the *disk_availability* check only counted storage mounted directly at *_/var_* (or *_/_* if *_/var_* is not a separate file system). Extra storage mounted below *_/var_*, for instance in *_/var/lib/docker_*, was not counted toward the required available storage, and thus the check could fail erroneously. This bug fix ensures that storage mounted below a file system is included in its total availability. As a result, the check should accurately account for availability of storage. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1491566[*BZ#1491566*])

* Fluentd input plug-ins treated messages as ASCII-8BIT by default. As a result, if one of these messages or any other message with UTF-8 characters was read in and later needed to be converted to UTF-8, the conversion failed with an "UndefinedConversionError". This bug fix adds a new *record_modifier* filter, which treats the messages as UTF-8. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1501993[*BZ#1501993*])

* The `_source` was not disabled for metrics records being stored in Elasticsearch. The records were taking up much more CPU, RAM, and disk resources than necessary. This bug fix completely disables the `_source` for `project.ovirt-metrics*` records. As a result, metrics records are much smaller and require fewer resources to handle. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1512132[*BZ#1512132*])

* OCP 3.6 rejected certain invalid master configuration values which OCP 3.5 silently accepted. When upgrading from 3.5 to 3.6, the master would fail to start if the `clusterNetworkCIDR` or `serviceNetworkCIDR` value in the master configuration was "invalid" (for example, if it had "172.30.1.1/16" instead of "172.30.0.0/16"). This bug fix ensures that OCP 3.6 accepts the same invalid values that OCP 3.5 accepted, but logs a warning about it. As a result, upgrades will now work, and the administrator is notified about the incorrect configuration values. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1508445[*BZ#1508445*])

[[ocp-3-6-rhba-2017-3438-enhancements]]
==== Enhancements

* Loading the Java console can sometimes take a while, and it is important to show to users that the Java console is still loading and not frozen. With this enhancement, a spinning image is shown under the top menu bar while the Java console is loading. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1426615[*BZ#1426615*])

[[ocp-3-6-rhba-2017-3438-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.5 or 3.6 cluster to this latest
release, use the automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-6-rhba-2018-0076]]
=== RHBA-2018:0076 - {product-title} 3.6.173.0.83-10 Images Update

Issued: 2018-01-10

{product-title} release 3.6.173.0.83-10 is now available. The list of container
images included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2018:0076[RHBA-2018:0076] advisory.

The container images in this release have been updated using the latest base
images.

[[ocp-3-6-rhba-2018-0076-images]]
==== Images

This release updates the Red Hat Container Registry
(`registry.access.redhat.com`) with the following images:

----
openshift3/node:v3.6.173.0.83-10
openshift3/logging-kibana:v3.6.173.0.83-9
openshift3/openvswitch:v3.6.173.0.83-11
----

[[ocp-3-6-rhba-2018-0076-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.5 or 3.6 cluster to this latest release, use the
automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing Automated In-place Cluster Upgrades] for instructions.

[[ocp-3-6-173-0-96]]
=== RHBA-2018:0113 - {product-title} 3.6.173.0.96 Bug Fix and Enhancement Update

Issued: 2018-01-22

{product-title} release 3.6.173.0.96 is now available. The packages and bug fixes
included in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2018:0113[RHBA-2018:0113] advisory.
The container images included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2018:0114[RHBA-2018:0114] advisory.

Space precluded documenting all of the bug fixes and images for this release in
the advisory. See the following sections for notes on upgrading and details on
the bug fixes, enhancements, and images included in this release.

[[ocp-3-6-173-0-96-rhba-2018-0113-bug-fixes]]
==== Bug Fixes

* A regression bug was reported whereby source-to-image builds would fail if the
source repository filesystem contained a broken symlink (pointing to a
non-existent item). This was resolved with this bug fix.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1519822[*BZ#1519822*])

* Categories like *all* were moved to the server, but some of them were only moved
after the upstream cut for the rebase, causing an incomplete list of resources.
Therefore, some resources could not be found in `oc get all` and some other `oc
get` calls. With this bug fix, the remaining upstream commits were picked to
include all needed resources and `oc get all` and the other problematic calls
were fixed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1515878[*BZ#1515878*])

* Node selectors were incorrectly set on template service broker daemonset object.
Consequently, the looping failed the deployment of template service broker pods
and there was excessive CPU usage on master and nodes. The node selectors are
now set correctly on the template service broker daemonset object and the
template service broker pods now deploy correctly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1524219[*BZ#1524219*])

* Fluentd fails to properly process messages when it is unable to determine the
namespace and pod UUIDs.The logging pipeline outputs a lot of messages and
sometimes blocks log flow to Elasticsearch. Check for the missing fields and
orphan the record if needed. With this bug fix, logs continue to flow and
orphaned records end up in an orphaned namespace.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1494612[*BZ#1494612*])

* Elasticsearch clusters that take a long time recovering data do not reach the
*YELLOW* state fast enough. The  {product-title} cluster restarts the pod
because the readiness probe fails, which starts the Elasticsearch node recovery
again. Check only for the Elasticsearch cluster to be listening on the desired
port. The {product-title} cluster does not terminate the Elasticsearch node
early, which allows it to complete its recovery. The cluster may be in the *RED*
state at this time, but is able to accept queries and writes.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1510697[*BZ#1510697*])

* When trying to alias an index that does not exist, the bulk alias operation
failed. Only alias *_.operations_* if there is at least one *_.operations_*
index. As a result, there will be an alias for all indices.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1519705[*BZ#1519705*])

* The *remote-syslog* plug-in in fluentd takes a configuration parameter `tag_key`
to use the field specified in `tag_key` from the record to set the syslog key.
When a field specified in `tag_key` does not exist, it caused a Ruby exception,
which was not caught. With this bug fix,  the field no longer exists and
`tag_key` is ignored and the default tag is used.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1519213[*BZ#1519213*])

* There was a logic error in the fluentd startup script. When an ops cluster was
first disabled then enabled, the proper ops configuration file was not enabled.
As a result, Sub-configuration files starting with `output-ops-extra-` did not
have a chance to be called from the ops configuration file. The logic error  s
now fixed When an ops cluster is first disabled then enabled, the proper ops
configuration file is enabled and its sub-configuration files are also enabled.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1519679[*BZ#1519679*])

* The annotation to identify the proper Kibana instance to use for ops namespaces
was being set regardless of if an ops logging cluster is used or not. Users were
directed to non-existent service from the web console when trying to view
operations logs. For existing deployments, manually run `oc annotate ns
$NAMESPACE  openshift.io/logging.ui.hostname-` for each affected namespace. New
deployments will only have this annotation set by the installer if
the`openshift_logging_use_ops` variable is set to `true`. With this bug fix,
users will be directed to the correct version of Kibana when viewing ops logs
from the web console.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1519808[*BZ#1519808*])

* Previously, when importing a template as YAML in the web console, then clicking
*Back* in the wizard, the *Next* button would stop working. The problem has been
fixed so that the *Next* button works correctly after clicking *Back* in the
*Import YAML* dialog.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1526215[*BZ#1526215*])

* The Kubernetes resource quota controller had a fatal a race condition.
Therefore, the master controller process occasionally crashes, writes a stack
dump, and restarts. With this bug fix,  the race condition is resolved and the
crash no longer occurs.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1519277[*BZ#1519277*])

* When a Network Egress DNS policy was used, a bug may have prevented further
 correct operation of the proxy, resulting in new pods not handling service
 requests. That bug is fixed and Egress DNS policies can now be used without
 triggering this bug.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1502602[*BZ#1502602*])

* A bug in the node container garbage collection and network setup prevented pod
sandboxes from being properly garbage collected. Nodes could exhaust the
available pool of pod IP addresses, especially if they are restarted and/or
containers were removed while the node was not running. Nodes now properly
garbage collect and tear down pod sandboxes, ensuring that IP addresses are
released to the pool for subsequent re-use. Newly installed nodes should no
longer experience IP address exhaustion due to pod sandbox teardown errors.
Upgraded nodes should remove all files in
*_/var/lib/cni/networks/openshift-sdn/_* during the upgrade, or after upgrade
when no pods are running on the node.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1516782[*BZ#1516782*])

* This bug fix corrects an interaction between runc and systemd where the sandbox
creation fails when the pod specifies CPU limit in 1000s of millicores and the
last digit is not 0.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1509467[*BZ#1509467*])

* This bug fix corrects an issue where `oc logs` exits with error *unexpected
stream type*.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1521026[*BZ#1521026*])

* When running the etcd v2 to v3 migration playbooks as included in the
{product-title} 3.7 release, the playbooks incorrectly assumed that all services
were HA services (`atomic-openshift-master-api` and
`atomic-openshift-master-controllers` rather than `atomic-openshift-master`),
which is the norm on version 3.7. However, the migration playbooks would be
executed prior to upgrading to version 3.7, so this was incorrect. The migration
playbooks have been updated to start and stop the correct services ensuring
proper migration.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1523814[*BZ#1523814*])

[[ocp-3-6-173-0-96-rhba-2018-0113-enhancements]]
==== Enhancements

* Elasticsearch `deploymentconfigs` are now modified to disable {product-title}
rollback behavior. Upgrades of the logging stack that do not result in the
readiness probe succeeding are rolled back to the last previous successful
deployment.  This can result in the deployed instances being out-of-date with
their configuration and old images. This change will make it so the deployments
will not be rolled back and operators can manually intervene without the image
and configuration mismatches.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1519622[*BZ#1519622*])

* An *_.operations_* index-mapping in a non-ops Elasticsearch cluster is no longer
displayed because operations indices will never exist in a non-ops Elasticsearch
cluster.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1519706[*BZ#1519706*])

[[ocp-3-6-173-0-96-rhba-2018-0113-images]]
==== Images

This release updates the Red Hat Container Registry
(*_registry.access.redhat.com_*) with the following images:

----
openshift3/apb-base:v3.6.173.0.96-2
openshift3/container-engine:v3.6.173.0.96-2
openshift3/jenkins-1-rhel7:v3.6.173.0.96-2
openshift3/jenkins-2-rhel7:v3.6.173.0.96-2
openshift3/jenkins-slave-base-rhel7:v3.6.173.0.96-2
openshift3/jenkins-slave-maven-rhel7:v3.6.173.0.96-2
openshift3/jenkins-slave-nodejs-rhel7:v3.6.173.0.96-2
openshift3/logging-auth-proxy:v3.6.173.0.96-2
openshift3/logging-curator:v3.6.173.0.96-2
openshift3/logging-elasticsearch:v3.6.173.0.96-3
openshift3/logging-fluentd:v3.6.173.0.96-2
openshift3/logging-kibana:v3.6.173.0.96-2
openshift3/mediawiki-123:v3.6.173.0.83-2
openshift3/mediawiki-apb:v3.6.173.0.96-2
openshift3/metrics-cassandra:v3.6.173.0.96-2
openshift3/metrics-deployer:v3.6.173.0.83-11.1513170044
openshift3/metrics-hawkular-metrics:v3.6.173.0.96-2
openshift3/metrics-hawkular-openshift-agent:v3.6.173.0.96-2
openshift3/metrics-heapster:v3.6.173.0.96-2
openshift3/node:v3.6.173.0.96-2
openshift3/openvswitch:v3.6.173.0.96-2
openshift3/ose-ansible-service-broker:v3.6.173.0.78-1
oopenshift3/ose-ansible:v3.6.173.0.96-3
openshift3/ose-base:v3.6.173.0.96-2
openshift3/ose-cluster-capacity:v3.6.173.0.96-2
openshift3/ose-deployer:v3.6.173.0.96-2
openshift3/ose-docker-builder:v3.6.173.0.96-2
openshift3/ose-docker-registry:v3.6.173.0.96-2
openshift3/ose-egress-http-proxy:v3.6.173.0.96-2
openshift3/ose-egress-router:v3.6.173.0.96-2
openshift3/ose-f5-router:v3.6.173.0.96-2
openshift3/ose-federation:v3.6.173.0.96-2
openshift3/ose-haproxy-router:v3.6.173.0.96-2
openshift3/ose-keepalived-ipfailover:v3.6.173.0.96-2
openshift3/ose-pod:v3.6.173.0.96-2
openshift3/ose-recycler:v3.6.173.0.96-2
openshift3/ose-service-catalog:v3.6.173.0.96-2
openshift3/ose-sti-builder:v3.6.173.0.96-2
openshift3/ose:v3.6.173.0.96-2
openshift3/postgresql-apb:v3.6.173.0.96-2
openshift3/registry-console:v3.6.173.0.96-2
----

[[ocp-3-6-173-0-96-upgrading]]
==== Upgrading

To upgrade an existing {product-title} 3.5 or 3.6 cluster to this latest
release, use the automated upgrade playbook. See
xref:../install_config/upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[Performing
Automated In-place Cluster Upgrades] for instructions.
