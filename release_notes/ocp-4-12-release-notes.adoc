:_content-type: ASSEMBLY
[id="ocp-4-12-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-12-about-this-release"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-2022:TODO[RHSA-2022:TODO]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md[Kubernetes 1.25] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. With the {cluster-manager-first} application for {product-title}, you can deploy OpenShift clusters to either on-premises or cloud environments.

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.4 and 8.5, as well as on {op-system-first} 4.12.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//TODO: Add the line below for EUS releases.
//{product-title} 4.6 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

//TODO: The line below is not true for 4.9 but should be used when it is next appropriate. Revisit in October 2022 timeframe.
//Version 4.6 going end of life in October 2022. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].


[id="ocp-4-12-add-on-support-status"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-12-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-12-rhcos"]
=== {op-system-first}

[id="ocp-4-12-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-12-aws-load-balancer-customization"]
==== Specify the load balancer type in AWS during installation
Beginning with {product-title} {product-version}, you can specify either Network Load Balancer (NLB) or Classic as a persistent load balancer type in AWS during installation. Afterwards, if an Ingress Controller is deleted, the load balancer type persists with the lbType configured during installation.

For more information, see xref:../installing/installing_aws/installing-aws-network-customizations.adoc[Installing a cluster on AWS with network customizations].

[id="ocp-4-12-installation-and-upgrade-gcp-marketplace"]
==== Google Cloud Platform Marketplace offering
{product-title} is now available on the GCP Marketplace. Installing an {product-title} with a GCP Marketplace image lets you create self-managed cluster deployments that are billed on pay-per-use basis (hourly, per core) through GCP, while still being supported directly by Red Hat.

For more information about installing using installer-provisioned infrastructure, see xref:../installing/installing_gcp/installing-gcp-customizations.adoc#installation-gcp-marketplace_installing-gcp-customizations[Using a GCP Marketplace image]. For more information about installing a using user-provisioned infrastructure, see xref:../installing/installing_gcp/installing-gcp-user-infra.adoc#installation-creating-gcp-worker_installing-gcp-user-infra[Creating additional worker machines in GCP].

[id="ocp-4-12-gcp-azure-serial-console-logs"]
==== Troubleshooting bootstrap failures during installation on GCP and Azure
The installer now gathers serial console logs from the bootstrap and control plane hosts on GCP and Azure. This log data is added to the standard bootstrap log bundle.

For more information, see xref:../installing/installing-troubleshooting.adoc#installation-bootstrap-gather_installing-troubleshooting[Troubleshooting installation issues].

[id="ocp-4-12-ibm-cloud-vpc"]
==== IBM Cloud VPC general availability
IBM Cloud VPC is now generally available in {product-title} {product-version}.

For more information about installing a cluster, see xref:../installing/installing_ibm_cloud_public/preparing-to-install-on-ibm-cloud.adoc#preparing-to-install-on-ibm-cloud[Preparing to install on IBM Cloud VPC].

[id="ocp-4-12-admin-ack-upgrading"]
==== Required administrator acknowledgment when upgrading from {product-title} 4.11 to 4.12

{product-title} 4.12 uses Kubernetes 1.25, which removed xref:../release_notes/ocp-4-12-release-notes.adoc#ocp-4-12-removed-kube-1-25-apis[several deprecated APIs].

A cluster administrator must provide a manual acknowledgment before the cluster can be upgraded from {product-title} 4.11 to 4.12. This is to help prevent issues after upgrading to {product-title} 4.12, where APIs that have been removed are still in use by workloads, tools, or other components running on or interacting with the cluster. Administrators must evaluate their cluster for any APIs in use that will be removed and migrate the affected components to use the appropriate new API version. After this is done, the administrator can provide the administrator acknowledgment.

All {product-title} 4.11 clusters require this administrator acknowledgment before they can be upgraded to {product-title} 4.12.

For more information, see xref:../updating/updating-cluster-prepare.adoc#updating-cluster-prepare[Preparing to update to {product-title} 4.12].

[id="ocp-4-12-feature-set"]
==== Enabling a feature set when installing a cluster
Beginning with {product-title} {product-version}, you can enable a feature set as part of the installation process. A feature set is a collection of {product-title} features that are not enabled by default.

For more information about enabling a feature set during installation, see xref:../nodes/clusters/nodes-cluster-enabling-features.adoc#nodes-cluster-enabling[Enabling {product-title} features using feature gates].

[id="ocp-4-12-post-installation"]
=== Post-installation configuration

[id="ocp-4-12-web-console"]
=== Web console

[id="ocp-4-12-Administrator-perspective"]
==== Administrator Perspective

With this release, there are several updates to the *Administrator* perspective of the web console.

[id="ocp-4-12-developer-perspective"]
==== Developer Perspective

With this release, there are several updates to the *Developer* perspective of the web console. You can perform the following actions:

* Export your application in the ZIP file format to another project or cluster by using the *Export application* option on the *+Add* page.
* Create a Kafka event sink to receive events from a particular source and send them to a Kafka topic.
* Set the default resource preference in the *User Preferences* -> *Applications* page. In addition, you can select another resource type to be the default.
** Optionally, set another resource type from the *Add* page by clicking *Import from Git* -> *Advanced options* -> *Resource type* and selecting the resource from the drop-down list.
* Make the `status.HostIP` node IP address for pods visible in the *Details* tab of the *Pods* page.
* See the resource quota alert label on the *Topology* and *Add* pages whenever any resource reaches the quota. The alert label link takes you to the *ResourceQuotas* list page. If the alert label link is for a single resource quota, it takes you to the *ResourceQuota details* page.
** For deployments, an alert is displayed in the topology node side panel if any errors are associated with resource quotas. Also, a yellow border is displayed around the deployment nodes when the resource quota is exceeded.
* Customize the following UI items using the form or YAML view:
** Perspectives visible to users
** Quick starts visible to users
** Cluster roles accessible to a project
** Actions visible on the *+Add* page
** Item types in the *Developer Catalog*
* See the common updates to the *Pipeline details* and *PipelineRun details* page visualization by performing the following actions:
** Use the mouse wheel to change the zoom factor.
** Hover over the tasks to see the task details.
** Use the standard icons to zoom in, zoom out, fit to screen, and reset the view.
** *PipelineRun details* page only: At specific zoom factors, the background color of the tasks changes to indicate the error or warning status. You can hover over the tasks badge to see the total number of tasks and the completed tasks.

[id="ocp-4-12-helm-page-improvements"]
===== Helm page improvements

In {product-title} 4.12, you can do the following from the *Helm* page:

* Create Helm releases and repositories using the *Create* button.
* Create, update, or delete a cluster-scoped or a namespace-scoped Helm chart repository.
* View the list of the existing Helm chart repositories with their scope in the *Repositories* page.
* View the newly created Helm release in the *Helm Releases* page.

[id="ocp-4-12-oc"]
=== OpenShift CLI (oc)

[id="ocp-4-12-ibm-z"]
=== IBM Z and LinuxONE

[id="ocp-4-12-ibm-power"]
=== IBM Power

[id="ocp-4-12-images"]
=== Images

A new import value, `importMode`, has been added to the `importPolicy` parameter of image streams. The following fields are available for this value:

* `Legacy`: `Legacy` is the default value for `importMode`. When active, the manifest list is discarded, and a single sub-manifest is imported. The platform is chosen in the following order of priority:
+
. Tag annotations
. Control plane architecture
. Linux/AMD64
. The first manifest in the list

* `PreserveOriginal`: When active, the original manifest is preserved. For manifest lists, the manifest list and all of its sub-manifests are imported.


[id="ocp-4-12-security"]
=== Security and compliance

[id="ocp-4-12-networking"]
=== Networking

[id="ocp-4-12-k8s-nmstate-support-for-vsphere"]
==== Kubernetes NMState in VMWare vSphere now supported
Beginning with {product-title} {product-version}, you can configure the networking settings such as DNS servers or search domains, VLANs, bridges, and interface bonding using the Kubernetes NMState Operator on your VMware vSphere instance.

For more information, see xref:../networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.adoc[About the Kubernetes NMState Operator].

[id="ocp-4-12-k8s-nmstate-support-for-openstack"]
==== Kubernetes NMState in OpenStack now supported
Beginning with {product-title} {product-version}, you can configure the networking settings such as DNS servers or search domains, VLANs, bridges, and interface bonding using the Kubernetes NMState Operator on your OpenStack instance.

For more information, see xref:../networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.adoc[About the Kubernetes NMState Operator].

[id="ocp-4-12-nw-external-dns-operator"]
==== External DNS Operator

In {product-title} 4.12, the External DNS Operator modifies the format of the ExternalDNS wildcard TXT records on AzureDNS. The External DNS Operator replaces the asterisk with `any` in ExternalDNS wildcard TXT records. You must avoid the ExternalDNS wildcard A and CNAME records having `any` leftmost subdomain because this might cause a conflict.

The upstream version of `ExternalDNS` for an {product-title} 4.12 is v0.13.1. 

[id="ocp-4-12-nw-metrics-telemetry"]
==== Capturing metrics and telemetry associated with the use of routes and shards

In {product-title} 4.12, the Cluster Ingress Operator exports a new metric named `route_metrics_controller_routes_per_shard`. The `shard_name` label of the metric specifies the name of the shards. This metric gives the total number of routes that are admitted by each shard.

The following metrics are sent through telemetry.

.Metrics sent through telemetry
[cols="1,1,1",options="header"]
|===
| Name | Recording rule expression | Description

| `cluster:route_metrics_controller_routes_per_shard:min`
| `min(route_metrics_controller_routes_per_shard)`
| Tracks the minimum number of routes admitted by any of the shards

| `cluster:route_metrics_controller_routes_per_shard:max`
| `max(route_metrics_controller_routes_per_shard)`
| Tracks the maximum number of routes admitted by any of the shards

| `cluster:route_metrics_controller_routes_per_shard:avg`
| `avg(route_metrics_controller_routes_per_shard)`
| Tracks the average value of the `route_metrics_controller_routes_per_shard` metric

| `cluster:route_metrics_controller_routes_per_shard:median`
| `quantile(0.5, route_metrics_controller_routes_per_shard)`
| Tracks the median value of the `route_metrics_controller_routes_per_shard` metric

| `cluster:openshift_route_info:tls_termination:sum`
| `sum (openshift_route_info) by (tls_termination)`
| Tracks the number of routes for each `tls_termination` value. The possible values for `tls_termination` are `edge`, `passthrough` and `reencrypt`

|===

[id="ocp-4-12-nw-ingress-autoscaling"]
==== Ingress Controller Autoscaling

You can now use the {product-title} Custom Metrics Autoscaler Operator to dynamically scale the default Ingress Controller based on metrics in your deployed cluster, such as the number of worker nodes available.

For more information, see xref:../networking/ingress-operator.adoc#nw-autoscaling-ingress-controller[Autoscaling an Ingress Controller].

[id="ocp-4-12-nw-ingress-haproxy-maxconn-default"]
==== HAProxy maxConnections default is now 50,000

In {product-title} 4.12, the default value for the `maxConnections` setting is now 50000. Previously starting with {product-title} 4.11, the default value for the `maxConnections` setting was 20000.

For more information, see xref:../networking/ingress-operator.adoc#nw-ingress-controller-configuration-parameters_configuring-ingress[Ingress Controller configuration parameters].

[id="ocp-4-12-nw-configure-dns-management"]
==== Configuration of an Ingress Controller for manual DNS management

You can now configure an Ingress Controller to stop automatic DNS management and start manual DNS management. Set the `dnsManagementPolicy` parameter to specify automatic or manual DNS management.

For more information, see xref:../networking/ingress-controller-dnsmgt.adoc#ingress-controller-dnsmgt[Configuring an Ingress Controller to manually manage DNS].

[id="ocp-4-12-networking-supported-hardware-for-sr-iov"]
==== Supported hardware for SR-IOV (Single Root I/O Virtualization)

{product-title} 4.12 adds support for the following SR-IOV devices:

* MT2892 Family [ConnectX&#8209;6{nbsp}Dx]
* MT42822 BlueField&#8209;2 in ConnectX&#8209;6 NIC mode
* Silicom STS Family

For more information, see xref:../networking/hardware_networks/about-sriov.adoc#supported-devices_about-sriov[Supported devices].

[id="ocp-4-12-switch-aws-load-balancer"]
==== Switch between AWS load balancer types without deleting the Ingress Controller

You can update the Ingress Controller to switch between an AWS Classic Load Balancer (CLB) and an AWS Network Load Balancer (NLB) without deleting the Ingress Controller.

For more information, see xref:../networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-aws.adoc[Configuring ingress cluster traffic on AWS].

[id="ocp-4-12-ipv6-gratuitous-ARPs-default-SR-IOV"]
==== IPv6 unsolicited neighbor advertisements and IPv4 gratuitous address resolution protocol now default on the SR-IOV CNI plug-in

Pods created with the Single Root I/O Virtualization (SR-IOV) CNI plug-in, where the IP address management CNI plug-in has assigned IPs, now send IPv6 unsolicited neighbor advertisements and/or IPv4 gratuitous address resolution protocol by default onto the network. This enhancement notifies hosts of the new pod's MAC address for a particular IP to refresh ARP/NDP caches with the correct information.

For more information, see xref:../networking/hardware_networks/about-sriov.adoc#supported-devices_about-sriov[Supported devices].

[id="ocp-4-12-coredns-cache-tuning"]
==== Support for CoreDNS cache tuning

You can now configure the time-to-live (TTL) duration of both successful and unsuccessful DNS queries cached by CoreDNS.

For more information, see xref:../networking/dns-operator.adoc#nw-dns-cache-tuning_dns-operator[Tuning the CoreDNS cache].

[id=ocp-4-12-egress-ip-support]
==== Egress IP support on {rh-openstack-first}

{rh-openstack}, paired with {product-title}, now supports automatic attachment and detachment of Egress IP addresses. The traffic from one or more pods in any number of namespaces has a consistent source IP address for services outside of the cluster. This support applies to OpenShift SDN and OVN-Kubernetes as default network providers.

[id="ocp-4-12-openshift-sdn-ovn-kubernetes-feature-migration-support"]
==== OpenShift SDN to OVN-Kubernetes feature migration support

If you plan to migrate from the OpenShift SDN network plug-in to the OVN-Kubernetes network plug-in, your configurations for the following capabilities are automatically converted to work with OVN-Kubernetes:

* Egress IP addresses
* Egress firewalls
* Multicast

For more information about how the migration to OVN-Kubernetes works, see xref:../networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc#migrate-from-openshift-sdn[Migrating from the OpenShift SDN cluster network provider].

[id="ocp-4-12-storage"]
=== Storage

[id="ocp-4-12-registry"]
=== Registry

[id="ocp-4-12-olm"]
=== Operator lifecycle

[id="ocp-4-12-osdk"]
=== Operator development

[id="ocp-4-12-jenkins"]
=== Jenkins

[id="ocp-4-12-machine-api"]
=== Machine API

[id="ocp-4-12-mapi-autoscaler-verbosity"]
==== Specifying cluster autoscaler log level verbosity

{product-title} now supports setting the log level verbosity of the cluster autoscaler by setting the `logVerbosity` parameter in the `ClusterAutoscaler` custom resource. For more information, see the xref:../machine_management/applying-autoscaling.adoc#cluster-autoscaler-cr_applying-autoscaling[`ClusterAutoscaler` resource definition].

[id="ocp-4-12-machine-config-operator"]
=== Machine Config Operator

[id="ocp-4-12-nodes"]
=== Nodes

[id="ocp-4-12-node-cron-job-time-zone"]
==== Cron job time zones (Technology Preview)

Setting a time zone for a cron job schedule is now offered as a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]. If a time zone is not specified, the Kubernetes controller manager interprets the schedule relative to its local time zone.

For more information, see xref:../nodes/jobs/nodes-nodes-jobs.adoc#nodes-nodes-jobs-creating-cron_nodes-nodes-jobs[Creating cron jobs].

[id="ocp-4-12-node-cgroups-v2"]
==== Linux Control Group version 2 promoted to Technology Preview

{product-title} support for link:https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html[Linux Control Group version 2] (cgroup v2) has been promoted to Technology Preview. cgroup v2 is the next version of the kernel link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch01[control groups]. cgroups v2 offers multiple improvements, including a unified hierarchy, safer sub-tree delegation, new features such as link:https://www.kernel.org/doc/html/latest/accounting/psi.html[Pressure Stall Information], and enhanced resource management and isolation. For more information, see xref:../nodes/clusters/nodes-cluster-cgroups-2.adoc#nodes-cluster-cgroups-2[Enabling Linux Control Group version 2 (cgroup v2)].

[id="ocp-4-12-node-cgroups-crun"]
==== crun container runtime (Technology Preview)

{product-title} now supports the crun container runtime in Technology Preview. You can switch between the crun container runtime and the default container runtime as needed by using a `ContainerRuntimeConfig` custom resource (CR). For more information, see xref:../nodes/containers/nodes-containers-using.adoc#nodes-containers-runtimes[About the container engine and container runtime].

[id="ocp-4-11-logging"]
=== Logging

[id="ocp-4-12-monitoring"]
=== Monitoring

[id="ocp-4-12-scalability-and-performance"]
=== Scalability and performance

[id="ocp-4-12-tuned"]
==== Tuned profile

The `tuned` profile now defines the `fs.aio-max-nr` `sysctl` value by default, improving asynchronous I/O performance for default node profiles.

[id="tuning-of-power-states"]
==== Power-saving configurations

In {product-title} {product-version}, by enabling C-states and OS-controlled P-states, you can use different power-saving configurations for critical and non-critical workloads. You can apply the configurations through the new `perPodPowerManagement` workload hint, and the `cpu-c-states.crio.io` and `cpu-freq-governor.crio.io` CRI-O annotations. For more information about the feature, see xref:../scalability_and_performance/cnf-low-latency-tuning.adoc#node-tuning-operator-pod-power-saving-config_cnf-master[Power-saving configurations].

[id="ocp-4-12-insights-operator"]
=== Insights Operator

[id="ocp-4-12-auth"]
=== Authentication and authorization

[id="ocp-4-12-notable-technical-changes"]
== Notable technical changes

{product-title} {product-version} introduces the following notable technical changes.

// Note: use [discrete] for these sub-headings.

[discrete]
[id="ocp-4-12-auth-aws-sts-endpoints"]
=== AWS Security Token Service regional endpoints

The Cloud Credential Operator utility (`ccoctl`) now creates secrets that use regional endpoints for the xref:../authentication/managing_cloud_provider_credentials/cco-mode-sts.adoc[AWS Security Token Service (AWS STS)]. This approach aligns with AWS recommended best practices.

[discrete]
[id="ocp-4-12-auth-ccoctl-gcp-del-dir"]
=== Credentials requests directory parameter for deleting GCP resources with the Cloud Credential Operator utility

With this release, when you xref:../installing/installing_gcp/uninstalling-cluster-gcp.adoc#cco-ccoctl-deleting-sts-resources_uninstalling-cluster-gcp[delete GCP resources with the Cloud Credential Operator utility], you must specify the directory containing the files for the component `CredentialsRequest` objects.

[id="ocp-4-12-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *DEP*: _Deprecated_
* *REM*: _Removed_

//TODO: remove anything that has been REM since 4.9?

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.9 |OCP 4.10 | OCP 4.11

|SQLite database format for Operator catalogs
|DEP
|DEP
|DEP

|`ImageChangesInProgress` condition for Cluster Samples Operator
|DEP
|DEP
|DEP

|`MigrationInProgress` condition for Cluster Samples Operator
|DEP
|DEP
|DEP

|Cluster Loader
|DEP
|REM
|REM

|Bring your own {op-system-base} 7 compute machines
|DEP
|REM
|REM

|Jenkins Operator
|DEP
|REM
|REM

|Grafana component in monitoring stack
|-
|DEP
|REM

|Access to Prometheus and Grafana UIs in monitoring stack
|
|DEP
|REM

|vSphere 6.7 Update 2 or earlier
|DEP
|DEP
|REM

|vSphere 7.0 Update 1 or earlier
|-
|-
|DEP

|Virtual hardware version 13
|DEP
|DEP
|REM

|VMware ESXi 6.7 Update 2 or earlier
|DEP
|DEP
|REM

|VMware ESXi 7.0 Update 1 or earlier
|-
|-
|DEP

|Snapshot.storage.k8s.io/v1beta1 API endpoint
|DEP
|DEP
|REM

|Minting credentials for Microsoft Azure clusters
|GA
|REM
|REM

|Persistent storage using FlexVolume
|-
|DEP
|DEP

|Automatic generation of service account token secrets
|GA
|GA
|REM

|Removal of Jenkins images from install payload
|GA
|GA
|REM

|====

[id="ocp-4-12-deprecated-features"]
=== Deprecated features

[id="ocp-4-12-rhv-deprecations"]
==== Red Hat Virtualization (RHV) as a host platform for {product-title} will be deprecated

Red Hat Virtualization (RHV) will be deprecated in an upcoming release of {product-title}. Support for {product-title} on RHV will be removed from a future {product-title} release, currently planned as {product-title} 4.14.

[id="ocp-4-12-removed-features"]
=== Removed features

[id="ocp-4-12-removed-kube-1-25-apis"]
==== Beta APIs removed from Kubernetes 1.25

Kubernetes 1.25 removed the following deprecated APIs, so you must migrate manifests and API clients to use the appropriate API version. For more information about migrating removed APIs, see the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25[Kubernetes documentation].

.APIs removed from Kubernetes 1.25
[cols="2,2,2,1",options="header",]
|===
|Resource |Removed API |Migrate to |Notable changes

|`CronJob`
|`batch/v1beta1`
|`batch/v1`
|No

|`EndpointSlice`
|`discovery.k8s.io/v1beta1`
|`discovery.k8s.io/v1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#endpointslice-v125[Yes]

|`Event`
|`events.k8s.io/v1beta1`
|`events.k8s.io/v1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#event-v125[Yes]

|`HorizontalPodAutoscaler`
|`autoscaling/v2beta1`
|`autoscaling/v2`
|No

|`PodDisruptionBudget`
|`policy/v1beta1`
|`policy/v1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#poddisruptionbudget-v125[Yes]

|`PodSecurityPolicy`
|`policy/v1beta1`
|link:https://kubernetes.io/docs/concepts/security/pod-security-admission/[Pod Security Admission] ^[1]^
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#psp-v125[Yes]

|`RuntimeClass`
|`node.k8s.io/v1beta1`
|`node.k8s.io/v1`
|No

|===
[.small]
1. For more information about pod security admission in {product-title}, see xref:../authentication/understanding-and-managing-pod-security-admission.adoc#understanding-and-managing-pod-security-admission[Understanding and managing pod security admission].

[id="ocp-4-12-future-removals"]
=== Future Kubernetes API removals

The next minor release of {product-title} is expected to use Kubernetes 1.26. Currently, Kubernetes 1.26 is scheduled to remove several deprecated APIs.

See the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-26[Deprecated API Migration Guide] in the upstream Kubernetes documentation for the list of planned Kubernetes API removals.

See link:https://access.redhat.com/articles/6955985[Navigating Kubernetes API deprecations and removals] for information about how to check your cluster for Kubernetes APIs that are planned for removal.

[id="ocp-4-12-bug-fixes"]
== Bug fixes
//Bug fix work for TELCODOCS-750
[discrete]
[id="ocp-4-12-bare-metal-hardware-bug-fixes"]
==== Bare Metal Hardware Provisioning

[discrete]
[id="ocp-4-12-builds-bug-fixes"]
==== Builds

[discrete]
[id="ocp-4-12-cloud-compute-bug-fixes"]
==== Cloud Compute

[discrete]
[id="ocp-4-12-cluster-version-operator-bug-fixes"]
==== Cluster Version Operator

[discrete]
[id="ocp-4-12-console-Metal3-bug-fixes"]
==== Console Metal3 Plug-in

[discrete]
[id="ocp-4-12-dns-bug-fixes"]
==== Domain Name System (DNS)

[discrete]
[id="ocp-4-12-image-registry-bug-fixes"]
==== Image Registry

[discrete]
[id="ocp-4-12-installer-bug-fixes"]
==== Installer

* Previously, the number of supported user-defined tags was 8, and reserved {product-title} tags were 2 for AWS resources. With this release, the number of supported user-defined tags is now 25 and reserved {product-title} tags are 25 for AWS resources. You can now add up to 25 user tags during installation. (link:https://issues.redhat.com/browse/CFE-592[*CFE#592*])

[discrete]
[id="ocp-4-12-kube-api-server-bug-fixes"]
==== Kubernetes API server

[discrete]
[id="ocp-4-12-kube-scheduler-bug-fixes"]
==== Kubernetes Scheduler

[discrete]
[id="ocp-4-12-machine-config-operator-bug-fixes"]
==== Machine Config Operator

[discrete]
[id="ocp-4-12-compliance-operator-bug-fixes"]
==== Compliance Operator

[discrete]
[id="ocp-4-12-management-console-bug-fixes"]
==== Management Console

[discrete]
[id="ocp-4-12-monitoring-bug-fixes"]
==== Monitoring

[discrete]
[id="ocp-4-12-networking-bug-fixes"]
==== Networking

[discrete]
[id="ocp-4-12-ne-perf-improvements"]
==== Networking performance improvements

[discrete]
[id="ocp-4-12-node-bug-fixes"]
==== Node

[discrete]
[id="ocp-4-12-openshift-cli-bug-fixes"]
==== OpenShift CLI (oc)

[discrete]
[id="ocp-4-12-Kubernetes-controller-manage-bug-fixes"]
==== Kubernetes Controller Manager

[discrete]
[id="ocp-4-12-olm-bug-fixes"]
==== Operator Lifecycle Manager (OLM)

[discrete]
[id="ocp-4-12-openshift-operator-sdk-bug-fixes"]
==== Operator SDK

[discrete]
[id="ocp-4-12-openshift-api-server-bug-fixes"]
==== OpenShift API server

[discrete]
[id="ocp-4-12-rhcos-bug-fixes"]
==== {op-system-first}

[discrete]
[id="ocp-4-12-performance-addon-operator-bug-fixes"]
==== Performance Addon Operator

[discrete]
[id="ocp-4-12-routing-bug-fixes"]
==== Routing

[discrete]
[id="ocp-4-12-scalability-and-performance-bug-fixes"]
==== Scalability and performance

[discrete]
[id="ocp-4-12-storage-bug-fixes"]
==== Storage

[discrete]
[id="ocp-4-12-web-console-developer-perspective-bug-fixes"]
==== Web console (Developer perspective)

[id="ocp-4-12-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_
* *DEP*: _Deprecated_

//TODO: remove anything that has been GA since 4.9?

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.10 |OCP 4.11 | OCP 4.12

|PTP single NIC hardware configured as boundary clock
|TP
|GA
|GA

|PTP dual NIC hardware configured as boundary clock
|-
|TP
|TP

|PTP events with boundary clock
|TP
|GA
|GA

|Shared Resources CSI Driver and Build CSI Volumes in OpenShift Builds
|TP
|TP
|TP

|CSI volume expansion
|TP
|GA
|GA

|CSI Azure File Driver Operator
|TP
|GA
|GA

|CSI automatic migration
(AWS EBS, Azure file, GCP disk, VMware vSphere)
|TP
|TP
|TP

|CSI automatic migration
(Azure Disk, OpenStack Cinder)
|TP
|GA
|GA

|CSI inline ephemeral volumes
|TP
|TP
|TP

|CSI generic ephemeral volumes
|-
|GA
|GA

|Shared Resource CSI Driver
|TP
|TP
|TP

|Automatic device discovery and provisioning with Local Storage Operator
|TP
|TP
|TP

|Adding kernel modules to nodes with kvc
|TP
|TP
|TP

|Non-preempting priority classes
|TP
|TP
|TP

|Assisted Installer
|TP
|TP
|TP

|`kdump` on `x86_64` architecture
|TP
|GA
|GA

|`kdump` on `arm64` architecture
|-
|TP
|TP

|`kdump` on `s390x` architecture
|TP
|TP
|TP

|`kdump` on `ppc64le` architecture
|TP
|TP
|TP

|IBM Cloud VPC clusters
|TP
|TP
|GA

|Serverless functions
|TP
|TP
|TP

|Cloud controller manager for Alibaba Cloud
|TP
|TP
|TP

|Cloud controller manager for Amazon Web Services
|TP
|TP
|TP

|Cloud controller manager for Google Cloud Platform
|TP
|TP
|TP

|Cloud controller manager for Microsoft Azure
|TP
|TP
|TP

|Cloud controller manager for {rh-openstack-first}
|TP
|TP
|TP

|Cloud controller manager for VMware vSphere
|TP
|TP
|TP

|Driver Toolkit
|TP
|TP
|TP

|Special Resource Operator (SRO)
|TP
|TP
|TP

|Node Health Check Operator
|TP
|GA
|GA

|Pod-level bonding for secondary networks
|TP
|GA
|GA

|Multicluster console
|TP
|TP
|TP

|Selectable Cluster Inventory
|TP
|TP
|TP

|Heterogeneous clusters
|-
|TP
|TP

|Hyperthreading-aware CPU manager policy
|TP
|TP
|TP

|Heterogeneous Clusters
|-
|TP
|TP

|Dynamic Plug-ins
|TP
|TP
|TP

|Hybrid Helm Operator
|TP
|TP
|TP

|Alert routing for user-defined projects monitoring
|TP
|GA
|GA

|Disconnected mirroring with the oc-mirror CLI plug-in
|TP
|GA
|GA

|Mount shared entitlements in BuildConfigs in RHEL
|TP
|TP
|TP

|Support for {rh-openstack} DCN
|TP
|TP
|TP

|Support for external cloud providers for clusters on {rh-openstack}
|TP
|TP
|TP

|OVS hardware offloading for clusters on {rh-openstack}
|TP
|GA
|GA

|External DNS Operator
|TP
|TP
|TP

|Alerting rules based on platform monitoring metrics
|-
|TP
|TP

|AWS Load Balancer Operator
|-
|TP
|TP

|Node Observability Operator
|-
|TP
|TP

|Java-based Operator
|-
|TP
|TP

|Hosted control planes for {product-title}
|-
|TP
|TP

|Managing machines with the Cluster API
|-
|TP
|TP

|Linux Control Group version 2 (cgroup v2)
|-
|-
|TP

|crun container runtime
|-
|-
|TP

|Cron job time zones
|-
|-
|TP

|====

[id="ocp-4-12-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.8 and beyond! This needs some SME/QE review before being updated for 4.11. Need to check if KI should be removed or should stay.
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to 4.8, you can either revoke or continue to allow unauthenticated access. Unless there is a specific need for unauthenticated access, you should revoke it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* Due to the inclusion of old images in some image indexes, running `oc adm catalog mirror` and `oc image mirror` might result in the following error: `error: unable to retrieve source image`. As a temporary workaround, you can use the `--skip-missing` option to bypass the error and continue downloading the image index. For more information, see link:https://access.redhat.com/solutions/6975305[Service Mesh Operator mirroring failed].

* There is a known issue with Nutanix installation where the installation fails if you use 4096-bit certificates with Prism Central 2022.x. Instead, use 2048-bit certificates. (link:https://access.redhat.com/solutions/6976743[*KCS*])

[id="ocp-4-12-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster-within-minor.adoc#updating-cluster-within-minor[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-12-0-ga"]
=== RHSA-2022:xxx - {product-title} 4.12.0 image release, bug fix, and security update advisory

Issued: TBD

{product-title} release 4.12.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2022:xxxx[RHSA-2022:xxxx] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2022:xxxx[RHSA-2022:xxxx] advisory.

Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.12.0 --pullspecs
----
//replace 4.y.z for the correct values for the release. You do not need to update oc to run this command.
