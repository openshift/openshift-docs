[id="ocp-4-9-release-notes"]
= {product-title} {product-version} release notes
include::modules/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multi-tenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-9-about-this-release"]
== About this release

// TODO: Update k8s link once there is a version-specific URL for the Kubernetes release
{product-title} (link:https://access.redhat.com/errata/RHSA-2021:2438[RHSA-2021:2438]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md[Kubernetes 1.22] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

//Red Hat did not publicly release {product-title} 4.9.0 as the GA version and, instead, is releasing {product-title} 4.9.TBD as the GA version.

{product-title} {product-version} clusters are available at https://cloud.redhat.com/openshift. The {cloud-redhat-com} application for {product-title} allows you to deploy OpenShift clusters to either on-premise or cloud environments.

{product-title} {product-version} is supported on {op-system-base-full} 7.9 and 8.4, as well as on {op-system-first} 4.9.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base-full} 7.9 or 8.4 for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//{product-title} 4.6 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

With the release of {product-title} 4.9, version 4.6 is now end of life. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift Container Platform Life Cycle Policy].

[id="ocp-4-9-inclusive-language"]
== Making open source more inclusive

Red Hat is committed to replacing problematic language in our code, documentation, and web properties.

As part of that effort, with this release the following changes are in place:

* The link:https://github.com/openshift/openshift-docs[OpenShift Docs GitHub repository] `master` branch has been renamed to `main`.
* We have begun to progressively replace the terminology of "master" with "control plane". You will notice throughout the documentation that we use both terms, with "master" in parenthesis. For example "... the control plane node (also known as the master node)". In a future release, we will update this to be "the control plane node".

[id="ocp-4-9-add-on-support-status"]
== {product-title} layered and dependant component support and compatibility

The scope of support for layered and dependant components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift Container Platform Life Cycle Policy].

[id="ocp-4-9-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-9-rhcos"]
=== {op-system-first}

[id="ocp-4-9-install-ign-removed"]
==== Installation Ignition config is removed upon boot

Nodes installed with the `coreos-installer` program previously retained the installation Ignition config in the `/boot/ignition/config.ign` file. Starting with the {product-title} 4.9 installation image, that file is removed when the node is provisioned. This change does not affect clusters that were installed on previous {product-title} versions because they still use an older bootimage.

[id="ocp-4-9-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-9-installation-ash-upi"]
==== Installing a cluster on Microsoft Azure Stack Hub using user-provisioned infrastructure

{product-title} 4.9 introduces support for installing a cluster on Azure Stack Hub using user-provisioned infrastructure.

You can incorporate example Azure Resource Manager (ARM) templates provided by Red Hat to assist in the deployment process, or create your own. You are also free to create the required resources through other methods; the ARM templates are just an example.

//See ../installing/install_azure_stack_hub/installing-azure-stack-hub-user-infra.adoc#installing-azure-stack-hub-user-infra[Installing a cluster on Azure Stack Hub using ARM templates] for details.


[id=ocp-4.9-azure-cidr]
==== Increased size of Azure subnets within the machine CIDR
The {product-title} installation program for Microsoft Azure now creates subnets as large as possible within the machine CIDR. This lets the cluster use a machine CIDR that is appropriately sized to accommodate the number of nodes in the cluster.

[id="ocp-4-9-aws-china-regions"]
==== Support for AWS regions in China
{product-title} {product-version} introduces support for AWS regions in China. You can now install and update {product-title} clusters in the `cn-north-1` (Beijing) and `cn-northwest-1`(Ningxia) regions.

For more information, see xref:../installing/installing_aws/installing-aws-china.adoc[Installing a cluster on AWS China].

[id="ocp-4-9-expanding-with-virtual-media-on-baremetal-network"]
==== Expanding the cluster with Virtual Media on the baremetal network

In {product-title} {product-version}, you can expand an installer provisioned cluster deployed using the `provisioning` network by using Virtual Media on the `baremetal` network. You can use this feature when the `ProvisioningNetwork` configuration setting is set to `Managed`. To use this feature, you must set the `virtualMediaViaExternalNetwork` configuration setting to `true` in the `provisioning` custom resource (CR). You must also edit the machineset to use the API VIP address. See xref:../installing/installing_bare_metal_ipi/ipi-install-expanding-the-cluster.html#preparing-to-deploy-with-virtual-media-on-the-baremetal-network_ipi-install-expanding[Preparing to deploy with Virtual Media on the baremetal network] for details.

[id="ocp-4-9-web-console"]
=== Web console

[id="ocp-4-9-assessing-node-logs-from-the-node-details-page"]
==== Accessing node logs from the *Node Details* page
With this update, admins now have the ability to access node logs from the *Node Details* page. From there, you can switch between individual log files and journal log units in order to inquire about the node.

[id="ocp-4-9-ibm-z"]
=== IBM Z and LinuxONE

With this release, IBM Z and LinuxONE are now compatible with {product-title} {product-version}. The installation can be performed with z/VM or {op-system-base} KVM. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_z/installing-ibm-z.adoc#installing-ibm-z[Installing a cluster with z/VM on IBM Z and LinuxONE]
* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc#installing-restricted-networks-ibm-z[Installing a cluster with z/VM on IBM Z and LinuxONE in a restricted network]
* xref:../installing/installing_ibm_z/installing-ibm-z-kvm.adoc#installing-ibm-z-kvm[Installing a cluster with {op-system-base} KVM on IBM Z and LinuxONE]
* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z-kvm.adoc#installing-restricted-networks-ibm-z-kvm[Installing a cluster with RHEL KVM on IBM Z and LinuxONE in a restricted network]

[discrete]
==== Notable enhancements

The following new features are supported on IBM Z and LinuxONE with {product-title} {product-version}:

* Helm
* Support for multiple network interfaces
* Service Binding Operator

[discrete]
==== Supported features

The following features are also supported on IBM Z and LinuxONE:

* Currently, the following Operators are supported:
** Cluster Logging Operator
** NFD Operator
** OpenShift Elasticsearch Operator
** Local Storage Operator
** Service Binding Operator

* Encrypting data stored in etcd
* Multipathing
* Persistent storage using iSCSI
* Persistent storage using local volumes (Local Storage Operator)
* Persistent storage using hostPath
* Persistent storage using Fibre Channel
* Persistent storage using Raw Block
* OVN-Kubernetes
* Three-node cluster support
* z/VM Emulated FBA devices on SCSI disks
* 4K FCP block device

These features are available only for {product-title} on IBM Z and LinuxONE for {product-version}:

* HyperPAV enabled on IBM Z and LinuxONE for the virtual machines for FICON attached ECKD storage

[discrete]
==== Restrictions

Note the following restrictions for {product-title} on IBM Z and LinuxONE:

* The following {product-title} Technology Preview features are unsupported:
** Precision Time Protocol (PTP) hardware

* The following {product-title} features are unsupported:
** Automatic repair of damaged machines with machine health checking
** CodeReady Containers (CRC)
** Controlling overcommit and managing container density on nodes
** CSI volume cloning
** CSI volume snapshots
** FIPS cryptography
** Multus CNI plug-in
** NVMe
** OpenShift Metering
** OpenShift Virtualization
** Tang mode disk encryption during {product-title} deployment

* Worker nodes must run {op-system-first}
* Persistent shared storage must be provisioned by using either NFS or other supported storage protocols
* Persistent non-shared storage must be provisioned using local storage, like iSCSI, FC, or using LSO with DASD, FCP, or EDEV/FBA

[id="ocp-4-9-ibm-power"]
=== IBM Power Systems

With this release, IBM Power Systems are now compatible with {product-title} {product-version}. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_power/installing-ibm-power.adoc#installing-ibm-power_installing-ibm-power[Installing a cluster on IBM Power Systems]
* xref:../installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc#installing-restricted-networks-ibm-power_installing-restricted-networks-ibm-power[Installing a cluster on IBM Power Systems in a restricted network]

[discrete]
==== Notable enhancements

The following new features are supported on IBM Power Systems with {product-title} {product-version}:

* Helm
* Support for Power10
* Support for multiple network interfaces
* Service Binding Operator

[discrete]
==== Supported features

The following features are also supported on IBM Power Systems:

* Currently, the following Operators are supported:
** Cluster Logging Operator
** NFD Operator
** OpenShift Elasticsearch Operator
** Local Storage Operator
** SR-IOV Network Operator
** Service Binding Operator

* Multipathing
* Persistent storage using iSCSI
* Persistent storage using local volumes (Local Storage Operator)
* Persistent storage using hostPath
* Persistent storage using Fibre Channel
* Persistent storage using Raw Block
* OVN-Kubernetes
* 4K Disk Support
* NVMe
* Encrypting data stored in etcd
* Three-node cluster support
* Multus SR-IOV

[discrete]
==== Restrictions

Note the following restrictions for {product-title} on IBM Power Systems:

* The following {product-title} Technology Preview features are unsupported:
** Precision Time Protocol (PTP) hardware

* The following {product-title} features are unsupported:
** Automatic repair of damaged machines with machine health checking
** CodeReady Containers (CRC)
** Controlling overcommit and managing container density on nodes
** FIPS cryptography
** OpenShift Metering
** OpenShift Virtualization
** Tang mode disk encryption during {product-title} deployment

* Worker nodes must run {op-system-first}
* Persistent storage must be of the Filesystem type that uses local volumes, Network File System (NFS), or Container Storage Interface (CSI)

[id="ocp-4-9-security"]
=== Security and compliance

[id="ocp-4-9-security-custom-audit-policy"]
==== Configuring the audit log policy with custom rules

You now have more fine-grained control over the audit logging level for {product-title}. You can use custom rules to specify a different audit policy profile (`Default`, `WriteRequestBodies`, `AllRequestBodies`, or `None`) for different groups.

For more information, see xref:../security/audit-log-policy-config.adoc#configuring-audit-policy-custom_audit-log-policy-config[Configuring the audit log policy with custom rules].

[id="ocp-4-9-security-disable-audit-logging"]
==== Disabling audit logging

You can now disable audit logging for {product-title} by using the `None` audit policy profile.

[WARNING]
====
It is not recommended to disable audit logging unless you are fully aware of the risks of not logging data that can be beneficial when troubleshooting issues. If you disable audit logging and a support situation arises, you might need to enable audit logging and reproduce the issue in order to troubleshoot properly.
====

For more information, see xref:../security/audit-log-policy-config.adoc#configuring-audit-policy-disable_audit-log-policy-config[Disabling audit logging].

[id="ocp-4-9-security-customize-oauth-server-url"]
==== Customizing the OAuth server URL

You can now customize the URL for the internal OAuth server. For more information, see xref:../authentication/configuring-internal-oauth.adoc#customizing-the-oauth-server-url_configuring-internal-oauth[Customizing the internal OAuth server URL].

[id="ocp-4-9-networking"]
=== Networking

[id="ocp-4-9-new-configs-linuxptp-services"]
==== Enhancements to linuxptp services

{product-title} {product-version} introduces the following updates to PTP:

* the `ptp4lConf` field

* new option to configure `linuxptp` services as a boundary clock

For more information, see xref:../networking/configuring-ptp.adoc#configuring-linuxptp-services-as-boundary-clock_configuring-ptp[Configuring linuxptp services as boundary clock].

[id="ocp-4-9-ovn-kubernetes-egress-ips-balance"]
==== OVN-Kubernetes cluster network provider egress IP feature balances across nodes

The egress IP feature of OVN-Kubernetes now balances network traffic approximately equally across nodes for a given namespace, if that namespace is assigned multiple egress IP addresses. Each IP address must reside on a different node.
For more information, refer to xref:../networking/ovn_kubernetes_network_provider/configuring-egress-ips-ovn.adoc#configuring-egress-ips-ovn[Configuring egress IPs for a project] for OVN-Kubernetes.

[id="ocp-4-9-sriov-dpdk-ga"]
==== SR-IOV containerized Data Plane Development Kit (DPDK) is GA

The containerized Data Plane Development Kit (DPDK) is now GA in {product-title} {product-version}. For more information, see xref:../networking/hardware_networks/using-dpdk-and-rdma.adoc#using-dpdk-and-rdma[Using virtual functions (VFs) with DPDK and RDMA modes].

[id="ocp-4-9-networking-metallb"]
==== MetalLB load balancer

This release introduces the MetalLB Operator. After installing and configuring the MetalLB Operator, you can deploy MetalLB to provide a native load balancer implementation for services on bare-metal clusters. Other on-premise infrastructures that are like bare metal can also benefit.

The Operator introduces a custom resource, `AddressPool`. You configure address pools with ranges of IP addresses that MetalLB can assign to services. When you add a service of type `LoadBalancer`, MetalLB assigns an IP address from a pool.

For this release, Red Hat only supports using MetalLB in layer 2 mode.

For more information, see xref:../networking/metallb/about-metallb.adoc#about-metallb[About MetalLB and the MetalLB Operator].

[id="ocp-4-9-networking-CNI-VRF-plug-in"]
==== CNI VRF plug-in is generally available

The CNI VRF plug-in was previously introduced as a Technology Preview feature in {product-title} 4.7 and is now generally available in {product-title} {product-version}.

For more information, see xref:../networking/multiple_networks/assigning-a-secondary-network-to-a-vrf.adoc#cnf-assigning-a-secondary-network-to-a-vrf[Assigning a secondary network to a VRF].

[id="ocp-4-9-storage"]
=== Storage

[id="ocp-4-9-storage-aws-ebs-csi-ga"]
==== Persistent storage using AWS EBS CSI driver operator is generally available
{product-title} is capable of provisioning persistent volumes (PVs) using the Container Storage Interface (CSI) driver for AWS Elastic Block Store (EBS).
This feature was previously introduced as a Technology Preview feature in {product-title} 4.5 and is now generally available and enabled by default in {product-title} 4.9.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-ebs.adoc#persistent-storage-csi-ebs[AWS EBS CSI Driver Operator].

[id="ocp-4-9-storage-azure-stack-hub-csi"]
==== Persistent storage using the Azure Stack Hub CSI Driver Operator (general availability)
{product-title} is capable of provisioning PVs using the CSI driver for Azure Stack Hub Storage.
Azure Stack Hub, which is part of the Azure Stack portfolio, allows you to run apps in an on-premises environment and deliver Azure services in your datacenter.
The Azure Stack Hub CSI Driver Operator that manages this driver is new for 4.9 and generally available.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-azure-stack-hub.adoc#persistent-storage-csi-azure-stack-hub[Azure Stack Hub CSI Driver Operator].

[id="ocp-4-9-storage-aws-efs-csi"]
==== Persistent storage using the AWS EFS CSI Driver Operator (Technology Preview)
{product-title} is capable of provisioning PVs using the CSI driver for AWS Elastic File Service (EFS).
The AWS EFS CSI Driver Operator that manages this driver is in Technology Preview.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-aws-efs.adoc#persistent-storage-csi-aws-efs[AWS EFS CSI Driver Operator].

[id="ocp-4-9-storage-auto-migration-csi-gce"]
==== Automatic CSI migration supports GCE (Technology Preview)
Starting with {product-title} 4.8, automatic migration for in-tree volume plug-ins to their equivalent CSI drivers became available as a Technology Preview feature.
This feature now supports automatic migration from Google Compute Engine Persistent Disk (GCE PD) in-tree plug-in to the Google Cloud Platform (GCP) Persistent Disk CSI driver.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-migration.adoc#persistent-storage-csi-migration[CSI Automatic Migration].

[id="ocp-4-9-storage-auto-migration-csi-azure-disk"]
==== Automatic CSI migration supports Azure Disk (Technology Preview)
Starting with {product-title} 4.8, automatic migration for in-tree volume plug-ins to their equivalent CSI drivers became available as a Technology Preview feature.
This feature now supports automatic migration from the Azure Disk in-tree plug-in to the Azure Disk CSI driver.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-migration.adoc#persistent-storage-csi-migration[CSI Automatic Migration].

[id="ocp-4-9-vsphere-csi-new-features"]
==== VMWare vSphere CSI Driver Operator creates storage policy automatically (Technology Preview)
The vSphere CSI Operator Driver storage class now uses vSphereâ€™s storage policy. {product-title} automatically creates a storage policy that targets datastore configured in cloud configuration.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-migration.adoc#persistent-storage-csi-vsphere[VMWare vSphere CSI Driver Operator].

[id="ocp-4-9-lso-new-metrics"]
==== New metrics provided for Local Storage Operator
{product-title} 4.9 provides the following new metrics for the Local Storage Operator:

* `lso_discovery_disk_count`: total number of discovered devices on each node

* `lso_lvset_provisioned_PV_count`: total number of PVs created by `LocalVolumeSet` objects

* `lso_lvset_unmatched_disk_count`: total number of disks that Local Storage Operator did not select for provisioning because of mismatching criteria

* `lso_lvset_orphaned_symlink_count`: number of devices with PVs that no longer match `LocalVolumeSet` object criteria

* `lso_lv_orphaned_symlink_count`: number of devices with PVs that no longer match `LocalVolume` object criteria

* `lso_lv_provisioned_PV_count`: total number of provisioned PVs for `LocalVolume`

For more information, see xref:../storage/persistent_storage/persistent-storage-local.adoc#persistent-storage-local[Persistent storage using local volumes].

[id="ocp-4-9-registry"]
=== Registry

[id="ocp-4-9-olm"]
=== Operator lifecycle

[id="ocp-4-9-osdk"]
=== Operator development

[id="ocp-4-9-builds"]
=== Builds

[id="ocp-4-9-images"]
=== Images

[id="ocp-4-9-wildcard-domain"]
==== Wildcard domains as registry sources

This release introduces support for using wildcard domains as registry sources in your image registry settings. With a wildcard domain, such as `*.example.com`, you can set your cluster to push and pull images from multiple subdomains without having to manually enter each one. For more information, see xref:../openshift_images/image-configuration.adoc#images-configuration-parameters_image-configuration[Image controller configuration parameters].

[id="ocp-4-9-machine-api"]
=== Machine API

[id="ocp-4-9-machine-api-rhel8"]
==== {op-system-base-full} 8 now supported for compute machines

Starting in {product-title} {product-version}, you can now use {op-system-base-full} 8.4 for compute machines. Previously, {op-system-base} 8 was not supported for compute machines.

You cannot upgrade {op-system-base} 7 compute machines to {op-system-base} 8. You must deploy new {op-system-base} 8 hosts, and the old {op-system-base} 7 hosts should be removed.

[id="ocp-4-9-nodes"]
=== Nodes

[id="ocp-4-9-nodes-scheduler-profiles"]
==== Scheduler profiles GA

Scheduling pods using a scheduler profile is now generally available. This is a replacement for configuring a scheduler policy. The following scheduler profiles are available:

* `LowNodeUtilization`: This profile attempts to spread pods evenly across nodes to get low resource usage per node.

* `HighNodeUtilization`: This profile attempts to place as many pods as possible onto as few nodes as possible, to minimize node count with high usage per node.

* `NoScoring`: This is a low-latency profile that strives for the quickest scheduling cycle by disabling all score plug-ins. This might sacrifice better scheduling decisions for faster ones.

For more information, see xref:../nodes/scheduling/nodes-scheduler-profiles.adoc#nodes-scheduler-profiles[Scheduling pods using a scheduler profile].

[id="ocp-4-9-nodes-descheduler"]
==== New descheduler profiles and customization

The following descheduler profiles are now available:

* `SoftTopologyAndDuplicates`: This profile is the same as `TopologyAndDuplicates`, except that pods with soft topology constraints, such as `whenUnsatisfiable: ScheduleAnyway`, are also considered for eviction.
* `EvictPodsWithLocalStorage`: This profile allows pods with local storage to be eligible for eviction.
* `EvictPodsWithPVC`: This profile allows pods with persistent volume claims to be eligible for eviction.

You can also customize the pod lifetime value for the `LifecycleAndUtilization` profile.

For more information, see xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler[Evicting pods using the descheduler].

[id="ocp-4-9-logging"]
=== Red Hat OpenShift Logging

In {product-title} 4.7, _Cluster Logging_ became _Red Hat OpenShift Logging_. For more information, see xref:../logging/cluster-logging-release-notes.adoc[Release notes for Red Hat OpenShift Logging].

[id="ocp-4-9-monitoring"]
=== Monitoring

[id="ocp-4-9-metering"]
=== Metering

The Metering Operator is deprecated as of {product-title} 4.6, and is scheduled to be removed in {product-title} 4.9.

[id="ocp-4-9-scale"]
=== Scale

[id="ocp-4-9-scale-memory-manager-in-beta-version"]
==== Memory Manager feature (Technology Preview)
The Memory Manager feature is now enabled by default for all pods running on the node that is configured with one of the following Topology Manager policies:

* `single-numa-node`
* `restricted`

For more information, see xref:../scalability_and_performance/using-topology-manager.adoc#topology_manager_policies_using-topology-manager[Topology Manager policies].

[id="ocp-4-9-backup-and-restore"]
=== Backup and restore

[id="ocp-4-9-dev-exp"]
=== Developer experience

[id="ocp-4-9-insights-operator"]
=== Insights Operator

[id="ocp-4-9-insights-operator-sca"]
==== Importing RHEL Simple Content Access certificates (Technology Preview)

In {product-title} 4.9, Insights Operator can import RHEL Simple Content Access (SCA) certificates from {cloud-redhat-com}.

For more information, see xref:../support/remote_health_monitoring/insights-operator-simple-access.adoc[_Importing RHEL Simple Content Access certificates with Insights Operator_].

[id="ocp-4-9-insights-operator-data-collection-enhancements"]
==== Insights Operator data collection enhancements

In {product-title} 4.9, the Insights Operator collects the following additional information:

* All of the `MachineConfig` resource definitions from a cluster.
* The names of the `PodSecurityPolicies` installed in a cluster.
* If installed, the `ClusterLogging` resource definition.
* If the `SamplesImagestreamImportFailing` alert is firing, then the `ImageStream` definitions and the last 100 lines of container logs from the `openshift-cluster-samples-operator` namespace.

With this additional information, Red Hat can provide improved remediation steps in Insights Advisor.

[id="ocp-4-9-auth"]
=== Authentication and authorization

[id="ocp-4-9-cco-ash-support"]
==== Support for Microsoft Azure Stack Hub with Cloud Credential Operator in manual mode

With this release, installations on Microsoft Azure Stack Hub can be performed by configuring the Cloud Credential Operator (CCO) in manual mode.

For more information, see xref:../authentication/managing_cloud_provider_credentials/cco-mode-manual.adoc[Using manual mode].

[id="ocp-4-9-sandboxed-containers"]
=== {sandboxed-containers-first}

[id="ocp-4-9-notable-technical-changes"]
== Notable technical changes

{product-title} 4.9 introduces the following notable technical changes.

[discrete]
[id="octavia-ovn-nodeport-changes"]
==== Octavia OVN NodePort changes
Previously, on {rh-openstack-first} deployments, opening traffic on NodePorts was constrained to the CIDR of the node's subnet. In order to support LoadBalancer services using the Octavia Open Virtual Network (OVN) provider, the security group rules that allow NodePort traffic to master and worker nodes are now changed to open `0.0.0.0/0`. 

[discrete]
[id="osp-loadbalancer-configuration-changes"]
==== OpenStack Platform LoadBalancer configuration changes
The {rh-openstack-first} cloud provider LoadBalancer configuration now defaults to `use-octavia=True`. An exception to this rule is a deployment with Kuryr, in which case `use-octavia` is set to `false`, because Kuryr handles LoadBalancer services on its own.

// Note: use [discrete] for these sub-headings.

[id="ocp-4-9-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more fine-grained functionality that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *TP*: _Technology Preview_
* *DEP*: _Deprecated_
* *REM*: _Removed_

//TODO: remove anything that has been REM since 4.6

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.7 |OCP 4.8 |OCP 4.9

|Package manifest format (Operator Framework)
|DEP
|REM
|

|`oc adm catalog build`
|DEP
|REM
|

|`--filter-by-os` flag for `oc adm catalog mirror`
|DEP
|REM
|

|v1beta1 CRDs
|DEP
|DEP
|

|Docker Registry v1 API
|DEP
|DEP
|

|Metering Operator
|DEP
|DEP
|

|Scheduler policy
|DEP
|DEP
|

|`ImageChangesInProgress` condition for Cluster Samples Operator
|DEP
|DEP
|

|`MigrationInProgress` condition for Cluster Samples Operator
|DEP
|DEP
|

|Use of `v1` in `apiVersion` for {product-title} resources
|DEP
|DEP
|

|Use of `dhclient` in {op-system-first}
|DEP
|DEP
|

|Cluster Loader
|GA
|DEP
|

|Bring your own RHEL 7 compute machines
|DEP
|DEP
|DEP

|`lastTriggeredImageID` field in the `BuildConfig` spec for Builds
|GA
|DEP
|

|Jenkins Operator
|TP
|DEP
|

|HPA custom metrics adapter based on Prometheus
|TP
|REM
|

|====

[id="ocp-4-9-deprecated-features"]
=== Deprecated features

[id="ocp-4-9-removed-features"]
=== Removed features

[id="ocp-4-9-removed-kube-1-22-apis"]
==== Beta APIs removed from Kubernetes 1.22

Kubernetes 1.22 removed the following deprecated `v1beta1` APIs. Migrate manifests and API clients to use the `v1` API version. For more information about migrating removed APIs, see the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22[Kubernetes documentation].

.`v1beta1` APIs removed from Kubernetes 1.22
[cols="2,2,1",options="header",]
|===
|Resource |API |Notable changes

|`APIService`
|`apiregistration.k8s.io/v1beta1`
|No

|`CertificateSigningRequest`
|`certificates.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#certificatesigningrequest-v122[Yes]

|`ClusterRole`
|`rbac.authorization.k8s.io/v1beta1`
|No

|`ClusterRoleBinding`
|`rbac.authorization.k8s.io/v1beta1`
|No

|`CSIDriver`
|`storage.k8s.io/v1beta1`
|No

|`CSINode`
|`storage.k8s.io/v1beta1`
|No

|`CustomResourceDefinition`
|`apiextensions.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#customresourcedefinition-v122[Yes]

|`Ingress`
|`extensions/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#ingress-v122[Yes]

|`Ingress`
|`networking.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#ingress-v122[Yes]

|`IngressClass`
|`networking.k8s.io/v1beta1`
|No

|`Lease`
|`coordination.k8s.io/v1beta1`
|No

|`LocalSubjectAccessReview`
|`authorization.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#subjectaccessreview-resources-v122[Yes]

|`MutatingWebhookConfiguration`
|`admissionregistration.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#webhook-resources-v122[Yes]

|`PriorityClass`
|`scheduling.k8s.io/v1beta1`
|No

|`Role`
|`rbac.authorization.k8s.io/v1beta1`
|No

|`RoleBinding`
|`rbac.authorization.k8s.io/v1beta1`
|No

|`SelfSubjectAccessReview`
|`authorization.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#subjectaccessreview-resources-v122[Yes]

|`StorageClass`
|`storage.k8s.io/v1beta1`
|No

|`SubjectAccessReview`
|`authorization.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#subjectaccessreview-resources-v122[Yes]

|`TokenReview`
|`authentication.k8s.io/v1beta1`
|No

|`ValidatingWebhookConfiguration`
|`admissionregistration.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#webhook-resources-v122[Yes]

|`VolumeAttachment`
|`storage.k8s.io/v1beta1`
|No

|===

[id="ocp-4-9-bug-fixes"]
== Bug fixes

*assisted-installer*

*Bare Metal Hardware Provisioning*

*Builds*

*Cloud Compute*

*Cloud Credential Operator*

*Cluster Version Operator*

*Compliance Operator*

*Console Kubevirt Plug-in*

*Console Storage Plug-in*

*DNS*

*etcd*

*Image Registry*

*ImageStreams*

*Insights Operator*

*Installer*

*kube-apiserver*

*Machine Config Operator*

*Metering Operator*
//TODO: remove if this Operator is removed as planned

*Monitoring*

*Networking*

*Node*

*oauth-apiserver*

*oc*

*Operator Lifecycle Manager (OLM)*

*Operator SDK*

*openshift-apiserver*

*Performance Addon Operator*

*RHCOS*

*Routing*

*Samples*

*service-ca*

*Storage*

*Web console (Administrator perspective)*

*Web console (Developer perspective)*

*Windows Containers*

[id="ocp-4-9-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_
* *DEP*: _Deprecated_

//TODO: remove anything that has been GA since 4.6

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.7 |OCP 4.8 |OCP 4.9

|Precision Time Protocol (PTP)
|TP
|TP
|

|`oc` CLI plug-ins
|TP
|GA
|

|Descheduler
|GA
|GA
|

|HPA for memory utilization
|GA
|GA
|

|Service Binding
|TP
|TP
|

|Raw Block with Cinder
|TP
|GA
|

|CSI volume snapshots
|GA
|GA
|GA

|CSI volume expansion
|TP
|TP
|TP

|vSphere Problem Detector Operator
|GA
|GA
|GA

|CSI Azure Disk Driver Operator
|-
|TP
|TP

|CSI Azure Stack Hub Driver Operator
|-
|-
|GA

|CSI GCP PD Driver Operator
|TP
|GA
|GA

|CSI OpenStack Cinder Driver Operator
|TP
|TP
|TP

|CSI AWS EBS Driver Operator
|TP
|TP
|GA

|CSI AWS EFS Driver Operator
|-
|-
|TP

|CSI automatic migration
|-
|TP
|TP

|CSI inline ephemeral volumes
|TP
|TP
|TP

|CSI vSphere Driver Operator
|-
|TP
|TP

|Automatic device discovery and provisioning with Local Storage Operator
|TP
|TP
|TP

|OpenShift Pipelines
|TP
|GA
|

|OpenShift GitOps
|TP
|GA
|

|OpenShift sandboxed containers
|-
|TP
|

|Vertical Pod Autoscaler
|TP
|GA
|

|Cron jobs
|TP
|GA
|

|PodDisruptionBudget
|TP
|GA
|

|Adding kernel modules to nodes with kvc
|TP
|TP
|

|Egress router CNI plug-in
|TP
|GA
|

|Scheduler profiles
|TP
|TP
|GA

|Non-preempting priority classes
|TP
|TP
|

|Kubernetes NMState Operator
|TP
|TP
|

|Assisted Installer
|TP
|TP
|

|AWS Security Token Service (STS)
|TP
|GA
|GA

|Kdump
|TP
|TP
|

|OpenShift Serverless
|-
|GA
|

|Serverless functions
|-
|TP
|

|Data Plane Development Kit (DPDK) support
|TP
|TP
|GA

|Memory Manager feature
|
|
|TP

|CNI VRF plug-in
|TP
|TP
|GA

|====

[id="ocp-4-9-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.8 and beyond! This needs some SME/QE review before being updated for 4.9.
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to 4.8, you can either revoke or continue to allow unauthenticated access. It is recommended to revoke unauthenticated access unless there is a specific need for it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* An Open Virtual Network (OVN) bug causes persistent connectivity issues with Octavia load balancers. When Octavia load balancers are created, OVN might not plug them into some Neutron subnets. These load balancers might be unreachable for some of the Neutron subnets. This problem affects Neutron subnets, which are created for each OpenShift namespace at random when Kuryr is configured. As a result, when this problem occurs the load balancer that implements OpenShift `Service` objects will be unreachable from OpenShift namespaces affected by the issue. Because of this bug, {product-title} 4.8 deployments that use Kuryr SDN are not recommended on {rh-openstack-first} 16.1 with OVN and OVN Octavia configured. This will be fixed in a future release of {rh-openstack}. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1937392[*BZ#1937392*])

* Installations on {rh-openstack-first} with Kuryr will not work if configured with a cluster-wide proxy when the cluster-wide proxy is required to access {rh-openstack} APIs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1985486[*BZ#1985486*])

* Due to a race condition, the {rh-openstack-first} cloud provider might not start properly. Consequently, LoadBalancer services might never get an `EXTERNAL-IP` set. As a temporary workaround, you can restart the kube-controller-manager pods using the procedure described in link:https://bugzilla.redhat.com/show_bug.cgi?id=2004542[*BZ#2004542*]. 


[id="ocp-4-9-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} 4.9 are released as asynchronous errata through the Red Hat Network. All {product-title} 4.9 errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified via email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} 4.9. Versioned asynchronous releases, for example with the form {product-title} 4.9.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster.adoc#updating-cluster[updating your cluster] properly.
====

[id="ocp-4-9-0-ga"]
=== #RHSA-2021:TBD1# - {product-title} #4.9.0# image release, bug fix, and security update advisory

Issued: #2021-10-18#

{product-title} release #4.9.0#, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the #link:https://access.redhat.com/errata/RHSA-2021:TBD1[RHSA-2021:TBD1]# advisory. The RPM packages that are included in the update are provided by the #link:https://access.redhat.com/errata/RHSA-2021:TBD2[RHSA-2021:TBD2]# advisory.

Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:

#link:https://access.redhat.com/solutions/TBD3[{product-title} 4.9.0 container image list]#
