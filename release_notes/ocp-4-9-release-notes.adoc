[id="ocp-4-9-release-notes"]
= {product-title} {product-version} release notes
include::modules/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multi-tenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-9-about-this-release"]
== About this release

// TODO: Update k8s link once there is a version-specific URL for the Kubernetes release
{product-title} (link:https://access.redhat.com/errata/RHSA-2021:2438[RHSA-2021:2438]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md[Kubernetes 1.22] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

//Red Hat did not publicly release {product-title} 4.9.0 as the GA version and, instead, is releasing {product-title} 4.9.TBD as the GA version.

{product-title} {product-version} clusters are available at https://cloud.redhat.com/openshift. The {cloud-redhat-com} application for {product-title} allows you to deploy OpenShift clusters to either on-premise or cloud environments.

{product-title} {product-version} is supported on {op-system-base-full} 7.9 and 8.4, as well as on {op-system-first} 4.9.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base-full} 7.9 or 8.4 for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//TODO: Add the line below for EUS releases.
//{product-title} 4.6 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

//TODO: The line below is not true for 4.9 but should be used when it is next appropriate. Revisit in October 2022 timeframe.
//With the release of {product-title} 4.9, version 4.6 is now end of life. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift Container Platform Life Cycle Policy].

[id="ocp-4-9-inclusive-language"]
== Making open source more inclusive

Red Hat is committed to replacing problematic language in our code, documentation, and web properties.

As part of that effort, with this release the following changes are in place:

* The link:https://github.com/openshift/openshift-docs[OpenShift Docs GitHub repository] `master` branch has been renamed to `main`.
* We have begun to progressively replace the terminology of "master" with "control plane". You will notice throughout the documentation that we use both terms, with "master" in parenthesis. For example "... the control plane node (also known as the master node)". In a future release, we will update this to be "the control plane node".

[id="ocp-4-9-add-on-support-status"]
== {product-title} layered and dependant component support and compatibility

The scope of support for layered and dependant components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift Container Platform Life Cycle Policy].

[id="ocp-4-9-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-9-rhcos"]
=== {op-system-first}

[id="ocp-4-9-install-ign-removed"]
==== Installation Ignition config is removed upon boot

Nodes installed with the `coreos-installer` program previously retained the installation Ignition config in the `/boot/ignition/config.ign` file. Starting with the {product-title} 4.9 installation image, that file is removed when the node is provisioned. This change does not affect clusters that were installed on previous {product-title} versions because they still use an older bootimage.

[id="ocp-4-9-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-9-installation-ash-upi"]
==== Installing a cluster on Microsoft Azure Stack Hub using user-provisioned infrastructure

{product-title} 4.9 introduces support for installing a cluster on Azure Stack Hub using user-provisioned infrastructure.

You can incorporate example Azure Resource Manager (ARM) templates provided by Red Hat to assist in the deployment process, or create your own. You are also free to create the required resources through other methods; the ARM templates are just an example.

See xref:../installing/installing_azure_stack_hub/installing-azure-stack-hub-user-infra.adoc#installing-azure-stack-hub-user-infra[Installing a cluster on Azure Stack Hub using ARM templates] for details.

[id="ocp-4-9-upgrade-pause-mhc"]
==== Pausing machine health checks before updating the cluster

During the upgrade process, nodes in the cluster might become temporarily unavailable. In the case of worker nodes, the machine health check might identify such nodes as unhealthy and reboot them. To avoid rebooting such nodes, {product-title} {product-version} introduces the `cluster.x-k8s.io/paused=""` annotation to let you pause the `MachineHealthCheck` resources before updating the cluster.

For more information, see xref:../updating/updating-cluster-cli.adoc#machine-health-checks-pausing_updating-cluster-cli[Pausing a MachineHealthCheck resource].

[id=ocp-4.9-azure-cidr]
==== Increased size of Azure subnets within the machine CIDR
The {product-title} installation program for Microsoft Azure now creates subnets as large as possible within the machine CIDR. This lets the cluster use a machine CIDR that is appropriately sized to accommodate the number of nodes in the cluster.

[id="ocp-4-9-aws-china-regions"]
==== Support for AWS regions in China
{product-title} {product-version} introduces support for AWS regions in China. You can now install and update {product-title} clusters in the `cn-north-1` (Beijing) and `cn-northwest-1`(Ningxia) regions.

For more information, see xref:../installing/installing_aws/installing-aws-china.adoc[Installing a cluster on AWS China].

[id="ocp-4-9-expanding-with-virtual-media-on-baremetal-network"]
==== Expanding the cluster with Virtual Media on the baremetal network

In {product-title} {product-version}, you can expand an installer provisioned cluster deployed using the `provisioning` network by using Virtual Media on the `baremetal` network. You can use this feature when the `ProvisioningNetwork` configuration setting is set to `Managed`. To use this feature, you must set the `virtualMediaViaExternalNetwork` configuration setting to `true` in the `provisioning` custom resource (CR). You must also edit the machineset to use the API VIP address. See xref:../installing/installing_bare_metal_ipi/ipi-install-expanding-the-cluster.html#preparing-to-deploy-with-virtual-media-on-the-baremetal-network_ipi-install-expanding[Preparing to deploy with Virtual Media on the baremetal network] for details.

[id="ocp-4-9-admin-ack-upgrading"]
==== Required administrator acknowledgment when upgrading from {product-title} 4.8 to 4.9

{product-title} 4.9 uses Kubernetes 1.22, which removed a xref:../release_notes/ocp-4-9-release-notes.adoc#ocp-4-9-removed-kube-1-22-apis[significant number of deprecated `v1beta1` APIs].

4.8.14 introduced a requirement that an administrator must provide a manual acknowledgment before the cluster can be upgraded from {product-title} 4.8 to 4.9. This is to help prevent issues after upgrading to {product-title} 4.9, where APIs that have been removed are still in use by workloads, tools, or other components running on or interacting with the cluster. Administrators must evaluate their cluster for any APIs in use that will be removed and migrate the affected components to use the appropriate new API version. After this is done, the administrator can provide the administrator acknowledgment.

All {product-title} 4.8 clusters require this administrator acknowledgment before they can be upgraded to {product-title} 4.9.

For more information, see xref:../updating/updating-cluster-prepare.adoc#updating-cluster-prepare[Preparing to update to {product-title} 4.9].

[id="ocp-4-9-openshift-on-openstack-pci-passthrough-support"]
==== Support for installation on {rh-openstack} deployments that use PCI passthrough

{product-title} {product-version} introduces support for installation on {rh-openstack-first} deployments that rely on link:https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/configuring_the_compute_service_for_instance_creation/configuring-pci-passthrough[PCI passthrough].

[id="ocp-4-9-upgrading-etcd-backup"]
==== Upgrading etcd version 3.4 to 3.5

{product-title} 4.9 supports etcd 3.5. Before you upgrade the cluster, verify that a valid etcd backup exists. An etcd backup ensures that the cluster can be restored if an upgrade failure occurs. In {product-title} 4.9, etcd upgrades are automatic. Depending on the cluster’s transition state to version 4.9, an etcd backup might be available. However, verifying that a backup exists before the cluster upgrade starts is recommended.

[id="ocp-4-9-installation-ibm-cloud"]
==== Installing a cluster on IBM Cloud using installer-provisioned infrastructure

{product-title} 4.9 introduces support for installing a cluster on IBM Cloud&#174; using installer-provisioned infrastructure. The procedure is nearly identical to installer-provisioned infrastructure on bare metal with these differences:

* Installer-provisioned installation of {product-title} 4.9 on IBM Cloud requires the `provisioning` network, IPMI, and PXE boot. Red Hat does not support deployment with Redfish and virtual media on IBM Cloud.
* You must create and configure public and private VLANs on the IBM Cloud.
* IBM Cloud nodes must be available before starting the installation process. So you must create the IBM Cloud nodes first.
* You must prepare the provisioner node.
* You must install and configure a DHCP server on the public `baremetal` network.
* You must configure the `install-config.yaml` file so that each node points to the BMC using IPMI, and sets the IPMI privilege level to `OPERATOR.`

See xref:../installing/installing_ibm_cloud/install-ibm-cloud-prerequisites.adoc[Deploying installer-provisioned clusters on IBM Cloud] for details.

[id="ocp-4-9-fujitsu-raid-bios-support"]
==== Improved support for Fujitsu hardware on installer-provisioned clusters

{product-title} 4.9 adds BIOS configuration support for worker nodes when deploying installer-provisioned clusters on Fujitsu hardware and using the Fujitsu integrated Remote Management Controller (iRMC). See xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#configuring-bios-for-worker-node_ipi-install-configuration-files[Configuring BIOS for worker node] for details.

[id="ocp-4-9-web-console"]
=== Web console

[id="ocp-4-9-assessing-node-logs-from-the-node-details-page"]
==== Accessing node logs from the *Node Details* page
With this update, admins now have the ability to access node logs from the *Node Details* page. From there, you can switch between individual log files and journal log units in order to inquire about the node.

[id="ocp-4-9-ibm-z"]
=== IBM Z and LinuxONE

With this release, IBM Z and LinuxONE are now compatible with {product-title} {product-version}. The installation can be performed with z/VM or {op-system-base} KVM. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_z/installing-ibm-z.adoc#installing-ibm-z[Installing a cluster with z/VM on IBM Z and LinuxONE]
* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc#installing-restricted-networks-ibm-z[Installing a cluster with z/VM on IBM Z and LinuxONE in a restricted network]
* xref:../installing/installing_ibm_z/installing-ibm-z-kvm.adoc#installing-ibm-z-kvm[Installing a cluster with {op-system-base} KVM on IBM Z and LinuxONE]
* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z-kvm.adoc#installing-restricted-networks-ibm-z-kvm[Installing a cluster with RHEL KVM on IBM Z and LinuxONE in a restricted network]

[discrete]
==== Notable enhancements

The following new features are supported on IBM Z and LinuxONE with {product-title} {product-version}:

* Helm
* Support for multiple network interfaces
* Service Binding Operator

[discrete]
==== Supported features

The following features are also supported on IBM Z and LinuxONE:

* Currently, the following Operators are supported:
** Cluster Logging Operator
** NFD Operator
** OpenShift Elasticsearch Operator
** Local Storage Operator
** Service Binding Operator

* Encrypting data stored in etcd
* Multipathing
* Persistent storage using iSCSI
* Persistent storage using local volumes (Local Storage Operator)
* Persistent storage using hostPath
* Persistent storage using Fibre Channel
* Persistent storage using Raw Block
* OVN-Kubernetes
* Three-node cluster support
* z/VM Emulated FBA devices on SCSI disks
* 4K FCP block device

These features are available only for {product-title} on IBM Z and LinuxONE for {product-version}:

* HyperPAV enabled on IBM Z and LinuxONE for the virtual machines for FICON attached ECKD storage

[discrete]
==== Restrictions

Note the following restrictions for {product-title} on IBM Z and LinuxONE:

* The following {product-title} Technology Preview features are unsupported:
** Precision Time Protocol (PTP) hardware

* The following {product-title} features are unsupported:
** Automatic repair of damaged machines with machine health checking
** CodeReady Containers (CRC)
** Controlling overcommit and managing container density on nodes
** CSI volume cloning
** CSI volume snapshots
** FIPS cryptography
** Multus CNI plug-in
** NVMe
** OpenShift Metering
** OpenShift Virtualization
** Tang mode disk encryption during {product-title} deployment

* Worker nodes must run {op-system-first}
* Persistent shared storage must be provisioned by using either NFS or other supported storage protocols
* Persistent non-shared storage must be provisioned using local storage, like iSCSI, FC, or using LSO with DASD, FCP, or EDEV/FBA

[id="ocp-4-9-ibm-power"]
=== IBM Power Systems

With this release, IBM Power Systems are now compatible with {product-title} {product-version}. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_power/installing-ibm-power.adoc#installing-ibm-power_installing-ibm-power[Installing a cluster on IBM Power Systems]
* xref:../installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc#installing-restricted-networks-ibm-power_installing-restricted-networks-ibm-power[Installing a cluster on IBM Power Systems in a restricted network]

[discrete]
==== Notable enhancements

The following new features are supported on IBM Power Systems with {product-title} {product-version}:

* Helm
* Support for Power10
* Support for multiple network interfaces
* Service Binding Operator

[discrete]
==== Supported features

The following features are also supported on IBM Power Systems:

* Currently, the following Operators are supported:
** Cluster Logging Operator
** NFD Operator
** OpenShift Elasticsearch Operator
** Local Storage Operator
** SR-IOV Network Operator
** Service Binding Operator

* Multipathing
* Persistent storage using iSCSI
* Persistent storage using local volumes (Local Storage Operator)
* Persistent storage using hostPath
* Persistent storage using Fibre Channel
* Persistent storage using Raw Block
* OVN-Kubernetes
* 4K Disk Support
* NVMe
* Encrypting data stored in etcd
* Three-node cluster support
* Multus SR-IOV

[discrete]
==== Restrictions

Note the following restrictions for {product-title} on IBM Power Systems:

* The following {product-title} Technology Preview features are unsupported:
** Precision Time Protocol (PTP) hardware

* The following {product-title} features are unsupported:
** Automatic repair of damaged machines with machine health checking
** CodeReady Containers (CRC)
** Controlling overcommit and managing container density on nodes
** FIPS cryptography
** OpenShift Metering
** OpenShift Virtualization
** Tang mode disk encryption during {product-title} deployment

* Worker nodes must run {op-system-first}
* Persistent storage must be of the Filesystem type that uses local volumes, Network File System (NFS), or Container Storage Interface (CSI)

[id="ocp-4-9-security"]
=== Security and compliance

[id="ocp-4-9-security-custom-audit-policy"]
==== Configuring the audit log policy with custom rules

You now have more fine-grained control over the audit logging level for {product-title}. You can use custom rules to specify a different audit policy profile (`Default`, `WriteRequestBodies`, `AllRequestBodies`, or `None`) for different groups.

For more information, see xref:../security/audit-log-policy-config.adoc#configuring-audit-policy-custom_audit-log-policy-config[Configuring the audit log policy with custom rules].

[id="ocp-4-9-security-disable-audit-logging"]
==== Disabling audit logging

You can now disable audit logging for {product-title} by using the `None` audit policy profile.

[WARNING]
====
It is not recommended to disable audit logging unless you are fully aware of the risks of not logging data that can be beneficial when troubleshooting issues. If you disable audit logging and a support situation arises, you might need to enable audit logging and reproduce the issue in order to troubleshoot properly.
====

For more information, see xref:../security/audit-log-policy-config.adoc#configuring-audit-policy-disable_audit-log-policy-config[Disabling audit logging].

[id="ocp-4-9-security-customize-oauth-server-url"]
==== Customizing the OAuth server URL

You can now customize the URL for the internal OAuth server. For more information, see xref:../authentication/configuring-internal-oauth.adoc#customizing-the-oauth-server-url_configuring-internal-oauth[Customizing the internal OAuth server URL].

[id="ocp-4-9-etcd"]
=== etcd

[id="ocp-4-9-security-auto-rotate-cert"]
==== Automatic rotation of etcd certificates

In {product-title} 4.9, etcd certificates are automatically rotated and are managed by the system.

[id="ocp-4-9-security-tls-profile-setting"]
==== Additional TLS security profile setting on the API server

The Kubernetes API server TLS security profile setting is now also honored by etcd.

[id="ocp-4-9-networking"]
=== Networking

[id="ocp-4-9-new-configs-linuxptp-services"]
==== Enhancements to linuxptp services

{product-title} {product-version} introduces the following updates to PTP:

* New `ptp4lConf` field

* New option to configure `linuxptp` services as a boundary clock

For more information, see xref:../networking/using-ptp.adoc#configuring-linuxptp-services-as-boundary-clock_using-ptp[Configuring linuxptp services as boundary clock].

[id="ocp-4-9-ptp-fast-event-notifications"]
==== Monitoring PTP fast events with the PTP fast event notification framework

Fast event notifications for PTP events are now available for bare-metal clusters. The PTP Operator generates event notifications for every configured PTP-capable network interface. Events are made available through a REST API for applications running on the same node. Fast event notifications are transported by an Advanced Message Queuing Protocol (AMQP) message bus provided by the AMQ Interconnect Operator.

For more information, see xref:../networking/using-ptp.adoc#cnf-about-ptp-and-clock-synchronization_using-ptp[About PTP and clock synchronization error events].

[id="ocp-4-9-ovn-kubernetes-egress-ips-balance"]
==== OVN-Kubernetes cluster network provider egress IP feature balances across nodes

The egress IP feature of OVN-Kubernetes now balances network traffic approximately equally across nodes for a given namespace, if that namespace is assigned multiple egress IP addresses. Each IP address must reside on a different node.
For more information, refer to xref:../networking/ovn_kubernetes_network_provider/configuring-egress-ips-ovn.adoc#configuring-egress-ips-ovn[Configuring egress IPs for a project] for OVN-Kubernetes.

[id="ocp-4-9-sriov-dpdk-ga"]
==== SR-IOV containerized Data Plane Development Kit (DPDK) is GA

The containerized Data Plane Development Kit (DPDK) is now GA in {product-title} {product-version}. For more information, see xref:../networking/hardware_networks/using-dpdk-and-rdma.adoc#using-dpdk-and-rdma[Using virtual functions (VFs) with DPDK and RDMA modes].

[id="ocp-4-9-sriov-vhost-net"]
==== SR-IOV support for using vhost-net with Fast Datapath DPDK applications

SR-IOV now supports vhost-net for use with Fast Datapath DPDK applications on Intel and Mellanox NICs. You can enable this feature by configuring the `SriovNetworkNodePolicy` resource. For more information, see xref:../networking/hardware_networks/configuring-sriov-device.adoc#nw-sriov-networknodepolicy-object_configuring-sriov-device[SR-IOV network node configuration object].

[id="ocp-4-9-sriov-single-node"]
==== SR-IOV support for single node clusters

Single node clusters support SR-IOV hardware and the SR-IOV Network Operator. Be aware that configuring an SR-IOV network device causes the single node to reboot and that you must configure the `disableDrain` field for the Operator. For more information, see xref:../networking/hardware_networks/configuring-sriov-operator.adoc#configuring-sriov-operator[Configuring the SR-IOV Network Operator].

[id="ocp-4-9-networking-metallb"]
==== MetalLB load balancer

This release introduces the MetalLB Operator. After installing and configuring the MetalLB Operator, you can deploy MetalLB to provide a native load balancer implementation for services on bare-metal clusters. Other on-premise infrastructures that are like bare metal can also benefit.

The Operator introduces a custom resource, `AddressPool`. You configure address pools with ranges of IP addresses that MetalLB can assign to services. When you add a service of type `LoadBalancer`, MetalLB assigns an IP address from a pool.

For this release, Red Hat only supports using MetalLB in layer 2 mode.

For more information, see xref:../networking/metallb/about-metallb.adoc#about-metallb[About MetalLB and the MetalLB Operator].

[id="ocp-4-9-networking-CNI-VRF-plug-in"]
==== CNI VRF plug-in is generally available

The CNI VRF plug-in was previously introduced as a Technology Preview feature in {product-title} 4.7 and is now generally available in {product-title} {product-version}.

For more information, see xref:../networking/multiple_networks/assigning-a-secondary-network-to-a-vrf.adoc#cnf-assigning-a-secondary-network-to-a-vrf[Assigning a secondary network to a VRF].

[id="ocp-4-9-nw-timeout-configuration-parameters"]
==== Ingress controller timeout configuration parameters

This release introduces six timeout configurations for the Ingress Controller `tuningOptions` parameter:

* `clientTimeout` specifies how long a connection is held open while waiting for a client response.

* `serverFinTimeout` specifies how long a connection is held open while waiting for the server response to the client that is closing the connection.

* `serverTimeout` specifies how long a connection is held open while waiting for a server response.

* `clientFinTimeout` specifies how long a connection is held open while waiting for the client response to the server closing the connection.

* `tlsInspectDelay` specifies how long the router can hold data to find a matching route.

* `tunnelTimeout` specifies how long a tunnel connection, including WebSocket connections, remains open while the tunnel is idle.

For more information, see xref:../networking/ingress-operator.adoc#nw-ingress-controller-configuration-parameters_configuring-ingress[Ingress controller configuration parameters].

[id="ocp-4-9-nw-mutual-TLS-authentication"]
==== Mutual TLS Authentication

You can now configure the Ingress Controller to enable mutual TLS (mTLS) authentication by setting `spec.clientTLS`. The `clientTLS` field specifies configuration for the Ingress Controller to verify client certificates.

For more information, see xref:../networking/ingress-operator.adoc#nw-mutual-tls-auth_configuring-ingress[Configuring Mutual TLS Authentication].

[id="ocp-4-9-nw-customize-ingress-error-pages"]
==== Customizing HAProxy error code response pages

Cluster administrators can specify a custom HTTP error code response page for either 503, 404, or both error pages.

For more information, see xref:../networking/ingress-operator.adoc#nw-customize-ingress-error-pages_configuring-ingress[Customizing HAProxy error code response pages].

[id="ocp-4-9-nw-provisioningnetworkinterface-optional"]
==== The provisioningNetworkInterface configuration setting is optional

In {product-title} 4.9, the `provisioningNetworkInterface` configuration setting for installer-provisioned clusters is optional. The `provisioningNetworkInterface` configuration setting identifies the NIC name used for the `provisioning` network. In {product-title} 4.9, you can alternatively specify the `bootMACAddress` configuration setting in the `install-config.yml` file, which enables Ironic to identify the IP address for the NIC connected to the `provisioning` network and bind to it. You can also omit the `provisioningInterface` configuration setting in the provisioning custom resource so that the provisioning custom resource uses the `bootMACAddress` configuration setting instead.

[id="ocp-4-9-networking-dns-management-state"]
==== DNS Operator managementState

In {product-title} {product-version}, you can now change the DNS Operator `managementState`. The `managementState` of the DNS Operator is set to `Managed` by default, which means that the DNS Operator is actively managing its resources. You can change it to `Unmanaged`, which means the DNS Operator is not managing its resources.

The following are use cases for changing the DNS Operator `managementState`:

* You are a developer and want to test a configuration change to see if it fixes an issue in CoreDNS. You can stop the DNS Operator from overwriting the change by setting the `managementState` to `Unmanaged`.

* You are a cluster administrator and have reported an issue with CoreDNS, but need to apply a workaround until the issue is fixed. You can set the `managementState` field of the DNS Operator to `Unmanaged` to apply the workaround.

For more information, see xref:../networking/dns-operator.adoc#nw-dns-operator-managementState_dns-operator[Changing the DNS Operator managementState].

[id="ocp-4-9-networking-openshift-on-openstack-cloud-provider-options"]
==== Load balancer configuration as a cloud provider option for clusters on {rh-openstack}

For clusters that run on {rh-openstack}, you can now configure Octavia for load balancing as a cloud provider option.

For more information, see xref:../installing/installing_openstack/installing-openstack-installer-custom.adoc#installation-osp-setting-cloud-provider-options_installing-openstack-installer-custom[Setting cloud provider options].

[id="ocp-4-9-nw-tls-profile"]
==== Support added for TLS 1.3 and the Modern profile

This release adds Ingress Controller support for TLS 1.3 and the `Modern` profile in HAProxy.

For more information, see xref:../networking/ingress-operator.adoc#configuring-ingress-controller-tls[Ingress Controller TLS security profiles].

[id="ocp-4-9-networking-hsts"]
==== Global admission plug-in for HTTP Strict Transport Security requirements

Cluster administrators can configure HTTP Strict Transport Security (HSTS) verification on a per-domain basis with the addition of an admission plug-in for the router, called `route.openshift.io/RequiredRouteAnnotations`. If a cluster administrator configures this plug-in to enforce HSTS, then any newly created route must be configured with a compliant HSTS Policy, which is verified against the global setting on the cluster Ingress configuration, called `ingresses.config.openshift.io/cluster`.

For more information, see xref:../networking/routes/route-configuration.adoc#nw-enabling-hsts_route-configuration[HTTP Strict Transport Security].

[id="ocp-4-9-networking-ingress-empty-requests-policy"]
==== Ingress empty requests policy

In {product-title} {product-version} you can now configure the Ingress Controller to log or ignore empty requests by setting the `logEmptyRequests` and `HTTPEmptyRequestsPolicy` fields.

For more information, see xref:../networking/ingress-operator.adoc#nw-ingress-controller-configuration-parameters_configuring-ingress[Ingress controller configuration parameters].

[id="ocp-4-9-storage"]
=== Storage

[id="ocp-4-9-storage-aws-ebs-csi-ga"]
==== Persistent storage using AWS EBS CSI driver operator is generally available
{product-title} is capable of provisioning persistent volumes (PVs) using the Container Storage Interface (CSI) driver for AWS Elastic Block Store (EBS).
This feature was previously introduced as a Technology Preview feature in {product-title} 4.5 and is now generally available and enabled by default in {product-title} 4.9.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-ebs.adoc#persistent-storage-csi-ebs[AWS EBS CSI Driver Operator].

[id="ocp-4-9-storage-azure-stack-hub-csi"]
==== Persistent storage using the Azure Stack Hub CSI Driver Operator (general availability)
{product-title} is capable of provisioning PVs using the CSI driver for Azure Stack Hub Storage.
Azure Stack Hub, which is part of the Azure Stack portfolio, allows you to run apps in an on-premises environment and deliver Azure services in your datacenter.
The Azure Stack Hub CSI Driver Operator that manages this driver is new for 4.9 and generally available.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-azure-stack-hub.adoc#persistent-storage-csi-azure-stack-hub[Azure Stack Hub CSI Driver Operator].

[id="ocp-4-9-storage-aws-efs-csi"]
==== Persistent storage using the AWS EFS CSI Driver Operator (Technology Preview)
{product-title} is capable of provisioning PVs using the CSI driver for AWS Elastic File Service (EFS).
The AWS EFS CSI Driver Operator that manages this driver is in Technology Preview.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-aws-efs.adoc#persistent-storage-csi-aws-efs[AWS EFS CSI Driver Operator].

[id="ocp-4-9-storage-auto-migration-csi-gce"]
==== Automatic CSI migration supports GCE (Technology Preview)
Starting with {product-title} 4.8, automatic migration for in-tree volume plug-ins to their equivalent CSI drivers became available as a Technology Preview feature.
This feature now supports automatic migration from Google Compute Engine Persistent Disk (GCE PD) in-tree plug-in to the Google Cloud Platform (GCP) Persistent Disk CSI driver.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-migration.adoc#persistent-storage-csi-migration[CSI Automatic Migration].

[id="ocp-4-9-storage-auto-migration-csi-azure-disk"]
==== Automatic CSI migration supports Azure Disk (Technology Preview)
Starting with {product-title} 4.8, automatic migration for in-tree volume plug-ins to their equivalent CSI drivers became available as a Technology Preview feature.
This feature now supports automatic migration from the Azure Disk in-tree plug-in to the Azure Disk CSI driver.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-migration.adoc#persistent-storage-csi-migration[CSI Automatic Migration].

[id="ocp-4-9-vsphere-csi-new-features"]
==== VMWare vSphere CSI Driver Operator creates storage policy automatically (Technology Preview)
The vSphere CSI Operator Driver storage class now uses vSphere’s storage policy. {product-title} automatically creates a storage policy that targets datastore configured in cloud configuration.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-migration.adoc#persistent-storage-csi-vsphere[VMWare vSphere CSI Driver Operator].

[id="ocp-4-9-lso-new-metrics"]
==== New metrics provided for Local Storage Operator
{product-title} 4.9 provides the following new metrics for the Local Storage Operator:

* `lso_discovery_disk_count`: total number of discovered devices on each node

* `lso_lvset_provisioned_PV_count`: total number of PVs created by `LocalVolumeSet` objects

* `lso_lvset_unmatched_disk_count`: total number of disks that Local Storage Operator did not select for provisioning because of mismatching criteria

* `lso_lvset_orphaned_symlink_count`: number of devices with PVs that no longer match `LocalVolumeSet` object criteria

* `lso_lv_orphaned_symlink_count`: number of devices with PVs that no longer match `LocalVolume` object criteria

* `lso_lv_provisioned_PV_count`: total number of provisioned PVs for `LocalVolume`

For more information, see xref:../storage/persistent_storage/persistent-storage-local.adoc#persistent-storage-local[Persistent storage using local volumes].

[id="ocp-4-9-registry"]
=== Registry

[id="ocp-4-9-olm"]
=== Operator lifecycle

The following new features and enhancements relate to running Operators with Operator Lifecycle Manager (OLM).

[id="ocp-4-9-olm-k8s-1-22"]
==== Operator Lifecycle Manager (OLM) upgraded to Kubernetes 1.22

Starting in {product-title} {product-version}, Operator Lifecycle Manager (OLM) supports Kubernetes 1.22. As a result, xref:../release_notes/ocp-4-9-release-notes.adoc#ocp-4-9-removed-kube-1-22-apis[a significant number of `v1beta1` APIs have been removed and updated to `v1`]. Operators that depend on the removed `v1beta1` APIs will not run on {product-title} {product-version}. Cluster administrators should xref:../operators/admin/olm-upgrading-operators.adoc[upgrade their installed Operators] to the `latest` channel before upgrading a cluster to {product-title} {product-version}.

[IMPORTANT]
====
Kubernetes 1.22 introduces link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#customresourcedefinition-v122[several notable changes] to `v1` of the `CustomResorceDefinition` API.
====

[id="ocp-4-9-fb-catalogs"]
==== File-based catalogs

File-based catalogs are the latest iteration of the catalog format in Operator Lifecycle Manager (OLM). The format is a plain text-based (JSON or YAML) and declarative config evolution of the earlier, and now deprecated, xref:../release_notes/ocp-4-9-release-notes.adoc#ocp-4-9-sqlite-catalogs-deprecated[SQLite database format], and it is fully backwards compatible. The goal of this format is to enable Operator catalog editing, composability, and extensibility.

For more information about the file-based catalog specification, see xref:../operators/understanding/olm-packaging-format.adoc#olm-file-based-catalogs_olm-packaging-format[Operator Framework packaging format].

For instructions about creating file-based catalogs by using the `opm` CLI, see xref:../operators/admin/olm-managing-custom-catalogs.adoc#olm-creating-fb-catalog-image_olm-managing-custom-catalogs[Managing custom catalogs].

[id="ocp-4-9-olm-operand-deletion"]
==== Removing Operands when uninstalling an Operator using the web console

When you uninstall an Operator by using the web console, you can now remove Operands managed by the Operator automatically, including custom resources (CRs). For more information, see
xref:../operators/admin/olm-deleting-operators-from-cluster.adoc#olm-deleting-operators-from-a-cluster-using-web-console_olm-deleting-operators-from-a-cluster[Deleting Operators from a cluster using the web console].

[id="ocp-4-9-olm-admin-error-reporting"]
==== Enhanced error reporting for cluster administrators
Because administrators should not require an understanding of the interaction process between the various low-level APIs or access to the Operator Lifecycle Manager (OLM) pod logs to successfully debug such issues, {product-title} {product-version} introduces the following enhancements in OLM to provide administrators with more comprehensible error reporting and messages:

[id="ocp-4-9-olm-operator-group-status-conditions"]
===== Updating Operator group status conditions
Previously, if a namespace contained multiple Operator groups or could not find a service account, the status of the Operator group would not report an error. With this enhancement, these scenarios now update the status condition of the Operator group to report an error.

[id="ocp-4-9-olm-subscription-conditions"]
===== Indicating the reason for install plan failures
Before this release, if an install plan failed, the subscription condition would not state why the failure occurred. Now, if an install plan fails, the subscription status condition indicates the reason for the failure.

[id="ocp-4-9-olm-subscription-resolution-errors"]
===== Indicating resolution conflicts on subscription statuses
Because dependency resolution treats all components in a namespace as a single unit, if a resolution failure occurs, all subscriptions on the namespace now indicate the error.

[id="ocp-4-9-catalog-image-template"]
==== Image template for custom catalog sources

To avoid cluster upgrades potentially leaving Operator installations in an unsupported state or without a continued update path, you can enable automatically changing your Operator catalog's index image version as part of cluster upgrades.

Set the `olm.catalogImageTemplate` annotation to your catalog image name and use one or more of the Kubernetes cluster version variables when constructing the template for the image tag.

For more information, see xref:../operators/understanding/olm/olm-understanding-olm.adoc#olm-catalogsource-image-template_olm-understanding-olm[Image template for custom catalog sources].

[id="ocp-4-9-osdk"]
=== Operator development

The following new features and enhancements relate to developing Operators with the Operator SDK.

[id="ocp-4-9-osdk-ha-sno"]
==== High-availability or single node cluster detection and support

An {product-title} cluster can be configured in high-availability (HA) mode, which uses multiple nodes, or in non-HA mode, which uses a single node. A single node cluster, also known as Single Node OpenShift (SNO), is likely to have more conservative resource constraints. Therefore, it is important that Operators installed on a single node cluster can adjust accordingly and still run well.

By accessing the cluster high-availability mode API provided in {product-title}, Operator authors can use the Operator SDK to enable their Operator to detect a cluster's infrastructure topology, either HA or non-HA mode. Custom Operator logic can be developed that uses the detected cluster topology to automatically switch the resource requirements, both for the Operator and for any Operands or workloads it manages, to a profile that best fits the topology.

For more information, see xref:../operators/operator_sdk/osdk-ha-sno.adoc#osdk-ha-sno[High-availability or single node cluster detection and support].

[id="ocp-4-9-builds"]
=== Builds

As a developer using {product-title} for builds, with this update, you can use the following new capabilities:

* You can mount build volumes to give running builds access to information that you do not want to persist in the output container image. Build volumes can provide sensitive information, such as repository credentials, which the build environment or configuration only needs at build-time. Build volumes are different from build inputs, whose data can persist in the output container image.
//For more information, see xref:TBD[Using Build Volumes]

* You can configure image changes to trigger builds based on information recorded in the BuildConfig status. This way, you can use `ImageChange` triggers with builds in a GitOps workflow.
//For more information, see xref:TBD[Using ImageChanges to trigger builds]

[id="ocp-4-9-images"]
=== Images

[id="ocp-4-9-wildcard-domain"]
==== Wildcard domains as registry sources

This release introduces support for using wildcard domains as registry sources in your image registry settings. With a wildcard domain, such as `*.example.com`, you can set your cluster to push and pull images from multiple subdomains without having to manually enter each one. For more information, see xref:../openshift_images/image-configuration.adoc#images-configuration-parameters_image-configuration[Image controller configuration parameters].

[id="ocp-4-9-machine-api"]
=== Machine API

[id="ocp-4-9-machine-api-rhel8"]
==== {op-system-base-full} 8 now supported for compute machines

Starting in {product-title} {product-version}, you can now use {op-system-base-full} 8.4 for compute machines. Previously, {op-system-base} 8 was not supported for compute machines.

You cannot upgrade {op-system-base} 7 compute machines to {op-system-base} 8. You must deploy new {op-system-base} 8 hosts, and the old {op-system-base} 7 hosts should be removed.

[id="ocp-4-9-nodes"]
=== Nodes

[id="ocp-4-9-nodes-scheduler-profiles"]
==== Scheduler profiles GA

Scheduling pods using a scheduler profile is now generally available. This is a replacement for configuring a scheduler policy. The following scheduler profiles are available:

* `LowNodeUtilization`: This profile attempts to spread pods evenly across nodes to get low resource usage per node.

* `HighNodeUtilization`: This profile attempts to place as many pods as possible onto as few nodes as possible, to minimize node count with high usage per node.

* `NoScoring`: This is a low-latency profile that strives for the quickest scheduling cycle by disabling all score plug-ins. This might sacrifice better scheduling decisions for faster ones.

For more information, see xref:../nodes/scheduling/nodes-scheduler-profiles.adoc#nodes-scheduler-profiles[Scheduling pods using a scheduler profile].

[id="ocp-4-9-nodes-descheduler"]
==== New descheduler profiles and customization

The following descheduler profiles are now available:

* `SoftTopologyAndDuplicates`: This profile is the same as `TopologyAndDuplicates`, except that pods with soft topology constraints, such as `whenUnsatisfiable: ScheduleAnyway`, are also considered for eviction.
* `EvictPodsWithLocalStorage`: This profile allows pods with local storage to be eligible for eviction.
* `EvictPodsWithPVC`: This profile allows pods with persistent volume claims to be eligible for eviction.

You can also customize the pod lifetime value for the `LifecycleAndUtilization` profile.

For more information, see xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler[Evicting pods using the descheduler].

[id="ocp-4-9-nodes-multiple-logins"]
==== Multiple logins to the same registry

When configuring the `docker/config.json` file to allow pods to pull images from private registries, you can now list specific repositories in the same registry, each with credentials specific to that registry path. Previously, you could list only one repository from a given registry. You can also now define a registry with a specific namespace.

[id="ocp-4-9-nodes-enhanced-monitoring"]
==== Enhanced monitoring of node resources

Node-related metrics and alerts have been enhanced to give you an earlier indication of when the stability of a node is compromised.

[id="ocp-4-9-node-health-check-operator"]
==== Deploy node health checks with the Node Health Check Operator (Technology Preview)

You can use the Node Health Check Operator to deploy the `NodeHealthCheck` controller. The controller identifies unhealthy nodes and uses the Poison Pill Operator to remediate the unhealthy nodes.

[id="ocp-4-9-logging"]
=== Red Hat OpenShift Logging

In {product-title} 4.7, _Cluster Logging_ became _Red Hat OpenShift Logging_. For more information, see xref:../logging/cluster-logging-release-notes.adoc[Release notes for Red Hat OpenShift Logging].

[id="ocp-4-9-monitoring"]
=== Monitoring

[id="ocp-4-9-metering"]
=== Metering

The Metering Operator is deprecated as of {product-title} 4.6, and is scheduled to be removed in {product-title} 4.9.

[id="ocp-4-9-scale"]
=== Scale

[id="ocp-4-9-scale-special-resource-operator"]
==== Special Resource Operator (Technology Preview)
You can now use the Special Resource Operator (SRO) to help manage the deployment of kernel modules and drivers on an existing {product-title} cluster. This is currently a Technology Preview feature.

For more information, see xref:../hardware_enablement/psap-special-resource-operator.adoc#about-special-resource-operator_special-resource-operator[About the Special Resource Operator].

[id="ocp-4-9-scale-memory-manager-in-beta-version"]
==== Memory Manager feature (Technology Preview)
The Memory Manager feature is now enabled by default for all pods running on the node that is configured with one of the following Topology Manager policies:

* `single-numa-node`
* `restricted`

For more information, see xref:../scalability_and_performance/using-topology-manager.adoc#topology_manager_policies_using-topology-manager[Topology Manager policies].

[id="ocp-4-9-scale-latency-tools-for-latency-testing"]
==== Additional tools for latency testing

{product-title} {product-version} introduces two additional tools to measure system latency:

* `hwladetect` measures the baseline that the bare hardware can achieve
* `cyclictest` schedules a repeated timer after `hwlatdetect` passes validation and measures the difference between the desired and the actual trigger times

For more information, see xref:../scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc#cnf-performing-end-to-end-tests-running-the-tests_cnf-master[Running the latency tests].

[id="ocp-4-9-scale-cluster-maximums"]
==== Cluster maximums

Updated guidance around xref:../scalability_and_performance/planning-your-environment-according-to-object-maximums.adoc#planning-your-environment-according-to-object-maximums[cluster maximums] for {product-title} {product-version} is now available.

[IMPORTANT]
====
No large scale testing for performance against OVN-Kubernetes testing was executed for this release.
====

Use the link:https://access.redhat.com/labs/ocplimitscalculator/[{product-title} Limit Calculator] to estimate cluster limits for your environment.

[id="ocp-4-9-backup-and-restore"]
=== Backup and restore

[id="ocp-4-9-dev-exp"]
=== Developer experience

[id="ocp-4-9-insights-operator"]
=== Insights Operator

[id="ocp-4-9-insights-operator-sca"]
==== Importing RHEL Simple Content Access certificates (Technology Preview)

In {product-title} 4.9, Insights Operator can import RHEL Simple Content Access (SCA) certificates from {cloud-redhat-com}.

For more information, see xref:../support/remote_health_monitoring/insights-operator-simple-access.adoc[_Importing RHEL Simple Content Access certificates with Insights Operator_].

[id="ocp-4-9-insights-operator-data-collection-enhancements"]
==== Insights Operator data collection enhancements

In {product-title} 4.9, the Insights Operator collects the following additional information:

* All of the `MachineConfig` resource definitions from a cluster.
* The names of the `PodSecurityPolicies` installed in a cluster.
* If installed, the `ClusterLogging` resource definition.
* If the `SamplesImagestreamImportFailing` alert is firing, then the `ImageStream` definitions and the last 100 lines of container logs from the `openshift-cluster-samples-operator` namespace.

With this additional information, Red Hat can provide improved remediation steps in Insights Advisor.

[id="ocp-4-9-auth"]
=== Authentication and authorization

[id="ocp-4-9-cco-ash-support"]
==== Support for Microsoft Azure Stack Hub with Cloud Credential Operator in manual mode

With this release, installations on Microsoft Azure Stack Hub can be performed by configuring the Cloud Credential Operator (CCO) in manual mode.

For more information, see xref:../authentication/managing_cloud_provider_credentials/cco-mode-manual.adoc[Using manual mode].

[id="ocp-4-9-sandboxed-containers"]
=== {sandboxed-containers-first}

[id="ocp-4-9-notable-technical-changes"]
== Notable technical changes

{product-title} 4.9 introduces the following notable technical changes.

// Note: use [discrete] for these sub-headings.

[discrete]
[id="ocp-4-9-security-etcd-auto-defrag"]
==== Automatic defragmentation for etcd data
In {product-title} 4.9, etcd data is automatically defragmented by the etcd Operator.

[discrete]
[id="octavia-ovn-nodeport-changes"]
==== Octavia OVN NodePort changes
Previously, on {rh-openstack-first} deployments, opening traffic on NodePorts was constrained to the CIDR of the node's subnet. In order to support LoadBalancer services using the Octavia Open Virtual Network (OVN) provider, the security group rules that allow NodePort traffic to master and worker nodes are now changed to open `0.0.0.0/0`.

[discrete]
[id="osp-loadbalancer-configuration-changes"]
==== OpenStack Platform LoadBalancer configuration changes
The {rh-openstack-first} cloud provider LoadBalancer configuration now defaults to `use-octavia=True`. An exception to this rule is a deployment with Kuryr, in which case `use-octavia` is set to `false`, because Kuryr handles LoadBalancer services on its own.

[discrete]
[id="ocp-4-9-haproxy-2.2.15-upgrade"]
==== Ingress Controller upgraded to HAProxy 2.2.15

The {product-title} Ingress Controller is upgraded to HAProxy version 2.2.15.

[discrete]
[id="ocp-4-9-coreDNS-version-update"]
==== CoreDNS update to version 1.8.4

In {product-title} {product-version}, CoreDNS uses version 1.8.4, which includes bug fixes.

[discrete]
[id="ocp-4-9-cluster-cloud-controller-manager-operator"]
==== Implementation of cloud controller managers for cloud providers

The Kubernetes controller manager that manages cloud provider deployments does not include support for Azure Stack Hub as a provider. Because using cloud controller managers is the preferred method for interacting with underlying cloud platforms, there is no plan to add this support. As a result, the Azure Stack Hub implementation in {product-title} uses cloud controller managers.

In addition, this release supports using cloud controller managers for Amazon Web Services (AWS), Microsoft Azure, and {rh-openstack-first} as a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]. Any new cloud platform support that is added to {product-title} will also use cloud controller managers.

To learn more about the cloud controller manager, see the link:https://kubernetes.io/docs/concepts/architecture/cloud-controller/[Kubernetes documentation on this component].

To manage the cloud controller manager and cloud node manager deployments and lifecycles, this release introduces the Cluster Cloud Controller Manager Operator.

For more information, see the xref:../operators/operator-reference.adoc#cluster-cloud-controller-manager-operator_red-hat-operators[Cluster Cloud Controller Manager Operator] entry in the _Red Hat Operators reference_.

[discrete]
[id="ocp-4-9-canary-rollout-update-changes"]
==== Performing a canary rollout update

With {product-title} 4.9, a new process to perform a canary rollout update has been introduced. For a detailed overview of this process, see xref:../updating/update-using-custom-machine-config-pools.adoc#update-using-custom-machine-config-pools[Performing a canary rollout update].

[discrete]
[id="ocp-4-9-operator-sdk-v-1-10-1"]
==== Operator SDK v1.10.1

{product-title} 4.9 supports Operator SDK v1.10.1. See xref:../cli_reference/osdk/cli-osdk-install.adoc#cli-osdk-install[Installing the Operator SDK CLI] to install or update to this latest version.

[NOTE]
====
Operator SDK v1.10.1 supports Kubernetes 1.21.
====

If you have any Operator projects that were previously created or maintained with Operator SDK v1.8.0, see xref:../operators/operator_sdk/osdk-upgrading-projects.adoc#osdk-upgrading-projects[Upgrading projects for newer Operator SDK versions] to ensure your projects are upgraded to maintain compatibility with Operator SDK v1.10.1.

[id="ocp-4-9-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more fine-grained functionality that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *TP*: _Technology Preview_
* *DEP*: _Deprecated_
* *REM*: _Removed_

//TODO: remove anything that has been REM since 4.6

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.7 |OCP 4.8 |OCP 4.9

|Package manifest format (Operator Framework)
|DEP
|REM
|REM

|SQLite database format for Operator catalogs
|GA
|GA
|DEP

|`oc adm catalog build`
|DEP
|REM
|REM

|`--filter-by-os` flag for `oc adm catalog mirror`
|DEP
|REM
|REM

|v1beta1 CRDs
|DEP
|DEP
|REM

|Docker Registry v1 API
|DEP
|DEP
|REM

|Metering Operator
|DEP
|DEP
|REM

|Scheduler policy
|DEP
|DEP
|

|`ImageChangesInProgress` condition for Cluster Samples Operator
|DEP
|DEP
|

|`MigrationInProgress` condition for Cluster Samples Operator
|DEP
|DEP
|

|Use of `v1` in `apiVersion` for {product-title} resources
|DEP
|DEP
|

|Use of `dhclient` in {op-system}
|DEP
|DEP
|REM

|Cluster Loader
|GA
|DEP
|

|Bring your own RHEL 7 compute machines
|DEP
|DEP
|DEP

|`lastTriggeredImageID` field in the `BuildConfig` spec for Builds
|GA
|DEP
|REM

|Jenkins Operator
|TP
|DEP
|DEP

|HPA custom metrics adapter based on Prometheus
|TP
|REM
|

|vSphere 6.7 Update 2 or earlier and virtual hardware version 13
|GA
|GA
|DEP

|====

[id="ocp-4-9-deprecated-features"]
=== Deprecated features

[id="ocp-4-9-sqlite-catalogs-deprecated"]
==== SQLite database format for Operator catalogs

The SQLite database format used by Operator Lifecycle Manager (OLM) for catalogs and index images has been deprecated, including the related `opm` CLI commands. Cluster administrators and catalog maintainers are encouraged to familiarize themselves with the new xref:../release_notes/ocp-4-9-release-notes.adoc#ocp-4-9-fb-catalogs[file-based catalog format] introduced in {product-title} 4.9 and begin migrating catalog workflows.

[NOTE]
====
The default xref:../operators/understanding/olm-rh-catalogs.adoc#olm-rh-catalogs[Red Hat-provided Operator catalogs] for {product-title} 4.6 and later are currently still shipped in the SQLite database format.
====

[id="ocp-4-9-removed-features"]
=== Removed features

[id="ocp-4-9-removed-metering"]
==== Metering
This release removes the {product-title} Metering Operator feature.

[id="ocp-4-9-removed-kube-1-22-apis"]
==== Beta APIs removed from Kubernetes 1.22

Kubernetes 1.22 removed the following deprecated `v1beta1` APIs. Migrate manifests and API clients to use the `v1` API version. For more information about migrating removed APIs, see the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22[Kubernetes documentation].

.`v1beta1` APIs removed from Kubernetes 1.22
[cols="2,2,1",options="header",]
|===
|Resource |API |Notable changes

|`APIService`
|`apiregistration.k8s.io/v1beta1`
|No

|`CertificateSigningRequest`
|`certificates.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#certificatesigningrequest-v122[Yes]

|`ClusterRole`
|`rbac.authorization.k8s.io/v1beta1`
|No

|`ClusterRoleBinding`
|`rbac.authorization.k8s.io/v1beta1`
|No

|`CSIDriver`
|`storage.k8s.io/v1beta1`
|No

|`CSINode`
|`storage.k8s.io/v1beta1`
|No

|`CustomResourceDefinition`
|`apiextensions.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#customresourcedefinition-v122[Yes]

|`Ingress`
|`extensions/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#ingress-v122[Yes]

|`Ingress`
|`networking.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#ingress-v122[Yes]

|`IngressClass`
|`networking.k8s.io/v1beta1`
|No

|`Lease`
|`coordination.k8s.io/v1beta1`
|No

|`LocalSubjectAccessReview`
|`authorization.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#subjectaccessreview-resources-v122[Yes]

|`MutatingWebhookConfiguration`
|`admissionregistration.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#webhook-resources-v122[Yes]

|`PriorityClass`
|`scheduling.k8s.io/v1beta1`
|No

|`Role`
|`rbac.authorization.k8s.io/v1beta1`
|No

|`RoleBinding`
|`rbac.authorization.k8s.io/v1beta1`
|No

|`SelfSubjectAccessReview`
|`authorization.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#subjectaccessreview-resources-v122[Yes]

|`StorageClass`
|`storage.k8s.io/v1beta1`
|No

|`SubjectAccessReview`
|`authorization.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#subjectaccessreview-resources-v122[Yes]

|`TokenReview`
|`authentication.k8s.io/v1beta1`
|No

|`ValidatingWebhookConfiguration`
|`admissionregistration.k8s.io/v1beta1`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#webhook-resources-v122[Yes]

|`VolumeAttachment`
|`storage.k8s.io/v1beta1`
|No

|===

[id="ocp-4-9-removed-descheduler-v1beta1-api"]
==== Descheduler v1beta1 API removed

The deprecated `v1beta1` API for the descheduler has been removed in {product-title} 4.9. Migrate any resources using the descheduler `v1beta1` API version to `v1`.

[id="ocp-4-9-dhclient-removed"]
==== Use of dhclient in {op-system} removed

The deprecated `dhclient` binary has been removed from {op-system}. Starting with {product-title} 4.6, {op-system} switched to using `NetworkManager` in the `initramfs` to configure networking during early boot. Use the `NetworkManager` internal DHCP client for networking configuration instead. See link:https://bugzilla.redhat.com/show_bug.cgi?id=1908462[*BZ#1908462*] for more information.

[id="ocp-4-9-builds-lasttriggeredimageid-parameter"]
==== Cease updating the `lastTriggeredImageID` field and ignore it

The current release stops updating the `buildConfig.spec.triggers[i].imageChange.lastTriggeredImageID` field when the `ImageStreamTag` referenced by `buildConfig.spec.triggers[i].imageChage` points to a new image. Instead, this release updates the `buildConfig.status.imageChangeTriggers[i].lastTriggeredImageID` field.

Additionally, the Build Image Change Trigger controller ignores the `buildConfig.spec.triggers[i].imageChange.lastTriggeredImageID` field.

Now, the Build Image Change Trigger controller starts a build based on the `buildConfig.status.imageChangeTriggers[i].lastTriggeredImageID` field and how it compares to the image ID now referenced by the `ImageStreamTag` referenced in the `buildConfig.spec.triggers[i].imageChange`.

Therefore, update scripts and jobs that inspect `buildConfig.spec.triggers[i].imageChange.lastTriggeredImageID` accordingly. (link:https://issues.redhat.com/browse/BUILD-190[*BUILD-190*])

[id="ocp-4-9-bug-fixes"]
== Bug fixes

[discrete]
[id="ocp-4-9-api-server-auth-bug-fixes"]
==== API server and authentication

* Previously, encryption conditions could remain indefinitely and be reported as a degraded condition for some Operators. Stale encryption conditions are now cleared properly and no longer improperly reported.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1974520[*BZ#1974520*])

* Previously, the CA for API server client certificates was rotated early in the lifetime of a cluster, which prevented the Authentication Operator from creating a certificate signing request (CSR) because a previous CSR with the same name still existed. The Kubernetes API server was unable to authenticate itself to the OAuth API server when sending `TokenReview` requests, which caused authentication to fail. Generated names are now used when creating CSRs by the Authentication Operator, so an early rotation of the CA for API server client certificates no longer causes authentication failures.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1978193[*BZ#1978193*])

[discrete]
[id="ocp-4-9-assisted-installer-bug-fixes"]
==== Assisted installer

[discrete]
[id="ocp-4-9-bare-metal-hardware-bug-fixes"]
==== Bare Metal Hardware Provisioning

[discrete]
[id="ocp-4-9-builds-bug-fixes"]
==== Builds

* In {product-title} and later, the fix for bug link:https://bugzilla.redhat.com/show_bug.cgi?id=1884270[BZ#1884270] incorrectly pruned SSH protocol URLs in an attempt to provide SCP-styled URL capabilities. This error caused the `oc new-build` command not to pick an automatic source clone secret: the build could not use the `build.openshift.io/sbuild.openshift.io/source-secret-match-uri-1ource-secret-match-uri-1` annotation to map SSH keys with the associated secrets, and therefore could not perform git cloning. This update reverts the changes from BZ#1884270 so that builds can use the annotation and perform git cloning.

[discrete]
[id="ocp-4-9-cloud-compute-bug-fixes"]
==== Cloud Compute

* Previously, when a root volume was created for a new server, and that server was not successfully created, the automatic deletion for the volume was not triggered because there was no deletion of a server associated with the volume. In some situations, this led to the creation of many extra volumes, and caused errors if the volume quota was reached. With this release, newly created root volumes are deleted when server creation call fails.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1943378[*BZ#1943378*])

* Previously, when using the default value for `instanceType`, the Machine API created `m4.large` instances on AWS. This is different than the `m5.large` instance type for machines that are created by the {product-title} installer. With this release, the Machine API creates `m5.large` instances for new machines on AWS when the default value is specified.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1953063[*BZ#1953063*])

* Previously, the Machine API Operator constantly attached new targets even if they were already attached. The excessive calls to the AWS API resulted in a high number of errors. With this release, the Operator checks whether a load balancer attachment is required before attempting the attachment process. This change reduces the frequency of failed API requests.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1965080[*BZ#1965080*])

* Previously, when using automatic pinning for a VM, the name of the property was `disabled`, `existing`, or `adjust`. With this release, the name better describes each policy, and `existing` was removed because it is blocked on oVirt. The new property names are `none` and `resize_and_pin`, which align with the oVirt user interface.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1972747[*BZ#1972747*])

* Previously, the cluster autoscaler was unable to access the `csidrivers.storage.k8s.io` or `csistoragecapacities.storage.k8s.io` resources, which resulted in permissions errors. This fix updates the role assigned to the cluster autoscaler so that it includes permissions for these resources.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1973567[*BZ#1973567*])

* Previously, it was possible to delete a machine with a node that has been deleted. This caused the machine to remain in a deleting phase indefinitely. This fix allows you to delete machines in this state properly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1977369[*BZ#1977369*])

* When using `boot-from-volume` image, creating a new instance leaks volumes if the machine controller is rebooted. This caused the previously created volume to never be cleaned up. This fix ensures that the volume created previously is either pruned or reused.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1983612[*BZ#1983612*])

* Previously, when deployed on {rh-openstack-first} with a combination of proxy and custom CA certificate, a cluster would not become fully operational. This fix passes the proxy settings to the HTTP transport used when connecting with a custom CA certificate, ensuring that all cluster components work as expected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1986540[*BZ#1986540*])

[discrete]
[id="ocp-4-9-cloud-credential-operator-bug-fixes"]
==== Cloud Credential Operator

[discrete]
[id="ocp-4-9-cluster-version-operator-bug-fixes"]
==== Cluster Version Operator

* Previously, the Cluster Version Operator (CVO) did not respect the `noProxy` property in the proxy configuration resource. As a result, the CVO was denied access to update recommendations or release signatures when only unproxied connections completed. Now, when the proxy resource requests direct, unproxied access, the CVO reaches the upstream update service and signature stores directly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1978749[*BZ#1978749*])

* Previously, the Cluster Version Operator (CVO) loaded its proxy configuration from proxy resource specification properties instead of from status properties that were verified by the Network Operator. As a result, any incorrectly configured values would prevent the CVO from reaching the upstream update service or signature stores. Now, the CVO loads its proxy configuration only from the verified status properties.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1978774[*BZ#1978774*])

* Previously, the Cluster Version Operator (CVO) did not remove volume mounts that were added outside of the manifest. As a result, pod creation could fail during a volume failure. Now, CVO removes all volume mounts that do not appear in the manifest.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2004568[*BZ#2004568*])

[discrete]
[id="ocp-4-9-compliance-operator-bug-fixes"]
==== Compliance Operator

[discrete]
[id="ocp-4-9-console-kubevirt-bug-fixes"]
==== Console KubeVirt Plug-in

* This update simplifies the *Template provider* menu in the {product-title} *Virtualization* console.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1952737[*BZ#1952737*])

* Previously, the KubeVirt plugin used the v1 API version. As a result, older versions of KubeVirt did not create a virtual machine because of the API mismatch. This update checks the API version available and uses the correct version.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1977037[*BZ#1977037*]), (link:https://bugzilla.redhat.com/show_bug.cgi?id=1979114[*BZ#1979114*])

* Previously, templates based on {op-system-base-full} were promoted by default. As a result, {op-system-base} 6 was promoted. This update fixes that issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1978200[*BZ#1978200*])

* This update fixes an issue where {op-system-base-full} 6 was labeled as a *Community* provided template.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1978202[*BZ#1978202*])

* Previously, the web console could not retrieve some information about a virtual machine such as the time zone and number of active users. The web console is updated to retrieve more information from virtual machines.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1979190[*BZ#1979190*])

[discrete]
[id="ocp-4-9-console-storage-bug-fixes"]
==== Console Storage Plug-in

* Previously, when working with Ceph storage, the Console Storage Plug-in unnecessarily included a redundant use of a namespace parameter. This bug had no customer-visible impact; however, the plug-in has been updated to avoid the redundant use of the namespace. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1982682[*BZ#1982682*])

[discrete]
[id="ocp-4-9-dns-bug-fixes"]
==== DNS

[discrete]
[id="ocp-4-9-etcd-bug-fixes"]
==== etcd

[discrete]
[id="ocp-4-9-image-registry-bug-fixes"]
==== Image Registry

* The Operator to check if the registry should use custom tolerations was checking `spec.nodeSelector` instead of `spec.tolerations`. The custom tolerations from `spec.tolerations` are applied only when `spec.nodeSelector` is set. This fix uses the field `spec.tolerations` to check for the presence of custom tolerations. Now, the Operator uses custom tolerations if `spec.tolerations` are set.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1973318[*BZ#1973318*])

* The `spec.managementState` in `configs.imageregistry` is set to `Removed`, which caused the image pruner pod to generate warnings about deprecated CronJob in `v1.21` and later, and that `batch/v1` should be used. This fix updates `batch/v1beta1` with `batch/v1` in {product-title} `oc`. Now, warnings about the deprecated CronJob in image pruner pods no longer appear.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1976112[*BZ#1976112*])

[discrete]
[id="ocp-4-9-image-streams-bug-fixes"]
==== Image Streams

[discrete]
[id="ocp-4-9-insights-operator-bug-fixes"]
==== Insights Operator

[discrete]
[id="ocp-4-9-installer-bug-fixes"]
==== Installer

* Previously, the network interface on Azure control plane nodes was missing a hyphen in the interface name. This was inconsistent compared to other platforms, which caused issues. The missing hyphen has been added. Now all control plane nodes are named the same, regardless of the platform. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1882490[*BZ#1882490*])

* You can now configure the `autoPinningPolicy` and `hugepages` fields in the `install-config.yaml` file for oVirt. The `autoPinningPolicy` field allows you to automatically set the non-uniform memory access (NUMA) pinning settings and CPU topology changes for the cluster. The `hugepages` field allows you to set the Hugepages of the hypervisor. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1925203[*BZ#1925203*])

* Previously, the installation program did not output any errors when the Ed25519 SSH key type was used with FIPS enabled, even though it could not be used. Now the installation program validates SSH key types, outputting an error when an SSH key type is not supported with FIPS enabled; only RSA and ECDSA SSH key types are allowed when FIPS is enabled. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1962414[*BZ#1962414*])

* In certain conditions, {rh-openstack-first} network trunks do not contain a tag to indicate that the trunk belongs to the cluster. Consequently, cluster deletion missed the trunk ports and got stuck in a loop until they timed out. Deleting the cluster now deletes trunks for which the tagged port is a parent.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1971518[*BZ#1971518*])

* Previously, when uninstalling a cluster on {rh-openstack-first}, the installer used an inefficient algorithm to delete resources. The inefficient algorithm caused the uninstall process to require more time than was necessary. The installer is updated with a more efficient algorithm that should uninstall the cluster more quickly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1974598[*BZ#1974598*])

* Previously, if the `AWS_SHARED_CREDENTIALS_FILE` environment variable was set to an empty file, the installer prompted for credentials and then created a `aws/credentials` file, ignoring the value of the environment variable and possibly overwriting existing credentials. With this fix, the installer is updated to store credentials in the specified file. If the specified file has invalid credentials, the installer produces an error instead of overwriting the file and risking information loss. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1974640[*BZ#1974640*])

* Previously, users encountered a vague error message when they deleted a cluster on Azure that shared resources with another cluster, making it difficult to understand why the deletion failed. This update adds an error message that explains why the failure occurs. (https://bugzilla.redhat.com/show_bug.cgi?id=1976016[*BZ#1976016*])

* Previously, because of a typo, Kuryr deployments were being checked against the wrong requirements, meaning that installations with Kuryr could succeed even if they did not meet the minimum requirements for Kuryr. This fix eliminates the error, allowing the installer to check the right requirements. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1978213[*BZ#1978213*])

* Before this update, the ingress checks for `keepalived` did not include fall and raise directives, which meant that a single failed check could cause an ingress virtual IP failover. This bug fix introduces fall and raise directives to prevent such failovers. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1982766[*BZ#1982766*])

[discrete]
[id="ocp-4-9-kube-api-server-bug-fixes"]
==== Kubernetes API server

* Previously, when a deployment and image stream were created at the same time, a race condition could occur which caused the deployment controller to create replica sets in an infinite loop. The responsibilities of the API server's image policy plug-in were lowered and concurrent creation of a deployment and image stream no longer causes infinite replica sets.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1925180[*BZ#1925180*]), (link:https://bugzilla.redhat.com/show_bug.cgi?id=1976775[*BZ#1976775*])

* Previously, there was a race between the installer pod and the cert-syncer container, which were writing to the same path. This could leave some certificates empty and prevent the server from running. Kubernetes API server certificates are now written in an atomic way to prevent races between multiple processes.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1971624[*BZ#1971624*])

[discrete]
[id="ocp-4-9-machine-config-operator-bug-fixes"]
==== Machine Config Operator

[discrete]
[id="ocp-4-9-monitoring-bug-fixes"]
==== Monitoring

[discrete]
[id="ocp-4-9-networking-bug-fixes"]
==== Networking

* When using the OVN-Kubernetes cluster network provider, the logical flow cache was configured without any memory limit. As a result, in some situations high memory pressure could cause a node to become unusable. With this update, the logical flow cache is configured with a 1 GB memory limit by default. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1961757[*BZ#1961757*])

* When using the OVN-Kubernetes cluster network provider, any network policies created in a {product-title} 4.5 cluster that was subsequently upgraded might allow or drop unexpected traffic. In later versions of {product-title}, OVN-Kubernetes uses a different convention for managing IP address sets, and any network policies created in {product-title} 4.5 did not use this convention. Now, during upgrades all network policies are migrated to the new convention. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1962387[*BZ#1962387*])

* For the OVN-Kubernetes cluster network provider, when using `must-gather` to retrieve Open vSwitch (OVS) logs, the `INFO` log level was missing from the gathered logging data. Now all log levels are included in OVS logging data. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1970129[*BZ#1970129*])

* Previously, performance testing revealed that the service controller metrics had a significant increase in cardinality due to a label requirement. As a result, memory usage was elevated on Open Virtual Network (OVN) Prometheus pods. With this update, the label requirement is removed. Service controller cardinality metrics and memory usage are now reduced. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1974967[*BZ#1974967*])

* Previously, `ovnkube-trace` required iproute to be installed in the source and/or destination pod because it needed to detect the interfaces `link` index. This causes `ovnkube-trace` to fail on pods if there is no iproute installed. Now, you can get the `link` index from `/sys/class/net/<interface>/iflink` instead of iproute. As a result, `ovnkube-trace` no longer requires iproute to be installed in source and destination pods. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1978137[*BZ#1978137*])

* Previously, the Cluster Network Operator (CNO) deployed a service monitor for the `network-check-source` service to get discovered by Prometheus without correct annotations and role-based access control (RBAC). As a result, the service and its metrics never populated in Prometheus. Now, the correct annotations and RBAC are added to the namespace of `network-check-source` service. Now, metrics of service `network-check-source` get scraped by Prometheus.(link:https://bugzilla.redhat.com/show_bug.cgi?id=1986061[*BZ#1986061*])

* Previously, when using IPv6 DHCP, node interface addresses might be leased with a `/128` prefix. Consequently, OVN-Kubernetes uses the same prefix to infer the node's network and routes any other address traffic, including traffic to other cluster nodes, through the gateway. With this update, OVN-Kubernetes inspects the node's routing table and checks for the wider routing entry for the node's interface address and uses that prefix to infer the node's network. As a result, traffic to other cluster nodes is no longer routed through the gateway. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1994624[*BZ#1994624*])

* Previously, when a cluster used the OVN-Kubernetes Container Network Interface provider, attempting to add an egress router with IPv6 address failed. With the fix, support for IPv6 is added to the egress router CNI plug-in and adding adding egress routers succeeds.   (link:https://bugzilla.redhat.com/show_bug.cgi?id=1989688[*BZ#1989688*])

[discrete]
[id="ocp-4-9-node-bug-fixes"]
==== Node

* Previously, in containers, CRI-O did not create a symbolic link from `/proc/mounts` file to the `/etc/mtab` file. As a consequence, the user could not view the list of the mounted devices in the container's `/etc/mtab` file. CRI-O now adds the symbolic link. As a result, users can view the container's mounted devices.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1868221[*BZ#1868221*])

* Previously, if pods were deleted quickly after creation, the kubelet might not clean up the pods properly. This resulted in pods being stuck in a terminating state, and could impact the availability of upgrades. This fix improves the pod lifecycle logic to avoid this problem.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1952224[*BZ#1952224*])

* Previously, the `SystemMemoryExceedsReserved` alert would fire when the system memory usage exceeded 90% of the reserved memory. As a result, clusters could fire an excessive number of alerts. The threshold for this alarm was changed to fire at 95% of reserved memory.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1980844[*BZ#1980844*])

* Previously, a bug in CRI-O caused CRI-O to leak a child PID of a process it created. As a result, if under load, systemd could create a significant number of zombie processes. This could lead to node failure if the node ran out of PIDs. CRI-O was fixed to prevent the leakage. As a result, these zombie processes are no longer being created.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2003197[*BZ#2003197*])

[discrete]
[id="ocp-4-9-oauth-api-server-bug-fixes"]
==== OAuth API server

[discrete]
[id="ocp-4-9-openshift-cli-bug-fixes"]
==== OpenShift CLI (oc)

* Previously, the `oc` command-line tool was crashing while mirroring the registry, causing a `slice bounds out of range` panic runtime error because of an unchecked index operation on a slice when using the `--max-components` argument. With this fix, a check has been added to ensure that the components check does not request an out-of-range index value so that the `oc` tool no longer panics when using the `--max-components` argument. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1786835[*BZ#1786835*])

* Previously, the `oc describe quota` command showed inconsistent units in `Used` memory for the `ClusterResourceQuota` value, which was unpredictable and difficult to read. With this fix, the `Used` memory now always uses the same unit as the `Hard` memory so that the `oc describe quota` command shows predictable values. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1955292[*BZ#1955292*])

* Previously, the `oc logs` command did not work with pipeline builds because of a missing client setup. The client setup has been fixed in the `oc logs` command so that it now works with pipeline builds. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1973643[*BZ#1973643*])

[discrete]
[id="ocp-4-9-olm-bug-fixes"]
==== Operator Lifecycle Manager (OLM)

* Previously, the Operator Lifecycle Manager (OLM) upgradeable condition message was unclear when installed Operators set `olm.maxOpenShiftVersion` to a minor {product-title} version less than or equal to the current version. This resulted in an incorrect error message that has been fixed to specify that only minor and major version upgrades are blocked when `olm.maxOpenShiftVersion` is set to version different than the current {product-title} version. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1992677[*BZ#1992677*])

* Previously, the `opm` command failed to deprecate bundles when they were present in the index. Consequently, bundles truncated as part of another deprecation in the same call were reported as missing. This update adds a check for bundles before any deprecation takes place to differentiate between a bundle that is not present and one that has been truncated. As a result, deprecated bundles along the same upgrade path are no longer reported as missing. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1950534[*BZ#1950534*])

* A transient error can occur when Operator Lifecycle Manager (OLM) attempts to update a custom resource definition (CRD) object in the cluster. This caused OLM to permanently fail the install plan containing the CRD. This bug fix updates OLM to retry CRD updates on resource-modified conflict errors. As a result, OLM is now more resilient to this class of transient errors. Install plans no longer permanently fail on conflict errors that OLM is able to retry and resolve. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1923111[*BZ#1923111*])

* The `opm index|registry add` commands attempted to verify the existence of Operator bundles in an index that are replaced, regardless of whether they were already truncated from the index. The commands would consistently fail after a bundle was deprecated for a given package. This bug fix updates the `opm` CLI to handle this edge case and no longer verify the existence of truncated bundles. As a result, the commands no longer fail after a bundle is deprecated for a given package. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1952101[*BZ#1952101*])

* Operator Lifecycle Manager (OLM) can now allow priority classes to be projected into registry pods using labels in catalog source resources. The default catalog sources are important components in namespaces managed by the cluster, which mandate priority classes. With this enhancement, all default catalog sources in the `openshift-marketplace` namespace have a `system-cluster-critical` priority class. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1954869[*BZ#1954869*])

* The Marketplace Operator was using the leader-for-life implementation where a config map holding the leasing owner's identity has owner references placed by the controller's pod. This is problematic in the case where the node that the pod was scheduled on became unavailable, and the pod was unable to be terminated. This made the config map unable to be properly garbage collected so a new leader could be elected. Minor version cluster upgrades were blocked as the newer Marketplace Operator version could not gain leader election. Manual cleanup of the config map holding the leader election lease was required in order to release the lock and complete the upgrade of the Marketplace component. This bug fix switches to using the leader-for-lease leader election implementation. As a result, leader election no longer gets stuck in this scenario. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1958888[*BZ#1958888*])

* Previously, a new `Failed` phase was introduced for install plans. Failure to detect a valid Operator group (OG) or service account (SA) resource for the namespace the install plan was being created in would transition the install plan to the failed state. That is, failure to detect these resources when the install plan was reconciled the first time was considered a permanant failure. This was a regression from the following previous behavior of install plans:
+
--
* Failure to detect OG or SA resources would requeue the install plan for reconciliation.
* Creation of the required resources before the retry limit of the informer queue was reached would transition the install plan from the `Installing` phase to the `Complete` phase, unless the bundle unpacking step failed.
--
+
This regression introduced strange behavior for users who had infrastructure built that applied a set of manifests simultaneously to install an Operator that included a subscription, which creates install plans, along with the required OG and SA resources. In those cases, whenever there was a delay in the reconciliation of the OG and SA, the install plan would be transitioned to a state of permanent failure.
+
This bug fix removes the logic that transitioned the install plan to the `Failed` phase. Instead, the
install plan is now requeued for any reconciliation error. As a result, when no OG is detected, the following condition is set:
+
[source,yaml]
----
conditions:
 - lastTransitionTime: ""2021-06-23T18:16:00Z""
 lastUpdateTime: ""2021-06-23T18:16:16Z""
 message: attenuated service account query failed - no operator group found that
 is managing this namespace
 reason: InstallCheckFailed
 status: ""False""
 type: Installed
----
+
When a valid OG is created, the following condition is set:
+
[source,yaml]
----
conditions:
 - lastTransitionTime: ""2021-06-23T18:33:37Z""
 lastUpdateTime: ""2021-06-23T18:33:37Z""
 status: ""True""
----
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1960455[*BZ#1960455*])

* When updating a catalog source, a `Get` call is immediately followed by a `Delete` call on a number of resources related to the catalog source. In some instances, the resource had already been deleted but the resource still existed in the cache. This allowed the `Get` call to succeed, but the following `Delete` call failed as the resource did not exist on the cluster. This bug fix updates Operator Lifecycle Manager (OLM) to ignore the error returned by the `Delete` call if the resource is not found. As a result, OLM no longer reports an error when updating a catalog source due to a caching issue that results in a "Resource Not Found" error from the `Delete` call. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1967621[*BZ#1967621*])

* A cluster service version (CSV) with a name over the limit of 63 characters causes an invalid `ownerref` label. Previously, when Operator Lifecycle Manager (OLM) used the `ownerref` reference to retrieve owned resources, including cluster role bindings, the lister returned all cluster role bindings in the namespaces due to the invalid label. This bug fix updates OLM to use a different method to let the server reject invalid `ownerref` labels instead. As a result, when CSVs have an invalid name, OLM no longer removes cluster role bindings. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1970910[*BZ#1970910*])

* Previously, Operator dependencies were not always persisted after installation time. After installing an Operator that declares dependencies, later updates and installations within the same namespace could fail to honor the previously installed Operator's dependencies. With this bug fix, dependencies are now persisted, along with all declared properties for the Operator, in an annotation on the Operator's `ClusterServiceVersion` (CSV) object. As a result, the declared dependencies of installed Operators continue to be respected for future installations. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1978310[*BZ#1978310*])

* Previously, when you removed an Operator with a deprecated bundle, the deprecation history was not included in the garbage collection. As a result, if you reinstalled the Operator, the bundle version would show up the deprecated table. This update fixes the issue with better garbage collection for deprecated bundles. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1982781[*BZ#1982781*])

* Previously, the z-stream version of a cluster was used in Operator compatibility calculations. As a result, micro releases of {product-title} were blocked. This update fixes the issue by ignoring cluster z-stream versions in Operator compatibility comparisons. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1993286[*BZ#1993286*])

[discrete]
[id="ocp-4-9-operator-sdk-bug-fixes"]
==== Operator SDK

[discrete]
[id="ocp-4-9-openshift-api-server-bug-fixes"]
==== OpenShift API server

* Previously, a single failed request to the discovery endpoint of a service could make an Operator report `Available=False`. To increase resilience, a set of improvements have been introduced to prevent some Operators from reporting `Available=False` during an update due to various transient errors. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1948089[*BZ#1948089*])

[discrete]
[id="ocp-4-9-openshift-update-service-bug-fixes"]
==== OpenShift Update Service

* Previously, when creating an update service application through the web console, an invalid host error occurred. This occurred because the default OpenShift Update Service (OSUS) application name was too long. A shorter default name is now in place and the error no longer occurs.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1939788[*BZ#1939788*])

[discrete]
[id="ocp-4-9-performance-addon-operator-bug-fixes"]
==== Performance Addon Operator

[discrete]
[id="ocp-4-9-rhcos-bug-fixes"]
==== {op-system-first}

* Previously, systemd was unable to read environment files in `/etc/kubernetes`. The SELinux policy caused this and as a result, the kubelet did not start. The policy has been modified. The kubelet starts and the environment files are read. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1969998[*BZ#1969998*])

* In an s390x Kernel Virtual Machine (KVM) with an ECKD DASD attached, the DASD would appear to be a regular virtio storage device but would become inaccessible if the VTOC was removed. As a result, you could not use DASD as a virtio block device when installing {op-system-first} on the KVM. The `coreos-installer` program has been updated so that it now installs RHCOS with a VTOC-format partition table when the installation destination is a virtio storage device such as an ECKD DASD attached to a KVM. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1960485[*BZ#1960485*])

* Previously, the `NetworkManager-wait-online-service` timed out too early, which prevented establishing a connection before the `coreos-installer` program could start. Consequently, if the network took too long to start, the `coreos-installer` program failed to fetch the Ignition config. With this update, the `NetworkManager-wait-online-service` timeout has been increased to its default upstream value. As a result, the `coreos-installer` program no longer fails to fetch the Ignition config. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1967483[*BZ#1967483*])

[discrete]
[id="ocp-4-9-routing-bug-fixes"]
==== Routing

* Previously, there was config drift when the Cluster Network Operator (CNO) attempted to sanitize the proxy configuration, specifically the `no_proxy` config. This resulted in a specific IPv6 CIDR missing from the `no_proxy`. This fix implements logic that updates the dual stack (IPV4 and IPV6) for all scenarios. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1981975[*BZ#1981975*])

* Previously, if the `.spec.privateZone` field of the `dns.config.openshift.io` Operator was filled out incorrectly so that the Ingress Operator was not able to find the private hosted zone, then the Ingress Operator became degraded. However, even after fixing the `.spec.privateZone` field, the Ingress Operator stayed degraded. The Ingress Operator finds the hosted zone and adds the `.apps` resource record, but the Ingress Operator does not reset the degraded status. This fix watches the DNS config object and monitors changes regarding the `spec.privateZone` field. It applies the appropriate logic and updates the Operator status accordingly. The Operator status returns to degraded, or `False`, once the correct `.spec.privateZone` field is set. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1942657[*BZ#1942657*])

[discrete]
[id="ocp-4-9-samples-bug-fixes"]
==== Samples

* Previously, the lack of a connection timeout led to lengthy delays. This occurred when the Cluster Samples Operator, with `managementState` set to `Removed`, tested the connection to `registry.redhat.io`. With the addition of a connection timeout, the delay is eliminated. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1990140[*BZ#1990140*])

[discrete]
[id="ocp-4-9-service-ca-bug-fixes"]
==== Service CA

[discrete]
[id="ocp-4-9-storage-bug-fixes"]
==== Storage

* Previously, you could delete `LocalVolumeSets` with in-use PVs, which required manual clean up. This fix ensures that all released PVs are cleaned automatically. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1862429[*BZ#1862429*])

* Previously, the `oc get volumesnapshotcontent` command did not display the namespace for a volume snapshot, which meant that the volume snapshot was not uniquely identified. This command now displays the namespace for the volume snapshot. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1965263[*BZ#1965263*])

[discrete]
[id="ocp-4-9-web-console-admin-perspective-bug-fixes"]
==== Web console (Administrator perspective)

* Previously, all rows on the `PF4` table were rerendering. With this update, the content in `React.memo` was wrapped so the content does not rerender on every scroll event.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1856355[*BZ#1856355*])

* Previously, the charts in *Cluster Utilization* in the {product-title} web console displayed the data time span in a confusing manner. For example, if the six-hour time span option is selected, but data exists only for the final three hours, those three data points are stretched to fill the entire chart. The first three hours are not displayed. This could result in the assumption that the chart is showing the full six-hour time span. To avoid confusion, the charts now show blank space for missing information. In this example, the chart displays the entire six-hour time span with data starting at the fourth hour. The first three hours are blank.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1904155[*BZ#1904155*])

* Previously, `NetworkPolicy` was not translated to Korean or Chinese on the web console. With this update, `NetworkPolicy` is now translated correctly when viewing the web console in Korean or Chinese.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1965930[*BZ#1965930*])

* Previously, an issue with the `Needs Attention` state of the *Console Overview* section showed Operators as `upgrading`, even if they were not upgrading. This update fixes the `Needs Attention` state so that the correct status of an Operator is displayed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1967047[*BZ#1967047*])

* Previously, the alert for a failed Cluster Service Version (CSV) displayed a generic `status.message` that did not help troubleshoot the the failed CSV. With this update, copied CSVs show a helpful message and a link to the original CSV to troubleshoot.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1967658[*BZ#1967658*])

* Previously, a user was unable to use the drop-down options in the masthead with a keyboard. With this update, users are now able to access the drop-down options with a keyboard.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1967979[*BZ#1967979*])

* Previously, a utility function used for matching Operator-owned resources with their owners would return false matches. Consequently, *Managed by* links on Operator-owned resource pages would sometimes link to the incorrect URL. This fix updates the function logic to correctly match owned Operators. As a result, *Managed by* links now link to the correct URL.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1970011[*BZ#1970011*])

* Previously, the *OperatorHub* web console interface would lead users to unrelated install plans. With this update, *OperatorHub* links users to the Operator *Subscription* details tab to view installation progress.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1970466[*BZ#1970466*])

* Previously, items in the *Add* drop-down list on the *OAuth details* page were not internationalized. With this update, these items are internationalized and the user experience for non-English speakers is improved.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1970604[*BZ#1970604*])

* Previously, an invalid localization property prevented some messages from being internationalized. This update removes the invalid property. As a result, these messages are internationalized and the user experience for non-English speakers is improved.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1970604[*BZ#1970604*])

* This update removes a tooltip that appeared when mousing over a resource link on a list page, because the information did not improve the user experience.
 (link:https://bugzilla.redhat.com/show_bug.cgi?id=1971532[*BZ#1971532*])

* Previously, console pods were deployed with the `preferredDuringSchedulingIgnoredDuringExecution` anti-affinity rule, which sometimes resulted in both console pods being scheduled on the same control plane node. This fix changes the rule to `requiredDuringSchedulingIgnoredDuringExecution` so that the pods must be scheduled on different nodes if the condition matches.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1975379[*BZ#1975379*])

* Previously, uninstalling an Operator failed to remove all of the enabled plugins. With this release, uninstalling an Operator now removes all enabled plugins.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1975820[*BZ#1975820*])

* Previously, front-end Operator Lifecycle Manager (OLM) descriptor handling only used the first x-descriptor to render a property on an operand details page. Consequently, if multiple x-descriptors were defined for a property and the first one in the list was invalid or unsupported, it would not render as expected. This fix updates the descriptor validation logic to prioritize supported x-descriptors over unsupported x-descriptors. As a result, descriptor-decorated properties are rendered on the *Operand details* page using the first valid and supported x-descriptor found in the list.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1976072[*BZ#1976072*])

* Previously, string data was used for encoded secrets. As a result, binary secret data was not properly uploaded by the web console. This update encodes secrets and uses data instead of string data in the API. As a result, binary secrets now upload correctly.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1978724[*BZ#1978724*])

* Previously, when processes running on the cluster were manually terminated, the terminal `ps -aux` command showed that some processes were not cleared. This caused stray processes to remain, leaving the cluster in an invalid state. This fix ensures that all processes terminate properly on the cluster and do not appear in the list of active processes that are listed on the terminal.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1979571[*BZ#1979571*])

* Previously, when a default pull secret was added to a new project and credentials for multiple registries were uploaded, only the first credential was listed on the `Project Details` page. There was also no indication that the list was truncated. As a result, when a user clicked the project details from `Default pull secret`, only the first credential was listed. This fix ensures that all of the credentials are listed and informs the users that additional credentials exist if they are not listed on the current page.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1980704[*BZ#1980704*])

* Previously, when users changed the default browser language to Simplified Chinese, the cluster utilization resource metrics on the *Overview* page of the web console displayed in a combination of English and Simplified Chinese characters. This fix allows the user to view the cluster utilization resources entirely in the selected language.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1982079[*BZ#1982079*])

* Previously, when the language was changed to Simplified Chinese, the cluster utilization usage statistic did not match the translation in the left menu for `project`, `pod`, and `node`. This fix corrects the Simplified Chinese translation so that the cluster utilization metrics are consistent with the `top consumers` filter.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1982090[*BZ#1982090*])

* Previously, users saw an error instead of the default pull secret from the service account. This resulted in incomplete information on the project details screen. The user had to go to the default ServiceAccount to view the entire list of default pull secrets. This fix allows the user to view the entire list of pull secrets from the default ServiceAccount on the project details page.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1983091[*BZ#1983091*])

* Previously, if you resized the web page for a node or pod while viewing the *Terminal* tab, sometimes the browser displayed two vertical scrollbars. The console is now updated to display one scrollbar only when the window is resized.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1983220[*BZ#1983220*])

* Previously, the web console did not deploy when installing {product-title} 4.8.2 using a single node developer profile. If a valid Operator group or service account was not detected for the namespace in which the install plan was being created, the install plan was placed in a failed state. No further attempts were made. With this revision, the failed install plan is set to run again until an Operator group or service account is detected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1986129[*BZ#1986129*])

* Previously, in the *Events Dashboard*, `More` and `Show Less` were not internationalized, which resulted in poor user experience. With this update, the text is now internationalized.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1986754[*BZ#1986754*])

* Previously, the logic that constructed the fully qualified domain name (FQDN) of a service in the *Console* page was missing. As a result, FQDN information was missing on the service's detail page. This update adds logic that constructs the FQDN so that the service's FQDN information is now available on the page.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1998616[*BZ#1996816*])

[discrete]
[id="ocp-4-9-web-console-developer-perspective-bug-fixes"]
==== Web console (Developer perspective)

[id="ocp-4-9-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_
* *DEP*: _Deprecated_

//TODO: remove anything that has been GA since 4.6

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.7 |OCP 4.8 |OCP 4.9

|Precision Time Protocol (PTP)
|TP
|TP
|

|`oc` CLI plug-ins
|TP
|GA
|

|Descheduler
|GA
|GA
|

|HPA for memory utilization
|GA
|GA
|

|Service Binding
|TP
|TP
|TP

|Raw Block with Cinder
|TP
|GA
|

|CSI volume snapshots
|GA
|GA
|GA

|CSI volume expansion
|TP
|TP
|TP

|vSphere Problem Detector Operator
|GA
|GA
|GA

|CSI Azure Disk Driver Operator
|-
|TP
|TP

|CSI Azure Stack Hub Driver Operator
|-
|-
|GA

|CSI GCP PD Driver Operator
|TP
|GA
|GA

|CSI OpenStack Cinder Driver Operator
|TP
|TP
|TP

|CSI AWS EBS Driver Operator
|TP
|TP
|GA

|CSI AWS EFS Driver Operator
|-
|-
|TP

|CSI automatic migration
|-
|TP
|TP

|CSI inline ephemeral volumes
|TP
|TP
|TP

|CSI vSphere Driver Operator
|-
|TP
|TP

|Automatic device discovery and provisioning with Local Storage Operator
|TP
|TP
|TP

|OpenShift Pipelines
|TP
|GA
|

|OpenShift GitOps
|TP
|GA
|

|OpenShift sandboxed containers
|-
|TP
|

|Vertical Pod Autoscaler
|TP
|GA
|

|Cron jobs
|TP
|GA
|

|PodDisruptionBudget
|TP
|GA
|

|Adding kernel modules to nodes with kvc
|TP
|TP
|

|Egress router CNI plug-in
|TP
|GA
|

|Scheduler profiles
|TP
|TP
|GA

|Non-preempting priority classes
|TP
|TP
|

|Kubernetes NMState Operator
|TP
|TP
|

|Assisted Installer
|TP
|TP
|

|AWS Security Token Service (STS)
|TP
|GA
|GA

|Kdump
|TP
|TP
|

|OpenShift Serverless
|-
|GA
|

|Serverless functions
|-
|TP
|

|Data Plane Development Kit (DPDK) support
|TP
|TP
|GA

|Memory Manager feature
|-
|-
|TP

|CNI VRF plug-in
|TP
|TP
|GA

|Cluster Cloud Controller Manager Operator
|-
|-
|GA

|Cloud controller manager for AWS
|-
|-
|TP

|Cloud controller manager for Azure
|-
|-
|TP

|Cloud controller manager for OpenStack
|-
|-
|TP

|Driver Toolkit
|-
|TP
|TP

|Special Resource Operator (SRO)
|-
|-
|TP

|Node Health Check Operator
|-
|-
|TP

|====

[id="ocp-4-9-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.8 and beyond! This needs some SME/QE review before being updated for 4.9.
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to 4.8, you can either revoke or continue to allow unauthenticated access. It is recommended to revoke unauthenticated access unless there is a specific need for it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* An Open Virtual Network (OVN) bug causes persistent connectivity issues with Octavia load balancers. When Octavia load balancers are created, OVN might not plug them into some Neutron subnets. These load balancers might be unreachable for some of the Neutron subnets. This problem affects Neutron subnets, which are created for each OpenShift namespace at random when Kuryr is configured. As a result, when this problem occurs the load balancer that implements OpenShift `Service` objects will be unreachable from OpenShift namespaces affected by the issue. Because of this bug, {product-title} 4.8 deployments that use Kuryr SDN are not recommended on {rh-openstack-first} 16.1 with OVN and OVN Octavia configured. This will be fixed in a future release of {rh-openstack}. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1937392[*BZ#1937392*])

* Installations on {rh-openstack-first} with Kuryr will not work if configured with a cluster-wide proxy when the cluster-wide proxy is required to access {rh-openstack} APIs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1985486[*BZ#1985486*])

* Due to a race condition, the {rh-openstack-first} cloud provider might not start properly. Consequently, LoadBalancer services might never get an `EXTERNAL-IP` set. As a temporary workaround, you can restart the kube-controller-manager pods using the procedure described in link:https://bugzilla.redhat.com/show_bug.cgi?id=2004542[*BZ#2004542*].

* The SR-IOV network configuration daemon pod will cordon the node and mark it as unschedulable. It will use the add or delete `SriovNetworkNodePolicy` custom resource (CR) before waiting for the `syncStatus` of the CR to change to `Succeeded`. As a temporary workaround, before adding or deleting a `SriovNetworkNodePolicy` CR, make sure the `syncStatus` of the `SriovNetworkNodeState` CRs is in the `Succeeded` state. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2002508[*BZ#2002508*])

* You can only install {product-title} on Azure Stack Hub with public endpoints, such as the ARM endpoint, that are secured with certificates signed by a publicly trusted certificate authority (CA). Support for internal CAs will be added in a future z-stream release of {product-title}. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2012173[*BZ#2012173*])

* Cluster administrators can specify a custom HTTP error code response page for either 503, 404, or both error pages. The router does not reload to reflect custom error code pages updates. As a workaround, rsh in the router pods and run `reload-haproxy` in all the router pods that serve the custom http error code pages:
+
[source,terminal]
----
$ oc -n openshift-ingress rsh router-default-6647d984d8-q7b58
sh-4.4$ bash -x /var/lib/haproxy/reload-haproxy
----
+
Alternatively, you can annotate the route to force a reload.(link:https://bugzilla.redhat.com/show_bug.cgi?id=1990020[*BZ#1990020*])

* This release contains a known issue. If you customize the hostname and certificate of the OpenShift OAuth route, Jenkins no longer trusts the OAuth server endpoint. As a result, users cannot log in to the Jenkins console if they rely on the OpenShift OAuth integration to manage identity and access. A workaround is not available at this time. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1991448[*BZ#1991448*])

[id="ocp-4-9-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} 4.9 are released as asynchronous errata through the Red Hat Network. All {product-title} 4.9 errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified via email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} 4.9. Versioned asynchronous releases, for example with the form {product-title} 4.9.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster.adoc#updating-cluster[updating your cluster] properly.
====

[id="ocp-4-9-0-ga"]
=== #RHSA-2021:TBD1# - {product-title} #4.9.0# image release, bug fix, and security update advisory

Issued: #2021-10-18#

{product-title} release #4.9.0#, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the #link:https://access.redhat.com/errata/RHSA-2021:TBD1[RHSA-2021:TBD1]# advisory. The RPM packages that are included in the update are provided by the #link:https://access.redhat.com/errata/RHSA-2021:TBD2[RHSA-2021:TBD2]# advisory.

Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:

#link:https://access.redhat.com/solutions/TBD3[{product-title} 4.9.0 container image list]#
