:_mod-docs-content-type: ASSEMBLY
[id="ocp-4-16-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-16-about-this-release_{context}"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-2023:7198[RHSA-2023:7198]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.29.md[Kubernetes 1.29] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. With the {cluster-manager-first} application for {product-title}, you can deploy {product-title} clusters to either on-premises or cloud environments.

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.8 and 8.9, and on {op-system-first} 4.15.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

For {product-title} 4.12 on `x86_64` architecture, Red Hat has added a 6-month Extended Update Support (EUS) phase that extends the total available lifecycle from 18 months to 24 months. For {product-title} 4.12 running on 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures, the EUS lifecycle remains at 18 months.

Starting with {product-title} 4.14, each EUS phase for even numbered releases on all supported architectures, including `x86_64`, 64-bit ARM (`aarch64`), {ibm-power-name} (`ppc64le`), and {ibm-z-name} (`s390x`) architectures, has a total available lifecycle of 24 months.

Starting with {product-title} 4.14, Red Hat offers a 12-month additional EUS add-on, denoted as _Additional EUS Term 2_, that extends the total available lifecycle from 24 months to 36 months. The Additional EUS Term 2 is available on all architecture variants of {product-title}.

For more information about this support, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

//TODO: The line below should be used when it is next appropriate. Revisit in August 2023 time frame.
Maintenance support ends for version 4.12 on 25 January 2025 and goes to extended life phase. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

Commencing with the {product-version} release, Red Hat is simplifying the administration and management of Red Hat shipped cluster Operators with the introduction of three new life cycle classifications; Platform Aligned, Platform Agnostic, and Rolling Stream. These life cycle classifications provide additional ease and transparency for cluster administrators to understand the life cycle policies of each Operator and form cluster maintenance and upgrade plans with predictable support boundaries. For more information, see link:https://access.redhat.com/webassets/avalon/j/includes/session/scribe/?redirectTo=https%3A%2F%2Faccess.redhat.com%2Fsupport%2Fpolicy%2Fupdates%2Fopenshift_operators[OpenShift Operator Life Cycles].

// Added in 4.14. Language came directly from Kirsten Newcomer.
{product-title} is designed for FIPS. When running {op-system-base-full} or {op-system-first} booted in FIPS mode, {product-title} core components use the {op-system-base} cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the `x86_64`, `ppc64le`, and `s390x` architectures.

For more information about the NIST validation program, see link:https://csrc.nist.gov/Projects/cryptographic-module-validation-program/validated-modules[Cryptographic Module Validation Program]. For the latest NIST status for the individual versions of {op-system-base} cryptographic libraries that have been submitted for validation, see link:https://access.redhat.com/articles/2918071#fips-140-2-and-fips-140-3-2[Compliance Activities and Government Standards].

[id="ocp-4-16-add-on-support-status_{context}"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-16-new-features-and-enhancements_{context}"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-16-rhcos_{context}"]
=== {op-system-first}

[id="ocp-4-16-rhcos-rhel-9-4_{context}"]
==== {op-system} now uses {op-system-base} 9.4

{op-system} now uses {op-system-base-full} 9.4 packages in {product-title} {product-version}. These packages ensure that your {product-title} instances receive the latest fixes, features, enhancements, hardware support, and driver updates. As an Extended Update Support (EUS) release, {product-title} 4.14 is excluded from this change and will continue to use {op-system-base} 9.2 EUS packages for the entirety of its lifecycle.

[id="ocp-4-16-iscsi-support_{context}"]
==== Support for iSCSI boot volumes

With this release, you can now install {op-system} to iSCSI boot devices. Multipathing for iSCSI is also supported. For more information, see xref:../installing/installing_bare_metal/installing-bare-metal.adoc#rhcos-install-iscsi-manual[Installing {op-system} manually on an iSCSI boot device] and xref:../installing/installing_bare_metal/installing-bare-metal.adoc#rhcos-install-iscsi-ibft[Installing {op-system} on an iSCSI boot device using iBFT]

[id="ocp-4-16-installation-and-update_{context}"]
=== Installation and update

[id="ocp-4-16-installation-and-update-aws-capi_{context}"]
==== Cluster API replaces Terraform for AWS installations
In {product-title} {product-version}, the installation program uses Cluster API (CAPI) instead of Terraform to provision cluster infrastructure during installations on AWS. There are several additional required permissions as a result of this change. For more information, see xref:../installing/installing_aws/installing-aws-account.adoc#installation-aws-permissions_installing-aws-account[Required AWS permissions for the IAM user].

Additionally, SSH access to control plane and compute machines is no longer open to the machine network, but is restricted to the security groups associated with the control plane and compute plane machines.

[WARNING]
====
Installing a cluster on AWS into a secret or top-secret region has not been tested with CAPI as of the release of {product-title} {product-version}. This document will be updated when installation into a secret region has been tested. There is a known issue with Network Load Balancers' support for security groups in secret or top secret regions that causes installations to fail. For more information, see link:https://issues.redhat.com/browse/OCPBUGS-33311[*OCPBUGS-33311*].
====

[id="ocp-4-16-installation-and-update-vsphere-capi_{context}"]
==== Cluster API replaces Terraform for {vmw-first} installations
In {product-title} {product-version}, the installation program uses Cluster API instead of Terraform to provision cluster infrastructure during installations on {vmw-first}.

[id="ocp-4-16-installation-and-update-nutanix-capi_{context}"]
==== Cluster API replaces Terraform for Nutanix installations
In {product-title} {product-version}, the installation program uses Cluster API instead of Terraform to provision cluster infrastructure during installations on Nutanix.

[id="ocp-4-16-installation-and-update-gcp-capi_{context}"]
==== Cluster API replaces Terraform for {gcp-first} installations (Technology Preview)
In {product-title} {product-version}, the installation program uses Cluster API instead of Terraform to provision cluster infrastructure during installations on {gcp-short}. This feature is available as a Technology Preview in {product-title} {product-version}. To enable Technology Preview features, set the `featureSet: TechPreviewNoUpgrade` parameter in the `install-config.yaml` file before installation. Alternatively, add the following stanza to the `install-config.yaml` file before installation to enable Cluster API without any other Technology Preview features:

[source,yaml]
----
featureSet: CustomNoUpgrade
featureGates:
- ClusterAPIInstall=true
----

For more information, see xref:../installing/installing_gcp/installation-config-parameters-gcp.adoc#installation-configuration-parameters-optional_installation-config-parameters-gcp[Installation configuration parameters].

[id="ocp-4-16-installation-and-update-optional-ccm_{context}"]
==== Optional cloud controller manager cluster capability

In {product-title} {product-version}, you can disable the cloud controller manager capability during installation.
For more information, see xref:../installing/cluster-capabilities.adoc#cluster-cloud-controller-manager-operator_cluster-capabilities[Cloud controller manager capability].

[id="ocp-4-16-installation-fips-rhel9_{context}"]
==== FIPS installation requirements in {product-title} {product-version}

With this update, if you install a FIPS-enabled cluster, you must run the installation program from a {op-system-base} 9 computer that is configured to operate in FIPS mode, and you must use a FIPS-capable version of the installation program. For more information, see xref:../installing/installing-fips.adoc#installing-fips[Support for FIPS cryptography].

[id="ocp-4-16-installation-and-update-vsphere-tagging_{context}"]
==== Optional additional tags for {vmw-first}

In {product-title} {product-version}, you can add up to ten tags to attach to the VMs provisioned by a {vmw-first} cluster.
These tags are in addition to the unique cluster-specific tag that the installation program uses to identify and remove associated VMs when a cluster is decommissioned.

You can define the tags on the {vmw-first} VMs in the `install-config.yaml` file during cluster creation.
For more information, see xref:../installing/installing_vsphere/ipi/installing-restricted-networks-installer-provisioned-vsphere.adoc#installation-installer-provisioned-vsphere-config-yaml_installing-restricted-networks-installer-provisioned-vsphere[Sample `install-config.yaml` file for an installer-provisioned {vmw-first} cluster].

You can define tags for compute or control plane machines on an existing cluster by using machine sets.
For more information, see "Adding tags to machines by using machine sets" for xref:../machine_management/creating_machinesets/creating-machineset-vsphere.adoc#machine-api-vmw-add-tags_creating-machineset-vsphere[compute] or xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-vsphere.adoc#machine-api-vmw-add-tags_cpmso-config-options-vsphere[control plane] machine sets.

[id="ocp-4-16-admin-ack-updating_{context}"]
==== Required administrator acknowledgment when updating from {product-title} 4.15 to 4.16

{product-title} 4.16 uses Kubernetes 1.29, which removed several xref:../release_notes/ocp-4-16-release-notes.adoc#ocp-4-16-removed-kube-1-29-apis_{context}[deprecated APIs].

A cluster administrator must provide manual acknowledgment before the cluster can be updated from {product-title} 4.15 to 4.16. This is to help prevent issues after updating to {product-title} 4.16, where APIs that have been removed are still in use by workloads, tools, or other components running on or interacting with the cluster. Administrators must evaluate their cluster for any APIs in use that will be removed and migrate the affected components to use the appropriate new API version. After this is done, the administrator can provide the administrator acknowledgment.

All {product-title} 4.15 clusters require this administrator acknowledgment before they can be updated to {product-title} 4.16.

For more information, see xref:../updating/preparing_for_updates/updating-cluster-prepare.adoc#updating-cluster-prepare[Preparing to update to {product-title} 4.16].

[id="ocp-4-16-installation-and-update-kubeadmin_{context}"]
==== Secure kubeadmin password from being displayed in the console

With this release, you can prevent the `kubeadmin` password from being displayed in the console after the installation by using the `--skip-password-print` flag during cluster creation. The password remains accessible in the `auth` directory.

[id="ocp-4-16-appliance-utility_{context}"]
==== OpenShift-based Appliance Builder (Technology Preview)

With this release, the OpenShift-based Appliance Builder is available as a Technology Preview feature.
The Appliance Builder enables self-contained {product-title} cluster installations, meaning that it does not rely on internet connectivity or external registries.
It is a container-based utility that builds a disk image that includes the Agent-based Installer, which can then be used to install multiple {product-title} clusters.

For more information, see the link:https://access.redhat.com/articles/7065136[OpenShift-based Appliance Builder User Guide].

[id="ocp-4-16-installation-and-update-byoip_{context}"]
==== Bring your own IPv4 (BYOIP) feature enabled for installation on AWS

With this release, you can enable bring your own public IPv4 addresses (BYOIP) feature when installing on AWS by using the `publicIpv4Pool` field to allocate Elastic IP addresses (EIPs). You must ensure that you have the xref:../installing/installing_aws/installing-aws-account.adoc#installation-aws-permissions_installing-aws-account[required permissions] to enable BYOIP. For more information, see xref:../installing/installing_aws/installation-config-parameters-aws.adoc#installation-configuration-parameters-optional-aws_installation-config-parameters-aws[Optional AWS configuration parameters].

[id="ocp-4-16-installation-and-update-gcp-regions_{context}"]
==== Deploy {gcp-short} in the Dammam (Saudi Arabia) and Johannesburg (South Africa) regions

You can deploy {product-title} {product-version} in {gcp-first} in the Dammam, Saudi Arabia (`me-central2`) region and in the Johannesburg, South Africa (`africa-south1`) region. For more information, see xref:../installing/installing_gcp/installing-gcp-account.adoc#installation-gcp-regions_installing-gcp-account[Supported GCP regions].

[id="ocp-4.16-postinstallation-configuration_{context}"]
=== Postinstallation configuration

[id="ocp-4.16-multiarch-tuning-operator_{context}"]
==== Managing workloads on multi-architecture clusters by using the Multiarch Tuning Operator (Technology Preview)

With this release, you can manage workloads on multi-architecture clusters by using the Multiarch Tuning Operator. This Operator enhances the operational experience within multi-architecture clusters, and single-architecture clusters that are migrating to a multi-architecture compute configuration. It implements the `ClusterPodPlacementConfig` custom resource (CR) to support architecture-aware workload scheduling.

For more information, see xref:../post_installation_configuration/configuring-multi-arch-compute-machines/multiarch-tuning-operator.adoc#multiarch-tuning-operator[Managing workloads on multi-architecture clusters by using the Multiarch Tuning Operator].

[IMPORTANT]
====
The Multiarch Tuning Operator is a Technology Preview feature only. It does not support clusters with restricted network scenarios.
====

[id="ocp-4.16-arm-control-plane-with-x86-compute-machines_{context}"]
==== Support for adding 64-bit x86 compute machines to a cluster with 64-bit ARM control plane machines

This feature provides support for adding 64-bit x86 compute machines to a multi-architecture cluster with 64-bit ARM control plane machines. With this release, you can add 64-bit x86 compute machines to a cluster that uses 64-bit ARM control plane machines and already includes 64-bit ARM compute machines.

[id="ocp-4.16-agent-installer-cluster-with-multi-payload_{context}"]
==== Support for installing an Agent-based Installer cluster with multi payload

This feature provides support for installing an Agent-based Installer cluster with `multi` payload. After installing the Agent-based Installer cluster with `multi` payload, you can add compute machines with different architectures to the cluster.

[id="ocp-4-16-web-console_{context}"]
=== Web console

[id="ocp-4-16-web-console-language"]
==== Language support for French and Spansih
With this release, French and Spanish are suported in the web console. You can update the language in the web console from the *Language* list on the *User Preferences* page.

[id="ocp-4-16-patternfly4-deprecated"]
==== Patternfly 4 is now deprecated with {product-version}
With this release, Patternfly 4 and React Router 5 are deprecated in the web console. All plugins should migrate to Patternfly 5 and React Router 6 as soon as possible.

[id="ocp-4-16-administrator-perspective_{context}"]
==== Administrator perspective

This release introduces the following updates to the *Administrator* perspective of the web console:

* A {gcp-first} token authorization, `Auth Token GCP`, and a `Configurable TLS ciphers` filter was added to the *Infrastructure features* filter in the OperatorHub.
* A new quick start, *Impersonating the system:admin user*, is available with information on impersonating the `system:admin` user.
* A pod's last termination state is now available to view on the *Container list* and *Container details* pages.
* An `Impersonate Group` action is now available from the *Groups* and *Group details* pages without having to search for the appropriate `RoleBinding`.

[id="ocp-4-16-node-csr-handling"]
===== Node CSR handling in the {product-title} web console
With this release, the {product-title} web console supports node certificate signing requests (CSRs).

[id="ocp-4-16-cross-storage-class-clone-restore"]
===== Cross Storage Class clone and restore

With this release, you can choose a storage class from the same provider when completing clone or restore operations. This flexibility allows seamless transitions between storage classes with different replica counts. For example, moving from a storage class with 3 replicas to 2/1 replicas.

[id="ocp-4-16-developer-perspective_{context}"]
==== Developer Perspective

This release introduces the following updates to the *Developer* perspective of the web console:

* When searching, a new section was added to the list of *Resources* on the *Search* page to display the recently searched items in the order they were searched.
* With this release, you can collapse and expand the *Getting started* section.

[id="ocp-4-16-console-telemetry"]
===== Console Telemetry

With this release, anonymized user analytics were enabled if cluster telemetry is also enabled. This is the default for most of the cluster and provides Red Hat with metrics for how the web console is used. Cluster administrators can update this in each cluster and opt-in, opt-out, or disable frontend telemetry.

[id="ocp-4-16-openshift-cli_{context}"]
=== OpenShift CLI (oc)

[id="ocp-4-16-oc-adm-upgrade-status-tp_{context}"]
==== Introducing the `oc adm upgrade status` command (Technology Preview)

Previously, the `oc adm upgrade` command provided limited information about the status of a cluster update. This release adds the `oc adm upgrade status` command, which decouples status information from the `oc adm upgrade` command and provides specific information regarding a cluster update, including the status of the control plane and worker node updates.

[id="ocp-4-16-duplicate-short-name-warning_{context}"]
==== Warning for duplicate resource short names

With this release, if you query a resource by using its short name, the OpenShift CLI  (`oc`) returns a warning if more than one custom resource definition (CRD) with the same short name exists in the cluster.

.Example warning
[source,terminal]
----
Warning: short name "ex" could also match lower priority resource examples.test.com
----

[id="ocp-4-16-interactive-flag_{context}"]
==== New flag to require confirmation when deleting resources (Technology Preview)

This release introduces a new `--interactive` flag for the `oc delete` command. When the `--interactive` flag is set to `true`, the resource is deleted only if the user confirms the deletion. This flag is available as a Technology Preview feature.

[id="ocp-4-16-ibm-z_{context}"]
=== {ibm-z-title} and {ibm-linuxone-title}

[id="ocp-4-16-ibm-power_{context}"]
=== {ibm-power-title}

[id="ocp-4-16-auth_{context}"]
=== Authentication and authorization

[id="ocp-4-16-networking_{context}"]
=== Networking

[id="ocp-4-16-networking-openshift-sdn-upgrade-block_{context}"]
==== OpenShift SDN network plugin blocks future major upgrades

As part of the {product-title} transition to OVN-Kubernetes as the only supported network plugin, starting with {product-title} 4.16, if your cluster uses the OpenShift SDN network plugin, you cannot upgrade to future major versions of {product-title} without migrating to OVN-Kubernetes. For more information about migrating to OVN-Kubernetes, see xref:../networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc#migrate-from-openshift-sdn[Migrating from the OpenShift SDN network plugin].

If you attempt an upgrade, the Cluster Network Operator reports the following status:

[source,yaml]
----
- lastTransitionTime: "2024-04-11T05:54:37Z"
  message: Cluster is configured with OpenShiftSDN, which is not supported in the
    next version. Please follow the documented steps to migrate from OpenShiftSDN
    to OVN-Kubernetes in order to be able to upgrade. https://docs.openshift.com/container-platform/4.16/networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.html
  reason: OpenShiftSDNConfigured
  status: "False"
  type: Upgradeable
----

[id="ocp-4-16-ptp-dual-nic-gm-ga_{context}"]
==== Dual-NIC Intel E810 Westport Channel as PTP grandmaster clock (Generally Available)

Configuring linuxptp services as grandmaster clock (T-GM) for dual Intel E810 Westport Channel NICs is now a generally available feature in {product-title}.
The host system clock is synchronized from the NIC that is connected to the Global Navigation Satellite Systems (GNSS) time source. The second NIC is synced to the 1PPS timing output provided by the NIC that is connected to GNSS.
For more information see xref:../networking/ptp/configuring-ptp.adoc#configuring-linuxptp-services-as-grandmaster-clock-dual-nic_configuring-ptp[Configuring linuxptp services as a grandmaster clock for dual E810 Westport Channel NICs].

[id="ocp-4-16-ptp-dual-nic-tbc-ha_{context}"]
==== Dual-NIC Intel E810 PTP boundary clock with highly available system clock (Generally Available)

You can configure the `linuxptp` services `ptp4l` and `phc2sys` as a highly available (HA) system clock for dual PTP boundary clocks (T-BC).

For more information, see xref:../networking/ptp/configuring-ptp.adoc#ptp-configuring-linuxptp-services-as-ha-bc-for-dual-nic_configuring-ptp[Configuring linuxptp as a highly available system clock for dual-NIC Intel E810 PTP boundary clocks]

[id="ocp-4-16-networking-connectivity-pod-placement_{context}"]
==== Configuring pod placement to check network connectivity

To periodically test network connectivity amongst cluster components, the Cluster Network Operator (CNO) creates the `network-check-source` deployment and the `network-check-target` daemon set. In {product-title} 4.16, you can configure the nodes by setting node selectors and run the source and target pods to check the network connectivity. For more information, see xref:../networking/verifying-connectivity-endpoint.adoc#verifying-connectivity-endpoint[Verifying connectivity to an endpoint].

[id="ocp-4-16-networking-multiple-cidr_{context}"]
==== Define multiple CIDR blocks for one network security group (NSG) rule

With this release, IP addresses and ranges are handled more efficiently in NSGs for {product-title} clusters hosted on Azure. As a result, the maximum limit of CIDRs for all Ingress Controllers in Azure clusters, using the `allowedSourceRanges` field, increases from approximately 1000 to 4000 CIDRs.

[id="ocp-4-16-sdn-ovnk-migration-nutanix-support_{context}"]
==== Migration from OpenShift SDN to OVN-Kubernetes on Nutanix

With this release, migration from the OpenShift SDN network plugin to OVN-Kubernetes is now supported on Nutanix platforms. For more information, see xref:../networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc#nw-ovn-kubernetes-migration-about_migrate-from-openshift-sdn[Migration to the OVN-Kubernetes network plugin].

[id="ocp-4-16-coredns-improved-egress-firewall_{context}"]
==== Improved integration between CoreDNS and egress firewall (Technology Preview)

With this release, OVN-Kubernetes uses a new `DNSNameResolver` custom resource to keep track of DNS records in your egress firewall rules, and is available as a Technology Preview. This custom resource supports the use of both wildcard DNS names and regular DNS names and allows access to DNS names regardless of the IP addresses associated with its change.

[id="ocp-4-16-networking-sriov-draining_{context}"]
==== Parallel node draining during SR-IOV network policy updates

With this release, you can configure the SR-IOV Network Operator to drain nodes in parallel during network policy updates. The option to drain nodes in parallel enables faster rollouts of SR-IOV network configurations. You can use the `SriovNetworkPoolConfig` custom resource to configure parallel node draining and define the maximum number of nodes in the pool that the Operator can drain in parallel.

For further information, see xref:../networking/hardware_networks/configuring-sriov-device.adoc#configure-sr-iov-operator-parallel-nodes_configuring-sriov-device[Configuring parallel node draining during SR-IOV network policy updates].

[id="ocp-4-16-networking-create-sriovoperatorconfig_{context}"]
==== SR-IOV Network Operator no longer automatically creates the SriovOperatorConfig CR

As of {product-title} 4.16, the SR-IOV Network Operator no longer automatically creates a `SriovOperatorConfig` custom resource (CR). Create the `SriovOperatorConfig` CR using the procedure described in xref:../networking/hardware_networks/configuring-sriov-operator.adoc#nw-sriov-configuring-operator_configuring-sriov-operator[Configuring the SR-IOV Network Operator].

[id="ocp-4-16-q-in-q-support_{context}"]
==== Supporting double-tagged packets (QinQ)

This release introduces 802.1Q-in-802.1Q also known as QinQ support. QinQ introduces a second VLAN tag, where the service provider designates the outer tag for their use, offering them flexibility, while the inner tag remains dedicated to the customer’s VLAN. When two VLAN tags are present in a packet, the outer VLAN tag can be either 802.1Q or 802.1ad. The inner VLAN tag must always be 802.1Q.

For more information, see xref:../networking/hardware_networks/configuring-sriov-qinq-support.adoc#configuring-qinq-support[Configuring QinQ support for SR-IOV enabled workloads].

[id="ocp-4-16-networking-user-managed-lb_{context}"]
==== Configuring a user-managed load balancer for on-premise infrastructure

With this release, you can configure an {product-title} cluster on any on-premise infrastructure, such as bare-metal, {vmw-first}, {rh-openstack-first}, or Nutanix, to use a user-managed load balancer in place of the default load balancer. For this configuration, you must specify `loadBalancer.type: UserManaged` in your cluster’s `install-config.yaml` file.

For more information about this feature on bare-metal infrastructure, see xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#nw-osp-services-external-load-balancer_ipi-install-installation-workflow[Services for a user-managed load balancer] in _Setting up the environment for an OpenShift installation_.

[id="ocp-4-16-detect-warning-for-iptables_{context}"]
[id="ocp-4-16-detect-warning-for-iptables"]
==== Detect and warning for iptables
With this release, if you have pods in your cluster using `iptables` rules the following event message is given to warn against future deprecation:
`This pod appears to have created one or more iptables rules. IPTables is deprecated and will no longer be available in RHEL 10 and later. You should consider migrating to another API such as nftables or eBPF.`

For more information, see link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_firewalls_and_packet_filters/getting-started-with-nftables_firewall-packet-filters#doc-wrapper[Getting started with nftables]. If you are running third-party software, check with your vendor to ensure they will have an `nftables` based version available soon.

[id="ocp-4-16-networking-ingress-network-flow_{context}"]
==== Ingress network flows for {product-title} services

With this release, you can view the ingress network flows for {product-title} services. You can use this information to manage ingress traffic for your network and improve network security.

For more information, see xref:../installing/install_config/configuring-firewall.adoc#network-flow-matrix_configuring-firewall[{product-title} network flow matrix].

[id="ocp-4-16-networking-pathing-existing-dual-stack"]
==== Patching an existing dual-stack network

With this release, you can add IPv6 virtual IPs (VIPs) for API and Ingress services to an existing dual-stack-configured cluster by patching the cluster infrastructure.

If you have already upgraded your cluster to {product-title} {product-version} and you need to convert the single-stack cluster network to a dual-stack cluster network, you must specify the following for your cluster in the YAML configuration patch file:

* An IPv4 network for API and Ingress services on the first `machineNetwork` configuration.
* An IPv6 network for API and Ingress services on the second `machineNetwork` configuration.

For more information, see xref:../networking/ovn_kubernetes_network_provider/converting-to-dual-stack.adoc#nw-dual-stack-convert_converting-to-dual-stack[Converting to a dual-stack cluster network] in _Converting to IPv4/IPv6 dual-stack networking_.

[id="ocp-4-16-networking-metallb-frr-k8s_{context}"]
==== Integration of MetalLB and FRR-K8s (Technology Preview)

This release introduces `FRR-K8s`, a Kubernetes based `DaemonSet` that exposes a subset of the `FRR` API in a Kubernetes-compliant manner.
As a cluster administrator, you can use the `FRRConfiguration` custom resource (CR) to configure the MetalLB Operator to use the `FRR-K8s` daemon set as the backend.
You can use this to operate FRR services, such as receiving routes.

For more information, see xref:../networking/metallb/metallb-frr-k8s.adoc#metallb-configure-frr-k8s[Configuring the integration of MetalLB and FRR-K8s].

[id="ocp-4-16-anp"]
=== AdminNetworkPolicy is generally available
This feature provides two new APIs, `AdminNetworkPolicy` (ANP) and `BaselineAdminNetworkPolicy` (BANP). Before namespaces are created, cluster Administrators can use ANP and BANP to apply cluster-scoped network policies and safeguards for an entire cluster. Because it is cluster scoped, ANP provides Administrators a solution to manage the security of their network at scale without having to duplicate their network policies on each namespace.

For more information, see xref:../networking/ovn_kubernetes_network_provider/converting-to-dual-stack.adoc#nw-dual-stack-convert_converting-to-dual-stack[Converting to a dual-stack cluster network] in _Converting to IPv4/IPv6 dual-stack networking_.

[id="ocp-4-16-networking-sdn-ovnk-live-migration_{context}"]
==== Live migration to the OVN-Kubernetes network plugin

Previously, when migrating from OpenShift SDN to OVN-Kubernetes, the only available option was an _offline_ migration method. This process included some downtime, during which clusters were unreachable.

This release introduces a _live_ migration method. The live migration method is the process in which the OpenShift SDN network plugin and its network configurations, connections, and associated resources are migrated to the OVN-Kubernetes network plugin without service interruption. It is available for {product-title}, {product-dedicated}, {product-rosa}, and Azure Red Hat OpenShift deployment types. It is not available for HyperShift deployment types. This migration method is valuable for deployment types that require constant service availability and offers the following benefits:

* Continuous service availability

* Minimized downtime

* Automatic node rebooting

* Seamless transition from the OpenShift SDN network plugin to the OVN-Kubernetes network plugin

Migration to OVN-Kubernetes is intended to be a one-way process.

For more information, see xref:../networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc#nw-ovn-kubernetes-live-migration-about_migrate-from-openshift-sdn[Live migration to the OVN-Kubernetes network plugin overview].

[id="ocp-4-16-networking-changing-ovn-kubernetes-network-plugin-internal-range_{context}"]
==== Support for changing the OVN-Kubernetes network plugin internal IP address ranges

If you use the OVN-Kubernetes network plugin, you can configure the transit, join, and masquerade subnets. The transit and join subnets can be configured either during cluster installation or after. The masquerade subnet must be configured during installation and cannot be changed after. The subnet defaults are:

- Transit subnet: `100.88.0.0/16` and `fd97::/64`
- Join subnet: `100.64.0.0/16` and `fd98::/64`
- Masquerade subnet: `169.254.169.0/29` and `fd69::/125`

For more information about these configuration fields, see xref:../networking/cluster-network-operator.adoc#nw-operator-cr-cno-object_cluster-network-operator[Cluster Network Operator configuration object]. For more information about configuring the transit and join subnets on an existing cluster, see Configure OVN-Kubernetes internal IP address subnets.

[id="ocp-4-16-registry_{context}"]
=== Registry

[id="ocp-4-16-storage_{context}"]
=== Storage

[id="ocp-4-16-storage-secrets-store-csi-driver-vault_{context}"]
==== HashiCorp Vault is now available for the {secrets-store-operator} (Technology Preview)
You can now use the {secrets-store-operator} to mount secrets from HashiCorp Vault to a CSI volume in {product-title}. The {secrets-store-operator} is available as a Technology Preview feature.

For the full list of available secrets store providers, see xref:../nodes/pods/nodes-pods-secrets-store.adoc#secrets-store-providers_nodes-pods-secrets-store[Secrets store providers].

For information about using the {secrets-store-operator} to mount secrets from HashiCorp Vault, see xref:../nodes/pods/nodes-pods-secrets-store.adoc#secrets-store-vault_nodes-pods-secrets-store[Mounting secrets from HashiCorp Vault].

[id="ocp-4-16-storage-clone-azure-file"]
==== Volume cloning supported for Azure File (Technology Preview)
{product-title} 4.16 introduces volume cloning for the Microsoft Azure File Container Storage Interface (CSI) Driver Operator as a Technology Preview feature. Volume cloning duplicates an existing persistent volume (PV) to help protect against data loss in {product-title}. You can also use a volume clone just as you would use any standard volume.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-azure-file.adoc#persistent-storage-csi-azure-file[Azure File CSI Driver Operator] and xref:..//storage/container_storage_interface/persistent-storage-csi-cloning.adoc#persistent-storage-csi-cloning[CSI volume cloning].

[id="ocp-4-16-storage-node-expansion-secret"]
==== Node Expansion Secret is generally available
The Node Expansion Secret feature allows your cluster to expand storage of mounted volumes, even when access to those volumes requires a secret (for example, a credential for accessing a Storage Area Network (SAN) fabric) to perform the node expand operation. {product-title} 4.16 supports this feature as generally available.

[id="ocp-4-16-vsphere-max-snapshot"]
==== Changing vSphere CSI maximum number of snapshots is generally available
The default maximum number of snapshots in VMWare vSphere Container Storage Interface (CSI) is 3 per volume. In {product-title} 4.16, you can now change this maximum number of snapshots to a maximum of 32 per volume. You also have granular control of the maximum number of snapshots for vSAN and Virtual Volume datastores. {product-title} 4.16 supports this feature as generally available.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#vsphere-change-max-snapshot_persistent-storage-csi-vsphere[Changing the maximum number of snapshots for vSphere].

[id="ocp-4-16-pv-last-phase-transition-time"]
==== Persistent volume last phase transition time parameter (Technology Preview)
In {product-title} 4.16 introduces a new parameter, `LastPhaseTransitionTime`, which has a timestamp that is updated every time a persistent volume (PV) transitions to a different phase (`pv.Status.Phase`). This feature is being released with Technology Preview status.

[id="ocp-4-16-storage-csi-smb-cifs-driver-operator"]
==== Persistent storage using CIFS/SMB CSI Driver Operator (Technology Preview)
{Product-title} is capable of provisioning persistent volumes (PVs) with a Container Storage Interface (CSI) driver for the Common Internet File System (CIFS) dialect/Server Message Block (SMB) protocol. The CIFS/SMB CSI Driver Operator that manages this driver is in Technology Preview status.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-smb-cifs.adoc#persistent-storage-csi-smb-cifs[CIFS/SMB CSI Driver Operator].

[id="ocp-4-16-storage-rwop-selinux-context-mode"]
==== RWOP with SELinux context mount is generally available
{product-title} 4.14 introduced a new access mode with Technical Preview status for persistent volumes (PVs) and persistent volume claims (PVCs) called ReadWriteOncePod (RWOP). RWOP can be used only in a single pod on a single node compared to the existing ReadWriteOnce access mode where a PV or PVC can be used on a single node by many pods. If the driver enables it, RWOP uses the SELinux context mount set in the `PodSpec` or container, which allows the driver to mount the volume directly with the correct SELinux labels. This eliminates the need to recursively relabel the volume, and pod startup can be significantly faster.

In {product-title} 4.16, this feature is generally available.

For more information, see xref:../storage/understanding-persistent-storage.adoc#pv-access-modes_understanding-persistent-storage[Access modes].

[id="ocp-4-16-storage-csi-vsphere-compliance-check"]
==== vSphere CSI Driver 3.1 updated CSI topology requirements
To support VMware vSphere Container Storage Interface (CSI) volume provisioning and usage in multi-zonal clusters, the deployment should match certain requirements imposed by CSI driver. These requirements have changed starting with 3.1.0, and although {product-title} 4.16 accepts both the old and new tagging methods, you should use the new tagging method since VMware considers the old way an invalid configuration; thus to prevent problems, you should not use the old tagging method.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#vsphere-csi-topology-requirements[vSphere CSI topology requirements].

[id="ocp-4-16-oci"]

[id="ocp-4-16-storage-thick-provisioning_{context}"]
==== Support for configuring thick-provisioned storage

This feature provides support for configuring thick-provisioned storage. If you exclude the `deviceClasses.thinPoolConfig` field in the `LVMCluster` custom resource (CR), logical volumes are thick provisioned.
Using thick-provisioned storage includes the following limitations:

* No copy-on-write support for volume cloning.
* No support for `VolumeSnapshotClass`. Therefore, CSI snapshotting is not supported.
* No support for over-provisioning. As a result, the provisioned capacity of PersistentVolumeClaims (PVCs) is immediately reduced from the volume group.
* No support for thin metrics. Thick-provisioned devices only support volume group metrics.

For information about configuring the `LVMCluster` CR, see xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#about-lvmcluster_logical-volume-manager-storage[About the LVMCluster custom resource].

[id="ocp-4-16-storage-no-deviceselector-warning-message_{context}"]
==== Support for a new warning message when device selector is not configured in the LVMCluster custom resource

This update provides a new warning message when you do not configure the `deviceSelector` field in the `LVMCluster` custom resource (CR).

The `LVMCluster` CR supports a new field, `deviceDiscoveryPolicy`, which indicates whether the `deviceSelector` field is configured. If you do not configure the `deviceSelector` field, {lvms} automatically sets the `deviceDiscoveryPolicy` field to `RuntimeDynamic`. Otherwise, the `deviceDiscoveryPolicy` field is set to `Preconfigured`.

It is not recommended to exclude the `deviceSelector` field from the `LMVCluster` CR. For more information about the limitations of not configuring the `deviceSelector` field, see xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#about-adding-devices-to-a-vg_logical-volume-manager-storage[About adding devices to a volume group].

[id="ocp-4-16-storage-adding-encrypted-devices-to-vg_{context}"]
==== Support for adding encrypted devices to a volume group

This feature provides support for adding encrypted devices to a volume group. You can enable disk encryption on the cluster nodes during an {product-title} installation. After encrypting a device, you can specify the path to the LUKS encrypted device in the `deviceSelector` field in the `LVMCluster` custom resource. For information about disk encryption, xref:../installing/install_config/installing-customizing.adoc#installation-special-config-encrypt-disk_installing-customizing[About disk encryption] and xref:../installing/install_config/installing-customizing.adoc#installation-special-config-storage-procedure_installing-customizing[Configuring disk encryption and mirroring].

For more information about adding devices to a volume group, see xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#about-adding-devices-to-a-vg_logical-volume-manager-storage[About adding devices to a volume group].


[id="ocp-4-16-oci_{context}"]
=== Oracle(R) Cloud Infrastructure

[id="ocp-4-16-olm_{context}"]
=== Operator lifecycle

[id="ocp-4-16-olm-improved-status-conditions_{context}"]
==== Improved status condition messages and deprecation notices for cluster extensions in {olmv1-first} (Technology Preview)

With this release, {olmv1} displays the following status condition messages for installed cluster extensions:

* Specific bundle name
* Installed version
* Improved health reporting
* Deprecation notices for packages, channels, and bundles

[id="ocp-4-16-osdk_{context}"]
=== Operator development

[id="ocp-4-16-builds_{context}"]
=== Builds

[id="ocp-4-16-machine-config-operator_{context}"]
=== Machine Config Operator

[id="ocp-4-16-machine-config-operator-gc_{context}"]
==== Garbage collection of unused rendered machine configs

With this release, you can now garbage collect unused rendered machine configs. By using the `oc adm prune renderedmachineconfigs` command, you can view the unused rendered machine configs, determine which to remove, then batch delete the rendered machine configs that you no longer need. Having too many machine configs can make working with the machine configs confusing and can also contribute to disk space and performance issues. For more information, see xref:../post_installation_configuration/machine-configuration-tasks.adoc#machineconfig-garbage-collect_post-install-machine-configuration-tasks[Managing unused rendered machine configs].

[id="ocp-4-16-machine-config-operator-node-disruption_{context}"]
==== Node disruption policies

By default, when you make certain changes to the parameters in a `MachineConfig` object, the Machine Config Operator (MCO) drains and reboots the nodes associated with that machine config. However, you can create a node disruption policy in the MCO namespace that defines a set of Ignition config objects changes that would require little or no disruption to your workloads. For more information, see xref:../post_installation_configuration/machine-configuration-tasks.adoc#machine-config-node-disruption_post-install-machine-configuration-tasks[Understanding node restart behaviors after machine config changes].

[id="ocp-4-16-machine-config-operator-on-cluster_{context}"]
==== On-cluster {op-system} image layering

With {op-system-first} image layering, you can now automatically build the custom layered image directly in your cluster, as a Technology Preview feature. Previously, you needed to build the custom layered image outside of the cluster, then pull the image into the cluster. The image layering feature allows you to extend the functionality of your base {op-system} image by layering additional images onto the base image. For more information, see xref:../post_installation_configuration/coreos-layering.adoc#coreos-layering[RHCOS image layering].

[id="ocp-4-16-machine-config-operator-boot-image_{context}"]
==== Updating boot images

By default, the MCO does not delete the boot image it uses to bring up a {op-system-first} node. Consequently, the boot image in your cluster is not updated along with your cluster. You can now configure your cluster to update the boot image whenever you update your cluster. For more information, see xref:../post_installation_configuration/machine-configuration-tasks.adoc#mco-update-boot-images_post-install-machine-configuration-tasks[Updating boot images].

[id="ocp-4-16-machine-management_{context}"]
=== Machine management

[id="ocp-4-16-cluster-autoscaler-expanders_{context}"]
==== Configuring expanders for the cluster autoscaler

With this release, the cluster autoscaler can use the `LeastWaste`, `Priority`, and `Random` expanders.
You can configure these expanders to influence the selection of machine sets when scaling the cluster.
For more information, see xref:../machine_management/applying-autoscaling.adoc#configuring-clusterautoscaler_applying-autoscaling[Configuring the cluster autoscaler].

[id="ocp-4-16-capi-tp-vmw_{context}"]
==== Managing machines with the Cluster API for {vmw-full} (Technology Preview)

This release introduces the ability to manage machines by using the upstream Cluster API, integrated into {product-title}, as a Technology Preview for {vmw-full} clusters.
This capability is in addition or an alternative to managing machines with the Machine API.
For more information, see xref:../machine_management/cluster_api_machine_management/cluster-api-about.adoc#cluster-api-about[About the Cluster API].

[id="ocp-4-16-cpms-fd-vmw_{context}"]
==== Defining a {vmw-short} failure domain for a control plane machine set

With this release, the previously Technology Preview feature of defining a {vmw-short} failure domain for a control plane machine set is Generally Available.
For more information, see xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-vsphere.adoc#cpmso-config-options-vsphere[Control plane configuration options for VMware vSphere]

[id="ocp-4-16-nodes_{context}"]
=== Nodes

[id="ocp-4-16-nodes-move-vpa-pods_{context}"]
==== Moving the Vertical Pod Autoscaler Operator pods

The Vertical Pod Autoscaler Operator (VPA) consists of three components: the recommender, updater, and admission controller. The Operator and each component has its own pod in the VPA namespace on the control plane nodes. You can move the VPA Operator and component pods to infrastructure or worker nodes. For more information, see xref:../nodes/pods/nodes-pods-vertical-autoscaler.adoc#infrastructure-moving-vpa_nodes-pods-vertical-autoscaler[Moving the Vertical Pod Autoscaler Operator components].

[id="ocp-4-16-must-gather-directory_{context}"]
==== Additional information collected by must-gather

With this release, the `oc adm must-gather` command collects the following additional information:

* OpenShift CLI (`oc`) binary version
* Must-gather logs

These additions help identify issues that might stem from using a specific version of `oc`. The `oc adm must-gather` command also lists what image was used and if any data could not be gathered in the must-gather logs.

For more information, see xref:../support/gathering-cluster-data.adoc#about-must-gather_gathering-cluster-data[About the must-gather tool].

[id="ocp-4-16-upgrading-or-downgrading-host-firmware_{context}"]
==== Upgrading or downgrading host firmware

Beginning with {product-title} {product-version}, you can upgrade or downgrade the firmware for a bare-metal node to a specific version. Metal^3^ provides the `HostFirmwareComponents` resource, which describes BIOS and baseboard management controller (BMC) firmware versions. Upgrading or downgrading firmware is useful when deploying an {product-title} cluster on bare metal with validated patterns that have been tested against specific firmware versions. See xref:../post_installation_configuration/bare-metal-configuration.adoc#about-the-hostfirmwarecomponents-resource_post-install-bare-metal-configuration[About the HostFirmwareComponents resource] for additional details.

[id="ocp-4-16-editing-the-baremetalhost-resource_{context}"]
==== Editing the BareMetalHost resource

In {product-title} {product-version} and later, you can edit the baseboard management controller (BMC) address in the `BareMetalHost` resource of a bare-metal node. The node must be in the `Provisioned`, `ExternallyProvisioned`, `Registering`, or `Available` state. Editing the BMC address in the `BareMetalHost` resource will not result in deprovisioning the node. See xref:../post_installation_configuration/bare-metal-configuration.adoc#editing-a-baremetalhost-resource_post-install-bare-metal-configuration[Editing a BareMetalHost resource] for additional details.

[id="ocp-4-16-monitoring_{context}"]
=== Monitoring

The in-cluster monitoring stack for this release includes the following new and modified features.

[id="ocp-4-16-monitoring-updates-to-monitoring-stack-components-and-dependencies"]
==== Updates to monitoring stack components and dependencies
This release includes the following version updates for in-cluster monitoring stack components and dependencies:

* kube-state-metrics to 2.12.0
* Metrics Server to 0.7.1
* node-exporter to 1.8.0
* Prometheus to 2.52.0
* Prometheus Operator to 0.73.2
* Thanos to 0.35.0

[id="ocp-4-16-monitoring-changes-to-alerting-rules"]
==== Changes to alerting rules

[NOTE]
====
Red{nbsp}Hat does not guarantee backward compatibility for recording rules or alerting rules.
====

* Added the `ClusterMonitoringOperatorDeprecatedConfig` alert to monitor when the Cluster Monitoring Operator configuration uses a deprecated field.
* Added the `PrometheusOperatorStatusUpdateErrors` alert to monitor when the Prometheus Operator fails to update object status.

[id="ocp-4-16-monitoring-metrics-server-to-access-metrics-api-ga"]
==== Metrics Server component to access the Metrics API general availability (GA)

Metrics Server component is now generally available and automatically installed instead of the deprecated Prometheus Adapter. Metrics Server collects resource metrics and exposes them in the `metrics.k8s.io` Metrics API service for use by other tools and APIs, which frees the core platform Prometheus stack from handling this functionality. For more information, see xref:../observability/monitoring/config-map-reference-for-the-cluster-monitoring-operator.adoc#metricsserverconfig[MetricsServerConfig] in the config map API reference for the Cluster Monitoring Operator.

[id="ocp-4-16-monitoring-new-role-to-allow-read-only-access-to-alertmanager-api"]
==== New monitoring role to allow read-only access to the Alertmanager API

This release introduces a new `monitoring-alertmanager-view` role to allow read-only access to the Alertmanager API in the `openshift-monitoring` project.

[id="ocp-4-16-monitoring-vpa-metrics-available-in-kube-state-metrics-agent"]
==== VPA metrics are available in the kube-state-metrics agent

Vertical Pod Autoscaler (VPA) metrics are now available through the `kube-state-metrics` agent. VPA metrics follow a similar exposition format like they did before being deprecated and removed from native support upstream.

[id="ocp-4-16-monitoring-change-in-proxy-service-for-monitoring-components"]
==== Change in proxy service for monitoring components
With this release, the proxy service in front of Prometheus, Alertmanager, and Thanos Ruler has been updated from OAuth to `kube-rbac-proxy`. This change might affect service accounts and users accessing these API endpoints without the appropriate roles and cluster roles.

[id="ocp-4-16-monitoring-change-in-how-prometheus-handles-duplicate-samples"]
==== Change in how Prometheus handles duplicate samples
With this release, when Prometheus scrapes a target, duplicate samples are no longer silently ignored, even if they have the same value. The first sample is accepted and the `prometheus_target_scrapes_sample_duplicate_timestamp_total` counter is incremented, which might trigger the `PrometheusDuplicateTimestamps` alert.

[id="ocp-4-16-network-observability-1-5_{context}"]
=== Network Observability Operator
The Network Observability Operator releases updates independently from the {product-title} minor version release stream. Updates are available through a single, Rolling Stream which is supported on all currently supported versions of {product-title} 4. Information regarding new features, enhancements, and bug fixes for the Network Observability Operator is found in the xref:../observability/network_observability/network-observability-operator-release-notes.adoc#network-observability-rn[Network Observability release notes].

[id="ocp-4-16-scalability-and-performance_{context}"]
=== Scalability and performance

[id="workload-partitioning-enhancement_{context}"]
==== Workload partitioning enhancement
With this release, platform pods deployed with a workload annotation that includes both CPU limits and CPU requests will have the CPU limits accurately calculated and applied as a CPU quota for the specific pod. In prior releases, if a workload partitioned pod had both CPU limits and requests set, they were ignored by the webhook. The pod did not benefit from workload partitioning and was not locked down to specific cores. This update ensures the requests and limits are now interpreted correctly by the webhook.

[NOTE]
====
It is expected that if the values for CPU limits are different from the value for requests in the annotation, the CPU limits are taken as being the same as the requests.
====

For more information, see xref:../scalability_and_performance/enabling-workload-partitioning.adoc#enabling-workload-partitioning[Workload partitioning].

[id="ocp-4-16-nodes-cgroupv2-default_{context}"]
==== Linux Control Groups version 2 is now supported with the performance profile feature

Beginning with {product-title} 4.16, Control Groups version 2 (cgroup v2), also known as cgroup2 or cgroupsv2, is enabled by default for all new deployments, even when performance profiles are present.

Since {product-title} 4.14, cgroups v2 has been the default, but the performance profile feature required the use of cgroups v1. This issue has been resolved.

cgroup v1 is still used in upgraded clusters with performance profiles that have initial installation dates before {product-title} 4.16. cgroup v1 can still be used in the current version by changing the `cgroupMode` field in the `node.config` object to `v1`.

For more information, see xref:../nodes/clusters/nodes-cluster-cgroups-2.adoc#nodes-clusters-cgroups-2[Configuring the Linux cgroup version on your nodes].

[id="ocp-4-16-etcd-increase-db-size_{context}"]
==== Support for increasing the etcd database size (Technology Preview)

With this release, you can increase the disk quota in etcd. This is a Technology Preview feature. For more information, see xref:../scalability_and_performance/recommended-performance-scale-practices/recommended-etcd-practices.adoc#etcd-increase-db_recommended-etcd-practices[Increasing the database size for etcd].

[id="ocp-4-16-reserved-core-freq-tuning_{context}"]
====  Reserved core frequency tuning

With this release, the Node Tuning Operator supports setting CPU frequencies in the `PerformanceProfile` for reserved and isolated core CPUs.
This is an optional feature that you can use to define specific frequencies.
The Node Tuning Operator then sets those frequencies by enabling the `intel_pstate` CPUFreq driver in the Intel hardware.
You must follow Intel's recommendations on frequencies for FlexRAN-like applications, which require the default CPU frequency to be set to a lower value than the default running frequency.

[id="ocp-4-16-node-tuning-operator-intel_{context}"]
====  Node Tuning Operator intel_pstate driver default setting

Previously, for the RAN DU-profile, setting the `realTime` workload hint to `true` in the `PerformanceProfile` always disabled the `intel_pstate`.
With this release, the Node Tuning Operator detects the underlying Intel hardware using `TuneD` and appropriately sets the `intel_pstate` kernel parameter based on the processor’s generation.
This decouples the `intel_pstate` from the `realTime` and `highPowerConsumption` workload hints.
The `intel_pstate` now depends only on the underlying processor generation.

For pre-IceLake processors, the `intel_pstate` is deactivated by default, whereas for IceLake and later generation processors, the `intel_pstate` is set to `active`.

[id="ocp-4-16-edge-computing_{context}"]
=== Edge computing

[id="ocp-4-16-edge-computing_pg-ztp-rhacm_{context}"]
==== Using {rh-rhacm} PolicyGenerator resources to manage {ztp} cluster policies (Technology Preview)

You can now use `PolicyGenerator` resources and {rh-rhacm-first} to deploy polices for managed clusters with {ztp}.
The `PolicyGenerator` API is part of the link:https://open-cluster-management.io/[Open Cluster Management] standard and provides a generic way of patching resources which is not possible with the `PolicyGenTemplate` API.
Using `PolicyGenTemplate` resources to manage and deploy polices will be deprecated in an upcoming OpenShift Container Platform release.

For more information, see xref:../edge_computing/policygenerator_for_ztp/ztp-configuring-managed-clusters-policygenerator.adoc#ztp-configuring-managed-clusters-policygenerator[Configuring managed cluster policies by using PolicyGenerator resources].

[NOTE]
====
The `PolicyGenerator` API does not currently support merging patches with custom Kubernetes resources that contain lists of items. For example, in `PtpConfig` CRs.
====

[id="ocp-4-16-edge-computing-talm-enfore-policies_{context}"]
==== {cgu-operator} policy remediation

With this release, {cgu-operator-first} uses a {rh-rhacm-first} feature to remediate `inform` policies on managed clusters. This enhancement removes the need for the Operator to create `enforce` copies of `inform` policies during policy remediation. This enhancement also reduces the workload on the hub cluster due to copied policies, and can reduce the overall time required to remediate policies on managed clusters.

For more information, see xref:../edge_computing/cnf-talm-for-cluster-upgrades.adoc#talo-policies-concept_cnf-topology-aware-lifecycle-manager[Update policies on managed clusters].

[id="ocp-4-16-edge-computing-accelerated-ztp_{context}"]
==== Accelerated provisioning of {ztp} (Technology Preview)

With this release, you can reduce the time taken for cluster installation by using accelerated provisioning of {ztp} for {sno}.
Accelerated ZTP speeds up installation by applying Day 2 manifests derived from policies at an earlier stage.

The benefits of accelerated provisioning of {ztp} increase with the scale of your deployment.
Full acceleration gives more benefit on a larger number of clusters.
With a smaller number of clusters, the reduction in installation time is less significant.

For more information, see xref:../edge_computing/ztp-deploying-far-edge-sites.adoc#ztp-sno-accelerated-ztp_ztp-deploying-far-edge-sites[Accelerated provisioning of {ztp}].

[id="ocp-4-16-edge-computing-image-based-upgrade_{context}"]
==== Image-based upgrade for {sno} clusters using {lcao}

With this release, you can use the {lcao} to orchestrate an image-based upgrade for {sno} clusters from {product-title} <4.y> to <4.y+2>, and <4.y.z> to <4.y.z+n>.
The {lcao} generates an OCI image that matches the configuration of participating clusters.
In addition to the OCI image, the image-based upgrade uses the `ostree` library and the OADP Operator to reduce upgrade and service outage duration when transitioning between the original and target platform versions.

For more information, see xref:../edge_computing/image_based_upgrade/cnf-understanding-image-based-upgrade.adoc#understanding-image-based-upgrade-for-sno[Understanding the image-based upgrade for {sno} clusters].

[id="ocp-4-16-edge-computing-ipsec-encryption-for-managed-clusters-ztp_{context}"]
==== Deploying IPsec encryption to managed clusters with {ztp} and {rh-rhacm} (Technology Preview)

You can now enable IPsec encryption in managed {sno} clusters that you deploy with {ztp} and {rh-rhacm-first}.
You can encrypt external traffic between pods and IPsec endpoints external to the managed cluster.
All pod-to-pod network traffic between nodes on the OVN-Kubernetes cluster network is encrypted with IPsec in Transport mode.

For more information, see xref:../edge_computing/ztp-deploying-far-edge-sites.adoc#ztp-configuring-ipsec-using-ztp-and-siteconfig_ztp-deploying-far-edge-sites[Configuring IPsec encryption for {sno} clusters using {ztp} and SiteConfig resources].

[id="ocp-4-16-hcp_{context}"]
=== Hosted control planes

[id="ocp-4-16-insights-operator_{context}"]
=== Insights Operator

[id="ocp-4-16-etcd-certificates_{context}"]
=== Security

A new signer certificate authority (CA), `openshift-etcd`, is now available to sign certificates. It is contained in a trust bundle with the existing CA. Two CA secrets,`etcd-signer` and `etcd-metric-signer`, are also available for rotation. Starting with this release, all certificates will move to a proven library. This change allows for the automatic rotation of all certificates that were not managed by `cluster-etcd-operator`. All node-based certificates will continue with the current update process.

[id="ocp-4-16-notable-technical-changes_{context}"]
== Notable technical changes

{product-title} {product-version} introduces the following notable technical changes.

// Note: use [discrete] for these sub-headings.
// example sub-heading below:
//[discrete]
//[id="ocp-4-16-cluster-cloud-controller-manager-operator"]
//=== Cloud controller managers for additional cloud providers
//

[discrete]
[id="ocp-4-16-HAProxy-version_{context}"]
=== HAProxy version 2.8

{product-title} {product-version} uses HAProxy 2.8.

[discrete]
[id="ocp-4-16-sha-haproxy-support-removed_{context}"]
=== SHA-1 certificates no longer supported for use with HAProxy

SHA-1 certificates are no longer supported for use with HAProxy. Both existing and new routes using SHA-1 certificates in {product-title} 4.16 are rejected and no longer function. For more information about creating secure routes, see xref:../networking/routes/secured-routes.html[Secured Routes].

[discrete]
[id="ocp-4-16-etcd-tuning-profiles_{context}"]
=== etcd tuning parameters

With this release, the etcd tuning parameters can be set to values that optimize performance and decrease latency, as follows.

* `""` (Default)
* `Standard`
* `Slower`

[discrete]
[id="ocp-4-16-remove-dasd-artifact_{context}"]
=== {op-system} `dasd` image artifacts no longer supported on {ibm-z-name} and {ibm-linuxone-name} (`s390x`)

With this release, `dasd` image artifacts for the `s390x` architecture are removed from the {product-title} image building pipeline. You can still use the `metal4k` image artifact, which is identical and contains the same functionality.

[discrete]
[id="egress-ip-etp-local-support_{context}"]
=== Support for EgressIP with ExternalTrafficPolicy=Local services

Previously, it was unsupported for EgressIP selected pods to also serve as backends for services with `externalTrafficPolicy` set to `Local`. When attempting this configuration, service ingress traffic reaching the pods was incorrectly rerouted to the egress node hosting the EgressIP. This affected how responses to incoming service traffic connections were handled and led to non-functional services when `externalTrafficPolicy` was set to `Local`, as connections were dropped and the service became unavailable.

With {product-title} {product-version}, OVN-Kubernetes now supports the use of `ExternalTrafficPolicy=Local` services and EgressIP configurations at the same time on the same set of selected pods. OVN-Kubernetes now only reroutes the traffic originating from the EgressIP pods towards the egress node while routing the responses to ingress service traffic from the EgressIP pods via the same node where the pod is located.

[discrete]
[id="ocp-4-16-olm-ce-rename_{context}"]
=== Operator API renamed to ClusterExtension

Earlier Technology Preview phases of {olmv1-first} introduced a new `Operator` API, provided as `operator.operators.operatorframework.io` by the Operator Controller component. In {product-title} 4.16, this API is renamed `ClusterExtension`, provided as `clusterextension.olm.operatorframework.io`, for this Technology Preview phase of {olmv1}.

This API still streamlines management of installed extensions, which includes Operators via the `registry+v1` bundle format, by consolidating user-facing APIs into a single object. The rename to `ClusterExtension` addresses the following:

* More accurately reflects the simplified functionality of extending a cluster's capabilities
* Better represents a more flexible packaging format
* `Cluster` prefix clearly indicates that `ClusterExtension` objects are cluster-scoped, a change from {olmv0} where Operators could be either namespace-scoped or cluster-scoped

[discrete]
[id="ocp-4-16-legacy-sa-tokens_{context}"]
=== Legacy service account API token secrets are no longer generated for each service account

Prior to {product-title} 4.16, when the integrated {product-registry} was enabled, a legacy service account API token secret was generated for every service account in the cluster. Starting with {product-title} 4.16, when the integrated {product-registry} is enabled, the legacy service account API token secret is no longer generated for each service account.

Additionally, when the integrated {product-registry} is enabled, the image pull secret generated for every service account no longer uses a legacy service account API token. Instead, the image pull secret now uses a bound service account token that is automatically refreshed before it expires.

For more information, see xref:../nodes/pods/nodes-pods-secrets.adoc#auto-generated-sa-token-secrets_nodes-pods-secrets[Automatically generated image pull secrets].

For information about detecting legacy service account API token secrets that are in use in your cluster or deleting them if they are not needed, see the Red Hat Knowledgebase article link:https://access.redhat.com/articles/7058801[Long-lived service account API tokens in OpenShift Container Platform].

[discrete]
[id="ocp-4-16-support-ext-cloud-auth_{context}"]
=== Support for external cloud authentication providers

In this release, the functionality to authenticate to private registries on {aws-first}, {gcp-first}, and {azure-first} clusters is moved from the in-tree provider to binaries that ship with {product-title}.
This change supports the default external cloud authentication provider behavior that is introduced in Kubernetes 1.29.

[id="ocp-4-16-deprecated-removed-features_{context}"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* _General Availability_
* _Deprecated_
* _Removed_

[discrete]
[id="ocp-4-16-operators-dep-rem_{context}"]
=== Operator lifecycle and development deprecated and removed features

.Operator lifecycle and development deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Operator SDK
|General Availability
|General Availability
|Deprecated

|Scaffolding tools for Ansible-based Operator projects
|General Availability
|General Availability
|Deprecated

|Scaffolding tools for Helm-based Operator projects
|General Availability
|General Availability
|Deprecated

|Scaffolding tools for Go-based Operator projects
|General Availability
|General Availability
|Deprecated

|Scaffolding tools for Hybrid Helm-based Operator projects
|Technology Preview
|Technology Preview
|Deprecated

|Scaffolding tools for Java-based Operator projects
|Technology Preview
|Technology Preview
|Deprecated

|Platform Operators
|Technology Preview
|Technology Preview
|Removed

|Plain bundles
|Technology Preview
|Technology Preview
|Removed

|SQLite database format for Operator catalogs
|Deprecated
|Deprecated
|Deprecated
|====

[discrete]
[id="ocp-4-16-images-dep-rem_{context}"]
=== Images deprecated and removed features

.Cluster Samples Operator deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Cluster Samples Operator
|General Availability
|General Availability
|Deprecated

|====

[discrete]
[id="ocp-4-16-monitoring-dep-rem_{context}"]
=== Monitoring deprecated and removed features

.Monitoring deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|`dedicatedServiceMonitors` setting that enables dedicated service monitors for core platform monitoring
|General Availability
|Deprecated
|Removed

|`prometheus-adapter` component that queries resource metrics from Prometheus and exposes them in the metrics API.
|General Availability
|Deprecated
|Removed

|====

[discrete]
[id="ocp-4-16-install-dep-rem_{context}"]
=== Installation deprecated and removed features

.Installation deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|OpenShift SDN network plugin
|Deprecated
|Removed ^[1]^
|Removed

|`--cloud` parameter for `oc adm release extract`
|Deprecated
|Deprecated
|Deprecated

|CoreDNS wildcard queries for the `cluster.local` domain
|Deprecated
|Deprecated
|Deprecated

|`compute.platform.openstack.rootVolume.type` for {rh-openstack}
|Deprecated
|Deprecated
|Deprecated

|`controlPlane.platform.openstack.rootVolume.type` for {rh-openstack}
|Deprecated
|Deprecated
|Deprecated

|`ingressVIP` and `apiVIP` settings in the `install-config.yaml` file for installer-provisioned infrastructure clusters
|Deprecated
|Deprecated
|Deprecated

|Package-based {op-system-base} compute machines
|General Availability
|General Availability
|Deprecated

|`platform.aws.preserveBootstrapIgnition` parameter for Amazon Web Services (AWS)
|General Availability
|General Availability
|Deprecated

|====
[.small]
--
1. While the OpenShift SDN network plugin is no longer supported by the installation program in version 4.15, you can upgrade a cluster that uses the OpenShift SDN plugin from version 4.14 to version 4.15.
--
[discrete]
[id="ocp-4-16-clusters-dep-rem_{context}"]
=== Updating clusters deprecated and removed features

.Updating clusters deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|====

[discrete]
=== Storage deprecated and removed features

.Storage deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Persistent storage using FlexVolume
|Deprecated
|Deprecated
|Deprecated

|AliCloud Disk CSI Driver Operator
|General Availability
|General Availability
|Removed

|====

[discrete]
[id="ocp-4-16-hardware-an-driver-dep-rem_{context}"]
=== Specialized hardware and driver enablement deprecated and removed features

.Specialized hardware and driver enablement deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|====

[discrete]
[id="ocp-4-16-networking-dep-rem_{context}"]
=== Networking deprecated and removed features

.Networking deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Kuryr on {rh-openstack}
|Deprecated
|Removed
|Removed

|OpenShift SDN network plugin
|Deprecated
|Deprecated
|Deprecated

|iptables
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
[id="ocp-4-16-web-console-dep-rem_{context}"]
=== Web console deprecated and removed features

.Web console deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Patternfly 4
|General Availability
|Deprecated
|Deprecated

|React Router 5
|General Availability
|Deprecated
|Deprecated

|====

[discrete]
[id="ocp-4-16-node-dep-rem_{context}"]
=== Node deprecated and removed features

.Node deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|`ImageContentSourcePolicy` (ICSP) objects
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/zone`
|Deprecated
|Deprecated
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/region`
|Deprecated
|Deprecated
|Deprecated

|cgroup v1
|General Availability
|General Availability
|Deprecated

|====

[discrete]
[id="ocp-4-16-cli-dep-rem_{context}"]
=== OpenShift CLI (oc) deprecated and removed features
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|====

[discrete]
[id="ocp-4-16-workloads-dep-rem_{context}"]
=== Workloads deprecated and removed features

.Workloads deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|`DeploymentConfig` objects
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
[id="ocp-4-16-bare-metal-dep-rem_{context}"]
=== Bare metal monitoring deprecated and removed features

.Bare Metal Event Relay Operator tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Bare Metal Event Relay Operator
|Technology Preview
|Deprecated
|Deprecated

|====

[id="ocp-4-16-deprecated-features_{context}"]
=== Deprecated features

[id="ocp-4-14-nodes-cgroupv1-deprecated_{context}"]
==== Linux Control Groups version 1 is now deprecated

In {op-system-base-full} 9, the default mode is cgroup v2. When {op-system-base-full} 10 is released, systemd will not support booting in the cgroup v1 mode and only cgroup v2 mode will be available. As such, cgroup v1 is deprecated in {product-title} 4.16 and later. cgroup v1 will be removed in a future {product-title} release.

[id="ocp-4-16-deprecation-samples-operator_{context}"]
==== Cluster Samples Operator

The Cluster Samples Operator is deprecated with the {product-title} 4.16 release. The Cluster Samples Operator will stop managing and providing support to the non-S2I samples (image streams and templates). No new templates, samples or non-Source-to-Image (Non-S2I) image streams will be added to the Cluster Samples Operator. However, the existing S2I builder image streams and templates will continue to receive updates until the Cluster Samples Operator is removed in a future release.

[id="ocp-4-16-rhel-worker-nodes-deprecation_{context}"]
==== Package-based {op-system-base} compute machines

With this release, installation of package-based {op-system-base} worker nodes is deprecated. In a subsequent future release, {op-system-base} worker nodes will be removed and no longer supported.

{op-system} image layering will replace this feature and supports installing additional packages on the base operating system of your worker nodes.

For more information about image layering, see xref:../post_installation_configuration/coreos-layering.adoc[{op-system} image layering].

[id="ocp-4-16-deprecation-operator-sdk_{context}"]
==== Operator SDK CLI tool and related testing and scaffolding tools are deprecated

The Red{nbsp}Hat-supported version of the Operator SDK CLI tool, including the related scaffolding and testing tools for Operator projects, is deprecated and is planned to be removed in a future release of {product-title}. Red{nbsp}Hat will provide bug fixes and support for this feature during the current release lifecycle, but this feature will no longer receive enhancements and will be removed from future {product-title} releases.

The Red{nbsp}Hat-supported version of the Operator SDK is not recommended for creating new Operator projects. Operator authors with existing Operator projects can use the version of the Operator SDK CLI tool released with {product-title} {product-version} to maintain their projects and create Operator releases targeting newer versions of {product-title}.

The following related base images for Operator projects are _not_ deprecated. The runtime functionality and configuration APIs for these base images are still supported for bug fixes and for addressing CVEs.

* The base image for Ansible-based Operator projects
* The base image for Helm-based Operator projects

For information about the unsupported, community-maintained, version of the Operator SDK, see link:https://sdk.operatorframework.io[Operator SDK (Operator Framework)].

[id="ocp-4-16-removed-features_{context}"]
=== Removed features

[id="ocp-4-16-nodes-diskpartition-deprecated_{context}"]
==== Deprecated disk partition configuration method

The `nodes.diskPartition` section in the `SiteConfig` custom resource (CR) is deprecated with the {product-title} 4.16 release. It has been replaced with the `ignitionConfigOverride` method, which provides a more flexible way of creating a disk partition for any use case.

For more information, see xref:../edge_computing/policygenerator_for_ztp/ztp-advanced-policygenerator-config.adoc#ztp-configuring-disk-partitioning_ztp-advanced-policygenerator-config[Configuring disk partitioning with SiteConfig].

[id="ocp-4-16-removed-features-platform-operators_{context}"]
==== Removal of platform Operators and plain bundles (Technology Preview)

{product-title} {product-version} removes platform Operators (Technology Preview) and plain bundles (Technology Preview), which were protoypes for {olmv1-first} (Technology Preview).

//.APIs removed from Kubernetes 1.29
//[cols="2,2,2",options="header",]
//|===
//|Resource |Removed API |Migrate to

//|`kube-scheduler`
//|`selectorSpread`
//|`podTopologySpread`

//|===

[id="ocp-4-16-dell-idrac-removed_{context}"]
==== Dell iDRAC driver for BMC addressing removed

{product-title} {product-version} supports baseboard management controller (BMC) addressing with Dell servers as documented in xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#bmc-addressing-for-dell-idrac_ipi-install-installation-workflow[BMC addressing for Dell iDRAC]. Specifically, it supports `idrac-virtualmedia`, `redfish`, and `ipmi`. In previous versions, `idrac` was included, but not documented or supported. In {product-title} {product-version}, `idrac` has been removed.

[id="ocp-4-16-dedicated-service-monitors-removed"]
==== Dedicated service monitors for core platform monitoring

With this release, the dedicated service monitors feature for core platform monitoring has been removed. You can no longer enable this feature in the `cluster-monitoring-config` config map object in the `openshift-monitoring` namespace. To replace this feature, Prometheus functionality has been improved to ensure that alerts and time aggregations are accurate. This improved functionality is active by default and makes the dedicated service monitors feature obsolete.

[id="ocp-4-16-prometheus-adapter-removed"]
==== Prometheus Adapter for core platform monitoring

With this release, the Prometheus Adapter component for core platform monitoring has been removed. It has been replaced by the new Metrics Server component.

[id="ocp-4-16-metallb-addresspool-removed_{context}"]
==== MetalLB AddressPool custom resource definition (CRD) removed

The MetalLB `AddressPool` custom resource definition (CRD) has been deprecated for several versions. However, in this release, the CRD is completely removed. The sole supported method of configuring MetalLB address pools is by using the `IPAddressPools` CRD.

[id="ocp-4-16-service-binding-operator-documentation-removed_{context}"]
==== Service Binding Operator documentation removed
With this release, the documentation for the Service Binding Operator (SBO) has been removed because this Operator is no longer supported.

[id="ocp-4-16-alicloud-csi-driver-removed_{context}"]
==== AliCloud CSI Driver Operator is no longer supported
{product-title} 4.16 no longer supports AliCloud Container Storage Interface (CSI) Driver Operator.

[id="ocp-4-16-removed-kube-1-29-apis_{context}"]
==== Beta APIs removed from Kubernetes 1.29

Kubernetes 1.29 removed the following deprecated APIs, so you must migrate manifests and API clients to use the appropriate API version. For more information about migrating removed APIs, see the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-29[Kubernetes documentation].

.APIs removed from Kubernetes 1.29
[cols="2,2,2,1",options="header",]
|===
|Resource |Removed API |Migrate to |Notable changes

|`FlowSchema`
|`flowcontrol.apiserver.k8s.io/v1beta2`
|`flowcontrol.apiserver.k8s.io/v1` or `flowcontrol.apiserver.k8s.io/v1beta3`
|No

|`PriorityLevelConfiguration`
|`flowcontrol.apiserver.k8s.io/v1beta2`
|`flowcontrol.apiserver.k8s.io/v1` or `flowcontrol.apiserver.k8s.io/v1beta3`
|link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#flowcontrol-resources-v129[Yes]

|===

[id="ocp-4-16-future-deprecation"]
=== Notice of future deprecation

[id="ocp-4-16-bug-fixes_{context}"]
== Bug fixes
//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / TALO
//Telco Edge / ZTP

[discrete]
[id="ocp-4-16-api-auth-bug-fixes_{context}"]
==== API Server and Authentication

* Previously, `ephemeral` and `csi` volumes were not properly added to security context constraints (SCCs) on upgraded clusters. With this release, SCCs on upgraded clusters are properly updated to have `ephemeral` and `csi` volumes. (link:https://issues.redhat.com/browse/OCPBUGS-33522[*OCPBUGS-33522*])

* Previously, the `ServiceAccounts` resource could not be used with OAuth clients for a cluster with the `ImageRegistry` capability enabled. With this release, this issue is fixed. (link:https://issues.redhat.com/browse/OCPBUGS-30319[*OCPBUGS-30319*])

* Previously, when you created a pod with an empty security context and you have access to all security context constraints (SCCs), the pod would receive the `anyuid` SCC. After the `ovn-controller` component added a label to the pod, the pod would be re-admitted for SCC selection, where the pod received an escalated SCC, such as `privileged`. With this release, this issue is resolved so the pod is not re-admitted for SCC selection. (link:https://issues.redhat.com/browse/OCPBUGS-11933[*OCPBUGS-11933*])

[discrete]
[id="ocp-4-16-bare-metal-hardware-bug-fixes_{context}"]
==== Bare Metal Hardware Provisioning

[discrete]
[id="ocp-4-16-builds-bug-fixes_{context}"]
==== Builds

[discrete]
[id="ocp-4-16-cloud-compute-bug-fixes_{context}"]
==== Cloud Compute

* Previously, the installation program populated the `network.devices`, `template` and `workspace` fields in the `spec.template.spec.providerSpec.value` section of the {vmw-full} control plane machine set custom resource (CR).
These fields should be set in the {vmw-short} failure domain, and the installation program populating them caused unintended behaviors.
Updating these fields did not trigger an update to the control plane machines, and these fields were cleared when the control plane machine set was deleted.
With this release, the installation program is updated to no longer populate values that are included in the failure domain configuration.
If these values are not defined in a failure domain configuration, for instance on a cluster that is updated to {product-title} {product-version} from an earlier version, the values defined by the installation program are used.
(link:https://issues.redhat.com/browse/OCPBUGS-32947[*OCPBUGS-32947*])

[discrete]
[id="ocp-4-16-cloud-cred-operator-bug-fixes_{context}"]
==== Cloud Credential Operator

* Previously, the Cloud Credential Operator was missing some permissions required to create a private cluster on {azure-first}.
These missing permissions prevented installation of an {azure-short} private cluster using {entra-first}.
This release includes the missing permissions and enables installation of an {azure-short} private cluster using {entra-short}.
(link:https://issues.redhat.com/browse/OCPBUGS-25193[*OCPBUGS-25193*])

* Previously, a bug caused the Cloud Credential Operator (CCO) to report an incorrect mode in the metrics. Even though the cluster was in the default mode, the metrics reported that it was in the credentials removed mode. This update uses a live client in place of a cached client so that it is able to obtain the root credentials, and the CCO no longer reports an incorrect mode in the metrics. (link:https://issues.redhat.com/browse/OCPBUGS-26488[*OCPBUGS-26488*])

* Previously, the Cloud Credential Operator credentials mode metric on an {product-title} cluster that uses {entra-first} reported using manual mode.
With this release, clusters that use {entra-short} are updated to report that they are using manual mode with pod identity.
(link:https://issues.redhat.com/browse/OCPBUGS-27446[*OCPBUGS-27446*])

* Previously, creating an {aws-first} root secret on a bare metal cluster caused the Cloud Credential Operator (CCO) pod to crash.
The issue is resolved in this release.
(link:https://issues.redhat.com/browse/OCPBUGS-28535[*OCPBUGS-28535*])

* Previously, removing the root credential from a {gcp-first} cluster that used the Cloud Credential Operator (CCO) in mint mode caused the CCO to become degraded after approximately one hour.
In a degraded state, the CCO cannot manage the component credential secrets on a cluster.
The issue is resolved in this release.
(link:https://issues.redhat.com/browse/OCPBUGS-28787[*OCPBUGS-28787*])

* Previously, the Cloud Credential Operator (CCO) checked for a nonexistent `s3:HeadBucket` permission during installation on {aws-first}.
When the CCO failed to find this permission, it considered the provided credentials insufficient for mint mode.
With this release, the CCO no longer checks for the nonexistent permission.
(link:https://issues.redhat.com/browse/OCPBUGS-31678[*OCPBUGS-31678*])

[discrete]
[id="ocp-4-16-cluster-version-operator-bug-fixes_{context}"]
==== Cluster Version Operator

* This release expands the `ClusterOperatorDown` and `ClusterOperatorDegraded` alerts to cover ClusterVersion conditions and send alerts for `Available=False` (`ClusterOperatorDown`) and `Failing=True` (`ClusterOperatorDegraded`). In previous releases, those alerts only covered ClusterOperator conditions. (link:https://issues.redhat.com/browse/OCPBUGS-9133[*OCPBUGS-9133*])

* Previously, Cluster Version Operator (CVO) changes that were introduced in {product-title} 4.15.0, 4.14.0, 4.13.17, and 4.12.43 caused failing risk evaluations to block the CVO from fetching new update recommendations. When the risk evaluations failed, the bug caused the CVO to overlook the update recommendation service. With this release, the CVO continues to poll the update recommendation service, regardless of whether update risks are being successfully evaluated and the issue has been resolved. (link:https://issues.redhat.com/browse/OCPBUGS-25708[*OCPBUGS-25708*])

[discrete]
[id="ocp-4-16-dev-console-bug-fixes_{context}"]
==== Developer Console

* Previously, when a serverless function was created in the create serverless form, `BuilldConfig` was not created. With this update, if the Pipelines Operator is not installed, the pipeline resource is not created for particular resource, or the pipeline is not added while creating a serverless function, it will create `BuildConfig` as expected. (link:https://issues.redhat.com/browse/OCPBUGS-34143[*OCPBUGS-34143*])

* Previously, after installing the Pipelines Operator, Pipeline templates took some time to become available in the cluster, but users were still able to create the deployment. With this update, the *Create* button on the *Import from Git* page is disabled if there is no pipeline template present for the resource selected. (link:https://issues.redhat.com/browse/OCPBUGS-34142[*OCPBUGS-34142*])

* Previously, the maximum number of nodes was set to `100` on the *Topology* page. A persistant alert, "Loading is taking longer than expected." was provided. With this update, the limit of nodes is increased to `300`. (link:https://issues.redhat.com/browse/OCPBUGS-32307[*OCPBUGS-32307*])

* With this update, an alert message to notify you that Service Bindings are deprecated with {product-title} 4.15 was added to the *ServiceBinding list*, *ServiceBinding details*, *Add*, and *Topology* pages when creating a `ServiceBinding`, binding a component, or a `ServiceBinding` was found in the current namespace. (link:https://issues.redhat.com/browse/OCPBUGS-32222[*OCPBUGS-32222*])

* Previously, the Helm Plugin index view did not display the same number of charts as the Helm CLI if the chart names varied. With this release, the Helm catalog now looks for `charts.openshift.io/name` and `charts.openshift.io/provider` so that all versions are grouped together in a single catalog title. (link:https://issues.redhat.com/browse/OCPBUGS-32059[*OCPBUGS-32059*])

* Previously, the `TaskRun` status was not displayed near the `TaskRun` name on the *TaskRun details* page. With this update, the `TaskRun` status is located beside the name of the `TaskRun` in the page heading. (link:https://issues.redhat.com/browse/OCPBUGS-31745[*OCPBUGS-31745*])

* Previously, there is an error when adding parameters to the Pipeline when the resources field was added to the payload, and as resources are deprecated. With this update, the resources fields have been removed from the payload, and you can add parameters to the Pipeline. (link:https://issues.redhat.com/browse/OCPBUGS-31082[*OCPBUGS-31082*])

* This release updates the OpenShift Pipelines plugin to support the latest Pipeline Trigger API version for the custom resource definitions (CRDs) `ClusterTriggerBinding`, `TriggerTemplate` and `EventListener`. (link:https://issues.redhat.com/browse/OCPBUGS-30958[*OCPBUGS-30958*])

* Previously, `CustomTasks` were not recognized or remained in a `Pending` state. With this update, `CustomTasks` can be easily identified from the Pipelines *List* and *Details* pages. (link:https://issues.redhat.com/browse/OCPBUGS-29513[*OCPBUGS-29513*])

* Previously, if there was a build output image with an `Image` tag then the `Output Image` link would not redirect to the correct *ImageStream* page. With this update, this has been fixed by generating a URL for the *ImageStream* page without adding the tag in the link. (link:https://issues.redhat.com/browse/OCPBUGS-29355[*OCPBUGS-29355*])

* Previously, `BuildRun` logs were not visible in the *Logs* tab of the *BuildRun* page due to a recent update in the API version of the specified resources. With this update, the logs of the `TaskRuns` were added back into the *Logs* tab of the `BuildRun` page for both v1alpha1 and v1beta1 versions of the Builds Operator. (link:https://issues.redhat.com/browse/OCPBUGS-27473[*OCPBUGS-27473*])

* Previously, the annotations to set scale bound values were setting to `autoscaling.knative.dev/maxScale` and `autoscaling.knative.dev/minScale`. With this update, the annotations to set scale bound values are updated to `autoscaling.knative.dev/min-scale` and `autoscaling.knative.dev/max-scale` to determine the minimum and maximum numbers of replicas that can serve an application at any given time. You can set scale bounds for an application to help prevent cold starts or control computing costs. (link:https://issues.redhat.com/browse/OCPBUGS-27469[*OCPBUGS-27469*])

* Previously, the *Log* tab for *PipelineRuns* from the Tekton Results API never finished loading. With this release, this tab loads fully complete for PipelineRuns loaded from the Kubernetes API or the Tekton Results API. (link:https://issues.redhat.com/browse/OCPBUGS-25612[*OCPBUGS-25612*])

* Previously, there was no indicator shown to differentiate between `PipelineRuns` that are loaded from the Kubernetes API or the Tekton Results API. With this update, a small archived icon in the *PipelineRun list* and *details* page to differentiate between `PipelineRuns` that are loaded from the Kubernetes API or the Tekton Results API. (link:https://issues.redhat.com/browse/OCPBUGS-25396[*OCPBUGS-25396*])

* Previously, on the *PipelineRun list* page, all TaskRuns were fetched and separated based on `pipelineRun` name. With this update, TaskRuns are fetched only for `Failed` and `Cancelled` PipelineRun. A caching mechanism was also added to fetch PipelineRuns and TaskRuns associated to the `Failed` and `Cancelled` PipelineRuns. (link:https://issues.redhat.com/browse/OCPBUGS-23480[*OCPBUGS-23480*])

* Previously, the visual connector was not present between the VMs node and other non-VMs nodes in the *Topology* view. With this update, the visual connector is located between VMs nodes and non-VMs nodes. (link:https://issues.redhat.com/browse/OCPBUGS-13114[*OCPBUGS-13114*])

[discrete]
[id="ocp-4-16-cloud-etcd-operator-bug-fixes_{context}"]
==== etcd Cluster Operator

* Previously, the `wait-for-ceo` command that was used during bootstrap to verify etcd rollout did not report errors for some failure modes. With this release, those error messages now are visible on the `bootkube` script if the `cmd` exits in an error case. (link:https://issues.redhat.com/browse/OCPBUGS-33495[*OCPBUGS-33495*])

* Previously, the etcd Cluster Operator entered a state of panic during pod health checks and this caused requests to an `etcd` cluster to fail. With this release, the issue is fixed so that these panic situations no longer occur.(link:https://issues.redhat.com/browse/OCPBUGS-27959[*OCPBUGS-27959*])

* Previously, the etcd Cluster Operator wrongly identified non-running controllers as deadlocked and this caused an unnecessary pod restart. With this release, this issue is now fixed so that the Operator marks a non-running controller as an unhealthy etcd member without restarting a pod. (link:https://issues.redhat.com/browse/OCPBUGS-30873[*OCPBUGS-30873*])

[discrete]
[id="ocp-4-16-hosted-control-plane-bug-fixes_{context}"]
==== Hosted Control Plane

[discrete]
[id="ocp-4-16-image-registry-bug-fixes_{context}"]
==== Image Registry

* Previously, after you imported image streams tags, the `ImageContentSourcePolicy` (ICSP) custom resource (CR) could not co-exist with the `ImageDigestMirrorSet` (IDMS) or `ImageTagMirrorSet` (ITMS) CR. {product-title} chose ICSP instead of the other CR types. With this release, these custom resources can co-exist, so after you import image stream tags, {product-title} can choose the required CR. (link:https://issues.redhat.com/browse/OCPBUGS-30279[*OCPBUGS-30279*])

* Previously, the `oc tag` command did not validate tag names when the command created new tags. After images were created from tags with invalid names, the `podman pull` command would fail. With this release, a validation step checks new tags for invalid names and you can now delete existing tags that have invalid names, so that this issue no longer exists. (link:https://issues.redhat.com/browse/OCPBUGS-25703[*OCPBUGS-25703*])

[discrete]
[id="ocp-4-16-installer-bug-fixes_{context}"]
==== Installer

* Previously, installation of a three node cluster with an invalid configuration on {gcp-first} failed with a panic error that did not report the reason for the failure. With this release, the installation program validates the installation configuration to successfully install a three node cluster on {gcp-short}. (link:https://issues.redhat.com/browse/OCPBUGS-35103[*OCPBUGS-35103*])

* Previously, installations with the Assisted Installer failed if the pull secret contained a colon in the password. With this release, pull secrets containing a colon in the password do not cause the Assisted Installer to fail. (link:https://issues.redhat.com/browse/OCPBUGS-34400[*OCPBUGS-34400*])

* Previously, the `monitor-add-nodes` command, which is used to monitor the process of adding nodes to an Agent-based cluster, failed to run due to a permission error. With this release, the command operates in the correct directory where it has permissions. (link:https://issues.redhat.com/browse/OCPBUGS-34388[*OCPBUGS-34388*])

* Previously, long cluster names were trimmed without warning the user. With this release, the installation program warns the user when trimming long cluster names. (link:https://issues.redhat.com/browse/OCPBUGS-33840[*OCPBUGS-33840*])

* Previously, when installing a cluster, the Ingress capability was enabled even if it was disabled in `install-config.yaml` because it is required. With this release, the installation program fails if the Ingress capability is disabled in `install-config.yaml`. (link:https://issues.redhat.com/browse/OCPBUGS-33794[*OCPBUGS-33794*])

* Previously, {product-title} did not perform quota checking for clusters installed in the `ca-west-1` an {aws-first} region. With this release, quotas are properly enforced in this region. (link:https://issues.redhat.com/browse/OCPBUGS-33649[*OCPBUGS-33649*])

* Previously, the installation program could sometimes fail to detect that the {product-title} API is unavailable. An additional error was resolved by increasing the disk size of the bootstrap node in {azure-first} installations. With this release, the installation program correctly detects if the API is unavailable. (link:https://issues.redhat.com/browse/OCPBUGS-33610[*OCPBUGS-33610*])

* Previously, control plane nodes on {azure-first} clusters were using `Read-only` caches. With this release, {azure-first} control plane nodes use `ReadWrite` caches. (link:https://issues.redhat.com/browse/OCPBUGS-33470[*OCPBUGS-33470*])

* Previously, when installing an Agent-based cluster with a proxy configured, the installation failed if the proxy configuration contained a string starting with a percent sign (`%`).  With this release, the installation program correctly validates this configuration text. (link:https://issues.redhat.com/browse/OCPBUGS-33024[*OCPBUGS-33024*])

* Previously, installations on {gcp-short} could fail because the installation program attempted to create a bucket twice. With this release, the installation program no longer attempts to create the bucket twice. (link:https://issues.redhat.com/browse/OCPBUGS-32133[*OCPBUGS-32133*])

* Previously, a rare timing issue could prevent all control plane nodes from being added to an Agent-based cluster during installation. With this release, all control plane nodes are successfully rebooted and added to the cluster during installation. (link:https://issues.redhat.com/browse/OCPBUGS-32105[*OCPBUGS-32105*])

* Previously, when using the Agent-based installation program in a disconnected environment, unnecessary certificates were added to the Certificate Authority (CA) trust bundle. With this release, the CA bundle `ConfigMap` only contains CAs explicitly specified by the user. (link:https://issues.redhat.com/browse/OCPBUGS-32042[*OCPBUGS-32042*])

* Previously, the installation program required a non-existent permission `s3:HeadBucket` when installing a cluster on {aws-first}. With this release, the installation program correctly requires the permission `s3:ListBucket` instead. (link:https://issues.redhat.com/browse/OCPBUGS-31813[*OCPBUGS-31813*])

* Previously, if the installation program failed to gather logs from the bootstrap due to an SSH connection issue, it would also not provide virtual machine (VM) serial console logs even if they were collected. With this release, the installation program provides VM serial console logs even if the SSH connection to the bootstrap machine fails. (link:https://issues.redhat.com/browse/OCPBUGS-30774[*OCPBUGS-30774*])

* Previously, when installing a cluster on {vmw-full} with static IP addresses, the cluster could create control plane machines without static IP addresses due to a conflict with other Technology Preview features. With this release, the Control Plane Machine Set Operator correctly manages the static IP assignment for control plane machines. (link:https://issues.redhat.com/browse/OCPBUGS-29114[*OCPBUGS-29114*])

* Previously, when installing a cluster on {gcp-short} with user-provided DNS, the installation program still attempted to validate DNS within the {gcp-short} DNS network. With this release, the installation program does not perform this validation for user-provided DNS. (link:https://issues.redhat.com/browse/OCPBUGS-29068[*OCPBUGS-29068*])

* Previously, when deleting a private cluster on {ibm-cloud-name} that used the same domain name as a non-private {ibm-cloud-name} cluster, some resources were not deleted. With this release, all private cluster resources are deleted when the cluster is removed. (link:https://issues.redhat.com/browse/OCPBUGS-28870[*OCPBUGS-28870*])

* Previously, when installing a cluster using a proxy with a character string that used the percent sign (`%`) in the configuration string, the cluster installation failed. With this release, the installation program correctly validates proxy configuration strings containing "%". (link:https://issues.redhat.com/browse/OCPBUGS-27965[*OCPBUGS-27965*])

* Previously, the installation program still allowed the use of the `OpenShiftSDN` network plugin even though it was removed. With this release, the installation program correctly prevents installing a cluster with this network plugin. (link:https://issues.redhat.com/browse/OCPBUGS-27813[*OCPBUGS-27813*])

* Previously, when installing a cluster on {aws-first} Wavelengths or Local Zones into a region that supports either Wavelengths or Local Zones, but not both, the installation failed. With this release, installations into regions that support either Wavelengths or Local Zones can succeed. (link:https://issues.redhat.com/browse/OCPBUGS-27737[*OCPBUGS-27737*])

* Previously, when a cluster installation was attempted that used the same cluster name and base domain as an existing cluster and the installation failed due to DNS record set conflicts, removal of the second cluster would also remove the DNS record sets in the original cluster. With this release, the stored metadata contains the private zone name rather than the cluster domain, so only the correct DNS records are deleted rom a removed cluster. (link:https://issues.redhat.com/browse/OCPBUGS-27156[*OCPBUGS-27156*])

* Previously, platform specific passwords that were configured in the installation configuration file of an Agent-based installation could be present in the output of the `agent-gather` command. With this release, passwords are redacted from the `agent-gather` output. (link:https://issues.redhat.com/browse/OCPBUGS-26434[*OCPBUGS-26434*])

* Previously, a {product-title} cluster installed with version 4.15 or 4.16 showed a default upgrade channel of version 4.14. With this release, clusters have the correct upgrade channel after installation. (link:https://issues.redhat.com/browse/OCPBUGS-26048[*OCPBUGS-26048*])

* Previously, when deleting a {vmw-full} cluster, some `TagCategory` objects failed to be deleted. With this release, all cluster-related objects are correctly deleted when the cluster is removed. (link:https://issues.redhat.com/browse/OCPBUGS-25841[*OCPBUGS-25841*])

* Previously, when specifying the `baremetal` platform type but disabling the `baremetal` capability in `install-config.yaml`, the installation failed after a long timeout without a helpful error. With this release, the installation program provides a descriptive error and does not attempt a bare metal installation if the `baremetal` capability is disabled. (link:https://issues.redhat.com/browse/OCPBUGS-25835[*OCPBUGS-25835*])

* Previously, installations on {vmw-full} using the Assisted Installer could fail by preventing {vmw-full} from initializing nodes correctly. With this release, Assisted Installer installations on {vmw-full} successfully complete with all nodes initialized. (link:https://issues.redhat.com/browse/OCPBUGS-25718[*OCPBUGS-25718*])

* Previously, if a VM type was selected that did not match the architecture specified in the `install-config.yaml` file, the installation would fail. With this release, a validation check ensures that the architectures match before the installation begins. (link:https://issues.redhat.com/browse/OCPBUGS-25600[*OCPBUGS-25600*])

* Previously, agent-based installations could fail if an invalid number of control plane replicas was specified, such as 2. With this release, the installation program enforces the requirement of specifying either 1 or 3 control plane replicas for agent-based installations. (link:https://issues.redhat.com/browse/OCPBUGS-25462[*OCPBUGS-25462*])

* Previously, when installing a cluster on {vmw-full} using the control plane machine set Technology Preview feature, the resulting control plane machine sets had duplicate failure domains in their configuration. With this release, the installation program creates the control plane machine sets with the correct failure domains. (link:https://issues.redhat.com/browse/OCPBUGS-25453[*OCPBUGS-25453*])

* Previously, the required `iam:TagInstanceProfile` permission was not validated before an installer-provisioned installation, causing an installation to fail if the Identity and Access Management (IAM) permission was missing. With this release, a validation check ensures that the permission is included before the installation begins. (link:https://issues.redhat.com/browse/OCPBUGS-25440[*OCPBUGS-25440*])

* Previously, the installation program did not prevent users from installing a cluster on non-bare-metal platforms with the Cloud Credential capability disabled, although it is required. With this release, the installation program produces an error and prevents installation with the Cloud Credential capability disabled, except for on the bare-metal platform. (link:https://issues.redhat.com/browse/OCPBUGS-24956[*OCPBUGS-24956*])

* Previously, setting an architecture different from the one supported by the instance type resulted in the installation failing mid-process, after some resources were created. With this release, a validation check verifies that the instance type is compatible with the specified architecture. If the architecture is not compatible, the process fails before the installation begins. (link:https://issues.redhat.com/browse/OCPBUGS-24575[*OCPBUGS-24575*])

* Previously, the installation program did not prevent a user from installing a cluster on a cloud provider with the Cloud Controller Manager disabled, which failed without a helpful error message. With this release, the installation program produces an error stating that the Cloud Controller Manager capability is required for installations on cloud platforms. (link:https://issues.redhat.com/browse/OCPBUGS-24415[*OCPBUGS-24415*])

* Previously, the installation program could fail to remove a cluster installed on {ibm-cloud-name} due to unexpected results from the {ibm-cloud-name} API. With this release, clusters installed on {ibm-cloud-name} can reliably be deleted by the installation program. (link:https://issues.redhat.com/browse/OCPBUGS-20085[*OCPBUGS-20085*])

* Previously, the installation program did not enforce the requirement that FIPS-enabled clusters were installed from FIPS-enabled {op-system-base-full} hosts. With this release, the installation program enforces the FIPS requirement. (link:https://issues.redhat.com/browse/OCPBUGS-15845[*OCPBUGS-15845*])

* Previously, proxy information that was set in the `install-config.yaml` file was not applied to the bootstrap process. With this release, proxy information is applied to bootstrap ignition data, which is then applied to the bootstrap machine. (link:https://issues.redhat.com/browse/OCPBUGS-12890[*OCPBUGS-12890*])

[discrete]
[id="ocp-4-16-insights-operator-bug-fixes_{context}"]
==== Insights Operator

* The Insights Operator now collects instances outside of the `openshift-monitoring` of the following custom resources:
- Kind: `Prometheus` Group: `monitoring.coreos.com`
- Kind: `AlertManager` Group: `monitoring.coreos.com`

[discrete]
[id="ocp-4-16-kube-controller-bug-fixes_{context}"]
==== Kubernetes Controller Manager

[discrete]
[id="ocp-4-16-kube-scheduler-bug-fixes_{context}"]
==== Kubernetes Scheduler

[discrete]
[id="ocp-4-16-machine-config-operator-bug-fixes_{context}"]
==== Machine Config Operator

[discrete]
[id="ocp-4-16-management-console-bug-fixes_{context}"]
==== Management Console

* Previously, the *Debug container* link was not shown for pods with a `Completed` status. With this release, the link shows as expected. (link:https://issues.redhat.com/browse/OCPBUGS-34711[*OCPBUGS-34711*])

* Previously, due to an issue in PatternFly 5, text boxes in the web console were no longer resizable. With this release, text boxes are again resizable. (link:https://issues.redhat.com/browse/OCPBUGS-34393[*OCPBUGS-34393*])

* Previously, French and Spanish were not available in the web console. With this release, translations for French and Spanish are now available. (link:https://issues.redhat.com/browse/OCPBUGS-33965[*OCPBUGS-33965*])

* Previously, the masthead logo was not restricted to a `max-height` of 60px. As a result, logos that are larger than 60px high display at their native size and cause the masthead size too to be too large. With this release, the masthead logo is restricted to a max-height of 60px. (link:https://issues.redhat.com/browse/OCPBUGS-33523[*OCPBUGS-33523*])

* Previously, there was a missing return statement in the `HealthCheck` controller causing it to panic under certain circumstances. With this release, the proper return statement was added to the `HealthCheck` controller so it no longer panics. (link:https://issues.redhat.com/browse/OCPBUGS-33505[*OCPBUGS-33505*]

* Previously, an incorrect field was sent to the API server that was not noticeable. With the implementation of Admission Webhook display warning the same action would return a warning notification. A fix was provided to resolve the issue. (link:https://issues.redhat.com/browse/OCPBUGS-33222[*OCPBUGS-33222*])

* Previously, the message text of a `StatusItem` might have been vertically misaligned with the icon when a timestamp was not present. With this release, the message text is correctly aligned. (link:https://issues.redhat.com/browse/OCPBUGS-33219[*OCPBUGS-33219*])

* Previously, the creator field was autopopulated and not mandatory. Updates to the API made the field empty from {product-title} 4.15 and higher. With this release, the field is marked as mandatory for correct validation. (link:https://issues.redhat.com/browse/OCPBUGS-31931[*OCPBUGS-31931*])

* Previously, the YAML editor in the web console did not have the *Create* button and samples did not show on the web console. With this release, you can now see the *Create* button and the samples. (link:https://issues.redhat.com/browse/OCPBUGS-31703[*OCPBUGS-31703*])

* Previously, changes to the bridge server flags on an external OpenID Connect (OIDC) feature caused the bridge server fail to start in local development. With this release, the flags usage are updated and the bridge server starts. (link:https://issues.redhat.com/browse/OCPBUGS-31695[*OCPBUGS-31695*])

* Previously, when editing a {vmw-full} connection, the form could be submitted even if no values were actually changed. This resulted in unnecessary node reboots. With this release, the console now detects the form changes, and does not allow submission if no value was changed. (link:https://issues.redhat.com/browse/OCPBUGS-31613[*OCPBUGS-31613*])

* Previously, the `NetworkAttachmentDefinition` was always created in the default namespace if the form method `from the console` was used. The selected name is also not honored, and creates the `NetworkAttachmentDefinition` object with the selected name and a random suffix. With this release, the `NetworkAttachmentDefinition` object is created in the current project. (link:https://issues.redhat.com/browse/OCPBUGS-31558[*OCPBUGS-31558*])

* Previously, when clicking the *Configure* button by the `AlertmanagerRecieversNotConfigured` alert, the *Configuration* page did not show. With this release, the link in the `AlertmanagerRecieversNotConfigured` alert is fixed and directs you to the *Configuration* page. (link:https://issues.redhat.com/browse/OCPBUGS-30805[*OCPBUGS-30805*])

* Previously, plugins using `ListPageFilters` were only using two filters: label and name. With this release, a parameter was added that enables plugins to configure multiple text-based search filters. (link:https://issues.redhat.com/browse/OCPBUGS-30077[*OCPBUGS-30077*])

* Previously, there was no response when clicking on quick start items. With this release, the quick start window shows when clicking on the quick start selections. (link:https://issues.redhat.com/browse/OCPBUGS-29992[*OCPBUGS-29992*])

* Previously, the {product-title} web console terminated unexpectedly if authentication discovery failed on the first attempt. With this release, authentication initialization was updated to retry up to 5 minutes before failing. (link:https://issues.redhat.com/browse/OCPBUGS-29479[*OCPBUGS-29479*])

* Previously there was an issue causing an error message on the *Image Manifest Vulnerability* page after an Image Manifest Vulnerability (IMV) was created in the CLI. With this release, the error message no longer shows. (link:https://issues.redhat.com/browse/OCPBUGS-28967[*OCPBUGS-28967*])

* Previously, when using the modal dialog in a hook as part of the actions hook, an error occured because the console framework passed null objects as part of the render cycle. With this release, `getGroupVersionKindForResource` is now null-safe and will return `undefined` if the `apiVersion` or `kind` are undefined. Additionally, the run time error for `useDeleteModal` no longer occurs, but note that it will not work with an `undefined` resource. (link:https://issues.redhat.com/browse/OCPBUGS-28856[*OCPBUGS-28856*])

* Previously, the *Expand PersistentVolumeClaim* modal assumes the `pvc.spec.resources.requests.stroage` value includes a unit. With this release, the size is updated to 2GiB and you can change the value of the persistent volume claim (PVC). (link:https://issues.redhat.com/browse/OCPBUGS-27779[*OCPBUGS-27779*])

* Previously, the value of image vulnerabilities reported in the {product-title} web console were inconsistent. With this release, the image vulnerabilities on the *Overview* page were removed. (link:https://issues.redhat.com/browse/OCPBUGS-27455[*OCPBUGS-27455*])

* Previously, a certificate signing request (CSR) could show for a recently approved Node. With this release, the duplication is detected and does not show CSRs for approved Nodes. (link:https://issues.redhat.com/browse/OCPBUGS-27399[*OCPBUGS-27399*])

* Previously, the *Type* column was not first on the condition table on the *MachineHealthCheck detail* page. With this release, the *Type* is now listed first on the condition table. (link:https://issues.redhat.com/browse/OCPBUGS-27246[*OCPBUGS-27246*])

* Previously, the console plugin proxy was not copying the status code from plugin service responses. This caused all responses from the plugin service to have a `200` status, causing unexpected behavior, especially around browser caching. With this release, the console proxy logic was updated to forward the plugin service proxy response status code. Proxied plugin requests now behave as expected. (link:https://issues.redhat.com/browse/OCPBUGS-26933[*OCPBUGS-26933*])

* Previously, when cloning a persistent volume claim (PVC), the modal assumes `pvc.spec.resources.requests.storage` value includes a unit. With this release, `pvc.spec.resources.requests.storage` includes a unit suffix and the *Clone PVC* modal works as expected. (link: https://issues.redhat.com/browse/OCPBUGS-26772[*OCPBUGS-26772*])

* Previously, escaped strings were not handled properly when editing {vmw-full} connection, causing broken {vmw-full} configuration. With this release, the escape strings work as expected and the {vmw-full} configuration no longer breaks. (link:https://issues.redhat.com/browse/OCPBUGS-25942[*OCPBUGS-25942*])

* Previously, when configuring a {vmw-full} connection,  the `resourcepool-path` key was not added to the {vmw-full} config map which might have caused issues connecting to {vmw-full}. With this release, there are no longer issues connecting to {vmw-full}. (link:https://issues.redhat.com/browse/OCPBUGS-25927[*OCPBUGS-25927*])

* Previously, there was missing text in the *Customer feedback* modal. With this release, the link text is restored and the correct Red Hat image is displayed. (link:https://issues.redhat.com/browse/OCPBUGS-25843[*OCPBUGS-25843*])

* Previously, the *Update cluster* modal would not open when clicking *Select a version* from the *Cluster Settings* page. With this release, the *Update cluster* modal shows when clicking *Select a version*. (link:https://issues.redhat.com/browse/OCPBUGS-25780[*OCPBUGS-25780*])

* Previously, on a mobile device, the filter part in the resource section of the *Search* page did not work on a mobile device. With this release, filtering now works as expected ona mobile device. (link:https://issues.redhat.com/browse/OCPBUGS-25530[*OCPBUGS-25530*])

* Previously, the console Operator was using a client instead of listeners for fetching a cluster resource. This caused the Operator to do operations on resources with an older revision. With this release, the console Operator uses list to fetch data from cluster instead of clients. (link:https://issues.redhat.com/browse/OCPBUGS-25484[*OCPBUGS-25484*])

* Previously, the console was incorrectly parsing restore size values from volume snapshots in the restore as new persistent volume claims  (PVC) modal. With this release, the modal parses the restore size correctly. (link:https://issues.redhat.com/browse/OCPBUGS-24637[*OCPBUGS-24637*])

* Previously, the *Alerting*, *Metrics*, and *Target* pages were not available in the console due to a change on the routing library. With this release, routes load correctly. (link:https://issues.redhat.com/browse/OCPBUGS-24515[*OCPBUGS-24515*])

* Previously, there was a runtime error on the *Node details* page when a `MachineHealthCheck` without conditions existed. With this release, the *Node details* page loads as expected. (link:https://issues.redhat.com/browse/OCPBUGS-24408[*OCPBUGS-24408*])

* Previously, the console backend would proxy operand list requests to the public API server endpoint, which caused CA certificate issues under some circumstances. With this release, the proxy configuration was updated to point to the internal API server endpoint which fixed this issue. (link:https://issues.redhat.com/browse/OCPBUGS-22487[*OCPBUGS-22487*])

* Previously, a deployment could not be scaled up or down when a `HorizontalPodAutoscaler` was present. With this release, when a deployment with an `HorizontalPodAutoscaler` is scaled down to `zero`, an *Enable Autoscale* button is displayed so you can enable pod autoscaling. (link:https://issues.redhat.com/browse/OCPBUGS-22405[*OCPBUGS-22405*])

* Previously, when editing a file, the `Info alert:Non-printable file detected. File contains non-printable characters. Preview is not available.` error was presented. With this release, a check was added to determine if a file is binary, and you are able to edit the file as expected. (link:https://issues.redhat.com/browse/OCPBUGS-18699[*OCPBUGS-18699*])

* Previously, the console API conversion webhook server could not update serving certificates at runtime, and would fail if these certificates were updated by deleting the signing key. This would cause the console to not recover when CA certs were rotated. With this release, console conversion webhook server was updated to detect CA certificate changes, and handle them at runtime. The server now remains available and the console recovers as expected after CA certificates are rotated. (link:https://issues.redhat.com/browse/OCPBUGS-15827[*OCPBUGS-15827*])

* Previously, production builds of the console front-end bundle have historically had source maps disabled. As a consequence, browser tools for analyzing source code could not be used on production builds. With this release, the console Webpack configuration is updated to enable source maps on production builds. Browser tools will now work as expected for both dev and production builds. (link:https://issues.redhat.com/browse/OCPBUGS-10851[*OCPBUGS-10851*])

* Previously, the console redirect service had the same service Certificate Authority (CA) controller annotation as the console service. This caused the service CA controller to sometimes incorrectly sync CA certs for these services, and the console would not function correctly after removing and reinstalling. With this release, the console Operator was updated to remove this service CA annotation from the console redirect service. The console services and CA certs now function as expected when the Operator transitions from a removed to a managed state. (link:https://issues.redhat.com/browse/OCPBUGS-7656[*OCPBUGS-7656*])

[discrete]
[id="ocp-4-16-monitoring-bug-fixes_{context}"]
==== Monitoring

* Previously, setting an invalid `.spec.endpoints.proxyUrl` attribute in the `ServiceMonitor` resource would result in breaking, reloading, and restarting Prometheus. This update fixes the issue by validating the `proxyUrl` attribute against invalid syntax. (link:https://issues.redhat.com/browse/OCPBUGS-30989[*OCPBUGS-30989*])

[discrete]
[id="ocp-4-16-networking-bug-fixes_{context}"]
==== Networking

* Previously, the API documentation for the `status.componentRoutes.currentHostnames` field in the Ingress API included developer notes. After you entered the `oc explain ingresses.status.componentRoutes.currentHostnames --api-version=config.openshift.io/v1` command, developer notes would show in the output along with the intended information. With this release, the developer notes are removed from the `status.componentRoutes.currentHostnames` field, so that after you enter the command, the output lists current hostnames used by the route. (link:https://issues.redhat.com/browse/OCPBUGS-31058[*OCPBUGS-31058*])

* Previously, the load balancing algorithm did not differentiate between active and inactive services when determining weights, and it employed a random algorithm excessively in environments with many inactive services or environments routing backends with weight `0`. This led to increased memory usage and a higher risk of excessive memory consumption. With this release, changes optimize traffic direction towards active services only and prevent unnecessary use of a random algorithm with higher weights, reducing the potential for excessive memory consumption. (link:https://issues.redhat.com/browse/OCPBUGS-29690[*OCPBUGS-29690*])

* Previously, if multiple routes were specified in the same certificate or if a route specified the default certificate as a custom certificate, and HTTP/2 was enabled on the router, an HTTP/2 client could perform connection coalescing on routes. Clients, such as a web browser, could re-use connections and potentially connect to the wrong backend server. With this release, the {product-title} router now checks when the same certificate is specified on more than one route or when a route specifies the default certificate as a custom certificate. When either one of these conditions is detected, the router configures the HAProxy load balancer so to not allow HTTP/2 client connections to any routes that use these certificate.(link:https://issues.redhat.com/browse/OCPBUGS-29373[*OCPBUGS-29373*])

* Previously, if you configured a deployment with the `routingViaHost` parameter set to `true`, traffic failed to reach the IPv6 `ExternalTrafficPolicy=Local` load balancer service. With this release, the issue is fixed. (link:https://issues.redhat.com/browse/OCPBUGS-27211[*OCPBUGS-27211*])

* Previously, a pod selected by an `EgressIp` object that was hosted on a secondary network interface controller (NIC) caused connections to node IP addresses to timeout. With this release, the issue is fixed. (link:https://issues.redhat.com/browse/OCPBUGS-26979[*OCPBUGS-26979*])

* Previously, a leap file package that the {product-title} Precision Time Protocol (PTP) Operator installed could not be used by the `ts2phc` process because the package expired. With this release, the leap file package is updated to read leap events from Global Positioning System (GPS) signals and update the offset dynamically so that the expired package situation no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-25939[*OCPBUGS-25939*])

* Previously, pods assigned an IP from the pool created by the Whereabouts CNI plugin were getting stuck in the `ContainerCreating` state after a node forced a reboot. With this release, the Whereabouts CNI plugin issue associated with the IP allocation after a node force reboot is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-24608[*OCPBUGS-24608*])

* Previously, there was a conflict between two scripts on {product-title} in IPv6, including single and dual-stack, deployments. One script set the hostname to a fully qualified domain name (FQDN) but the other script might set it to a short name too early. This conflict happened because the event that triggered setting the hostname to FQDN might run after the script that set it to a short name. This occurred due to asynchronous network events. With this release, new code has been added to ensure that the FQDN is set properly. This new code ensures that there is a wait for a specific network event before allowing the hostname to be set. (link:https://issues.redhat.com/browse/OCPBUGS-22324[*OCPBUGS-22324*])

* Previously, if a pod selected by an `EgressIP` through a secondary interface had its label removed, another pod in the same namespace would also lose its `EgressIP` assignment, breaking its connection to the external host. With this release, the issue is fixed, so that when a pod label is removed and it stops using the `EgressIP`, other pods with the matching label continue to use the `EgressIP` without interruption. (link:https://issues.redhat.com/browse/OCPBUGS-20220[*OCPBUGS-20220*])

* Previously, `EgressIP` pods hosted by a secondary interface did not failover because of a race condition. Users would receive an error message indicating that the `EgressIP` pod could not be assigned because it conflicted with an existing IP address. With this release, the `EgressIP` pod moves to an egress node. (https://issues.redhat.com/browse/OCPBUGS-20209[*OCPBUGS-20209*])

* Previously, when a MAC address changed on the physical interface being used by OVN-Kubernetes, it would not be updated correctly within OVN-Kubernetes and could cause traffic disruption and Kube API outages from the node for a prolonged period of time. This was most common when a bond interface was being used, where the MAC address of the bond might swap depending on which device was the first to come up. With this release, the issues if fixed so that OVN-Kubernetes dynamically detects MAC address changes and updates it correctly. (link:https://issues.redhat.com/browse/OCPBUGS-18716[*OCPBUGS-18716*])

* Previously, the global navigation satellite system (GNSS) module was capable of reporting both the GPS `fix` position and the GNSS `offset` position, which represents the offset between the GNSS module and the constellations. The previous T-GM did not use the `ubloxtool` CLI tool to probe the `ublox` module for reading `offset` and `fix` positions. Instead, it could only read the GPS `fix` information via GPSD. The reason for this was that the previous implementation of the `ubloxtool` CLI tool took 2 seconds to receive a response, and with every call it increased CPU usage by threefold. With this release, the `ubloxtool` request is now optimized, and the GPS `offset` position is now available. (link:https://issues.redhat.com/browse/OCPBUGS-17422[*OCPBUGS-17422*])
* Previously, `EgressIP` pods hosted by a secondary interface would not failover because of a race condition. Users would receive an error message indicating that the `EgressIP` pod could not be assigned because it conflicted with an existing IP address. With this release, the `EgressIP` pod moves to an egress node. (https://issues.redhat.com/browse/OCPBUGS-20209[*OCPBUGS-20209*])

* Previously, when a MAC address changed on the physical interface being used by OVN-Kubernetes, it would not be updated correctly within OVN-Kubernetes and could cause traffic disruption and Kube API outages from the node for a prolonged period of time. This was most common when a bond interface was being used, where the MAC address of the bond might swap depending on which device was the first to come up. With this release, the issues if fixed so that OVN-Kubernetes dynamically detects MAC address changes and updates it correctly. (link:https://issues.redhat.com/browse/OCPBUGS-18716[*OCPBUGS-18716*])

* Previously, the global navigation satellite system (GNSS) module was capable of reporting both the GPS `fix` position and the GNSS `offset` position, which represents the offset between the GNSS module and the constellations. The previous T-GM did not use the `ubloxtool` CLI tool to probe the `ublox` module for reading `offset` and `fix` positions. Instead, it could only read the GPS `fix` information via GPSD. The reason for this was that the previous implementation of the `ubloxtool` CLI tool took two seconds to receive a response, and with every call it increased CPU usage by threefold. With this release, the `ubloxtool` request is now optimized, and the GPS `offset` position is now available. (link:https://issues.redhat.com/browse/OCPBUGS-17422[*OCPBUGS-17422*])

////
* Previously for {product-title} clusters on {azure-full} that used OVN-Kubernetes as the Container Network Interface (CNI), an issue existed where the source IP recognized by the pod was the OVN gateway router of the node when the load balancer service with `externalTrafficPolicy: Local` was used. This occurred due to a Source Network Address Translation (SNAT) being applied to UDP packets.
+
With this release, session affinity without a timeout is possible by setting the affinity timeout to a higher value, for example, `86400` seconds, or 24 hours. As a result, the affinity is treated as permanent unless there are network disruptions such as endpoints or nodes going down. As a result, session affinity is more persistent. (link:https://issues.redhat.com/browse/OCPBUGS-24219[*OCPBUGS-24219*])
////

* Previously, IPv6 was unsupported when assigning an egress IP to a network interface that was not the primary network interface. This issue has been resolved, and the egress IP can be IPv6. (https://issues.redhat.com/browse/OCPBUGS-24271[*OCPBUGS-24271*])

[discrete]
[id="ocp-4-16-node-bug-fixes_{context}"]
==== Node

[discrete]
[id="ocp-4-16-node-tuning-operator-bug-fixes_{context}"]
==== Node Tuning Operator (NTO)

[discrete]
[id="ocp-4-16-openshift-cli-bug-fixes_{context}"]
==== OpenShift CLI (oc)

* Previously, during the disk-to-mirror process for fully disconnected environments, the oc-mirror plugin v1 failed to mirror the catalog image when access to Red{nbsp}Hat registries was blocked. Additionally, if the `ImageSetConfiguration` used a `targetCatalog` for the mirrored catalog, mirroring would fail due to incorrect catalog image references regardless of the workflow. This issue has been resolved by updating the catalog image source for mirroring to the mirror registry. (link:https://issues.redhat.com/browse/OCPBUGS-31536[*OCPBUGS-31536*])

* Previously, when mirroring operator images with incompatible semantic versioning, oc-mirror plugin v2 (Technology Preview) would fail and exit. This fix ensures that a warning appears in the console, indicating the skipped image and allowing the mirroring process to continue without interruption. (link:https://issues.redhat.com/browse/OCPBUGS-34587[*OCPBUGS-34587*])

* Previously, oc-mirror plugin v2 (Technology Preview) failed to mirror certain Operator catalogs that included image references with both `tag` and `digest` formats. This issue prevented the creation of cluster resources, such as `ImageDigestMirrorSource` (IDMS) and `ImageTagMirrorSource` (ITMS). With this update, oc-mirror resolves the issue by skipping images that have both `tag` and `digest` references, while displaying an appropriate warning message in the console output. (link:https://issues.redhat.com/browse/OCPBUGS-33196[*OCPBUGS-33196*])

* Previously, with oc-mirror plugin v2 (Technology Preview), mirroring errors were only displayed in the console output, making it difficult for users to analyze and troubleshoot other issues. For example, an unstable network might require a rerun, while a manifest unknown error might need further analysis to skip an image or Operator. With this update, a file is generated that contains all errors in the workspace `working-dir/logs` folder. And all the errors that occur during the mirroring process are now logged in `mirroring_errors_YYYYMMdd.txt`. (link:https://issues.redhat.com/browse/OCPBUGS-33098[*OCPBUGS-33098*])

* Previously, the Cloud Credential Operator utility (`ccoctl`) could not run on a {op-system-base} 9 host with FIPS enabled. With this release, a user can run a version of the `ccoctl` utility that is compatible with the {op-system-base} version of their host, including {op-system-base} 9. (link:https://issues.redhat.com/browse/OCPBUGS-32080[*OCPBUGS-32080*])

* Previously, when mirroring operator catalogs, `oc-mirror` would rebuild the catalogs and regenerate their internal cache based on `imagesetconfig` catalog filtering specifications. This process required the `opm` binary from within the catalogs. Starting with version 4.15, operator catalogs include the `opm` {op-system-base} 9 binary, which caused the mirroring process to fail when executed on {op-system-base} 8 systems. With this release, `oc-mirror` no longer rebuilds catalogs by default; instead, it simply mirrors them to their destination registries.
+
To retain the catalog rebuilding functionality, use `--rebuild-catalog`. However, note that no changes were made to the current implementation, so using this flag might result in the cache not being generated or the catalog not being deployed to the cluster. If you use this command, you can export `OPM_BINARY` to specify a custom `opm` binary that corresponds to the catalog versions and platform found in {product-title}. Mirroring of catalog images is now done without signature verification. Use `--enable-operator-secure-policy` to enable signature verification during mirroring.
(link:https://issues.redhat.com/browse/OCPBUGS-31536[*OCPBUGS-31536*])

* Previously, some credentials requests were not extracted properly when running the `oc adm release extract --credentials-requests` command with an `install-config.yaml` file that included the `CloudCredential` cluster capability. With this release, the `CloudCredential` capability is correctly included in the OpenShift CLI (`oc`) so that this command extracts credentials requests properly. (link:https://issues.redhat.com/browse/OCPBUGS-24834[*OCPBUGS-24834*])

* Previously, users encountered sequence errors when using the `tar.gz` artifact with the oc-mirror plugin. To resolve this, the oc-mirror plugin now ignores these errors when executed with the `--skip-pruning` flag. This update ensures that the sequence error, which no longer affects the order of `tar.gz` usage in mirroring, is effectively handled. (link:https://issues.redhat.com/browse/OCPBUGS-23496[*OCPBUGS-23496*])

* Previously, when using the oc-mirror plugin to mirror local Open Container Initiative Operator catalogs located in hidden folders, oc-mirror previously failed with an error: ".hidden_folder/data/publish/latest/catalog-oci/manifest-list/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c is not a valid image reference: invalid reference format “. With this release, oc-mirror now calculates references to images within local Open Container Initiative catalogs differently, ensuring that the paths to hidden catalogs no longer disrupt the mirroring process.
(link:https://issues.redhat.com/browse/OCPBUGS-23327[*OCPBUGS-23327*])

* Previously, oc-mirror would not stop and return a valid error code when mirroring failed. With this release, oc-mirror now exits with the correct error code when encountering “operator not found”, unless the `--continue-on-error` flag is used. (link:https://issues.redhat.com/browse/OCPBUGS-23003[*OCPBUGS-23003*])

* Previously, when mirroring operators, oc-mirror would ignore the `maxVersion` constraint in `imageSetConfig` if both `minVersion` and `maxVersion` were specified. This resulted in mirroring all bundles up to the channel head. With this release, oc-mirror now considers the `maxVersion` constraint as specified in `imageSetConfig`.
(link:https://issues.redhat.com/browse/OCPBUGS-21865[*OCPBUGS-21865*])

* Previously, oc-mirror failed to mirror releases using `eus-\*` channels, as it did not recognize that `eus-\*` channels are designated for even-numbered releases only. With this release, oc-mirror plugin now properly acknowledges that `eus-\*` channels are intended for even-numbered releases, enabling users to successfully mirror releases using these channels.
(link:https://issues.redhat.com/browse/OCPBUGS-19429[*OCPBUGS-19429*])

* Previously, the addition of the `defaultChannel` field in the `mirror.operators.catalog.packages` file enabled users to specify their preferred channel, overriding the `defaultChannel` set in the operator. With this release, oc-mirror plugin now enforces an initial check if the `defaultChannel` field is set, users must also define it in the channels section of the `ImageSetConfig`. This update ensures that the specified `defaultChannel` is properly configured and applied during operator mirroring. (link:https://issues.redhat.com/browse/OCPBUGS-385[*OCPBUGS-385*])

* Previously, when running a cluster with FIPS enabled, you might have received the following error when running the {oc-first} on a {op-system-base} 9 system: `FIPS mode is enabled, but the required OpenSSL backend is unavailable`. With this release, the default version of OpenShift CLI (`oc`) is compiled with {op-system-base-full} 9 and works properly when running a cluster with FIPS enabled on {op-system-base} 9. Additionally, a version of `oc` compiled with {op-system-base} 8 is also provided, which must be used if you are running a cluster with FIPS enabled on {op-system-base} 8. (link:https://issues.redhat.com/browse/OCPBUGS-23386[*OCPBUGS-23386*], link:https://issues.redhat.com/browse/OCPBUGS-28540[*OCPBUGS-28540*])

[discrete]
[id="ocp-4-16-olm-bug-fixes_{context}"]
==== Operator Lifecycle Manager (OLM)

* Previously, Operator catalogs were not being refreshed properly, due to the `imagePullPolicy` field being set to `IfNotPresent` for the index image. This bug fix updates {olm} to use the appropriate image pull policy for catalogs, and as a result catalogs are refreshed properly. (link:https://issues.redhat.com/browse/OCPBUGS-30132[*OCPBUGS-30132*])

* Previously, cluster upgrades could be blocked due to {olm} getting stuck in a `CrashLoopBackOff` state. This was due to an issue with resources having multiple owner references. This bug fix updates {olm} to avoid duplicate owner references and only validate the related resources that it owns. As a result, cluster upgrades can proceed as expected. (link:https://issues.redhat.com/browse/OCPBUGS-28744[*OCPBUGS-28744*])

* Previously, default {olm} catalog pods backed by a `CatalogSource` object would not survive an outage of the node that they were being run on. The pods remained in termination state, despite the tolerations that should move them. This caused Operators to no longer be able to be installed or updated from related catalogs. This bug fix updates {olm} so catalog pods that get stuck in this state are deleted. As a result, catalog pods now correctly recover from planned or unplanned node maintenance. (link:https://issues.redhat.com/browse/OCPBUGS-32183[*OCPBUGS-32183*])

* Previously, installing an Operator could sometimes fail if the same Operator had been previously installed and uninstalled. This was due to a caching issue. This bug fix updates {olm} to correctly install the Operator in this scenario, and as a result this issue no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-31073[*OCPBUGS-31073*])

* Previously, the `catalogd` component could crash loop after an etcd restore. This was due to the garbage collection process causing a looping failure state when the API server was unreachable. This bug fix updates `catalogd` to add a retry loop, and as a result `catalogd` no longer crashes in this scenario. (link:https://issues.redhat.com/browse/OCPBUGS-29453[*OCPBUGS-29453*])

* Previously, the default catalog source pod would not receive updates, requiring users to manually re-create it to get updates. This was caused by image IDs for catalog pods not getting detected correctly. This bug fix updates {olm} to correctly detect catalog pod image IDs, and as a result, default catalog sources are updated as expected. (link:https://issues.redhat.com/browse/OCPBUGS-31438[*OCPBUGS-31438*])

* Previously, users could experience Operator installation errors due to {olm} not being able to find existing `ClusterRoleBinding` or `Service` resources and creating them a second time. This bug fix updates {olm} to pre-create these objects, and as a result these installation errors no longer occur. (link:https://issues.redhat.com/browse/OCPBUGS-24009[*OCPBUGS-24009*])

[discrete]
[id="ocp-4-16-openshift-api-server-bug-fixes_{context}"]
==== OpenShift API server

[discrete]
[id="ocp-4-16-rhcos-bug-fixes_{context}"]
==== {op-system-first}

[discrete]
[id="ocp-4-16-scalability-and-performance-bug-fixes_{context}"]
==== Scalability and performance

[discrete]
[id="ocp-4-16-storage-bug-fixes_{context}"]
==== Storage

* Previously, some `LVMVolumeGroupNodeStatus` operands were not deleted on the cluster during the deletion of the `LVMCluster` custom resource (CR). With this release, deleting the `LVMCluster` CR triggers the deletion of all the `LVMVolumeGroupNodeStatus` operands. (link:https://issues.redhat.com/browse/OCPBUGS-32954[*OCPBUGS-32954*])

* Previously, {lvms} uninstallation was stuck waiting for the deletion of the `LVMVolumeGroupNodeStatus` operands. This fix corrects the behavior by ensuring all operands are deleted, allowing {lvms} to be uninstalled without delay. (link:https://issues.redhat.com/browse/OCPBUGS-32753[*OCPBUGS-32753*])

* Previously, {lvms} did not support minimum storage size for persistent volume claims (PVCs). This can lead to mount failures while provisioning PVCs. With this release, {lvms} supports minimum storage size for PVCs. The following are the minimum storage sizes that you can request for each file system type:

** `block`: 8 MiB
** `xfs`: 300 MiB
** `ext4`: 32 MiB
+
If the value of the `requests.storage` field in the `PersistentVolumeClaim` object is less than the minimum storage size, the requested storage size is rounded to the minimum storage size. If the value of the `limits.storage field` is less than the minimum storage size, PVC creation fails with an error. (link:https://issues.redhat.com/browse/OCPBUGS-30266[*OCPBUGS-30266*])

* Previously, {lvms} created persistent volume claims (PVCs) with storage size requests that were not multiples of the disk sector size. This can cause issues during LVM2 volume creation. This fix corrects the behavior by rounding the storage size requested by PVCs to the nearest multiple of 512. (link:https://issues.redhat.com/browse/OCPBUGS-30032[*OCPBUGS-30032*])

* Previously, the `LVMCluster` custom resource (CR) contained an excluded status element for a device that is set up correctly. This fix filters the correctly set device from being considered for an excluded status element, so it only appears in the ready devices. (link:https://issues.redhat.com/browse/OCPBUGS-29188[*OCPBUGS-29188*])

[discrete]
[id="ocp-4-16-windows-containers-bug-fixes_{context}"]
==== Windows containers

[id="ocp-4-16-technology-preview-tables_{context}"]
== Technology Preview features status

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the following tables, features are marked with the following statuses:

* _Technology Preview_
* _General Availability_
* _Not Available_
* _Deprecated_

[discrete]
=== Networking Technology Preview features

.Networking Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Ingress Node Firewall Operator
|General Availability
|General Availability
|General Availability

|Advertise using L2 mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Technology Preview
|Technology Preview
|Technology Preview

|Multi-network policies for SR-IOV networks
|Technology Preview
|General Availability
|General Availability

|OVN-Kubernetes network plugin as secondary network
|General Availability
|General Availability
|General Availability

|Updating the interface-specific safe sysctls list
|Technology Preview
|Technology Preview
|Technology Preview

|Egress service custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|VRF specification in `BGPPeer` custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|VRF specification in `NodeNetworkConfigurationPolicy` custom resource
|Technology Preview
|Technology Preview
|Technology Preview

|Admin Network Policy (`AdminNetworkPolicy`)
|Technology Preview
|Technology Preview
|General Availability

|IPsec external traffic (north-south)
|Technology Preview
|General Availability
|General Availability

|Integration of MetalLB and FRR-K8s
|Not Available
|Not Available
|Technology Preview

|Dual-NIC hardware as PTP boundary clock
|General Availability
|General Availability
|General Availability

|Dual-NIC Intel E810 PTP boundary clock with highly available system clock
|Not Available
|Not Available
|General Availability

|Intel E810 Westport Channel NIC as PTP grandmaster clock
|Technology Preview
|Technology Preview
|General Availability

|Dual-NIC Intel E810 Westport Channel as PTP grandmaster clock
|Not Available
|Technology Preview
|General Availability

|Configure the `br-ex` bridge needed by OVN-Kuberenetes using NMState
|Not Available
|Not Available
|Technology Preview

| Live migration to OVN-Kubernetes from OpenShift SDN
| Not Available
| Not Available
| General Availability
|====

[discrete]
=== Storage Technology Preview features

.Storage Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Automatic device discovery and provisioning with Local Storage Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Google Filestore CSI Driver Operator
|General Availability
|General Availability
|General Availability

|{ibm-power-server-name} Block CSI Driver Operator
|Technology Preview
|General Availability
|General Availability

|Read Write Once Pod access mode
|Technology Preview
|Technology Preview
|General Availability

|Build CSI Volumes in OpenShift Builds
|General Availability
|General Availability
|General Availability

|Shared Resources CSI Driver in OpenShift Builds
|Technology Preview
|Technology Preview
|Technology Preview

|{secrets-store-operator}
|Technology Preview
|Technology Preview
|Technology Preview

|CIFS/SMB CSI Driver Operator
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Installation Technology Preview features

.Installation Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Installing {product-title} on {oci-first} with VMs
|Developer Preview
|Technology Preview
|Technology Preview

|Installing {product-title} on {oci-first} on bare metal
|Developer Preview
|Developer Preview
|Developer Preview

|Adding kernel modules to nodes with kvc
|Technology Preview
|Technology Preview
|Technology Preview

|Enabling NIC partitioning for SR-IOV devices
|Technology Preview
|Technology Preview
|Technology Preview

|User-defined labels and tags for Google Cloud Platform (GCP)
|Technology Preview
|Technology Preview
|Technology Preview

|Installing a cluster on Alibaba Cloud by using installer-provisioned infrastructure
|Technology Preview
|Technology Preview
|Technology Preview

|Mount shared entitlements in BuildConfigs in RHEL
|Technology Preview
|Technology Preview
|Technology Preview

|{product-title} on Oracle Cloud Infrastructure (OCI)
|Developer Preview
|Technology Preview
|Technology Preview

|Selectable Cluster Inventory
|Technology Preview
|Technology Preview
|Technology Preview

|Static IP addresses with vSphere (IPI only)
|Technology Preview
|Technology Preview
|General Availability

|Support for iSCSI devices in {op-system}
|Not Available
|Technology Preview
|General Availability

|====

[discrete]
=== Node Technology Preview features

.Nodes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|`MaxUnavailableStatefulSet` featureset
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Multi-Architecture Technology Preview features

.Multi-Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|{ibm-power-server-name} using installer-provisioned infrastructure
|Technology Preview
|General Availability
|General Availability

|`kdump` on `arm64` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `s390x` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `ppc64le` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|Multiarch Tuning Operator
|Not available
|Not available
|Technology Preview

|====

[discrete]
=== Specialized hardware and driver enablement Technology Preview features

.Specialized hardware and driver enablement Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Driver Toolkit
|General Availability
|General Availability
|General Availability

|Kernel Module Management Operator
|General Availability
|General Availability
|General Availability

|Kernel Module Management Operator - Hub and spoke cluster support
|General Availability
|General Availability
|General Availability

|Node Feature Discovery
|General Availability
|General Availability
|General Availability

|====

//Removed multicluster console. It has not been in development for the past few releases.

[discrete]
[id="ocp-4-16-scalability-tech-preview_{context}"]
=== Scalability and performance Technology Preview features

.Scalability and performance Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|{factory-prestaging-tool}
|Technology Preview
|Technology Preview
|Technology Preview

|Hyperthreading-aware CPU manager policy
|Technology Preview
|Technology Preview
|Technology Preview

|HTTP transport replaces AMQP for PTP and bare-metal events
|Technology Preview
|Technology Preview
|General Availability

|Mount namespace encapsulation
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Tuning etcd latency tolerances
|Technology Preview
|Technology Preview
|General Availability

|Increasing the etcd database size
|Not Available
|Not Available
|Technology Preview

|Using {rh-rhacm} `PolicyGenerator` resources to manage {ztp} cluster policies
|Not Available
|Not Available
|Technology Preview
|====

[discrete]
[id="ocp-4-16-operators-tech-preview_{context}"]
=== Operator lifecycle and development Technology Preview features

.Operator lifecycle and development Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Operator Lifecycle Manager (OLM) v1
|Technology Preview
|Technology Preview
|Technology Preview

|RukPak
|Technology Preview
|Technology Preview
|Technology Preview

|Platform Operators
|Technology Preview
|Technology Preview
|Removed

|Scaffolding tools for Hybrid Helm-based Operator projects
|Technology Preview
|Technology Preview
|Deprecated

|Scaffolding tools for Java-based Operator projects
|Technology Preview
|Technology Preview
|Deprecated
|====

[discrete]
=== Monitoring Technology Preview features

.Monitoring Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Metrics Collection Profiles
|Technology Preview
|Technology Preview
|Technology Preview

|Metrics Server
|Not Available
|Technology Preview
|General Availability

|====


[discrete]
=== {rh-openstack-first} Technology Preview features

.{rh-openstack} Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Dual-stack networking with installer-provisioned infrastructure
|Technology Preview
|General Availability
|General Availability

|Dual-stack networking with user-provisioned infrastructure
|Not Available
|General Availability
|General Availability

|CAPO integration into the cluster CAPI Operator
|Not Available
|Technology Preview
|Technology Preview

|Control Plane with `rootVolumes` and `etcd` on local disk
|Not Available
|Technology Preview
|Technology Preview
|====

[discrete]
=== Hosted Control Plane Technology Preview features

.Hosted Control Plane Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Hosted control planes for {product-title} on Amazon Web Services (AWS)
|Technology Preview
|Technology Preview
|Technology Preview

|Hosted control planes for {product-title} on bare metal
|General Availability
|General Availability
|General Availability

|Hosted control planes for {product-title} on {VirtProductName}
|General Availability
|General Availability
|General Availability

|Hosted control planes for {product-title} using non-bare metal agent machines
|Not Available
|Technology Preview
|Technology Preview

|Hosted control planes for an ARM64 {product-title} cluster on AWS
|Technology Preview
|Technology Preview
|Technology Preview

|Hosted control planes for {product-title} on {ibm-power-title}
|Technology Preview
|Technology Preview
|Technology Preview

|Hosted control planes for {product-title} on {ibm-z-title}
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Machine management Technology Preview features

.Machine management Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Managing machines with the Cluster API for {aws-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {gcp-full}
|Technology Preview
|Technology Preview
|Technology Preview

|Managing machines with the Cluster API for {vmw-full}
|Not Available
|Not Available
|Technology Preview

|Defining a {vmw-short} failure domain for a control plane machine set
|Not Available
|Technology Preview
|General Availability

|Cloud controller manager for Alibaba Cloud
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Google Cloud Platform
|Technology Preview
|General Availability
|General Availability

|Cloud controller manager for {ibm-power-server-name}
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Authentication and authorization Technology Preview features

.Authentication and authorization Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Pod security admission restricted enforcement
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Machine Config Operator Technology Preview features

.Machine Config Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Improved MCO state reporting
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
[id="edge-computing-tp-features_{context}"]
=== Edge computing Technology Preview features

.Edge computing Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.14 |4.15 |4.16

|Accelerated provisioning of {ztp}
|Not Available
|Not Available
|Technology Preview

|Deploying IPsec encryption to managed clusters with {ztp} and {rh-rhacm}
|Not Available
|Not Available
|Technology Preview
|====

[id="ocp-4-16-known-issues_{context}"]
== Known issues

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* {run-once-operator} (RODOO) cannot be installed on clusters managed by the Hypershift Operator. (link:https://issues.redhat.com/browse/OCPBUGS-17533[*OCPBUGS-17533*])

* {product-title} {product-version} installation on {aws-short} in a secret or top secret region fails due to an issue with Network Load Balancers (NLBs) and security groups in these regions. (link:https://issues.redhat.com/browse/OCPBUGS-33311[*OCPBUGS-33311*])

* When you run Cloud-native Network Functions (CNF) latency tests on an {product-title} cluster, the `oslat` test can sometimes return results greater than 20 microseconds. This results in an `oslat` test failure.
(link:https://issues.redhat.com/browse/RHEL-9279[*RHEL-9279*])

* When installing a cluster on {aws-first} using Local Zones, edge nodes fail to deploy if deployed in the `us-east-1-iah-2a` region. (link:https://issues.redhat.com/browse/OCPBUGS-35538[*OCPBUGS-35538*])

* For a bonding network interface that holds a `br-ex` bridge device, do not set the `mode=6 balance-alb` bond mode in a node network configuration. This bond mode is not supported on {product-title} and it can cause the Open vSwitch (OVS) bridge device to disconnect from your networking environment. (link:https://issues.redhat.com/browse/OCPBUGS-34430[*OCPBUGS-34430*]).

// HCIDOCS-365 - This issue should be fixed soon, in a 4.16.z release
* Do not update firmware for the `BareMetalHosts` (BMH) resource by editing the `HostFirmwareComponents` resource. Otherwise, the BMH remains in the `Preparing` state and executes the firmware update repeatedly. There is no workaround. (link:https://issues.redhat.com/browse/OCPBUGS-35559[*OCPBUGS-35559*])

* When installing a cluster on {aws-first} in a VPC that contains multiple CIDR blocks, if the machine network is configured to use a non-default CIDR block in the `install-config.yaml` file, the installation fails. (link:https://issues.redhat.com/browse/OCPBUGS-35054[*OCPBUGS-35054*])

[id="ocp-telco-ran-4-16-known-issues_{context}"]
* The current PTP grandmaster clock (T-GM) implementation has a single National Marine Electronics Association (NMEA) sentence generator sourced from the GNSS without a backup NMEA sentence generator.
If NMEA sentences are lost before reaching the e810 NIC, the T-GM cannot synchronize the devices in the network synchronization chain and the PTP Operator reports an error.
A proposed fix is to report a `FREERUN` event when the NMEA string is lost.
Until this limitation is addressed, T-GM does not support PTP clock holdover state.
(link:https://issues.redhat.com/browse/OCPBUGS-19838[*OCPBUGS-19838*])

[id="ocp-telco-core-4-16-known-issues_{context}"]

* When a worker node's Topology Manager policy is changed, the NUMA-aware secondary pod scheduler does not respect this change, which can result in incorrect scheduling decisions and unexpected topology affinity errors. As a workaround, restart the NUMA-aware scheduler by deleting the NUMA-aware scheduler pod. (link:https://issues.redhat.com/browse/OCPBUGS-34583[*OCPBUGS-34583*])

* Due to an issue with Kubernetes, the CPU Manager is unable to return CPU resources from the last pod admitted to a node to the pool of available CPU resources. These resources are allocatable if a subsequent pod is admitted to the node. However, this pod then becomes the last pod, and again, the CPU manager cannot return this pod's resources to the available pool.
+
This issue affects CPU load balancing features, which depend on the CPU Manager releasing CPUs to the available pool. Consequently, non-guaranteed pods might run with a reduced number of CPUs. As a workaround, schedule a pod with a `best-effort` CPU Manager policy on the affected node. This pod will be the last admitted pod and this ensures the resources will be correctly released to the available pool.(link:https://issues.redhat.com/browse/OCPBUGS-17792[*OCPBUGS-17792*])

* After applying a `SriovNetworkNodePolicy` resource, the CA certificate might be replaced during SR-IOV Network Operator webhook reconciliation. As a consequence, you might see `unknown authority` errors when applying SR-IOV Network node policies. As a workaround, try to re-apply the failed policies. (link:https://issues.redhat.com/browse/OCPBUGS-32139[*OCPBUGS-32139*])

* If you delete a `SriovNetworkNodePolicy` resource for a virtual function with a `vfio-pci` driver type, the SR-IOV Network Operator is unable to reconcile the policy. As a consequence the `sriov-device-plugin` pod enters a continuous restart loop. As a workaround, delete all remaining policies affecting the physical function, then re-create them. (link:https://issues.redhat.com/browse/OCPBUGS-34934[*OCPBUGS-34934*])

[id="ocp-4-16-asynchronous-errata-updates_{context}"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating_a_cluster/updating-cluster-web-console.adoc#updating-cluster-web-console[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-16-0-ga_{context}"]
=== RHSA-2024:XXXX - {product-title} {product-version}.0 image release, bug fix, and security update advisory

Issued: TBD

{product-title} release {product-version}.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2023:7198[RHSA-2023:7198] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2023:7201[RHSA-2023:7201] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.16.0 --pullspecs
----
//replace 4.y.z for the correct values for the release. You do not need to update oc to run this command.
