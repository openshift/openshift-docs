[id="ocp-4-6-release-notes"]
= {product-title} {product-version} release notes
include::modules/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid
cloud application platform for deploying both new and existing applications on
secure, scalable resources with minimal configuration and management overhead.
{product-title} supports a wide selection of programming languages and
frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on Red Hat Enterprise Linux and Kubernetes, {product-title}
provides a more secure and scalable multi-tenant operating system for today's
enterprise-class applications, while delivering integrated application runtimes
and libraries. {product-title} enables organizations to meet security, privacy,
compliance, and governance requirements.

[id="ocp-4-6-about-this-release"]
== About this release

// TODO: Update this link once there is a 1.19-specific URL for the Kubernetes 1.19 release notes (requested in https://github.com/kubernetes/website/issues/23855)
Red Hat {product-title}
(link:https://access.redhat.com/errata/RHBA-2020:1234[RHBA-2020:1234]) is now
available. This release uses link:https://kubernetes.io/docs/setup/release/notes/[Kubernetes 1.19] with CRI-O runtime. New features, changes, and known issues that pertain to
{product-title} {product-version} are included in this topic.

//Red Hat did not publicly release {product-title} 4.5.0 as the GA version and,
//instead, is releasing {product-title} 4.5.1 as the GA version.

{product-title} {product-version} clusters are available at
https://cloud.redhat.com/openshift. The {cloud-redhat-com}
application for {product-title} allows you to deploy OpenShift clusters to
either on-premise or cloud environments.

{product-title} {product-version} is supported on Red Hat Enterprise Linux 7.7 or
later, as well as {op-system-first} 4.6.

You must use {op-system} for the control plane, which are also known as master machines, and
can use either {op-system} or Red Hat Enterprise Linux 7.7 or later for
compute machines, which are also known as worker machines.

[IMPORTANT]
====
Because only Red Hat Enterprise Linux version 7.7 or later is supported for compute
machines, you must not upgrade the Red Hat Enterprise Linux compute machines to
version 8.
====

With the release of {product-title} 4.6, version 4.3 is now end of life. For
more information, see the
link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift Container Platform Life Cycle Policy].

[id="ocp-4-6-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-6-rhcos"]
=== {op-system-first}

[id="ocp-4-6-live-env-with-coreos-installer"]
==== {op-system} PXE and ISO now live environment

The PXE media and ISO available for {op-system} are now a fully live
environment. Unlike the previous dedicated PXE media and ISO used for
{op-system} installation for {product-title} clusters on user-provisioned
infrastructure, the {op-system} live environment can be configured with Ignition
and contains all the same packages as the main {op-system} image, such as
`coreos-installer`, `nmcli`, and `podman`. This allows arbitrary scripting of
pre- or post-installation workflows. For example, you could run
`coreos-installer` and then make an HTTP request to signal success to a
provisioning server. PXE boots use the normal `ignition.config.url`. The ISO can
be configured with Ignition by using the following command:

[source,terminal]
----
$ coreos-installer iso ignition embed
----

[id="ocp-4-6-coreos-installer-rewritten"]
==== `coreos-installer` has been rewritten

The `coreos-installer` is now rewritten to support more features including:

* Modifying the kernel arguments of the installed system.
* Fetching Ignition configs.
* Preserving previously existing partitions.
* Configuring Ignition for the new live ISO using the `coreos-installer iso ignition` command.

[id="ocp-4-6-ignition-spect-updated-v3"]
==== Ignition Spec updated to v3

{op-system} now uses Ignition spec v3 as the only supported spec
version of Ignition. This allows for more complex disk configuration support
in the future.

The change should be mostly transparent for those using installer-provisioned
infrastructure. For user-provisioned infrastructure installations, you must
adapt any custom Ignition configurations to use Ignition spec 3. The
`openshift-install` program now generates Ignition spec 3.

If you are creating Machine Configs for day 1 or day 2 operations that use
Ignition snippets, they should be created using Ignition spec v3. However, the
Machine Config Operator (MCO) still supports Ignition spec v2.

[id="ocp-4-6-extensions-supported-for-rhcos-mco"]
==== Extensions now supported for {op-system} and MCO

{op-system} and the MCO now support the following extensions to the default
{op-system} installation.

* `kernel-devel`
* `usbguard`

[id="ocp-4-6-4kn-disk-support"]
==== 4Kn Disks now supported

{op-system} now supports installing to disks that use 4K sector sizes.

[id="ocp-4-6-4k-var-partition-support"]
==== `/var` partitions now supported

{op-system} now supports `/var` being a separate partition, as well as any other
subdirectory of `/var`.

[id="ocp-4-6-static-ip-config-with-ova"]
==== Static IP configuration for vSphere using OVA

You can now override default Dynamic Host Configuration Protocol (DHCP)
networking by setting the `guestinfo.afterburn.initrd.network-kargs` property
before booting a VM from an OVA in vSphere:

[source,terminal]
----
$ govc vm.change -vm "<vm_name>" -e "guestinfo.afterburn.initrd.network-kargs=<static_ip_config>"
----

This lowers the barrier for automatic {op-system-first} deployment in
environments without DHCP. This enhancement allows for higher-level automation
to provision an {op-system} OVA in environments with static networking.

For more information, see
link:https://bugzilla.redhat.com/show_bug.cgi?id=1785122[*BZ1785122*].

[id="ocp-4-6-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-6-installing-aws-govcloud"]
==== Installing a cluster into an AWS GovCloud region

You can now install a cluster on Amazon Web Services (AWS) into a GovCloud
region. AWS GovCloud is designed for US government agencies, contractors,
educational institutions, and other US customers that must run sensitive
workloads.

Because GovCloud regions do not have {op-system} AMIs published by Red Hat, you
must upload a custom AMI that belongs to that region.

// For more information, see installing/installing_aws/installing-aws-government-region.adoc

[id="ocp-4-6-defining-custom-aws-api-endpoints"]
==== Defining custom AWS API endpoints

You can now define a `serviceEndpoints` field in the `install-config.yaml`
file, which lets you specify a list of custom endpoints to override the default
service endpoints on AWS.

[id="ocp-4-6-installing-azure-gov"]
==== Installing a cluster into a Microsoft Azure Government region

You can now install a cluster on Azure into a Microsoft Azure Government (MAG)
region. Microsoft Azure Government (MAG) is designed for US government agencies
and their partners that must run sensitive workloads.

// For more information, see installing/installing_azure/installing-azure-government-region.adoc

[id="ocp-4-6-udr-for-azure"]
==== User-defined outbound routing for clusters running on Azure

You can now choose your own outbound routing for a cluster running on Azure to
connect to the internet. This allows you to skip the creation of public IP
addresses and public load balancers.

// For more information, see installing/installing_azure/installing-azure-private.adoc#installation-azure-user-defined-routing_installing-azure-private

[id="ocp-4-6-vsphere-v7"]
==== Installing a cluster to vSphere version 7.0

You can now deploy a cluster to VMware vSphere version 7.0. See xref:../installing/installing_vsphere/installing-vsphere-installer-provisioned.adoc#installation-vsphere-infrastructure_installing-vsphere-installer-provisioned[VMware vSphere infrastructure requirements] for more information.

[id="ocp-4-6-bare-metal-ipi"]
==== Installing a cluster on bare metal using installer-provisioned infrastructure

{product-title} {product-version} introduces support for installing a cluster on
bare metal using installer-provisioned infrastructure.

For more information, see xref:../installing/installing_bare_metal_ipi/ipi-install-overview.adoc#ipi-install-overview[Installing a cluster on bare metal]

[id="ocp-4-6-handling-credential-requests"]
==== Handling credential requests for cloud API access on AWS, Azure, and GCP

There is now a new `credentialsMode` field in the `install-config.yaml` file
that defines how `CredentialsRequest`s are handled for {product-title}
components requiring cloud API access on AWS, Azure, and GCP. There are three
new modes that can be configured:

* Mint
* Passthrough
* Manual

If the `credentialsMode` field is set to any of the three modes, the
installation program does not check the credential for proper permissions prior
to installing {product-title}. This is useful for when the supplied user
credentials cannot be properly validated due to limitations in the cloud policy
simulator.

// For more information on these modes, see operators/operator-reference.adoc#cloud-credential-operator_red-hat-operators[Cloud Credential Operator].

[id="ocp-4-6-specifying-disk-type-and-size-for-nodes"]
==== Specifying disk type and size for control plane and compute nodes

You can now configure the disk type and size on control plane and compute nodes
for clusters running on Azure and GCP. This can be specified in the
`install-config.yaml` file with the following fields:

* `osDisk.diskSizeGB`
* `osDisk.diskType`

For example:

[source,yaml]
----
...
compute:
...
  platform:
  - osDisk:
      diskSizeGB: 120
      diskType: pd-standard
  replicas: 3
controlPlane:
...
  platform:
  - osDisk:
      diskSizeGB: 120
      diskType: pd-ssd
...
----

[id="ocp-4-6-latest-version-operators-required"]
==== Latest version of Operators required before cluster upgrade

Starting in {product-title} 4.6, the Red Hat-provided default catalogs used by Operator Lifecycle Manager (OLM) and OperatorHub are now shipped as index images specific to the minor version of {product-title}. Cluster administrators must ensure all Operators previously installed through OLM are updated to their latest versions in their latest channels before upgrading to {product-title} 4.6.

See xref:../release_notes/ocp-4-6-release-notes.adoc#ocp-4-6-operator-catalogs-per-version[Default Operator catalogs now shipped per cluster version] for more details and important Operator upgrade prerequisites.

[id="ocp-4-6-deployment-without-a-provisioning-network"]
==== Deployment without a provisioning network

{product-title} now supports deployment without a provisioning network and for RedFish Virtual Media.

See xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#ipi-install-installation-workflow[Setting up the environment for an OpenShift installation] for more information.

[id="ocp-4-6-deployment-root-device-hints"]
==== Deployment now supports root device hints

Deployment now supports xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#root-device-hints_ipi-install-configuration-files[root device hints].

[id="ocp-4-6-installer-improvements"]
==== Installer improvements

Deployment now performs introspection on nodes to ensure that nodes meet installation requirements instead of generating errors if they do not.

[id="ocp-4-6-installation-osp-availability-zones"]
==== {rh-openstack} availability zones selection at installation

You can now select {rh-openstack-first} Compute (Nova) availability zones while installing a cluster on {rh-openstack}.

For more information, see the {product-title} on {rh-openstack} installation documentation.

[id="ocp-4-6-installation-osp-floating-ips"]
==== Floating IP addresses are no longer required for installation on {rh-openstack}

You no longer need floating IP addresses to complete a {product-title} installation on {rh-openstack}.

For more information, see the {product-title} on {rh-openstack} installation documentation.

[id="ocp-4-6-security"]
=== Security and compliance

[id="ocp-4-6-compliance-operator"]
==== Compliance Operator

The Compliance Operator is now available. This feature allows the use of OpenSCAP tools to check that a deployment complies with security standards and provides remediation options. See xref:../security/compliance_operator/compliance-operator-understanding.adoc#compliance-operator-understanding[Understanding the Compliance Operator] for more information.

[id="ocp-4-6-oauth-token-inactivity-timeout"]
==== Configure OAuth token inactivity timeout

You can now configure OAuth tokens to expire after a certain amount of time that they have been inactive. By default, there is no token inactivity timeout set. You can configure the timeout for the internal OAuth server and for OAuth clients.

See xref:../authentication/configuring-internal-oauth.adoc#oauth-token-inactivity-timeout_configuring-internal-oauth[Configuring token inactivity timeout for the internal OAuth server] and xref:../authentication/configuring-oauth-clients.adoc#oauth-token-inactivity-timeout_configuring-oauth-clients[Configuring token inactivity timeout for an OAuth client] for more information.

[id="ocp-4-6-oauth-token-storage"]
==== Secure OAuth token storage format

OAuth access token and OAuth authorize token object names are now stored as non-sensitive object names.

Previously, secret information was used as the OAuth access token and OAuth authorize token object names. When etcd is encrypted, only the value is encrypted, so this sensitive information was not encrypted.

[IMPORTANT]
====
If you are upgrading your cluster to {product-title} 4.6, old tokens from {product-title} 4.5 will still have the secret information exposed in the object name. By default, the expiration for tokens is 24 hours, but this setting can be changed by administrators. Sensitive data can still be exposed until all old tokens have either expired or have been deleted by an administrator.
====


[id="ocp-4-6-file-integrity-operator"]
==== File Integrity Operator is now available

The
xref:../security/file_integrity_operator/file-integrity-operator-understanding.adoc#understanding-file-integrity-operator[File
Integrity Operator], an {product-title} Operator that continually runs file
integrity checks on the cluster nodes, is now available. It deploys a daemon set
that initializes and runs privileged advanced intrusion detection environment
(AIDE) containers on each node, providing a status object with a log of files
that are modified during the initial run of the daemon set Pods.

[id="ocp-4-6-machine-api"]
=== Machine API

[id="ocp-4-6-machine-api-multiple-block-device-mappings"]
==== Support for multiple block device mappings

The Machine API now supports multiple block device mappings for machines running on AWS. If more than one block device is given, you can now store logs, data in empty directory pods, and docker images in block devices that are separate from the root device on a machine.

[id="ocp-4-6-default-validation-providerspec-apis"]
==== Defaults and validation for the Machine API `providerSpec`

Defaults and validation are now enabled on a particular cloud provider API before input from the `providerSpec` is persisted to etcd. Validation is run against machines and MachineSets when they are created. Feedback is returned when the configuration is known to prevent machines from being created by the cloud provider. For example, a MachineSet is rejected if location information is required but is not provided.

[id="ocp-4-6-azure-machinesets-support-spot-vms"]
==== MachineSets running on Azure support Spot VMs

MachineSets running on Azure now support Spot VMs. You can create a MachineSet
that deploys machines as Spot VMs to save on costs compared to
standard VM prices. For more information, see xref:../machine_management/creating_machinesets/creating-machineset-azure.adoc#machineset-non-guaranteed-instance_creating-machineset-azure[MachineSets that deploy machines as Spot VMs].

Configure Spot VMs by adding `spotVMOptions` under the `providerSpec` field in the MachineSet YAML file:

[source,yaml]
----
providerSpec:
  value:
    spotVMOptions: {}
----

[id="ocp-4-6-gcp-machinesets-support-preemptible-vm-instances"]
==== MachineSets running on GCP support preemptible VM instances

MachineSets running on GCP now support preemptible VM instances. You can create a MachineSet
that deploys machines as preemptible VM instances to save on costs compared to
normal instance prices. For more information, see xref:../machine_management/creating_machinesets/creating-machineset-gcp.adoc#machineset-non-guaranteed-instance_creating-machineset-gcp[MachineSets that deploy machines as preemptible VM instances].

Configure preemptible VM instances by adding `preemptible` under the `providerSpec` field in the MachineSet YAML file:

[source,yaml]
----
providerSpec:
  value:
    preemptible: true
----

[id="ocp-4-6-web-console"]
=== Web console

[id="ocp-4-6-web-console-improved-upgrade-experience"]
==== Improved upgrade experience in the web console

* Administrators are now better informed about the differences between the upgrade
channels by helpful text and links in the web console.
* A link to the list of bug fixes and enhancements is now included for each minor
or patch release.
* There is now a visual representation of the different upgrade paths.
* Alerts now inform administrators when new patch releases, new minor releases,
and news channels become available.

[id="ocp-4-6-web-console-improved-operator-installation-workflow"]
==== Improved Operator installation workflow with OperatorHub

When administrators install Operators with OperatorHub, they now get immediate
feedback to ensure that the Operator is installing properly.

[id="ocp-4-6-web-console-improved-operand-details-view"]
==== Improved operand details view

You can now see the schema grouping of `specDescriptor` fields and the status of
your Operands on the operand's details view, so that you can easily see the
status and configure the `spec` of the operand instance.

[id="ocp-4-6-web-console-view-operators-related-objects"]
==== View related objects for cluster Operators

Previously, when viewing a cluster Operator, it was not clear what resources the
Operator was associated with. When troubleshooting a cluster Operator, it could
be challenging to locate the logs for all the resources that the Operator
managed, which might be needed for troubleshooting. Now, with {product-title}
{product-version}, you can expose a list of related objects of a cluster
Operator and easily review one of the related objects' details or YAML code for
troubleshooting.

[id="ocp-4-6-web-console-warning-messages-when-editing-managed-resources"]
==== Warning messages when editing managed resources

Some resources are managed, such an Operator managed by a deployment, route,
service, or ConfigMap. Users are discouraged from editing these resources.
Instead, users should edit the custom resources for the Operator and its
operand, and expect the Operator to update its related resources. With this
update:

* A *Managed by* label now appears below the resource name with a clickable
resource link for the managing resource.
* When the resource is modified or deleted, a message appears warning the user
that their changes might be reverted.

[id="ocp-4-6-web-console-specDescriptor-supports-CRD-instance"]
====  The `k8sResourcePrefix` specDescriptor supports CRD instance

Operator authors, maintainers, and providers can now specify the
`k8sResourcePrefix` specDescriptor with `Group/Version/Kind` for assigning a CRD
resource type besides Kubernetes core API.

For more information, see
link:https://github.com/openshift/console/blob/master/frontend/packages/operator-lifecycle-manager/src/components/descriptors/reference/reference.md[OLM
Descriptor Reference].

[id="ocp-4-6-web-console-column-management"]
==== Column management on resources page

A *Manage columns* icon image:manage-columns.png[title="Manage Columns icon"] is
now added to some resources pages, for example the Pods page. When you click on
the icon, default column names are listed with check boxes on the left side of
the modal and additional column names are listed on the right. Deselecting a
check box will remove that column from the table view. Selecting a check box
will add that column to the table view. A maximum combination of nine columns
from both sides of the modal are available for display at one time. Clicking
*Save* will save the changes that you make. Clicking *Restore Default Columns*
will restore the default settings of the columns.

[id="ocp-4-6-scale"]
=== Scale

[id="ocp-4-6-scale-cluster-maximums"]
==== Cluster maximums

Updated guidance around
xref:../scalability_and_performance/planning-your-environment-according-to-object-maximums.adoc#planning-your-environment-according-to-object-maximums[cluster
maximums] for {product-title} {product-version} is now available.

Use the link:https://access.redhat.com/labs/ocplimitscalculator/[{product-title}
Limit Calculator] to estimate cluster limits for your environment.

[id="ocp-4-6-scale-real-time-profile-added-to-node-tuning-operator"]
==== Real-time profile added to the Node Tuning Operator

Partial Tuned real-time profile support became available in {product-title} 4.4.
Now, the real-time profiles are fully compatible with what the real-time
profiles do in Tuned on {op-system-base-full}.

[id="ocp-4-6-performance-addon-operator"]
==== The Performance Addon Operator is now fully supported

The Performance Addon Operator helps the administrator with tuning worker nodes for low latency and real-time workloads. It takes a high-level tuning intent in the form of a `PerformanceProfile` custom resource and translates it into all the actions necessary to configure the Linux kernel, operating system, huge pages, and kubelet for low latency purposes.

In addition to the features provided in the previous pre-releases, this version includes the following:

* CPU load balancing can be enabled per Pod.
* Multiple huge page sizes can be specified at the same time.
* Improvements to supportability, such as integration gathering and better status reporting.
* A method for in-field emergency configuration overrides was devised and documented.

[id="ocp-4-6-networking"]
=== Networking

[id="ocp-4-6-ovn-kubernetes-ga"]
==== OVN-Kubernetes default Pod network provider GA

The OVN-Kubernetes Pod network provider is now GA. For more information, including details on feature parity with OpenShift SDN, refer to xref:../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[About the OVN-Kubernetes default Container Network Interface (CNI) network provider].

For this release, OpenShift SDN remains the default Pod network provider.

[id="ocp-4-6-expand-node-service-port-range"]
==== Expand node service port range

The node service port range is expandable beyond the default range of `30000-32767`. You can use this expanded range in your `Service` objects. For more information, refer to xref:../networking/configuring-node-port-service-range.adoc#configuring-node-port-service-range[Configuring the node port service range].

[id="ocp-4-6-sr-iov-infiniband-devices"]
==== SR-IOV Network Operator InfiniBand device support

The Single Root I/O Virtualization (SR-IOV) Network Operator now supports InfiniBand (IB) network devices.
// For more information on configuring an IB network device for your cluster, refer to xref::../networking/hardware_networks/...[This link].

[id="ocp-4-6-pod-network-connectivity-checks"]
==== Pod network connectivity checks

Operators can now configure `PodNetworkConnectivityCheck` resources to check each network connection from the Pods that are managed by the Operator. This allows you to more easily identify and troubleshoot issues with important network connections in your cluster.

This resource keeps track of the latest reachable condition, the last 10 successes, the last 10 failures, and details about detected outages. The results are also logged and events are created when outages are detected and resolved.

By default, the following network connections are checked:

* Between the Kubernetes API server and:
** the OpenShift API server service
** each OpenShift API server endpoint
** each etcd endpoint
** the internal API load balancer
** the external API load balancer

* Between the OpenShift API server and:
** the Kubernetes API server service
** each Kubernetes API server endpoint
** each etcd endpoint
** the internal API load balancer
** the external API load balancer

[id="ocp-4-6-secondary-device-metrics-network-attachments"]
==== Secondary device metrics can be associated with network attachments

Secondary devices, or interfaces, are used for different purposes. It is important to have a way to classify them so that you can aggregate the metrics for secondary devices with the same classification.

The kubelet is already publishing a set of network-observable related metrics. The labels in these metrics contain, among others:

* Pod name
* Pod namespace
* Interface name, such as eth0

This works well until new interfaces are added to the Pod, for example via Multus, as it will not be clear what the interface names refer to. The interface label refers to the interface name, but it is not clear what that interface is meant for. In case of many different interfaces, it would be impossible to understand what network the metrics we are monitoring refer to. This is addressed by introducing the new `pod_network_name_info` metric, which can be used to build queries containing both the values exposed by the kubelet and the name of the network attachment definition the metrics relates to, which identifies the type of network.

[id="ocp-4-6-cnf-tests-discovery-mode"]
==== CNF tests can be run in discovery mode

There is an optional mode where the Cloud-native Network Functions (CNF) tests try to look for configurations on the cluster instead of applying the new ones. The CNF tests image is a containerized version of the CNF conformance test suite. It is intended to be run against a CNF-enabled {product-title} cluster where all the components required for running CNF workloads are installed.

The tests must perform an environment configuration every time they are executed. This involves items such as creating SR-IOV Node Policies, Performance Profiles, or PtpProfiles. Allowing the tests to configure an already configured cluster might affect the functionality of the cluster. Also, changes to configuration items such as SR-IOV Node Policy might result in the environment being temporarily unavailable until the configuration change is processed.

Discovery mode validates the functionality of a cluster without altering its configuration. Existing environment configurations are used for the tests. The tests attempt to find the configuration items needed and use those items to execute the tests. If resources needed to run a specific test are not found, the test is skipped, providing an appropriate message to the user. After the tests are finished, no cleanup of the pre-configured configuration items is done, and the test environment can be used immediately for another test run.

[id="ocp-4-6-storage"]
=== Storage

[id="ocp-4-6-csi-driver-cso"]
==== CSI drivers now managed by the Cluster Storage Operator
The Container Storage Interface (CSI) Driver Operators and drivers for xref:../storage/container_storage_interface/persistent-storage-csi-ebs.adoc#persistent-storage-csi-ebs[AWS Elastic Block Store (EBS)], xref:../storage/container_storage_interface/persistent-storage-csi-ovirt.adoc#persistent-storage-csi-ovirt[Red Hat Virtualization (oVirt)], and xref:../storage/container_storage_interface/persistent-storage-csi-manila.adoc#persistent-storage-csi-manila[OpenStack Manila shared file system service] are now managed by the Cluster Storage Operator in {product-title}.

For AWS EBS and oVirt, this feature installs the CSI Driver Operator and driver in the `openshift-cluster-csi-drivers` namespace by default. For Manila, the CSI Driver Operator is installed in `openshift-cluster-csi-drivers` and the driver is installed in the `openshift-manila-csi-driver` namespace.

[IMPORTANT]
====
If you installed a CSI Driver Operator and driver on an {product-title} 4.5 cluster:

* The AWS EBS CSI Driver Operator and driver must be uninstalled before you update to a newer version of {product-title}.
* The OpenStack Manila CSI Driver Operator is no longer available in Operator Lifecycle Manager (OLM). It has been automatically converted by the Cluster Version Operator.
====

[id="ocp-4-6-lso-automation"]
==== Automatic device discovery and provisioning with the Local Storage Operator (Technology Preview)
The Local Storage Operator now has the ability to:

* Automatically discover a list of available disks in a cluster. You can select a list of nodes, or all nodes, for auto-discovery to be continuously applied to.
* Automatically provision local persistent volumes from attached devices. Appropriate devices are filtered and persistent volumes are provisioned based on the filtered devices.

For more information, see xref:../storage/persistent_storage/persistent-storage-local.adoc#local-storage-discovery_persistent-storage-local[Automating discovery and provisioning for local storage devices].

[id="ocp-4-6-operators"]
=== Operator lifecycle

[id="ocp-4-6-version-dependency"]
==== Operator version dependency

Operator developers can now ensure their Operators include dependencies on specific versions of other Operators by using the `olm.package` type in the `dependencies.yaml` file.

See xref:../operators/understanding/olm/olm-understanding-dependency-resolution.adoc#olm-bundle-format-dependencies_olm-understanding-dependency-resolution[Operator Lifecycle Manager dependency resolution] for more information.

[id="ocp-4-6-addtl-objects-bundle"]
==== Additional objects supported in Operator bundles

The Operator Bundle Format now supports the following additional Kubernetes objects:

* PodDisruptionBudget
* PriorityClass
* VerticalPodAutoScaler

See xref:../operators/understanding/olm-packaging-format.adoc#olm-bundle-format-manifests-optional_olm-packaging-format[Operator Framework packaging formats] for more information.

[id="ocp-4-6-selective-mirroring-opm"]
==== Selective bundle image mirroring with `opm`

Operator administrators can now to select which bundle images to mirror by using the `opm index prune` command.

See xref:../operators/admin/olm-restricted-networks.adoc#olm-restricted-networks[Pruning an index image] for more information.

[id="ocp-4-6-conversion-webhook-support"]
==== Conversion webhook support for global Operators

Operator developers can now use conversion webhooks for Operators that target all namespaces, also known as global Operators.

See xref:../operators/operator_sdk/osdk-generating-csvs.adoc#olm-defining-csv-webhook_osdk-generating-csvs[Defining webhooks] for more information.

[id="ocp-4-6-operator-api"]
==== Operator API now supported

The Operator API introduced in {product-title} 4.5 as a Technology Preview feature is now supported and enabled by default. Installing Operators using Operator Lifecycle Manager (OLM) has required cluster administrators to be aware of multiple APIs, including CatalogSources, Subscriptions, ClusterServiceVersions, and InstallPlans. This single Operator API resource is a first step towards a more simplified experience discovering and managing the lifecycle of Operators in a {product-title} cluster.

Relevant resources are now automatically labeled accordingly for the new Operator API for any Operators where the CSV is installed using a Subscription. Cluster administrators can use the CLI with this single API to interact with installed Operators. For example:

[source,terminal]
----
$ oc get operators
----

[source,terminal]
----
$ oc describe operator <operator_name>
----

[id="ocp-4-6-operator-rm-tp"]
===== Removing Technology Preview Operator API before cluster upgrade

If you enabled the Technology Preview feature version of the Operator API in {product-title} 4.5, you must disable it before upgrading to {product-title} 4.6. Failure to do so blocks your cluster upgrade, because the feature required a xref:../architecture/architecture-installation.adoc#unmanaged-operators_architecture-installation[Cluster Version Operator (CVO) override].

.Prerequisites

* {product-title} 4.5 cluster with Technology Preview Operator API enabled

.Procedure

. Because Operator API labels are applied to relevant resources automatically in {product-title} 4.6, you must remove any `operators.coreos.com/<name>` labels you previously applied manually.

.. You can check which resources are currently labeled for your Operator by running the following command and reviewing the `status.components.refs` section:
+
[source,terminal]
----
$ oc describe operator <operator_name>
----
+
For example:
+
[source,terminal]
----
$ oc describe operator etcd-test
----
+
.Example output
[source,terminal]
----
...
Status:
  Components:
    Label Selector:
      Match Expressions:
        Key:       operators.coreos.com/etcd-test
        Operator:  Exists
    Refs:
      API Version:  apiextensions.k8s.io/v1
      Conditions:
        Last Transition Time:  2020-07-02T05:50:40Z
        Message:               no conflicts found
        Reason:                NoConflicts
        Status:                True
        Type:                  NamesAccepted
        Last Transition Time:  2020-07-02T05:50:41Z
        Message:               the initial names have been accepted
        Reason:                InitialNamesAccepted
        Status:                True
        Type:                  Established
      Kind:                    CustomResourceDefinition <1>
      Name:                    etcdclusters.etcd.database.coreos.com <1>
...
----
<1> Resource type.
<2> Resource name.

.. Remove the labels from all relevant resources. For example:
+
[source,terminal]
----
$ oc label sub etcd operators.coreos.com/etcd-test- -n test-project
$ oc label ip install-6c5mr operators.coreos.com/etcd-test- -n test-project
$ oc label csv etcdoperator.v0.9.4 operators.coreos.com/etcd-test- -n test-project
$ oc label crd etcdclusters.etcd.database.coreos.com operators.coreos.com/etcd-test-
$ oc label crd etcdbackups.etcd.database.coreos.com operators.coreos.com/etcd-test-
$ oc label crd etcdrestores.etcd.database.coreos.com operators.coreos.com/etcd-test-
----

. Delete the Operator custom resource definition (CRD):
+
[source,terminal]
----
$ oc delete crd operators.operators.coreos.com
----

. Remove the `OperatorLifecycleManagerV2=true` feature gate from the OLM Operator.

.. Edit the Deployment for the OLM Operator:
+
[source,terminal]
----
$ oc -n openshift-operator-lifecycle-manager \
    edit deployment olm-operator
----

.. Remove the following flags from the `args` section in the Deployment:
+
[source,terminal]
----
...
    spec:
      containers:
      - args:
...
        - --feature-gates <1>
        - OperatorLifecycleManagerV2=true <1>
----
<1> Remove these flags.

.. Save your changes.

. Re-enable CVO management of OLM:
+
[source,terminal]
----
$ oc patch clusterversion version \
    --type=merge -p \
    '{
       "spec":{
          "overrides":[
             {
                "kind":"Deployment",
                "name":"olm-operator",
                "namespace":"openshift-operator-lifecycle-manager",
                "unmanaged":false,
                "group":"apps/v1"
             }
          ]
       }
    }'
----

. Verify that the Operator resource is no longer available:
+
[source,terminal]
----
$ oc get operators
----
+
.Example output
[source,terminal]
----
error: the server doesn't have a resource type "operators"
----

Your upgrade to {product-title} 4.6 should now no longer be blocked by this feature.

[id="ocp-4-6-images"]
=== Images

[id="ocp-4-6-cloud-credential-operator-mode-support"]
==== Support for Cloud Credential Operator modes

In addition to the existing default mode of operation, the xref:../operators/operator-reference.adoc#cloud-credential-operator_red-hat-operators[Cloud Credential Operator (CCO)] can now be explicitly configured to operate in the following modes: `Mint`, `Passthrough`, and `Manual`. This feature provides transparency and flexibility in how the CCO uses cloud credentials to process `CredentialRequests` in the cluster for installation and other tasks.

[id="ocp-4-6-samples-operator"]
==== Cluster Samples Operator on Power and Z

Imagestreams and templates for Power and Z architectures are now available and installed by the Cluster Samples Operator by default.

[id="ocp-4-6-metering"]
=== Metering

[id="ocp-4-6-metering-operator"]
==== Configuring a retention period of metering Reports

You can now set a retention period on a metering Report. The metering Report custom resource has a new `expiration` field. If the `expiration` duration value is set on a Report, and no other Reports or ReportQueries depend on the expiring Report, the Metering Operator removes the Report from your cluster at the end of its retention period. For more information, see metering Reports xref:../metering/reports/metering-about-reports.adoc#metering-expiration_metering-about-reports[expiration].

[id="ocp-4-6-nodes"]
=== Nodes

[id="ocp-4-6-nodes-audit-log-policy"]
==== Configure the node audit log policy

You can now control the amount of information that is logged to the node audit logs by choosing the audit log policy profile to use.

See xref:../nodes/nodes/nodes-nodes-audit-config.adoc#nodes-nodes-audit-config[Configuring the node audit log policy] for more information.

[id="ocp-4-6-nodes-pod-topology-spread-constraints"]
==== Configure pod topology spread constraints

You can now configure pod topology spread constraints for more fine-grained control the placement of your pods across nodes, zones, regions, or other user-defined topology domains. This can help you improve high availability and resource utilization.

See xref:../nodes/scheduling/nodes-scheduler-pod-topology-spread-constraints.adoc#nodes-scheduler-pod-topology-spread-constraints[Controlling pod placement by using pod topology spread constraints] for more information.

[id="ocp-4-6-descheduler-strategy"]
==== New descheduler strategy is available (Technology Preview)

The descheduler now allows you to configure the `PodLifeTime` strategy. This strategy evicts pods after they reach a certain, configurable age.

See xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler-strategies_nodes-descheduler[Descheduler strategies] for more information.

[id="ocp-4-6-descheduler-filtering"]
==== Descheduler filtering by namespace and priority (Technology Preview)

You can now configure whether descheduler strategies should consider pods for eviction based on their namespace and priority.

See xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler-filtering-namespace_nodes-descheduler[Filtering pods by namespace] and xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler-filtering-priority_nodes-descheduler[Filtering pods by priority] for more information.

[id="ocp-4-6-descheduler-new-param"]
==== New parameter for the `RemoveDuplicates` descheduler strategy (Technology Preview)

The `RemoveDuplicates` strategy now provides an optional parameter, `ExcludeOwnerKinds`, that allows you to specify a list of `Kind` types. If a pod has any of these types listed as an `OwnerRef`, that pod is not considered for eviction.

See xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler-strategies_nodes-descheduler[Descheduler strategies] for more information.

[id="ocp-4-6-imagecontentsourcepolicy-scoped-to-registry"]
==== Generate ImageContentSourcePolicy scoped to a registry

The `oc adm catalog mirror` command generates an `ImageContentSourcePolicy` that maps the original container image repository to a new location where it will be mirrored, typically inside a disconnected environment. When a new or modified In-Circuit Serial Programming (ICSP) is applied to a cluster, it is converted to a config file for CRI-O and placed onto each node. The process of placing the config file on a node includes rebooting that node.

This enhancement adds  the `--icsp-scope` flag to `oc adm catalog mirror`. Scopes can be registry or repository. By default, the `oc adm catalog mirror` command generates an ICSP where each entry is specific to a repository. For example, it would map `registry.redhat.io/cloud/test-db` to `mirror.internal.customer.com/cloud/test-db`.  Widening the mirror to registry scope in the ICSP file minimizes the number of times the cluster must
reboot its nodes. Using the same example, `registry.redhat.io` would map to `mirror.internal.customer.com`.

Having a widely scoped ICSP reduces the number of times the ICSP might need to change in the future and, thus, reduces the number of times a cluster must reboot all of its nodes.

[id="ocp-4-6-logging"]
=== Cluster logging

[discrete]
[id="ocp-4-6-logging-api-ga"]
==== Log Forwarding API is generally available

The xref:../logging/cluster-logging-external.adoc[Log Forwarding API] is now generally available. The Log Forwarding API allows you to send container, infrastructure, and audit logs to specific endpoints within and outside your cluster by configuring a custom resource with the endpoints to forward the logs. The Log Forwarding API now supports forwarding to Kafka brokers and supports syslog RFC 3164 and RFC 5424 including TLS. You can also forward application logs from a specific projects to an endpoint.

With the GA, the Log Forwarding API has a number of changes, including changes to parameter names in the Log Forwarding custom resource (CR). If you used the Log Forwarding Technology Preview, you need to manually make the needed changes to your existing Log Forwarding CR.

[discrete]
[id="ocp-4-6-logging-labels"]
==== Adding labels to log messages

The Log Forwarding API allows you to add free-text labels to log messages that are affixed to outbound log messages. For example, you could label logs by data center or label the logs by type. Labels added to objects are also forwarded with the log message.

[discrete]
[id="ocp-4-6-logging-dashboards"]
==== New cluster logging dashboards

Two new dashboards have been added to the {product-title} web console that display charts with important, low-level metrics for detailed investigation and troubleshooting of your cluster logging and Elasticsearch instances.

The *OpenShift Logging* dashboard contains charts that show details about your Elasticsearch instance at a cluster-level, including cluster resources, garbage collection, shards in the cluster, and Fluentd statistics.

The *Logging/Elasticsearch Nodes* dashboard contains charts that show details about your Elasticsearch instance, many at node-level, including details on indexing, shards, resources, and so forth.

[discrete]
[id="ocp-4-6-fluentd-parameters"]
==== New parameters for tuning Fluentd

New Fluentd parameters allow you to performance-tune your Fluentd log collector. With these parameters, you can change:

* the size of Fluentd chunks and chunk buffer
* the Fluentd chunk flushing behavior
* the Fluentd chunk forwarding retry behavior

These parameters can help you determine the trade-offs between latency and throughput in your cluster logging instance.

[id="ocp-4-6-monitoring"]
=== Monitoring

[id="ocp-4-6-monitoring-for-user-defined-projects"]
==== Monitoring for user-defined projects

In {product-title} 4.6, you can enable monitoring for user-defined projects in addition to the default platform monitoring. You can now monitor your own projects in {product-title} without the need for an additional monitoring solution. Using this new feature centralizes monitoring for core platform components and user-defined projects.

With this new feature, you can perform the following tasks:

* Enable and configure monitoring for user-defined projects
* Create recording and alerting rules that use metrics from your own pods and services
* Access metrics and information about alerts through a single, multi-tenant interface
* Cross-correlate the metrics for user-defined projects with platform metrics

For more information, see xref:../monitoring/understanding-the-monitoring-stack.adoc#understanding-the-monitoring-stack[Understanding the monitoring stack].

[id="ocp-4-6-monitoring-alerting-rule-changes"]
==== Alerting rule changes

{product-title} 4.6 includes the following alerting rule changes:

* The `PrometheusOperatorListErrors` alert is added. The alert provides notification of errors when running list operations on controllers.
* The `PrometheusOperatorWatchErrors` alert is added. The alert provides notification of errors when running watch operations on controllers.
* The `KubeQuotaExceeded` alert is replaced by `KubeQuotaFullyUsed`. Previously, the `KubeQuotaExceeded` alert fired if a resource quota exceeded a 90% threshold. The `KubeQuotaFullyUsed` alert fires if a resource quota is fully used.
* etcd alerts now support the addition of custom labels for metrics.
* The `KubeAPILatencyHigh` and `KubeAPIErrorsHigh` alerts are replaced by the `KubeAPIErrorBudgetBurn` alert. `KubeAPIErrorBudgetBurn` combines API error and latency alerts and fires only when the conditions are severe enough.
* The readiness and liveness probe metrics exposed by the kubelet are now scraped. This provides historical liveness and readiness data for containers, which can be helpful when troubleshooting container issues.
* The alerting rules for the Thanos Ruler are updated so that alerts are paged if recording rules and alerting rules are not correctly evaluated. This update ensures that critical alerts are not lost when rule and alert evaluation in the Thanos Ruler is not completed.
* The `KubeStatefulSetUpdateNotRolledOut` alert is updated so that it does not fire when a stateful set is being deployed.
* The `KubeDaemonSetRolloutStuck` alert is updated to account for daemon set roll out progress.
* The severity of cause-based alerts are adjusted from *critical* to *warning*.

[NOTE]
====
Red Hat does not guarantee backward compatibility for metrics, recording rules, or alerting rules.
====

[id="ocp-4-6-monitoring-prometheus-rule-validation"]
==== Prometheus rule validation

{product-title} 4.6 introduces validation of Prometheus rules through a webhook that calls the validating admission plug-in. With this enhancement, PrometheusRule custom resources in all projects are checked against the Prometheus Operator rule validation API.

[id="ocp-4-6-monitoring-metrics-and-alerting-rules-added-for-thanos-querier"]
==== Metrics and alerting rules added for the Thanos Querier

The Thanos Querier aggregates and optionally deduplicates core {product-title} metrics and metrics for user-defined projects under a single, multi-tenant interface. In {product-title} 4.6, a service monitor and alerting rules are now deployed for the Thanos Querier, which enables monitoring of the Thanos Querier by the monitoring stack.

[id="ocp-4-6-notable-technical-changes"]
== Notable technical changes

{product-title} 4.6 introduces the following notable technical changes.

[discrete]
[id="ocp-4-6-operator-catalogs-per-version"]
==== Default Operator catalogs now shipped per cluster version

Starting in {product-title} 4.6, the Red Hat-provided default catalogs used by Operator Lifecycle Manager (OLM) and OperatorHub are now shipped as index images specific to the minor version of {product-title}. This allows Operator providers to ship intentional ranges of Operator versions per cluster version.

These index images, based on the Bundle Format, replace the App Registry catalog images, based on the deprecated Package Manifest Format, that are distributed for previous versions of {product-title} 4. {product-title} 4.1 through 4.5 will continue to share a single App Registry catalog.

[NOTE]
====
While App Registry catalog images are not distributed by Red Hat for {product-title} 4.6 and later, custom catalog images based on the Package Manifest Format are still supported.
====

See xref:../operators/understanding/olm-packaging-format.adoc#olm-bundle-format_olm-packaging-format[Operator Framework packaging formats] for more information on the Bundle Format and index images.

[discrete]
[id="ocp-4-6-operator-catalogs-upgrade"]
===== Important Operator upgrade requirements

Cluster administrators must ensure all Operators previously installed through Operator Lifecycle Manager (OLM) are updated to their latest versions in their latest channels before upgrading to {product-title} 4.6. Updating the Operators ensures that they have a valid upgrade path when the default OperatorHub catalogs switch from using the App Registry catalogs in {product-title} 4.5 to the new index image-based catalogs in {product-title} 4.6 during the cluster upgrade.

See xref:../operators/admin/olm-upgrading-operators.adoc#olm-upgrading-operators[Upgrading installed Operators] for more information on ensuring installed Operators are on the latest channels and upgraded either using automatic or manual approval strategies.

.Additional resources

* See the following Red Hat Knowledgebase Article for a list of minimum versions of deployed Red Hat Integration components (including Red Hat Fuse, Red Hat AMQ, and Red Hat 3scale) that are required for {product-title} 4.6:
+
link:https://access.redhat.com/articles/5423161[]

[discrete]
[id="ocp-4-6-default-cni-np-uses-ovs-on-cluster-nodes"]
==== CNI network provider now uses OVS installed on cluster nodes

Both the OpenShift SDN and OVN-Kubernetes Container Network Interface (CNI) network providers now use the Open vSwitch (OVS) version installed on the cluster nodes. Previously, OVS ran in a container on each node, managed by a DaemonSet. Using the host OVS eliminates any possible downtime from upgrading the containerized version of OVS.

[discrete]
[id="ocp-4-6-warnings-when-using-deprecated-apis"]
==== Warnings when using deprecated APIs

Warnings are now visible in `client-go` and `oc` on every invocation against a
deprecated API. Calling a deprecated API returns a warning message containing
the target Kubernetes removal release and replacement API, if applicable.

For example:

[source,terminal]
----
warnings.go:67] batch/v1beta1 CronJob is deprecated in v1.22+, unavailable in v1.25+
----

This is new functionality included with Kubernetes 1.19.

[discrete]
[id="ocp-4-6-operator-sdk-v-0-19-4"]
==== Operator SDK v0.19.4

{product-title} supports Operator SDK v0.19.4, which introduces the following
notable technical changes:

* Operator SDK now aligns with the {product-title}-wide switch to using UBI-8
and Python 3. Downstream base images now use UBI-8 and include Python 3.
* The command `run --local` is deprecated in favor of `run local`.
* The commands `run --olm` and `--kubeconfig` are deprecated in favor of `run packagemanifests`.
* The default CRD version changed from `apiextensions.k8s.io/v1beta1` to `apiextensions.k8s.io/v1` for commands that create or generate CRDs.
* The `--kubeconfig` flag is added to the `<run|cleanup> packagemanifests` command.

Ansible-based Operator enhancements include:

* The Ansible Operator is now available as a supported release.
* The Ansible Operator now includes a `healthz` endpoint and `liveness` probe.

Helm-based Operator enhancements include:

* Helm Operators can watch and reconcile when cluster-scoped release resources are changed.
* Helm Operators can now reconcile logic by using three-way strategic merge patches for native Kubernetes objects so that array patch strategies are correctly honored and applied.
* Helm Operators have the default API version changed to `helm.operator-sdk/v1alpha1`.

[id="ocp-4-6-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to
be supported; however, it will be removed in a future release of this product
and is not recommended for new deployments. For the most recent list of major
functionality deprecated and removed within {product-title} {product-version},
refer to the table below. Additional details for more fine-grained functionality
that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *DEP*: _Deprecated_
* *REM*: _Removed_

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.4 |OCP 4.5 |OCP 4.6

|Service Catalog
|DEP
|REM
|REM

|Template Service Broker
|DEP
|REM
|REM

|OperatorSources
|DEP
|DEP
|REM

|CatalogSourceConfigs
|DEP
|REM
|REM

|Package Manifest Format (Operator Framework)
|DEP
|DEP
|DEP

|`oc adm catalog build`
|DEP
|DEP
|DEP

|v1beta1 CRDs
|GA
|DEP
|DEP

|====

[id="ocp-4-6-deprecated-features"]
=== Deprecated features

[id="ocp-4-6-removed-features"]
=== Removed features

[id="ocp-4-6-operatorsources-removed"]
==== OperatorSources removed

The OperatorSources resource, part of the Marketplace API for the Operator
Framework, has been deprecated for several {product-title} releases and is now
removed. In {product-title} 4.6, the default catalogs for OperatorHub in the
`openshift-marketplace` namespace now only use CatalogSources with the `polling`
feature enabled. The default catalogs poll for new updates in their referenced
index images every 15 minutes.

[id="ocp-4-6-bug-fixes"]
== Bug fixes

*Web console (Developer perspective)*

* Previously, when you tried to delete a Knative application through the *Topology* view, a false positive error about a non-existing *Knative route* was reported. This issue is now fixed and the error is no longer displayed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1866214[BZ#1866214])

* Previously, the Developer Console did not allow images from insecure registries to be imported. This bug fix adds a checkbox that allows users to use the insecure registries in the *Deploy image* form. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1826740[BZ#1826740])

* When a user selected the *From Catalog* option to create an application, the *Developer Catalog* displayed a blank page instead of a list of templates to create an application. This was caused when the 1.18.0 Jaeger Operator was installed. This issue has now been fixed and the templates are displayed as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1845279[BZ#1845279])

* When deleting a parallel task in a Pipeline through the *Pipeline Builder* in the Developer Console, the interface was rearranging the tasks connected to the parallel task incorrectly, creating orphan tasks. With this fix, the tasks connected to the deleted parallel task are reconnected with the original Pipeline. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1856155[BZ#1856155])

* The web console was crashing with a JavaScript exception when the user cancelled the creation of a Pipeline through the web console with a side panel opened at the same time. This was fixed by improving the internal state handling. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1856267[BZ#1856267])

* A user with the required permissions was unable to retrieve and deploy an image from another project. The required RoleBindings have now been created to fix this issue. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1843222[BZ#1843222])

* When you tried to deploy an application from a Git repository with the *Import from Git* function, the Developer Console reported a false positive error `Git repository is not reachable` for private repositories reachable by the cluster. This was fixed by adding information on making the private repository available to the cluster in the error message. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1877739[BZ#1877739])

* When a Go application was created through the Developer Console, a route to the application was not created. This was caused by a bug in `build-tools` and incorrectly configured ports. The issue has been fixed by picking either the user-provided port or the default port 8080 as the target port. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1874817[BZ#1874817])

* When you created an application with the *Import from Git* function, a subsequent change of the application's Git repository from the web console was not possible. This was caused by changing the application name in subsequent editing of the Git repository URL. This was fixed by making the application name read-only when editing the application Git repository URL. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1873095[BZ#1873095])

* Previously, a user without administrative or project listing privileges could not see the metrics of any projects. This bug fix removes the checks for user privileges when accessing the cluster metrics. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1842875[BZ#1842875])

* Users with the `@` character in their user names, like `user@example.com`, could not start a Pipeline from the Developer Console. This was caused by a limitation in Kubernetes labels. The issue was fixed by moving the "Started by" metadata from a Kubernetes label to a Kubernetes annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1868653[BZ#1868653])


[id="ocp-4-6-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These
experimental features are not intended for production use. Note the
following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview
Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.4 |OCP 4.5 |OCP 4.6

|Precision Time Protocol (PTP)
|TP
|TP
|TP

|`oc` CLI Plug-ins
|TP
|TP
|TP

|experimental-qos-reserved
|TP
|TP
|TP

|Pod Unidler
|TP
|GA
|GA

|Ephemeral Storage Limit/Requests
|TP
|TP
|TP

|Descheduler
|TP
|TP
|TP

|Podman
|TP
|TP
|TP

|Sharing Control of the PID Namespace
|TP
|GA
|GA

|OVN-Kubernetes Pod network provider
|TP
|TP
|GA

|HPA custom metrics adapter based on Prometheus
|TP
|TP
|TP

|HPA for memory utilization
|TP
|TP
|TP

|Three-node bare metal deployments
|TP
|GA
|GA

|Service Binding
|TP
|TP
|TP

|Log forwarding
|TP
|GA
|GA

|Monitoring for user-defined projects
|TP
|TP
|GA

|Compute Node Topology Manager
|TP
|GA
|GA

|Raw Block with Cinder
|TP
|TP
|TP

|External provisioner for AWS EFS
|TP
|TP
|TP

|CSI volume snapshots
|TP
|TP
|TP

|CSI volume cloning
|TP
|TP
|GA

|CSI AWS EBS Driver Operator
|-
|TP
|TP

|OpenStack Manila CSI Driver Operator
|-
|GA
|GA

|Red Hat Virtualization (oVirt) CSI Driver Operator
|-
|-
|GA

|CSI inline ephemeral volumes
|-
|TP
|TP

|Automatic device discovery and provisioning with Local Storage Operator
|-
|-
|TP

|OpenShift Pipelines
|TP
|TP
|TP

|Vertical Pod Autoscaler
|-
|TP
|TP

|Operator API
|-
|TP
|GA

|Adding kernel modules to nodes
|TP
|TP
|TP

|====

[id="ocp-4-6-4-ibm-power"]
===== IBM Power Systems

With this release, IBM Power Systems are now compatible with {product-title} {product-version}. See xref:../installing/installing_ibm_power/installing-ibm-power.adoc#installing-ibm-power_installing-ibm-power[Installing a cluster on IBM Power] or xref:../installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc#installing-restricted-networks-ibm-power_installing-restricted-networks-ibm-power[Installing a cluster on IBM Power in a restricted network].

[discrete]
====== Restrictions

Note the following restrictions for {product-title} on IBM Power:

* {product-title} for IBM Power Systems does not include the following Technology Preview features:
+
** OpenShift Virtualization
** OpenShift Serverless (knative, FaaS integrations)

* The following {product-title} features are unsupported:
** Red Hat OpenShift Service Mesh (istio, jaeger, kiali)
** CodeReady Workspaces
** CodeReady Containers (CRC)
** OpenShift Pipelines based on Tekton
** {product-title} Metering
** Multus Plugins (SR-IOV, IPVAN, Bridge with VLAN, Static IPAM)
** SR-IOV CNI plug-in
** Red Hat Single Sign-On
** OpenShift Metering (Presto, Hive)


* Worker nodes must run {op-system-first}.
* Persistent storage must be of the `Filesystem` mode using local volumes, Network File System (NFS), OpenStack Cinder, or Container Storage Interface (CSI).
* Networking must use either DHCP or static addressing with Red Hat Openshift SDN.
* AdoptOpenJDK with OpenJ9
* Installer-provisioned infrastructure
* Device Manager for NVIDIA GPUs
* Special Resources Operator
* OpenShift Ansible Service Broker Operator (deprecated)
* dotNET on RHEL



[discrete]
====== Supported Features

* Currently, four Operators are supported:
** Cluster-Logging-Operator
** Cluster-NFD-Operator
** Elastic Search-Operator
** Local Storage Operator

* User-provisioned infrastructure deployment scenario on bare-metal
* OpenShift Cluster Monitoring
* Node Tuning Operator
* OpenShift Jenkins
* OpenShift Logging
* OpenShift Do (odo)
* Machine Configuration Operator, which is used in installations with installer-provisioned infrastructure
* Node Feature Discovery Operator
* {product-title} core (CVO Operators)
* Installation program for clusters that use user-provisioned infrastructure
* OVS/OVN
* RHEL8 Based container support
* RHEL CoreOS
* Ansible Engine
* Red Hat Software Collections
* Fibre Channel
* HostPath
* Raw Block
* iSCSI
* 4k Disk support


[id="ocp-4-6-known-issues"]
== Known issues

* Currently, upgrading from {product-title} 4.5 to {product-version} with the OVN-Kubernetes pod network provider will not work. This will be resolved in a future 4.6.z release. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1880591[*BZ#1880591*])

* Currently, scaling up {op-system-base-full} worker nodes on a cluster with the OVN-Kubernetes pod network provider will not work. This will be resolved in a future {op-system-base} 7.8.z and {op-system-base} 7.9.z release.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1884323[*BZ#1884323*], link:https://bugzilla.redhat.com/show_bug.cgi?id=1871935[*BZ#1871935*])

* Downgrading from {product-title} {product-version} to 4.5 will be fixed in a future 4.5.z release.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1882394[*BZ#1882394*], link:https://bugzilla.redhat.com/show_bug.cgi?id=1886148[*BZ#1886148*], link:https://bugzilla.redhat.com/show_bug.cgi?id=1886127[*BZ#1886127*])

* Currently, upgrading from {product-title} 4.5 to {product-version} with {op-system-base-full} worker nodes does not work. This will be resolved in a future 4.6.z release. First, upgrade {op-system-base}, then upgrade the cluster, and then run the normal {op-system-base} upgrade playbook again. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1887607[*BZ#1887607*])

* {product-title} 4.5 to {product-version} upgrade fails when an external network is configured on a bond device; the `ovs-configuration` service fails and nodes becomes unreachable. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1887545[*BZ#1887545*])

// TODO: This known issue should carry forward to 4.7 and beyond!
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to 4.6, you can either revoke or continue to allow unauthenticated access. It is recommended to revoke unauthenticated access unless there is a specific need for it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP 403 errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

* Running the `operator-sdk new` or `operator-sdk create api` commands without the
`--helm-chart` flag builds a Helm-based Operator that uses the default
boilerplate Nginx chart. While this example chart works correctly on upstream
Kubernetes, it fails to deploy successfully on {product-title}.
+
To work around this issue, use the `--helm-chart` flag to provide a Helm chart
that deploys successfully on {product-title}. For example:
+
[source,terminal]
----
$ operator-sdk new <operator_name> --type=helm \
  --helm-chart=<repo>/<name>
----
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1874754[*BZ#1874754*])

* When installing {product-title} on bare metal nodes with the Redfish Virtual
Media feature, a failure occurs when the Baseboard Management Controller (BMC)
attempts to load the virtual media image from the provisioning network. This
happens if the BMC is not using the provisioning network, or its network does
not have routing set up to the provisioning network. As a workaround, when using
virtual media, the provisioning network must be turned off, or the BMCs must be
routed to the provisioning network as a prerequisite.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1872787[*BZ#1872787*])

* On a power environment, when a pod is created using the FC persistent volume claim and the targetWWN as a parameter, the FC volume attach fails with “no fc disk found” error and the pod remains in `ContainerCreating state`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1887026[*BZ#1887026*])

* When a node providing an egress IP is shut down, the pods hosted on that node
are not moved to another node providing an egress IP. This causes the outgoing
traffic of the pods to always fail when a node providing an egress IP is shut
down.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1877273[*BZ#1877273*])

* Private, disconnected cluster installations are not supported for AWS GovCloud when installing in the `us-gov-east-1` region due to a known issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1881262[*BZ#1881262*]).

* Firewall rules used by machines not prefixed with the infrastructure ID are preserved when destroying a cluster running on Google Cloud Platform (GCP) with installer-provisioned infrastructure. This causes the destroy process of the installation program to fail. As a workaround, you must manually delete the firewall rule of the machine in the GCP web console:
+
[source,terminal]
----
$ gcloud compute firewall-rules delete <firewall_rule_name>
----
+
Once the firewall rule of the machine with the missing infrastructure ID is removed, the cluster can be destroyed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1801968[*BZ#1801968*])

* The `opm alpha bundle build` command fails on Windows 10. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1883773[*BZ#1883773*])

* In {product-title} 4.6, the resource metrics API server provides support for custom metrics. The resource metrics API server does not implement the OpenAPI specification, and the following messages are sent to the `kube-apiserver` logs:
+
[source,terminal]
----
controller.go:114] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: OpenAPI spec does not exist
controller.go:127] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
----
+
In some cases, these errors might cause the `KubeAPIErrorsHigh` alert to fire, but the underlying issue is not known to degrade {product-title} functionality.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1819053[*BZ#1819053*])

* Rules API back-ends are sometimes not detected if Store API stores are discovered before Rules API stores. When this occurs, a store reference is created without a Rules API client, and the Rules API endpoint from Thanos Querier does not return any rules.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1870287[*BZ#1870287*])

* If an AWS account is configured to use AWS Organizations service control policies (SCPs) that use a global condition to deny all actions or require a specific permission, the AWS policy simulator API that validates permissions produces a false negative. When the permissions cannot be validated, {product-title} AWS installations fail, even if the provided credentials have the required permissions for installation.
+
To work around this issue, you can bypass the AWS policy simulator permissions check by setting a value for the `credentialsMode` parameter in the `install-config.yaml` configuration file. The value of `credentialsMode` changes the behavior of the Cloud Credential Operator (CCO) to one of xref:../operators/operator-reference.adoc#cloud-credential-operator_red-hat-operators[three supported modes].
+
.Example `install-config.yaml` configuration file
+
[source,yaml]
----
apiVersion: v1
baseDomain: cluster1.example.com
credentialsMode: Mint <1>
compute:
- architecture: amd64
  hyperthreading: Enabled
...
----
<1> This line is added to set the `credentialsMode` parameter to `Mint`.
+
When bypassing this check, ensure that the credentials you provide have the xref:../operators/operator-reference.adoc#cloud-credential-operator_red-hat-operators[permissions that are required for the specified mode].
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1829101[*BZ#1829101*])

* Clusters that run on {rh-openstack} and use Kuryr create unnecessary Neutron ports
for each `hostNetworking` pod. You can delete these ports safely.
Automatic port deletion is planned for a future release of
{product-title}.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1888318[*BZ#1888318*])

* When deploying an egress router pod in DNS proxy mode, the pod fails to initialize. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1888024[*BZ#1888024*])

[id="ocp-4-6-asynchronous-errata-updates"]
== Asynchronous errata updates

[[rhba-2020-1234]]
=== RHBA-2020:1234 - {product-title} 4.6 image release and bug fix advisory

Issued: 2020-xx-xx

{product-title} release 4.6 is now available. The list of container images and
bug fixes includes in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2020:xxxx[RHBA-2020:xxxx] advisory.
The RPM packages included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2020:xxxx[RHBA-2020:xxxx] advisory.

Space precluded documenting all of the container images for this release in the
advisory. See the following article for notes on the container images in this
release:

link:https://access.redhat.com/solutions/[{product-title} 4.6.0 container image list]
