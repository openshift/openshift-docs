:_content-type: ASSEMBLY
[id="ocp-4-13-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-13-about-this-release"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-2022:7399[RHSA-2022:7399]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.26.md[Kubernetes 1.26] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. With the {cluster-manager-first} application for {product-title}, you can deploy OpenShift clusters to either on-premises or cloud environments.

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.4 and 8.5, as well as on {op-system-first} 4.13.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//TODO: Remove this for 4.13
Starting with {product-title} {product-version} an additional six months of Extended Update Support (EUS) phase on even numbered releases from 18 months to two years. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

//TODO: Add the line below for EUS releases.
{product-title} 4.8 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

//TODO: The line below should be used when it is next appropriate. Revisit in April 2023 timeframe.
Maintenance support ends for version 4.8 in January 2023 and goes to extended life phase. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].


[id="ocp-4-13-add-on-support-status"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-13-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-13-rhcos"]
=== {op-system-first}

[id="ocp-4-13-rhcos-rhel-9-2-packages"]
==== {op-system} now uses {op-system-base} 9.2

{op-system} now uses {op-system-base-full} 9.2 packages in {product-title} {product-version}. This enables you to have the latest fixes, features, and enhancements, as well as the latest hardware support and driver updates.

[id="ocp-4-13-rhel-9-considerations"]
===== Considerations for upgrading to {product-title} with {op-system-base} 9.2

With this release, {product-title} {product-version} introduces a {op-system-base} 9.2 based {op-system} and there are some considerations you must make before upgrading:

* Some component configuration options and services might have changed between {op-system-base} 8.6 and {op-system-base} 9.2, which means existing machine configuration files might no longer be valid.

* {op-system-base} 6 base image containers are not supported on {op-system} container hosts, but are supported on {op-system-base} 8 worker nodes. For more information, see the link:https://access.redhat.com/support/policy/rhel-container-compatibility[Red Hat Container Compatibility] matrix.

* Some device drivers have been deprecated, see the link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/considerations_in_adopting_rhel_9/assembly_hardware-enablement_considerations-in-adopting-rhel-9#unmaintained-hardware-support[{op-system-base} documentation] for more information.

[id="ocp-4-13-secure-execution-z-linux-one"]
==== IBM Secure Execution on {ibmzProductName} and {linuxoneProductName}
This feature was introduced as a Technology Preview in
{product-title} 4.12 and is now generally available in
{product-title} 4.13.
 IBM Secure Execution is a hardware enhancement that protects memory boundaries for KVM guests. IBM Secure Execution provides the highest level of isolation and security for cluster workloads, and you can enable it by using an IBM Secure Execution-ready QCOW2 boot image.

To use IBM Secure Execution, you must have host keys for your host machine(s) and they must be specified in your Ignition configuration file. IBM Secure Execution automatically encrypts your boot volumes using LUKS encryption.

For more information, see xref:../installing/installing_ibm_z/installing-ibm-z-kvm.adoc#installing-rhcos-using-ibm-secure-execution_installing-ibm-z-kvm[Installing {op-system} using IBM Secure Execution].

[id="ocp-4-13-lsof-rhcos"]
==== {op-system} now includes lsof
{product-title} {product-version} now includes the `lsof` command in {op-system}.

[id="ocp-4-13-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-13-installation-vsphere-8-support"]
==== Support for VMware vSphere version 8.0
{product-title} {product-version} supports VMware vSphere version 8.0. You can continue to install an {product-title} cluster on VMware vSphere version 7.0 Update 2.

[id="ocp-4-13-installation-vsphere-region-zone"]
==== VMware vSphere region and zone enablement
You can deploy an {product-title} cluster to multiple vSphere datacenters or regions that run in a single VMware vCenter. Each datacenter can run multiple clusters or zones. This configuration reduces the risk of a hardware failure or network outage causing your cluster to fail.

[IMPORTANT]
====
The VMware vSphere region and zone enablement feature is only available with a newly installed cluster, because this feature requires the vSphere Container Storage Interface (CSI) driver as the default storage driver in the cluster.

A cluster that was upgraded from a previous release defaults to using the in-tree vSphere driver. As a result, you must enable CSI automatic migration for the cluster to use this feature. You can then configure multiple regions and zones for the upgraded cluster.
====

[id="ocp-4-13-installation-vsphere-default-config-yaml"]
==== Changes to the default vSphere `install-config.yaml` file
After you run the installation program for {product-title} on vSphere, the default `install-config.yaml` file now includes `vcenters` and `failureDomains` fields, so that you can choose to specify multiple datacenters, region, and zone information for your cluster. You can leave these fields blank if you want to install an {product-title} cluster in a vSphere environment that consists of single datacenter running in a VMware vCenter.

[id="ocp-4-13-installation-and-upgrade-three-node"]
==== Three-node cluster support

Beginning with {product-title} {product-version}, deploying a three-node cluster is supported on Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), and VMware vSphere. This type of {product-title} cluster is a smaller, more resource efficient cluster, as it consists of only three control plane machines, which also act as compute machines.

For more information, see xref:../installing/installing_aws/installing-aws-three-node.adoc#installing-aws-three-node[Installing a three-node cluster on AWS], xref:../installing/installing_azure/installing-azure-three-node.adoc#installing-azure-three-node[Installing a three-node cluster on Azure], xref:../installing/installing_gcp/installing-gcp-three-node.adoc#installing-gcp-three-node[Installing a three-node cluster on GCP], and xref:../installing/installing_vsphere/installing-vsphere-three-node.adoc#installing-vsphere-three-node[Installing a three-node cluster on vSphere].

[id="ocp-4-13-installation-and-upgrade-ibm-cloud-vpc"]
==== IBM Cloud VPC and existing VPC resources
If you are deploying an {product-title} cluster to an existing virtual private cloud (VPC), you can now use the `networkResourceGroupName` parameter to specify the name of the resource group that contains these existing resources. This enhancement lets you keep the existing VPC resources and subnets separate from the cluster resources that the installation program provisions. You can then use the `resourceGroupName` parameter to specify the name of an existing resource group that the installation program can use to deploy all of the installer-provisioned cluster resources. If `resourceGroupName` is undefined, a new resource group is created for the cluster.

For more information, see xref:../installing/installing_ibm_cloud_public/installing-ibm-cloud-customizations.adoc#installation-configuration-parameters-additional-ibm-cloud_installing-ibm-cloud-customizations[Additional IBM Cloud VPC configuration parameters].

[id="ocp-4-13-installation-gcp-required-permissions"]
==== Minimum required permissions for GCP to install and delete an {product-title} cluster
In {product-title} {product-version}, instead of using the predefined roles, you can now define your custom roles to include the minimum required permissions for Google Cloud Platform (GCP) to install and delete an {product-title} cluster. These permissions are available for installer-provisioned infrastructure and user-provisioned infrastructure.

[id="ocp-4-13-user-defined-tags-azure"]
==== User-defined tags for Azure
In {product-title} 4.13, you can configure the tags in Azure for grouping resources and for managing resource access and cost. Support for tags is available only for the resources created in the Azure Public Cloud, and in {product-title} 4.13 as a Technology Preview (TP). You can define the tags on the Azure resources in the `install-config.yaml` file only during {product-title} cluster creation.

[id="ocp-4-13-gcp-shared-vpc"]
==== Installing an {product-title} cluster on GCP into a shared Virtual Private Cloud (VPC)
In {product-title} {product-version}, you can install a cluster into a shared Virtual Private Cloud (VPC) on Google Cloud Platform (GCP). This installation method configures the cluster to share a VPC with another GCP project. A shared VPC enables an organization to connect resources from multiple projects over a common VPC network. A common VPC network can increase the security and efficiency of organizational communications by using internal IP addresses.

For more information, see xref:../installing/installing_gcp/installing-gcp-shared-vpc.adoc#installing-gcp-shared-vpc[Installing a cluster on GCP into a shared VPC].

[id="ocp-4-13-installation-gcp-shielded-vms"]
==== Installing a cluster on GCP using Shielded VMs
In {product-title} {product-version}, you can use Shielded VMs when installing your cluster. Shielded VMs have extra security features including secure boot, firmware and integrity monitoring, and rootkit detection. For more information, see xref:../installing/installing_gcp/installing-gcp-customizations.adoc#installation-gcp-enabling-shielded-vms_installing-gcp-customizations[Enabling Shielded VMs] and Google's documentation on link:https://cloud.google.com/shielded-vm[Shielded VMs].

[id="ocp-4-13-admin-ack-upgrading"]
==== Required administrator acknowledgment when upgrading from {product-title} 4.12 to 4.13

{product-title} 4.13 uses Kubernetes 1.26, which removed xref:../release_notes/ocp-4-13-release-notes.adoc#ocp-4-13-removed-kube-1-26-apis[several deprecated APIs].

A cluster administrator must provide a manual acknowledgment before the cluster can be upgraded from {product-title} 4.12 to 4.13. This is to help prevent issues after upgrading to {product-title} 4.13, where APIs that have been removed are still in use by workloads, tools, or other components running on or interacting with the cluster. Administrators must evaluate their cluster for any APIs in use that will be removed and migrate the affected components to use the appropriate new API version. After this is done, the administrator can provide the administrator acknowledgment.

All {product-title} 4.12 clusters require this administrator acknowledgment before they can be upgraded to {product-title} 4.13.

For more information, see xref:../updating/updating-cluster-prepare.adoc#updating-cluster-prepare[Preparing to update to {product-title} 4.13].

[id="ocp-4-13-installation-minimum-required-permissions-azure"]
==== Minimum required permissions for Microsoft Azure to install and delete an {product-title} cluster
In {product-title} {product-version}, instead of using the built-in roles, you can now define your custom roles to include the minimum required permissions for Microsoft Azure to install and delete an {product-title} cluster. These permissions are available for installer-provisioned infrastructure and user-provisioned infrastructure.

[id="ocp-4-13-post-installation"]
=== Post-installation configuration

[id="ocp-4-13-post-installation-vsphere-failure-domains"]
==== Specifying multiple failure domains for your cluster on VSphere
As an administrator, you can specify multiple failure domains for your {product-title} cluster that runs on a VMware VSphere instance. This means that you can distribute key control planes and workload elements among varied hardware resources for a datacenter. Additionally, you can configure your cluster to use a multiple layer 2 network configuration, so that data transfer among nodes can span across multiple networks.

[id="ocp-4-13-web-console"]
=== Web console
[id="ocp-4-13-developer-perspective"]
==== Developer Perspective

With this release, you can now perform the following actions in the *Developer* perspective of the web console:

* Create a *Serverless Function* by using the *Import from Git* flow.
* Create a *Serverless Function* by using the *Create Serverless Function* flow available on *Add page*.
* Select *pipeline-as-code* as an option in the *Import from Git* workflow.
* View which pods are receiving traffic in the following locations in the user interface:
** The side pane of the *Topology* view
** The *Details* view for a pod
** The *Pods* list view
* Customize the timeout period or provide your own image when instantiating a *Web Terminal*.
* As an administrator, set default resources to be pre-pinned in the *Developer* perspective navigation for all users.

[id="ocp-4-13-pipelines-page-improvements"]
===== Pipelines page improvements
In {product-title} 4.13, you can see the following navigation improvements on the *Pipelines* page:

* The tab you previously selected remains visible when you return to the *Pipelines* page.
* The default tab for the *Repository details* page is now *PipelinesRuns*, but when you are following the *Create Git Repository* flow, the default tab is *Details*.

[id="ocp-4-13-helm-page-improvements"]
===== Helm page improvements
In {product-title} 4.13, the *Helm* page now contains the following new and updated features:

* The terminology used on the page now refers to creating and deleting Helm releases rather than installing and uninstalling Helm charts.
* You can create and delete Helm releases asynchronously and not wait for actions to complete before performing the next task in the web console.
* The Helm release list now includes a *Status* column.


[id="ocp-4-13-oc"]
=== OpenShift CLI (oc)

[id="ocp-4-13-oc-must-gather-namespace"]
==== New flag added to run must-gather in a specified namespace

With {product-title} 4.13, the `--run-namespace` flag is now available for the `oc adm must-gather` command. You can use this flag to specify an existing namespace to run the must-gather tool in.

For more information, see xref:../support/gathering-cluster-data.adoc#about-must-gather_gathering-cluster-data[About the must-gather tool].

[id="ocp-4-13-import-image"]
==== Importing manifests with the OpenShift CLI (oc)

As of {product-title} 4.12, limited usability (Technology Preview) support for manifest listed images on image streams is available.

With {product-title} {product-version}, a new `oc` command line interface (CLI) flag, `--import-mode`, has been added to the following `oc` commands:

* `oc import-image`
* `oc tag`

With this enhancement, users can set the `--import-mode` flag to `Legacy` or `PreserveOriginal`, which provides users the option to import a single sub-manifest, or all manifests, of a manifest list when running the `oc import-image` or `oc tag` commands.

For more information, see xref:../openshift_images/image-streams-manage.adoc#images-imagestream-import-import-mode_image-streams-managing[Working with manifest lists].

[id="ocp-4-13-oc-describe-enhancement"]
==== Returning os/arch and digests of an image

With {product-title} {product-version}, running `oc describe` on an image now returns os/arch and digests of each manifest.

[id="ocp-4-13-ibm-z"]
=== IBM Z and LinuxONE

[id="ocp-4-13-images"]
=== Images

[id="ocp-4-13-security"]
=== Security and compliance

[id="ocp-4-13-aesgcm-encryption"]
==== AES-GCM encryption is now supported

The AES-GCM encryption type is now supported when enabling etcd encryption for {product-title}. Encryption keys for the AES-GCM encryption type are rotated weekly.

For more information, see xref:../security/encrypting-etcd.adoc#etcd-encryption-types_encrypting-etcd[Supported encryption types].

[id="ocp-4-13-auth"]
=== Authentication and authorization

[id="ocp-4-13-psa-tp-feature-set"]
==== Pod security admission restricted enforcement (Technology Preview)

With this release, pod security admission restricted _enforcement_ is available as a Technology Preview feature by enabling the `TechPreviewNoUpgrade` feature set. If you enable the `TechPreviewNoUpgrade` feature set, pods are rejected if they violate pod security standards, instead of only logging a warning.

[WARNING]
====
Enabling the `TechPreviewNoUpgrade` feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
====

For more information, see xref:../nodes/clusters/nodes-cluster-enabling-features.adoc#nodes-cluster-enabling-features-about_nodes-cluster-enabling[Understanding feature gates].

[id="ocp-4-13-networking"]
=== Networking

[id="ocp-4-13-networking-metrics"]
==== Enhancements to networking metrics
===== egress_ips_rebalance_total
* Metric name: `ovnkube_master_egress_ips_rebalance_total`
* *Help message:* `The total number of times assigned egress IP(s) needed to be moved to a different node.`

===== egress_ips_node_unreachable_total
* Metric name: `ovnkube_master_egress_ips_node_unreachable_total`
* *Help message*: `The total number of times assigned egress IP(s) were unreachable.`

===== egress_ips_unassign_latency_seconds
* Metric name: `ovnkube_master_egress_ips_unassign_latency_seconds`
* *Help message*: `The latency of egress IP unassignment from OVN northbound database.`

===== interfaces_total
* Metric name: `ovs_vswitchd_interfaces_total`
* *Help message*: `The total number of Open vSwitch interface(s) created for pods` and  `Open vSwitch interface until its available.`

===== interface_up_wait_seconds_total
* Metric name: `ovs_vswitchd_interface_up_wait_seconds_total`
* *Help message*: `The total number of seconds that is required to wait for pod.` and `Open vSwitch interface until its available.`

===== ovnkube_resource_retry_failures_total
* Metric name: `ovnkube_resource_retry_failures_total`
* *Help message*: `The total number of times processing a Kubernetes resource reached the maximum retry limit and was no longer processed.`

[id="ocp-4-13-networking-alerts"]
==== Enhancements to networking alerts
* OVN Kubernetes retries a claim up to 15 times before dropping it. With this update, if this failure happens, {product-title} alerts the cluster administrator. A description of each alert can be viewed in the console.

===== NoOvnMasterLeader
* Summary: There is no ovn-kubernetes master leader.
* Description in console:
[source,text]
----
Networking control plane is degraded. Networking configuration updates applied to the cluster will not be implemented while there is no OVN Kubernetes leader. Existing workloads should continue to have connectivity. OVN-Kubernetes control plane is not functional.
----

===== OVNKubernetesNodeOVSOverflowUserspace
* Summary: OVS vSwitch daemon drops packets due to buffer overflow.
* Description in console:
[source,text]
----
Netlink messages dropped by OVS vSwitch daemon due to netlink socket buffer overflow. This will result in packet loss.
----

===== OVNKubernetesNodeOVSOverflowKernel
* Summary: OVS kernel module drops packets due to buffer overflow.
* Description in console:
[source,text]
----
Netlink messages dropped by OVS kernel module due to netlink socket buffer overflow. This will result in packet loss.
----

[id="ocp-4-13-nw-metallb-ipaddresspool-assignment"]
==== Assign IP addresses in MetalLB IPAddressPool resources to specific namespaces and services
With this update, you can assign IP addresses from a MetalLB `IPAddressPool` resource to services, namespaces, or both. This is useful in a muti-tenant, bare-metal environment that requires MetalLB to pin IP addresses from an IP address pool to specific services and namespaces. You can assign IP addresses from many IP address pools to services and namespaces. You can then define the prioritization for these IP address pools so that MetalLB assigns IP addresses starting from the higher priority IP address pool.

For more information about assigning IP addresses from an IP address pool to services and namespaces, see xref:../networking/metallb/metallb-configure-address-pools.adoc#nw-metallb-configure-address-pool_configure-metallb-address-pools[Configuring MetalLB address pools].

[id="ocp-4-13-supporting-ocp-nodes-with-dual-port-nics"]
==== Supporting {product-title} installation on nodes with dual-port NICs (Technology Preview)

With this update, {product-title} cluster can be deployed on a bond interface with 2 virtual function (VFs) on 2 physical functions (PFs) using the following methods:

* Agent-based installer
* Installer-provisioned infrastructure installation
* User-provisioned infrastructure installation

For more information about installing {product-title} on nodes with dual-port NICs, see xref:../installing/installing_bare_metal/preparing-to-install-on-bare-metal.adoc#nw-sriov-dual-nic-con_preparing-to-install-on-bare-metal[NIC partitioning for SR-IOV devices].

[id="ocp-4-13-bf2-switching-dpu-nic"]
==== Support for switching the BlueField-2 network device from data processing unit (DPU) mode to network interface controller (NIC) mode is now GA

In this release, switching the BlueField-2 network device from data processing unit (DPU) mode to network interface controller (NIC) mode is now generally available.

For more information, see xref:../networking/hardware_networks/switching-bf2-nic-dpu.adoc#switching-bf2-nic-dpu[Switching BlueField-2 from DPU to NIC].

[id="ocp-4-13-networking-supported-hardware-for-ovs"]
==== Hardware offload for the MT2892 Family [ConnectX-6 Dx] of network cards is GA

{product-title} 4.13 adds OvS Hardware Offload support for the MT2892 Family [ConnectX-6 Dx] of network cards.

For more information, see xref:../networking/hardware_networks/configuring-hardware-offloading.adoc#supported_devices_configuring-hardware-offloading[Supported devices].

[id="ocp-4-13-migrate-to-openshift-sdn-network-plugin"]
==== Migrating to the OpenShift SDN network plugin

If you are using the OVN-Kubernetes network plugin, you can migrate to the OpenShift SDN network plugin.

For more information, see xref:../networking/openshift_sdn/migrate-to-openshift-sdn.adoc#migrate-to-openshift-sdn[Migrating to the OpenShift SDN network plugin].

[id="ocp-4-13-expand-cluster-network-ip-address-range"]
==== Expand cluster network IP address range

The cluster network can be expanded to support the addition of nodes to the cluster. For example, if you deployed a cluster and specified `10.128.0.0/19` as the cluster network range and a host prefix of `23`, you are limited to 16 nodes. You can expand that to 510 nodes by changing the CIDR mask on a cluster to `/14`. For more information, see xref:../networking/configuring-cluster-network-range.adoc#configuring-cluster-network-range[Configuring the cluster network range].

[id="ocp-4-13-storage"]
=== Storage

[id="ocp-4-13-storage-lvms-dual-stack-support"]
==== Dual-stack support for {lvms-first}

In {product-title} {product-version}, {lvms} is supported in dual-stack for IPv4 and IPv6 network environments. For more information, see xref:../networking/ovn_kubernetes_network_provider/converting-to-dual-stack.adoc#nw-dual-stack-convert_converting-to-dual-stack[Converting to a dual-stack cluster network].

[id="ocp-4-13-olm"]
=== Operator lifecycle

[id="ocp-4-13-osdk"]
=== Operator development

[id="ocp-4-13-suggested-namespace-template"]
==== Setting a suggested namespace template with default node selector

With this release, Operator authors can set a default node selector on the suggested namespace where the Operator runs. The suggested namespace is created using the namespace manifest in the YAML which is included in the `ClusterServiceVersion` (CSV).  When adding the Operator to a cluster using OperatorHub, the web console automatically populates the suggested namespace for the cluster administrator during the installation process.

For more information, see xref:../operators/operator_sdk/osdk-generating-csvs.adoc#osdk-suggested-namespace-default-node_osdk-generating-csvs[Setting a suggested namespace with default node selector].

[id="ocp-4-13-nto"]
==== Node Tuning Operator

The Node Tuning Operator (NTO) can now be enabled/disabled using the `NodeTuning` cluster capability. If disabled at cluster install, it can be re-enabled later. For more information, see xref:../installing/cluster-capabilities.adoc#about-node-tuning-operator_cluster-capabilities[Node Tuning capability].

[id="ocp-4-13-machine-api"]
=== Machine API

[id="ocp-4-13-mapi-cpms-platform-support"]
==== Additional platform support for control plane machine sets

* With this release, control plane machine sets are supported for Google Cloud Platform clusters.

* This release includes an enhancement to the user experience for the control plane machine set on Microsoft Azure clusters. For Azure clusters that are installed with or upgraded to {product-title} version 4.13, you are no longer required to create a control plane machine set custom resource (CR).
+
--
* Clusters that are installed with version 4.13 have a control plane machine set that is active by default.

* For clusters that are upgraded to version 4.13, an inactive CR is generated for the cluster and can be activated after you verify that the values in the CR are correct for your control plane machines.
--

For more information, see xref:../machine_management/control_plane_machine_management/cpmso-getting-started.adoc#cpmso-getting-started[Getting started with the Control Plane Machine Set Operator].

[id="ocp-4-13-machine-config-operator"]
=== Machine Config Operator

[id="ocp-4-13-machine-config-operator-layering"]
==== Support for adding third party and custom content to {op-system}

You can now use {op-system-first} image layering to add {op-system-base-full} and third-party packages to cluster nodes.

For more information, see  xref:../post_installation_configuration/coreos-layering.adoc#coreos-layering[{op-system-first} image layering].

[id="ocp-4-13-machine-config-operator-core"]
==== Support for setting the `core` user password

You can now create a password for the {op-system} `core` user. If you cannot use SSH or the `oc debug node` command to access a node, this password allows you to use the `core` user to access the node through a cloud provider serial console or a bare metal baseboard controller manager (BMC).

For more information, see xref:../post_installation_configuration/machine-configuration-tasks.adoc#core-user-password_post-install-machine-configuration-tasks[Changing the core user password for node access].

[id="ocp-4-13-nodes"]
=== Nodes

[id="ocp-4-13-nodes-mirror"]
==== Image registry repository mirroring by tags

You can now pull images from a mirrored registry by using image tags in addition to digest specifications. To accomplish this change, the `ImageContentSourcePolicy` (ICSP) object is deprecated. You can now use an `ImageDigestMirrorSet` (IDMS) object to pull images by using digest specifications or an `ImageTagMirrorSet` (ITMS) object to pull images by using image tags.

If you have existing YAML files that you used to create ICSP objects, you can use the `oc adm migrate icsp` command to convert those files to an IDMS YAML file.

For more information on these new objects, see xref:../post_installation_configuration/preparing-for-users.adoc#images-configuration-registry-mirror_post-install-preparing-for-users[Configuring image registry repository mirroring].

For more information on converting existing ICSP YAML files to IDMS YAML files, see xref:../post_installation_configuration/preparing-for-users.adoc#images-configuration-registry-mirror-convert_post-install-preparing-for-users[Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring].

[id="ocp-4-13-nodes-crun-ga"]
==== crun general availability

The crun low-level container runtime is now generally available in {product-title} 4.13. There is no new functionality in the GA version.

[id="ocp-4-13-nodes-cgroup-v2-ga"]
==== Linux Control Group version 2 (cgroup v2) general availability

Linux Control Group version 2 (cgroup v2) is now generally available in {product-title} 4.13. There is no new functionality in the GA version.

[id="ocp-4-13-monitoring"]
=== Monitoring
The monitoring stack for this release includes the following new and modified features.

[id="ocp-4-13-monitoring-updates-to-monitoring-stack-components-and-dependencies"]
==== Updates to monitoring stack components and dependencies
This release includes the following version updates for monitoring stack components and dependencies:

* Alertmanager to 0.25.0
* kube-state-metrics to 2.8.1
* node-exporter to 1.5.0
* prom-label-proxy to 0.6.0
* Prometheus to 2.42.0
* prometheus-operator to 0.63.0
* Thanos to 0.30.2

[id="ocp-4-13-monitoring-changes-to-alerting-rules"]
==== Changes to alerting rules

[NOTE]
====
Red Hat does not guarantee backward compatibility for recording rules or alerting rules.
====

* The `NodeFilesystemAlmostOutOfSpace` alert no longer fires for certain `tmpfs` mount points that are always full by design.

[id="ocp-4-13-monitoring-new-option-to-add-secrets-to-the-alertmanager-configuration"]
==== New option to add secrets to the Alertmanager configuration
With this release, you can add secrets to the Alertmanager configuration for core platform monitoring and for user-defined projects.
If you need to authenticate with a receiver so that Alertmanager can send alerts to it, you can now configure Alertmanager to use a secret that contains authentication credentials for the receiver.

[id="ocp-4-13-monitoring-new-option-to-configure-node-exporter-collectors"]
==== New option to configure node-exporter collectors
With this release, you can customize Cluster Monitoring Operator (CMO) config map settings for the following node-exporter collectors.
The following node-exporter collectors are now optional and can be enabled or disabled:

* `buddyinfo` collector
* `cpufreq` collector
* `netclass` collector
* `netdev` collector
* `netlink` backend for the `netclass` collector
* `tcpstat` collector

[id="ocp-4-13-monitoring-new-option-to-filter-node-related-dashboards-by-node-role"]
==== New option to filter node-related dashboards by node role
In the {product-title} web console, you can now filter data in node-related monitoring dashboards based on node roles.
You can use this new filter to quickly select relevant node roles if you want to see dashboard data only for nodes with certain roles, such as worker nodes.

[id="ocp-4-13-monitoring-new-option-to-enable-metrics-collection-profiles-technology-preview"]
==== New option to enable metrics collection profiles (Technology Preview)
This release introduces a Technology Preview feature for default platform monitoring in which an administrator can set a metrics collection profile to collect either the default amount of metrics data or a minimal amount of metrics data.
When you enable the minimal profile, basic monitoring features such as alerting continue to work, but the CPU and memory resources required by Prometheus decrease.

[id="ocp-4-13-scalability-and-performance"]
=== Scalability and performance

[id="ocp-4-13-NUMA-scheduler-ga"]
==== NUMA-aware scheduling with the NUMA Resources Operator is generally available

NUMA-aware scheduling with the NUMA Resources Operator was previously introduced as a Technology Preview in {product-title} 4.10 and is now generally available in {product-title} {product-version}.

The NUMA Resources Operator deploys a NUMA-aware secondary scheduler that makes scheduling decisions for workloads based on a complete picture of available NUMA zones in clusters. This enhanced NUMA-aware scheduling ensures that latency-sensitive workloads are processed in a single NUMA zone for maximum efficiency and performance.

This update adds the following features:

* Fine-tuning of API polling for NUMA resource reports.
* Configuration options at the node group level for the node topology exporter.

For more information, see xref:../scalability_and_performance/cnf-numa-aware-scheduling.adoc#cnf-about-numa-aware-scheduling_numa-aware[Scheduling NUMA-aware workloads].

[id="ocp-4-13-configuring-power-states-using-ztp"]
==== Configuring power states using zero touch provisioning (ZTP)

{product-title} 4.12 introduced the ability to set power states for critical and non-critical workloads. This release now enables the user to configure power states by using ZTP.

For more information about the feature, see xref:../scalability_and_performance/ztp_far_edge/ztp-advanced-policy-config.adoc#ztp-using-pgt-to-configure-power-saving-states_ztp-advanced-policy-config[Configuring power states using PolicyGenTemplates CRs].

[id="ocp-4-13-insights-operator"]
=== Insights Operator

[id="ocp-4-13-hcp"]
=== Hosted control planes (Technology Preview)

[id="ocp-4-13-rhv"]
=== Red Hat Virtualization (RHV)

[id="install-sno-requirements-for-installing-on-a-single-node"]
=== Requirements for installing OpenShift on a single node

{product-version} now supports `x86_64` and `arm64` CPU architectures.

[id="ocp-4-13-notable-technical-changes"]
== Notable technical changes

{product-title} {product-version} introduces the following notable technical changes.

// Note: use [discrete] for these sub-headings.

[discrete]
[id="ocp-4-13-cluster-cloud-controller-manager-operator"]
=== Cloud controller managers for additional cloud providers

The Kubernetes community plans to deprecate the use of the Kubernetes controller manager to interact with underlying cloud platforms in favor of using cloud controller managers. As a result, there is no plan to add Kubernetes controller manager support for any new cloud platforms.

The Nutanix implementation that is added in this release of {product-title} uses cloud controller managers. In addition, this release introduces the General Availability of using cloud controller managers for VMware vSphere.

To learn more about the cloud controller manager, see the link:https://kubernetes.io/docs/concepts/architecture/cloud-controller/[Kubernetes Cloud Controller Manager documentation].

To manage the cloud controller manager and cloud node manager deployments and lifecycles, use the Cluster Cloud Controller Manager Operator.

For more information, see the xref:../operators/operator-reference.adoc#cluster-cloud-controller-manager-operator_cluster-operators-ref[Cluster Cloud Controller Manager Operator] entry in the _Platform Operators reference_.

[discrete]
[id="ocp-4-13-mco-certificate-changes"]
=== The MCD now syncs kubelet CA certificates on paused pools

Previously, the Machine Config Operator (MCO) updated the kubelet client certificate authority (CA) certificate, `/etc/kubernetes/kubelet-ca.crt`, as a part of the regular machine config update. Starting with {product-title} (product-version}, the `kubelet-ca.crt` no longer gets updated as a part of the regular machine config update. As a result of this change, the Machine Config Daemon (MCD) automatically keeps the `kubelet-ca.crt` up to date whenever changes to the certificate occur.

Also, if a machine config pool is paused, the MCD is now able to push the newly rotated certificates to those nodes. A new rendered machine config, which contains the changes to the certificate, is generated for the pool, like in previous versions. The pool will indicate that an update is required; this condition will be removed in a future release of this product. However, because the certificate is updated separately, it is safe to keep the pool paused, assuming there are no further updates.

Also, the `MachineConfigControllerPausedPoolKubeletCA` alert has been removed, because the nodes should always have the most up-to-date `kubelet-ca.crt`.

[discrete]
[id="ocp-4-13-rhcos-ssh-key-location"]
==== Change in SSH key location
{product-title} {product-version} introduces a {op-system-base} 9.2 based {op-system}. Before this update, SSH keys were located in `/home/core/.ssh/authorized_keys` on {op-system}. With this update, on {op-system-base} 9.2 based {op-system}, SSH keys are located in `/home/core/.ssh/authorized_keys.d/ignition`.

[discrete]
[id="ocp-4-13-psa-restricted-enforcement"]
=== Future restricted enforcement for pod security admission

Currently, pod security violations are shown as warnings and logged in the audit logs, but do not cause the pod to be rejected.

Global restricted enforcement for pod security admission is currently planned for the next minor release of {product-title}. When this restricted enforcement is enabled, pods with pod security violations will be rejected.

To prepare for this upcoming change, ensure that your workloads match the pod security admission profile that applies to them. Workloads that are not configured according to the enforced security standards defined globally or at the namespace level will be rejected. The `restricted-v2` SCC admits workloads according to the link:https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted[Restricted] Kubernetes definition.

If you are receiving pod security violations, see the following resources:

* See xref:../authentication/understanding-and-managing-pod-security-admission.adoc#security-context-constraints-psa-alert-eval_understanding-and-managing-pod-security-admission[Identifying pod security violations] for information about how to find which workloads are causing pod security violations.

* See xref:../authentication/understanding-and-managing-pod-security-admission.adoc#security-context-constraints-psa-synchronization_understanding-and-managing-pod-security-admission[Security context constraint synchronization with pod security standards] to understand when pod security admission label synchronization is performed. Pod security admission labels are not synchronized in certain situations, such as the following situations:
** The workload is running in a system-created namespace that is prefixed with `openshift-`.
** The workload is running on a pod that was created directly without a pod controller.

* If necessary, you can set a custom admission profile on the namespace or pod by setting the `pod-security.kubernetes.io/enforce` label.

[id="ocp-4-13-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* _General Availability_
* _Deprecated_
* _Removed_

[discrete]
=== Operator deprecated and removed features

.Operator deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|SQLite database format for Operator catalogs
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
=== Images deprecated and removed features

.Images deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|`ImageChangesInProgress` condition for Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated

|`MigrationInProgress` condition for Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated

|====

//[discrete]
//=== Monitoring deprecated and removed features

//.Monitoring deprecated and removed tracker
//[cols="4,1,1,1",options="header"]
//|====
//|Feature |4.11 |4.12 |4.13

//|====

[discrete]
=== Installation deprecated and removed features

.Installation deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|vSphere 7.0 Update 1 or earlier
|General Availability
|Deprecated
|Deprecated

|VMware ESXi 7.0 Update 1 or earlier
|General Availability
|Deprecated
|Deprecated

|CoreDNS wildcard queries for the `cluster.local` domain
|General Availability
|General Availability
|Deprecated

|`ingressVIP` and `apiVIP` settings in the `install-config.yaml` file for installer-provisioned infrastructure clusters
|General Availability
|General Availability
|Deprecated

|====

//[discrete]
//=== Updating clusters deprecated and removed features

//.Updating clusters deprecated and removed tracker
//[cols="4,1,1,1",options="header"]
//|====
//|Feature |4.11 |4.12 |4.13

//|====

[discrete]
=== Storage deprecated and removed features

.Storage deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Persistent storage using FlexVolume
|Deprecated
|Deprecated
|Deprecated

|====

//[discrete]
//=== Authentication and authorization deprecated and removed features

//.Authentication and authorization deprecated and removed tracker
//[cols="4,1,1,1",options="header"]
//|====
//|Feature |4.11 |4.12 |4.13

//|====

[discrete]
=== Specialized hardware and driver enablement deprecated and removed features

.Specialized hardware and driver enablement deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Special Resource Operator (SRO)
|Technology Preview
|Technology Preview
|Removed

|====

[discrete]
=== Multi-architecture deprecated and removed features

.Multi-architecture deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|IBM Power8 all models (`ppc64le`)
|General Availability
|General Availability
|Removed

|{ibmpowerProductName} AC922 (`ppc64le`)
|General Availability
|General Availability
|Removed

|{ibmpowerProductName} IC922 (`ppc64le`)
|General Availability
|General Availability
|Removed

|{ibmpowerProductName} LC922 (`ppc64le`)
|General Availability
|General Availability
|Removed

|IBM z13 all models (`s390x`)
|General Availability
|General Availability
|Removed

|{linuxoneProductName} Emperor (`s390x`)
|General Availability
|General Availability
|Removed

|{linuxoneProductName} Rockhopper (`s390x`)
|General Availability
|Deprecated
|Removed

|AMD64 (x86_64) v1 CPU
|General Availability
|Deprecated
|Removed

|====

[discrete]
=== Networking deprecated and removed features

.Networking deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Kuryr on {rh-openstack}
|General Availability
|General Availability
|Deprecated

|====

[discrete]
=== Node deprecated and removed features

.Node deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|`ImageContentSourcePolicy` (ICSP) objects
|General Availability
|General Availability
|Deprecated

|====

[id="ocp-4-13-deprecated-features"]
=== Deprecated features

[id="ocp-4-13-rhv-deprecations"]
==== Red Hat Virtualization (RHV) as a host platform for {product-title} will be deprecated

Red Hat Virtualization (RHV) will be deprecated in an upcoming release of {product-title}. Support for {product-title} on RHV will be removed from a future {product-title} release, currently planned as {product-title} 4.14.

[id="ocp-4-13-ne-deprecations"]
==== Wildcard DNS queries for the `cluster.local` domain are deprecated

CoreDNS will stop supporting wildcard DNS queries for names under the `cluster.local` domain. These queries will resolve in {product-title} {product-version} as they do in earlier versions, but support will be removed from a future {product-title} release.

[id="ocp-4-13-ne-kuryr"]
==== Kuryr support for clusters that run on {rh-openstack}

In {product-title} 4.12, support for Kuryr on clusters that run on {rh-openstack} is deprecated. Support will be removed no earlier than {product-title} 4.14.

[id="ocp-4-13-icsp"]
==== `ImageContentSourcePolicy` objects

The `ImageContentSourcePolicy` (ICSP) object is now deprecated. You can now use an `ImageDigestMirrorSet` (IDMS) object to pull images by using digest specifications or an `ImageTagMirrorSet` (ITMS) object to pull images by using image tags.

For more information on these new objects, see xref:../post_installation_configuration/preparing-for-users.adoc#images-configuration-registry-mirror_post-install-preparing-for-users[Configuring image registry repository mirroring].

For more information on converting existing ICSP YAML files to IDMS YAML files, see xref:../post_installation_configuration/preparing-for-users.adoc#images-configuration-registry-mirror-convert_post-install-preparing-for-users[Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring].

[id="ocp-4-13-toolbox-deprecation"]
==== Toolbox is deprecated in {op-system}
The toolbox script is deprecated and support will be removed from a future {product-title} release.

[id="ocp-4-13-rhel-9-device-driver-deprecation"]
==== {op-system-base} 9 driver deprecations
{product-title} {product-version} introduces a {op-system-base} 9.2 based {op-system}. Some kernel device drivers are deprecated in {op-system-base} 9. See the link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/considerations_in_adopting_rhel_9/assembly_hardware-enablement_considerations-in-adopting-rhel-9[{op-system-base} documentation] for more information.

[id="ocp-4-13-vSphere-configuration-parameters"]
==== VMware vSphere configuration parameters

{product-title} {product-version} deprecates the following vSphere configuration parameters. You can continue to use these parameters, but the installation program does not automatically specify these parameters in the `install-config.yaml` file.

* `platform.vsphere.vCenter`
* `platform.vsphere.username`
* `platform.vsphere.password`
* `platform.vsphere.datacenter`
* `platform.vsphere.defaultDatastore`
* `platform.vsphere.cluster`
* `platform.vsphere.folder`
* `platform.vsphere.resourcePool`
* `platform.vsphere.apiVIP`
* `platform.vsphere.ingressVIP`
* `platform.vsphere.network`

[id="ocp-4-13-removed-features"]
=== Removed features

[id="ocp-4-13-removed-kube-1-26-apis"]
==== Beta APIs removed from Kubernetes 1.26

Kubernetes 1.26 removed the following deprecated APIs, so you must migrate manifests and API clients to use the appropriate API version. For more information about migrating removed APIs, see the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-26[Kubernetes documentation].

.APIs removed from Kubernetes 1.26
[cols="2,2,2",options="header",]
|===
|Resource |Removed API |Migrate to

|`FlowSchema`
|`flowcontrol.apiserver.k8s.io/v1beta1`
|`flowcontrol.apiserver.k8s.io/v1beta3`

|`HorizontalPodAutoscaler`
|`autoscaling/v2beta2`
|`autoscaling/v2`

|`PriorityLevelConfiguration`
|`flowcontrol.apiserver.k8s.io/v1beta1`
|`flowcontrol.apiserver.k8s.io/v1beta3`

|===

[id="ocp-4-13-future-removals"]
=== Future Kubernetes API removals

The next minor release of {product-title} is expected to use Kubernetes 1.27. Currently, Kubernetes 1.27 is scheduled to remove a deprecated API.

See the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-27[Deprecated API Migration Guide] in the upstream Kubernetes documentation for the list of planned Kubernetes API removals.

See link:https://access.redhat.com/articles/6955985[Navigating Kubernetes API deprecations and removals] for information about how to check your cluster for Kubernetes APIs that are planned for removal.

[id="ocp-4-13-z13-power8-x86v1-remove"]
==== Specific hardware models on `ppc64le`, `s390x`, and `x86_64` v1 CPU architectures are removed

In {product-title} 4.13, support for {op-system} functionality is removed for the following deprecated hardware models:

* IBM Power8 all models (`ppc64le`)
* {ibmpowerProductName} AC922 (`ppc64le`)
* {ibmpowerProductName} IC922 (`ppc64le`)
* {ibmpowerProductName} LC922 (`ppc64le`)
* IBM z13 all models (`s390x`)
* {linuxoneProductName} Emperor (`s390x`)
* {linuxoneProductName} Rockhopper (`s390x`)
* AMD64 (`x86_64`) v1 CPU

[id="ocp-4-13-bug-fixes"]
== Bug fixes

[discrete]
[id="ocp-4-13-api-auth-bug-fixes"]
==== API Server and Authentication

//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / TALO
//Telco Edge / ZTP

[discrete]
[id="ocp-4-13-bare-metal-hardware-bug-fixes"]
==== Bare Metal Hardware Provisioning

[discrete]
[id="ocp-4-13-builds-bug-fixes"]
==== Builds

[discrete]
[id="ocp-4-13-cloud-compute-bug-fixes"]
==== Cloud Compute

* For some configurations of Google Cloud Platform clusters, the internal load balancer uses instance groups that are created by the installation program. Previously, when a control plane machine was replaced manually, the new control plane node was not assigned to a control plane instance group. This prevented the node from being reachable via the internal load balancer. To resolve the issue, administrators had to manually move the control plane machine to the correct instance group by using the Google Cloud console.
+
With this release, replacement control plane nodes are assigned to the correct instance group.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1970464[*BZ#1970464*], link:https://issues.redhat.com/browse/OCPCLOUD-1562[*OCPCLOUD-1562*])

[discrete]
[id="ocp-4-13-dev-console-bug-fixes"]
==== Developer Console

[discrete]
[id="ocp-4-13-image-registry-bug-fixes"]
==== Image Registry

[discrete]
[id="ocp-4-13-installer-bug-fixes"]
==== Installer

[discrete]
[id="ocp-4-13-kube-controller-bug-fixes"]
==== Kubernetes Controller Manager

[discrete]
[id="ocp-4-13-kube-scheduler-bug-fixes"]
==== Kubernetes Scheduler

* Previously, when the `LifeCycleUtilization` profile was excluded to test namespace filtering, the following error was logged in the Descheduler Operator logs: `belowE0222 12:43:14.331258 1 target_config_reconciler.go:668] key failed with : only namespace exclusion supported with LowNodeUtilization`. Consequently, the Descheduler cluster pod would not start. This issue has been resolved and namespace exclusion now works with the `LifeCycleUtilization` profile. (link:https://issues.redhat.com/browse/OCPBUGS-7876[*OCPBUGS-7876*])

[discrete]
[id="ocp-4-13-machine-config-operator-bug-fixes"]
==== Machine Config Operator

[discrete]
[id="ocp-4-13-management-console-bug-fixes"]
==== Management Console
* Previously, user permissions were not checked when rendering the *Create Pod* button, and the button rendered for users without needed permissions. With this update, user permissions are checked when rendering the *Create Pod* button, and it renders for users for users with the needed permissions. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2005232[*BZ#2005232*])

* Previously, the `Pod` resource had a `PDB` *_add_*, *_edit_*, and *_remove_* actions in the Pod resource action menu that are not required. With this update, the actions are removed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2110565[*BZ#2110565*])

* Previously, the `PodDisruptiojnBudget` field on the *Details* page had an incorrect help message. With this update, the help message is now more descriptive. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2084452[*BZ#2084452*])

* Previously, when navigating to the root path of the console, the URL redirected to the *Overview* page even if metrics were disabled and it did not appear in the navigation menu. With this update, when clicking the masthead logo or navigating to the root path of the console, the URL redirects to the *project list* page if metrics are disabled. (link:https://issues.redhat.com/browse/OCPBUGS-3033[*OCPBUGS-3033*])

* Previously, the cluster dropdown was positioned so that it was not always visible, making it unclear which cluster you were viewing. With this update, the cluster dropdown is now in the masthead so the cluster dropdown is always visible, and you can always see which cluster you are viewing. (link:https://issues.redhat.com/browse/OCPBUGS-7089[*OCPBUGS-7089*])

* Previously, the node progress bars were set to display when the cluster version had a status of `Failing`, `UpdatingAndFailing`, and `Updating`, causing the node progress bars to display when the cluster is not updating. With this update, the node progress bars only display when the cluster version has a status of `UpdatingAndFailing` or `Updating`. (link:https://issues.redhat.com/browse/OCPBUGS-6049[*OCPBUGS-6049*])

* Previously, when downloading a `kubeconfig` file for a ServiceAccount, an error was displayed and the ServiceAccount token was unable to be reached. This error was due to the removal of automatically generated secrets. With this update, the download `kubeconfig` action has been removed and the error no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-7308[*BZ#OCPBUGS-7308*])

[discrete]
[id="ocp-4-13-monitoring-bug-fixes"]
==== Monitoring

[discrete]
[id="ocp-4-13-networking-bug-fixes"]
==== Networking

* Previously, the Ingress Operator displayed a success message for the `updateIngressClass` function logs when an error message should be displayed. With this update, the log message for Ingress Operator is accurate. (link:https://issues.redhat.com/browse/OCPBUGS-6700[*OCPBUGS-6700*])

* Previously, the Ingress Operator did not specify `ingressClass.spec.parameters.scope`, while the Ingress Class API object specifies type `cluster` by default. This caused unnecessary updates to all Ingress Classes when the Operator starts. With this update, the Ingress Operator specifies `ingressClass.spec.parameters.scope` of type `cluster`. (link:https://issues.redhat.com/browse/OCPBUGS-6701[*OCPBUGS-6701*])

* Previously, the Ingress Operator had the wrong service name in `ensureNodePortService` log message causing incorrect information to be logged. With this update, the Ingress Operator accurately logs the service in `ensureNodePortService`. (link:https://issues.redhat.com/browse/OCPBUGS-6698[*OCPBUGS-6698*])

* Previously, in {product-title} 4.7.0 and 4.6.20, the Ingress Operator used an annotation for router pods that was specific for {product-title}. This was a temporary way to configure the liveness probe's grace period in order to fix a bug. As a result, {product-title} was required to carry a patch to implement the fix. With this update, the Ingress Operator uses `terminationGracePeriodSeconds` API field making the previous patch removable in future releases. (link:https://issues.redhat.com/browse/OCPBUGS-4703[*OCPBUGS-4703*])

[discrete]
[id="ocp-4-13-node-bug-fixes"]
==== Node

* Previously, the `LowNodeUtilization` strategy, which is enabled by the `LifecycleAndUtilization` descheduler profile, did not support namespace exclusion. With this release, namespaces are excluded properly when the `LifecycleAndUtilization` descheduler profile is set. (link:https://issues.redhat.com/browse/OCPBUGS-513[*OCPBUGS-513*])

[discrete]
[id="ocp-4-13-openshift-cli-bug-fixes"]
==== OpenShift CLI (oc)

[discrete]
[id="ocp-4-13-olm-bug-fixes"]
==== Operator Lifecycle Manager (OLM)

[discrete]
[id="ocp-4-13-openshift-operator-sdk-bug-fixes"]
==== Operator SDK

[discrete]
[id="ocp-4-13-file-integrity-operator-bug-fixes"]
==== File Integrity Operator

[discrete]
[id="ocp-4-13-compliance-operator-bug-fixes"]
==== Compliance Operator

[discrete]
[id="ocp-4-13-openshift-api-server-bug-fixes"]
==== OpenShift API server

[discrete]
[id="ocp-4-13-rhcos-bug-fixes"]
==== {op-system-first}

[discrete]
[id="ocp-4-13-scalability-and-performance-bug-fixes"]
==== Scalability and performance

[discrete]
[id="ocp-4-13-storage-bug-fixes"]
==== Storage

[id="ocp-4-13-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the following tables, features are marked with the following statuses:

* _Technology Preview_
* _General Availability_
* _Not Available_
* _Deprecated_

[discrete]
=== Networking Technology Preview features

.Networking Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|PTP single NIC hardware configured as boundary clock
|Technology Preview
|General Availability
|General Availability

|PTP dual NIC hardware configured as boundary clock
|Not Available
|Technology Preview
|Technology Preview

|PTP events with boundary clock
|Technology Preview
|General Availability
|General Availability

|Pod-level bonding for secondary networks
|Technology Preview
|General Availability
|General Availability

|External DNS Operator
|Technology Preview
|General Availability
|General Availability

|AWS Load Balancer Operator
|Not Available
|Technology Preview
|Technology Preview

|Ingress Node Firewall Operator
|Not Available
|Technology Preview
|Technology Preview

|Advertise using BGP mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Not Available
|Technology Preview
|General Availability

|Advertise using L2 mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Not Available
|Technology Preview
|Technology Preview

|Multi-network policies for SR-IOV networks
|Not Available
|Not Available
|Technology Preview

|Updating the interface-specific safe sysctls list
|Not Available
|Not Available
|Technology Preview

|MT2892 Family [ConnectX-6 Dx] SR-IOV support
|Not Available
|Not Available
|Technology Preview

|MT2894 Family [ConnectX-6 Lx] SR-IOV support
|Not Available
|Not Available
|Technology Preview

|MT42822 BlueField-2 in ConnectX-6 NIC mode SR-IOV support
|Not Available
|Not Available
|Technology Preview

|Silicom STS Family SR-IOV support
|Not Available
|Not Available
|Technology Preview

|MT2892 Family [ConnectX-6 Dx] OvS Hardware Offload support
|Not Available
|Technology Preview
|General Availability

|MT2894 Family [ConnectX-6 Lx] OvS Hardware Offload support
|Not Available
|Not Available
|Technology Preview

|MT42822 BlueField-2 in ConnectX-6 NIC mode OvS Hardware Offload support
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Storage Technology Preview features

.Storage Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Shared Resources CSI Driver and Build CSI Volumes in OpenShift Builds
|Technology Preview
|Technology Preview
|Technology Preview

|CSI volume expansion
|Technology Preview
|General Availability
|General Availability

|CSI Azure File Driver Operator
|Technology Preview
|General Availability
|General Availability

|CSI Google Filestore Driver Operator
|Not Available
|Not Available
|Technology Preview

|CSI automatic migration
(Azure file, VMware vSphere)
|Technology Preview
|Technology Preview
|General Availability

|CSI automatic migration
(Azure Disk, OpenStack Cinder)
|Technology Preview
|General Availability
|General Availability

|CSI automatic migration
(AWS EBS, GCP disk)
|Technology Preview
|Technology Preview
|General Availability

|CSI inline ephemeral volumes
|Technology Preview
|Technology Preview
|Technology Preview

|CSI generic ephemeral volumes
|Not Available
|General Availability
|General Availability

|Shared Resource CSI Driver
|Technology Preview
|Technology Preview
|Technology Preview

|CSI Google Filestore Driver Operator
|Not Available
|Not Available
|Technology Preview

|Automatic device discovery and provisioning with Local Storage Operator
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Installation Technology Preview features

.Installation Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Adding kernel modules to nodes with kvc
|Technology Preview
|Technology Preview
|Technology Preview

|IBM Cloud VPC clusters
|Technology Preview
|Technology Preview
|General Availability

|Selectable Cluster Inventory
|Technology Preview
|Technology Preview
|Technology Preview

|Multi-architecture compute machines
|Not Available
|Technology Preview
|General Availability

|Mount shared entitlements in BuildConfigs in RHEL
|Technology Preview
|Technology Preview
|Technology Preview

|Agent-based {product-title} Installer
|Not Available
|Not Available
|General Availability

|Enabling NIC partitioning for SR-IOV devices
|Not Available
|Not Available
|Technology Preview

|Azure Tagging
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Node Technology Preview features

.Nodes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Non-preempting priority classes
|Technology Preview
|Technology Preview
|Technology Preview

|Linux Control Group version 2 (cgroup v2)
|Developer Preview
|Technology Preview
|General Availability

|crun container runtime
|Not Available
|Technology Preview
|General Availability

|====

[discrete]
=== Multi-Architecture Technology Preview features

.Multi-Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|`kdump` on `arm64` architecture
|Not Available
|Technology Preview
|Technology Preview

|`kdump` on `s390x` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `ppc64le` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|IBM Secure Execution on {ibmzProductName} and {linuxoneProductName}
|Not Available
|Technology Preview
|General Availability

|{ibmpowerProductName} Virtual Server using installer-provisioned infrastructure
|Not Available
|Not Available
|Technology Preview

|{ibmpowerProductName} Virtual Server Block CSI Driver Operator
|Not Available
|Not Available
|Technology Preview
|====

[discrete]
=== Specialized hardware and driver enablement Technology Preview features

.Specialized hardware and driver enablement Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Driver Toolkit
|Technology Preview
|Technology Preview
|General Availability

|Special Resource Operator (SRO)
|Technology Preview
|Technology Preview
|Not Available

|Hub and spoke cluster support
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Web console Technology Preview features

.Web console Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

//|Multicluster console
//|Technology Preview
//|Technology Preview
//|Technology Preview

|Dynamic Plug-ins
|Technology Preview
|General Availability
|General Availability

|====

[discrete]
=== Scalability and performance Technology Preview features

.Scalability and performance Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Hyperthreading-aware CPU manager policy
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Not Available
|Technology Preview
|Technology Preview

|{factory-prestaging-tool}
|Not Available
|Not Available
|Technology Preview

|Single-node OpenShift cluster expansion with worker nodes
|Not Available
|Not Available
|Technology Preview

|{cgu-operator-first}
|Technology Preview
|Technology Preview
|General Availability

|Mount namespace encapsulation
|Not Available
|Not Available
|Technology Preview

|NUMA-aware scheduling with NUMA Resources Operator
|Technology Preview
|Technology Preview
|General Availability

|====

[discrete]
=== Operator Technology Preview features

.Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Hybrid Helm Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Java-based Operator
|Not Available
|Technology Preview
|Technology Preview

|Multi-cluster Engine Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Not Available
|Not Available
|Technology Preview

|Network Observability Operator
|Not Available
|Not Available
|General Availability

|Platform Operators
|Not Available
|Technology Preview
|Technology Preview

|RukPak
|Not Available
|Not Available
|Technology Preview

|Cert-manager Operator
|Technology Preview
|General Availability
|General Availability

|====

[discrete]
=== Monitoring Technology Preview features

.Monitoring Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Alert routing for user-defined projects monitoring
|Technology Preview
|General Availability
|General Availability

|Alerting rules based on platform monitoring metrics
|Not Available
|Technology Preview
|Technology Preview

|Metrics Collection Profiles
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== {rh-openstack-first} Technology Preview features

.{rh-openstack} Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Support for {rh-openstack} DCN
|Technology Preview
|Technology Preview
|Technology Preview

|Support for external cloud providers for clusters on {rh-openstack}
|Technology Preview
|Technology Preview
|General Availability

|====

[discrete]
=== Architecture Technology Preview features

.Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Hosted control planes for {product-title} on bare metal
|Not Available
|Technology Preview
|Technology Preview

|Hosted control planes for {product-title} on Amazon Web Services (AWS)
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Machine management Technology Preview features

.Machine management Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Managing machines with the Cluster API
|Not Available
|Technology Preview
|Technology Preview

|Cron job time zones
|Not Available
|Not Available
|Technology Preview

|Cloud controller manager for Alibaba Cloud
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Amazon Web Services
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Google Cloud Platform
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Microsoft Azure
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for {rh-openstack-first}
|Technology Preview
|General Availability
|General Availability

|Cloud controller manager for VMware vSphere
|Technology Preview
|Technology Preview
|General Availability

|Cloud controller manager for Vsphere
|Not Available
|Not Available
|General Availability

|Cloud controller manager for Nutanix
|Not Available
|Not Available
|General Availability

|====

[discrete]
=== Authentication and authorization Technology Preview features

.Authentication and authorization Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Pod security admission restricted enforcement
|Not Available
|Not Available
|Technology Preview

|====

[id="ocp-4-13-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.8 and beyond! This needs some SME/QE review before being updated for 4.11. Need to check if KI should be removed or should stay.
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to {product-version}, you can either revoke or continue to allow unauthenticated access. Unless there is a specific need for unauthenticated access, you should revoke it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* Adding a Git Repository and configuring it with a GitLab and Bitbucket `pipeline-as-code` repository creates an invalid repository resource. As a result, the `spec.git_provider.url` Git provider URL is removed for GitLab and Bitbucket providers.
+
Workaround: Add the mandatory `spec.git_provider.user` field for Bitbucket. In addition, select either *Git access token* or *Git access token secret* to continue adding a Git Repository. (link:https://issues.redhat.com/browse/OCPBUGS-7036[*OCPBUGS-7036*])



[id="ocp-4-13-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster-within-minor.adoc#updating-cluster-within-minor[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-13-0-ga"]
=== RHSA-2022:xxxx - {product-title} 4.13.0 image release, bug fix, and security update advisory

Issued: 2023-TBD

{product-title} release 4.13.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2022:7399[RHSA-2022:7399] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2022:7398[RHSA-2022:7398] advisory.

Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.13.0 --pullspecs
----
//replace 4.y.z for the correct values for the release. You do not need to update oc to run this command.
