:_content-type: ASSEMBLY
[id="ocp-4-13-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-13-about-this-release"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-2022:7399[RHSA-2022:7399]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.26.md[Kubernetes 1.26] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. With the {cluster-manager-first} application for {product-title}, you can deploy OpenShift clusters to either on-premises or cloud environments.

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.4 and 8.5, as well as on {op-system-first} 4.13.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//TODO: Remove this for 4.13
Starting with {product-title} {product-version} an additional six months of Extended Update Support (EUS) phase on even numbered releases from 18 months to two years. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

//TODO: Add the line below for EUS releases.
{product-title} 4.8 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

//TODO: The line below should be used when it is next appropriate. Revisit in April 2023 timeframe.
Maintenance support ends for version 4.8 in January 2023 and goes to extended life phase. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].


[id="ocp-4-13-add-on-support-status"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-13-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-13-rhcos"]
=== {op-system-first}

[id="ocp-4-13-rhcos-rhel-9-2-packages"]
==== {op-system} now uses {op-system-base} 9.2

{op-system} now uses {op-system-base-full} 9.2 packages in {product-title} {product-version}. This enables you to have the latest fixes, features, and enhancements, as well as the latest hardware support and driver updates.

[id="ocp-4-13-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-13-installation-vsphere-8-support"]
==== Support for VMware vSphere version 8.0
{product-title} {product-version} supports VMware vSphere version 8.0. You can continue to install an {product-title} cluster on VMware vSphere version 7.0 Update 2.

[id="ocp-4-13-installation-vsphere-region-zone"]
==== VMware vSphere region and zone enablement
You can deploy an {product-title} cluster to multiple vSphere datacenters or regions that run in a single VMware vCenter. Each datacenter can run multiple clusters or zones. This configuration reduces the risk of a hardware failure or network outage causing your cluster to fail.

You can also specify multiple regions and zones on a cluster after installation. You must upgrade your existing cluster to {product-title} {product-version} before you configure a multiple region and zone environment.

[id="ocp-4-13-installation-vsphere-default-config-yaml"]
==== Changes to the default `install-config.yaml` file
After you run the installation program for {product-title} on vSphere, the default `install-config.yaml` file now includes `vcenters` and `failureDomains` fields, so that you can choose to specify multiple datacenters, region, and zone information for your cluster. You can leave these fields blank if you want to install an {product-title} cluster in a vSphere environment that consists of single datacenter running in a VMware vCenter.

[id="ocp-4-13-installation-and-upgrade-three-node"]
==== Three-node cluster support

Beginning with {product-title} {product-version}, deploying a three-node cluster is supported on Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), and VMware vSphere. This type of {product-title} cluster is a smaller, more resource efficient cluster, as it consists of only three control plane machines, which also act as compute machines.

For more information, see xref:../installing/installing_aws/installing-aws-three-node.adoc#installing-aws-three-node[Installing a three-node cluster on AWS], xref:../installing/installing_azure/installing-azure-three-node.adoc#installing-azure-three-node[Installing a three-node cluster on Azure], xref:../installing/installing_gcp/installing-gcp-three-node.adoc#installing-gcp-three-node[Installing a three-node cluster on GCP], and xref:../installing/installing_vsphere/installing-vsphere-three-node.adoc#installing-vsphere-three-node[Installing a three-node cluster on vSphere].

[id="ocp-4-13-installation-and-upgrade-ibm-cloud-vpc"]
==== IBM Cloud VPC and existing VPC resources
If you are deploying an {product-title} cluster to an existing virtual private cloud (VPC), you can now use the `networkResourceGroupName` parameter to specify the name of the resource group that contains these existing resources. This enhancement lets you keep the existing VPC resources and subnets separate from the cluster resources that the installation program provisions. You can then use the `resourceGroupName` parameter to specify the name of an existing resource group that the installation program can use to deploy all of the installer-provisioned cluster resources. If `resourceGroupName` is undefined, a new resource group is created for the cluster.

For more information, see xref:../installing/installing_ibm_cloud_public/installing-ibm-cloud-customizations.adoc#installation-configuration-parameters-additional-ibm-cloud_installing-ibm-cloud-customizations[Additional IBM Cloud VPC configuration parameters].

[id="ocp-4-13-installation-gcp-required-permissions"]
==== Minimum required permissions for GCP to install and delete an {product-title} cluster
In {product-title} {product-version}, instead of using the predefined roles, you can now define your custom roles to include the minimum required permissions for Google Cloud Platform (GCP) to install and delete an {product-title} cluster. These permissions are available for installer-provisioned infrastructure and user-provisioned infrastructure.

[id="ocp-4-13-post-installation"]
=== Post-installation configuration

[id="ocp-4-13-post-installation-vsphere-failure-domains"]
==== Specifying multiple failure domains for your cluster on VSphere
As an administrator, you can specify multiple failure domains for your {product-title} cluster that runs on a VMware VSphere instance. This means that you can distribute key control planes and workload elements among varied hardware resources for a datacenter. Additionally, you can configure your cluster to use a multiple layer 2 network configuration, so that data transfer among nodes can span across multiple networks.

[id="ocp-4-13-web-console"]
=== Web console

[id="ocp-4-13-oc"]
=== OpenShift CLI (oc)

[id="ocp-4-13-oc-must-gather-namespace"]
==== New flag added to run must-gather in a specified namespace

With {product-title} 4.13, the `--run-namespace` flag is now available for the `oc adm must-gather` command. You can use this flag to specify an existing namespace to run the must-gather tool in.

For more information, see xref:../support/gathering-cluster-data.adoc#about-must-gather_gathering-cluster-data[About the must-gather tool].

[id="ocp-4-13-import-image"]
==== Importing manifests with the OpenShift CLI (oc)

As of {product-title} 4.12, limited usability (Technology Preview) support for manifest listed images on image streams is available.

With {product-title} {product-version}, a new `oc` command line interface (CLI) flag, `--import-mode`, has been added to the following `oc` commands:

* `oc import-image`
* `oc tag`

With this enhancement, users can set the `--import-mode` flag to `Legacy` or `PreserveOriginal`, which provides users the option to import a single sub-manifest, or all manifests, of a manifest list when running the `oc import-image` or `oc tag` commands.

For more information, see xref:../openshift_images/image-streams-manage.adoc#images-imagestream-import-import-mode_image-streams-managing[Working with manifest lists].

[id="ocp-4-13-oc-describe-enhancement"]
==== Returning os/arch and digests of an image

With {product-title} {product-version}, running `oc describe` on an image now returns os/arch and digests of each manifest.

[id="ocp-4-13-ibm-z"]
=== IBM Z and LinuxONE

[id="ocp-4-13-images"]
=== Images

[id="ocp-4-13-networking"]
=== Networking

[id="ocp-4-13-networking-metrics"]
==== Enhancements to networking metrics
===== egress_ips_rebalance_total
* Metric name: `ovnkube_master_egress_ips_rebalance_total`
* *Help message:* `The total number of times assigned egress IP(s) needed to be moved to a different node.`

===== egress_ips_node_unreachable_total
* Metric name: `ovnkube_master_egress_ips_node_unreachable_total`
* *Help message*: `The total number of times assigned egress IP(s) were unreachable.`

===== egress_ips_unassign_latency_seconds
* Metric name: `ovnkube_master_egress_ips_unassign_latency_seconds`
* *Help message*: `The latency of egress IP unassignment from OVN northbound database.`

===== interfaces_total
* Metric name: `ovs_vswitchd_interfaces_total`
* *Help message*: `The total number of Open vSwitch interface(s) created for pods` and  `Open vSwitch interface until its available.`

===== interface_up_wait_seconds_total
* Metric name: `ovs_vswitchd_interface_up_wait_seconds_total`
* *Help message*: `The total number of seconds that is required to wait for pod.` and `Open vSwitch interface until its available.`

===== ovnkube_resource_retry_failures_total
* Metric name: `ovnkube_resource_retry_failures_total`
* *Help message*: `The total number of times processing a Kubernetes resource reached the maximum retry limit and was no longer processed.`

[id="ocp-4-13-networking-alerts"]
==== Enhancements to networking alerts
* OVN Kubernetes retries a claim up to 15 times before dropping it. With this update, if this failure happens, {product-title} alerts the cluster administrator. A description of each alert can be viewed in the console.

===== NoOvnMasterLeader
* Summary: There is no ovn-kubernetes master leader.
* Description in console:
[source,text]
----
Networking control plane is degraded. Networking configuration updates applied to the cluster will not be implemented while there is no OVN Kubernetes leader. Existing workloads should continue to have connectivity. OVN-Kubernetes control plane is not functional.
----

===== OVNKubernetesNodeOVSOverflowUserspace
* Summary: OVS vSwitch daemon drops packets due to buffer overflow.
* Description in console:
[source,text]
----
Netlink messages dropped by OVS vSwitch daemon due to netlink socket buffer overflow. This will result in packet loss.
----

===== OVNKubernetesNodeOVSOverflowKernel
* Summary: OVS kernel module drops packets due to buffer overflow.
* Description in console:
[source,text]
----
Netlink messages dropped by OVS kernel module due to netlink socket buffer overflow. This will result in packet loss.
----

[id="ocp-4-13-networking-haproxy-update"]
==== Update to HAProxy 2.6
{product-title} updated to HAProxy 2.6.

[id="ocp-4-13-nw-metallb-ipaddresspool-assignment"]
==== Assign IP addresses in MetalLB IPAddressPool resources to specific namespaces and services
With this update, you can assign IP addresses from a MetalLB `IPAddressPool` resource to services, namespaces, or both. This is useful in a muti-tenant, bare-metal environment that requires MetalLB to pin IP addresses from an IP address pool to specific services and namespaces. You can assign IP addresses from many IP address pools to services and namespaces. You can then define the prioritization for these IP address pools so that MetalLB assigns IP addresses starting from the higher priority IP address pool.

For more information about assigning IP addresses from an IP address pool to services and namespaces, see xref:../networking/metallb/metallb-configure-address-pools.adoc#nw-metallb-configure-address-pool_configure-metallb-address-pools[Configuring MetalLB address pools].

[id="ocp-4-13-storage"]
=== Storage

[id="ocp-4-13-olm"]
=== Operator lifecycle

[id="ocp-4-13-osdk"]
=== Operator development

[id="ocp-4-13-machine-api"]
=== Machine API

[id="ocp-4-13-mapi-cpms-platform-support"]
==== Additional platform support for control plane machine sets

* With this release, control plane machine sets are supported for Google Cloud Platform clusters.

* This release includes an enhancement to the user experience for the control plane machine set on Microsoft Azure clusters. For Azure clusters that are installed with or upgraded to {product-title} version 4.13, you are no longer required to create a control plane machine set custom resource (CR).
+
--
* Clusters that are installed with version 4.13 have a control plane machine set that is active by default.

* For clusters that are upgraded to version 4.13, an inactive CR is generated for the cluster and can be activated after you verify that the values in the CR are correct for your control plane machines.
--

For more information, see xref:../machine_management/control_plane_machine_management/cpmso-getting-started.adoc#cpmso-getting-started[Getting started with the Control Plane Machine Set Operator].

[id="ocp-4-13-machine-config-operator"]
=== Machine Config Operator

[id="ocp-4-13-machine-config-operator-layering"]
==== Support for adding third party and custom content to {op-system}

You can now use {op-system-first} image layering to add {op-system-base-full} and third-party packages to cluster nodes.

For more information, see  xref:../post_installation_configuration/coreos-layering.adoc#coreos-layering[{op-system-first} image layering].

[id="ocp-4-13-machine-config-operator-core"]
==== Support for setting the `core` user password

You can now create a password for the {op-system} `core` user. In the event that you cannot use SSH or the `oc debug node` command to access a node, this password allows you to use the `core` user to access the node through a cloud provider serial console or a bare metal baseboard controller manager (BMC).

For more information, see xref:../post_installation_configuration/machine-configuration-tasks.adoc#core-user-password_post-install-machine-configuration-tasks[Changing the core user password for node access].

[id="ocp-4-13-nodes"]
=== Nodes

[id="ocp-4-13-nodes-mirror"]
==== Image registry repository mirroring by tags

You can now pull images from a mirrored registry by using image tags in addition to digest specifications. To accomplish this change, the `ImageContentSourcePolicy` (ICSP) object is deprecated. You can now use an `ImageDigestMirrorSet` (IDMS) object to pull images by using digest specifications or an `ImageTagMirrorSet` (ITMS) object to pull images by using image tags.

If you have existing YAML files that you used to create ICSP objects, you can use the `oc adm migrate icsp` command to convert those files to an IDMS YAML file.

For more information on these new objects, see xref:../post_installation_configuration/preparing-for-users.adoc#images-configuration-registry-mirror_post-install-preparing-for-users[Configuring image registry repository mirroring].

For more information on converting existing ICSP YAML files to IDMS YAML files, see xref:../post_installation_configuration/preparing-for-users.adoc#images-configuration-registry-mirror-convert_post-install-preparing-for-users[Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring].

[id="ocp-4-13-nodes-crun-ga"]
==== crun general availability

The crun low-level container runtime is now generally available in {product-title} 4.13. There is no new functionality in the GA version.

[id="ocp-4-13-nodes-cgroup-v2-ga"]
==== Linux Control Group version 2 (cgroup v2) general availability

Linux Control Group version 2 (cgroup v2) is now generally available in {product-title} 4.13. There is no new functionality in the GA version.

[id="ocp-4-13-monitoring"]
=== Monitoring

[id="ocp-4-13-scalability-and-performance"]
=== Scalability and performance

[id="ocp-4-13-NUMA-scheduler-ga"]
==== NUMA-aware scheduling with the NUMA Resources Operator is generally available

NUMA-aware scheduling with the NUMA Resources Operator was previously introduced as a Technology Preview in {product-title} 4.10 and is now generally available in {product-title} {product-version}.

The NUMA Resources Operator deploys a NUMA-aware secondary scheduler that makes scheduling decisions for workloads based on a complete picture of available NUMA zones in clusters. This enhanced NUMA-aware scheduling ensures that latency-sensitive workloads are processed in a single NUMA zone for maximum efficiency and performance.

This update adds the following features:

* Fine-tuning of API polling for NUMA resource reports.
* Configuration options at the node group level for the node topology exporter.

For more information, see xref:../scalability_and_performance/cnf-numa-aware-scheduling.adoc#cnf-about-numa-aware-scheduling_numa-aware[Scheduling NUMA-aware workloads].

[id="ocp-4-13-insights-operator"]
=== Insights Operator

[id="ocp-4-13-auth"]
=== Authentication and authorization

[id="ocp-4-13-hcp"]
=== Hosted control planes (Technology Preview)

[id="ocp-4-13-rhv"]
=== Red Hat Virtualization (RHV)

[id="install-sno-requirements-for-installing-on-a-single-node"]
=== Requirements for installing OpenShift on a single node

{product-version} now supports `x86_64` and `arm64` CPU architectures.

[id="ocp-4-13-notable-technical-changes"]
== Notable technical changes

{product-title} {product-version} introduces the following notable technical changes.

// Note: use [discrete] for these sub-headings.

[discrete]
[id="ocp-4-13-cluster-cloud-controller-manager-operator"]
==== Cloud controller managers for additional cloud providers

The Kubernetes community plans to deprecate the use of the Kubernetes controller manager to interact with underlying cloud platforms in favor of using cloud controller managers. As a result, there is no plan to add Kubernetes controller manager support for any new cloud platforms.

The Nutanix implementation that is added in this release of {product-title} uses cloud controller managers. In addition, this release introduces the General Availability of using cloud controller managers for VMware vSphere.

To learn more about the cloud controller manager, see the link:https://kubernetes.io/docs/concepts/architecture/cloud-controller/[Kubernetes Cloud Controller Manager documentation].

To manage the cloud controller manager and cloud node manager deployments and lifecycles, use the Cluster Cloud Controller Manager Operator.

For more information, see the xref:../operators/operator-reference.adoc#cluster-cloud-controller-manager-operator_cluster-operators-ref[Cluster Cloud Controller Manager Operator] entry in the _Platform Operators reference_.

[id="ocp-4-13-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* _General Availability_
* _Deprecated_
* _Removed_

[discrete]
=== Operator deprecated and removed features

.Operator deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|SQLite database format for Operator catalogs
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
=== Images deprecated and removed features

.Images deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|`ImageChangesInProgress` condition for Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated

|`MigrationInProgress` condition for Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
=== Monitoring deprecated and removed features

.Monitoring deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|====

[discrete]
=== Installation deprecated and removed features

.Installation deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|vSphere 7.0 Update 1 or earlier
|General Availability
|Deprecated
|Deprecated

|VMware ESXi 7.0 Update 1 or earlier
|General Availability
|Deprecated
|Deprecated

|CoreDNS wildcard queries for the `cluster.local` domain
|General Availability
|General Availability
|Deprecated

|`ingressVIP` and `apiVIP` settings in the `install-config.yaml` file for installer-provisioned infrastructure clusters
|General Availability
|General Availability
|Deprecated

|====

[discrete]
=== Updating clusters deprecated and removed features

.Updating clusters deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|====

[discrete]
=== Storage deprecated and removed features

.Storage deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Persistent storage using FlexVolume
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
=== Authentication and authorization deprecated and removed features

.Authentication and authorization deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|====

[discrete]
=== Specialized hardware and driver enablement deprecated and removed features

.Specialized hardware and driver enablement deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Special Resource Operator (SRO)
|Technology Preview
|Technology Preview
|Removed

|====

[discrete]
=== Multi-architecture deprecated and removed features

.Multi-architecture deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|IBM POWER8 all models (`ppc64le`)
|General Availability
|General Availability
|Deprecated

|IBM IBM POWER9 AC922 (`ppc64le`)
|General Availability
|General Availability
|Deprecated

|IBM IBM POWER9 IC922 (`ppc64le`)
|General Availability
|General Availability
|Deprecated

|IBM IBM POWER9 LC922 (`ppc64le`)
|General Availability
|General Availability
|Deprecated

|IBM z13 all models (`s390x`)
|General Availability
|General Availability
|Deprecated

|IBM LinuxONE Emperor (`s390x`)
|General Availability
|General Availability
|Deprecated

|IBM LinuxONE Rockhopper (`s390x`)
|General Availability
|General Availability
|Deprecated

|AMD64 (x86_64) v1 CPU
|General Availability
|General Availability
|Deprecated

|====

[discrete]
=== Networking deprecated and removed features

.Networking deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Kuryr on {rh-openstack}
|General Availability
|General Availability
|Deprecated

|====

[discrete]
=== Node deprecated and removed features

.Node deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|`ImageContentSourcePolicy` (ICSP) objects
|General Availability
|General Availability
|Deprecated

|====

[id="ocp-4-13-deprecated-features"]
=== Deprecated features

[id="ocp-4-13-rhv-deprecations"]
==== Red Hat Virtualization (RHV) as a host platform for {product-title} will be deprecated

Red Hat Virtualization (RHV) will be deprecated in an upcoming release of {product-title}. Support for {product-title} on RHV will be removed from a future {product-title} release, currently planned as {product-title} 4.14.

[id="ocp-4-13-ne-deprecations"]
==== Wildcard DNS queries for the `cluster.local` domain are deprecated

CoreDNS will stop supporting wildcard DNS queries for names under the `cluster.local` domain. These queries will resolve in {product-title} {product-version} as they do in earlier versions, but support will be removed from a future {product-title} release.

[id="ocp-4-13-ne-kuryr"]
==== Kuryr support for clusters that run on {rh-openstack}

In {product-title} 4.12, support for Kuryr on clusters that run on {rh-openstack} is deprecated. Support will be removed no earlier than {product-title} 4.14.

[id="ocp-4-13-icsp"]
==== `ImageContentSourcePolicy` objects

The `ImageContentSourcePolicy` (ICSP) object is now deprecated. You can now use an `ImageDigestMirrorSet` (IDMS) object to pull images by using digest specifications or an `ImageTagMirrorSet` (ITMS) object to pull images by using image tags.

For more information on these new objects, see xref:../post_installation_configuration/preparing-for-users.adoc#images-configuration-registry-mirror_post-install-preparing-for-users[Configuring image registry repository mirroring].

For more information on converting existing ICSP YAML files to IDMS YAML files, see xref:../post_installation_configuration/preparing-for-users.adoc#images-configuration-registry-mirror-convert_post-install-preparing-for-users[Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring].

[id="ocp-4-13-removed-features"]
=== Removed features

[id="ocp-4-13-bug-fixes"]
== Bug fixes

[discrete]
[id="ocp-4-13-api-auth-bug-fixes"]
==== API Server and Authentication

//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / TALO
//Telco Edge / ZTP

[discrete]
[id="ocp-4-13-bare-metal-hardware-bug-fixes"]
==== Bare Metal Hardware Provisioning

[discrete]
[id="ocp-4-13-builds-bug-fixes"]
==== Builds

[discrete]
[id="ocp-4-13-cloud-compute-bug-fixes"]
==== Cloud Compute

* For some configurations of Google Cloud Platform clusters, the internal load balancer uses instance groups that are created by the installation program. Previously, when a control plane machine was replaced manually, the new control plane node was not assigned to a control plane instance group. This prevented the node from being reachable via the internal load balancer. To resolve the issue, administrators had to manually move the control plane machine to the correct instance group by using the Google Cloud console.
+
With this release, replacement control plane nodes are assigned to the correct instance group.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1970464[*BZ#1970464*], link:https://issues.redhat.com/browse/OCPCLOUD-1562[*OCPCLOUD-1562*])

[discrete]
[id="ocp-4-13-dev-console-bug-fixes"]
==== Developer Console

[discrete]
[id="ocp-4-13-image-registry-bug-fixes"]
==== Image Registry

[discrete]
[id="ocp-4-13-installer-bug-fixes"]
==== Installer

[discrete]
[id="ocp-4-13-kube-controller-bug-fixes"]
==== Kubernetes Controller Manager

[discrete]
[id="ocp-4-13-kube-scheduler-bug-fixes"]
==== Kubernetes Scheduler

[discrete]
[id="ocp-4-13-machine-config-operator-bug-fixes"]
==== Machine Config Operator

[discrete]
[id="ocp-4-13-management-console-bug-fixes"]
==== Management Console

[discrete]
[id="ocp-4-13-monitoring-bug-fixes"]
==== Monitoring

[discrete]
[id="ocp-4-13-networking-bug-fixes"]
==== Networking

[discrete]
[id="ocp-4-13-node-bug-fixes"]
==== Node

[discrete]
[id="ocp-4-13-openshift-cli-bug-fixes"]
==== OpenShift CLI (oc)

[discrete]
[id="ocp-4-13-olm-bug-fixes"]
==== Operator Lifecycle Manager (OLM)

[discrete]
[id="ocp-4-13-openshift-operator-sdk-bug-fixes"]
==== Operator SDK

[discrete]
[id="ocp-4-13-file-integrity-operator-bug-fixes"]
==== File Integrity Operator

[discrete]
[id="ocp-4-13-compliance-operator-bug-fixes"]
==== Compliance Operator

[discrete]
[id="ocp-4-13-openshift-api-server-bug-fixes"]
==== OpenShift API server

[discrete]
[id="ocp-4-13-rhcos-bug-fixes"]
==== {op-system-first}

[discrete]
[id="ocp-4-13-scalability-and-performance-bug-fixes"]
==== Scalability and performance

[discrete]
[id="ocp-4-13-storage-bug-fixes"]
==== Storage

[id="ocp-4-13-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the following tables, features are marked with the following statuses:

* _Technology Preview_
* _General Availability_
* _Not Available_
* _Deprecated_

[discrete]
=== Networking Technology Preview features

.Networking Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|PTP single NIC hardware configured as boundary clock
|Technology Preview
|General Availability
|General Availability

|PTP dual NIC hardware configured as boundary clock
|Not Available
|Technology Preview
|Technology Preview

|PTP events with boundary clock
|Technology Preview
|General Availability
|General Availability

|Pod-level bonding for secondary networks
|Technology Preview
|General Availability
|General Availability

|External DNS Operator
|Technology Preview
|General Availability
|General Availability

|AWS Load Balancer Operator
|Not Available
|Technology Preview
|Technology Preview

|Ingress Node Firewall Operator
|Not Available
|Not Available
|Technology Preview

|Advertise using BGP mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Not Available
|Technology Preview
|General Availability

|Advertise using L2 mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Not Available
|Technology Preview
|Technology Preview

|Multi-network policies for SR-IOV networks
|Not Available
|Not Available
|Technology Preview

|Updating the interface-specific safe sysctls list
|Not Available
|Not Available
|Technology Preview

|MT2892 Family [ConnectX-6 Dx] SR-IOV support
|Not Available
|Not Available
|Technology Preview

|MT2894 Family [ConnectX-6 Lx] SR-IOV support
|Not Available
|Not Available
|Technology Preview

|MT42822 BlueField-2 in ConnectX-6 NIC mode SR-IOV support
|Not Available
|Not Available
|Technology Preview

|Silicom STS Family SR-IOV support
|Not Available
|Not Available
|Technology Preview

|MT2892 Family [ConnectX-6 Dx] OvS Hardware Offload support
|Not Available
|Not Available
|Technology Preview

|MT2894 Family [ConnectX-6 Lx] OvS Hardware Offload support
|Not Available
|Not Available
|Technology Preview

|MT42822 BlueField-2 in ConnectX-6 NIC mode OvS Hardware Offload support
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Storage Technology Preview features

.Storage Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Shared Resources CSI Driver and Build CSI Volumes in OpenShift Builds
|Technology Preview
|Technology Preview
|Technology Preview

|CSI volume expansion
|Technology Preview
|General Availability
|General Availability

|CSI Azure File Driver Operator
|Technology Preview
|General Availability
|General Availability

|CSI Google Filestore Driver Operator
|Not Available
|Not Available
|Technology Preview

|CSI automatic migration
(Azure file, VMware vSphere)
|Technology Preview
|Technology Preview
|Technology Preview

|CSI automatic migration
(Azure Disk, OpenStack Cinder)
|Technology Preview
|General Availability
|General Availability

|CSI automatic migration
(AWS EBS, GCP disk)
|Technology Preview
|Technology Preview
|General Availability

|CSI inline ephemeral volumes
|Technology Preview
|Technology Preview
|Technology Preview

|CSI generic ephemeral volumes
|Not Available
|General Availability
|General Availability

|Shared Resource CSI Driver
|Technology Preview
|Technology Preview
|Technology Preview

|CSI Google Filestore Driver Operator
|Not Available
|Not Available
|Technology Preview

|Automatic device discovery and provisioning with Local Storage Operator
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Installation Technology Preview features

.Installation Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Adding kernel modules to nodes with kvc
|Technology Preview
|Technology Preview
|Technology Preview

|IBM Cloud VPC clusters
|Technology Preview
|Technology Preview
|General Availability

|Selectable Cluster Inventory
|Technology Preview
|Technology Preview
|Technology Preview

|Multi-architecture compute machines
|Not Available
|Technology Preview
|General Availability

|Mount shared entitlements in BuildConfigs in RHEL
|Technology Preview
|Technology Preview
|Technology Preview

|Agent-based {product-title} Installer
|Not Available
|Not Available
|General Availability

|====

[discrete]
=== Node Technology Preview features

.Nodes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Non-preempting priority classes
|Technology Preview
|Technology Preview
|Technology Preview

|Linux Control Group version 2 (cgroup v2)
|Developer Preview
|Technology Preview
|General Availability

|crun container runtime
|Not Available
|Technology Preview
|General Availability

|====

[discrete]
=== Multi-Architecture Technology Preview features

.Multi-Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|`kdump` on `arm64` architecture
|Not Available
|Technology Preview
|Technology Preview

|`kdump` on `s390x` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `ppc64le` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|IBM Secure Execution on {ibmzProductName} and LinuxONE
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Serverless Technology Preview features

.Serverless Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Serverless functions
|General Availability
|General Availability
|General Availability

|====

[discrete]
=== Specialized hardware and driver enablement Technology Preview features

.Specialized hardware and driver enablement Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Driver Toolkit
|Technology Preview
|Technology Preview
|General Availability

|Special Resource Operator (SRO)
|Technology Preview
|Technology Preview
|Not Available

|Hub and spoke cluster support
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Web console Technology Preview features

.Web console Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Multicluster console
|Technology Preview
|Technology Preview
|Technology Preview

|Dynamic Plug-ins
|Technology Preview
|Technology Preview
|General Availability

|====

[discrete]
=== Scalability and performance Technology Preview features

.Scalability and performance Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Hyperthreading-aware CPU manager policy
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Not Available
|Technology Preview
|Technology Preview

|{factory-prestaging-tool}
|Not Available
|Not Available
|Technology Preview

|Single-node OpenShift cluster expansion with worker nodes
|Not Available
|Not Available
|Technology Preview

|{cgu-operator-first}
|Technology Preview
|Technology Preview
|General Availability

|Mount namespace encapsulation
|Not Available
|Not Available
|Technology Preview

|NUMA-aware scheduling with NUMA Resources Operator
|Technology Preview
|Technology Preview
|General Availability

|====

[discrete]
=== Operator Technology Preview features

.Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Hybrid Helm Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Java-based Operator
|Not Available
|Technology Preview
|Technology Preview

|Multi-cluster Engine Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Not Available
|Not Available
|Technology Preview

|Network Observability Operator
|Not Available
|Not Available
|General Availability

|Platform Operators
|Not Available
|Technology Preview
|Technology Preview

|RukPak
|Not Available
|Not Available
|Technology Preview

|Cert-manager Operator
|Technology Preview
|General Availability
|General Availability

|====

[discrete]
=== Monitoring Technology Preview features

.Monitoring Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Alert routing for user-defined projects monitoring
|Technology Preview
|General Availability
|General Availability

|Alerting rules based on platform monitoring metrics
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== {rh-openstack-first} Technology Preview features

.{rh-openstack} Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Support for {rh-openstack} DCN
|Technology Preview
|Technology Preview
|Technology Preview

|Support for external cloud providers for clusters on {rh-openstack}
|Technology Preview
|Technology Preview
|General Availability

|====

[discrete]
=== Architecture Technology Preview features

.Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Hosted control planes for {product-title} on bare metal
|Not Available
|Technology Preview
|Technology Preview

|Hosted control planes for {product-title} on Amazon Web Services (AWS)
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Machine management Technology Preview features

.Machine management Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Managing machines with the Cluster API
|Not Available
|Technology Preview
|Technology Preview

|Cron job time zones
|Not Available
|Not Available
|Technology Preview

|Cloud controller manager for Alibaba Cloud
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Amazon Web Services
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Google Cloud Platform
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Microsoft Azure
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for {rh-openstack-first}
|Technology Preview
|General Availability
|General Availability

|Cloud controller manager for VMware vSphere
|Technology Preview
|Technology Preview
|General Availability

|====

[id="ocp-4-13-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.8 and beyond! This needs some SME/QE review before being updated for 4.11. Need to check if KI should be removed or should stay.
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to {product-version}, you can either revoke or continue to allow unauthenticated access. Unless there is a specific need for unauthenticated access, you should revoke it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

[id="ocp-4-13-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster-within-minor.adoc#updating-cluster-within-minor[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-13-0-ga"]
=== RHSA-2022:xxxx - {product-title} 4.13.0 image release, bug fix, and security update advisory

Issued: 2023-TBD

{product-title} release 4.13.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2022:7399[RHSA-2022:7399] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2022:7398[RHSA-2022:7398] advisory.

Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.13.0 --pullspecs
----
//replace 4.y.z for the correct values for the release. You do not need to update oc to run this command.
