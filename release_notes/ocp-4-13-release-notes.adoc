:_content-type: ASSEMBLY
[id="ocp-4-13-release-notes"]
= {product-title} {product-version} release notes
include::_attributes/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-13-about-this-release"]
== About this release

// TODO: Update with the relevant information closer to release.
{product-title} (link:https://access.redhat.com/errata/RHSA-2023:1326[RHSA-2023:1326]) is now available. This release uses link:https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.26.md[Kubernetes 1.26] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. With the {cluster-manager-first} application for {product-title}, you can deploy {product-title} clusters to either on-premises or cloud environments.

{product-title} {product-version} is based on {op-system-base-full} 9.2. {op-system-base} 9.2 has not yet been submitted for FIPS validation. Red Hat expects, though cannot commit to a specific timeframe, to obtain FIPS validation for {op-system-base} 9.0 and {op-system-base} 9.2 modules, and later even minor releases of {op-system-base} 9.x. Updates will be available in link:https://access.redhat.com/articles/2918071[Compliance Activities and Government Standards].

// Double check OP system versions
{product-title} {product-version} is supported on {op-system-base-full} 8.6, 8.7, and 8.8 as well as on {op-system-first} 4.13.

You must use {op-system} machines for the control plane, and you can use either {op-system} or {op-system-base} for compute machines.
//Removed the note per https://issues.redhat.com/browse/GRPA-3517

//TODO: Remove this for 4.13
//Starting with {product-title} {product-version} an additional six months of Extended Update Support (EUS) phase on even numbered releases from 18 months to two years. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

//TODO: Add the line below for EUS releases.
//{product-title} 4.8 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

//TODO: The line below should be used when it is next appropriate. Revisit in April 2023 timeframe.
//Maintenance support ends for version 4.8 in January 2023 and goes to extended life phase. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].


[id="ocp-4-13-add-on-support-status"]
== {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat {product-title} Life Cycle Policy].

[id="ocp-4-13-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-13-rhcos"]
=== {op-system-first}

[id="ocp-4-13-rhcos-rhel-9-2-packages"]
==== {op-system} now uses {op-system-base} 9.2

{op-system} now uses {op-system-base-full} 9.2 packages in {product-title} {product-version}. This enables you to have the latest fixes, features, and enhancements, as well as the latest hardware support and driver updates.

[id="ocp-4-13-rhel-9-considerations"]
===== Considerations for upgrading to {product-title} with {op-system-base} 9.2

With this release, {product-title} {product-version} introduces a {op-system-base} 9.2 based {op-system} and there are some considerations you must make before upgrading:

* Some component configuration options and services might have changed between {op-system-base} 8.6 and {op-system-base} 9.2, which means existing machine configuration files might no longer be valid.

* {op-system-base} 6 base image containers are not supported on {op-system} container hosts, but are supported on {op-system-base} 8 worker nodes. For more information, see the link:https://access.redhat.com/support/policy/rhel-container-compatibility[Red Hat Container Compatibility] matrix.

* Some device drivers have been deprecated, see the link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/considerations_in_adopting_rhel_9/assembly_hardware-enablement_considerations-in-adopting-rhel-9#unmaintained-hardware-support[{op-system-base} documentation] for more information.

[id="ocp-4-13-ipi-powervs"]
==== {ibmpowerProductName} Virtual Server using installer-provisioned infrastructure (Technology Preview)

Installer-provisioned Infrastructure (IPI) provides a full-stack installation and setup of {product-title}.

For more information, see xref:../installing/installing_ibm_powervs/preparing-to-install-on-ibm-power-vs.adoc#preparing-to-install-on-ibm-power-vs[Preparing to install on {ibmpowerProductName} Virtual Server].

[id="ocp-4-13-secure-execution-z-linux-one"]
==== IBM Secure Execution on {ibmzProductName} and {linuxoneProductName}

This feature was introduced as a Technology Preview in
{product-title} 4.12 and is now generally available in
{product-title} 4.13.
 IBM Secure Execution is a hardware enhancement that protects memory boundaries for KVM guests. IBM Secure Execution provides the highest level of isolation and security for cluster workloads, and you can enable it by using an IBM Secure Execution-ready QCOW2 boot image.

To use IBM Secure Execution, you must have host keys for your host machine(s) and they must be specified in your Ignition configuration file. IBM Secure Execution automatically encrypts your boot volumes using LUKS encryption.

For more information, see xref:../installing/installing_ibm_z/installing-ibm-z-kvm.adoc#installing-rhcos-using-ibm-secure-execution_installing-ibm-z-kvm[Installing {op-system} using IBM Secure Execution].

[id="ocp-4-13-assisted-installer-p-z-linuxone"]
==== {ai-full} SaaS provides platform integration support for {ibmpowerProductName}, {ibmzProductName}, and {linuxoneProductName}

{ai-full} SaaS on link:https://console.redhat.com[console.redhat.com] supports installation of {product-title} on the {ibmpowerProductName}, {ibmzProductName}, and {linuxoneProductName} platforms using either the {ai-full} user interface or the REST API. Integration enables users to manage their infrastructure from a single interface. There are a few additional installation steps to enable {ibmpowerProductName}, {ibmzProductName}, and {linuxoneProductName} integration with Assisted Installer SaaS.

For more information, see xref:../installing/installing_on_prem_assisted/installing-on-prem-assisted.adoc#installing-on-prem-assisted[Installing an on-premise cluster using the {ai-full}].

[id="ocp-4-13-lsof-rhcos"]
==== {op-system} now includes lsof
{product-title} {product-version} now includes the `lsof` command in {op-system}.

[id="ocp-4-13-installation-and-update"]
=== Installation and update

[id="ocp-4-13-installation-vsphere-8-support"]
==== Support for VMware vSphere version 8.0
{product-title} {product-version} supports VMware vSphere version 8.0. You can continue to install an {product-title} cluster on VMware vSphere version 7.0 Update 2.

[id="ocp-4-13-installation-vsphere-region-zone"]
==== VMware vSphere region and zone enablement
You can deploy an {product-title} cluster to multiple vSphere datacenters or regions that run in a single VMware vCenter. Each datacenter can run multiple clusters or zones. This configuration reduces the risk of a hardware failure or network outage causing your cluster to fail.

[IMPORTANT]
====
The VMware vSphere region and zone enablement feature is only available with a newly installed cluster, because this feature requires the vSphere Container Storage Interface (CSI) driver as the default storage driver in the cluster.

A cluster that was upgraded from a previous release defaults to using the in-tree vSphere driver. As a result, you must enable CSI automatic migration for the cluster to use this feature. You can then configure multiple regions and zones for the upgraded cluster.
====

For more information, see xref:../installing/installing_vsphere/installing-vsphere-installer-provisioned-customizations.adoc#installation-vsphere-regions-zones_installing-vsphere-installer-provisioned-customizations[VMware vSphere region and zone enablement].

[id="ocp-4-13-installation-vsphere-default-config-yaml"]
==== Changes to the default vSphere install-config.yaml file
After you run the installation program for {product-title} on vSphere, the default `install-config.yaml` file now includes `vcenters` and `failureDomains` fields, so that you can choose to specify multiple datacenters, region, and zone information for your cluster. You can leave these fields blank if you want to install an {product-title} cluster in a vSphere environment that consists of single datacenter running in a VMware vCenter.

For more information, see xref:../installing/installing_vsphere/installing-vsphere-installer-provisioned-customizations.adoc#configuring-vsphere-regions-zones_installing-vsphere-installer-provisioned-customizations[Configuring regions and zones for a VMware vCenter].

[id="ocp-4-13-installation-vsphere-external-lb-multisubnets"]
==== External load balancers that support multiple vSphere subnets

You can configure an {product-title} cluster to use an external load balancer that supports multiple subnets. If you use multiple subnets, you can explicitly list all the IP addresses in any networks that are used by your load balancer targets. This configuration can reduce maintenance overhead because you can create and destroy nodes within those networks without reconfiguring the load balancer targets.

For more information, see xref:../installing/installing_vsphere/installing-vsphere-installer-provisioned.adoc#nw-osp-configuring-external-load-balancer_installing-vsphere-installer-provisioned[Configuring an external load balancer].

[id="ocp-4-13-installation-vsphere-encrypt-vm"]
==== Support for encrypting a VM before installing a cluster on VMware vSphere

For {product-title} {product-version}, you can encrypt your virtual machines before you install a cluster on VMware vSphere with user-provisioned infrastructure.

For more information, see xref:../installing/installing_vsphere/installing-vsphere.adoc#installation-vsphere-encrypted-vms_installing-vsphere[Requirements for encrypting virtual machines]

[id="ocp-4-13-installation-and-upgrade-three-node"]
==== Three-node cluster support

Beginning with {product-title} {product-version}, deploying a three-node cluster is supported on Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), and VMware vSphere. This type of {product-title} cluster is a smaller, more resource efficient cluster, as it consists of only three control plane machines, which also act as compute machines.

For more information, see xref:../installing/installing_aws/installing-aws-three-node.adoc#installing-aws-three-node[Installing a three-node cluster on AWS], xref:../installing/installing_azure/installing-azure-three-node.adoc#installing-azure-three-node[Installing a three-node cluster on Azure], xref:../installing/installing_gcp/installing-gcp-three-node.adoc#installing-gcp-three-node[Installing a three-node cluster on GCP], and xref:../installing/installing_vsphere/installing-vsphere-three-node.adoc#installing-vsphere-three-node[Installing a three-node cluster on vSphere].

[id="ocp-4-13-installation-and-upgrade-ibm-cloud-vpc"]
==== IBM Cloud VPC and existing VPC resources
If you are deploying an {product-title} cluster to an existing virtual private cloud (VPC), you can now use the `networkResourceGroupName` parameter to specify the name of the resource group that contains these existing resources. This enhancement lets you keep the existing VPC resources and subnets separate from the cluster resources that the installation program provisions. You can then use the `resourceGroupName` parameter to specify the name of an existing resource group that the installation program can use to deploy all of the installer-provisioned cluster resources. If `resourceGroupName` is undefined, a new resource group is created for the cluster.

For more information, see xref:../installing/installing_ibm_cloud_public/installing-ibm-cloud-customizations.adoc#installation-configuration-parameters-additional-ibm-cloud_installing-ibm-cloud-customizations[Additional IBM Cloud VPC configuration parameters].

[id="ocp-4-13-installation-gcp-required-permissions"]
==== Minimum required permissions for GCP to install and delete an {product-title} cluster
In {product-title} {product-version}, instead of using the predefined roles, you can now define your custom roles to include the minimum required permissions for Google Cloud Platform (GCP) to install and delete an {product-title} cluster. These permissions are available for installer-provisioned infrastructure and user-provisioned infrastructure.

[id="ocp-4-13-user-defined-tags-azure"]
==== User-defined tags for Azure
In {product-title} {product-version}, you can configure the tags in Azure for grouping resources and for managing resource access and cost. Support for tags is available only for the resources created in the Azure Public Cloud, and in {product-title} {product-version} as a Technology Preview (TP). You can define the tags on the Azure resources in the `install-config.yaml` file only during {product-title} cluster creation.

[id="ocp-4-13-gcp-shared-vpc"]
==== Installing an {product-title} cluster on GCP into a shared Virtual Private Cloud (VPC)
In {product-title} {product-version}, you can install a cluster into a shared Virtual Private Cloud (VPC) on Google Cloud Platform (GCP). This installation method configures the cluster to share a VPC with another GCP project. A shared VPC enables an organization to connect resources from multiple projects over a common VPC network. A common VPC network can increase the security and efficiency of organizational communications by using internal IP addresses.

For more information, see xref:../installing/installing_gcp/installing-gcp-shared-vpc.adoc#installing-gcp-shared-vpc[Installing a cluster on GCP into a shared VPC].

[id="ocp-4-13-installation-gcp-shielded-vms"]
==== Installing a cluster on GCP using Shielded VMs
In {product-title} {product-version}, you can use Shielded VMs when installing your cluster. Shielded VMs have extra security features including secure boot, firmware and integrity monitoring, and rootkit detection. For more information, see xref:../installing/installing_gcp/installing-gcp-customizations.adoc#installation-gcp-enabling-shielded-vms_installing-gcp-customizations[Enabling Shielded VMs] and Google's documentation on link:https://cloud.google.com/shielded-vm[Shielded VMs].

[id="ocp-4-13-installation-gcp-confidential-vms"]
==== Installing a cluster on GCP using Confidential VMs
In {product-title} {product-version}, you can use Confidential VMs when installing your cluster. Confidential VMs encrypt data while it is being processed. For more information, see Google's documentation about link:https://cloud.google.com/confidential-computing[Confidential Computing]. You can enable Confidential VMs and Shielded VMs at the same time, although they are not dependent on each other.

[IMPORTANT]
====
Due to a known issue, you cannot use persistent volume storage on a cluster with Confidential VMs. For more information, see link:https://issues.redhat.com/browse/OCPBUGS-7582[OCPBUGS-7582].
====

[id="ocp-4-13-installation-aws-local-zones"]
==== Installing a cluster on AWS into an existing Virtual Private Cloud (VPC) improvements

In {product-title} {product-version}, the installation process for clusters that use AWS VPCs is simplified. This release also introduces the _edge pool_, a pool of machines that are optimized for AWS Local Zones.

For more information, see xref:../installing/installing_aws/installing-aws-localzone.adoc#installing-aws-localzone[Installing a cluster using AWS Local Zones].

[id="ocp-4-13-admin-ack-upgrading"]
==== Required administrator acknowledgment when upgrading from {product-title} 4.12 to 4.13

{product-title} 4.13 uses Kubernetes 1.26, which removed xref:../release_notes/ocp-4-13-release-notes.adoc#ocp-4-13-removed-kube-1-26-apis[several deprecated APIs].

A cluster administrator must provide a manual acknowledgment before the cluster can be upgraded from {product-title} 4.12 to 4.13. This is to help prevent issues after upgrading to {product-title} 4.13, where APIs that have been removed are still in use by workloads, tools, or other components running on or interacting with the cluster. Administrators must evaluate their cluster for any APIs in use that will be removed and migrate the affected components to use the appropriate new API version. After this is done, the administrator can provide the administrator acknowledgment.

All {product-title} 4.12 clusters require this administrator acknowledgment before they can be upgraded to {product-title} 4.13.

For more information, see xref:../updating/updating-cluster-prepare.adoc#updating-cluster-prepare[Preparing to update to {product-title} 4.13].

[id="ocp-4-13-installation-minimum-required-permissions-azure"]
==== Minimum required permissions for Microsoft Azure to install and delete an {product-title} cluster
In {product-title} {product-version}, instead of using the built-in roles, you can now define your custom roles to include the minimum required permissions for Microsoft Azure to install and delete an {product-title} cluster. These permissions are available for installer-provisioned infrastructure and user-provisioned infrastructure.

[id="ocp-4-13-updating-clusters-migrating-cluster-to-multi-arch"]
==== Single-architecture to multi-architecture payload migration
{product-title} {product-version} introduces the `oc adm upgrade --to-multi-arch` command, which lets you migrate a cluster with single-architecture compute machines to a cluster with multi-architecture compute machines. By updating to a multi-architecture, manifest-listed payload, you can add mixed architecture compute machines to your cluster.

[id="ocp-4-13-install-configure-vips-to-run-on-control-plane"]
==== Configuring network components to run on the control plane in vSphere

If you need the virtual IP (VIP) addresses to run on the control plane nodes in a vSphere installation, you must configure the `ingressVIP` addresses to run exclusively on the control plane nodes. By default, {product-title} allows any node in the worker machine configuration pool to host the `ingressVIP` addresses. Because vSphere environments deploy worker nodes in separate subnets from the control plane nodes, configuring the `ingressVIP` addresses to run exclusively on the control plane nodes prevents issues from arising due to deploying worker nodes in separate subnets. For additional details, see xref:../installing/installing_vsphere/installing-vsphere-installer-provisioned-network-customizations.adoc#configure-network-components-to-run-on-the-control-plane_installing-vsphere-installer-provisioned-network-customizations[Configuring network components to run on the control plane in vSphere].

[id="ocp-4-13-install-aws-on-a-single-node"]
==== Installing an {product-title} cluster on AWS with a single node

In {product-title} {product-version}, you can install a cluster with a single node on Amazon Web Services (AWS). Installing on a single node increases the resource requirements for the node. For additional details, see xref:../installing/installing_aws/preparing-to-install-on-aws.adoc#choosing-an-method-to-install-ocp-on-aws-single-node[Installing a cluster on a single node].

[id="ocp-4-13-expand-aws-cluster-with-on-premise-bare-metal-nodes"]
==== Expanding an AWS cluster with on-premise bare metal nodes

In {product-title} 4.12, the Baremetal Operator was enabled on {product-title} clusters deployed on AWS. In {product-title} {product-version}, you can expand a cluster deployed on AWS with on-premise bare metal nodes. This requires establishing secure network connectivity between the AWS environment and the on-premise bare metal environment. For additional details, see xref:../installing/installing_aws/installing-aws-expanding-a-cluster-with-on-premise-bare-metal-nodes.adoc#expanding-a-cluster-with-on-premise-bare-metal-nodes[Expanding a cluster with on-premise bare metal nodes].

[id="ocp-4-13-upi-scaling-hosts-with-bmo"]
==== Scale bare metal hosts in a user-provisioned cluster by using the Bare Metal Operator

With {product-title} {product-version}, you can scale bare metal hosts in an existing user-provisioned infrastructure cluster by using the Bare Metal Operator (BMO) and other metal^3^ components. By using the Bare Metal Operator in a user-provisioned cluster, you can simplify and automate the management and scaling of hosts.

Using the BMO, you can add or remove hosts by configuring a `BareMetalHost` object. You can also keep track of existing hosts by enrolling them as `externallyProvisioned` in the `BareMetalHost` object inventory.

[NOTE]
====
You cannot use a provisioning network to scale user-provisioned infrastructure clusters by using the Bare Metal Operator. Because this workflow does not support a provisioning network, you can only use bare-metal host drivers that support virtual media network booting, for example `redfish-virtualmedia` and `idrac-virtualmedia`.
====

For more information about scaling a user-provisioned cluster by using the BMO, see xref:../installing/installing_bare_metal/scaling-a-user-provisioned-cluster-with-the-bare-metal-operator.adoc#scaling-a-user-provisioned-cluster-with-the-bare-metal-operator[Scaling a user-provisioned cluster with the Bare Metal Operator].

[id="ocp-4-13-OCP-on-ARM"]
==== {product-title} on 64-bit ARM
{product-title} {product-version} is now supported on 64-bit ARM architecture-based Azure user-provisioned installations. The Agent based installation program is also now supported on 64-bit ARM systems. For more information about instance availability and installation documentation, see xref:../installing/installing-preparing.adoc#supported-installation-methods-for-different-platforms[Supported installation methods for different platforms].

// OCPBUGS-8431
[id="ocp-4-13-jenkins-git-lfs"]
====  Support for the git-lfs package
The OpenShift Jenkins image now supports the `git-lfs` package. With this package, you can use artifacts larger than 200 megabytes (MB) in your OpenShift Jenkins image.

[id="ocp-4-13-installation-oc-mirror-oci-ga"]
==== Using the oc-mirror plugin to include local OCI Operator catalogs is now generally available

You can now use the oc-mirror plugin to mirror local OCI Operator catalogs on disk to a mirror registry. This feature was previously introduced as a Technology Preview in {product-title} 4.12 and is now generally available in {product-title} 4.13.

This release introduces support for the following features when local OCI catalogs are included:

* Pruning images from the target mirror registry
* Incremental mirroring to only mirror what has changed since the last time you ran the tool
* Namespace hierarchy for alternative names for catalogs in the target mirror registry

[IMPORTANT]
====
* If you used the Technology Preview OCI local catalogs feature for the oc-mirror plugin for {product-title} 4.12, you can no longer use the OCI feature of the oc-mirror plugin to copy a catalog locally and convert it to OCI format as a first step to mirroring to a fully disconnected cluster.

* When mirroring local OCI catalogs, any {product-title} releases or additional images that you want to mirror along with the local OCI-formatted catalog must be pulled from a registry. You cannot mirror OCI catalogs along with an oc-mirror image set file on disk.

* The `--use-oci-feature` flag has been deprecated. Use the `--include-local-oci-catalogs` flag instead to enable mirroring of local OCI catalogs.
====

For more information, see xref:../installing/disconnected_install/installing-mirroring-disconnected.adoc#oc-mirror-oci-format_installing-mirroring-disconnected[Including local OCI Operator catalogs].

[id="ocp-4-13-installation-openstack-failure-domains"]
==== Deploy clusters that use failure domains on {rh-openstack} (Technology Preview)

You can now deploy clusters that span multiple failure domains on {rh-openstack}. For deployments at scale, failure domains improve resilience and performance.

For more information, see xref:../installing/installing_openstack/installing-openstack-installer-custom.adoc#installation-configuration-parameters-failure-domains-osp_installing-openstack-installer-custom[{rh-openstack} parameters for failure domains].

[id="ocp-4-13-installation-openstack-external-load-balancers"]
==== Deploy clusters with user-managed load balancers on {rh-openstack} (Technology Preview)

You can now deploy clusters on {rh-openstack} with user-managed load balancers rather than the default, internal load balancer.

For more information, see xref:../installing/installing_openstack/installing-openstack-installer-custom.adoc#install-osp-external-lb-config_installing-openstack-installer-custom[Installation configuration for a cluster on OpenStack with a user-managed load balancer].

[id="ocp-4-13-installation-nutanix-projects-categories"]
==== Using projects and categories when installing a cluster on Nutanix

In {product-title} {product-version}, you can use projects and categories to organize compute plane virtual machines in a cluster installed on Nutanix. Projects define logical groups of user roles for managing permissions, networks, and other parameters. You can use categories to apply policies to groups of virtual machines based on shared characteristics.

For more information, see xref:../installing/installing_nutanix/installing-nutanix-installer-provisioned.adoc#installation-configuration-parameters-additional-vsphere_installing-nutanix-installer-provisioned[Installing a cluster on Nutanix].

[id="ocp-4-13-installation-agent-console-application"]
==== Agent-based Installer now performs network connectivity checks

For installations of {product-title} {product-version} using the Agent-based Installer, a console application (with a textual user interface) performs a pull check early in the installation process to verify that the current host can retrieve the configured release image. The console application supports troubleshooting issues by allowing users to directly modify network configurations.

For more information, see xref:../installing/installing_with_agent_based_installer/installing-with-agent-based-installer.adoc#installing-ocp-agent-tui_installing-with-agent-based-installer[Verifying that the current installation host can pull release images].

[id="ocp-4-13-post-installation"]
=== Post-installation configuration

[id="ocp-4-13-multi-arch-compute-machines-ga"]
==== {product-title} clusters with multi-architecture compute machines

{product-title} {product-version} clusters with multi-architecture compute machines is now generally available. As a Day 2 operation, you can now create a cluster with compute nodes of different architectures on AWS and Azure installer provisioned infrastructures. User-provisioned installation on bare metal are in Technology Preview. For more information on creating a cluster with multi-architecture compute machines, see xref:../post_installation_configuration/multi-architecture-configuration.adoc#multi-architecture-configuration[Configuring multi-architecture compute machines on an {product-title} cluster].


[id="ocp-4-13-post-installation-vsphere-failure-domains"]
==== Specifying multiple failure domains for your cluster on VSphere
As an administrator, you can specify multiple failure domains for your {product-title} cluster that runs on a VMware VSphere instance. This means that you can distribute key control planes and workload elements among varied hardware resources for a datacenter. Additionally, you can configure your cluster to use a multiple layer 2 network configuration, so that data transfer among nodes can span across multiple networks.

For more information, see xref:../post_installation_configuration/post-install-vsphere-zones-regions-configuration.adoc#specifying-regions-zones-infrastructure-vsphere_post-install-vsphere-zones-regions-configuration[Specifying multiple failure domains for your cluster on VSphere].

[id="ocp-4-13-web-console"]
=== Web console
[id="ocp-4-13-developer-perspective"]
==== Developer Perspective

With this release, you can now perform the following actions in the *Developer* perspective of the web console:

* Create a *Serverless Function* by using the *Import from Git* flow.
* Create a *Serverless Function* by using the *Create Serverless Function* flow available on *Add page*.
* Select *pipeline-as-code* as an option in the *Import from Git* workflow.
* View which pods are receiving traffic in the following locations in the user interface:
** The side pane of the *Topology* view
** The *Details* view for a pod
** The *Pods* list view
* Customize the timeout period or provide your own image when instantiating a *Web Terminal*.
* As an administrator, set default resources to be pre-pinned in the *Developer* perspective navigation for all users.

[id="ocp-4-13-pipelines-page-improvements"]
===== Pipelines page improvements
In {product-title} 4.13, you can see the following navigation improvements on the *Pipelines* page:

* The tab you previously selected remains visible when you return to the *Pipelines* page.
* The default tab for the *Repository details* page is now *PipelinesRuns*, but when you are following the *Create Git Repository* flow, the default tab is *Details*.

[id="ocp-4-13-helm-page-improvements"]
===== Helm page improvements
In {product-title} 4.13, the *Helm* page now contains the following new and updated features:

* The terminology used on the page now refers to creating and deleting Helm releases rather than installing and uninstalling Helm charts.
* You can create and delete Helm releases asynchronously and not wait for actions to complete before performing the next task in the web console.
* The Helm release list now includes a *Status* column.


[id="ocp-4-13-oc"]
=== OpenShift CLI (oc)

[id="ocp-4-13-oc-must-gather-namespace"]
==== New flag added to run must-gather in a specified namespace

With {product-title} 4.13, the `--run-namespace` flag is now available for the `oc adm must-gather` command. You can use this flag to specify an existing namespace to run the must-gather tool in.

For more information, see xref:../support/gathering-cluster-data.adoc#about-must-gather_gathering-cluster-data[About the must-gather tool].

[id="ocp-4-13-import-image"]
==== Importing manifests with the OpenShift CLI (oc)

With {product-title} {product-version}, a new `oc` command line interface (CLI) flag, `--import-mode`, has been added to the following `oc` commands:

* `oc import-image`
* `oc tag`

With this enhancement, users can set the `--import-mode` flag to `Legacy` or `PreserveOriginal`, which provides users the option to import a single sub-manifest, or all manifests, of a manifest list when running the `oc import-image` or `oc tag` commands.

For more information, see xref:../openshift_images/image-streams-manage.adoc#images-imagestream-import-import-mode_image-streams-managing[Working with manifest lists].

[id="ocp-4-13-oc-describe-enhancement"]
==== Returning os/arch and digests of an image

With {product-title} {product-version}, running `oc describe` on an image now returns os/arch and digests of each manifest.

[id="ocp-4-13-ibm-z"]
=== {ibmzProductName} and {linuxoneProductName}

With this release, {ibmzProductName} and {linuxoneProductName} are now compatible with {product-title} {product-version}. The installation can be performed with z/VM or {op-system-base-full} Kernel-based Virtual Machine (KVM). For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_z/installing-ibm-z.adoc#installing-ibm-z[Installing a cluster with z/VM on {ibmzProductName} and {linuxoneProductName}]
* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc#installing-restricted-networks-ibm-z[Installing a cluster with z/VM on {ibmzProductName} and {linuxoneProductName} in a restricted network]
* xref:../installing/installing_ibm_z/installing-ibm-z-kvm.adoc#installing-ibm-z-kvm[Installing a cluster with {op-system-base} KVM on {ibmzProductName} and {linuxoneProductName}]
* xref:../installing/installing_ibm_z/installing-restricted-networks-ibm-z-kvm.adoc#installing-restricted-networks-ibm-z-kvm[Installing a cluster with RHEL KVM on {ibmzProductName} and {linuxoneProductName} in a restricted network]

[IMPORTANT]
====
Compute nodes must run {op-system-first}
====

[discrete]
==== {ibmzProductName} and {linuxoneProductName} notable enhancements

The {ibmzProductName} and {linuxoneProductName} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components and concepts.

This release introduces support for the following features on {ibmzProductName} and {linuxoneProductName}:

* Assisted Installer
* Cluster Resource Override Operator
* Egress IP
* MetalLB Operator
* Network-Bound Disk Encryption - External Tang Server
////
* FIPS cryptography
////

[discrete]
==== IBM Secure Execution

{product-title} now supports configuring {op-system-first} nodes for IBM Secure Execution on {ibmzProductName} and {linuxoneProductName} (s390x architecture).

For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_z/installing-ibm-z-kvm.html#installing-rhcos-using-ibm-secure-execution_installing-ibm-z-kvm[Installing {op-system} using IBM Secure Execution]

[id="ocp-4-13-ibm-power"]
=== {ibmpowerProductName}

With this release, {ibmpowerProductName} is now compatible with {product-title} {product-version}. For installation instructions, see the following documentation:

* xref:../installing/installing_ibm_power/installing-ibm-power.adoc#installing-ibm-power_installing-ibm-power[Installing a cluster on {ibmpowerProductName}]
* xref:../installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc#installing-restricted-networks-ibm-power_installing-restricted-networks-ibm-power[Installing a cluster on {ibmpowerProductName} in a restricted network]

[IMPORTANT]
====
Compute nodes must run {op-system-first}
====

[discrete]
==== {ibmpowerProductName} notable enhancements

The {ibmpowerProductName} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components and concepts.

This release introduces support for the following features on {ibmpowerProductName}:

* Assisted Installer
* Cluster Resource Override Operator
* {ibmpowerProductName} Virtual Server Block CSI Driver Operator (Technology Preview)
* Egress IP
* Installer-provisioned Infrastructure Enablement for {ibmpowerProductName} Virtual Server (Technology Preview)
* MetalLB Operator
* Network-Bound Disk Encryption - External Tang Server
////
* FIPS cryptography
////

[discrete]
=== {ibmpowerProductName},{ibmzProductName}, and {linuxoneProductName} support matrix

.{product-title} features
[cols="3,1,1",options="header"]
|====
|Feature |{ibmpowerProductName} |{ibmzProductName} and {linuxoneProductName}

|Alternate authentication providers
|Supported
|Supported

|Assisted Installer
|Supported
|Supported

|Automatic Device Discovery with Local Storage Operator
|Unsupported
|Supported

|Automatic repair of damaged machines with machine health checking
|Unsupported
|Unsupported

|Cloud controller manager for IBM Cloud
|Supported
|Unsupported

|Controlling overcommit and managing container density on nodes
|Unsupported
|Unsupported

|Cron jobs
|Supported
|Supported

|Descheduler
|Supported
|Supported

|Egress IP
|Supported
|Supported

|Encrypting data stored in etcd
|Supported
|Supported

//|FIPS cryptography
//|Supported
//|Supported

|Helm
|Supported
|Supported

|Horizontal pod autoscaling
|Supported
|Supported

|IPv6
|Supported
|Supported

|Monitoring for user-defined projects
|Supported
|Supported

|Multipathing
|Supported
|Supported

|Network-Bound Disk Encryption - External Tang Server
|Supported
|Supported

|Non--volatile memory express drives (NVMe)
|Supported
|Unsupported

|{oc-first} plugins
|Supported
|Supported

|Operator API
|Supported
|Supported

|OpenShift Virtualization
|Unsupported
|Unsupported

|OVN-Kubernetes, including IPsec encryption
|Supported
|Supported

|PodDisruptionBudget
|Supported
|Supported

|Precision Time Protocol (PTP) hardware
|Unsupported
|Unsupported

|{openshift-local-productname}
|Unsupported
|Unsupported

|Scheduler profiles
|Supported
|Supported

|Stream Control Transmission Protocol (SCTP)
|Supported
|Supported

|Support for multiple network interfaces
|Supported
|Supported

|Three-node cluster support
|Supported
|Supported

|Topology Manager
|Supported
|Unsupported

|z/VM Emulated FBA devices on SCSI disks
|Unsupported
|Supported

|4K FCP block device
|Supported
|Supported
|====

.Persistent storage options
[cols="2,1,1",options="header"]
|====
|Feature |{ibmpowerProductName} |{ibmzProductName} and {linuxoneProductName}
|Persistent storage using iSCSI
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using local volumes (LSO)
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using hostPath
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Fibre Channel
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using Raw Block
|Supported ^[1]^
|Supported ^[1]^,^[2]^

|Persistent storage using EDEV/FBA
|Supported ^[1]^
|Supported ^[1]^,^[2]^
|====
[.small]
--
1. Persistent shared storage must be provisioned by using either {rh-storage-first} or other supported storage protocols.
2. Persistent non-shared storage must be provisioned using local storage, like iSCSI, FC, or using LSO with DASD, FCP, or EDEV/FBA.
--

.Operators
[cols="2,1,1",options="header"]
|====
|Feature |{ibmpowerProductName} |{ibmzProductName} and {linuxoneProductName}

|Cluster Logging Operator
|Supported
|Supported

|Cluster Resource Override Operator
|Supported
|Supported

|Compliance Operator
|Supported
|Supported

|File Integrity Operator
|Supported
|Supported

|Local Storage Operator
|Supported
|Supported

|MetalLB Operator
|Supported
|Supported

|NFD Operator
|Supported
|Supported

|NMState Operator
|Supported
|Supported

|OpenShift Elasticsearch Operator
|Supported
|Supported

|Service Binding Operator
|Supported
|Supported

|Vertical Pod Autoscaler Operator
|Supported
|Supported
|====

.Multus CNI plugins
[cols="2,1,1",options="header"]
|====
|Feature |{ibmpowerProductName} |{ibmzProductName} and {linuxoneProductName}

|Bridge
|Supported
|Supported

|Host-device
|Supported
|Supported

|IPAM
|Supported
|Supported

|IPVLAN
|Supported
|Supported
|====

.CSI Volumes
[cols="2,1,1",options="header"]
|====
|Feature |{ibmpowerProductName} |{ibmzProductName} and {linuxoneProductName}

|Cloning
|Supported
|Supported

|Expansion
|Supported
|Supported

|Snapshot
|Supported
|Supported
|====

[id="ocp-4-13-images"]
=== Images

[id="ocp-4-13-manifest-list-support"]
==== Support for manifest listed images on image streams

With {product-title} {product-version}, support for manifest listed images on image streams is now generally available.

[id="ocp-4-13-security"]
=== Security and compliance

[id="ocp-4-13-aesgcm-encryption"]
==== AES-GCM encryption is now supported

The AES-GCM encryption type is now supported when enabling etcd encryption for {product-title}. Encryption keys for the AES-GCM encryption type are rotated weekly.

For more information, see xref:../security/encrypting-etcd.adoc#etcd-encryption-types_encrypting-etcd[Supported encryption types].

//[id="ocp-4-13-auth"]
//=== Authentication and authorization

[id="ocp-4-13-networking"]
=== Networking

[id="ocp-4-13-networking-metrics"]
==== Enhancements to networking metrics

[id="ocp-4-13-egress-ips-rebalance-total"]
===== egress_ips_rebalance_total
* Metric name: `ovnkube_master_egress_ips_rebalance_total`
* *Help message:* `The total number of times assigned egress IP(s) needed to be moved to a different node.`

[id="ocp-4-13-egress-ips-node-unreachable"]
===== egress_ips_node_unreachable_total
* Metric name: `ovnkube_master_egress_ips_node_unreachable_total`
* *Help message*: `The total number of times assigned egress IP(s) were unreachable.`

[id="ocp-4-13-egress-ips-unassign-latency-seconds"]
===== egress_ips_unassign_latency_seconds
* Metric name: `ovnkube_master_egress_ips_unassign_latency_seconds`
* *Help message*: `The latency of egress IP unassignment from OVN northbound database.`

[id="ocp-4-13-interface-total"]
===== interfaces_total
* Metric name: `ovs_vswitchd_interfaces_total`
* *Help message*: `The total number of Open vSwitch interface(s) created for pods` and  `Open vSwitch interface until its available.`

[id="ocp-4-13-interface-up-wait"]
===== interface_up_wait_seconds_total
* Metric name: `ovs_vswitchd_interface_up_wait_seconds_total`
* *Help message*: `The total number of seconds that is required to wait for pod.` and `Open vSwitch interface until its available.`

[id="ocp-4-13-ovnkube-resources-retry-failures-total"]
===== ovnkube_resource_retry_failures_total
* Metric name: `ovnkube_resource_retry_failures_total`
* *Help message*: `The total number of times processing a Kubernetes resource reached the maximum retry limit and was no longer processed.`

[id="ocp-4-13-networking-alerts"]
==== Enhancements to networking alerts
* OVN Kubernetes retries a claim up to 15 times before dropping it. With this update, if this failure happens, {product-title} alerts the cluster administrator. A description of each alert can be viewed in the console.

[id="ocp-4-13-noovnmasterleader"]
===== NoOvnMasterLeader
* Summary: There is no ovn-kubernetes master leader.
* Description in console:
+
[source,text]
----
Networking control plane is degraded. Networking configuration updates applied to the cluster will not be implemented while there is no OVN Kubernetes leader. Existing workloads should continue to have connectivity. OVN-Kubernetes control plane is not functional.
----

[id="ocp-4-13-NodeOVSOverflowUserspace"]
===== OVNKubernetesNodeOVSOverflowUserspace
* Summary: OVS vSwitch daemon drops packets due to buffer overflow.
* Description in console:
+
[source,text]
----
Netlink messages dropped by OVS vSwitch daemon due to netlink socket buffer overflow. This will result in packet loss.
----

[id="ocp-4-13-NodeOVSOverflowKernel"]
===== OVNKubernetesNodeOVSOverflowKernel
* Summary: OVS kernel module drops packets due to buffer overflow.
* Description in console:
+
[source,text]
----
Netlink messages dropped by OVS kernel module due to netlink socket buffer overflow. This will result in packet loss.
----

[id="ocp-4-13-network-observability-1_1-1_2"]
==== Network Observability Operator
The Network Observability Operator releases updates independently from the {product-title} minor version release stream. Updates are available through a single, rolling stream which is supported on all currently supported versions of {product-title} 4. Information regarding new features, enhancements, and bug fixes for the Network Observability Operator can be found in the xref:../networking/network_observability/network-observability-operator-release-notes.adoc[Network Observability release notes].

[id="ocp-4-13-nw-metallb-ipaddresspool-assignment"]
==== Assign IP addresses in MetalLB IPAddressPool resources to specific namespaces and services
With this update, you can assign IP addresses from a MetalLB `IPAddressPool` resource to services, namespaces, or both. This is useful in a muti-tenant, bare-metal environment that requires MetalLB to pin IP addresses from an IP address pool to specific services and namespaces. You can assign IP addresses from many IP address pools to services and namespaces. You can then define the prioritization for these IP address pools so that MetalLB assigns IP addresses starting from the higher priority IP address pool.

For more information about assigning IP addresses from an IP address pool to services and namespaces, see xref:../networking/metallb/metallb-configure-address-pools.adoc#nw-metallb-configure-address-pool_configure-metallb-address-pools[Configuring MetalLB address pools].

[id="ocp-4-13-supporting-ocp-nodes-with-dual-port-nics"]
==== Supporting {product-title} installation on nodes with dual-port NICs (Technology Preview)

With this update, {product-title} cluster can be deployed on a bond interface with 2 virtual function (VFs) on 2 physical functions (PFs) using the following methods:

* Agent-based installer
* Installer-provisioned infrastructure installation
* User-provisioned infrastructure installation

For more information about installing {product-title} on nodes with dual-port NICs, see xref:../installing/installing_bare_metal/preparing-to-install-on-bare-metal.adoc#nw-sriov-dual-nic-con_preparing-to-install-on-bare-metal[NIC partitioning for SR-IOV devices].

[id="ocp-4-13-bf2-switching-dpu-nic"]
==== Support for switching the BlueField-2 network device from data processing unit (DPU) mode to network interface controller (NIC) mode is now GA

In this release, switching the BlueField-2 network device from data processing unit (DPU) mode to network interface controller (NIC) mode is now generally available.

For more information, see xref:../networking/hardware_networks/switching-bf2-nic-dpu.adoc#switching-bf2-nic-dpu[Switching BlueField-2 from DPU to NIC].

[id="ocp-4-13-networking-supported-hardware-for-ovs"]
==== Hardware offload for the MT2892 Family [ConnectX-6 Dx] of network cards is GA

{product-title} 4.13 adds OvS Hardware Offload support for the MT2892 Family [ConnectX-6 Dx] of network cards.

For more information, see xref:../networking/hardware_networks/configuring-hardware-offloading.adoc#supported_devices_configuring-hardware-offloading[Supported devices].

[id="ocp-4-13-migrate-to-openshift-sdn-network-plugin"]
==== Migrating to the OpenShift SDN network plugin

If you are using the OVN-Kubernetes network plugin, you can migrate to the OpenShift SDN network plugin.

For more information, see xref:../networking/openshift_sdn/migrate-to-openshift-sdn.adoc#migrate-to-openshift-sdn[Migrating to the OpenShift SDN network plugin].

[id="ocp-4-13-core-dns-update"]
==== CoreDNS updated to 1.10.1

{product-title} {product-version} updates CoreDNS to 1.10.1. CoreDNS now uses the DNSSEC DO Bit that was specified on the originating client query. This reduces the DNS response UDP packet size when a client is not requesting DNSSEC. Consequently, the smaller packet size reduces both the chance of DNS truncation decreasing by TCP connection retries and overall DNS bandwidth.

[id="ocp-4-13-expand-cluster-network-ip-address-range"]
==== Expand cluster network IP address range

The cluster network can be expanded to support the addition of nodes to the cluster. For example, if you deployed a cluster and specified `10.128.0.0/19` as the cluster network range and a host prefix of `23`, you are limited to 16 nodes. You can expand that to 510 nodes by changing the CIDR mask on a cluster to `/14`. For more information, see xref:../networking/configuring-cluster-network-range.adoc#configuring-cluster-network-range[Configuring the cluster network range].

[id="ocp-4-13-dual-stack-vmware-vsphere-clusters"]
==== Dual-stack IPv4/IPv6 on VMware vSphere clusters

On installer-provisioned vSphere clusters, you can use dual-stack networking with IPv4 as the primary IP family, and IPv6 as the secondary address family. For more information, see xref:../installing/installing_vsphere/installing-vsphere-installer-provisioned-network-customizations.adoc#installation-configuration-parameters-network_installing-vsphere-installer-provisioned-network-customizations[Network configuration parameters].

[id="ocp-4-13-ipv6-primary-address-family-bare-metal-dual-stack-clusters"]
==== IPv6 as primary IP address family on bare metal dual-stack clusters

During cluster installation on bare metal, you can configure IPv6 as the primary IP address family on a dual-stack cluster. To enable this feature when installing a new cluster, specify an IPv6 address family before an IPv4 address family for the machine network, cluster network, service network, API VIPs, and ingress VIPs.

For more information, refer to the following sources:

* Installer-provisioned infrastructure: xref:../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#modifying-install-config-for-dual-stack-network_ipi-install-installation-workflow[Deploying with dual-stack networking]
* User-provisioned infrastructure: xref:../installing/installing_bare_metal/installing-bare-metal-network-customizations.adoc#installation-configuration-parameters-network_installing-bare-metal-network-customizations[Network configuration parameters]

[id="ocp-4-13-ovn-kubernetes-plugin-secondary-network"]
==== OVN-Kubernetes is available as a secondary network (Technology Preview)

With this release, the {openshift-networking} OVN-Kubernetes network plug-in allows the configuration of secondary network interfaces for pods. As a secondary network, OVN-Kubernetes supports a layer 2 (switched) topology network. This is available as a Technology Preview feature.

For more information about OVN-Kubernetes as a secondary network, see xref:../networking/multiple_networks/configuring-additional-network.html#configuration-ovnk-additional-networks_configuring-additional-network[Configuration for an OVN-Kubernetes additional network].

[id="ocp-4-13-nodeselector-egressfirewall-enhancement"]
==== Node selector added to egress firewall for OVN-Kubernetes network plugin

In {product-title} {product-version}, `nodeSelector` has been added to the egress firewall destination spec in OVN-Kubernetes network plug-in. This feature allows users to add a label to one or multiple nodes and the IP addresses of the selected nodes are included in the associated rule. For more information, see xref:../networking/ovn_kubernetes_network_provider/configuring-egress-firewall-ovn.adoc#configuringNodeSelector-example_configuring-egress-firewall-ovn[Example nodeSelector for EgressFirewall]

[id="ocp-4-13-nw-openstack-kuryr-migration"]
==== Kuryr to OVN-Kubernetes migration procedure for clusters that run on {rh-openstack} (Technology Preview)

You can now migrate a cluster that runs on {rh-openstack} and uses Kuryr to OVN-Kubernetes.

For more information, see xref:../networking/ovn_kubernetes_network_provider/migrate-from-kuryr-sdn.adoc#migrate-from-kuryr-sdn[Migrating from the Kuryr network plugin to the OVN-Kubernetes network plugin].

[id="ocp-4-13-nw-openstack-egressip"]
==== Improved egress IP support for clusters that run on {rh-openstack}

For clusters that run on {rh-openstack} and use OVN-Kubernetes, manually reassigning floating IP addresses for reservation ports is no longer necessary. If a reservation port is removed from one node and recreated on another one, the reassignment now happens automatically.

[id="ocp-4-13-networking-supported-hardware-for-sr-iov"]
==== Supported hardware for SR-IOV (Single Root I/O Virtualization)

{product-title} {product-version} adds support for the following SR-IOV devices:

* Intel E810-XXVDA4T

For more information, see xref:../networking/hardware_networks/about-sriov.adoc#supported-devices_about-sriov[Supported devices].

[id="ocp-4-13-storage"]
=== Storage

[id="ocp-4-13-aws-customer-managed-keys-kms"]
==== Support for customer-managed keys for re-encryption in the KMS

With this update, the default credentials request for AWS has been modified to allow customer-managed keys to be used for re-encryption in the Key Management Service (KMS). For clusters with the Cloud Credential Operator (CCO) configured to use manual mode, administrators must apply those changes manually by adding `kms:ReEncrypt*` permission to their key policy. Other administrators are not impacted by this change. (link:https://issues.redhat.com/browse/OCPBUGS-5410[*OCPBUGS-5410*])

[id="ocp-4-13-storage-lvms-dual-stack-support"]
==== Dual-stack support for {lvms-first}
In {product-title} {product-version}, {lvms} is supported in dual-stack for IPv4 and IPv6 network environments. For more information, see xref:../networking/ovn_kubernetes_network_provider/converting-to-dual-stack.adoc#nw-dual-stack-convert_converting-to-dual-stack[Converting to a dual-stack cluster network].

[id="ocp-4-13-storage-lvms-in-zpt"]
==== Support for {lvms} in GitOps ZTP
In {product-title} {product-version}, you can add and configure {lvms-first} through {ztp}. For more information, see xref:../scalability_and_performance/ztp_far_edge/ztp-advanced-policy-config.adoc#ztp-provisioning-lvm-storage_ztp-advanced-policy-config[Configuring LVM Storage using PolicyGenTemplate CRs] and xref:../scalability_and_performance/ztp_far_edge/ztp-reference-cluster-configuration-for-vdu.adoc#lvms-configuring-lvms-on-sno_sno-configure-for-vdu[LVM Storage].

[id="ocp-4-13-storage-lvms-in-disconnected-env"]
==== Support for {lvms} in disconnected environments

In {product-title} {product-version}, you can install {lvms} in disconnected environments. For more information, see xref:../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#lvms-installing-lvms-disconnected-env_logical-volume-manager-storage[Installing LVM Storage in a disconnected environment].

[id="ocp-4-13-storage-user-managed-encryption"]
==== User-managed encryption is generally available
The user-managed encryption feature allows you to provide keys during installation that encrypt {product-title} node root volumes, and enables all managed storage classes to use these keys to encrypt provisioned storage volumes. This allows you to encrypt storage volumes with your selected key, instead of the platform’s default account key.

This features supports the following storage types:

* Amazon Web Services (AWS) Elastic Block storage (EBS) (for more information, see xref:../storage/container_storage_interface/persistent-storage-csi-ebs.adoc#byok_persistent-storage-csi-ebs[User-managed encryption])

* Microsoft Azure Disk storage (for more information, see xref:../storage/container_storage_interface/persistent-storage-csi-azure.adoc#byok_persistent-storage-csi-azure[User-managed encryption])

* Google Cloud Platform (GCP) persistent disk (PD) storage (for more information, see xref:../storage/container_storage_interface/persistent-storage-csi-gcp-pd.adoc#byok_persistent-storage-csi-gcp-pd[User-managed encryption])

[id="ocp-4-13-storage-csi-vol-detach-non-graceful-shutdown"]
==== Detach CSI volumes after non-graceful node shutdown (Technology Preview)
Container Storage Interface (CSI) drivers can now automatically detach volumes when a node goes down non-gracefully. When a non-graceful node shutdown occurs, you can then manually add an out-of-service taint on the node to allow volumes to automatically detach from the node. This feature is supported with Technology Preview status.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vol-detach-non-graceful-shutdown.adoc[Detach CSI volumes after non-graceful node shutdown].

[id="ocp-4-13-storage-vsphere-encryption-support"]
==== VMware vSphere encryption support is generally available
You can encrypt virtual machines (VMs) and persistent volumes (PVs) on {product-title} running on vSphere.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#vsphere-pv-encryption[vSphere persistent disks encryption].

[id="ocp-4-13-storage-vpshere-top-support-multi-datacenters"]
==== VMware vSphere CSI topology support for multiple datacenters is generally available
{product-title} 4.12 introduced the ability to deploy {product-title} for vSphere on different zones and regions, which allows you to deploy over multiple compute clusters, thus helping to avoid a single point of failure. {product-title} 4.13 introduces support for deploying over multiple datacenters and to set up the topology using failure domains created during installation or post-installation.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere-top-aware_persistent-storage-csi-vsphere[vSphere CSI topology].

[id="ocp-4-13-storage-multiple-default-sc"]
==== Creating more than one default storage class is generally available
{product-title} 4.13 allows you create more than one default storage class. This feature makes it easier to change the default storage class because you can create a second storage class defined as the default. You then temporarily have two default storage classes before removing default status from the previous default storage class. While it is acceptable to have multiple default storage classes for a short time, you should ensure that eventually only one default storage class exists.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-sc-manage.adoc#change-default-storage-class_persistent-storage-csi-sc-manage[Changing the default storage class] and xref:../storage/container_storage_interface/persistent-storage-csi-sc-manage.adoc#multiple-default-storage-classes[Multiple default storage classes].

[id="ocp-4-13-storage-managing-default-sc"]
==== Managing the default storage class is generally available
{product-title} 4.13 introduces the `spec.storageClassState` field in the `ClusterCSIDriver` object, which allows you to manage the default storage class generated by {product-title} to accomplish several different objectives:

* When you have other preferred storage classes, preventing the storage operator from re-creating the initial default storage class.

* Renaming, or otherwise changing, the default storage class

* Enforcing static provisioning by disabling dynamic provisioning.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-sc-manage.adoc[Managing the default storage class].

[id="ocp-4-13-storage-retroactive-default-sc-assignment"]
==== Retroactive default StorageClass assignment (Technology Preview)
Previously, if there was no default storage class, persistent volumes claims (PVCs) that were created that requested the default storage class remained stranded in the pending state indefinitely, unless you manually delete and recreate them. {product-title} can now retroactively assign a default storage class to these PVCs, so that they do not remain in the pending state. With this feature enabled, after a default storage class is created, or one of the existing storage classes is declared the default, these previously stranded PVCs are assigned to the default storage class.

This feature is supported with Technology Preview status.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-sc-manage.adoc#absent-default-storage-class[Absent default storage class].

[id="ocp-4-13-powervs-csi-driver-operator"]
==== {ibmpowerProductName} Virtual Server Block CSI Driver Operator (Technology Preview)

{product-title} is capable of provisioning persistent volumes (PVs) by using the Container Storage Interface (CSI) driver for {ibmpowerProductName} Virtual Server Block Storage.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-ibm-powervs-block.adoc#persistent-storage-csi-ibm-powervs-block[{ibmpowerProductName} Virtual Server Block CSI Driver Operator].

[id="ocp-4-13-storage-csi-inline-ephemeral-vols-GA"]
==== CSI inline ephemeral volumes is generally available
Container Storage Interface (CSI) inline ephemeral volumes were introduced in {product-title} 4.5 as a Technology Preview feature, which allows you to define a pod spec that creates inline ephemeral volumes when a pod is deployed and delete them when a pod is destroyed. This feature is now generally available.

This feature is only available with supported Container Storage Interface (CSI) drivers.

This feature also includes the CSI Volume Admission plug-in, which provides a mechanism where the use of an individual CSI driver capable of provisioning CSI ephemeral volumes can be restricted on pod admission. Administrators or distributions can add a `csi-ephemeral-volume-profile` label to a `CSIDriver` object, and the label is then inspected by the Admission plug-in and used in enforcement, warning, and audit decisions.

For more information, see xref:../storage/container_storage_interface/ephemeral-storage-csi-inline.adoc[CSI inline ephemeral volumes].

[id="ocp-4-13-storage-csi-migration-azure-file-GA"]
==== Automatic CSI migration for Microsoft Azure File is generally available
Starting with {product-title} 4.8, automatic migration for in-tree volume plugins to their equivalent Container Storage Interface (CSI) drivers became available as a Technology Preview feature. Support for Azure File was provided in this feature in {product-title} 4.10. {product-title} 4.13 now supports automatic migration for Azure File as generally available. CSI migration for Azure File is now enabled by default and requires no action by an administrator.

This feature automatically translates in-tree objects to their counterpart CSI representations and should be completely transparent to users. Translated objects are not stored on disk, and user data is not migrated.

Although storage class referencing to the in-tree storage plug-in will continue working, it is recommended that you switch the default storage class to the CSI storage class.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-migration.adoc[CSI Automatic Migration].

[id="ocp-4-13-storage-csi-migration-vsphere-GA"]
==== Automatic CSI migration for VMware vSphere is generally available
Starting with {product-title} 4.8, automatic migration for in-tree volume plugins to their equivalent Container Storage Interface (CSI) drivers became available as a Technology Preview feature. Support for vSphere was provided in this feature in {product-title} 4.10. {product-title} 4.13 now supports automatic migration for vSphere as generally available. CSI migration for vSphere is now enabled by default and requires no action by an administrator.

This feature automatically translates in-tree objects to their counterpart CSI representations and should be completely transparent to users.

Although storage class referencing to the in-tree storage plug-in will continue working, it is recommended that you switch the default storage class to the CSI storage class.

For new installations of OpenShift Container Platform 4.13, or later, automatic migration is enabled by default. However, when upgrading from {product-title} 4.12, or earlier, to 4.13, automatic CSI migration for vSphere only occurs if you opt in. xref:../storage/container_storage_interface/persistent-storage-csi-migration.adoc#upgrading-openshift-container-platform[Carefully review the indicated consequences before opting in to migration].

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-migration.adoc[CSI Automatic Migration].

[id="ocp-4-13-storage-aws-efs-cross-account-support"]
==== Cross account support for AWS EFS CSI driver is generally available
Cross account support allows you to have an {product-title} cluster in one Amazon Web Services (AWS) account and mount your file system in another AWS account using the AWS Elastic File System (EFS) Container Storage Interface (CSI) driver.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-aws-efs.adoc#persistent-storage-csi-efs-cross-account_persistent-storage-csi-aws-efs[AWS EFS CSI cross account support].

[id="ocp-4-13-storage-fsgroup-to-csi-driver"]
==== Delegate FSGroup to CSI Driver instead of Kubelet is generally available
This feature allows {product-title} to supply a pod's FSGroup to a Container Storage Interface (CSI) driver when a volume is mounted. Microsoft Azure File CSI driver depends on this feature.

[id="ocp-4-13-olm"]
=== Operator lifecycle

[id=ocp-4-13-olm-discover-operator-versions]
==== Finding Operator versions by using the OpenShift CLI
In {product-title} 4.13, you can find which versions and channels of an Operator you can install on your system by running the following {oc-first} command:

.Example `oc describe` command syntax
[source,terminal]
----
$ oc describe packagemanifests <operator_name> -n <catalog_namespace>
----

You can specify the output format of an Operator's version and channel information by running the following command:

.Example `oc get` command syntax
[source,terminal]
----
$ oc get packagemanifests <operator_name> -n <catalog_namespace> -o <output_format>
----

For more information, see xref:../operators/admin/olm-adding-operators-to-cluster.adoc#olm-installing-specific-version-cli_olm-adding-operators-to-a-cluster[Installing a specific version of an Operator].

[id=ocp-4-13-olm-multitenant]
==== Operators in multitenant clusters

The default behavior for Operator Lifecycle Manager (OLM) aims to provide simplicity during Operator installation. However, this behavior can lack flexibility, especially in multitenant clusters.

Guidance and a recommended solution for Operator management in multitenant clusters has been added with the following topics:

* xref:../operators/understanding/olm-multitenancy.adoc#olm-multitenancy[Operators in multitenant clusters]
* xref:../operators/admin/olm-adding-operators-to-cluster.adoc#olm-preparing-operators-multitenant_olm-adding-operators-to-a-cluster[Preparing for multiple instances of an Operator for multitenant clusters]

[id=ocp-4-13-olm-colocation-namespace]
==== Colocation of Operators in a namespace

Operator Lifecycle Manager (OLM) handles OLM-managed Operators that are installed in the same namespace, meaning their Subscription resources are colocated in the same namespace, as related Operators. Even if they are not actually related, OLM considers their states, such as their version and update policy, when any one of them is updated.

Guidance on Operator colocation and an alternative procedure that uses custom namespaces has been added with the following topics:

* xref:../operators/understanding/olm/olm-colocation.adoc#olm-colocation-namespaces_olm-colocation[Colocation of Operators in a namespace]
* xref:../operators/admin/olm-adding-operators-to-cluster.adoc#olm-installing-global-namespaces_olm-adding-operators-to-a-cluster[Installing global Operators in custom namespaces]

[id=ocp-4-13-olm-disabled-csvs]
==== Updated web console behavior when disabling copied CSVs

The {product-title} web console has been updated to provide better Operator discovery when copied cluster service versions (CSVs) are disabled on a cluster.

When copied CSVs are disabled by a cluster administrator, the web console is modified to show copied CSVs from the `openshift` namespace in every namespace for regular users, even though the CSVs are not actually copied to every namespace. This allows regular users to still be able to view the details of these Operators in their namespaces and create custom resources (CRs) brought in by globally installed Operators.

For more information, see xref:../operators/admin/olm-config.adoc#olm-disabling-copied-csvs_olm-config[Disabling copied CSVs].

[id="ocp-4-13-osdk"]
=== Operator development

[id="ocp-4-13-suggested-namespace-template"]
==== Setting a suggested namespace template with default node selector

With this release, Operator authors can set a default node selector on the suggested namespace where the Operator runs. The suggested namespace is created using the namespace manifest in the YAML which is included in the `ClusterServiceVersion` (CSV).  When adding the Operator to a cluster using OperatorHub, the web console automatically populates the suggested namespace for the cluster administrator during the installation process.

For more information, see xref:../operators/operator_sdk/osdk-generating-csvs.adoc#osdk-suggested-namespace-default-node_osdk-generating-csvs[Setting a suggested namespace with default node selector].

[id="ocp-4-13-nto"]
==== Node Tuning Operator

The Node Tuning Operator (NTO) can now be enabled/disabled using the `NodeTuning` cluster capability. If disabled at cluster install, it can be re-enabled later. For more information, see xref:../installing/cluster-capabilities.adoc#about-node-tuning-operator_cluster-capabilities[Node Tuning capability].

[id="ocp-4-13-machine-api"]
=== Machine API

[id="ocp-4-13-mapi-cpms-platform-support"]
==== Additional platform support for control plane machine sets

* With this release, control plane machine sets are supported for Google Cloud Platform clusters.

* This release includes an enhancement to the user experience for the control plane machine set on Microsoft Azure clusters. For Azure clusters that are installed with or upgraded to {product-title} version 4.13, you are no longer required to create a control plane machine set custom resource (CR).
+
--
* Clusters that are installed with version 4.13 have a control plane machine set that is active by default.

* For clusters that are upgraded to version 4.13, an inactive CR is generated for the cluster and can be activated after you verify that the values in the CR are correct for your control plane machines.
--

For more information, see xref:../machine_management/control_plane_machine_management/cpmso-getting-started.adoc#cpmso-getting-started[Getting started with the Control Plane Machine Set Operator].

[id="ocp-4-13-machine-config-operator"]
=== Machine Config Operator

[id="ocp-4-13-machine-config-operator-layering"]
==== {op-system-first} image layering is generally available

{op-system-first} image layering is now generally available. With this feature, you can extend the functionality of your base RHCOS image by layering additional images onto the base image.

For more information, see  xref:../post_installation_configuration/coreos-layering.adoc#coreos-layering[{op-system-first} image layering].

[id="ocp-4-13-machine-config-operator-layering-third-party"]
==== Support for adding third party and custom content to {op-system}

You can now use {op-system-first} image layering to add {op-system-base-full} and third-party packages to cluster nodes.

For more information, see  xref:../post_installation_configuration/coreos-layering.adoc#coreos-layering[{op-system-first} image layering].

[id="ocp-4-13-machine-config-operator-core"]
==== Support for setting the core user password

You can now create a password for the {op-system} `core` user. If you cannot use SSH or the `oc debug node` command to access a node, this password allows you to use the `core` user to access the node through a cloud provider serial console or a bare metal baseboard controller manager (BMC).

For more information, see xref:../post_installation_configuration/machine-configuration-tasks.adoc#core-user-password_post-install-machine-configuration-tasks[Changing the core user password for node access].

[id="ocp-4-13-nodes"]
=== Nodes

[id="ocp-4-13-nodes-mirror"]
==== Image registry repository mirroring by tags

You can now pull images from a mirrored registry by using image tags in addition to digest specifications. To accomplish this change, the `ImageContentSourcePolicy` (ICSP) object is deprecated. You can now use an `ImageDigestMirrorSet` (IDMS) object to pull images by using digest specifications or an `ImageTagMirrorSet` (ITMS) object to pull images by using image tags.

If you have existing YAML files that you used to create ICSP objects, you can use the `oc adm migrate icsp` command to convert those files to an IDMS YAML file.

For more information on these new objects, see xref:../post_installation_configuration/preparing-for-users.adoc#images-configuration-registry-mirror_post-install-preparing-for-users[Configuring image registry repository mirroring].

For more information on converting existing ICSP YAML files to IDMS YAML files, see xref:../post_installation_configuration/preparing-for-users.adoc#images-configuration-registry-mirror-convert_post-install-preparing-for-users[Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring].

[id="ocp-4-13-nodes-crun-ga"]
==== crun general availability

The crun low-level container runtime is now generally available in {product-title} 4.13. There is no new functionality in the GA version.

[id="ocp-4-13-nodes-cgroup-v2-ga"]
==== Linux Control Group version 2 (cgroup v2) general availability

Linux Control Group version 2 (cgroup v2) is now generally available in {product-title} 4.13. There is no new functionality in the GA version.

[id="ocp-4-13-pdb-eviction-policy"]
==== Pod disruption budget (PDB) unhealthy pod eviction policy (Technology Preview)

With this release, specifying an unhealthy pod eviction policy for pod disruption budgets (PDBs) is available as a Technology Preview feature. This can help evict malfunctioning applications during a node drain.

To use this Technology Preview feature, you must enable the `TechPreviewNoUpgrade` feature set.

[WARNING]
====
Enabling the `TechPreviewNoUpgrade` feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
====

For more information, see xref:../nodes/pods/nodes-pods-configuring.adoc#pod-disruption-eviction-policy_nodes-pods-configuring[Specifying the eviction policy for unhealthy pods].

[id="ocp-4-13-graceful-node-shutdown"]
==== Support for graceful node shutdown

A graceful node shutdown delays the eviction of pods during a node shutdown. In {product-title} {product-version}, you can configure the kubelet to enable a graceful node shutdown so that pods running critical workloads are not interrupted.

To configure graceful node shutdowns, you can specify a termination grace period for regular and critical pods in the `KubeletConfig` custom resource. A termination grace period defines a time period for the pod to complete any ongoing tasks before terminating. You can also add priority classes to pods to specify the order of termination.

For further information see xref:../nodes/nodes/nodes-nodes-graceful-shutdown.adoc#nodes-nodes-graceful-shutdown[Managing graceful node shutdown].

[id="ocp-4-13-metal3-remediation-support"]
==== Metal3 remediation support

Previously, Machine Health Checks could self-remediate or use the Self Node Remediation provider. With this release, the new Metal3 remediation provider is also supported on bare metal clusters.

For more information, see xref:../machine_management/deploying-machine-health-checks.html#mgmt-power-remediation-baremetal-about_deploying-machine-health-checks[About power-based remediation of bare metal].

[id="ocp-4-13-monitoring"]
=== Monitoring
The monitoring stack for this release includes the following new and modified features.

[id="ocp-4-13-monitoring-updates-to-monitoring-stack-components-and-dependencies"]
==== Updates to monitoring stack components and dependencies
This release includes the following version updates for monitoring stack components and dependencies:

* Alertmanager to 0.25.0
* kube-state-metrics to 2.8.1
* node-exporter to 1.5.0
* prom-label-proxy to 0.6.0
* Prometheus to 2.42.0
* prometheus-operator to 0.63.0
* Thanos to 0.30.2

[id="ocp-4-13-monitoring-changes-to-alerting-rules"]
==== Changes to alerting rules

[NOTE]
====
Red Hat does not guarantee backward compatibility for recording rules or alerting rules.
====

* The `NodeFilesystemAlmostOutOfSpace` alert no longer fires for certain `tmpfs` mount points that are always full by design.

[id="ocp-4-13-monitoring-new-option-to-add-secrets-to-the-alertmanager-configuration"]
==== New option to add secrets to the Alertmanager configuration
With this release, you can add secrets to the Alertmanager configuration for core platform monitoring and for user-defined projects.
If you need to authenticate with a receiver so that Alertmanager can send alerts to it, you can now configure Alertmanager to use a secret that contains authentication credentials for the receiver.

[id="ocp-4-13-monitoring-new-option-to-configure-node-exporter-collectors"]
==== New option to configure node-exporter collectors
With this release, you can customize Cluster Monitoring Operator (CMO) config map settings for the following node-exporter collectors.
The following node-exporter collectors are now optional and can be enabled or disabled:

* `buddyinfo` collector
* `cpufreq` collector
* `netclass` collector
* `netdev` collector
* `netlink` backend for the `netclass` collector
* `tcpstat` collector

[id="ocp-4-13-monitoring-new-option-to-filter-node-related-dashboards-by-node-role"]
==== New option to filter node-related dashboards by node role
In the {product-title} web console, you can now filter data in node-related monitoring dashboards based on node roles.
You can use this new filter to quickly select relevant node roles if you want to see dashboard data only for nodes with certain roles, such as worker nodes.

[id="ocp-4-13-monitoring-new-option-to-enable-metrics-collection-profiles-technology-preview"]
==== New option to enable metrics collection profiles (Technology Preview)
This release introduces a Technology Preview feature for default platform monitoring in which an administrator can set a metrics collection profile to collect either the default amount of metrics data or a minimal amount of metrics data.
When you enable the minimal profile, basic monitoring features such as alerting continue to work, but the CPU and memory resources required by Prometheus decrease.

[id="ocp-4-13-scalability-and-performance"]
=== Scalability and performance

[id="ocp-4-13-NUMA-scheduler-ga"]
==== NUMA-aware scheduling with the NUMA Resources Operator is generally available

NUMA-aware scheduling with the NUMA Resources Operator was previously introduced as a Technology Preview in {product-title} 4.10 and is now generally available in {product-title} {product-version}.

The NUMA Resources Operator deploys a NUMA-aware secondary scheduler that makes scheduling decisions for workloads based on a complete picture of available NUMA zones in clusters. This enhanced NUMA-aware scheduling ensures that latency-sensitive workloads are processed in a single NUMA zone for maximum efficiency and performance.

This update adds the following features:

* Fine-tuning of API polling for NUMA resource reports.
* Configuration options at the node group level for the node topology exporter.

For more information, see xref:../scalability_and_performance/cnf-numa-aware-scheduling.adoc#cnf-about-numa-aware-scheduling_numa-aware[Scheduling NUMA-aware workloads].

[id="ocp-4-13-ran-workload-partitioning-three-node-standard-node"]
==== Support for workload partitioning for three-node clusters and standard clusters (Technology Preview)

Before this update, workload partitioning was supported for {sno} clusters only.
Now, you can also configure workload partitioning for three-node compact clusters and standard clusters.
Use workload partitioning to isolate {product-title} services, cluster management workloads, and infrastructure pods to run on a reserved set of CPUs.

For more information, see xref:../scalability_and_performance/enabling-workload-partitioning.adoc#enabling-workload-partitioning[Workload partitioning].

[id="ocp-4-13-configuring-power-states-using-ztp"]
==== Configuring power states using {ztp}

{product-title} 4.12 introduced the ability to set power states for critical and non-critical workloads.
In {product-title} 4.13, you can now configure power states with {ztp}.

For more information about the feature, see xref:../scalability_and_performance/ztp_far_edge/ztp-advanced-policy-config.adoc#ztp-using-pgt-to-configure-power-saving-states_ztp-advanced-policy-config[Configuring power states using PolicyGenTemplates CRs].

[id="ocp-4-13-scalability-and-performance-talm-updates"]
==== Pre-caching container images for managed cluster updates with {cgu-operator} and {ztp}

This release adds two new {cgu-operator-first} features for use with {ztp}:

* A new check ensures that there is sufficient available disk space on the managed cluster host before cluster updates.
Now, during container image pre-caching, {cgu-operator} compares the available host disk space with the estimated {product-title} image size to ensure that there is enough disk space on the host.

* A new `excludePrecachePatterns` field in the `ConfigMap` CR is available that controls which pre-cache images {cgu-operator} downloads to the cluster host before an update.

For more information see xref:../scalability_and_performance/cnf-talm-for-cluster-upgrades.adoc#talo-precache-feature-image-filter_cnf-topology-aware-lifecycle-manager[Using the container image pre-cache filter].

[id="ocp-4-13-ran-http-transport"]
==== HTTP transport replaces AMQP for PTP and bare-metal events (Technology Preview)

HTTP is now the default transport in the PTP and bare-metal events infrastructure.
AMQ Interconnect is end of life (EOL) from 30 June 2024.
When you use HTTP transport for PTP and bare-metal events, you must persist the events subscription in the cluster using a `PersistentVolume` resource.

For more information, see xref:../networking/using-ptp.adoc#cnf-about-ptp-fast-event-notifications-framework_using-ptp[About the PTP fast event notifications framework].

[id="ocp-4-13-ran-westport-channel-grandmaster"]
==== Support for Intel E810 Westport Channel NIC as PTP grandmaster clock (Technology Preview)

You can now configure the Intel E810 Westport Channel NIC as a PTP grandmaster clock by using the PTP Operator.
PTP grandmaster clocks use `ts2phc` (time stamp 2 physical clock) for system clock and network time synchronization.

For more information, see xref:../networking/using-ptp.adoc#configuring-linuxptp-services-as-grandmaster-clock_using-ptp[Configuring linuxptp services as a grandmaster clock].

[id="ocp-413-ran-ztp-crun-container-runtime"]
==== Configuring crun as the default container runtime for managed clusters in {ztp}
A `ContainerRuntimeConfig` CR that configures crun as the default container runtime has been added to the GitOps ZTP `ztp-site-generate` container.

For optimal performance in clusters that you install with {ztp}, enable crun for control plane and worker nodes in {sno}, {3no}, and standard clusters alongside additional Day 0 installation manifest CRs.

For more information, see xref:../scalability_and_performance/ztp_far_edge/ztp-reference-cluster-configuration-for-vdu.adoc#ztp-sno-du-configuring-crun-container-runtime_sno-configure-for-vdu[Configuring crun as the default container runtime].

[id="ocp-4-13-etcd-overview"]
==== Documentation enhancement: Overview of etcd is now available

An overview of etcd, including the benefits it provides and how it works, is now available in the {product-title} documentation. As the primary data store for Kubernetes, etcd provides a reliable approach to cluster configuration and management on {product-title} through the etcd Operator. For more information, see xref:../architecture/control-plane.adoc#etcd-overview_control-plane[Overview of etcd].

[id="ocp-4-13-insights-operator"]
=== Insights Operator
// https://issues.redhat.com/browse/OCPBUGS-6832
The Insights Operator can now collect the `openshift_apps_deploymentconfigs_strategy_total` metric. This metric gathers deployment strategy information from a deployment's configuration.

[id="ocp-4-13-hcp"]
=== Hosted control planes (Technology Preview)

[id="ocp-4-13-hcp-book"]
==== Hosted control planes section is now available in the documentation

The {product-title} documentation now includes a section dedicated to hosted control planes, where you can find an overview of the feature and information about configuring and managing hosted clusters. For more information, see xref:../hosted_control_planes/index.adoc#hosted-control-planes-overview_hcp-overview[Hosted control planes].

[id="ocp-4-13-hosted-control-planes-updates"]
==== Updating hosted control planes

The {product-title} documentation now includes information about updating hosted control planes. Updating hosted control planes involves updating the hosted cluster and the node pools. For more information, see xref:../hosted_control_planes/hcp-managing.adoc#updates-for-hosted-control-planes_hcp-managing[Updates for hosted control planes].

[id="install-sno-requirements-for-installing-on-a-single-node"]
=== Requirements for installing {product-title} on a single node

{product-version} now supports `x86_64` and `arm64` CPU architectures.

[id="ocp-4-13-notable-technical-changes"]
== Notable technical changes

{product-title} {product-version} introduces the following notable technical changes.

// Note: use [discrete] for these sub-headings.

[discrete]
[id="ocp-4-13-cluster-cloud-controller-manager-operator"]
=== Cloud controller managers for additional cloud providers

The Kubernetes community plans to deprecate the use of the Kubernetes controller manager to interact with underlying cloud platforms in favor of using cloud controller managers. As a result, there is no plan to add Kubernetes controller manager support for any new cloud platforms.

The Nutanix implementation that is added in this release of {product-title} uses cloud controller managers. In addition, this release introduces the General Availability of using cloud controller managers for VMware vSphere.

To learn more about the cloud controller manager, see the link:https://kubernetes.io/docs/concepts/architecture/cloud-controller/[Kubernetes Cloud Controller Manager documentation].

To manage the cloud controller manager and cloud node manager deployments and lifecycles, use the Cluster Cloud Controller Manager Operator.

For more information, see the xref:../operators/operator-reference.adoc#cluster-cloud-controller-manager-operator_cluster-operators-ref[Cluster Cloud Controller Manager Operator] entry in the _Platform Operators reference_.

[discrete]
[id="ocp-4-13-mco-certificate-changes"]
=== The MCD now syncs kubelet CA certificates on paused pools

Previously, the Machine Config Operator (MCO) updated the kubelet client certificate authority (CA) certificate, `/etc/kubernetes/kubelet-ca.crt`, as a part of the regular machine config update. Starting with {product-title} {product-version}, the `kubelet-ca.crt` no longer gets updated as a part of the regular machine config update. As a result of this change, the Machine Config Daemon (MCD) automatically keeps the `kubelet-ca.crt` up to date whenever changes to the certificate occur.

Also, if a machine config pool is paused, the MCD is now able to push the newly rotated certificates to those nodes. A new rendered machine config, which contains the changes to the certificate, is generated for the pool, like in previous versions. The pool will indicate that an update is required; this condition will be removed in a future release of this product. However, because the certificate is updated separately, it is safe to keep the pool paused, assuming there are no further updates.

Also, the `MachineConfigControllerPausedPoolKubeletCA` alert has been removed, because the nodes should always have the most up-to-date `kubelet-ca.crt`.

[discrete]
[id="ocp-4-13-rhcos-ssh-key-location"]
=== Change in SSH key location
{product-title} {product-version} introduces a {op-system-base} 9.2 based {op-system}. Before this update, SSH keys were located in `/home/core/.ssh/authorized_keys` on {op-system}. With this update, on {op-system-base} 9.2 based {op-system}, SSH keys are located in `/home/core/.ssh/authorized_keys.d/ignition`.

[discrete]
[id="ocp-4-13-psa-restricted-enforcement"]
=== Future restricted enforcement for pod security admission

Currently, pod security violations are shown as warnings and logged in the audit logs, but do not cause the pod to be rejected.

Global restricted enforcement for pod security admission is currently planned for the next minor release of {product-title}. When this restricted enforcement is enabled, pods with pod security violations will be rejected.

To prepare for this upcoming change, ensure that your workloads match the pod security admission profile that applies to them. Workloads that are not configured according to the enforced security standards defined globally or at the namespace level will be rejected. The `restricted-v2` SCC admits workloads according to the link:https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted[Restricted] Kubernetes definition.

If you are receiving pod security violations, see the following resources:

* See xref:../authentication/understanding-and-managing-pod-security-admission.adoc#security-context-constraints-psa-alert-eval_understanding-and-managing-pod-security-admission[Identifying pod security violations] for information about how to find which workloads are causing pod security violations.

* See xref:../authentication/understanding-and-managing-pod-security-admission.adoc#security-context-constraints-psa-synchronization_understanding-and-managing-pod-security-admission[Security context constraint synchronization with pod security standards] to understand when pod security admission label synchronization is performed. Pod security admission labels are not synchronized in certain situations, such as the following situations:
** The workload is running in a system-created namespace that is prefixed with `openshift-`.
** The workload is running on a pod that was created directly without a pod controller.

* If necessary, you can set a custom admission profile on the namespace or pod by setting the `pod-security.kubernetes.io/enforce` label.

[discrete]
[id="ocp-4-13-oc-mirror-openshift-endpoint"]
=== The oc-mirror plugin now retrieves graph data container images from an OpenShift API endpoint

The oc-mirror {oc-first} plugin now downloads the graph data tarball from an OpenShift API endpoint instead of downloading the entire graph data repository from GitHub. Retrieving this data from Red Hat instead of an external vendor is more suitable for users with stringent security and compliance restrictions on external content.

The data that the oc-mirror plugin downloads now excludes content that is in the graph data repository but not needed by the OpenShift Update Service. The container also uses UBI Micro as the base image instead of UBI, resulting in a container image that is significantly smaller than before.

These changes do not affect the user workflow for the oc-mirror plugin.

[discrete]
[id="ocp-4-13-graph-data-openshift-endpoint"]
=== The Dockerfile for the graph data container image is now retrieved from an OpenShift API endpoint

If you are creating a graph data container image for the OpenShift Update Service by using the Dockerfile, note that the graph data tarball is now downloaded from an OpenShift API endpoint instead of GitHub.

For more information, see xref:../updating/updating-restricted-network-cluster/restricted-network-update-osus.adoc#update-service-graph-data_updating-restricted-network-cluster-osus[Creating the OpenShift Update Service graph data container image].

[discrete]
[id="ocp-4-13-ocm-nodeip-configuration-service"]
=== The nodeip-configuration service is now enabled on a vSphere user-provisioned infrastructure cluster

In {product-title} {product-version}, the `nodeip-configuration` service is now enabled on a vSphere user-provisioned infrastructure cluster. This service determines the network interface controller (NIC) that {product-title} uses for communication with the Kubernetes API server when the node boots. In rare circumstances, the service might select an incorrect node IP after an upgrade. If this happens, you can use the `NODEIP_HINT` feature to restore the original node IP. See xref:../support/troubleshooting/troubleshooting-network-issues.adoc#troubleshooting-network-issues[Troubleshooting network issues].

[discrete]
[id="ocp-4-13-operator-sdk-1-28-z"]
=== Operator SDK 1.28

{product-title} {product-version} supports Operator SDK 1.28. See xref:../cli_reference/osdk/cli-osdk-install.adoc#cli-osdk-install[Installing the Operator SDK CLI] to install or update to this latest version.

[NOTE]
====
Operator SDK 1.28 supports Kubernetes 1.26.

====

If you have Operator projects that were previously created or maintained with Operator SDK 1.25, update your projects to keep compatibility with Operator SDK 1.28.

* xref:../operators/operator_sdk/golang/osdk-golang-updating-projects.adoc#osdk-upgrading-projects_osdk-golang-updating-projects[Updating Go-based Operator projects]

* xref:../operators/operator_sdk/ansible/osdk-ansible-updating-projects.adoc#osdk-upgrading-projects_osdk-ansible-updating-projects[Updating Ansible-based Operator projects]

* xref:../operators/operator_sdk/helm/osdk-helm-updating-projects.adoc#osdk-upgrading-projects_osdk-helm-updating-projects[Updating Helm-based Operator projects]

* xref:../operators/operator_sdk/helm/osdk-hybrid-helm-updating-projects.adoc#osdk-upgrading-projects_osdk-hybrid-helm-updating-projects[Updating Hybrid Helm-based Operator projects]

* xref:../operators/operator_sdk/java/osdk-java-updating-projects.adoc#osdk-upgrading-projects_osdk-java-updating-projects[Updating Java-based Operator projects]

[discrete]
[id="ocp-4-13-rhcos-symbolic-disk-naming-change"]
=== Change in disk ordering behavior for {op-system} based on {op-system-base} 9.2
{product-title} {product-version} introduces a {op-system-base} 9.2 based {op-system}. With this update, symbolic disk naming can change across reboots. This can cause issues if you apply configuration files after installation or when provisioning a node that references a disk which uses symbolic naming, such as `/dev/sda`, for creating services. The effects of this issue depend on the component you are configuring. It is recommended to use a specific naming scheme for devices, including for any specific disk references, such as `dev/disk/by-id`.

With this change, you might need to adjust existing automation workflows in the cases where monitoring collects information about the install device for each node.

For more information, see the link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_file_systems/assembly_overview-of-persistent-naming-attributes_managing-file-systems[{op-system-base} documentation].

[discrete]
[id="ocp-4-13-hosted-control-planes-moved-content"]
=== Documentation about backup, restore, and disaster recovery for hosted control planes moved
In the documentation for {product-title} {product-version}, the procedures to back up and restore etcd on a hosted cluster and to restore a hosted cluster within an AWS region were moved from the "Backup and restore" section to the "Hosted control planes" section. The content itself was not changed.

[id="ocp-4-13-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* _General Availability_
* _Deprecated_
* _Removed_

[discrete]
=== Operator deprecated and removed features

.Operator deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|SQLite database format for Operator catalogs
|Deprecated
|Deprecated
|Deprecated

|====

[discrete]
=== Images deprecated and removed features

.Images deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|`ImageChangesInProgress` condition for Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated

|`MigrationInProgress` condition for Cluster Samples Operator
|Deprecated
|Deprecated
|Deprecated

|====

//[discrete]
//=== Monitoring deprecated and removed features

//.Monitoring deprecated and removed tracker
//[cols="4,1,1,1",options="header"]
//|====
//|Feature |4.11 |4.12 |4.13

//|====

[discrete]
=== Installation deprecated and removed features

.Installation deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|vSphere 7.0 Update 1 or earlier
|Deprecated
|Deprecated
|Removed ^[1]^

|VMware ESXi 7.0 Update 1 or earlier
|Deprecated
|Deprecated
|Removed ^[1]^

|CoreDNS wildcard queries for the `cluster.local` domain
|General Availability
|Deprecated
|Deprecated

|`ingressVIP` and `apiVIP` settings in the `install-config.yaml` file for installer-provisioned infrastructure clusters
|General Availability
|Deprecated
|Deprecated

|====
[.small]
--
1. For {product-title} {product-version}, you must install the {product-title} cluster on a VMware vSphere version 7.0 Update 2 or later instance, including VMware vSphere version 8.0, that meets the requirements for the components that you use.
--
//[discrete]
//=== Updating clusters deprecated and removed features

//.Updating clusters deprecated and removed tracker
//[cols="4,1,1,1",options="header"]
//|====
//|Feature |4.11 |4.12 |4.13

//|====

[discrete]
=== Storage deprecated and removed features

.Storage deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Persistent storage using FlexVolume
|Deprecated
|Deprecated
|Deprecated

|====

//[discrete]
//=== Authentication and authorization deprecated and removed features

//.Authentication and authorization deprecated and removed tracker
//[cols="4,1,1,1",options="header"]
//|====
//|Feature |4.11 |4.12 |4.13

//|====

[discrete]
=== Specialized hardware and driver enablement deprecated and removed features

.Specialized hardware and driver enablement deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Special Resource Operator (SRO)
|Technology Preview
|Removed
|Removed

|====

[discrete]
=== Multi-architecture deprecated and removed features

.Multi-architecture deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|IBM Power8 all models (`ppc64le`)
|General Availability
|Deprecated
|Removed

|{ibmpowerProductName} AC922 (`ppc64le`)
|General Availability
|Deprecated
|Removed

|{ibmpowerProductName} IC922 (`ppc64le`)
|General Availability
|Deprecated
|Removed

|{ibmpowerProductName} LC922 (`ppc64le`)
|General Availability
|Deprecated
|Removed

|IBM z13 all models (`s390x`)
|General Availability
|Deprecated
|Removed

|{linuxoneProductName} Emperor (`s390x`)
|General Availability
|Deprecated
|Removed

|{linuxoneProductName} Rockhopper (`s390x`)
|General Availability
|Deprecated
|Removed

|AMD64 (x86_64) v1 CPU
|General Availability
|Deprecated
|Removed

|====

[discrete]
=== Networking deprecated and removed features

.Networking deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Kuryr on {rh-openstack}
|General Availability
|Deprecated
|Deprecated

|====

[discrete]
=== Web console deprecated and removed features

.Web console deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Multicluster console
|Technology Preview
|Technology Preview
|Removed

|====
[discrete]
=== Node deprecated and removed features

.Node deprecated and removed tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|`ImageContentSourcePolicy` (ICSP) objects
|General Availability
|General Availability
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/zone`
|General Availability
|General Availability
|Deprecated

|Kubernetes topology label `failure-domain.beta.kubernetes.io/region`
|General Availability
|General Availability
|Deprecated

|====

[id="ocp-4-13-deprecated-features"]
=== Deprecated features

[id="ocp-4-13-rhv-deprecations"]
==== Red Hat Virtualization (RHV) as a host platform for {product-title} will be deprecated

Red Hat Virtualization (RHV) as a host platform for {product-title} is now deprecated, and will be removed in the next {product-title} release, currently planned as {product-title} 4.14.

[id="ocp-4-13-ne-deprecations"]
==== Wildcard DNS queries for the cluster.local domain are deprecated

CoreDNS will stop supporting wildcard DNS queries for names under the `cluster.local` domain. These queries will resolve in {product-title} {product-version} as they do in earlier versions, but support will be removed from a future {product-title} release.

[id="ocp-4-13-ne-kuryr"]
==== Kuryr support for clusters that run on {rh-openstack}

In {product-title} 4.12, support for Kuryr on clusters that run on {rh-openstack} was deprecated. Support will be removed no earlier than {product-title} 4.14.

[id="ocp-4-13-icsp"]
==== ImageContentSourcePolicy objects

The `ImageContentSourcePolicy` (ICSP) object is now deprecated. You can now use an `ImageDigestMirrorSet` (IDMS) object to pull images by using digest specifications or an `ImageTagMirrorSet` (ITMS) object to pull images by using image tags.

For more information on these new objects, see xref:../post_installation_configuration/preparing-for-users.adoc#images-configuration-registry-mirror_post-install-preparing-for-users[Configuring image registry repository mirroring].

For more information on converting existing ICSP YAML files to IDMS YAML files, see xref:../post_installation_configuration/preparing-for-users.adoc#images-configuration-registry-mirror-convert_post-install-preparing-for-users[Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring].

[id="ocp-4-13-toolbox-deprecation"]
==== Toolbox is deprecated in {op-system}
The toolbox script is deprecated and support will be removed from a future {product-title} release.

[id="ocp-4-13-rhel-9-device-driver-deprecation"]
==== {op-system-base} 9 driver deprecations
{product-title} {product-version} introduces a {op-system-base} 9.2 based {op-system}. Some kernel device drivers are deprecated in {op-system-base} 9. See the link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/considerations_in_adopting_rhel_9/assembly_hardware-enablement_considerations-in-adopting-rhel-9[{op-system-base} documentation] for more information.

[id="ocp-4-13-vSphere-configuration-parameters"]
==== VMware vSphere configuration parameters

{product-title} {product-version} deprecates the following vSphere configuration parameters. You can continue to use these parameters, but the installation program does not automatically specify these parameters in the `install-config.yaml` file.

* `platform.vsphere.vCenter`
* `platform.vsphere.username`
* `platform.vsphere.password`
* `platform.vsphere.datacenter`
* `platform.vsphere.defaultDatastore`
* `platform.vsphere.cluster`
* `platform.vsphere.folder`
* `platform.vsphere.resourcePool`
* `platform.vsphere.apiVIP`
* `platform.vsphere.ingressVIP`
* `platform.vsphere.network`

For more information, see xref:../installing/installing_vsphere/installing-vsphere-installer-provisioned-customizations.adoc#deprecated-parameters-vsphere_installing-vsphere-installer-provisioned-customizations[Deprecated VMware vSphere configuration parameters].

[id="ocp-4-13-node-dep-topology-labels"]
==== Kubernetes topology labels

Two commonly used Kubernetes topology labels are being replaced. The `failure-domain.beta.kubernetes.io/zone` label is replaced with `topology.kubernetes.io/zone`. The `failure-domain.beta.kubernetes.io/region` label is replaced with `topology.kubernetes.io/region`. The replacement labels are available starting with Kubernetes 1.17 and {product-title} version 4.4.

Currently, both the deprecated and replacement labels are supported, but support for the deprecated labels is planned to be removed in a future release. To prepare for the removal, you can modify any resources (such as volumes, deployments, or other workloads) that reference the deprecated labels to use the replacement labels instead.

[id="ocp-4-13-removed-features"]
=== Removed features

[id="ocp-4-13-removed-kube-1-26-apis"]
==== Beta APIs removed from Kubernetes 1.26

Kubernetes 1.26 removed the following deprecated APIs, so you must migrate manifests and API clients to use the appropriate API version. For more information about migrating removed APIs, see the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-26[Kubernetes documentation].

.APIs removed from Kubernetes 1.26
[cols="2,2,2",options="header",]
|===
|Resource |Removed API |Migrate to

|`FlowSchema`
|`flowcontrol.apiserver.k8s.io/v1beta1`
|`flowcontrol.apiserver.k8s.io/v1beta3`

|`HorizontalPodAutoscaler`
|`autoscaling/v2beta2`
|`autoscaling/v2`

|`PriorityLevelConfiguration`
|`flowcontrol.apiserver.k8s.io/v1beta1`
|`flowcontrol.apiserver.k8s.io/v1beta3`

|===

[id="ocp-4-13-future-removals"]
=== Future Kubernetes API removals

The next minor release of {product-title} is expected to use Kubernetes 1.27. Currently, Kubernetes 1.27 is scheduled to remove a deprecated API.

See the link:https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-27[Deprecated API Migration Guide] in the upstream Kubernetes documentation for the list of planned Kubernetes API removals.

See link:https://access.redhat.com/articles/6955985[Navigating Kubernetes API deprecations and removals] for information about how to check your cluster for Kubernetes APIs that are planned for removal.

[id="ocp-4-13-z13-power8-x86v1-remove"]
==== Specific hardware models on ppc64le, s390x, and x86_64 v1 CPU architectures are removed

In {product-title} 4.13, support for {op-system} functionality is removed for the following deprecated hardware models:

* IBM Power8 all models (`ppc64le`)
* {ibmpowerProductName} AC922 (`ppc64le`)
* {ibmpowerProductName} IC922 (`ppc64le`)
* {ibmpowerProductName} LC922 (`ppc64le`)
* IBM z13 all models (`s390x`)
* {linuxoneProductName} Emperor (`s390x`)
* {linuxoneProductName} Rockhopper (`s390x`)
* AMD64 (`x86_64`) v1 CPU

[id="ocp-4-13-bug-fixes"]
== Bug fixes

//[discrete]
//[id="ocp-4-13-api-auth-bug-fixes"]
//==== API Server and Authentication

//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / TALO
//Telco Edge / ZTP

[discrete]
[id="ocp-4-13-bare-metal-hardware-bug-fixes"]
==== Bare Metal Hardware Provisioning

* Previously, when you attempted to deploy an {product-title} cluster node on a server that is configured with the Integrated Lights-Out (iLO) Management Interface Driver, provisioning of the node would fail. The failure occurs because of a missing `[ilo]/use_web_server_for_images` configuration parameter in the iLO drive that caused the driver to attempt to use object storage as the default storage mechanism. Object storage is not present in the product. With this update, {product-title} {product-version} and later versions includes `[ilo]/use_web_server_for_images` in the iLO driver's configuration, so that the driver uses a web server that runs in the `metal3` pod. (link:https://issues.redhat.com/browse/OCPBUGS-5068[*OCPBUGS-5068*])

//[discrete]
//[id="ocp-4-13-builds-bug-fixes"]
//==== Builds

[discrete]
[id="ocp-4-13-cloud-compute-bug-fixes"]
==== Cloud Compute

* For some configurations of Google Cloud Platform clusters, the internal load balancer uses instance groups that are created by the installation program. Previously, when a control plane machine was replaced manually, the new control plane node was not assigned to a control plane instance group. This prevented the node from being reachable via the internal load balancer. To resolve the issue, administrators had to manually move the control plane machine to the correct instance group by using the Google Cloud console.
+
With this release, replacement control plane nodes are assigned to the correct instance group.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1970464[*BZ#1970464*], link:https://issues.redhat.com/browse/OCPCLOUD-1562[*OCPCLOUD-1562*])

* Previously, a compute machine set for Google Cloud Platform could try to reconcile invalid machines, which caused them to be stuck with no phase assigned. With this release, machines with an invalid configuration are put into the `Failed` state. (link:https://issues.redhat.com/browse/OCPBUGS-4574[*OCPBUGS-4574*])

* Previously, a control plane machine set replica was considered ready when its backing machine entered the `Running` state, even though the linked node needed to also be ready for the replica to be considered ready. With this release, a node and its machine must be in the `Ready` state for the control plane machine set replica to be considered ready. (link:https://issues.redhat.com/browse/OCPBUGS-8424[*OCPBUGS-8424*])

* Previously, the `mapi_instance_create_failed` alert metric did not start when an error occurred for the Accelerated Networking feature on Microsoft Azure clusters. This release adds the missing alert so that clusters with Accelerated Networking enabled can generate alerts when required. (link:https://issues.redhat.com/browse/OCPBUGS-5235[*OCPBUGS-5235*])

* Previously, when a machine entered the `Running` state, no further changes to the state of its node were checked for. The previous resolution of link:https://issues.redhat.com/browse/OCPBUGS-8424[OCPBUGS-8424] introduced the requirement for a node and its machine to be in the `Ready` state for the control plane machine set replica to be considered ready. As a result, if the control plane machine set missed the stage when the node and machine were ready, its replica could not become ready. This behavior caused the Control Plane Machine Set Operator to become unavailable, blocking upgrades. With this release, when a machine is running but the node is not ready, the node is checked at regular intervals until it becomes ready. This fix prevents the Control Plane Machine Set Operator from becoming unavailable and blocking upgrades. (link:https://issues.redhat.com/browse/OCPBUGS-10771[*OCPBUGS-10771*])

* Previously, when a machine health check exceeded the `maxUnhealthy` threshold and generated an alert, the metric was not reset when the cluster became healthy enough to reconcile machine health checks successfully, and the alert continued. With this release, the logic that determines when to trigger an alert is improved so that the alert now clears when the cluster is healthy. (link:https://issues.redhat.com/browse/OCPBUGS-4725[*OCPBUGS-4725*])

* The previous resolution of link:https://issues.redhat.com/browse/OCPBUGS-5546[OCPBUGS-5546] removed the `clusterName` assignment of `MachineConfig.Name` in the machine configuration object. As a result, the value of the parameter was an empty string and, when it was combined with the value of `machineName` to create an IP address name, it created an invalid value. The invalid value caused machines to fail during provisioning. With this release, the value for `clusterName` is obtained from the infrastructure object so that it creates a valid IP address name. (link:https://issues.redhat.com/browse/OCPBUGS-7696[*OCPBUGS-7696*])

* The Kubernetes 1.26 release introduced changes to the node infrastructure, such as removing an unhealthy node with a `NotReady` status from the public load balancer to prevent the node from receiving routing traffic. These changes impacted a node that ran inside a cluster on Microsoft Azure. As a result, the node was unable to regain a `Ready` status and subsequently establish an outbound connection. With this update, a node marked with a `NotReady` status is now detected by `kube-proxy` health probes without the need of node detachment from the public load balancer. This means that a node can retain an outbound internet connection throughout these phases. (link:https://issues.redhat.com/browse/OCPBUGS-7359[*OCPBUGS-7359*])

[discrete]
[id="ocp-4-13-cloud-cred-operator-bug-fixes"]
==== Cloud Credential Operator

* Amazon Simple Storage Service (Amazon S3) updated their Amazon S3 bucket configuration so a bucket created in an Amazon Web Services (AWS) region has S3 Block Public Access enabled and access control limits (ACLs) disabled by default. This configuration limits S3 bucket resources to private use. The {product-title} {product-version} updates the CCO utility (`ccoctl`) and the installation program to account for the default S3 bucket configuration so that S3 bucket resources are publicly available. (link:https://issues.redhat.com/browse/OCPBUGS-11706[*OCPBUGS-11706*] and link:https://issues.redhat.com/browse/OCPBUGS-11661[*OCPBUGS-11661*])

[discrete]
[id="ocp-4-13-dev-console-bug-fixes"]
==== Developer Console

* Previously, the {product-title} used API version `v1alpha1` for Knative Serving and Eventing but because of a bug, API version `v1beta1` was not supported. With this fix, the {product-title} supports both the API versions. (link:https://issues.redhat.com/browse/OCPBUGS-5164[*OCPBUGS-5164*])

* Previously, when editing any pipeline in the {product-title} console, the correct data was not rendered in the *Pipeline builder* and *YAML view* configuration options. Because of this issue, you could not edit the pipeline in the *Pipeline builder*. With this update, data is parsed correctly and you can edit the pipeline using the builder. (link:https://issues.redhat.com/browse/OCPBUGS-5016[*OCPBUGS-5016*])

* Previously, the topology sidebar did not display updated information. When you updated the resources directly from the topology sidebar, you had to reopen the sidebar to see the changes. With this fix, the updated resources are displayed correctly. As a result, you can see the latest changes directly in the topology sidebar. (link:https://issues.redhat.com/browse/OCPBUGS-4691[*OCPBUGS-4691*])

* Previously, the *Samples* page in the {product-title} did not allow distinguishing between the types of samples listed. With this fix, you can identify the sample from the badges displayed on the *Samples* page. (link:https://issues.redhat.com/browse/OCPBUGS-10679[*OCPBUGS-10679*])

[discrete]
[id="ocp-4-13-cloud-etcd-operator-bug-fixes"]
==== etcd Cluster Operator

* Previously, the Control Plane Machine Set Operator attempted to recreate a control plane machine before the cluster bootstrapping completed. This resulted in the removal of the bootstrap node from the etcd cluster membership that caused etcd quorum loss and the cluster to go offline. With this update, the Control Plane Machine Set Operator only recreates a control plane machine after the etcd Cluster Operator removes the bootstrap node. (link:https://issues.redhat.com/browse/OCPBUGS-10960[*OCPBUGS-10960*])

[discrete]
[id="ocp-4-13-hosted-control-plane-bug-fixes"]
==== Hosted Control Plane

* Previously, the `HostedControlPlane` object did not identify changes to scheduler profiles that were set by a `HostedCluster` resource. Further, `HostedControlPlane` did not propagate changes to the scheduler, so the scheduler did not restart control plane pods for them to receive the latest scheduler profile changes. With this update, `HostedControlPlane` now recognizes changes to scheduler profiles and then dynamically restarts the scheduler, so that the scheduler can apply profile changes to pods. (link:https://issues.redhat.com/browse/OCPBUGS-7091[*OCPBUGS-7091*])

* Previously, a hosted cluster did not account for OpenID Connect (OIDC) providers, `oidc`, unavailability that caused the deletion of `machine` and `machineset` objects to stale. With this update, a hosted cluster can detect the status of an unavailable `odic` provider, so that the deletion of `machine` and `machineset` objects to not stale because of an unavailable `oidc` provider. (link:https://issues.redhat.com/browse/OCPBUGS-10227[*OCPBUGS-10227*])

* Previously, the `spec.metadata.annotations` parameter value in an Amazon Web Services (AWS) compute machine set was not copied from a compute machine to its node. This caused the node to have missing annotations specified in the compute machine set. With this release, annotations are correctly applied to the node.
(link:https://issues.redhat.com/browse/OCPBUGS-4566[*OCPBUGS-4566*])

//[discrete]
//[id="ocp-4-13-image-registry-bug-fixes"]
//==== Image Registry

[discrete]
[id="ocp-4-13-installer-bug-fixes"]
==== Installer

* Previously, DNS records that the installation program created were not removed when uninstalling a private cluster. With this update, the installation program now correctly removes these DNS records. (link:https://issues.redhat.com/browse/OCPBUGS-7973[*OCPBUGS-7973*])

* Previously, the bare-metal installer-provisioned infrastructure used port 80 for providing images to the Baseboard Management Controller (BMC) and the deployment agent. Security risks could exist with port 80, because this port is commonly chosen for internet communications. The bare metal installer-provisioned infrastructure now uses port 6180 for serving images that are used by the `metal3` pod on deployed clusters. (link:https://issues.redhat.com/browse/OCPBUGS-8511[*OCPBUGS-8511*])

* Previously, SSH access to bootstrap and cluster nodes failed when the bastion host ran in the same VPC network as the cluster nodes. Additionally, this configuration caused SSH access from the temporary bootstrap node to the cluster nodes to fail. These issues are now fixed by updating the IBM Cloud security group rules to support SSH traffic between the temporary bootstrap node and cluster nodes, and to support SSH traffic from a bastion host to cluster nodes on the same VPC network. Log and debug information can be accurately collected for analysis during installer-provisioned infrastructure failure. (link:https://issues.redhat.com/browse/OCPBUGS-8035[*OCPBUGS-8035*])

* Previously, if you configured the rendezvous IP to the IP address of the host that has a `role` parameter set to `worker` and you generated an ISO image, the Agent-based installer would fail to install the cluster. Now, when you attempt to generate an ISO image based on this configuration, you will receive a validation failure message. On receiving this message, you must update the `rendezvousIP` field in the `agent-config.yaml` file to use the IP of a host with the `master` role. (link:https://issues.redhat.com/browse/OCPBUGS-2088[*OCPBUGS-2088*])

* Previously, the installation program did not accept the following new regions defined in the `aws-sdk-go` library: `ap-south-2`, `ap-southeast-4`, `eu-central-2`, `eu-south-2`, and `me-central-1`. When you used the installation program to create the installation configuration file, the installation program would not list these new regions or accept manual entries for these regions. With this update, the installation program supports these regions and you can specify them when you create the installation configuration file. (link:https://issues.redhat.com/browse/OCPBUGS-10213[*OCPBUGS-10213*])

* Previously, an issue existed with the code base that sets `Machine.PrimaryNetwork` based on the `controlPlane.platform.openstack.failureDomain` field in the `install-config.yaml` file. This issue impacts {product-title} that runs with Kuryr from identifying the port on a {rh-openstack-first} subnet that control plane machines use for communicating between them. With this update, when you set `control-plane` for `portTarget` in the `failureDomain` Technology Preview component, the installation program sets the port's information in the `Machine.PrimaryNetwork` field, so that your {product-title} cluster successfully runs with Kuryr. (link:https://issues.redhat.com/browse/OCPBUGS-10658[*OCPBUGS-10658*])

* Previously, uninstalling an AWS cluster that was deployed to the `us-gov-west-1` region failed because AWS resources could not be untagged. This resulted in the process going into an infinite loop, where the installation program tried to untag the resources. This update prevents the retry. As a result, uninstalling the cluster succeeds. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2070744[*BZ#2070744*])

* Previously, a private {product-title} cluster running on Google Cloud Platform (GCP) would receive additional firewall rules so that GCP could perform health checks for both internal and external load balancers. Private clusters only use internal load balancers, so performing health checks for external load balancers is unnecessary. With this update, a private cluster that runs on GCP no longer receives these additional firewall rules that stemmed from health checks for external load balancers. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2110982[*BZ#2110982*])

//[discrete]
//[id="ocp-4-13-kube-controller-bug-fixes"]
//==== Kubernetes Controller Manager

[discrete]
[id="ocp-4-13-kube-scheduler-bug-fixes"]
==== Kubernetes Scheduler

* Previously, when the `LifeCycleUtilization` profile was excluded to test namespace filtering, the following error was logged in the Descheduler Operator logs: `belowE0222 12:43:14.331258 1 target_config_reconciler.go:668] key failed with : only namespace exclusion supported with LowNodeUtilization`. Consequently, the descheduler cluster pod would not start. With this update, the namespace exclusion now works with the `LifeCycleUtilization` profile. (link:https://issues.redhat.com/browse/OCPBUGS-7876[*OCPBUGS-7876*])

//[discrete]
//[id="ocp-4-13-machine-config-operator-bug-fixes"]
//==== Machine Config Operator

[discrete]
[id="ocp-4-13-management-console-bug-fixes"]
==== Management Console
* Previously, user permissions were not checked when rendering the *Create Pod* button, and the button rendered for users without needed permissions. With this update, user permissions are checked when rendering the *Create Pod* button, and it renders for users for users with the needed permissions. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2005232[*BZ#2005232*])

* Previously, the `Pod` resource had a `PDB` *_add_*, *_edit_*, and *_remove_* actions in the Pod resource action menu that are not required. With this update, the actions are removed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2110565[*BZ#2110565*])

* Previously, the `PodDisruptionBudget` field on the *Details* page had an incorrect help message. With this update, the help message is now more descriptive. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2084452[*BZ#2084452*])

* Previously, when navigating to the root path of the console, the URL redirected to the *Overview* page even if metrics were disabled and it did not appear in the navigation menu. With this update, when clicking the masthead logo or navigating to the root path of the console, the URL redirects to the *project list* page if metrics are disabled. (link:https://issues.redhat.com/browse/OCPBUGS-3033[*OCPBUGS-3033*])

* Previously, the cluster dropdown was positioned so that it was not always visible, making it unclear which cluster you were viewing. With this update, the cluster dropdown is now in the masthead so the cluster dropdown is always visible, and you can always see which cluster you are viewing. (link:https://issues.redhat.com/browse/OCPBUGS-7089[*OCPBUGS-7089*])

* Previously, the node progress bars were set to display when the cluster version had a status of `Failing`, `UpdatingAndFailing`, and `Updating`, causing the node progress bars to display when the cluster is not updating. With this update, the node progress bars only display when the cluster version has a status of `UpdatingAndFailing` or `Updating`. (link:https://issues.redhat.com/browse/OCPBUGS-6049[*OCPBUGS-6049*])

* Previously, when downloading a `kubeconfig` file for a ServiceAccount, an error was displayed and the ServiceAccount token was unable to be reached. This error was due to the removal of automatically generated secrets. With this update, the download `kubeconfig` action has been removed and the error no longer occurs. (link:https://issues.redhat.com/browse/OCPBUGS-7308[*OCPBUGS-7308*])

* Previously, the *Terminal* tab on the *Node details* page displayed an error because of missing annotations that were caused by pod security measures. Without the required annotations, the node debug pod cannot start. With this update, {product-title} adds these annotations, so the node debug pod can start and the *Terminal* tab loads without any errors. (link:https://issues.redhat.com/browse/OCPBUGS-4252[*OCPBUGS-4252*])

* Previously, if a cluster administrator attempted to issue the `oc delete csv` command when uninstalling the Operator, the Operator's subscription becomes stuck. The administrator was unable to reinstall the Operator because a conflict existed with the subscription. With this update, a detailed error message displays when an administrator attempts to reinstall the uninstalled Operator. (link:https://issues.redhat.com/browse/OCPBUGS-3822[*OCPBUGS-3822*])

* Previously, if one or more existing plugins failed, the web console would not display a toast notification that prompted you to refresh the console. This action is required so that you can view a plugin after an operator adds the plugin to the console. With this update, the web console checks when the operator adds a plugin and then displays a toast notification on the console, regardless of any previously failed plugins. (link:https://issues.redhat.com/browse/OCPBUGS-10249[*OCPBUGS-10249*])

* Previously, a terminated container would render `{{label}}` and `{{exitCode}}` codes for each terminated container. With this update, the internationalization code is fixed to render a readable output message. (link:https://issues.redhat.com/browse/OCPBUGS-4206[*OCPBUGS-4206*])

* Previously, a regression was introduced causing the *Cluster Settings* page to return an error when the `clusterversion` `status.availableUpdates` had a value of `null` and `Upgradeable=False`. With this update, `status.availableUpdates` are allowed to have a `null` value. (link:https://issues.redhat.com/browse/OCPBUGS-6053[*OCPBUGS-6053*])

[discrete]
[id="ocp-4-13-monitoring-bug-fixes"]
==== Monitoring

* Previously, the Kubernetes scheduler could skip scheduling certain pods for a node that received multiple restart operations. {product-title} {product-version} counteracts this issue by including the `KubePodNotScheduled` alert for pods that cannot be scheduled within 30 minutes. (link:https://issues.redhat.com/browse/OCPBUGS-2260[*OCPBUGS-2260*])

* Previously, if more than one label was defined for Thanos Ruler, then the statefulset could enter a recreation loop because the `prometheus-operator` did not add the labels in a specified order each time it reconciled the custom resource. After this fix, the `prometheus-operator` now sorts extra labels before adding them to the statefulset. (link:https://issues.redhat.com/browse/OCPBUGS-6055[*OCPBUGS-6055*])

* With this release, the `NodeFilesystemAlmostOutOfSpace` no longer launches for certain read-only `tmpfs` instances. This change fixes an issue in which the alert launches for certain `tmpfs` mount points that were full by design. (link:https://issues.redhat.com/browse/OCPBUGS-6577[*OCPBUGS-6577*])

[discrete]
[id="ocp-4-13-networking-bug-fixes"]
==== Networking

* Previously, the Ingress Operator displayed a success message for the `updateIngressClass` function logs when an error message should be displayed. With this update, the log message for Ingress Operator is accurate. (link:https://issues.redhat.com/browse/OCPBUGS-6700[*OCPBUGS-6700*])

* Previously, the Ingress Operator did not specify `ingressClass.spec.parameters.scope`, while the Ingress Class API object specifies type `cluster` by default. This caused unnecessary updates to all Ingress Classes when the Operator starts. With this update, the Ingress Operator specifies `ingressClass.spec.parameters.scope` of type `cluster`. (link:https://issues.redhat.com/browse/OCPBUGS-6701[*OCPBUGS-6701*])

* Previously, the Ingress Operator had the wrong service name in `ensureNodePortService` log message causing incorrect information to be logged. With this update, the Ingress Operator accurately logs the service in `ensureNodePortService`. (link:https://issues.redhat.com/browse/OCPBUGS-6698[*OCPBUGS-6698*])

* Previously, in {product-title} 4.7.0 and 4.6.20, the Ingress Operator used an annotation for router pods that was specific for {product-title}. This was a temporary way to configure the liveness probe's grace period in order to fix a bug. As a result, {product-title} was required to carry a patch to implement the fix. With this update, the Ingress Operator uses `terminationGracePeriodSeconds` API field making the previous patch removable in future releases. (link:https://issues.redhat.com/browse/OCPBUGS-4703[*OCPBUGS-4703*])

* Previously, CoreDNS was using the old toolchain for building of the main binary and the old base image. With this update, {product-title} is using {product-version} for the build toolchain and the base image. (link:https://issues.redhat.com/browse/OCPBUGS-6228[*OCPBUGS-6228*])

[discrete]
[id="ocp-4-13-node-bug-fixes"]
==== Node

* Previously, the `LowNodeUtilization` strategy, which is enabled by the `LifecycleAndUtilization` descheduler profile, did not support namespace exclusion. With this release, namespaces are excluded properly when the `LifecycleAndUtilization` descheduler profile is set. (link:https://issues.redhat.com/browse/OCPBUGS-513[*OCPBUGS-513*])

* Previously, a regression in behavior caused Machine Config Operator (MCO) to create a duplicate `MachineConfig` object in the `kubeletconfig` or `containerruntimeconfig` custom resource (CR). The duplicate object degraded and the cluster failed to upgrade. With this update, the `kubeletconfig` and `containerruntimeconfig` controllers can detect any duplicate objects and then delete them. This action removes the degraded `MachineConfig` object error and does not impact the cluster upgrade operation. (link:https://issues.redhat.com/browse/OCPBUGS-7719[*OCPBUGS-7719*])

[discrete]
[id="ocp-4-13-node-tuning-operator-bug-fixes"]
==== Node Tuning Operator (NTO)

* Previously, the `hwlatdetect` tool that the Cloud-native Functions (CNF) tests image uses for running latency tests on a CNF-enabled {product-title} cluster was configured with a detection period of 10 seconds. This configuration when combined with the detection width configuration of 0.95 of a second increased the likelihood of `hwlatdetect` missing a latency spike, because the tool monitors a node about 9.5% of the time over the allocated detection period. With this update, the detection period is set to 1 second, so that the tool can now monitor nodes for about 95% of the time over the allocated detection period. The remaining 5% of monitoring time is left unallocated so that the kernel can perform system tasks. (link:https://issues.redhat.com/browse/OCPBUGS-12433[*OCPBUGS-12433*])

[discrete]
[id="ocp-4-13-openshift-cli-bug-fixes"]
==== OpenShift CLI (oc)

* Previously, `oc adm upgrade` command did not read `Failing=True` status in ClusterVersion. With this update, `oc adm upgrade` includes `Failing=True` condition information when summarizing cluster state. This raises the visibility of `Degraded=True` status for `ClusterOperators` and other issues which can impact the behavior of the current cluster or future updates. (link:https://issues.redhat.com/browse/OCPBUGS-3714[*OCPBUGS-3714*])

* Previously, the `oc-mirror` command built the catalog content in the console for OCI and FBC Operators from a mirrored disk image. Consequently, not all content of the catalog was mirrored so some content was missing from the catalog. With this update, the catalog image is built to reflect the mirrored content prior to pushing it to the destination registry resulting in a more complete catalog. (link:https://issues.redhat.com/browse/OCPBUGS-5805[*OCPBUGS-5805*])

* Previously, the oc-mirror {oc-first} plugin added the Operator catalog as an entry in the `ImageContentSourcePolicy` resource. This resource does not require this entry, because the Operator catalog is consumed directly from the destination registry in the `CatalogSource` resource. This issue impacted the cluster from receiving the release image signature resources because of an unexpected entry in the `ImageContentSourcePolicy` resource. With this update, the oc-mirror plugin removes Operator catalog entry from the `ImageContentSourcePolicy` resource, so that a cluster receives signature resources from the Operator catalog in the `CatalogSource` resource. (link:https://issues.redhat.com/browse/OCPBUGS-10320[*OCPBUGS-10320*])

[discrete]
[id="ocp-4-13-olm-bug-fixes"]
==== Operator Lifecycle Manager (OLM)

* The status of an Operator’s custom resource (CR) includes a list of components owned by the Operator. This list is ordered by `group/version/kind` (GVK), but the order of objects with the same GVK might change. If an Operator owns many components with the same GVK, it can cause Operator Lifecycle Manager (OLM) to continuously update the status of the Operator CR, because the order of its components has changed. This bug fix updates OLM so that the order of an Operator's component references is deterministic. As a result, OLM no longer attempts to update the CR repeatedly when the list of components remains constant. (link:https://issues.redhat.com/browse/OCPBUGS-2556[*OCPBUGS-2556*])

* Operator Lifecycle Manager (OLM) manages a set of `CatalogSource` objects from which Operators can be searched for and installed from. These catalog sources are the default sources for this action and are managed by Red Hat. However, it was possible to change these default catalog sources in a way that the OLM system would not notice. Modifying a default catalog source in a way that rendered it inoperable could cause cascading issues through OLM that might prevent a user from installing new or upgrading existing Operators on their cluster. This bug fix updates the `catalog-operator` runtime, which manages the default catalog sources, to be made aware of other changes to the `CatalogSource` spec. As a result, when a change is made to a default catalog source, OLM detects the change and resets it back to default. (link:https://issues.redhat.com/browse/OCPBUGS-5466[*OCPBUGS-5466*])

//[discrete]
//[id="ocp-4-13-openshift-operator-sdk-bug-fixes"]
//==== Operator SDK

//[discrete]
//[id="ocp-4-13-file-integrity-operator-bug-fixes"]
//==== File Integrity Operator

//[discrete]
//[id="ocp-4-13-compliance-operator-bug-fixes"]
//==== Compliance Operator

//[discrete]
//[id="ocp-4-13-openshift-api-server-bug-fixes"]
//==== OpenShift API server

[discrete]
[id="ocp-4-13-rhcos-bug-fixes"]
==== {op-system-first}

* Previously, on Azure, the SR-IOV interface was configured by NetworkManager during boot because the udev rule that marks it as `NM_UNMANAGED` was not in the `initramfs` file. With this update, the udev rule is now in the `initramfs` file and the SR-IOV interface should always be unmanaged by NetworkManager. (link:https://issues.redhat.com/browse/OCPBUGS-7173[*OCPBUGS-7173*])

[discrete]
[id="ocp-4-13-security-profiles-operator-bug-fixes"]
==== Security Profiles Operator

* Previously, a Security Profiles Operator (SPO) SELinux policy did not inherit low-level policy definitions from the container template if you selected another template, such as `net_container`. The policy would not work because it required low-level policy definitions that only existed in the container template. This issue occured when the SPO SELinux policy attempted to translate SELinux policies from the SPO custom format to the Common Intermediate Language (CIL) format. With this update, the container template appends to any SELinux policies that require translation from SPO to CIL. Additionally, the SPO SELinux policy can now inherit low-level policy definitions from any supported policy template. (link:https://issues.redhat.com/browse/OCPBUGS-12879[*OCPBUGS-12879*])


[discrete]
[id="ocp-4-13-scalability-and-performance-bug-fixes"]
==== Scalability and performance

* Previously, when a performance profile was generated, the CRI-O runtime files created automatically were configured to use `runc` as the CRI-O runtime.
+
Now that setting `crun` as the container runtime is generally available when a performance profile is generated, the runtime CRI-O files created match the `defaultRuntime` configured in the `ContainerRuntimeConfig` CR. This can be either `crun` or `runc`. The default is `runc`. (link:https://issues.redhat.com/browse/OCPBUGS-11813[*OCPBUGS-11813*])

[discrete]
[id="ocp-4-13-storage-bug-fixes"]
==== Storage

* Previously, the `openshift-manila-csi-driver` namespace did not include labels that are required for the management of workload partitioning. These missing labels impacted the operation of restricting Manila CSI pods to run on a selected set of CPUs. With this update, the `openshift-manila-csi-driver` namespace now includes the `workload.openshift.io/allowed` label. (link:https://issues.redhat.com/browse/OCPBUGS-11341[*OCPBUGS-11341*])

[discrete]
[id="ocp-4-13-windows-containers-bug-fixes"]
==== Windows containers

* Previously, Microsoft Windows container workloads were not completely emptied during the Windows node upgrade process. This resulted in service disruptions because the workloads remained on the node being upgraded. With this update, the Windows Machine Config Operator (WMCO) drains workloads and then cordons nodes until node upgrades finish. This action ensures a seamless upgrade for Microsoft Windows instances. (link:https://issues.redhat.com/browse/OCPBUGS-5732[*OCPBUGS-5732*])

* Previously, the Windows Machine Config Operator (WMCO) could not drain `DaemonSet` workloads. This issue caused Windows`DaemonSet` pods to block Windows nodes that the WMCO attempted to remove or upgrade. With this update, an WMCO includes additional role-based access control (RBAC) permissions, so that the WMCO can remove `DaemonSet` workloads. An WMCO can also delete any processes that were created with the `containerd` shim, so that `DaemonSet` containers do not exist on a Windows instance after a WMCO removes a node from a cluster. (link:https://issues.redhat.com/browse/OCPBUGS-5354[*OCPBUGS-5354*])

* Previously, the `containerd` container runtime reported an incorrect version on each Windows node because repository tags were not propagated to the build system. This configuration caused `containerd` to report its Go build version as the version for each Windows node. With this update, the correct version is injected into the binary during build time, so that `containerd` reports the correct version for each Windows node. (link:https://issues.redhat.com/browse/OCPBUGS-5378[*OCPBUGS-5378*])

[id="ocp-4-13-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the following tables, features are marked with the following statuses:

* _Technology Preview_
* _General Availability_
* _Not Available_
* _Deprecated_

[discrete]
=== Networking Technology Preview features

.Networking Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|PTP dual NIC hardware configured as boundary clock
|Technology Preview
|Technology Preview
|General Availability

|Ingress Node Firewall Operator
|Not Available
|Technology Preview
|Technology Preview

|Advertise using BGP mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Technology Preview
|General Availability
|General Availability

|Advertise using L2 mode the MetalLB service from a subset of nodes, using a specific pool of IP addresses
|Technology Preview
|Technology Preview
|Technology Preview

|Multi-network policies for SR-IOV networks
|Not Available
|Technology Preview
|Technology Preview

|OVN-Kubernetes network plugin as secondary network
|Not Available
|Not Available
|Technology Preview

|Updating the interface-specific safe sysctls list
|Not Available
|Technology Preview
|Technology Preview

|MT2892 Family [ConnectX-6 Dx] SR-IOV support
|Not Available
|Technology Preview
|General Availability

|MT2894 Family [ConnectX-6 Lx] SR-IOV support
|Not Available
|Technology Preview
|General Availability

|MT42822 BlueField-2 in ConnectX-6 NIC mode SR-IOV support
|Not Available
|Technology Preview
|General Availability

|Silicom STS Family SR-IOV support
|Not Available
|Technology Preview
|General Availability

|MT2892 Family [ConnectX-6 Dx] OvS Hardware Offload support
|Not Available
|Technology Preview
|General Availability

|MT2894 Family [ConnectX-6 Lx] OvS Hardware Offload support
|Not Available
|Technology Preview
|General Availability

|MT42822 BlueField-2 in ConnectX-6 NIC mode OvS Hardware Offload support
|Not Available
|Technology Preview
|General Availability

|Switching Bluefield-2 from DPU to NIC
|Not Available
|Technology Preview
|General Availability

|Intel E810-XXVDA4T
|Not Available
|Not Available
|General Availability

|====

[discrete]
=== Storage Technology Preview features

.Storage Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Shared Resources CSI Driver and Build CSI Volumes in OpenShift Builds
|Technology Preview
|Technology Preview
|Technology Preview

|CSI volume expansion
|Technology Preview
|General Availability
|General Availability

|CSI Azure File Driver Operator
|Technology Preview
|General Availability
|General Availability

|CSI Google Filestore Driver Operator
|Not Available
|Technology Preview
|Technology Preview

|CSI automatic migration
(Azure file, VMware vSphere)
|Technology Preview
|Technology Preview
|General Availability

|CSI automatic migration
(Azure Disk, OpenStack Cinder)
|General Availability
|General Availability
|General Availability

|CSI automatic migration
(AWS EBS, GCP disk)
|Technology Preview
|General Availability
|General Availability

|CSI inline ephemeral volumes
|Technology Preview
|Technology Preview
|General Availability

|CSI generic ephemeral volumes
|Not Available
|General Availability
|General Availability

|{ibmpowerProductName} Virtual Server Block CSI Driver Operator
|Not Available
|Not Available
|Technology Preview

|Automatic device discovery and provisioning with Local Storage Operator
|Technology Preview
|Technology Preview
|Technology Preview

|====

[discrete]
=== Installation Technology Preview features

.Installation Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Adding kernel modules to nodes with kvc
|Technology Preview
|Technology Preview
|Technology Preview

|IBM Cloud VPC clusters (x86_64)
|Technology Preview
|General Availability
|General Availability

|Selectable Cluster Inventory
|Technology Preview
|Technology Preview
|Technology Preview

|Multi-architecture compute machines
|Not Available
|Technology Preview
|General Availability

|Mount shared entitlements in BuildConfigs in RHEL
|Technology Preview
|Technology Preview
|Technology Preview

|Agent-based {product-title} Installer
|Not Available
|General Availability
|General Availability

|Enabling NIC partitioning for SR-IOV devices
|Not Available
|Not Available
|Technology Preview

|Azure Tagging
|Not Available
|Not Available
|Technology Preview

|GCP Confidential VMs
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Node Technology Preview features

.Nodes Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Non-preempting priority classes
|General Availability
|General Availability
|General Availability

|Linux Control Group version 2 (cgroup v2)
|Developer Preview
|Technology Preview
|General Availability

|crun container runtime
|Not Available
|Technology Preview
|General Availability

|Cron job time zones
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Multi-Architecture Technology Preview features

.Multi-Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|`kdump` on `arm64` architecture
|Not Available
|Technology Preview
|Technology Preview

|`kdump` on `s390x` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|`kdump` on `ppc64le` architecture
|Technology Preview
|Technology Preview
|Technology Preview

|IBM Secure Execution on {ibmzProductName} and {linuxoneProductName}
|Not Available
|Technology Preview
|General Availability

|{ibmpowerProductName} Virtual Server using installer-provisioned infrastructure
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Specialized hardware and driver enablement Technology Preview features

.Specialized hardware and driver enablement Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Driver Toolkit
|Technology Preview
|Technology Preview
|General Availability

|Special Resource Operator (SRO)
|Technology Preview
|Technology Preview
|Not Available

|Hub and spoke cluster support
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Web console Technology Preview features

.Web console Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

//|Multicluster console
//|Technology Preview
//|Technology Preview
//|Technology Preview

|Dynamic Plug-ins
|Technology Preview
|General Availability
|General Availability

|====

[discrete]
[id="ocp-413-scalability-tech-preview"]
=== Scalability and performance Technology Preview features

.Scalability and performance Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Hyperthreading-aware CPU manager policy
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Not Available
|Technology Preview
|Technology Preview

|{factory-prestaging-tool}
|Not Available
|Not Available
|Technology Preview

|{sno-caps} cluster expansion with worker nodes
|Not Available
|Technology Preview
|General Availability

|{cgu-operator-first}
|Technology Preview
|Technology Preview
|General Availability

|Mount namespace encapsulation
|Not Available
|Not Available
|Technology Preview

|NUMA-aware scheduling with NUMA Resources Operator
|Technology Preview
|Technology Preview
|General Availability

|HTTP transport replaces AMQP for PTP and bare-metal events
|Not Available
|Not Available
|Technology Preview

|Intel E810 Westport Channel NIC as PTP grandmaster clock
|Not Available
|Not Available
|Technology Preview

|Workload partitioning for three-node clusters and standard clusters
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Operator Technology Preview features

.Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Hybrid Helm Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Java-based Operator
|Not Available
|Technology Preview
|Technology Preview

|Multi-cluster Engine Operator
|Technology Preview
|Technology Preview
|Technology Preview

|Node Observability Operator
|Not Available
|Not Available
|Technology Preview

|Network Observability Operator
|Not Available
|General Availability
|General Availability

|Platform Operators
|Not Available
|Technology Preview
|Technology Preview

|RukPak
|Not Available
|Not Available
|Technology Preview

|Cert-manager Operator
|Technology Preview
|General Availability
|General Availability

|====

[discrete]
=== Monitoring Technology Preview features

.Monitoring Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Alert routing for user-defined projects monitoring
|Technology Preview
|General Availability
|General Availability

|Alerting rules based on platform monitoring metrics
|Not Available
|Technology Preview
|Technology Preview

|Metrics Collection Profiles
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== {rh-openstack-first} Technology Preview features

.{rh-openstack} Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Support for {rh-openstack} DCN
|Technology Preview
|General Availability
|General Availability

|Support for external cloud providers for clusters on {rh-openstack}
|Technology Preview
|General Availability
|General Availability

|====

[discrete]
=== Architecture Technology Preview features

.Architecture Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Hosted control planes for {product-title} on bare metal
|Not Available
|Technology Preview
|Technology Preview

|Hosted control planes for {product-title} on Amazon Web Services (AWS)
|Technology Preview
|Technology Preview
|Technology Preview

|Hosted control planes for {product-title} on {VirtProductName}
|Not Available
|Not Available
|Technology Preview

|====

[discrete]
=== Machine management Technology Preview features

.Machine management Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Managing machines with the Cluster API
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Alibaba Cloud
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Amazon Web Services
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Google Cloud Platform
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Microsoft Azure
|Technology Preview
|Technology Preview
|Technology Preview

|Cloud controller manager for Nutanix
|Not Available
|Not Available
|General Availability

|Cloud controller manager for {rh-openstack-first}
|Technology Preview
|General Availability
|General Availability

|Cloud controller manager for VMware vSphere
|Technology Preview
|Technology Preview
|General Availability

|====

[discrete]
=== Authentication and authorization Technology Preview features

.Authentication and authorization Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|Pod security admission restricted enforcement
|Not Available
|Technology Preview
|Technology Preview

|====

[discrete]
=== Machine Config Operator Technology Preview features

.Machine Config Operator Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |4.11 |4.12 |4.13

|{op-system-first} image layering
|Not Available
|Technology Preview
|General Availability

|====

[id="ocp-4-13-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.8 and beyond! This needs some SME/QE review before being updated for 4.11. Need to check if KI should be removed or should stay.
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to {product-version}, you can either revoke or continue to allow unauthenticated access. Unless there is a specific need for unauthenticated access, you should revoke it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

// TODO: This known issue should carry forward to 4.9 and beyond!
* The `oc annotate` command does not work for LDAP group names that contain an equal sign (`=`), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use `oc patch` or `oc edit` to add the annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1917280[*BZ#1917280*])

* Adding a Git Repository and configuring it with a GitLab and Bitbucket `pipeline-as-code` repository creates an invalid repository resource. As a result, the `spec.git_provider.url` Git provider URL is removed for GitLab and Bitbucket providers.
+
Workaround: Add the mandatory `spec.git_provider.user` field for Bitbucket. In addition, select either *Git access token* or *Git access token secret* to continue adding a Git Repository. (link:https://issues.redhat.com/browse/OCPBUGS-7036[*OCPBUGS-7036*])

* Currently, a certificate compliance issue, specifically outputted as `x509: certificate is not standards compliant`, exists when you run the installation program on macOS for the purposes of installing an {product-title} cluster on VMware vSphere. This issue relates to a known issue with the `golang` compiler in that the compiler does not recognize newly supported macOS certificate standards. No workaround exists for this issue. (link:https://issues.redhat.com/browse/OSDOCS-5694[*OSDOCS-5694*])

* When you include more than three failure domains in the `ControlPlaneMachineSet` definition, the load balancing algorithm does not prioritize existing control plane machines. If you add a fourth failure domain that is alphabetically higher in precedence than the existing three failure domains to the definition, the fourth failure domain takes precedence over any existing failure domains. This behavior can apply rolling forward updates to a control plane machine. You can prevent this issue by setting existing in-use failure domains to a higher precedence than the new and unused failure domains. This action stabilizes each control plane machine during the course of adding more than three failure domains to the definition. (link:https://issues.redhat.com/browse/OCPBUGS-11968[*OCPBUGS-11968*])

* On a {sno} instance, rebooting without draining the node to remove all of the running pods can cause issues with workload container recovery. After the reboot, the workload restarts before all the device plugins are ready, resulting in resources not being available or the workload running on the wrong NUMA node. The workaround is to restart the workload pods when all of the device plugins have re-registered themselves during the reboot recovery procedure. (link:https://issues.redhat.com/browse/OCPBUGS-2180[*OCPBUGS-2180*])

* An error might occur when deleting a pod that uses an SR-IOV netdevice. This error is caused by a change in {op-system-base} 9 where the previous name of a network interface is added to its alternative names list when it is renamed. As a consequence, when a pod attached to an SR-IOV virtual function (VF) is deleted, the VF returns to the pool with a new unexpected name, for example, `dev69`, instead of its original name, for example, `ensf0v2`. Although this error is non-fatal, Multus and SR-IOV logs might show the error while the system recovers on its own. Deleting the pod might take a few extra seconds due to this error. (link:https://issues.redhat.com/browse/OCPBUGS-11281[*OCPBUGS-11281*])

* An incorrect priority class name and syntax error in the YAML definition of the daemon set responsible for updating the interface-specific safe sysctl is preventing the modification of the safe sysctl list for interfaces by using the `cni-sysctl-allowlist` config map in the `openshift-multus` namespace.
+
Workaround: Manually or by using a daemon set, modify the file `/etc/cni/tuning/allowlist.conf` on the nodes to address this issue. (link:https://issues.redhat.com/browse/OCPBUGS-11046[*OCPBUGS-11046*])

* A new feature introduced in {product-title} 4.12 that enables UDP GRO also causes all veth devices to have one RX queue per available CPU (previously each veth had one queue). Those queues are dynamically configured by Open Virtual Network, and there is no synchronization between latency tuning and this queue creation. The latency tuning logic monitors the veth NIC creation events and starts configuring the RPS queue CPU masks before all the queues are properly created. This means that some of the RPS queue masks are not configured. Since not all NIC queues are configured properly there is a chance of latency spikes in a real-time application that uses timing-sensitive CPUs for communicating with services in other containers. Applications that do not use kernel networking stack are not affected. (link:https://issues.redhat.com/browse/OCPBUGS-4194[*OCPBUGS-4194*])

* The Cluster Network Operator (CNO) controller is monitoring more resources than it needs to. As a result, its reconciler is being triggered too frequently, causing a much higher rate of API requests than necessary. There is approximately 1 config map access request made every second. This increases the load on both the CNO and the `kube-apiserver`. (link:https://issues.redhat.com/browse/OCPBUGS-11565[*OCPBUGS-11565*])

* For {product-title} {product-version}, the Driver Toolkit (DTK) container image requires the `ubi9` image as the second layer of the software stack for building driver containers. If you attempt to use the `ubi8` image as the second layer in the your software stack, you will receive a build error. (link:https://issues.redhat.com/browse/OCPBUGS-11120[*OCPBUGS-11120*])

* In some {product-title} installations on the vSphere platform when using the CSI driver, the vSphere CSI driver might not come up correctly because during startup it fails to retrieve information about a node from vCenter, and then the CSI driver does not retry.
+
Workaround: By using SSH to connect to the node that is the current leader of the vsphere-syncer process and restarting the vsphere-syncer container (using crictl), this issue can be mitigated and the driver successfully comes up. (link:https://issues.redhat.com/browse/OCPBUGS-13385[*OCPBUGS-13385*])

* For {product-title} {product-version}, installing version 4.13 on top of {rh-openstack-first} 16.2 with baremetal workers fails because baremetal workers are not able to boot from the {op-system-first} image that comes with OpenShift 4.13. The fundamental issue is the {op-system} image lacks a byte order marker. These fixes are planned for the next 16.2 build. (link:https://issues.redhat.com/browse/OCPBUGS-13395[*OCPBUGS-13395*])

* Due to a known issue in RHEL 9.2, you cannot use persistent volumes on a GCP cluster with Confidential VMs. (link:https://issues.redhat.com/browse/OCPBUGS-7582[*OCPBUGS-7582*])

* {op-system-base-full} workers running in a {product-title} 4.12 cluster with `openvswitch2.15` installed fail when upgrading to {product-title} 4.13. The `upgrade.yml` playbook fails with the following error message `package openvswitch2.17-2.17.0-88.el8fdp.x86_64 conflicts with openvswitch2.15 provided by openvswitch2.15-2.15.0-136.el8fdp.x86_64`.
+
To work around this issue, before you update to {product-title} {product-version}, manually remove the `openvswitch2.15` package and install the `openvswitch2.17` package. Then, run the `upgrade.yml` playbook to update {op-system-base} workers and complete the update process. (link:https://issues.redhat.com/browse/OCPBUGS-11677[*OCPBUGS-11677*])

* There is a disk discovery delay when attaching storage to workloads. (link:https://issues.redhat.com/browse/OCPBUGS-11149[*OCPBUGS-11149*])

* When updating from {product-title} 4.12 to 4.13, the Mellanox NIC renames SR-IOV network node policies such as `ens7f0` to `ens7f0np0`. This name change is because of the update to the RHEL 9 kernel. Consequently, virtual functions (VFs) cannot be created because the interface cannot be found. Your SR-IOV network node policies must take this renaming into account. For example, if `ens7f0` is referenced in your policy, add `ens7f0np0` to your policy before updating.
+
To work around this issue, you must manually edit the `SriovNetworkNodePolicy` custom resource (CR) to add `ens7f0np0` before updating to {product-title} {product-version}. (link:https://issues.redhat.com/browse/OCPBUGS-13186[*OCPBUGS-13186*])
The following code provides an example of the policy updates with both names being added to `SriovNetworkNodePolicy` to ensure compatibility:
+
[source,yaml]
----
  # ...
  deviceType: netdevice
  nicSelector:
    deviceID: “101d”
    pfNames:
      - ens7f0
      - ens7f0np0
    vendor: ‘15b3’
  nodeSelector:
    feature.node.kubernetes.io/sriov-capable: ‘true’
  numVfs: 4
 # ...

----

* Resetting a MAC address on an SR-IOV virtual function (VF) upon pod deletion might fail for Intel E810 NICs.
As a result, creating a pod with an SR-IOV VF might take up to 2 minutes on Intel E810 NIC cards. (link:https://issues.redhat.com/browse/OCPBUGS-5892[*OCPBUGS-5892*])

[id="ocp-4-13-ran-known-issues"]
* If you specify an invalid subscription channel in the subscription policy that you use to perform a cluster upgrade, the {cgu-operator-first} indicates that the upgrade is successful immediately after {cgu-operator} enforces the policy because the `Subscription` resource remains in the `AtLatestKnown` state.
(link:https://issues.redhat.com/browse/OCPBUGS-9239[*OCPBUGS-9239*])

* After a system crash, `kdump` fails to generate the `vmcore` crash dump file on HPE Edgeline e920t and HPE ProLiant DL110 Gen10 servers with Intel E810 NIC and ice driver installed.
(link:https://issues.redhat.com/browse/RHELPLAN-138236[*RHELPLAN-138236*])

* In {ztp}, when you provision a managed cluster that contains more than a single node using a `SiteConfig` CR, disk partition fails when one or more nodes has a `diskPartition` resource configured in the `SiteConfig` CR.
(link:https://issues.redhat.com/browse/OCPBUGS-9272[*OCPBUGS-9272*])

* In clusters configured with PTP boundary clocks (T-BC) and deployed DU applications, messages are intermittently not sent from the follower interface of the T-BC on the vDU host for periods of up to 40 seconds.
The rate of errors in the logs can vary.
An example error log is below:
+
.Example output
[source,terminal]
----
2023-01-15T19:26:33.017221334+00:00 stdout F phc2sys[359186.957]: [ptp4l.0.config] nothing to synchronize
----
(link:https://issues.redhat.com/browse/RHELPLAN-145492[*RHELPLAN-145492*])

* When you install a {sno} cluster using {ztp} and configure PTP and bare-metal events with HTTP transport, the `linuxptp-daemon` daemon pod intermittently fails to deploy.
The required `PersistentVolumeClaim` (`PVC`) resource is created but is not mounted in the pod.
The following volume mount error is reported:
+
.Example output
[source,terminal]
----
mount: /var/lib/kubelet/plugins/kubernetes.io/local-volume/mounts/local-pv-bc42d358: mount(2) system call failed: Structure needs cleaning.
----
To workaround the issue, delete the `cloud-event-proxy-store-storage-class-http-events` `PVC` CR and re-deploy the PTP Operator.
(link:https://issues.redhat.com/browse/OCPBUGS-12358[*OCPBUGS-12358*])

[id="ocp-4-13-ran-known-issues-post-ga"]
* During {ztp-first} provisioning of a {sno} managed cluster with secure boot enabled in the `SiteConfig` CR, multiple `ProvisioningError` errors are reported for the `BareMetalHost` CR during host provisioning.
The error indicates that the secure boot setting is successfully applied in the Baseboard Management Controller (BMC), but the host is not powered on after the `BareMetalHost` CR is applied.
To workaround this issue, perform the following steps:
+
. Reboot the host.
This ensures that the {ztp} pipeline applies the secure boot setting.

. Restart {ztp} provisioning of the cluster with the same configuration.

+
(link:https://issues.redhat.com/browse/OCPBUGS-8434[*OCPBUGS-8434*])

* After installing a dual-stack {ztp} hub cluster, enabling dual-stack Virtual IP addresses (VIPs), and enabling the `virtualMediaViaExternalNetwork` flag in a `Provisioning` CR, the `IRONIC_EXTERNAL_URL_V6` environment variable incorrectly gets assigned an IPv4 address.
(link:https://issues.redhat.com/browse/OCPBUGS-4248[*OCPBUGS-4248*])

* ZT servers have the `BiosRegistry` language set to `en-US` instead of `en`.
This causes a problem during {ztp} provisioning of managed cluster hosts.
The `FirmwareSchema` CR generated for the ZT server doesn't have the `allowable_values`, `attribute_type`, and `read_only` fields populated.
(link:https://issues.redhat.com/browse/OCPBUGS-4388[*OCPBUGS-4388*])

* In {product-title} version 4.13.0, an error occurs when you try to install a cluster with the Agent-based installer. After the read disk stage, an error is returned and the cluster installation gets stuck.
This error has been detected on HPE ProLiant Gen10 servers.
(link:https://issues.redhat.com/browse/OCPBUGS-13138[*OCPBUGS-13138*])

* RFC2544 performance tests show that the `Max delay` value for a packet to traverse the network is over the minimum threshold. This regression is found in {product-title} 4.13 clusters running the Telco RAN DU profile.
(link:https://issues.redhat.com/browse/OCPBUGS-13224[*OCPBUGS-13224*])

* Performance tests run on a {sno} cluster with {product-title} 4.13 installed show an `oslat` maximum latency result greater than 20 microseconds.
(link:https://issues.redhat.com/browse/RHELPLAN-155443[*RHELPLAN-155443*])

* Performance tests run on a {sno} cluster with {product-title} 4.13 installed show a `cyclictest` maximum latency result greater than 20 microseconds.
(link:https://issues.redhat.com/browse/RHELPLAN-155460[*RHELPLAN-155460*])

* The `cpu-load-balancing.crio.io: "disable"` annotation associated with the low latency tuning described in xref:../scalability_and_performance/cnf-low-latency-tuning.adoc#node-tuning-operator-disabling-cpu-load-balancing-for-dpdk_cnf-master[Disabling CPU load balancing for DPDK] does not work on systems that do not have workload partitioning configured. More specifically, this affects clusters where the infrastructure does not set the `cpuPartitioningMode` to the `AllNodes` value as described in xref:../scalability_and_performance/enabling-workload-partitioning.adoc[Workload partitioning].
+
This affects the achievable latency of such clusters and might prevent proper operation of low latency workloads. (link:https://issues.redhat.com/browse/OCPBUGS-13163[*OCPBUGS-13163*])

* {product-title} 4.12 clusters on the Nutanix platform may have an `Upgradeable=False` condition if they are missing configuration needed for the Nutanix Cloud Control Manager (CCM). To resolve this condition, see: link:https://access.redhat.com/solutions/7014068[How to create the ConfigMaps and Secrets needed to upgrade to OpenShift 4.13 when using Nutanix as a Platform].

* Currently, when using a persistent volume (PV) that contains a very large number of files, the pod might not start or can take an excessive amount of time to start. For more information, see this link:https://access.redhat.com/solutions/6221251[knowledge base article]. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1987112[*BZ1987112*])

[id="ocp-4-13-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red Hat Network. All {product-title} {product-version} errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster-within-minor.adoc#updating-cluster-within-minor[updating your cluster] properly.
====

//Update with relevant advisory information
[id="ocp-4-13-0-ga"]
=== RHSA-2023:1326 - {product-title} 4.13.0 image release, bug fix, and security update advisory

Issued: 2023-05-17

{product-title} release 4.13.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2023:1326[RHSA-2023:1326] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2023:1325[RHSA-2023:1325] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.13.0 --pullspecs
----
//replace 4.y.z for the correct values for the release. You do not need to update oc to run this command.
[id="ocp-4-13-1"]
=== RHSA-2023:3304 - {product-title} 4.13.1 bug fix and security update

Issued: 2023-05-30

{product-title} release 4.13.1, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2023:3304[RHSA-2023:3304] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2023:3303[RHSA-2023:3303] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.13.1 --pullspecs
----

[id="ocp-4-13-1-bug-fixes"]
==== Bug fixes
* Previously, assisted installations could encounter a transient error. If that error occurred, the installation failed to recover. With this update, transient errors are re-tried correctly. (link:https://issues.redhat.com/browse/OCPBUGS-13138[*OCPBUGS-13138*])

* Previously, oc-mirror {oc-first} plugin would fail with a `401 unauthorized` error for some registries when a nested path exceeded the expected maximum path-components. With this update, the default integer of the `--max-nested-paths` flag is set to 0 (no limit). As a result, the generated `ImageContentSourcePolicy` will contain source and mirror references up to the repository level as opposed to the namespace level used by default. (link:https://issues.redhat.com/browse/OCPBUGS-13591[*OCPBUGS-13591*])

[id="ocp-4-13-1-updating"]
==== Updating

To update an existing {product-title} 4.13 cluster to this latest release, see xref:../updating/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI] for instructions.

[id="ocp-4-13-2"]
=== RHSA-2023:3367 - {product-title} 4.13.2 bug fix and security update

Issued: 2023-06-07

{product-title} release 4.13.2, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2023:3367[RHSA-2023:3367] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2023:3366[RHSA-2023:3366] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.13.2 --pullspecs
----

[id="ocp-4-13-2-updating"]
==== Updating

To update an existing {product-title} 4.13 cluster to this latest release, see xref:../updating/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI] for instructions.

[id="ocp-4-13-3"]
=== RHSA-2023:3537 - {product-title} 4.13.3 bug fix and security update

Issued: 2023-06-13

{product-title} release 4.13.3, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2023:3537[RHSA-2023:3537] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2023:3536[RHSA-2023:3536] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.13.3 --pullspecs
----

[id="ocp-4-13-3-features"]
==== Features

[id="ocp-4-13-ipxe-ztp"]
===== Support for iPXE network booting with ZTP
{ztp-first} uses the Bare Metal Operator (BMO) to boot {op-system-first} on the target host as part of the deployment of spoke clusters. With this update, {ztp} leverages the capabilities of the BMO by adding the option of Preboot Execution Environment (iPXE) network booting for these RHCOS installations.

[NOTE]
====
To use iPXE network booting, you must use {rh-rhacm-first} 2.8 or later.
====

For more information, see xref:../scalability_and_performance/ztp_far_edge/ztp-deploying-far-edge-sites.adoc#ztp-deploying-a-site_ztp-deploying-far-edge-sites[Deploying a managed cluster with SiteConfig and {ztp}].

[id="ocp-4-13-3-bug-fixes"]
==== Bug fixes

* Previously on {sno}, in case of node reboot there was a race condition that could result in admission of application pods requesting devices on the node even if devices were unhealthy or unavailable to be allocated. This resulted in runtime failures when the application tried to access devices. With this update, the resources requested by the pod are only allocated if the device plugin has registered itself to kubelet and healthy devices are present on the node to be allocated.
+
If these conditions are not met, the pod can fail at admission with `UnexpectedAdmissionError` error, which is an expected behavior. If the application pod is part of deployments, in case of failure subsequent pods are spun up and ultimately successfully run when devices are suitable to be allocated. (link:https://issues.redhat.com/browse/OCPBUGS-14438[*OCPBUGS-14438*])

* Previously, client TLS (mTLS) was configured on an Ingress Controller, and the certificate authority (CA) in the client CA bundle required more than 1MB of certificate revocation list (CRLs) to be downloaded. The CRL `ConfigMap` object size limitations prevented updates from occurring. As a result of the missing CRLs, connections with valid client certificates may have been rejected with the error `unknown ca`. With this update, the CRL `ConfigMap` for each Ingress Controller no longer exists; instead, each router pod directly downloads CRLs, ensuring connections with valid client certificates are no longer rejected. (link:https://issues.redhat.com/browse/OCPBUGS-13967[*OCPBUGS-13967*])

* Previously, because client TLS (mTLS) was configured on an Ingress Controller, mismatches between the distributing certificate authority (CA) and the issuing CA caused the incorrect certificate revocation list (CRL) to be downloaded. As a result, the incorrect CRL was downloaded instead of the correct CRL, causing connections with valid client certificates to be rejected with the error message `unknown ca`. With this update, downloaded CRLs are now tracked by the CA that distributes them. This ensures that valid client certificates are no longer rejected. (link:https://issues.redhat.com/browse/OCPBUGS-13964[*OCPBUGS-13964*])

[id="ocp-4-13-3-updating"]
==== Updating

To update an existing {product-title} 4.13 cluster to this latest release, see xref:../updating/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI] for instructions.

[id="ocp-4-13-4"]
=== RHSA-2023:3614 - {product-title} 4.13.4 bug fix and security update

Issued: 2023-06-23

{product-title} release 4.13.4, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2023:3614[RHSA-2023:3614] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2023:3612[RHSA-2023:3612] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.13.4 --pullspecs
----

[id="ocp-4-13-4-updating"]
==== Updating

To update an existing {product-title} 4.13 cluster to this latest release, see xref:../updating/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI] for instructions.

[id="ocp-4-13-5"]
=== RHSA-2023:4091 - {product-title} 4.13.5 bug fix and security update

Issued: 2023-07-20

{product-title} release 4.13.5, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2023:4091[RHSA-2023:4091] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2023:4093[RHSA-2023:4093] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.13.5 --pullspecs
----

[id="ocp-4-13-5-bug-fixes"]
==== Bug fixes
* Previously, the Gateway API feature did not provide DNS records with a trailing dot for the Gateway domain. This caused the status of the DNS record to never become available on the GCP platform. With this update, the DNS records for Gateway API Gateways are properly provisioned and the Gateway API feature works on GCP because the gateway service dns controller now adds a trailing dot if it is missing in the domain. (link:https://issues.redhat.com/browse/OCPBUGS-15434[*OCPBUGS-15434*])

* Previously, if you used the *Pipelines* page of the *Developer* console to add a repository, and you entered a GitLab or Bitbucket Pipelines as Code repository URL as the *Git Repo URL*, the created `Repository` resource was invalid. This was caused by a missing schema issue in the `git_provider.url` spec, which is now fixed. (link:https://issues.redhat.com/browse/OCPBUGS-15410[*OCPBUGS-15410*])

* In this release, the `git_provider.user` spec has been added for Pipelines as Code `Repository` objects. This spec requires you to provide a username if the Git provider is Bitbucket. (link:https://issues.redhat.com/browse/OCPBUGS-15410[*OCPBUGS-15410*])

* In this release, the *Secret* field in the *Pipelines* -> *Create* -> *Add Git Repository* page is now mandatory. You must click *Show configuration options*, and then configure either a Git access token or a Git access token secret for your repository. (link:https://issues.redhat.com/browse/OCPBUGS-15410[*OCPBUGS-15410*])

* Previously, if you tried to edit a Helm chart repository in the *Developer* console by navigating to *Helm*, clicking the *Repositories* tab, then selecting *Edit HelmChartRepository* through the kebab menu for your Helm chart repository, an *Error* page was displayed that showed a *404: Page Not Found* error. This was caused by a component path that was not up to date. This issue is now fixed. (link:https://issues.redhat.com/browse/OCPBUGS-15130[*OCPBUGS-15130*])

[id="ocp-4-13-5-updating"]
==== Updating

To update an existing {product-title} 4.13 cluster to this latest release, see xref:../updating/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI] for instructions.

[id="ocp-4-13-6"]
=== RHSA-2023:4226 - {product-title} 4.13.6 bug fix and security update

Issued: 2023-07-27

{product-title} release 4.13.6, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2023:4226[RHSA-2023:4226] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2023:4229[RHBA-2023:4229] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.13.6 --pullspecs
----

[id="ocp-4-13-6-updating"]
==== Updating

To update an existing {product-title} 4.13 cluster to this latest release, see xref:../updating/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI] for instructions.

[id="ocp-4-13-7"]
=== RHSA-2023:4305 - {product-title} 4.13.7 bug fix and security update

Issued: 2023-08-01

{product-title} release 4.13.7, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2023:4305[RHSA-2023:4305] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2023:4308[RHSA-2023:4308] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.13.7 --pullspecs
----

[id="ocp-4-13-7-bug-fixes"]
==== Bug fixes
* In this release, annotation for real loadbalancer IP address has been added as well as automatic topology awareness for Manila share provisioning. (link:https://issues.redhat.com/browse/OCPBUGS-15973[*OCPBUGS-15973*])

[id="ocp-4-13-7-updating"]
==== Updating

To update an existing {product-title} 4.13 cluster to this latest release, see xref:../updating/updating-cluster-cli.adoc#updating-cluster-cli[Updating a cluster using the CLI] for instructions.