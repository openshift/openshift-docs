[id="ocp-4-5-release-notes"]
= {product-title} {product-version} release notes
include::modules/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid
cloud application platform for deploying both new and existing applications on
secure, scalable resources with minimal configuration and management overhead.
{product-title} supports a wide selection of programming languages and
frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on Red Hat Enterprise Linux and Kubernetes, {product-title}
provides a more secure and scalable multi-tenant operating system for today's
enterprise-class applications, while delivering integrated application runtimes
and libraries. {product-title} enables organizations to meet security, privacy,
compliance, and governance requirements.

[id="ocp-4-5-about-this-release"]
== About this release

Red Hat {product-title}
(link:https://access.redhat.com/errata/RHBA-2020:2409[RHBA-2020:2409]) is now
available. This release uses link:https://v1-18.docs.kubernetes.io/docs/setup/release/notes/[Kubernetes 1.18] with CRI-O runtime. New features, changes, and known issues that pertain to
{product-title} {product-version} are included in this topic.

//Red Hat did not publicly release {product-title} 4.4.0 as the GA version and,
//instead, is releasing {product-title} 4.4.3 as the GA version.

{product-title} {product-version} clusters are available at
https://cloud.redhat.com/openshift. The {cloud-redhat-com}
application for {product-title} allows you to deploy OpenShift clusters to
either on-premise or cloud environments.

{product-title} {product-version} is supported on Red Hat Enterprise Linux 7.7 or
later, as well as {op-system-first} 4.5.

You must use {op-system} for the control plane, which are also known as master machines, and
can use either {op-system} or Red Hat Enterprise Linux 7.7 or later for
compute machines, which are also known as worker machines.

[IMPORTANT]
====
Because only Red Hat Enterprise Linux version 7.7 or later is supported for compute
machines, you must not upgrade the Red Hat Enterprise Linux compute machines to
version 8.
====

With the release of {product-title} 4.5, version 4.2 is now end of life. For
more information, see the
link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift Container Platform Life Cycle Policy].

[id="ocp-4-5-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-5-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-5-installing-cluster-on-vsphere-using-installer-provisioned-infra"]
==== Installing a cluster on vSphere using installer-provisioned infrastructure

{product-title} 4.5 introduces support for installing a cluster on vSphere using
installer-provisioned infrastructure.

// For more information, see ../installing/installing_vsphere/installing-vsphere-installer-provisioned.adoc#installing-vsphere-installer-provisioned[Installing a cluster on vSphere]

[id="ocp-4-5-installing-cluster-on-gcp-using-user-provisioned-infra-and-shared-vpc"]
==== Installing a cluster on GCP using user-provisioned infrastructure and a shared VPC

{product-title} 4.5 introduces support for installing a cluster on Google Cloud
Platform (GCP) using user-provisioned infrastructure and a shared VPC.

// For more information, see ../installing//installing_gcp/installing-gcp-user-infra-vpc.adoc#installing-gcp-user-infra-vpc[Installing a cluster on GCP using Deployment Manager templates and a shared VPC].

[id="ocp-4-5-three-node-bare-metal-deployments"]
==== Three-node bare metal deployments

You can install and run three-node clusters in {product-title} with no workers.
This provides smaller, more resource efficient clusters for deployment,
development, and testing.

For more information, see
xref:../installing/installing_bare_metal/installing-bare-metal.adoc#installation-three-node-cluster_installing-bare-metal[Running a three-node cluster].

[id="ocp-4-5-restricted-network-cluster-upgrade-improvements"]
==== Restricted network cluster upgrade improvements

The Cluster Version Operator (CVO) can now verify the release images if the
image signature is available as a ConfigMap in the cluster during the upgrade
process for a restricted network cluster. This removes the need for using the
`--force` flag during upgrades in a restricted network environment.

This improved upgrade workflow is completed by running the enhanced
`oc adm release mirror` command. The following actions are performed:

* Pulls the image signature from the release during the mirroring process.
* Applies the signature ConfigMap directly to the connected cluster.

//For more information, see
//../updating/updating-restricted-network-cluster.adoc#updating-restricted-network-cluster[Updating a restricted network cluster].

[id="ocp-4-5-migrate-azure-private-dns-zones"]
==== Migrating Azure private DNS zones

There is now a new `openshift-install migrate` command available for migrating
Azure private DNS zones. If you installed an {product-title} version 4.2 or 4.3
cluster on Azure that uses installer-provisioned infrastructure, your cluster
might use a legacy private DNS zone. If it does, you must migrate it to the new
type of private DNS zone.

// For more information on this new command and migrating Azure private DNS
// zones, see ../installing/installing_azure/migrating-azure-dns-zones.adoc#migrating-azure-dns-zones[Migrating legacy Azure DNS zones].

[id="ocp-4-5-built-in-help-for-installconfig-supported-fields"]
==== Built-in help for `install-config.yaml` supported fields

There is a new `openshift-install explain` command available that lists all the
fields for supported `install-config.yaml` file versions including a short
description explaining each resource. It also provides details on which fields
are mandatory and specifies their default value. Using the `explain` command
reduces the need to continually look up configuration options when creating or
customizing the `install-config.yaml` file.

[id="ocp-4-5-encrypt-ebs-instance-volumes-with-kms-key"]
==== Encrypt EBS instance volumes with a KMS key

You can now define a KMS key to encrypt EBS instance volumes. This is useful if
you have explicit compliance and security guidelines when deploying to AWS. The
KMS key can be configured in the `install-config.yaml` file by setting the
optional `kmsKeyARN` field. For example:

[source,yaml]
----
apiVersion: v1
baseDomain: example.com
compute:
- architecture: amd64
  hyperthreading: Enabled
  name: worker
  platform:
    aws:
      rootVolume:
        kmsKeyARN: arn:aws:kms:us-east-2:563456982459:key/4f5265b4-16f7-xxxx-xxxx-xxxxxxxxxxxx
...
----

If no key is specified, the account's default KMS key for that particular region
is used.

[id="ocp-4-5-install-to-pre-existing-vpc-with-multiple-cidrs-on-aws"]
==== Install to pre-existing VPC with multiple CIDRs on AWS

You can now install {product-title} to a VPC with more than one CIDR on AWS.
This lets you select secondary CIDRs for the machine network. When the VPC is
provisioned by the installer, it does not create multiple CIDRs or configure the
routing between subnets. Installing to a pre-existing VPC with multiple CIDRs is
supported for both user-provisioned and installer-provisioned infrastructure
installation workflows.

[id="ocp-4-5-adding-custom-domain-names-to-aws-vpc-dhcp-options"]
==== Adding custom domain names to AWS Virtual Private Cloud (VPC) DHCP option sets

Custom domain names can now be added to AWS Virtual Private Cloud (VPC) DHCP
option sets. This enables Certificate Signing Request (CSR) approval of new
nodes when custom DHCP options are used.

[id="ocp-4-5-provisioning-ipv6-in-ironic"]
==== Provisioning bare metal hosts using IPv6 with Ironic

Binaries required for IPv6 provisioning using the UEFI networking stack have now
been introduced in Ironic. You can now provision bare metal hosts using IPv6
with Ironic. The `snpnoly.efi` bootloader executable and compatible iPXE
binaries are now included in the `tftpboot` directory.

[id="ocp-4-5-openstack-custom-networking"]
==== Custom networks and subnets for clusters on {rh-openstack}

{product-title} {product-version} introduces support for installing clusters on {rh-openstack-first}
 that rely on preexisting networks and subnets.

[id="ocp-4-5-openstack-additional-networks"]
==== Additional networks for clusters on {rh-openstack}

{product-title} {product-version} introduces support for multiple networks in clusters that run on {rh-openstack}.
You can specify these networks for both control plane and compute machines during installation.

[id="ocp-4-5-openstack-kuryr-octavia"]
==== Improved {rh-openstack} load balancer upgrade experience for clusters that use Kuryr

Clusters that use Kuryr now have improved support for Octavia load-balancing services on {rh-openstack} clusters that were upgraded from 13 to 16. For example, these clusters now support the Octavia OVN provider driver.

For more information, see xref:../installing/installing_openstack/installing-openstack-installer-kuryr.html#installation-osp-kuryr-octavia-driver_installing-openstack-installer-kuryr[The Octavia OVN driver].

[id="ocp-4-5-security"]
=== Security

[id="ocp-4-5-security-oauth-proxy-imagestream-restricted-network"]
==== Using the `oauth-proxy` imagestream in restricted network installations

The `oauth-proxy` image can now be consumed by external components in restricted
network installations by using the `oauth-proxy` imagestream.

[id="ocp-4-5-machine-api"]
=== Machine API

[id="ocp-4-5-aws-machinesets-support-spot-instances"]
==== AWS MachineSets support spot instances

AWS MachineSets now support spot instances. This lets you create a MachineSet
that deploys machines as spot instances so you can save costs compared to
on-demand instance prices. You can configure spot instances by adding the
following line under the `providerSpec` field in the MachineSet YAML file:

[source,yaml]
----
providerSpec:
  spotMarketOptions: {}
----

[id="ocp-4-5-autoscaling-the-minimum-number-of-machines"]
==== Autoscaling the minimum number of machines to 0

You can now set the minimum number of replicas for a MachineAutoscaler to `0`.
This allows the autoscaler to be more cost-effective by scaling between zero
machines and the machine count necessary based on the resources your workloads
require.

For more information, see the
xref:../machine_management/applying-autoscaling.adoc#machine-autoscaler-cr_applying-autoscaling[MachineAutoscaler resource definition].

[id="ocp-4-5-machinehealthcheck-with-empty-selector-monitors-all-machines"]
==== MachineHealthCheck with empty selector monitors all machines

A MachineHealthCheck resource that contains an empty `selector` field now
monitors all machines.

For more information on the `selector` field in the MachineHealthCheck resource,
see the
xref:../machine_management/deploying-machine-health-checks.html#machine-health-checks-resource_deploying-machine-health-checks[Sample MachineHealthCheck resource].

[id="ocp-4-5-describing-machine-and-machineset-fields-using-oc-explain"]
==== Describing machine and MachineSet fields by using `oc explain`

A full OpenAPI schema is now provided for machine and MachineSet Custom
Resources. `oc explain` now provides descriptions for fields included in machine
and MachineSet API resources.

[id="ocp-4-5-nodes"]
=== Nodes

[id="ocp-4-5-descheduler-policy"]
==== New descheduler strategy is available (Technology Preview)

The descheduler now allows you to configure the `RemovePodsHavingTooManyRestarts` strategy. This strategy ensures that Pods that have been restarted too many times are removed from nodes.

See xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler-strategies_nodes-descheduler[Descheduler strategies]
for more information.

[id="ocp-4-5-node-pull-secrets"]
==== Node pull secrets

You can import and use images from any registry configured during or after the
cluster installation by sharing the node's pull secret credentials with the
`openshift-api`, `builder`, and `image-registry` Pods.

[id="ocp-4-5-vertical-pod-autoscaler-tp"]
==== Vertical Pod Autoscaler Operator (Technology Preview)

{product-title} {product-version} introduces the Vertical Pod Autoscaler Operator (VPA). The VPA reviews the historic and current CPU and memory resources for containers in Pods and can update the resource limits and requests based on the usage values it learns. You create individual custom resources (CR) to instruct the VPA to update all of the Pods associated with a workload object, such as a Deployment, Deployment Config, StatefulSet, Job, DaemonSet, ReplicaSet, or ReplicationController. The VPA helps you to understand the optimal CPU and memory usage for your Pods and can automatically maintain Pod resources through the Pod lifecycle.

[id="ocp-4-5-openstack-anti-affinity"]
==== Anti-affinity compute node scheduling on {rh-openstack}

If separate physical hosts are available on an {rh-openstack} deployment, compute nodes will be scheduled across all of them.

[id="ocp-4-5-cluster-monitoring"]
=== Cluster monitoring

[id="ocp-4-5-monitor-your-own-services-tp"]
==== Monitor your own services (Technology Preview)

The following improvements are now available to further enhance monitoring your
own services:

* Allow cross-correlation of the metrics of your own service with cluster metrics.
* Allow using metrics of services in user namespaces in recording and
alerting rules.
* Add tenancy support in front of the Alertmanager API.
* Add the ability to deploy user recording and alerting rules with higher
availability.
* Add the ability to introspect Thanos Stores using the Thanos Querier.
* Access metrics of all services together in the web console from a single view.

For more information see
xref:../monitoring/monitoring-your-own-services.adoc#monitoring-your-own-services[Monitoring your own services].

[id="ocp-4-5-web-console"]
=== Web console

[id="ocp-4-5-new-attribute-filters"]
====  New Infrastructure Features filters for Operators in OperatorHub

You can now filter Operators by *Infrastructure Features* in OperatorHub. For
example, select *Disconnected* to see Operators that work in
disconnected environments.

[id="ocp-4-5-developer-perspective"]
==== Developer Perspective

You can now use the *Developer* perspective to:

* Make informed decisions on installing Helm Charts in the *Developer Catalog* using the description and docs for them.
* Uninstall, upgrade, and rollback Helm Releases.
* Create and delete dynamic Knative event sources.
* Deploy virtual machines, launch applications in them, or delete the virtual machines.
* Provide Git webhooks, Triggers, and  Workspaces, manage credentials of private git repositories, and troubleshoot using better logs for OpenShift Pipelines.
* Add health checks during or after application deployment.
* Navigate efficiently and pin frequently searched items.

[id="ocp-4-5-scale"]
=== Scale

[id="ocp-4-5-scale-cluster-maximums"]
==== Cluster maximums

Updated guidance around
xref:../scalability_and_performance/planning-your-environment-according-to-object-maximums.adoc#planning-your-environment-according-to-object-maximums[Cluster
maximums] for {product-title} {product-version} is now available.

Use the link:https://access.redhat.com/labs/ocplimitscalculator/[{product-title}
Limit Calculator] to estimate cluster limits for your environment.

[id="ocp-4-5-networking"]
=== Networking

[id="ocp-4-5-mirgating-from-sdn-to-default-cni-tp"]
==== Migrating from the OpenShift SDN default CNI network provider (Technology Preview)

You can now migrate to the OVN-Kubernetes default Container Network Interface
(CNI) network provider from the OpenShift SDN default CNI network provider.

For more information, see
xref:../networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc#migrate-from-openshift-sdn[Migrate from the OpenShift SDN default CNI network provider].

[id="ocp-4-5-ingress-enhancements"]
==== Ingress enhancements

There are two noteworthy Ingress enhancements introduced in {product-title} 4.5:

* You can enable access logs for the Ingress Controller.
//:../networking/ingress-operator.adoc#nw-configure-ingress-access-logging_configuring-ingress[enable access logs for the Ingress Controller].
* You can xref:../networking/ingress-operator.adoc#using-wildcard-routes_configuring-ingress[specify a wildcard route policy through the Ingress Controller].

[id="ocp-4-5-developer-experience"]
=== Developer experience

[id="ocp-4-5-oc-new-app-deployment-resources"]
==== oc new-app now produces Deployment resources

The `oc new-app` command now produces Deployment resources instead of
DeploymentConfig resources by default. If you prefer to create DeploymentConfig
resources, you can pass the `--as-deployment-config` flag when invoking `oc
new-app`. For more information, see
xref:../applications/deployments/what-deployments-are.adoc#what-deployments-are[Understanding Deployments and DeploymentConfigs].

[id="ocp-4-5-support-nodeaffinity-scheduler-in-image-registry-crd"]
==== Support node affinity scheduler in image registry CRD

The node affinity scheduler is now supported to ensure image registry
deployments complete even when an infrastructure node does not exist. The node
affinity scheduler must be manually configured.

See xref:../nodes/scheduling/nodes-scheduler-node-affinity.adoc#nodes-scheduler-node-affinity[Controlling Pod placement on nodes using node affinity rules]
for more information.

[id="ocp-4-5-disaster-recovery"]
=== Disaster recovery

[id="ocp-4-5-auto-cert-recovery"]
==== Automatic control plane certificate recovery

{product-title} can now automatically recover from expired control plane certificates. The exception is that you must manually approve pending `node-bootstrapper` certificate signing requests (CSRs) to recover kubelet certificates.

See xref:../backup_and_restore/disaster_recovery/scenario-3-expired-certs.adoc#dr-scenario-3-recovering-expired-certs_dr-recovering-expired-certs[Recovering from expired control plane certificates] for more information.

[id="ocp-4-5-storage"]
=== Storage

[id="ocp-4-5-persistent-storage-csi-ebs"]
==== Persistent storage using the AWS EBS CSI Driver Operator (Technology Preview)

You can now use the Container Storage Interface (CSI) to deploy the CSI driver you need for provisioning AWS Elastic Block Store (EBS) persistent storage. This Operator is in Technology Preview. For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-ebs.adoc#persistent-storage-csi-ebs[AWS Elastic Block Store CSI Driver Operator].

[id="ocp-4-5-persistent-storage-csi-manila"]
==== Persistent storage using the OpenStack Manila CSI Driver Operator

You can now use CSI to provision a PersistentVolume using the CSI driver for the OpenStack Manila shared file system service. For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-manila.adoc#persistent-storage-csi-manila[OpenStack Manila CSI Driver Operator].

[id="ocp-4-5-persistent-storage-csi-inline"]
==== Persistent storage using CSI inline ephemeral volumes (Technology Preview)

You can now use CSI to specify volumes directly in the Pod specification, rather than in a PersistentVolume. This feature is in Technology Preview and is available by default when using CSI drivers. For more information, see xref:../storage/container_storage_interface/ephemeral-storage-csi-inline.adoc#ephemeral-storage-csi-inline[CSI inline ephemeral volumes].

[id="ocp-4-5-persistent-storage-csi-cloning"]
==== Persistent storage using CSI volume cloning

Volume cloning using CSI, previously in Technology Preview, is now fully supported in OpenShift Container Platform 4.5. For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-cloning.adoc#persistent-storage-csi-cloning[CSI volume cloning].

[id="ocp-4-5-operators"]
=== Operators

[id="ocp-4-5-bundle-format-opm"]
==== Bundle Format for packaging Operators and `opm` CLI tool

The Bundle Format for Operators is a new packaging format introduced by the
Operator Framework that is supported starting with {product-title} 4.5. To
improve scalability and better enable upstream users hosting their own catalogs,
the Bundle Format specification simplifies the distribution of Operator
metadata.

[NOTE]
====
While the legacy Package Manifest Format is deprecated in {product-title} 4.5,
it is still supported and Operators provided by Red Hat are currently shipped
using the Package Manifest Format.
====

An Operator bundle represents a single version of an Operator and can be
scaffolded with the Operator SDK. On-disk _bundle manifests_ are containerized
and shipped as a _bundle image_, a non-runnable container image that stores the
Kubernetes manifests and Operator metadata. Storage and distribution of the
bundle image is then managed using existing container tools like `podman` and
`docker` and container registries like Quay.

See
xref:../operators/olm-packaging-format.adoc#olm-bundle-format_olm-packaging-format[Packaging formats]
for more details on the Bundle Format.

The new `opm` CLI tool is also introduced alongside the Bundle Format. The `opm`
CLI allows you to create and maintain catalogs of Operators from a list of
bundles, called an index, that are equivalent to a "repository". The result is a
container image, called an _index image_, which can be stored in a container
registry and then installed on a cluster.

An index contains a database of pointers to Operator manifest content that can
be queried via an included API that is served when the container image is run.
On {product-title}, OLM can use the index image as a catalog by referencing it
in a CatalogSource, which polls the image at regular intervals to enable
frequent updates to installed Operators on the cluster.

See xref:../operators/olm-managing-custom-catalogs.adoc#olm-managing-custom-catalogs-bundle-format[Managing custom catalogs]
for more details on `opm` usage.

[id="ocp-4-5-olm-v1-crd"]
==== v1 CRD support in Operator Lifecycle Manager

Operator Lifecycle Manager (OLM) now supports Operators using v1
CustomResourceDefinitions (CRDs) when loading Operators into catalogs and
deploying them on cluster. Previously, OLM only supported v1beta1 CRDs; OLM now
manages both v1 and v1beta1 CRDs in the same way.

To support this feature, OLM now enforces CRD upgrades are safer by ensuring
existing CRD storage versions are not missing in the upgraded CRD, avoiding
potential data loss.

[id="ocp-4-5-olm-webhooks"]
==== Admission webhook support in OLM

Validating and mutating admission webhooks allow Operator authors to intercept,
modify, and accept or reject resources before they are handled by the Operator
controller. Operator Lifecycle Manager (OLM) can manage the lifecycle of these
webhooks when they are shipped alongside your Operator.

See xref:../operators/olm-webhooks.adoc#olm-webhooks[Managing admission webhooks in Operator Lifecycle Manager]
for more details.

[id="ocp-4-5-operator-api"]
==== Read-only Operator API (Technology Preview)

The new Operator API is now available as a Technology Preview feature in
read-only mode. Previously, installing Operators using Operator Lifecycle
Manager (OLM) required cluster administrators to be aware of multiple APIs,
including CatalogSources, Subscriptions, ClusterServiceVersions, and
InstallPlans. This single Operator API resource is a first step towards a more
simplified experience discovering and managing the lifecycle of Operators in a
{product-title} cluster.

Currently only available using the CLI and requiring a few manual steps to
enable, this feature previews interacting with Operators as a first-class API
object. Cluster administrators can discover previously installed Operators using
this API in read-only mode, for example using the `oc get operators` command.

To enable this Technology Preview feature:

.Procedure

. Disable xref:../architecture/architecture-installation.adoc#unmanaged-operators_architecture-installation[Cluster Version Operator (CVO) management] of the OLM:
+
----
$ oc patch clusterversion version \
    --type=merge -p \
    '{
       "spec":{
          "overrides":[
             {
                "kind":"Deployment",
                "name":"olm-operator",
                "namespace":"openshift-operator-lifecycle-manager",
                "unmanaged":true,
                "group":"apps/v1"
             }
          ]
       }
    }'
----

. Add the `OperatorLifecycleManagerV2=true` xref:../nodes/clusters/nodes-cluster-enabling-features.adoc#nodes-cluster-enabling[FeatureGate]
to the OLM Operator.

.. Edit the OLM Operator's Deployment:
+
----
$ oc -n openshift-operator-lifecycle-manager \
    edit deployment olm-operator
----

.. Add the following flag to the Deployment's `args` section:
+
----
...
    spec:
      containers:
      - args:
...
        - --feature-gates
        - OperatorLifecycleManagerV2=true
----

.. Save your changes.

. Install an Operator using the normal OperatorHub method if you have not already;
this example uses an etcd Operator installed in the project `test-project`.

. Create a new Operator resource for the installed etcd Operator.

.. Save the following to a file:
+
.`etcd-test-op.yaml` file
----
apiVersion: operators.coreos.com/v2alpha1
kind: Operator
metadata:
  name: etcd-test
----

.. Create the resource:
+
----
$ oc create -f etcd-test-op.yaml
----

. To have the installed Operator opt in to the new API, apply the
`operators.coreos.com/etcd-test` label to the following objects related to your
Operator:
+
--
* Subscription
* InstallPlan
* ClusterServiceVersion
* Any CRDs owned by the Operator
--
+
[NOTE]
====
In a future release, these objects will be automatically labeled for any
Operators where the CSV was installed using a Subscription.
====
+
For example:
+
----
$ oc label sub etcd operators.coreos.com/etcd-test="" -n test-project
$ oc label ip install-6c5mr operators.coreos.com/etcd-test="" -n test-project
$ oc label csv etcdoperator.v0.9.4 operators.coreos.com/etcd-test="" -n test-project
$ oc label crd etcdclusters.etcd.database.coreos.com operators.coreos.com/etcd-test=""
$ oc label crd etcdbackups.etcd.database.coreos.com operators.coreos.com/etcd-test=""
$ oc label crd etcdrestores.etcd.database.coreos.com operators.coreos.com/etcd-test=""
----

. Verify your Operator has opted in to the new API.

.. List all `operators` resources:
+
----
$ oc get operators

NAME        AGE
etcd-test   17m
----

.. Inspect your Operator's details and note that the objects you labeled are
represented:
+
----
$ oc describe operators etcd-test

Name:         etcd-test
Namespace:
Labels:       <none>
Annotations:  <none>
API Version:  operators.coreos.com/v2alpha1
Kind:         Operator
Metadata:
  Creation Timestamp:  2020-07-02T05:51:17Z
  Generation:          1
  Resource Version:    37727
  Self Link:           /apis/operators.coreos.com/v2alpha1/operators/etcd-test
  UID:                 6a441a4d-75fe-4224-a611-7b6c83716909
Status:
  Components:
    Label Selector:
      Match Expressions:
        Key:       operators.coreos.com/etcd-test
        Operator:  Exists
    Refs:
      API Version:  apiextensions.k8s.io/v1
      Conditions:
        Last Transition Time:  2020-07-02T05:50:40Z
        Message:               no conflicts found
        Reason:                NoConflicts
        Status:                True
        Type:                  NamesAccepted
        Last Transition Time:  2020-07-02T05:50:41Z
        Message:               the initial names have been accepted
        Reason:                InitialNamesAccepted
        Status:                True
        Type:                  Established
      Kind:                    CustomResourceDefinition
      Name:                    etcdclusters.etcd.database.coreos.com <1>
...
      API Version:             operators.coreos.com/v1alpha1
      Conditions:
        Last Transition Time:  2020-07-02T05:50:39Z
        Message:               all available catalogsources are healthy
        Reason:                AllCatalogSourcesHealthy
        Status:                False
        Type:                  CatalogSourcesUnhealthy
      Kind:                    Subscription
      Name:                    etcd <2>
      Namespace:               test-project
...
      API Version:             operators.coreos.com/v1alpha1
      Conditions:
        Last Transition Time:  2020-07-02T05:50:43Z
        Last Update Time:      2020-07-02T05:50:43Z
        Status:                True
        Type:                  Installed
      Kind:                    InstallPlan
      Name:                    install-mhzm8 <3>
      Namespace:               test-project
...
      Kind:                    ClusterServiceVersion
      Name:                    etcdoperator.v0.9.4 <4>
      Namespace:               test-project
Events:                        <none>
----
<1> One of the CRDs.
<2> The Subscription.
<3> The InstallPlan.
<4> The CSV.

[id="ocp-4-5-notable-technical-changes"]
== Notable technical changes

{product-title} 4.5 introduces the following notable technical changes.

[discrete]
[id="ocp-4-5-osdk-v0-17-1"]
==== Operator SDK v0.17.2

{product-title} 4.5 supports Operator SDK v0.17.2, which introduces the
following notable technical changes:

* The `--crd-version` flag was added to the `new`, `add api`, `add crd`, and
`generate crds` commands so that users can opt-in to `v1` CRDs. The default setting
is `v1beta1`.

Ansible-based Operator enhancements include:

* Support for relative Ansible roles and playbooks paths in the Ansible-based Operator Watches files.
* Event statistics output to the Operator logs.

Helm-based Operator enhancements include:

* Support for Prometheus metrics.

[discrete]
[id="ocp-4-5-termination-grace-period-support"]
==== terminationGracePeriod parameter support

{product-title} now properly supports the `terminationGracePeriodSeconds`
parameter with the CRI-O container runtime.

[id="ocp-4-5-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to
be supported; however, it will be removed in a future release of this product
and is not recommended for new deployments. For the most recent list of major
functionality deprecated and removed within {product-title} {product-version},
refer to the table below. Additional details for more fine-grained functionality
that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *DEP*: _Deprecated_
* *REM*: _Removed_

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.3 |OCP 4.4 |OCP 4.5

|Service Catalog
|DEP
|DEP
|REM

|Template Service Broker
|DEP
|DEP
|REM

|OpenShift Ansible Service Broker
|DEP
|REM
|REM

|OperatorSources
|DEP
|DEP
|DEP

|CatalogSourceConfigs
|DEP
|DEP
|REM

|Operator Framework's Package Manifest Format
|GA
|DEP
|DEP

|v1beta1 CRDs
|GA
|GA
|DEP

|====

[id="ocp-4-5-deprecated-features"]
=== Deprecated features

[id="ocp-4-5-jenkins-pipeline-build-strategy"]
==== Jenkins Pipeline build strategy

The Jenkins Pipeline build strategy is now deprecated. You should use
Jenkinsfiles directly on Jenkins or OpenShift Pipelines instead.

[id="ocp-4-5-deprecated-v1beta1-crds"]
==== v1beta1 CRDs

The `apiextensions.k8s.io/v1beta1` API version for CustomResourceDefinitions
(CRDs) is now deprecated. It will be removed in a future release of
{product-title}.

See xref:ocp-4-5-olm-v1-crd[v1 CRD support in Operator Lifecycle Manager] for related details.

[id="ocp-4-5-deprecation-of-operatorsources"]
==== OperatorSources and CatalogSourceConfigs block cluster upgrades

OperatorSources and CatalogSourceConfigs have been deprecated for several
{product-title} releases. Starting in {product-title} 4.4, if there are any
custom OperatorSources or CatalogSourceConfigs objects present on the cluster,
the `marketplace` cluster Operator sets an `Upgradeable=false` condition and
issues a *Warning* alert. This means that upgrades to {product-title} 4.5 are
blocked if the objects are still installed.

[NOTE]
====
Upgrades to {product-title} 4.4 z-stream releases are still permitted in this
state.
====

In {product-title} 4.5, OperatorSources are still deprecated and only exist for
the use of the default OperatorSources. CatalogSourceConfigs, however, are now
removed.

See the
link:https://docs.openshift.com/container-platform/4.4/release_notes/ocp-4-4-release-notes.html#ocp-4-4-marketplace-apis-deprecated[{product-title} 4.4 release notes]
for how to convert OperatorSources and CatalogSourceConfigs to using
CatalogSources directly, which clears the alert and enables cluster upgrades to
{product-title} 4.5.

[id="ocp-4-5-removed-features"]
=== Removed features

[id="ocp-4-5-oc-commands-flags-removed"]
==== OpenShift CLI commands and flags removed

The following `oc` commands and flags are affected:

* The `oc policy can-i` command was deprecated in {product-title} 3.9 and has
been removed. You must use `oc auth can-i` instead.

* The `--image` flag previously used for the `oc new-app` and `oc new-build`
commands was deprecated in {product-title} 3.2 and has been removed. You must
use the `--image-stream` flag with these commands instead.

* The `--list` flag previously used in the `oc set volumes` command was
deprecated in {product-title} 3.3 and has been removed. The `oc set volumes`
lists volumes without a flag.

* The `-t` flag previously used in the `oc process` command was deprecated in
{product-title} 3.11 and has been removed. You must use the `--template` flag
with this command instead.

* The `--output-version` flag previously used in the `oc process` command was
deprecated in {product-title} 3.11 and has been removed. This flag was already
ignored.

* The `-v` flag previously used in the `oc set deployment-hook` command was
deprecated in {product-title} 3.11 and has been removed. You must use the
`--volumes` flag with this command instead.

* The `-v` and `--verbose` flags previously used in the `oc status` command were
deprecated in {product-title} 3.11 and have been removed. You must use the
`--suggest` flag with this command instead.

[id="ocp-4-5-oc-run-pod"]
==== The `oc run` OpenShift CLI command now only creates Pods

The `oc run` command can now only be used to create Pods. Use the `oc create` command instead to create other resources.

[id="ocp-4-5-service-catalog-removed"]
==== Service Catalog, Template Service Broker, and their Operators

[IMPORTANT]
====
Service Catalog is not installed by default in {product-title} 4; however, it
now blocks upgrades to {product-title} 4.5 if installed.
====

Service Catalog, Template Service Broker, Ansible Service Broker, and their
associated Operators were deprecated starting in {product-title} 4.2. Ansible
Service Broker, including Ansible Service Broker Operator and related APIs and
APBs, were removed in {product-title} 4.4.

Service Catalog, Template Service Broker, and their associated Operators are now
removed in {product-title} 4.5, including the related
`.servicecatalog.k8s.io/v1beta1` API.

[NOTE]
====
Templates are still available in {product-title} 4.5, but they are no longer
handled by Template Service Broker. By default, the Samples Operator handles Red
Hat Enterprise Linux (RHEL)-based {product-title} ImageStreams and Templates.
See
xref:../openshift_images/configuring-samples-operator.adoc#configuring-samples-operator[Configuring the Samples Operator]
for details.
====

The `service-catalog-controller-manager` and `service-catalog-apiserver` cluster
Operators were set to `Upgradeable=false` in 4.4. This means that they block
cluster upgrades to the next minor version, 4.5 in this case, if they are still
installed at that time. Upgrades to z-stream releases such as 4.4.z, however,
are still permitted in this state.

If Service Catalog and Template Service Broker are enabled in 4.4, specifically
if their management state is set to `Managed`, the web console warns cluster
administrators that these features are still enabled. The following alerts can
be viewed from the *Monitoring* -> *Alerting* page on a 4.4 cluster and have a
*Warning* severity:

* `ServiceCatalogAPIServerEnabled`
* `ServiceCatalogControllerManagerEnabled`
* `TemplateServiceBrokerEnabled`

If they are still enabled on a 4.4 cluster, cluster administrators can see
link:https://docs.openshift.com/container-platform/4.4/applications/service_brokers/uninstalling-service-catalog.html#sb-uninstalling-service-catalog[Uninstalling Service Catalog] and link:https://docs.openshift.com/container-platform/4.4/applications/service_brokers/uninstalling-template-service-broker.html#sb-uninstalling-template-service-broker[Uninstalling Template Service Broker]
in the {product-title} 4.4 documentation to uninstall it, which permits cluster
upgrades to 4.5.

In 4.5, a pair of Jobs are created in a new `openshift-service-catalog-removed`
namespace to run during the cluster upgrade process. Their behavior depends on
the management state of Service Catalog:

* `Removed`: The Jobs remove the following Service Catalog items:
** Operators
** namespaces
** Custom Resources (CRs)
** ClusterRoles
** ClusterRoleBindings

* `Unmanaged`: The Jobs skip removal and do nothing.

* `Managed`: The Jobs report an error in logs. This state is unlikely to occur because
upgrades would have been blocked. The Jobs take no other actions.

The Jobs and `openshift-service-catalog-removed` namespace will be removed in a
future {product-title} release.

[NOTE]
====
As of {product-title} 4.5, all Red Hat-provided service brokers have been
removed. Any other broker installed by users is not removed by the upgrade
process. This is to avoid removing any services that might have been deployed
using the brokers. Users must remove these brokers manually.
====

[id="ocp-4-5-csc-removed"]
==== CatalogSourceConfigs removed

CatalogSourceConfigs are now removed. See
xref:ocp-4-5-deprecation-of-operatorsources[OperatorSources and CatalogSourceConfigs block cluster upgrades]
for more details.

[id="ocp-4-5-bug-fixes"]
== Bug fixes

*apiserver-auth*

* Previously, `oc login` was performing an HTTP request to decide which CA bundle to use to connect to the remote login server. This generated a `remote error: tls: bad certificate` error in the OAuth server logs upon every login attempt, even though the login would succeed. The server certificate chain is now retrieved from an insecure TLS handshake, so the correct CA bundle is chosen and the OAuth server no longer logs bad certificate errors on login attempts. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1819688[*BZ#1819688*])

* Previously, the incomplete security context of the OAuth server Pods might cause the Pods to crashloop when they pick up a custom security context constraint (SCC) that reverts the default behavior. The security context of the OAuth server Pods was modified and a custom SCC no longer prevents the OAuth server Pods from running. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1824800[*BZ#1824800*])

* Previously, the Cluster Authentication Operator always disabled challenge authentication flows for any OIDC identity provider, which meant that logging in with `oc login` was not successful. Now, when an OIDC identity provider is configured, the Cluster Authentication Operator checks whether it allows for the Resource Owner Password Credentials grant and allows challenge-based login if it does. You can now log in using `oc login` for OIDC identity providers that allow the Resource Owner Password Credentials authorization grant. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1727983[*BZ#1727983*])

* Previously, the Cluster Authentication Operator did not properly close connections to the OAuth server, causing the rate of traffic to the OAuth server to grow as connections were being opened faster than they were being dropped. The connections are now properly closed and the Cluster Authentication Operator does not degrade the service of its own payload. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1826341[*BZ#1826341*])

* Previously, the `oauth-proxy` container exited with an error if there was an error reaching the `kube-apiserver` during configuration. This caused multiple container restarts if the `kube-apiserver` and controllers were not stable or fast enough. Now, multiple attempts to perform checks against the `kube-apiserver` are allowed when the `oauth-proxy` container starts, so that it only fails when the underlying infrastructure is truly broken.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=1779388[*BZ#1779388*])

*Bare Metal Hardware Provisioning*

* Because the UEFI boot process was using the `ipxe.efi` binary when using IPv4
networks, the boot process reported that there were no network devices found.
As a result, the Preboot eXecution Environment (PXE) boots the machines with
*No network devices*. The `dnsmasq.conf` file has been updated to use the
`snponly.efi` binary for IPv4 networks. The machines booting with PXE utilize
the UEFI network drivers and are able to deploy as they have network
connectivity.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1830161[*BZ#1830161*])

* If a cluster has networking issues during install (for example a slow image download) the install could fail. To address this problem, the PXE boot has been changed to include retries and the networking maximum number of retries has been increased for communication between the bare metal provisioner and the nodes being provisioned. The installer will now handle slow network conditions.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1822763[*BZ#1822763*])

*Build*

* Before starting a build, the {product-title} builder would parse the supplied Dockerfile and reconstruct a modified version of it to use for the build. This process included adding labels and handling substitutions of the images named in `FROM` instructions. The generated Dockerfile did not always correctly reconstruct `ENV` and `LABEL` instructions; sometimes the generated Dockerfile would include `=` characters, although the original Dockerfile did not include them. This caused the build to fail with a syntax error. When generating the modified Dockerfile, the original text for `ENV` and `LABEL` instructions are now used verbatim, fixing this issue. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1821858[*BZ#1821858*])

* Previously, the last few lines of error logs were not being attached to a build if a failure occurred in a build pod init container. Subsequently, build errors in init containers, such as malformed Git URLs, were hard to diagnose. The build controller has been updated so that error logs are attached to a build when failures occur in init containers. Build failures are now easier to diagnose. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1809862[*BZ#1809862*])

* Previously, build failures caused by failed image imports or invalid Dockerfiles were only categorized as generic build errors. Non-default build logging levels were required to diagnose such issues. New failure reasons have now been introduced for failed image imports and invalid Dockerfiles. Build failures relating to failed image imports or invalid Dockerfiles can now be identified within the build object status. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1809861[*BZ#1809861*])

* Previously, build label generation and validation did not include complete Kubernetes validation routines. Builds with certain valid build configuration names would fail due to an invalid build label value being created. The build controller and build API server now use complete Kubernetes validation routines to ensure added build labels meet label criteria. Builds with any valid build configuration name will now result in a valid build label value being created. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1777337[*BZ#1777337*])

* Previously Buildah interpretted variables in Dockerfiles literally, rather than parsing the value contained within a variable. Subsequently, builds would fail when Dockerfiles contained variables. Buildah has been updated to expand Dockerfile variables. Buildah will now parse Dockerfile environment variable values when building container images. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1810174[*BZ#1810174*])

* With the `RunOnceDuration` admission plug-in being disabled in OpenShift 4, an `activeDeadlineSeconds` value was not automatically applied to build pods. Pods with `activeDeadlineSeconds` set to nil are matched to resource quotas that include `NotTerminating` scope. Subsequently, build pods failed to start due to quota limitations, in namespaces that had resource quotas with `NotTerminating` scope defined. The build controller now applies a suitable default `activeDeadlineSeconds` value to build pods. Build pods are now handled properly in namespaces that have resource quotas that include `NotTerminating` scope. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1829447[*BZ#1829447*])

*Cloud compute*

* The cluster autoscaler expects provider IDs across node and machine objects to be an exact match. Previously, if a machine configuration included a resource group name that had a mix of upper and lower case characters, the cluster autoscaler would terminate the machine after fifteen minutes, given that a match was not found. Resource group names are now sanitized so that all characters are set to lowercase. Now, matching provider IDs are correctly identified even when resource group names are entered using a mix of upper and lower case characters. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1837341[*BZ#1837341*])

* Previously, the `metadata` field within machine and MachineSet specifications was not validated when MachineSets were created or updated. Invalid metadata caused unmarshalling errors leading to controllers not being able to process objects. The `metadata` field is now validated when MachineSets are created or updated and invalid entries return an error. Invalid metadata is now identified before MachineSets are created so that subsequent object processing errors are prevented. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1702089[*BZ#1702089*])

* Occasionally during scale down operations, the last machine in a MachineSet will contain deletion annotations. That machine will not be removed by the autoscaler if the minimum MachineSet size is reached before its deletion. Previously, the last machine's deletion annotations would not have been removed after a scale down. A fix has been introduced that changes the way machine annotations are unmarked after a scale down. Now, the annotations no longer persist on the last machine in the MachineSet. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1820410[*BZ#1820410*])

* Previously, the AWS Identity and Access Management (IAM) role assigned to worker nodes did not have sufficient permissions to access the AWS Key Management Service (KMS) key to decrypt the Amazon Elastic Block Store (EBS) volume on mount. Subsequently, Amazon Elastic Compute Cloud (EC2) instances would be accepted, but they would fail to start because they could not read from their root drive. The required permissions have now been granted for EC2 instances to be able to decrypt KMS encrypted EBS volumes with Customer Managed Keys. When using a Customer Managed Key for encrypting EBS volumes, instances now have the required permissions to start successfully. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1815219[*BZ#1815219*])

* The `replicas` field in a MachineSet specification can be set to nil. Previously, if the autoscaler could not determine the number of replicas within a MachineSet, autoscaling operations were prevented. Now, if the `replicas` field is not set, the autoscaler makes a scaling decision based on the last number of observed replicas according to the MachineSet. Autoscaling operations can now proceed even if the `replicas` field in a MachineSet specification is set to nil, assuming that the MachineSet controller has recently synchronized the number of replicas to `MachineSet.Status.Replicas`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1820654[*BZ#1820654*])

* Previously, the autoscaler would reduce the size of a node group by one on every call to `DeleteNodes`, even if an existing node deletion had not yet completed. This resulted in a cluster having less than the minimum required node count. Now, if a node's machine already has a deletion timestamp, the size of the node group is not reduced further. This prevents the autoscaler from reducing the node count to less than the required capacity when it calls `DeleteNodes`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1804738[*BZ#1804738*])

*Cluster Version Operator*

* The Cluster Version Operator (CVO) had a race condition where it would consider a timed-out update reconciliation cycle a successful update. This only happened for restricted network clusters where the Operator timed out attempting to fetch release image signatures. This bug caused the CVO to enter its shuffled-manifest reconciliation mode, which could break the cluster if the manifests were applied in an order that the components could not handle. The CVO now treats timed-out updates as failures, so it no longer enters reconciling mode before the update succeeds. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1843526[*BZ#1843526*])

*Console Metal3 Plugin*

* Previously, there was no space between the *Powering on/off* message and the bare metal host link in the web console. A space has been added so that the message now reads properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1819614[*BZ#1819614*])

*Web console (Developer perspective)*

* Previously, it was difficult to see the list of Pods or resources associated with a Knative service in the *Topology* view. With this bug fix, when you select the Knative service, the sidebar displays a list of Pods along with a link to see the logs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1801752[*BZ#1801752*])

* When you edited an existing query using the PromQL editor in the *metrics* tab of the *Monitoring* view, the cursor moved to the end of the line. With this bug fix, the PromQL editor works as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1806114[*BZ#1806114*])

* For Knative images, in the *Add* -> *From Git* option, the *Advanced Options* for *Routing* would not provide a prefetched container port option. Also, if you created the service without updating the default port value of `8080`, the revisions would not show. With this bug fix, the user can select from the available port options using the drop-down list or provide input if they want to use another port and the revisions are shown as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1806552[*BZ#1806552*])

* Previously, a Knative service created using the CLI could not be edited using the console because the images could not be fetched. Now, if the associated ImageStreams are not found while editing, the value provided by the user for the container image in the YAML file is used. This allows the user to edit the service using the console, even if the service was created using the CLI. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1806994[*BZ#1806994*])

* In the *Topology* view, editing the image name in the external image registry for a Knative service did not create a new revision. With this bug fix, a new revision of the service is created when the name of the service is changed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1807868[*BZ#1807868*])

* When you used the *Add* -> *Container Image* option, and then selected the *Image stream tag from internal registry* option, the *ImageStreams* drop-down list did not list the option to deploy images from the *OpenShift* namespace. However, you were able to access them through the CLI. With this bug fix, all users have access to images in the *OpenShift* namespace through the console and the CLI. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1822112[*BZ#1822112*])

* Previously, in the *Pipeline Builder*, when you edited a Pipeline that referenced a Task that did not exist, the entire screen would go white. This fix now displays an icon to indicate that an action is required and a drop-down list is displayed to easily update the Task reference. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1839883[*BZ#1839883*])

* In the *Pipelines Details* page, when you changed existing fields in the *Parameters* and the *Resources* tabs, the *Save* button was disabled even though the new changes were detected. The validation criteria has now been modified and the *Save* button is enabled to submit changes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1804852[*BZ#1804852*])

* In the *Add* -> *From Git* option, the Pipeline templates provided by the OpenShift Pipelines Operator would fail when the *Deployment* or *Knative Services* resource options were selected. This bug fix adds support to use the resource type as well as the runtime to determine the Pipeline template, thus providing resource-specific Pipeline templates. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1796185[*BZ#1796185*])

* When a Pipeline was created using the *Pipeline Builder* and a Task parameter of the type array was used, the Pipeline did not start. With this bug fix, both array and string type parameters are supported. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1813707[*BZ#1813707*])

* In the *Topology* view, filtering nodes by application returned an error when the namespace had Operator-backed services. This bug fix adds the logic to filter out the Operator-backed service nodes based on the selected application group. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1810532[*BZ#1810532*])

* The *Developer Catalog* showed no catalog results until you selected the *Clear All Filters* option. With this bug fix, all catalog items are seen by default and you do not need to clear all filters. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1835548[*BZ#1835548*])

* Previously, users were unable to add environment variables for `knative` services. As a result, apps where `envVariables` would be needed might not have worked as expected. Now, support has been added for environment variables. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1839114[*BZ#1839114*])

* The Developer Console Navigation menu is now available and is aligned with the latest UX designs.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=1801278[*BZ#1801278*])

* Time Range and Refresh Interval drop menus have been added in the Monitoring dashboard tab in Developer Perspective.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=1807210[*BZ#1807210*])

* No Pipeline Resources were created in the namespace although the Start Pipeline modal required one. The user would see a disabled and empty dropdown above fields, losing some context of what the fields were for. With this bug fix, *Create Pipeline Resource* gives the user context of what they were doing inline in the Start Pipeline modal. The user now has a better experience starting a Pipeline from the start modal when there are no Pipeline Resources created in the namespace.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1826526[*BZ#1826526*])

* Layout padding was missing, which allowed the title to flow over the *Close* button. If text was over the *Close* button, it made it difficult to click. The layout is now fixed to prevent the title from overlapping the *Close* button and the button is now always accessible via mouse click. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1796516[*BZ#1796516*])

* Pipeline Builder incorrectly interpreted a default value of an empty string (`''`) as having no default. Some Operator-provided tasks needed this to be the default and, therefore, had issues working without it. Check for a default property and do not assume the validity of the value. Now, any values that the OpenShift Pipeline Operator deems as a valid default value are respected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1829567[*BZ#1829567*])

* The Pipeline Builder reads Task/ClusterTask definitions and incorrectly assumed that all Parameters were of type `string`.
When a Task Param of type `array` was encountered, it would cast the array to a string and represent it, losing the type; it would produce a value to the Task param as `string`, thus breaking the contract with the Task. The `array` type is now supported in the UI and the type is properly retrained. Managing both types allows the Pipeline Builder to work the way it was intended. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1813707[*BZ#1813707*])

* The Pipeline page was inconsistent with other pages. The *Create Pipeline* button was always enabled and did not take into consideration when no projects were available. The *Create Pipeline* button is now removed when the Getting Started guide is enabled. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1792693[*BZ#1792693*])

* Metrics queries for the *Dashboard & Metrics* tab got updated in the design document. The code need to be synced with w.r.t queries. The queries are now updated and the order of the metrics queries and their labels are synced with the design. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1806518[*BZ#1806518*])

* The tile description variable was incorrectly set to be the CRD description appended with the CSV description. This caused the tile descriptions to be wrong. The tile descriptions are now back to the original value and the appended value is now moved to its own variable. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1814639[*BZ#1814639*])

* The `eventSources` API Group is updated to the latest supported API Group, `sources.knative.dev`. This update allows sources generated by the new API Group to be recognized in the *Topology* view of the web console. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1836805[*BZ#1836805*])

* With the release of Red Hat OpenShift Serverless 1 Serverless Operator version 1.7.1, the Operator is generally available. The Tech Preview badge in the Developer perspective of the web console has been removed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1827042[*BZ#1827042*])

[id="ocp-4-5-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These
experimental features are not intended for production use. Note the
following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview
Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.3 |OCP 4.4 |OCP 4.5

|Precision Time Protocol (PTP)
|TP
|TP
|TP

|`oc` CLI Plug-ins
|TP
|TP
|TP

|experimental-qos-reserved
|TP
|TP
|TP

|Pod Unidler
|TP
|TP
|TP

|Ephemeral Storage Limit/Requests
|TP
|TP
|TP

|Descheduler
|-
|TP
|TP

|Podman
|TP
|TP
|TP

|Sharing Control of the PID Namespace
|TP
|TP
|GA

|OVN-Kubernetes Pod network provider
|TP
|TP
|TP

|HPA custom metrics adapter based on Prometheus
|TP
|TP
|TP

|HPA for memory utilization
|TP
|TP
|TP

|Machine health checks
|TP
|GA
|GA

|Three-node bare metal deployments
|TP
|TP
|GA

|Helm CLI
|TP
|GA
|GA

|Service Binding
|TP
|TP
|TP

|Log forwarding
|TP
|TP
|TP

|User workload monitoring
|TP
|TP
|TP

|OpenShift Serverless
|TP
|GA
|GA

|Compute Node Topology Manager
|TP
|TP
|GA

|Raw Block with Cinder
|TP
|TP
|TP

|External provisioner for AWS EFS
|TP
|TP
|TP

|CSI volume snapshots
|-
|TP
|TP

|CSI volume cloning
|-
|TP
|GA

|CSI AWS EBS Driver Operator
|-
|-
|TP

|OpenStack Manila CSI Driver Operator
|-
|-
|GA

|CSI inline ephemeral volumes
|-
|-
|TP

|OpenShift Pipelines
|-
|TP
|TP

|Vertical Pod Autoscaler
|-
|-
|TP

|Operator API
|-
|-
|TP

|====

[id="ocp-4-5-known-issues"]
== Known issues

* When upgrading to a new {product-title} z-stream release, connectivity to the API server might be interrupted as nodes are upgraded, causing API requests to fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1845411[*BZ#1845411*])

* When upgrading to a new {product-title} z-stream release, connectivity to routers might be interrupted as router pods are updated. For the duration of the upgrade, some applications might not be consistently reachable. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1809665[*BZ#1809665*])

* Because the `ImageContentSourcePolicy` for image registry pull-through is not
yet supported, the deployment Pod cannot mirror images by using a digest ID if
the imagestream has the pull-through policy enabled. In this case, an
`ImagePullBackOff` error displays.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1787112[*BZ#1787112*])

* The Che Workspace Operator was updated to use the DevWorkspace custom resource instead of the Workspace custom resource. However, the OpenShift web terminal continues to use the Workspace custom resource. Because of this, the OpenShift web terminal fails to work with the latest version of the Che Workspace Operator. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1846969[*BZ#1846969*])

* A `basic-user` is unable to view the *Dashboard* and *Metrics* tabs in the *Monitoring* view of the *Developer* perspective. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1846409[*BZ#1846409*])

* In the *Topology* view, when you right-click a Knative service, the *Edit Application Grouping* option is displayed twice in the context menu.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=1849107[*BZ#1849107*])

* The Special Resources Operator (SRO) cannot be deployed successfully on
{product-title} 4.5. This prevents the deployment of NVIDIA drivers, which
are required by the cluster to run workloads requiring GPU resources. Also, the
Topology Manager feature could not be tested with GPU resources as a result of
this known issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1847805[*BZ#1847805*])

[id="ocp-4-5-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} 4.5 are released
as asynchronous errata through the Red Hat Network. All {product-title} 4.5
errata is https://access.redhat.com/downloads/content/290/[available on the Red
Hat Customer Portal]. See the
https://access.redhat.com/support/policy/updates/openshift[{product-title}
Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account
settings for Red Hat Subscription Management (RHSM). When errata notifications
are enabled, users are notified via email whenever new errata relevant to their
registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming
{product-title} entitlements for {product-title} errata notification
emails to generate.
====

This section will continue to be updated over time to provide notes on
enhancements and bug fixes for future asynchronous errata releases of
{product-title} 4.5. Versioned asynchronous releases, for example with the form
{product-title} 4.5.z, will be detailed in subsections. In addition, releases
in which the errata text cannot fit in the space provided by the advisory will
be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on
xref:../updating/updating-cluster.adoc#updating-cluster[updating your cluster]
properly.
====

[[rhba-2020-2409]]
=== RHBA-2020:2409 - {product-title} 4.5 Image release and bug fix advisory

Issued: 2020-07-09

{product-title} release 4.5 is now available. The list of container images and
bug fixes includes in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2020:2409[RHBA-2020:2409] advisory.
The RPM packages included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2020:2408[RHBA-2020:2408] advisory.

Space precluded documenting all of the container images for this release in the
advisory. See the following article for notes on the container images in this
release:

link:https://access.redhat.com/solutions/5184131[{product-title} 4.5.0 container image list]
