:_mod-docs-content-type: ASSEMBLY
[id="tempo-troubleshoot"]
= Troubleshooting
include::_attributes/common-attributes.adoc[]
:context: tempo-troubleshoot

toc::[]

There are multiple ways on you can diagnistic and fix common problems with the Tempo stack, for some of them is very useful
to inspect the logs of the different components along with some useful metrics.


== Getting the Tempo stack logs
In order to get the logs from the tempo component, first localice the component you want to see the logs, this can be done listing all the deployments
on an specific namespace.

Then once you get those you can see which pods belong to that component and watch for the logs using `oc logs` command or the web console to retrieve the logs.


== Problems quering trace

Once you've confirmed that the data has been successfully ingested into Tempo, it's time to look into potential issues with querying the data.


Check the logs of the `query-frontend`. The `query-frontend` pod runs with two containers (`query-frontend` and `querier`. If you see something similiar to the following errors, it means that there is an error in the query path.

[source,terminal]
----
level=info ts=XXXXXXX caller=frontend.go:63 method=GET traceID=XXXXXXXXX url=/api/traces/XXXXXXXXX duration=xxxx status=500
could not dial 10.X.X.X:3200 connection refused
----

The tempo-querier and tempo-queryfrontend were unable to communicate with each other. Review the logs of both services to determine if either has crashed. Additionally, check your network settings and policies to ensure proper communication between them.


Other useful metrics to review:

- `cortex_query_frontend_connected_clients` exposed by the `query-frontend`. if `cortex_query_frontend_connected_clients`  > 0 is an indication than `queriers` are connected to the `query-frontend`.


== Problems ingesting traces

There are two potential problems with data ingestion into Tempo. Either the spans are not being sent properly, or they are not being sampled.


Unhealthy ingesters can result from failing due to out-of-memory (OOM) issues or scale-down events. If you have unhealthy ingesters, you might see some errors in the logs.


=== Review ingestion limits

In high-volume tracing environments, the default trace limits may not be adequate.If spans are being refused due to these limits, you will see logs like this at the distributor:


You will also see the `tempo_discarded_spans_total` metric incremented.
