[[architecture-additional-concepts-networking]]
= Networking
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview

Kubernetes ensures that pods are able to network with each other, and
allocates each pod an IP address from an internal network. This ensures all
containers within the pod behave as if they were on the same host. Giving each
pod its own IP address means that pods can be treated like physical hosts or
virtual machines in terms of port allocation, networking, naming, service
discovery, load balancing, application configuration, and migration.

Creating links between pods is unnecessary. However, it is not recommended that
you have a pod talk to another directly by using the IP address. Instead, we
recommend that you create a
xref:../../architecture/core_concepts/pods_and_services.adoc#services[service], then interact
with the service.

[[architecture-additional-concepts-openshift-dns]]
== {product-title} DNS

If you are running multiple
xref:../../architecture/core_concepts/pods_and_services.adoc#services[services], such as
frontend and backend services for use with multiple pods, in order for the
frontend pods to communicate with the backend services, environment variables
are created for user names, service IP, and more. If the service is deleted and
recreated, a new IP address can be assigned to the service, and requires the
frontend pods to be recreated in order to pick up the updated values for the
service IP environment variable. Additionally, the backend service has to be
created before any of the frontend pods to ensure that the service IP is
generated properly and that it can be provided to the frontend pods as an
environment variable.

For this reason, {product-title} has a built-in DNS so that the services can be
reached by the service DNS as well as the service IP/port. {product-title}
supports split DNS by running
link:https://github.com/skynetservices/skydns[SkyDNS] on the master that answers
DNS queries for services. The master listens to port 53 by default.

When the node starts, the following message indicates the Kubelet is correctly
resolved to the master:

----
0308 19:51:03.118430    4484 node.go:197] Started Kubelet for node
openshiftdev.local, server at 0.0.0.0:10250
I0308 19:51:03.118459    4484 node.go:199]   Kubelet is setting 10.0.2.15 as a
DNS nameserver for domain "local"
----

If the second message does not appear, the Kubernetes service may not be available.

On a node host, each container's nameserver has the master name added to the
front, and the default search domain for the container will be
`._<pod_namespace>_.cluster.local`. The container will then direct any nameserver
queries to the master before any other nameservers on the node, which is the
default behavior for Docker-formatted containers. The master will answer queries on the `.cluster.local` domain
that have the following form:

.DNS Example Names
[cols=".2,.^5,8",options="header"]
|===

|Object Type |Example

|Default
|<pod_namespace>.cluster.local

|Services
|<service>.<pod_namespace>.svc.cluster.local

|Endpoints
|<name>.<namespace>.endpoints.cluster.local
|===

This prevents having to restart frontend pods in order to pick up new services,
which creates a new IP for the service. This also removes the need to use
environment variables, as pods can use the service DNS. Also, as the DNS does not change, you can reference database services as
`db.local` in config files. Wildcard lookups are also supported, as any lookups
resolve to the service IP, and removes the need to create the backend service
before any of the frontend pods, since the service name (and hence DNS) is
established upfront.

This DNS structure also covers headless services, where a portal IP is not
assigned to the service and the kube-proxy does not load-balance or provide
routing for its endpoints. Service DNS can still be used and responds with
multiple A records, one for each pod of the service, allowing the client to
round-robin between each pod.

[[network-plugins]]
== Network Plug-ins

{product-title} supports the Kubernetes
link:https://kubernetes.io/docs/admin/network-plugins/#cni[Container Network
Interface (CNI)] as the interface between the {product-title} and Kubernetes.
Software defined network (SDN) plug-ins are a powerful and flexible way to match
network capabilities to your networking needs. There are several
xref:../../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[OpenShift SDN plugins]
available, as well as third party plug-ins. Additional plug-ins that support the
CNI interface can be added as needed.

The following network plug-ins are currently supported by {product-title}:

- xref:../../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[OpenShift SDN]

- xref:../../architecture/additional_concepts/sdn.adoc#network-isolation-multitenant[OpenShift Network Isolation SDN]

- xref:../../architecture/additional_concepts/flannel.adoc#architecture-additional-concepts-flannel[Flannel SDN]

ifdef::openshift-origin[] 
 xref:../../architecture/additional_concepts/contiv.adoc#architecture-additional-concepts-contiv[Contiv SDN] 
 endif::[]
 
ifdef::openshift-enterprise,openshift-origin[]
- xref:../../architecture/additional_concepts/networking.adoc#nuage-sdn[Nuage Networks SDN]
endif::[]

[[openshift-sdn]]

=== OpenShift SDN

{product-title} deploys a software-defined networking (SDN) approach for
connecting pods in an {product-title} cluster. The OpenShift SDN connects all
pods across all node hosts, providing a unified cluster network.

OpenShift SDN is automatically installed and configured as part of the
Ansible-based installation procedure. See the
xref:../../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[OpenShift SDN] section
for more information.


[[flannel-sdn]]
=== Flannel SDN

Flannel is an alternative SDN. See the
xref:../../architecture/additional_concepts/flannel.adoc#architecture-additional-concepts-flannel[Flannel
SDN] topic for more information.

[[contiv-sdn]]
=== Contiv SDN

Contiv is an alternative SDN. See
xref:../../architecture/additional_concepts/contiv.adoc#architecture-additional-concepts-contiv[Contiv
SDN] for more information.

ifdef::openshift-enterprise,openshift-origin[]
[[nuage-sdn]]
=== Nuage SDN for {product-title}

xref:../../install_config/configuring_nuagesdn.adoc#install-config-configuring-nuage-sdn[Nuage
Networks'] SDN solution delivers highly scalable, policy-based overlay
networking for pods in an {product-title} cluster. Nuage SDN can be installed
and configured as a part of the Ansible-based installation procedure. See the
xref:../../install_config/configuring_nuagesdn.adoc#install-config-configuring-nuage-sdn[Advanced
Installation] section for information on how to install and deploy
{product-title} with Nuage SDN.

link:http://www.nuagenetworks.net[Nuage Networks] provides a highly scalable,
policy-based SDN platform called Virtualized Services Platform (VSP). Nuage VSP
uses an SDN Controller, along with the open source Open vSwitch for the data
plane.

Nuage uses overlays to provide policy-based networking between {product-title}
and other environments consisting of VMs and bare metal servers. The platformâ€™s
real-time analytics engine enables visibility and security monitoring for
{product-title} applications.

Nuage VSP integrates with {product-title} to allows business applications to be
quickly turned up and updated by removing the network lag faced by DevOps teams.

.Nuage VSP Integration with {product-title}
image::nuagesdn_integration.png[Nuage VSP Integration with {product-title}]

There are two specific components responsible for the integration.

. The *nuage-openshift-monitor* service, which runs as a separate service on the
{product-title} master node.
. The *vsp-openshift* plug-in, which is invoked by the {product-title} runtime on each of the nodes of the cluster.

Nuage Virtual Routing and Switching software (VRS) is based on open source Open
vSwitch and is responsible for the datapath forwarding. The VRS runs on each
node and gets policy configuration from the controller.

[[architecture-additional-concepts-nuage-terminology]]
*Nuage VSP Terminology*

.Nuage VSP Building Blocks
image::nuage_terminology.png[Nuage VSP Building Blocks]

 . Domains: An organization contains one or more domains. A domain is a single "Layer 3" space. In standard networking terminology, a domain maps to a VRF instance.
 . Zones: Zones are defined under a domain. A zone does not map to anything on the network directly, but instead acts as an object with which policies are associated such that all endpoints in the zone adhere to the same set of policies.
 . Subnets: Subnets are defined under a zone. A subnet is a specific Layer 2 subnet within the domain instance. A subnet is unique and distinct within a domain, that is, subnets within a Domain are not allowed to overlap or to contain other subnets in accordance with the standard IP subnet definitions.
 . VPorts: A VPort is a new level in the domain hierarchy, intended to provide more granular configuration. In addition to containers and VMs, VPorts are also used to attach Host and Bridge Interfaces, which provide connectivity to Bare Metal servers, Appliances, and Legacy VLANs.
 . Policy Group: Policy Groups are collections of VPorts.

[[architecture-additional-concepts-nuage-concepts]]
*Mapping of Constructs*

Many
xref:../../architecture/core_concepts/index.adoc#architecture-core-concepts-index[{product-title}
concepts] have a direct mapping to Nuage VSP constructs:

.Nuage VSP and {product-title} mapping
image::nuageopenshift_mapping.png[Nuage VSP and {product-title} mapping]

A Nuage subnet is not mapped to an {product-title} node, but a subnet for a
particular project can span multiple nodes in {product-title}.

A pod spawning in {product-title} translates to a virtual port being created in
VSP. The *vsp-openshift* plug-in interacts with the VRS and gets a policy for
that virtual port from the VSD via the VSC. Policy Groups are supported to group
multiple pods together that must have the same set of policies applied to them.
Currently, pods can only be assigned to policy groups using the
xref:../../install_config/configuring_nuagesdn.adoc#nuage-sdn-and-openshift[operations
workflow] where a policy group is created by the administrative user in VSD. The
pod being a part of the policy group is specified by means of
`nuage.io/policy-group` label in the specification of the pod.

[[architecture-additional-concepts-nuage-integration-components]]
==== Integration Components

Nuage VSP integrates with {product-title} using two main components:

. *nuage-openshift-monitor*
. *vsp-openshift plugin*

[[nuage-openshift-monitor]]

*nuage-openshift-monitor*

*nuage-openshift-monitor* is a service that monitors the {product-title} API
server for creation of projects, services, users, user-groups, etc.

[NOTE]
=====
In case of a Highly Available (HA) {product-title} cluster with multiple
masters, *nuage-openshift-monitor* process runs on all the masters independently
without any change in functionality.
=====

For the developer workflow, *nuage-openshift-monitor* also auto-creates VSD
objects by exercising the VSD REST API to map {product-title} constructs to VSP
constructs. Each cluster instance maps to a single domain in Nuage VSP. This
allows a given enterprise to potentially have multiple cluster installations -
one per domain instance for that Enterprise in Nuage. Each {product-title}
project is mapped to a zone in the domain of the cluster on the Nuage VSP.
Whenever *nuage-openshift-monitor* sees an addition or deletion of the project,
it instantiates a zone using the VSDK APIs corresponding to that project and
allocates a block of subnet for that zone. Additionally, the
*nuage-openshift-monitor* also creates a network macro group for this project.
Likewise, whenever *nuage-openshift-monitor* sees an addition ordeletion of a
service, it creates a network macro corresponding to the service IP and assigns
that network macro to the network macro group for that project (user provided
network macro group using labels is also supported) to enable communication to
that service.

For the developer workflow, all pods that are created within the zone get IPs
from that subnet pool. The subnet pool allocation and management is done by
*nuage-openshift-monitor* based on a couple of plug-in specific parameters in
the master-config file. However the actual IP address resolution and vport
policy resolution is still done by VSD based on the domain/zone that gets
instantiated when the project is created. If the initial subnet pool is
exhausted, *nuage-openshift-monitor* carves out an additional subnet from the
cluster CIDR to assign to a given project.

For the operations workflow, the users specify Nuage recognized labels on their
application or pod specification to resolve the pods into specific user-defined
zones and subnets. However, this cannot be used to resolve pods in the zones or
subnets created via the developer workflow by *nuage-openshift-monitor*.

[NOTE]
=====
In the operations workflow, the administrator is responsible for pre-creating
the VSD constructs to map the pods into a specific zone/subnet as well as allow
communication between OpenShift entities (ACL rules, policy groups, network
macros, and network macro groups). Detailed description of how to use Nuage
labels is provided in the link:http://support.alcatel-lucent.com[Nuage VSP
Openshift Integration Guide].
=====

[[vsp-openshift-plugin]]

*vsp-openshift Plug-in*

The vsp-openshift networking plug-in is called by the {product-title} runtime on
each {product-title} node. It implements the network plug-in init and pod setup,
teardown, and status hooks. The vsp-openshift plug-in is also responsible for
allocating the IP address for the pods. In particular, it communicates with the
VRS (the forwarding engine) and configures the IP information onto the pod.
endif::[]
