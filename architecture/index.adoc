[[architecture-index]]
= {product-title}
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:linkattrs:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

ifdef::openshift-origin,openshift-online,openshift-enterprise,openshift-dedicated[]
OpenShift v3 is a layered system designed to expose underlying Docker-formatted
container image and Kubernetes concepts as accurately as possible, with a focus
on easy composition of applications by a developer. For example, install Ruby,
push code, and add MySQL.

Unlike OpenShift v2, more flexibility of configuration is exposed after creation
in all aspects of the model. The concept of an application as a separate object
is removed in favor of more flexible composition of "services", allowing two web
containers to reuse a database or expose a database directly to the edge of the
network.
endif::[]
ifdef::atomic-registry[]
{product-title} is based on OpenShift technology which features an
embedded registry based on the upstream
link:https://github.com/docker/distribution[Docker Distribution,
role="external", window="_blank"] library. {product-title} provides the
following capabilities:

* A user-focused xref:infrastructure_components/web_console.adoc#architecture-infrastructure-components-web-console[web console].
* Global xref:additional_concepts/authentication.adoc#architecture-additional-concepts-authentication[identity provider authentication].
* A xref:core_concepts/projects_and_users.adoc#architecture-core-concepts-projects-and-users[project namespace] model to
enable teams to collaborate through xref:additional_concepts/authorization.adoc#architecture-additional-concepts-authorization[role-based access control (RBAC)]
authorization.
* A xref:infrastructure_components/kubernetes_infrastructure.adoc#architecture-infrastructure-components-kubernetes-infrastructure[Kubernetes-based cluster]
to manage services.
* An image abstraction called xref:core_concepts/builds_and_image_streams.adoc#architecture-core-concepts-builds-and-image-streams[image streams] to enhance image management.

endif::[]
ifdef::openshift-origin,openshift-online,openshift-enterprise,openshift-dedicated[]

== What Are the Layers?

The Docker service provides the abstraction for packaging and creating
Linux-based, lightweight
xref:core_concepts/containers_and_images.adoc#containers[container images]. Kubernetes
provides the
xref:infrastructure_components/kubernetes_infrastructure.adoc#architecture-infrastructure-components-kubernetes-infrastructure[cluster management] and orchestrates containers on multiple hosts.

{product-title} adds:

- Source code management,
xref:core_concepts/builds_and_image_streams.adoc#builds[builds], and
xref:core_concepts/deployments.adoc#architecture-core-concepts-deployments[deployments] for developers
- Managing and promoting
xref:core_concepts/containers_and_images.adoc#docker-images[images] at scale
as they flow through your system
- Application management at scale
- Team and user tracking for organizing a large developer organization
- Networking infrastructure that supports the cluster

.{product-title} Architecture Overview
image::architecture_overview.png[{product-title} Architecture Overview]
endif::[]

== What Is the {product-title} Architecture?

{product-title} has a microservices-based architecture of smaller, decoupled units
that work together. It can run on top of (or alongside) a
xref:infrastructure_components/kubernetes_infrastructure.adoc#architecture-infrastructure-components-kubernetes-infrastructure[Kubernetes
cluster], with data about the objects stored in
xref:infrastructure_components/kubernetes_infrastructure.adoc#master[etcd], a
reliable clustered key-value store. Those services are broken down by function:

- xref:../rest_api/index.adoc#rest-api-index[REST APIs], which expose each of the
xref:core_concepts/index.adoc#architecture-core-concepts-index[core objects].
- Controllers, which read those APIs, apply changes to other objects, and report
status or write back to the object.

Users make calls to the REST API to change the state of the system. Controllers
use the REST API to read the user's desired state, and then try to bring the
other parts of the system into sync. For example, when a user requests a
xref:core_concepts/builds_and_image_streams.adoc#builds[build] they create a
"build" object. The build controller sees that a new build has been created, and
runs a process on the cluster to perform that build. When the build completes,
the controller updates the build object via the REST API and the user sees that
their build is complete.

ifdef::openshift-origin,openshift-online,openshift-enterprise,openshift-dedicated[]
The controller pattern means that much of the functionality in {product-title}
is extensible. The way that builds are run and launched can be customized
independently of how images are managed, or how
xref:core_concepts/deployments.adoc#architecture-core-concepts-deployments[deployments] happen. The controllers are
performing the "business logic" of the system, taking user actions and
transforming them into reality. By customizing those controllers or replacing
them with your own logic, different behaviors can be implemented. From a system
administration perspective, this also means the API can be used to script common
administrative actions on a repeating schedule. Those scripts are also
controllers that watch for changes and take action. {product-title} makes the
ability to customize the cluster in this way a first-class behavior.
endif::[]

To make this possible, controllers leverage a reliable stream of changes to the
system to sync their view of the system with what users are doing. This event
stream pushes changes from etcd to the REST API and then to the controllers as
soon as changes occur, so changes can ripple out through the system very quickly
and efficiently. However, since failures can occur at any time, the controllers
must also be able to get the latest state of the system at startup, and confirm
that everything is in the right state. This resynchronization is important,
because it means that even if something goes wrong, then the operator can
restart the affected components, and the system double checks everything before
continuing. The system should eventually converge to the user's intent, since
the controllers can always bring the system into sync.

== How Is {product-title} Secured?

The {product-title} and Kubernetes APIs
xref:additional_concepts/authentication.adoc#architecture-additional-concepts-authentication[authenticate] users who present
credentials, and then xref:additional_concepts/authorization.adoc#architecture-additional-concepts-authorization[authorize]
them based on their role. Both developers and administrators can be
authenticated via a number of means, primarily
xref:additional_concepts/authentication.adoc#oauth[OAuth tokens] and SSL
certificate authorization.

Developers (clients of the system) typically make REST API calls from a
xref:../cli_reference/index.adoc#cli-reference-index[client program] like `oc` or to the
xref:infrastructure_components/web_console.adoc#architecture-infrastructure-components-web-console[web console] via their browser,
and use OAuth bearer tokens for most communications. Infrastructure components
(like nodes) use client certificates generated by the system that contain their
identities. Infrastructure components that run in containers use a token
associated with their xref:../dev_guide/service_accounts.adoc#dev-guide-service-accounts[service account]
to connect to the API.

Authorization is handled in the {product-title} policy engine, which defines
actions like "create pod" or "list services" and groups them into roles in a
policy document. Roles are bound to users or groups by the user or group
identifier. When a user or service account attempts an action, the policy engine
checks for one or more of the roles assigned to the user (e.g., cluster
administrator or administrator of the current project) before allowing it to
continue.

ifdef::openshift-origin,openshift-online,openshift-enterprise,openshift-dedicated[]
Since every container that runs on the cluster is associated with a service
account, it is also possible to associate
xref:../dev_guide/secrets.adoc#dev-guide-secrets[secrets] to those service accounts and have them
automatically delivered into the container. This enables the infrastructure to
manage secrets for pulling and pushing images, builds, and the deployment
components, and also allows application code to easily leverage those secrets.
endif::[]

== What Is the {product-title} Network Architecture?

{product-title} uses software-defined networking (
xref:../architecture/additional_concepts/sdn.html#architecture-additional-concepts-sdn[SDN]
) plug-ins to provide a unified cluster network that manages communication in the cluster.
The SDN plug-ins use the Kubernetes
link:https://kubernetes.io/docs/admin/network-plugins/#cni[Container Network Interface (CNI)]
plug-in interface which supports many
xref:additional_concepts/networking.html#network-plugins[network plug-ins].
{product-title} includes the default
xref:additional_concepts/sdn.adoc#[ovs-multitenant and ovs-subnet]
plug-ins. The ovs-multitenant plug-in provides
xref:../architecture/additional_concepts/sdn.html#network-isolation-multitenant[isolation]
of the pods in a project/namespace from pods in all other projects/namespaces.

{product-title} uses IPv4 networking and it reserves subnets for its own use. The subnets
as well as other network parameters are configured in the
xref:../install_config/master_node_configuration.adoc#master-node-config-network-config[master]
and
xref:../install_config/master_node_configuration.adoc#node-config-pod-and-node-config[node]
configuration files.

Each project/namespace is assigned a subnet and each
xref:../architecture/core_concepts/pods_and_services.adoc#pods[pod] that is
created in the namespace is assigned an IP address from the subnet.
Each container that is created in the pod uses the pod's IP address and
picks its own ports to listen on.

xref:../architecture/additional_concepts/sdn.html#sdn-packet-flow[Packet flow]
decisions in the cluster are performed by the SDN plugin. {product-title} uses
OpenFlow rules in OVS.

A xref:../architecture/core_concepts/pods_and_services.adoc#services[service]
can be created in the namespace to load balance traffic among the pods in the
namespace. The service is assigned a clusterIP address from the cluster's reserved
IP subnet.  The service configuratin picks the ports that it will listen on.
The service ClusterIP is only accessible from within the cluster. To reach a
sevice from outside teh cluster the service may configre an ExternalIP
or use a loadbalancer to expose an IngressIP.

There are several ways to
xref:../architecture/core_concepts/getting_traffic_into_cluster.adoc#[get traffic into a cluster]. The
most common way is to use a
xref:../architecture/core_concepts/getting_traffic_into_cluster.adoc#using-a-router[router]. The router
proxies requests to pods based on
xref:../architecture/core_concepts/routes.adoc#[routes]
that are set up by the user.

link:http://kubernetes.io/docs/user-guide/services/#type-loadbalancer[Load balancers] are available on
xref:../install_config/configuring_aws.adoc#install-config-configuring-aws[AWS]
and
xref:../install_config/configuring_gce.adoc#install-config-configuring-gce[GCE]
clouds, and
xref:../admin_guide/tcp_ingress_external_ports.adoc#admin-guide-expose-external-ports[non-cloud]
options are also available. When access to the service from outside the cluster is desired an
xref:../architecture/core_concepts/pods_and_services.adoc#service-externalip[ExternalIP]
can be selected from the reserved external IP subnet or
xref:../architecture/core_concepts/pods_and_services.adoc#service-ingressip[automatically assigned]
from a reserved subnet.

There are two ways to control
xref:../admin_guide/managing_networking.adoc#admin-guide-controlling-egress-traffic[egress traffic]
from the: using an
xref:../admin_guide/managing_networking.adoc#admin-guide-limit-pod-access-egress[egress firewall]
or an
xref:../admin_guide/managing_networking.adoc#admin-guide-limit-pod-access-egress-router[egress router].

Networking capability in {product-title} is very configurable. Look here for a more detailed
xref:../architecture/additional_concepts/networking.adoc#[discussion of {product-title} networking]

PHIL notes ----- PHIL notes ----- PHIL notes ----- PHIL notes ----- PHIL notes
Ignore  ---- Ignore

OVS is no different. We use openshift-sdn which uses linux bridges, OVS
bridges, veth device pairs, network namespaces, netfilter/iptables,
vxlan tunnels, ip routing tables, and a whole host of other
technologies. Everyone please stop saying we use 'OVS' for our sdn.

xref:../install_config/configuring_native_container_routing.adoc#[Configuring Native Container Routing]

xref:../dev_guide/routes.adoc#[route discussion]

xref:../getting_started/beyond_the_basics.adoc#btb-configuring-routes[Configuring Routes]

xref:../dev_guide/deployments/advanced_deployment_strategies.adoc#[blue/green deployment]

xref:../dev_guide/deployments/advanced_deployment_strategies.adoc#advanced-deployment-one-service-multiple-deployment-configs[One Service, Multiple Deployment Configurations]

xref:../dev_guide/deployments/advanced_deployment_strategies.adoc#proxy-shard-traffic-splitter[Proxy Shard / Traffic Splitter]

xref:../install_config/routing_from_edge_lb.adoc#[Routing from Edge Load Balancers]

xref:../admin_guide/high_availability.adoc#[High availability]

xref:../getting_started/developers_cli.adoc#developers-cli-create-route[Create a Route]

xref:../install_config/router/customized_haproxy_router.adoc#[Deploying a Customized HAProxy Router]

xref:../install_config/router/f5_router.adoc#[Using the F5 Router Plug-in]

xref:../install_config/upgrading/manual_upgrades.adoc#upgrading-the-router[Upgrading the Router]

rrrrrrrrrrrrrrrrrrr

xref:../admin_guide/diagnostics_tool.adoc#[Diagnostics Tool]

xref:../admin_guide/sdn_troubleshooting.adoc#[Troubleshooting OpenShift SDN - external setup]

xref:../dev_guide/application_lifecycle/promoting_applications.adoc#dev-guide-promoting-applications-api-objects[API Objects]

xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#high-availability-masters[High Availability Masters]

Reference and notes

xref:../install_config/install/advanced_install.adoc#[Advanced Installation]


rrrrrrrrrrrrrrrrrrrrrr

rrrrrrrrrrrrrrrrrrrrrr

Re: [atomic-net] [openshift-sme] Egress router
I have ever tried to find how to make it works on EC2, but did not find any useful guide on Internet.

And found the unofficial doc[1] said that the macvlan cannot be used in AWS.

I am glad to hear that anyone can figure out how to make it works in EC2 or confirm that if it can be used there.


rrrrrrrrrrrrrrrrrrrrr

[openshift-sme] Openshift 3.3 with Calico

One of our customer is looking at Calico in place of the default Openshift SDN to have more granularity on Network policies and Network security.


I saw they presented recently at Openshift commons and they mentioned that they have successfully tested with Openshift 3.3 with Redhat , but was broke with changes in 3.4.

Do we have a instructions on setting up default cluster with Calico on 3.3 . The customer wants to see the demo.


https://blog.openshift.com/openshift-commons-briefing-65-simplify-secure-openshift-network-project-calico/#.WMVn0Sx3K6U.twitter

https://github.com/openshift/openshift-ansible/pull/3038

ericp
To the best of my knowledge we have no official relationship or support
with Calico no demos of how to use other people's software. They'll
have to work with Calico.

In 3.4+ we do provide support for the CNI interface, but again, it is
Calico who is responsible for the actual plugin. No idea what problem
Calico has with 3.4. Did Calico open a BZ with us if they think we are
doing something wrong?

-Eric

> Do we have any collateral on comparing Openshift Networking with
> something like Calico.

Not to my knowledge. We have NetworkPolicy support as a TECH PREVIEW in
3.5. Calico is great. The main drawback with Calico is that in some
network environments it just won't work. Usually because of certain
institutional policies surrounding the configuration and maintenance of
network equipment. If your customer does not suffer from such issues
then great.

Examples of such problems include a cluster where all of the nodes are
on different layer 2 networks. In such cases the intervening router(s)
must participate in the routing updates supplied by calico. For many
this is just not possible (thus the advent of overlays).

Calico on OpenStack is another example. It will not work unless you
disable port security. If you OpenStack admin is unwilling to do that
you are out of luck. (Thus the advent of overlays)

openshift-sdn is defined by trying to work everywhere and be "good
enough" for almost everyone. If it isn't good enough for you and there
are alternatives which work in your environment you should consider
those. Red Hat will support the CNI interface, but not the vendor's
code behind it. We won't support/debug Calico itself, you'd have to
talk to tigera, but if we are incorrectly interfacing with Calico we'd
work with tigera to get that corrected.

-Eric

rrrrrrrrrrrrrrrrrrrrrrrrrr

danw

Document new multicast and NetworkPolicy stuff #3575

This documents the new multicast and NetworkPolicy stuff. I wasn't quite sure how to refer to their "Tech Preview" status, so maybe that needs to be tweaked.

Also, there didn't seem to be any good place to put them, and I realized that we were kind of abusing managing_pods.adoc for a lot of not-especially-pod-related stuff, so I split all the networking stuff out of there into a new "managing_networking" file. It seems to work fine?





