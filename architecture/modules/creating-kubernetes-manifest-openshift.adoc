// Module included in the following assemblies:
//
// * architecture/understanding-openshift-development.adoc

[id="creating-kubernetes-manifest-openshift_{context}"]
= Creating a Kubernetes Manifest for OpenShift

While the container image is the most basic building block for a containerized application, more is needed to manage and deploy that application in a Kubernetes environment such as OpenShift. The typical next step after creating your image is to:

* Understand the different resources you work with in Kubernetes manifests
* Make some decisions on what kind of an application you are running
* Gather supporting components
* Create a manifest and store that manifest in a git repository where it can be stored in a source versioning system, audited, tracked, promoted and deployed to the next environment, rolled back to earlier versions, if necessary, and shared with others

[id="understanding-kubernetes-pods_{context}"]
== Understanding Kubernetes pods, services, and so on

While the container image is the basic unit with docker run, the basic units that Kubernetes works with are called https://www.google.com/url?q=https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/&sa=D&ust=1557950770720000[pods]. Pods represent the next step in building out an application. A pod can contain more than one container, although it can just contain one. The key is that the pod is what you deploy, scale up or down, and generally treat as a single unit.

Scalability and namespaces are probably the main things you think about when you decide what goes into a pod. For ease of deployment, you might want to deploy a container in a pod, along with its own logging and monitoring container. Later when you run the pod and need to scale up an additional instance later, those other containers are scaled up with it. For namespaces, containers in a pod share the same network interfaces, shared storage volumes, and resource limitations (such as memory and CPU), making it easier to manage the contents of the pod as a single unit. Containers in a pod can also communicate with each other using standard inter-process communications, such as System V semaphores or POSIX shared memory.

While individual pods represent a scalable unit in Kubernetes, a https://www.google.com/url?q=https://kubernetes.io/docs/concepts/services-networking/service/&sa=D&ust=1557950770721000[service] provides a means of grouping together a set of pods to create a more full-blown application that can be more stable and is able to do things like load balancing. A service also has the advantage of being less fleeting, because the service remains available from the same IP address, as long as it is not deleted. When the service is used, it is requested by name and the OpenShift/Kubernetes cluster resolves that name into the locations (IP addresses and ports) where the pods backing up that service can be reached.

By their nature, containerized applications are kept confined from the operating systems where they run and, by extension, the world that will eventually use them. So, part of your Kubernetes manifest involves exposing the application to internal and external networks by defining https://www.google.com/url?q=https://kubernetes.io/docs/concepts/services-networking/network-policies/&sa=D&ust=1557950770722000[network policies] that allow fine-grained control over communications with your containerized applications. To connect incoming requests for HTTP, HTTPS, and other services from outside your cluster to services inside your cluster, you can use an https://www.google.com/url?q=https://kubernetes.io/docs/concepts/services-networking/ingress/&sa=D&ust=1557950770723000[Ingress] resource.

If your container requires on-disk storage (as opposed to database storage, which might be provided through a service), you can add https://www.google.com/url?q=https://kubernetes.io/docs/concepts/storage/volumes/&sa=D&ust=1557950770724000[volumes] to your manifests to make that storage available to your pods. Those manifests can create physical volumes (PVs), or set up to dynamically create volumes, that are added to your pod definitions.

Once you have defined a group of pods that go together to make up your application, you can define those pods in https://www.google.com/url?q=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&sa=D&ust=1557950770724000[deployments] and https://www.google.com/url?q=https://docs.openshift.com/container-platform/4.1/applications/deployments/what-deployments-are.html&sa=D&ust=1557950770725000[deploymentconfigs].

Your next set of choices has to do with how your application is run. To make that determination, you need to consider the nature of your application.

[id="deciding-application_{context}"]
== Deciding what kind of application

So far, you have seen the kind of elements (pods, services, ingress, and so on) that need to go into your manifest. This next step is to consider the character of your application. That consideration helps drive the additional choices you need to make in creating the Kubernetes manifest for your application.

Kubernetes defines different types of workloads that are appropriate for different kinds of applications. To determine the appropriate workload for your application, you might ask yourself whether the application is:

* Meant to run to completion and be done? An example is an application that starts up to produce a report and exits when the report is done. The application might not run again then for a month. Suitable OpenShift objects for these types of applications include https://www.google.com/url?q=https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/&sa=D&ust=1557950770726000[Jobs] or https://www.google.com/url?q=https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/&sa=D&ust=1557950770726000[CronJob] objects.
* Expected to run continuously? For long-running applications, you can write a https://www.google.com/url?q=https://docs.openshift.com/container-platform/4.1/applications/deployments/what-deployments-are.html%23deployments-kube-deployments_what-deployments-are&sa=D&ust=1557950770727000[Deployment] or a https://www.google.com/url?q=https://docs.openshift.com/container-platform/4.1/applications/deployments/what-deployments-are.html%23deployments-and-deploymentconfigs_what-deployments-are&sa=D&ust=1557950770728000[DeploymentConfig].
* Required to be highly available? If your application requires high availability, then you want to size your deployment to have more than one instance. A Deployment or DeploymentConfig can incorporate a https://www.google.com/url?q=https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/&sa=D&ust=1557950770729000[ReplicaSet] for that type of application. With ReplicaSets, pods are run across multiple nodes, to make sure the application is always available, even if a worker goes down.
* Need to run on every node? There are some types of Kubernetes applications that are meant to run in the cluster itself on every master or worker node. DNS and monitoring applications are examples of applications that need to run continuously on every node. This type of applications can be run a https://www.google.com/url?q=https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&sa=D&ust=1557950770730000[DaemonSet]. A DaemonSet can also be run on a subset of nodes, based on labels.
* Require life-cycle management? When you want to hand off your application so that others can use it, consider creating an https://www.google.com/url?q=https://coreos.com/operators/&sa=D&ust=1557950770730000[Operators]. Operators let you build in intelligence, so it can handle things like backups and upgrades automatically. Coupled with the Operator Lifecycle Manager (OLM), cluster managers can expose Operators to selected namespaces so they can be run by users in the cluster.
* Have identity or numbering requirements? An application might have identity requirements or numbering requirements. For example, I might be required to run exactly three instances of the application and be required to name them 0, 1, and 2. In that case, you could use a https://www.google.com/url?q=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&sa=D&ust=1557950770731000[StatefulSets]. StatefulSets are most useful for applications that require independent storages, such as databases and zookeeper clusters.

The application you write may need supporting components, like a database or a logging component. To fulfill that need, the developer might be able to just get the needed component from Catalogs available from OpenShift.

[id="supporting-components_{context}"]
== Gather supporting components

If you are creating an application that needs a database, you almost certainly will want to grab an available database and use it to develop and later deploy with your application. In the OpenShift web UI, there are Catalogs you can choose from to get the applications you need to use as you write your application. Here are your choices:

* OperatorHub: Every OpenShift 4.1 cluster has the OperatorHub available. The OperatorHub makes Operators available from Red Hat, certified Red Hat partners, and community members to the cluster operator. The cluster operator can make those Operators available in all or selected namespaces in the cluster, so developers can launch them and configure them with their applications. More on Operators and the OperatorHub later.
* Service Catalog: Operators are the preferred method of getting packaged applications in OpenShift. However, there are some reasons why you might want to use the Service Catalog to get supporting applications for your own application. If you are an existing OpenShift 3 customer and you have already invested in Service Catalog applications or if you already have a Cloud Foundry environment from which you are interested in consuming brokers from other ecosystems.
* Templates: For a one-off type of application, where the lifecycle of a component is not important once it is installed, a template provides an easy way to get started developing a Kubernetes application with minimal overhead. A template can be a list of resource definitions, which could be deployments, services, routes, or other objects. if want to change names or resources, those items can be parameters in the template. +
There is a service broker for templates (Template Service Broker Operator). That template service broker lets you instantiate your own templates. You can also just  install templates directly from the command line.

If you find a supporting Operator, Service Catalog application, or template to use for your application development, a common practice is to configure it for the specific needs of your development team, then make it available in the namespaces in which your developers work. Many people add shared templates to the openshift namespace, as it is accessible from all other namespaces.

[id="manifest-creation-storage_{context}"]
== Create and store manifest

Kubernetes manifests let you create a more complete picture of the components that go into your Kubernetes applications. These manifests are written as yaml files and deployed by applying them to the cluster (for example, with the oc apply command).

At this point, you should be thinking more about ways to automate your container development process. Ideally, you would have some sort of CI pipeline that builds the images and pushes them to a registry. In particular, a GitOps pipeline integrates your container development with the git repositories being used to ultimately store the software needed to build your applications. The workflow to this point might look like:

* Day 1: You just write some yaml. You then run oc apply to apply that yaml to the cluster and test that it works.
* Day 2: You put your yaml container configuration file into your own git repository. From there, people who want to install that app, or help you improve it, can pull down that yaml and apply it to their cluster and they have the app running.
* Day 3: Consider writing an Operator for your application.