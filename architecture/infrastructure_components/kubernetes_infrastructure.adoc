[[architecture-infrastructure-components-kubernetes-infrastructure]]
= Kubernetes Infrastructure
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview

Within {product-title}, Kubernetes manages containerized applications across a
set of containers or hosts and provides mechanisms for deployment, maintenance,
and application-scaling. The container runtime packages, instantiates, and runs
containerized applications. A Kubernetes cluster consists of one or more masters
and a set of nodes.

ifdef::openshift-origin,openshift-dedicated,openshift-enterprise[]
You can optionally configure your masters for
xref:high-availability-masters[high availability] (HA) to ensure that the
cluster has no single point of failure.
endif::[]

[NOTE]
====
{product-title}
ifdef::openshift-enterprise,openshift-dedicated[]
{product-version}
endif::[]
uses Kubernetes 1.11 and Docker 1.13.1.
====

[[master]]
== Masters

The master is the host or hosts that contain the control plane components,
including the API server, controller manager server, and etcd. The master
manages xref:node[nodes] in its Kubernetes cluster and schedules
xref:../core_concepts/pods_and_services.adoc#pods[pods] to run on those nodes.

[[master-components]]
[cols="1,4"]
.Master Components
|===
|Component |Description

|API Server
|The Kubernetes API server validates and configures the data for pods, services,
and replication controllers. It also assigns pods to nodes and synchronizes pod
information with service configuration.

|etcd
|etcd stores the persistent master state while other components watch etcd
for changes to bring themselves into the desired state. etcd can be optionally
configured for high availability, typically deployed with 2n+1 peer services.

|Controller Manager Server
|The controller manager server watches etcd for changes to replication
controller objects and then uses the API to enforce the desired state.
Several such processes create a cluster with
one active leader at a time.

ifdef::openshift-enterprise,openshift-origin,openshift-dedicated[]
|HAProxy
a|Optional, used when configuring
xref:high-availability-masters[highly-available masters] with the `native`
method to balance load between API master endpoints.
endif::[]
ifdef::openshift-enterprise,openshift-origin[]
The xref:../../install/index.adoc#install-planning[cluster installation process]
can configure HAProxy for you with the `native` method. Alternatively, you can
use the `native` method but pre-configure your own load balancer of choice.
endif::[]
|===

[[control-plane-static-pods]]
=== Control Plane Static Pods

The core control plane components, the API
server and the controller manager components, run as
link:https://kubernetes.io/docs/tasks/administer-cluster/static-pod/[_static pods_]
operated by the kubelet.

ifdef::openshift-aro[]
On the master nodes, `etcd`, `openvswitch`, and `openshift-sdn` run as static Pods.
endif::[]
ifdef::openshift-dedicated,openshift-enterprise,openshift-online,openshift-origin[]
For masters that have etcd co-located on the same host, etcd is also moved to
static pods. RPM-based etcd is still supported on etcd hosts that are not also
masters.

In addition, the node components *openshift-sdn* and
*openvswitch* are now run using a DaemonSet instead of a *systemd* service.

.Control plane host architecture changes
image::ocp310-archupgrade.png["Control plane host architecture changes"]
endif::[]

ifdef::openshift-enterprise,openshift-origin[]
Even with control plane components running as static pods, master hosts still
source their configuration from the *_/etc/origin/master/master-config.yaml_*
file, as described in the
xref:../../install_config/master_node_configuration.adoc#install-config-master-node-configuration[Master and Node Configuration] topic.

[discrete]
[[control-plane-static-pods-mirror-pods]]
==== Mirror Pods

The kubelet on master nodes automatically creates _mirror pods_ on the API
server for each of the control plane static pods so that they are visible in the
cluster in the *kube-system* project. Manifests for these static pods are
installed by default by the *openshift-ansible* installer, located in the
*_/etc/origin/node/pods_* directory on the master host.

These pods have the following `hostPath` volumes defined:

[horizontal]
*_/etc/origin/master_*:: Contains all certificates, configuration files, and the *_admin.kubeconfig_* file.
*_/var/lib/origin_*:: Contains volumes and potential core dumps of the binary.
*_/etc/origin/cloudprovider_*:: Contains cloud provider specific configuration (AWS, Azure, etc.).
*_/usr/libexec/kubernetes/kubelet-plugins_*:: Contains additional third party volume plug-ins.
*_/etc/origin/kubelet-plugins_*:: Contains additional third party volume plug-ins for system containers.

The set of operations you can do on the static pods is limited. For example:

----
$ oc logs master-api-<hostname> -n kube-system
----

returns the standard output from the API server. However:

----
$ oc delete pod master-api-<hostname> -n kube-system
----

will not actually delete the pod.

As another example, a cluster administrator might want to perform a common
operation, such as increasing the `loglevel` of the API server to provide more
verbose data if a problem occurs. You must edit the
*_/etc/origin/master/master.env_* file, where the `--loglevel` parameter in the
`OPTIONS` variable can be modified, because this value is passed to the process running
inside the container. Changes require a restart of the process running inside
the container.

[discrete]
[[control-plane-static-pods-restarting-master-services]]
==== Restarting Master Services

To restart control plane services running in control plane static pods, use the
`master-restart` command on the master host.

To restart the master API:

----
# master-restart api
----

To restart the controllers:

----
# master-restart controllers
----

To restart etcd:

----
# master-restart etcd
----

[discrete]
[[control-plane-static-pods-viewing-master-services-logs]]
==== Viewing Master Service Logs

To view logs for control plane services running in control plane static pods,
use the `master-logs` command for the respective component:

----
# master-logs api api
# master-logs controllers controllers
# master-logs etcd etcd
----
endif::[]

ifdef::openshift-origin,openshift-enterprise,openshift-dedicated[]
[[high-availability-masters]]

=== High Availability Masters

endif::[]
ifdef::openshift-origin,openshift-enterprise[]
You can optionally configure your masters for high
availability (HA) to ensure that the cluster has no single point of failure.

To mitigate concerns about availability of the master, two activities are
recommended:

1. A https://en.wikipedia.org/wiki/Runbook[runbook] entry should be created for
reconstructing the master. A runbook entry is a necessary backstop for any
highly-available service. Additional solutions merely control the frequency
that the runbook must be consulted. For example, a cold standby of the master
host can adequately fulfill SLAs that require no more than minutes of downtime
for creation of new applications or recovery of failed application components.

2. Use a high availability solution to configure your masters and ensure that the
cluster has no single point of failure. The
xref:../../install/example_inventories.adoc#multiple-masters[cluster
installation documentation] provides specific examples using the `native` HA method and
configuring HAProxy. You can also take the concepts and apply them towards your
existing HA solutions using the `native` method instead of HAProxy.

[NOTE]
====
In production {product-title} clusters, you must maintain high availability
of the API Server load balancer. If the API Server load balancer is not
available, nodes cannot report their status, all their pods are marked dead,
and the pods' endpoints are removed from the service.

In addition to configuring HA for {product-title}, you must separately configure
HA for the API Server load balancer. To configure HA, it is much preferred to
integrate an enterprise load balancer (LB) such as an F5 Big-IP™ or a Citrix
Netscaler™ appliance. If such solutions are not available, it is possible to
run multiple HAProxy load balancers and use Keepalived to provide a floating
virtual IP address for HA. However, this solution is not recommended for
production instances.
====

endif::[]

ifdef::openshift-origin,openshift-enterprise,openshift-dedicated[]
When using the `native` HA method with HAProxy, master components have the
following availability:

[cols="1,1,3"]
.Availability Matrix with HAProxy
|===
|Role |Style |Notes

|etcd
|Active-active
|Fully redundant deployment with load balancing.
ifdef::openshift-origin,openshift-enterprise[]
Can be installed on separate hosts or collocated on master hosts.
endif::[]

|API Server
|Active-active
|Managed by HAProxy.

|Controller Manager Server
|Active-passive
|One instance is elected as a cluster leader at a time.

|HAProxy
|Active-passive
|Balances load between API master endpoints.
|===
endif::[]

ifdef::openshift-origin,openshift-enterprise[]
While clustered etcd requires an odd number of hosts for quorum, the master
services have no quorum or requirement that they have an odd number of hosts.
However, since you need at least two master services for HA, it is common to
maintain a uniform odd number of hosts when collocating master services and
etcd.
endif::[]

[[node]]
== Nodes

A node provides the runtime environments for containers. Each node in a
Kubernetes cluster has the required services to be managed by the master. Nodes
also have the required services to run pods, including the container runtime, a
kubelet, and a service proxy.

{product-title} creates nodes from a cloud provider, physical systems, or virtual
systems. Kubernetes interacts with xref:node-object-definition[node objects]
that are a representation of those nodes. The master uses the information from
node objects to validate nodes with health checks. A node is ignored until it
passes the health checks, and the master continues checking nodes until they are
valid. The link:https://kubernetes.io/docs/concepts/architecture/nodes/#management[Kubernetes documentation]
has more information on node statuses and management.

ifdef::openshift-aro[]
{product-title} creates two types of nodes:

* Infrastructure nodes, which run cluster management Pods
* Compute nodes, which run customer applications and builds
endif::[]

ifdef::openshift-enterprise,openshift-origin[]
Administrators can xref:../../admin_guide/manage_nodes.adoc#admin-guide-manage-nodes[manage nodes] in an
{product-title} instance using the CLI. To define full configuration and security
options when launching node servers, use
xref:../../install_config/master_node_configuration.adoc#install-config-master-node-configuration[dedicated node configuration files].

[IMPORTANT]
====
See the
xref:../../scaling_performance/cluster_maximums.adoc#scaling-performance-cluster-maximums[cluster
limits] section for the recommended maximum number of nodes.
====
endif::openshift-enterprise,openshift-origin[]

[[kubelet]]
=== Kubelet

Each node has a kubelet that updates the node as specified by a container
manifest, which is a YAML file that describes a pod. The kubelet uses a set of
manifests to ensure that its containers are started and that they continue to
run.

A container manifest can be provided to a kubelet by:

- A file path on the command line that is checked every 20 seconds.
- An HTTP endpoint passed on the command line that is checked every 20 seconds.
- The kubelet watching an etcd server, such as *_/registry/hosts/$(hostname -f)_*, and acting on any changes.
- The kubelet listening for HTTP and responding to a simple API to submit a new
 manifest.

[[service-proxy]]
=== Service Proxy

Each node also runs a simple network proxy that reflects the services defined in
the API on that node. This allows the node to do simple TCP and UDP stream
forwarding across a set of back ends.

[[node-object-definition]]
=== Node Object Definition

The following is an example node object definition in Kubernetes:

[source,yaml]
----
apiVersion: v1 <1>
kind: Node <2>
metadata:
  creationTimestamp: null
  labels: <3>
    kubernetes.io/hostname: node1.example.com
  name: node1.example.com <4>
spec:
  externalID: node1.example.com <5>
status:
  nodeInfo:
    bootID: ""
    containerRuntimeVersion: ""
    kernelVersion: ""
    kubeProxyVersion: ""
    kubeletVersion: ""
    machineID: ""
    osImage: ""
    systemUUID: ""
----
<1> `apiVersion` defines the API version to use.
<2> `kind` set to `Node` identifies this as a definition for a node
object.
<3> `metadata.labels` lists any
xref:../core_concepts/pods_and_services.adoc#labels[labels] that have been added
to the node.
<4> `metadata.name` is a required value that defines the name of the node
object. This value is shown in the `NAME` column when running the `oc get nodes`
command.
<5> `spec.externalID` defines the fully-qualified domain name where the node
can be reached. Defaults to the `metadata.name` value when empty.

[[node-bootstrapping]]
=== Node Bootstrapping

A node's configuration is bootstrapped from
the master, which means nodes pull their pre-defined configuration and client
and server certificates from the master. This allows faster node start-up by
reducing the differences between nodes, as well as centralizing more
configuration and letting the cluster converge on the desired state. Certificate
rotation and centralized certificate management are enabled by default.

.Node bootstrapping workflow overview
image::node_bootstrapping.png["Node bootstrapping workflow overview"]

When node services are started, the node checks if the
*_/etc/origin/node/node.kubeconfig_* file and other node configuration files
exist before joining the cluster. If they do not, the node pulls the
configuration from the master, then joins the cluster.

xref:../../dev_guide/configmaps.adoc#dev-guide-configmaps[ConfigMaps] are used
to store the node configuration in the cluster, which populates the
configuration file on the node host at *_/etc/origin/node/node-config.yaml_*.
ifdef::openshift-enterprise,openshift-origin[]
For definitions of the set of default node groups and their ConfigMaps, see
xref:../../install/configuring_inventory_file.adoc#configuring-inventory-defining-node-group-and-host-mappings[Defining Node Groups and Host Mappings]
in Installing Clusters.
endif::[]

ifdef::openshift-enterprise,openshift-origin[]
[discrete]
[[node-bootstrapping-workflow]]
==== Node Bootstrap Workflow

The process for automatic node bootstrapping uses the following workflow:

. By default during cluster installation, a set of `clusterrole`,
`clusterrolebinding` and `serviceaccount` objects are created for use in node
bootstrapping:
+
--
- The *system:node-bootstrapper* cluster role is used for creating certificate signing requests (CSRs) during node bootstrapping:
+
----
# oc describe clusterrole.authorization.openshift.io/system:node-bootstrapper

Name:			system:node-bootstrapper
Created:		17 hours ago
Labels:			kubernetes.io/bootstrapping=rbac-defaults
Annotations:		authorization.openshift.io/system-only=true
			openshift.io/reconcile-protect=false
Verbs			Non-Resource URLs	Resource Names	API Groups		Resources
[create get list watch]	[]			[]		[certificates.k8s.io]	[certificatesigningrequests]
----

- The following *node-bootstrapper* service account is created in the
*openshift-infra* project:
+
----
# oc describe sa node-bootstrapper -n openshift-infra

Name:                node-bootstrapper
Namespace:           openshift-infra
Labels:              <none>
Annotations:         <none>
Image pull secrets:  node-bootstrapper-dockercfg-f2n8r
Mountable secrets:   node-bootstrapper-token-79htp
                     node-bootstrapper-dockercfg-f2n8r
Tokens:              node-bootstrapper-token-79htp
                     node-bootstrapper-token-mqn2q
Events:              <none>
----

- The following *system:node-bootstrapper* cluster role binding is for the node
bootstrapper cluster role and service account:
+
----
# oc describe clusterrolebindings system:node-bootstrapper

Name:			system:node-bootstrapper
Created:		17 hours ago
Labels:			<none>
Annotations:		openshift.io/reconcile-protect=false
Role:			/system:node-bootstrapper
Users:			<none>
Groups:			<none>
ServiceAccounts:	openshift-infra/node-bootstrapper
Subjects:		<none>
Verbs			Non-Resource URLs	Resource Names	API Groups		Resources
[create get list watch]	[]			[]		[certificates.k8s.io]	[certificatesigningrequests]
----
--

. Also by default during cluster installation, the *openshift-ansible* installer creates a
{product-title} certificate authority and various other certificates, keys, and
*_kubeconfig_* files in the *_/etc/origin/master_* directory. Two files of note
are:
+
--
[horizontal]
*_/etc/origin/master/admin.kubeconfig_*:: Uses the *system:admin* user.
*_/etc/origin/master/bootstrap.kubeconfig_*:: Used for node bootstrapping nodes other than masters.
--

.. The *_/etc/origin/master/bootstrap.kubeconfig_* is created when the installer
uses the *node-bootstrapper* service account as follows:
+
----
$ oc --config=/etc/origin/master/admin.kubeconfig \
    serviceaccounts create-kubeconfig node-bootstrapper \
    -n openshift-infra
----

.. On master nodes, the *_/etc/origin/master/admin.kubeconfig_* is used as a
bootstrapping file and is copied to *_/etc/origin/node/boostrap.kubeconfig_*. On
other, non-master nodes, the *_/etc/origin/master/bootstrap.kubeconfig_* file is
copied to all other nodes in at *_/etc/origin/node/boostrap.kubeconfig_* on each
node host.

.. The *_/etc/origin/master/bootstrap.kubeconfig_* is then passed to kubelet using
the flag `--bootstrap-kubeconfig` as follows:
+
----
--bootstrap-kubeconfig=/etc/origin/node/bootstrap.kubeconfig
----

. The kubelet is first started with the supplied
*_/etc/origin/node/bootstrap.kubeconfig_* file. After initial connection
internally, the kubelet creates certificate signing requests (CSRs) and sends
them to the master.

. The CSRs are verified and approved via the controller manager (specifically the
certificate signing controller). If approved, the kubelet client and server
certificates are created in the *_/etc/origin/node/ceritificates_* directory.
For example:
+
----
# ls -al /etc/origin/node/certificates/
total 12
drwxr-xr-x. 2 root root  212 Jun 18 21:56 .
drwx------. 4 root root  213 Jun 19 15:18 ..
-rw-------. 1 root root 2826 Jun 18 21:53 kubelet-client-2018-06-18-21-53-15.pem
-rw-------. 1 root root 1167 Jun 18 21:53 kubelet-client-2018-06-18-21-53-45.pem
lrwxrwxrwx. 1 root root   68 Jun 18 21:53 kubelet-client-current.pem -> /etc/origin/node/certificates/kubelet-client-2018-06-18-21-53-45.pem
-rw-------. 1 root root 1447 Jun 18 21:56 kubelet-server-2018-06-18-21-56-52.pem
lrwxrwxrwx. 1 root root   68 Jun 18 21:56 kubelet-server-current.pem -> /etc/origin/node/certificates/kubelet-server-2018-06-18-21-56-52.pem
----

. After the CSR approval, the *_node.kubeconfig_* file is created at
*_/etc/origin/node/node.kubeconfig_*.

. The kubelet is restarted with the *_/etc/origin/node/node.kubeconfig_* file and
the certificates in the *_/etc/origin/node/certificates/_* directory, after
which point it is ready to join the cluster.

[discrete]
[[node-bootstrapping-configuration-workflow]]
==== Node Configuration Workflow

Sourcing a node's configuration uses the following workflow:

. Initially the node's kubelet is started with the bootstrap configuration file,
*_bootstrap-node-config.yaml_* in the *_/etc/origin/node/_* directory, created
at the time of node provisioning.

. On each node, the node service file uses the local script
*_openshift-node_* in the *_/usr/local/bin/_* directory to start the kubelet
with the supplied *_bootstrap-node-config.yaml_*.

. On each master, the directory *_/etc/origin/node/pods_* contains pod manifests
for *apiserver*, *controller* and *etcd* which are created as static pods on
masters.

. During cluster installation, a sync DaemonSet is created which creates a sync
pod on each node. The sync pod monitors changes in the file
*_/etc/sysconfig/atomic-openshift-node_*. It specifically watches for
`BOOTSTRAP_CONFIG_NAME` to be set. `BOOTSTRAP_CONFIG_NAME` is set by the
*openshift-ansible* installer and is the name of the ConfigMap based on the node
configuration group the node belongs to.
+
By default, the installer creates the following node configuration groups:
+
--
- *node-config-master*
- *node-config-infra*
- *node-config-compute*
- *node-config-all-in-one*
- *node-config-master-infra*
--
+
A ConfigMap for each group is created in the *openshift-node* project.

. The sync pod extracts the appropriate ConfigMap based on the value set in
`BOOTSTRAP_CONFIG_NAME`.

. The sync pod converts the ConfigMap data into kubelet configurations and creates
a *_/etc/origin/node/node-config.yaml_* for that node host. If a change is made
to this file (or it is the file's initial creation), the kubelet is restarted.

[discrete]
[[node-bootstrapping-modifying-configurations]]
==== Modifying Node Configurations

A node's configuration is modified by editing the appropriate ConfigMap in the
*openshift-node* project. The *_/etc/origin/node/node-config.yaml_* must not be
modified directly.

For example, for a node that is in the *node-config-compute* group, edit the
ConfigMap using:

----
$ oc edit cm node-config-compute -n openshift-node
----

endif::[]
