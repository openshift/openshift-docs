xref:../../install_config/configuring_kuryrsdn.adoc#install-config-configuring-kuryr-sdn[Kuryr]
(or more specifically Kuryr-Kubernetes) is an SDN solution built using
link:https://github.com/containernetworking/cni[CNI] and
link:https://docs.openstack.org/neutron/latest/[OpenStack Neutron]. Its
advantages include being able to use a wide range of Neutron SDN backends and
providing interconnectivity between Kubernetes pods and OpenStack virtual
machines (VMs).

Kuryr-Kubernetes and {product-title} integration is primarily designed for
{product-title} clusters running on OpenStack VMs.

[[architecture-additional-concepts-kuryr-openstack]]
=== OpenStack Deployment Requirements

Kuryr SDN has some requirements regarding configuration of OpenStack it will be
using. In particular:

* Minimal service set is Keystone and Neutron.
* A LBaaS service is required. It can be either
  link:https://docs.openstack.org/octavia/latest/[Octavia] (with all of its own
  dependencies) or deprecated Neutron-LBaaS v2.

[[architecture-additional-concepts-kuryr-controller]]
=== kuryr-controller

kuryr-controller is a service responsible for watching {product-title} API for
new pods being spawned and creating Neutron resources for them. For example,
when a pod gets created, kuryr-controller will notice that and call OpenStack
Neutron to create a new port. Then, information about that port (or VIF) is
saved into the pod's annotations. kuryr-controller is also able to use
precreated port pools for faster pod creation.

Currently, kuryr-controller must be run as a single service instance, so it is
modeled in {product-title} as `Deployment` with `replicas=1`. It requires
access to the underlying OpenStack service APIs.

[[architecture-additional-concepts-kuryr-cni]]
=== kuryr-cni

kuryr-cni container serves two roles in Kuryr-Kubernetes deployment. It is
responsible for installing and configuring Kuryr CNI script on {product-title}
nodes and running kuryr-daemon service that is networking the `Pods` on the
host. This means that kuryr-cni container needs to run on every {product-title}
node, so it is modeled as `DaemonSet`.

{product-title} CNI will call Kuryr CNI script every time a new pod is spawned
on or deleted from an {product-title} host. The script has the container ID of
the local kuryr-cni hardcoded and executes Kuryr CNI plug-in binary through
`docker exec` passing all the CNI call arguments. The plug-in then calls
kuryr-daemon over local HTTP socket, again passing all the parameters.

Please note that container ID that is hardcoded into the Kuryr CNI script is
being fetched through {product-title} API. This means kuryr-cni container needs
access to the API through standard `Service` called `kubernetes` that should
be present on all {product-title} deployments.

kuryr-daemon service is responsible for watching for `Pod's` annotations about
Neutron VIFs created for them. When CNI request for given `Pod` is received
daemon either has VIF information in memory already or waits for the annotation
to appear on `Pod` definition. Once VIF info in known all the networking
operations happen.
