A router is one way to get traffic into the cluster.
The F5 BIG-IP® Router plug-in is one of the available
xref:../../architecture/networking/network_plugins.adoc#architecture-additional-concepts-network-plugins[router plugins].

ifdef::openshift-enterprise[]
[NOTE]
====
The F5 router plug-in is available starting in OpenShift Enterprise 3.0.2.
====
endif::[]

The F5 router plug-in integrates with an existing *F5 BIG-IP®* system in your
environment. *F5 BIG-IP®* version 11.4 or newer is required in order to have the
F5 iControl REST API. The F5 router supports xref:../../architecture/networking/routes.adoc#route-types[unsecured],
xref:../../architecture/networking/routes.adoc#edge-termination[edge terminated],
xref:../../architecture/networking/routes.adoc#re-encryption-termination[re-encryption terminated], and
xref:../../architecture/networking/routes.adoc#passthrough-termination[passthrough terminated] routes matching on HTTP
vhost and request path.

The F5 router has feature parity with the
xref:../../architecture/networking/assembly_available_router_plugins.adoc#architecture-haproxy-router[HAProxy
template router], and has additional features over the *F5 BIG-IP®* support in
ifdef::openshift-enterprise[]
OpenShift Enterprise 2.
endif::[]
ifdef::openshift-origin[]
OpenShift v2.
endif::[]
Compared with the *routing-daemon* used in earlier
versions, the F5 router additionally supports:

- path-based routing (using policy rules),
- re-encryption (implemented using client and server SSL profiles)
- passthrough of encrypted connections (implemented using an iRule that parses
the SNI protocol and uses a data group that is maintained by the F5 router for
the servername lookup).

[NOTE]
====
Passthrough routes are a special case: path-based routing is technically
impossible with passthrough routes because *F5 BIG-IP®* itself does not see the
HTTP request, so it cannot examine the path. The same restriction applies to the
template router; it is a technical limitation of passthrough encryption, not a
technical limitation of {product-title}.
====

[[routing-traffic-to-pods-through-the-sdn]]
=== Routing Traffic to Pods Through the SDN

Because *F5 BIG-IP®* is external to the
xref:../../architecture/networking/sdn.adoc#architecture-additional-concepts-sdn[OpenShift SDN], a
cluster administrator must create a peer-to-peer tunnel between *F5 BIG-IP®* and
a host that is on the SDN, typically an {product-title} node host.
ifdef::openshift-dedicated[]
This _ramp node_ can be configured as unschedulable for pods so that it will not
be doing anything except act as a gateway for the *F5 BIG-IP®* host.
endif::[]
ifdef::openshift-enterprise,openshift-origin[]
This
xref:../../install_config/routing_from_edge_lb.adoc#establishing-a-tunnel-using-a-ramp-node[_ramp
node_] can be configured as
xref:../../admin_guide/manage_nodes.adoc#marking-nodes-as-unschedulable-or-schedulable[unschedulable]
for pods so that it will not be doing anything except act as a gateway for the
*F5 BIG-IP®* host.
endif::[]
It is also possible to configure multiple such hosts and use
the {product-title} *ipfailover* feature for redundancy; the *F5 BIG-IP®* host would
then need to be configured to use the *ipfailover* VIP for its tunnel's remote
endpoint.

[[f5-integration-details]]
=== F5 Integration Details

The operation of the F5 router is similar to that of the {product-title}
*routing-daemon* used in earlier versions. Both use REST API calls to:

- create and delete pools,
- add endpoints to and delete them from those pools, and
- configure policy rules to route to pools based on vhost.

Both also use `scp` and `ssh` commands to upload custom TLS/SSL certificates to
*F5 BIG-IP®*.

The F5 router configures pools and policy rules on virtual servers as follows:

- When a user creates or deletes a route on {product-title}, the router creates a
pool to *F5 BIG-IP®* for the route (if no pool already exists) and adds a rule to, or
deletes a rule from, the policy of the appropriate vserver: the HTTP vserver for
non-TLS routes, or the HTTPS vserver for edge or re-encrypt routes. In the case
of edge and re-encrypt routes, the router also uploads and configures the TLS
certificate and key. The router supports host- and path-based routes.
+
[NOTE]
====
Passthrough routes are a special case: to support those, it is necessary to
write an iRule that parses the SNI ClientHello handshake record and looks up the
servername in an F5 data-group. The router creates this iRule, associates the
iRule with the vserver, and updates the F5 data-group as passthrough routes are
created and deleted. Other than this implementation detail, passthrough routes
work the same way as other routes.
====

- When a user creates a service on {product-title}, the router adds a pool to *F5
BIG-IP®* (if no pool already exists). As endpoints on that service are created
and deleted, the router adds and removes corresponding pool members.

- When a user deletes the route and all endpoints associated with a particular
pool, the router deletes that pool.

[[architecture-f5-native-integration]]
=== F5 Native Integration

ifdef::openshift-enterprise,openshift-origin[]
With
xref:../../install_config/router/f5_router.adoc#setting-up-f5-native-integration-with-openshift[native
integration of F5 with {product-title}], you do not need to configure a ramp
node for F5 to be able to reach the pods on the overlay network as created by
OpenShift SDN.

Also, only *F5 BIG-IP®* appliance version 12.x and above works with the native integration
presented in this section. You also need `sdn-services` add-on license for the
integration to work properly.
For version 11.x, xref:../../install_config/routing_from_edge_lb.adoc#establishing-a-tunnel-using-a-ramp-node[set up a ramp
node].
endif::[]
ifdef::openshift-dedicated[]
With native integration of F5 with {product-title},
you do not need to configure a ramp node for F5 to be able to reach the pods on
the overlay network as created by OpenShift SDN.
endif::[]

[discrete]
[[architecture-f5-connection]]
==== Connection

The F5 appliance can connect to the {product-title} cluster via an L3
connection. An L2 switch connectivity is not required between {product-title}
nodes. On the appliance, you can use multiple interfaces to manage the
integration:

* Management interface - Reaches the web console of the F5 appliance.
* External interface - Configures the virtual servers for inbound web traffic.
* Internal interface - Programs the appliance and reaches out to the pods.

image::F5-OpenShift-Connection-Diagram.png[F5 and OpenShift Connection Diagram]

An F5 controller pod has `admin` access to the appliance. The F5 image is
launched within the {product-title} cluster (scheduled on any node) that uses
iControl REST APIs to program the virtual servers with policies, and configure
the VxLAN device.

[discrete]
[[architecture-f5-data-flow-packets-to-pods]]
===== Data Flow: Packets to Pods

[NOTE]
====
This section explains how the packets reach the pods, and vice versa. These
actions are performed by the F5 controller pod and the F5 appliance, not the
user.
====

When natively integrated, The F5 appliance reaches out to the pods directly
using VxLAN encapsulation. This integration works only when {product-title} is
using *openshift-sdn* as the network plug-in. The *openshift-sdn*  plug-in
employs VxLAN encapsulation for the overlay network that it creates.

To make a successful data path between a pod and the F5 appliance:

. F5 needs to encapsulate the VxLAN packet meant for the pods. This requires the
*sdn-services* license add-on. A VxLAN device needs to be created and the pod
overlay network needs to be routed through this device.

. F5 needs to know the VTEP IP address of the pod, which is the IP address of the
node where the pod is located.

. F5 needs to know which `source-ip` to use for the overlay network when
encapsulating the packets meant for the pods. This is known as the _gateway address_.

. {product-title} nodes need to know where the F5 gateway address is (the VTEP
address for the return traffic). This needs to be the internal interface’s
address. All nodes of the cluster must learn this automatically.

. Since the overlay network is multi-tenant aware, F5 must use a VxLAN ID that is
representative of an `admin` domain, ensuring that all tenants are reachable by
the F5. Ensure that F5 encapsulates all packets with a `vnid` of `0` (the
default `vnid` for the `admin` namespace in {product-title}) by putting an
annotation on the manually created `hostsubnet` -
`pod.network.openshift.io/fixed-vnid-host: 0`.

A ghost `hostsubnet` is manually created as part of the setup, which fulfills
the third and forth listed requirements. When the F5 controller pod is launched,
this new ghost `hostsubnet` is provided so that the F5 appliance can be
programmed suitably.

[NOTE]
====
The term _ghost_ `hostsubnet` is used because it suggests that a subnet has been
given to a node of the cluster. However, in reality, it is not a real node of
the cluster. It is hijacked by an external appliance.
====

The first requirement is fulfilled by the F5 controller pod once it is launched.
The second requirement is also fulfilled by the F5 controller pod, but it is an
ongoing process. For each new node that is added to the cluster, the controller
pod creates an entry in the VxLAN device’s VTEP FDB. The controller pod needs
access to the `nodes` resource in the cluster, which you can accomplish by
giving the service account appropriate privileges. Use the following command:

----
$ oc adm policy add-cluster-role-to-user system:sdn-reader system:serviceaccount:default:router
----

[discrete]
[[architecture-f5-data-flow-from-the-f5-host]]
=== Data Flow from the F5 Host

[NOTE]
====
These actions are performed by the F5 controller pod and the F5 appliance, not
the user.
====

. The destination pod is identified by the F5 virtual server for a packet.

. VxLAN dynamic FDB is looked up with pod’s IP address. If a MAC address is found, go to step 5.

. Flood all entries in the VTEP FDB with ARP requests seeking the pod’s MAC address.
ocated. An entry is made into the VxLAN dynamic FDB with the pod’s MAC
address and the VTEP to be used as the value.

. Encap an IP packet with VxLAN headers, where the MAC of the pod and the VTEP of
the node is given as values from the VxLAN dynamic FDB.

. Calculate the VTEP's MAC address by sending out an ARP or checking the host’s
neighbor cache.

. Deliver the packet through the F5 host’s internal address.

[discrete]
[[architecture-f5-data-flow-return-traffic-to-the-f5-host]]
=== Data Flow: Return Traffic to the F5 Host

[NOTE]
====
These actions are performed by the F5 controller pod and the F5 appliance, not
the user.
====

. The pod sends back a packet with the destination as the F5 host’s VxLAN gateway address.

. The `openvswitch` at the node determines that the VTEP for this packet is the
 F5 host’s internal interface address. This is learned from the ghost `hostsubnet`
 creation.

. A VxLAN packet is sent out to the internal interface of the F5 host.

[NOTE]
====
During the entire data flow, the VNID is pre-fixed to be `0` to bypass multi-tenancy.
====