[[install-config-configuring-sdn]]
= Configuring the SDN
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview

The xref:../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[OpenShift SDN] enables
communication between pods across the {product-title} cluster, establishing a _pod
network_. Two xref:../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[SDN plug-ins]
are currently available (*ovs-subnet* and *ovs-multitenant*), which provide
different methods for configuring the pod network. A third (*ovs-networkpolicy*) is currently in Tech Preview.

[[configuring-sdn-config-pod-network-ansible]]
== Configuring the Pod Network with Ansible

For initial xref:../install_config/install/advanced_install.adoc#install-config-install-advanced-install[advanced installations],
the *ovs-subnet* plug-in is installed and configured by default, though it can
be
xref:../install_config/install/advanced_install.adoc#configuring-ansible[overridden during installation]
using the
xref:../install_config/install/advanced_install.adoc#configuring-cluster-variables[`*os_sdn_network_plugin_name*` parameter],
which is configurable in the Ansible inventory file.

.Example SDN Configuration with Ansible
====

----
# Configure the multi-tenant SDN plugin (default is 'redhat/openshift-ovs-subnet')
# os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'

# Configure the NetworkPolicy SDN plugin (Tech Preview)
# os_sdn_network_plugin_name='redhat/openshift-ovs-networkpolicy'

# Disable the OpenShift SDN plugin
# openshift_use_openshift_sdn=False

# Configure SDN cluster network CIDR block. This network block should
# be a private block and should not conflict with existing network
# blocks in your infrastructure that pods may require access to.
# Can not be changed after deployment.
#osm_cluster_network_cidr=10.1.0.0/16

# default subdomain to use for exposed routes
#openshift_master_default_subdomain=apps.test.example.com

# Configure SDN cluster network and kubernetes service CIDR blocks. These
# network blocks should be private and should not conflict with network blocks
# in your infrastructure that pods may require access to. Can not be changed
# after deployment.
#osm_cluster_network_cidr=10.1.0.0/16
#openshift_portal_net=172.30.0.0/16

# Configure number of bits to allocate to each hostâ€™s subnet e.g. 8
# would mean a /24 network on the host.
#osm_host_subnet_length=8

# This variable specifies the service proxy implementation to use:
# either iptables for the pure-iptables version (the default),
# or userspace for the userspace proxy.
#openshift_node_proxy_mode=iptables
----
====

ifdef::openshift-enterprise[]
For initial xref:../install_config/install/quick_install.adoc#install-config-install-quick-install[quick installations],
the *ovs-subnet* plug-in is installed and configured by default as well, and can
be
xref:../install_config/master_node_configuration.adoc#master-configuration-files[reconfigured post-installation]
using the `*networkConfig*` stanza of the *_master-config.yaml_* file.
endif::[]

[[configuring-the-pod-network-on-masters]]
== Configuring the Pod Network on Masters

Cluster administrators can control pod network settings on masters by modifying
parameters in the `*networkConfig*` section of the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[master configuration file]
(located at *_/etc/origin/master/master-config.yaml_* by default):

====
[source,yaml]
----
networkConfig:
  clusterNetworkCIDR: 10.128.0.0/14 <1>
  hostSubnetLength: 9 <2>
  networkPluginName: "redhat/openshift-ovs-subnet" <3>
  serviceNetworkCIDR: 172.30.0.0/16 <4>
----
<1> Cluster network for node IP allocation
<2> Number of bits for pod IP allocation within a node
<3> Set to *redhat/openshift-ovs-subnet* for the *ovs-subnet* plug-in,
*redhat/openshift-ovs-multitenant* for the *ovs-multitenant* plug-in, or
*redhat/openshift-ovs-networkpolicy* for the *ovs-networkpolicy* plug-in
<4> Service IP allocation for the cluster
====

[IMPORTANT]
====
The `*serviceNetworkCIDR*` and `*hostSubnetLength*` values cannot be changed
after the cluster is first created, and `*clusterNetworkCIDR*` can only be
changed to be a larger network that still contains the original network. For
example, given the default value of *10.128.0.0/14*, you could change
`*clusterNetworkCIDR*` to *10.128.0.0/9* (i.e., the entire upper half of net
10) but not to *10.64.0.0/16*, because that does not overlap the original value.
====

[[configuring-the-pod-network-on-nodes]]
== Configuring the Pod Network on Nodes

Cluster administrators can control pod network settings on nodes by modifying
parameters in the `*networkConfig*` section of the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[node configuration file]
(located at *_/etc/origin/node/node-config.yaml_* by default):

====
[source,yaml]
----
networkConfig:
  mtu: 1450 <1>
  networkPluginName: "redhat/openshift-ovs-subnet" <2>
----
<1> Maximum transmission unit (MTU) for the pod overlay network
<2> Set to *redhat/openshift-ovs-subnet* for the *ovs-subnet* plug-in,
*redhat/openshift-ovs-multitenant* for the *ovs-multitenant* plug-in, or
*redhat/openshift-ovs-networkpolicy* for the *ovs-networkpolicy* plug-in
====

[[migrating-between-sdn-plugins]]
== Migrating Between SDN Plug-ins

If you are already using one SDN plug-in and want to switch to another:

. Change the `*networkPluginName*` parameter on all
xref:configuring-the-pod-network-on-masters[masters] and
xref:configuring-the-pod-network-on-nodes[nodes] in their configuration files.
ifdef::openshift-origin[]
. Restart the *origin-master* service on masters and the *origin-node* service
on nodes.
endif::[]
ifdef::openshift-enterprise[]
. Restart the *atomic-openshift-master* service on masters and the
*atomic-openshift-node* service on nodes.
endif::[]
. If you are switching from an OpenShift SDN plug-in to a
third-party plug-in, then clean up OpenShift SDN-specific
artifacts:
----
$ oc delete clusternetwork --all
$ oc delete hostsubnets --all
$ oc delete netnamespaces --all
----

When switching from the *ovs-subnet* to the *ovs-multitenant* OpenShift SDN plug-in,
all the existing projects in the cluster will be fully isolated (assigned unique VNIDs).
Cluster administrators can choose to xref:../admin_guide/managing_networking.adoc#admin-guide-pod-network[modify
the project networks] using the administrator CLI.

Check VNIDs by running:

----
$ oc get netnamespace
----

[[renumbering-sdn-networks]]
== Renumbering the SDN Network

It is difficult to change the networking configuration of an
{product-title} cluster after it has been deployed, but if absolutely
necessary, it can be done in some cases.

=== Expanding the Cluster IP Range By Adding Nodes

The easiest reconfiguration involves changing the
`*clusterNetworkCIDR*` to make room to add additional nodes, while
leaving existing Nodes and Pods untouched. (That is, changing
`*clusterNetworkCIDR*` to a new value which is larger than the old
value, and which includes the old value as a subset.)

For example, with the default `*clusterNetworkCIDR*` value of
*10.128.0.0/14* and the default `*hostSubnetLength*` of *9*, you can
have 512 nodes, with Pod subnets from *10.128.0.0/23* to
*10.131.254.0/23*. (Older versions of {product-title} had smaller
defaults.) If you changed `*clusterNetworkCIDR*` to *10.128.0.0/13*,
then you could have an additional 512 nodes, with Pod IPs going up to
*10.135.255.255*.

To do this, simply change the `*clusterNetworkCIDR*` value in the
`*networkConfig*` stanza of the *_master-config.yaml_* file, and then
restart the master; for single master clusters:

----
# systemctl restart atomic-openshift-master
----

For multi-master clusters, on each master:

----
# systemctl restart atomic-openshift-master-controllers
----

You will also need to restart the node service on the master and on
each of the nodes, to make them update their IP routing information to
take the larger cluster network into account:

----
# systemctl restart atomic-openshift-node
----

As stated above, this only works when the new `*clusterNetworkCIDR*`
value fully contains the original value; if you try to change it to
any other value, then when you restart the master, it will complain
about the mismatch and refuse to start.

=== Other Changes to the Cluster or Service Network IP Range

If you want to completely change `*clusterNetworkCIDR*` (eg, moving
from *10.128.0.0/14* to *192.168.9.0/8*) or you want to change
`*hostSubnetLength*` (to allow more or fewer pods per node) or
`*serviceNetworkCIDR*`, simply changing the configuration and
restarting will not work.

ifndef::openshift-enterprise[]
Although it is theoretically possible to make such changes in a
deployed cluster, it is not supported and not documented here. While
{product-name} may support such changes in the future, for now the
best solution is to reinstall the cluster with the new correct values.
endif::[]
ifdef::openshift-enterprise[]
Although it is possible to make such changes in a deployed cluster, it
is complicated and not fully supported; if it all possible, you should
consider reinstalling the cluster instead. FIXME talk to your support
person or look up the answer in the support db or something, I'm not
sure what to write here...
endif::[]

[[external-access-to-the-cluster-network]]
== External Access to the Cluster Network

If a host that is external to {product-title} requires access to the cluster network,
you have two options:

. Configure the host as an {product-title} node but mark it
xref:../admin_guide/manage_nodes.adoc#marking-nodes-as-unschedulable-or-schedulable[unschedulable]
so that the master does not schedule containers on it.
. Create a tunnel between your host and a host that is on the cluster network.

Both options are presented as part of a practical use-case in the documentation
for configuring xref:../install_config/routing_from_edge_lb.adoc#install-config-routing-from-edge-lb[routing from an
edge load-balancer to containers within OpenShift SDN].

[[using-flannel]]
== Using Flannel
As an alternative to the default SDN, {product-title} also provides Ansible
playbooks for installing *flannel*-based networking. This is useful if running
{product-title} within a cloud provider platform that also relies on SDN,
such as OpenStack, and you want to avoid encapsulating packets twice through
both platforms.

ifndef::openshift-origin[]
[IMPORTANT]
====
This is only supported for {product-title} on Red Hat OpenStack Platform.
====
endif::[]

To enable *flannel* within your {product-title} cluster, set the following
variables in your Ansible inventory file before running the installation.

----
openshift_use_openshift_sdn=false
openshift_use_flannel=true
flannel_interface=eth0
----

Setting the `*openshift_use_openshift_sdn*` variable to false disables the
default SDN and setting the `*openshift_use_flannel*` variable to true enables
*flannel* in place.

You can optionally specify the interface to use for inter-host communication
using the `*flannel_interface*` variable. Without this variable, the
{product-title} installation uses the default interface.
