[[install-config-configuring-sdn]]
= Configuring the SDN
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview

The xref:../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[{product-title} SDN] enables
communication between pods across the {product-title} cluster, establishing a _pod
network_. Two xref:../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[SDN plug-ins]
are currently available (*ovs-subnet* and *ovs-multitenant*), which provide
different methods for configuring the pod network.

[[configuring-sdn-config-pod-network-ansible]]
== Configuring the Pod Network with Ansible

For initial xref:../install_config/install/advanced_install.adoc#install-config-install-advanced-install[advanced installations],
the *ovs-subnet* plug-in is installed and configured by default, though it can
be
xref:../install_config/install/advanced_install.adoc#configuring-ansible[overridden during installation]
using the
xref:../install_config/install/advanced_install.adoc#configuring-cluster-variables[`*os_sdn_network_plugin_name*` parameter],
which is configurable in the Ansible inventory file.

.Example SDN Configuration with Ansible
====

----
# Configure the multi-tenant SDN plugin (default is 'redhat/openshift-ovs-subnet')
# os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'

# Disable the OpenShift SDN plugin
# openshift_use_openshift_sdn=False

# Configure SDN cluster network CIDR block. This network block should
# be a private block and should not conflict with existing network
# blocks in your infrastructure that pods may require access to.
# Can not be changed after deployment.
#osm_cluster_network_cidr=10.1.0.0/16

# default subdomain to use for exposed routes
#openshift_master_default_subdomain=apps.test.example.com

# Configure SDN cluster network and kubernetes service CIDR blocks. These
# network blocks should be private and should not conflict with network blocks
# in your infrastructure that pods may require access to. Can not be changed
# after deployment.
#osm_cluster_network_cidr=10.1.0.0/16
#openshift_portal_net=172.30.0.0/16

# Configure number of bits to allocate to each hostâ€™s subnet e.g. 8
# would mean a /24 network on the host.
#osm_host_subnet_length=8

# This variable specifies the service proxy implementation to use:
# either iptables for the pure-iptables version (the default),
# or userspace for the userspace proxy.
#openshift_node_proxy_mode=iptables
----
====

ifdef::openshift-enterprise[]
For initial xref:../install_config/install/quick_install.adoc#install-config-install-quick-install[quick installations],
the *ovs-subnet* plug-in is installed and configured by default as well, and can
be
xref:../install_config/master_node_configuration.adoc#master-configuration-files[reconfigured post-installation]
using the `*networkConfig*` stanza of the *_master-config.yaml_* file.
endif::[]

[[configuring-the-pod-network-on-masters]]
== Configuring the Pod Network on Masters

Cluster administrators can control pod network settings on masters by modifying
parameters in the `*networkConfig*` section of the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[master configuration file]
(located at *_/etc/origin/master/master-config.yaml_* by default):

====
[source,yaml]
----
networkConfig:
  clusterNetworkCIDR: 10.128.0.0/14 <1>
  hostSubnetLength: 9 <2>
  networkPluginName: "redhat/openshift-ovs-subnet" <3>
  serviceNetworkCIDR: 172.30.0.0/16 <4>
----
<1> Cluster network for node IP allocation
<2> Number of bits for pod IP allocation within a node
<3> Set to *redhat/openshift-ovs-subnet* for the *ovs-subnet* plug-in or
*redhat/openshift-ovs-multitenant* for the *ovs-multitenant* plug-in
<4> Service IP allocation for the cluster
====

[IMPORTANT]
====
The `*serviceNetworkCIDR*` and `*hostSubnetLength*` values cannot be changed
after the cluster is first created, and `*clusterNetworkCIDR*` can only be
changed to be a larger network that still contains the original network. For
example, given the default value of *10.128.0.0/14*, you could change
`*clusterNetworkCIDR*` to *10.128.0.0/9* (i.e., the entire upper half of net
10) but not to *10.64.0.0/16*, because that does not overlap the original value.
====

[[configuring-the-pod-network-on-nodes]]
== Configuring the Pod Network on Nodes

Cluster administrators can control pod network settings on nodes by modifying
parameters in the `*networkConfig*` section of the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[node configuration file]
(located at *_/etc/origin/node/node-config.yaml_* by default):

====
[source,yaml]
----
networkConfig:
  mtu: 1450 <1>
  networkPluginName: "redhat/openshift-ovs-subnet" <2>
----
<1> Maximum transmission unit (MTU) for the pod overlay network
<2> Set to *redhat/openshift-ovs-subnet* for the *ovs-subnet* plug-in or
*redhat/openshift-ovs-multitenant* for the *ovs-multitenant* plug-in
====

[[migrating-between-sdn-plugins]]
== Migrating Between SDN Plug-ins

If you are already using one SDN plug-in and want to switch to another:

. Change the `*networkPluginName*` parameter on all
xref:configuring-the-pod-network-on-masters[masters] and
xref:configuring-the-pod-network-on-nodes[nodes] in their configuration files.
ifdef::openshift-origin[]
. Restart the *origin-master* service on masters and the *origin-node* service
on nodes.
endif::[]
ifdef::openshift-enterprise[]
. Restart the *atomic-openshift-master* service on masters and the
*atomic-openshift-node* service on nodes.
endif::[]
. If the existing plug-in is {product-title} SDN plug-in, then cleanup
{product-title} SDN specific artifacts:
----
$ oc delete clusternetwork --all
$ oc delete hostsubnets --all
$ oc delete netnamespaces --all
----

When switching from the *ovs-subnet* to the *ovs-multitenant* {product-title} SDN plug-in,
all the existing projects in the cluster will be fully isolated (assigned unique VNIDs).
Cluster administrators can choose to xref:../admin_guide/managing_pods.adoc#admin-guide-pod-network[modify
the project networks] using the administrator CLI.

Check VNIDs by running:

----
$ oc get netnamespace
----

[[external-access-to-the-cluster-network]]
== External Access to the Cluster Network

If a host that is external to {product-title} requires access to the cluster network,
you have two options:

. Configure the host as an {product-title} node but mark it
xref:../admin_guide/manage_nodes.adoc#marking-nodes-as-unschedulable-or-schedulable[unschedulable]
so that the master does not schedule containers on it.
. Create a tunnel between your host and a host that is on the cluster network.

Both options are presented as part of a practical use-case in the documentation
for configuring xref:../install_config/routing_from_edge_lb.adoc#install-config-routing-from-edge-lb[routing from an
edge load-balancer to containers within {product-title} SDN].
