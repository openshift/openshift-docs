[[install-config-configuring-sdn]]
= Configuring the SDN
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview

The xref:../architecture/networking/sdn.adoc#architecture-additional-concepts-sdn[OpenShift SDN] enables
communication between pods across the {product-title} cluster, establishing a _pod
network_. Two xref:../architecture/networking/sdn.adoc#architecture-additional-concepts-sdn[SDN plug-ins]
are currently available (*ovs-subnet* and *ovs-multitenant*), which provide
different methods for configuring the pod network. A third (*ovs-networkpolicy*) is currently in Tech Preview.

[[admin-guide-configuring-sdn-available-sdn-providers]]
== Available SDN Providers

The upstream Kubernetes project does not come with a default network solution.
Instead, Kubernetes has developed a Container Network Interface (CNI) to allow
network providers for integration with their own SDN solutions.

There are several OpenShift SDN plugins available out of the box from Red Hat,
as well as third-party plug-ins.

Red Hat has worked with a number of SDN providers to certify their SDN network
solution on {product-title} via the Kubernetes CNI interface, including a
support process for their SDN plug-in through their product’s entitlement
process. Should you open a support case with OpenShift, Red Hat can facilitate
an exchange process so that both companies are involved in meeting your needs.

The following SDN solutions are validated and supported on {product-title}
directly by the 3rd party vendor:

* Cisco Contiv (™)
* Juniper Contrail (™)
* Nokia Nuage (™)
* Tigera Calico (™)
* VMware NSX-T (™)

[discrete]
**Installing VMware NSX-T (™) on {product-title}**

VMware NSX-T (™) provides an SDN and security infrastructure to build
cloud-native application environments. In addition to vSphere hypervisors (ESX),
these environments include KVM and native public clouds.

The current integration requires a _new_ install of both NSX-T and
{product-title}. Currently, NSX-T version 2.0 is supported, and only supports
the use of ESX and KVM hypervisors at this time.

See the
link:https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_openshift.pdf[NSX-T
Container Plug-in for OpenShift - Installation and Administration Guide] for
more information.


[[configuring-sdn-config-pod-network-ansible]]
== Configuring the Pod Network with Ansible

For initial xref:../install_config/install/advanced_install.adoc#install-config-install-advanced-install[advanced installations],
the *ovs-subnet* plug-in is installed and configured by default, though it can
be
xref:../install_config/install/advanced_install.adoc#configuring-ansible[overridden during installation]
using the
xref:../install_config/install/advanced_install.adoc#configuring-cluster-variables[`*os_sdn_network_plugin_name*` parameter],
which is configurable in the Ansible inventory file.

.Example SDN Configuration with Ansible
====

----
# Configure the multi-tenant SDN plugin (default is 'redhat/openshift-ovs-subnet')
# os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'

# Configure the NetworkPolicy SDN plugin (Tech Preview)
# os_sdn_network_plugin_name='redhat/openshift-ovs-networkpolicy'

# Disable the OpenShift SDN plugin
# openshift_use_openshift_sdn=False

# Configure SDN cluster network CIDR block. This network block should
# be a private block and should not conflict with existing network
# blocks in your infrastructure that pods may require access to.
# Can not be changed after deployment.
#osm_cluster_network_cidr=10.1.0.0/16

# default subdomain to use for exposed routes
#openshift_master_default_subdomain=apps.test.example.com

# Configure SDN cluster network and kubernetes service CIDR blocks. These
# network blocks should be private and should not conflict with network blocks
# in your infrastructure that pods may require access to. Can not be changed
# after deployment.
#osm_cluster_network_cidr=10.1.0.0/16
#openshift_portal_net=172.30.0.0/16

# Configure number of bits to allocate to each host’s subnet e.g. 8
# would mean a /24 network on the host.
#osm_host_subnet_length=8

# This variable specifies the service proxy implementation to use:
# either iptables for the pure-iptables version (the default),
# or userspace for the userspace proxy.
#openshift_node_proxy_mode=iptables
----
====

ifdef::openshift-enterprise[]
For initial xref:../install_config/install/quick_install.adoc#install-config-install-quick-install[quick installations],
the *ovs-subnet* plug-in is installed and configured by default as well, and can
be
xref:../install_config/master_node_configuration.adoc#master-configuration-files[reconfigured post-installation]
using the `*networkConfig*` stanza of the *_master-config.yaml_* file.
endif::[]

[[configuring-the-pod-network-on-masters]]
== Configuring the Pod Network on Masters

Cluster administrators can control pod network settings on masters by modifying
parameters in the `*networkConfig*` section of the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[master configuration file]
(located at *_/etc/origin/master/master-config.yaml_* by default):

====
[source,yaml]
----
networkConfig:
  clusterNetworkCIDR: 10.128.0.0/14 <1>
  hostSubnetLength: 9 <2>
  networkPluginName: "redhat/openshift-ovs-subnet" <3>
  serviceNetworkCIDR: 172.30.0.0/16 <4>
----
<1> Cluster network for node IP allocation
<2> Number of bits for pod IP allocation within a node
<3> Set to *redhat/openshift-ovs-subnet* for the *ovs-subnet* plug-in,
*redhat/openshift-ovs-multitenant* for the *ovs-multitenant* plug-in, or
*redhat/openshift-ovs-networkpolicy* for the *ovs-networkpolicy* plug-in
<4> Service IP allocation for the cluster
====

[IMPORTANT]
====
The `*serviceNetworkCIDR*` and `*hostSubnetLength*` values cannot be changed
after the cluster is first created, and `*clusterNetworkCIDR*` can only be
changed to be a larger network that still contains the original network. For
example, given the default value of *10.128.0.0/14*, you could change
`*clusterNetworkCIDR*` to *10.128.0.0/9* (i.e., the entire upper half of net
10) but not to *10.64.0.0/16*, because that does not overlap the original value.
====

[[configuring-the-pod-network-on-nodes]]
== Configuring the Pod Network on Nodes

Cluster administrators can control pod network settings on nodes by modifying
parameters in the `*networkConfig*` section of the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[node configuration file]
(located at *_/etc/origin/node/node-config.yaml_* by default):

====
[source,yaml]
----
networkConfig:
  mtu: 1450 <1>
  networkPluginName: "redhat/openshift-ovs-subnet" <2>
----
<1> Maximum transmission unit (MTU) for the pod overlay network
<2> Set to *redhat/openshift-ovs-subnet* for the *ovs-subnet* plug-in,
*redhat/openshift-ovs-multitenant* for the *ovs-multitenant* plug-in, or
*redhat/openshift-ovs-networkpolicy* for the *ovs-networkpolicy* plug-in
====

[[migrating-between-sdn-plugins]]
== Migrating Between SDN Plug-ins

If you are already using one SDN plug-in and want to switch to another:

. Change the `*networkPluginName*` parameter on all
xref:configuring-the-pod-network-on-masters[masters] and
xref:configuring-the-pod-network-on-nodes[nodes] in their configuration files.
ifdef::openshift-origin[]
. Restart the *origin-master* service on masters and the *origin-node* service
on nodes.
endif::[]
ifdef::openshift-enterprise[]
. Restart the *atomic-openshift-master* service on masters and the
*atomic-openshift-node* service on nodes.
endif::[]
. If you are switching from an OpenShift SDN plug-in to a
third-party plug-in, then clean up OpenShift SDN-specific
artifacts:
----
$ oc delete clusternetwork --all
$ oc delete hostsubnets --all
$ oc delete netnamespaces --all
----

When switching from the *ovs-subnet* to the *ovs-multitenant* OpenShift SDN plug-in,
all the existing projects in the cluster will be fully isolated (assigned unique VNIDs).
Cluster administrators can choose to xref:../admin_guide/managing_networking.adoc#admin-guide-pod-network[modify
the project networks] using the administrator CLI.

Check VNIDs by running:

----
$ oc get netnamespace
----

[[migrating-between-sdn-plugins-networkpolicy]]
=== Migrating from the ovs-multitenant Plugin to ovs-networkpolicy

Before migrating from *ovs-multitenant* to *ovs-networkpolicy*, you must ensure that every
namespace has a unique `NetID`; that means that if you have previously
xref:../admin_guide/managing_networking.adoc#joining-project-networks[joined projects
together] or
xref:../admin_guide/managing_networking.adoc#making-project-networks-global[made projects
global], you will need to undo that before switching to the *ovs-networkpolicy* plugin, or
NetworkPolicy objects may not function correctly.

A helper script is available that fixes `NetID`s, creates NetworkPolicy objects to isolate
previously-isolated namespaces, and enabled connections between previously-joined
namespaces.

. Download https://raw.githubusercontent.com/openshift/origin/master/contrib/migration/migrate-network-policy.sh
+
----
$ curl https://raw.githubusercontent.com/openshift/origin/master/contrib/migration/migrate-network-policy.sh
$ chmod a+x migrate-network-policy.sh
----
. While still running the *ovs-multitenant* plugin (that is, before running any of the
migration steps above), run that script as a user with cluster administrator rights on
the OpenShift cluster
+
----
$ ./migrate-network-policy.sh
----

After running the script, every project will now be fully isolated from every other
project, so connection attempts between pods in different projects will fail until you
complete the migration to the *ovs-networkpolicy* plugin.

If you want newly-created projects to also have the same policies by default, you can set
xref:../admin_guide/managing_networking.adoc#admin-guide-networking-networkpolicy-setting-default[default
NetworkPolicy objects] to be created matching the `default-deny` and
`allow-from-global-namespaces` policies created by the migration script.

[NOTE]
====
In case of script failures or other errors, or if you later decide you want to revert back
to the *ovs-multitenant* plugin, you can use the
link:https://raw.githubusercontent.com/openshift/origin/master/contrib/migration/unmigrate-network-policy.sh[un-migration
script]. This script undoes the changes made by the migration script and re-joins
previously-joined projects.
====

[[external-access-to-the-cluster-network]]
== External Access to the Cluster Network

If a host that is external to {product-title} requires access to the cluster network,
you have two options:

. Configure the host as an {product-title} node but mark it
xref:../admin_guide/manage_nodes.adoc#marking-nodes-as-unschedulable-or-schedulable[unschedulable]
so that the master does not schedule containers on it.
. Create a tunnel between your host and a host that is on the cluster network.

Both options are presented as part of a practical use-case in the documentation
for configuring xref:../install_config/routing_from_edge_lb.adoc#install-config-routing-from-edge-lb[routing from an
edge load-balancer to containers within OpenShift SDN].

[[using-flannel]]
== Using Flannel
As an alternate to the default SDN, {product-title} also provides Ansible
playbooks for installing *flannel*-based networking. This is useful if running
{product-title} within a cloud provider platform that also relies on SDN, such
as Red Hat OpenStack Platform, and you want to avoid encapsulating packets twice
through both platforms.

Flannel uses a single IP network space for all of the containers allocating a
contiguous subset of the space to each instance. Consequently, nothing prevents
a container from attempting to contact any IP address in the same network
space. This hinders multi-tenancy because the network cannot be used to isolate
containers in one application from another.

Depending on whether you prefer mutli-tenancy isolation or performance, you should determine the
appropriate choice when deciding between OpenShift SDN (multi-tenancy) and flannel (performance)
for internal networks.

ifndef::openshift-origin[]
[IMPORTANT]
====
Flannel is only supported for {product-title} on Red Hat OpenStack Platform.
====
endif::[]

[IMPORTANT]
====
The current version of Neutron enforces port security on ports by default. This
prevents the port from sending or receiving packets with a MAC address
different from that on the port itself. Flannel creates virtual MACs and IP
addresses and must send and receive packets on the port, so port security must
be disabled on the ports that carry flannel traffic.
====

To enable flannel within your {product-title} cluster:

. Neutron port security controls must be configured to be compatible with
Flannel. The default configuration of Red Hat OpenStack Platform disables user
control of `port_security`. Configure Neutron to allow users to control the
`port_security` setting on individual ports.
+
.. On the Neutron servers, add the following to the
*_/etc/neutron/plugins/ml2/ml2_conf.ini_* file:
+
----
[ml2]
...
extension_drivers = port_security
----
+
.. Then, restart the Neutron services:
+
----
service neutron-dhcp-agent restart
service neutron-ovs-cleanup restart
service neutron-metadata-agentrestart
service neutron-l3-agent restart
service neutron-plugin-openvswitch-agent restart
service neutron-vpn-agent restart
service neutron-server  restart
----

. When creating the {product-title} instances on Red Hat OpenStack Platform, disable both port security and security
groups in the ports where the container network flannel interface will be:
+
----
neutron port-update $port --no-security-groups --port-security-enabled=False
----
+
[NOTE]
====
Flannel gather information from etcd to configure and assign
the subnets in the nodes. Therefore, the security group attached to the etcd
hosts should allow access from nodes to port 2379/tcp, and nodes security
group should allow egress communication to that port on the etcd hosts.
====

.. Set the following variables in your Ansible inventory file before running the
installation:
+
----
openshift_use_openshift_sdn=false <1>
openshift_use_flannel=true <2>
flannel_interface=eth0
----
<1> Set `openshift_use_openshift_sdn` to `false` to disable the default SDN.
<2> Set `openshift_use_flannel` to `true` to enable *flannel* in place.

.. Optionally, you can specify the interface to use for inter-host communication
using the `flannel_interface` variable. Without this variable, the
{product-title} installation uses the default interface.
+
[NOTE]
====
Custom networking CIDR for pods and services using flannel will be supported in a future release.
link:https://bugzilla.redhat.com/show_bug.cgi?id=1473858[*BZ#1473858*]
====

. After the {product-title} installation, add a set of iptables rules on every {product-title} node:
+
----
iptables -A DOCKER -p all -j ACCEPT
iptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE
----
+
To persist those changes in the *_/etc/sysconfig/iptables_* use the following
command on every node:
+
----
cp /etc/sysconfig/iptables{,.orig}
sh -c "tac /etc/sysconfig/iptables.orig | sed -e '0,/:DOCKER -/ s/:DOCKER -/:DOCKER ACCEPT/' | awk '"\!"p && /POSTROUTING/{print \"-A POSTROUTING -o eth1 -j MASQUERADE\"; p=1} 1' | tac > /etc/sysconfig/iptables"
----
+
[NOTE]
====
The `iptables-save` command saves all the current _in memory_ iptables rules.
However, because Docker, Kubernetes and {product-title} create a high number of iptables rules
(services, etc.) not designed to be persisted, saving these rules can become problematic.
====

To isolate container traffic from the rest of the {product-title} traffic, Red Hat
recommends creating an isolated tenant network and attaching all the nodes to it.
If you are using a different network interface (eth1), remember to configure the
interface to start at boot time through the
*_/etc/sysconfig/network-scripts/ifcfg-eth1_* file:

----
DEVICE=eth1
TYPE=Ethernet
BOOTPROTO=dhcp
ONBOOT=yes
DEFTROUTE=no
PEERDNS=no
----


