[[install-config-configuring-local]]
= Configuring Local Volumes
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
{product-title} can be configured to access
xref:../install_config/persistent_storage/persistent_storage_local.adoc#install-config-persistent-storage-persistent-storage-local[local
volumes] for application data.

Local volumes are persistent volumes (PV) that represent locally-mounted file
systems. As of {product-title} 3.10, this also includes raw block devices. A raw
device offers a more direct route to the physical device and allows an
application more control over the timing of I/O operations to that physical
device. This makes raw devices suitable for complex applications such as
database management systems that typically do their own caching. Local volumes
have a few unique features. Any pod that uses a local volume PV is scheduled on
the node where the local volume is mounted.

In addition, local volumes include a provisioner that automatically creates PVs
for locally-mounted devices. This provisioner currently scans only
pre-configured directories. This provisioner cannot dynamically provision
volumes, but this feature might be implemented in a future release.

The local volume provisioner allows using local storage within {product-title}
and supports:

* Volumes
* PVs

[IMPORTANT]
====
Local volumes is a Technology Preview feature only. Technology Preview features
are not supported with Red Hat production service level agreements (SLAs), might
not be functionally complete, and Red Hat does not recommend to use them for
production. These features provide early access to upcoming product features,
enabling customers to test functionality and provide feedback during the
development process. For more information on Red Hat Technology Preview features support scope,
see https://access.redhat.com/support/offerings/techpreview/.
====

[[local-volume-mounting-local-volumes]]
== Mounting local volumes

[NOTE]
====
All local volumes must be manually mounted before they can be consumed by {product-title} as PVs.
====

To mount local volumes:

. Mount all volumes into the
*_/mnt/local-storage/<storage-class-name>/<volume>_* path. Administrators must
create local devices as needed using any method such as disk partition or LVM,
create suitable file systems on these devices, and mount these devices using a
script or `/etc/fstab` entries, for example:
+
[source]
----
# device name   # mount point                  # FS    # options # extra
/dev/sdb1       /mnt/local-storage/ssd/disk1 ext4     defaults 1 2
/dev/sdb2       /mnt/local-storage/ssd/disk2 ext4     defaults 1 2
/dev/sdb3       /mnt/local-storage/ssd/disk3 ext4     defaults 1 2
/dev/sdc1       /mnt/local-storage/hdd/disk1 ext4     defaults 1 2
/dev/sdc2       /mnt/local-storage/hdd/disk2 ext4     defaults 1 2
----

. Make all volumes accessible to the processes running within the Docker containers.
You can change the labels of mounted file systems to allow this, for example:
+
[source, bash]
----
$ chcon -R unconfined_u:object_r:svirt_sandbox_file_t:s0 /mnt/local-storage/
----

[[local-volume-configure-local-provisioner]]
== Configuring the local provisioner
{product-title} depends on an external provisioner to create PVs for local
devices and to clean up PVs when they are not in use to enable reuse.

[NOTE]
====
* The local volume provisioner is different from most provisioners and does not support dynamic provisioning.
* The local volume provisioner requires administrators to preconfigure the local volumes on each node and mount them under discovery directories. The provisioner then manages the volumes by creating and cleaning up PVs for each volume.
====

To configure the local provisioner:

. Configure the external provisioner using a ConfigMap to relate directories with storage classes. This configuration must be created before the provisioner is deployed, for example:
+
[source, yaml]
----
kind: ConfigMap
metadata:
  name: local-volume-config
data:
    storageClassMap: |
        local-ssd: <1>
            hostDir:  /mnt/local-storage/ssd <2>
            mountDir: /mnt/local-storage/ssd <3>
        local-hdd:
            hostDir: /mnt/local-storage/hdd
            mountDir: /mnt/local-storage/hdd
----
<1> Name of the storage class.
<2> Path to the directory on the host. It must be a subdirectory of *_/mnt/local-storage_*.
<3> Path to the directory in the provisioner pod. We recommend using the same directory structure as used on the host and `mountDir` can be omitted in this case.

. _(Optional)_ Create a standalone namespace for the local volume provisioner and its configuration, for example:
`oc new-project local-storage`.

With this configuration, the provisioner creates:

* One PV with storage class `local-ssd` for every subdirectory mounted in the *_/mnt/local-storage/ssd_* directory
* One PV with storage class `local-hdd` for every subdirectory mounted in the *_/mnt/local-storage/hdd_* directory

[WARNING]
====
The syntax of the ConfigMap has changed between {product-title} 3.9 and 3.10. Since this feature is in Technology Preview, the ConfigMap is not automatically converted during the update.
====

[[local-volume-deployment-local-provisioner]]
== Deploying the local provisioner

[NOTE]
====
Before starting the provisioner, mount all local devices and create a ConfigMap
with storage classes and their directories.
====

To deploy the local provisioner:
 
. Install the local provisioner from the link:https://raw.githubusercontent.com/openshift/origin/release-3.10/examples/storage-examples/local-examples/local-storage-provisioner-template.yaml[*_local-storage-provisioner-template.yaml_*] file.

. Create a service account that allows running pods as a root user, using
hostPath volumes, and using any SELinux context to monitor, manage,
and clean local volumes:
+
[source, bash]
----
$ oc create serviceaccount local-storage-admin
$ oc adm policy add-scc-to-user privileged -z local-storage-admin
----
To allow the provisioner pod to delete content on local volumes created by any pod, root privileges and any SELinux context are required. hostPath is required to access the *_/mnt/local-storage_* path on the host.

. Install the template:
+
[source, bash]
----
$ oc create -f https://raw.githubusercontent.com/openshift/origin/release-3.10/examples/storage-examples/local-examples/local-storage-provisioner-template.yaml
----

. Instantiate the template by specifying values for the `CONFIGMAP`, `SERVICE_ACCOUNT`, `NAMESPACE`, and `PROVISIONER_IMAGE` parameters:
+
[source, bash]
----
$ oc new-app -p CONFIGMAP=local-volume-config \
  -p SERVICE_ACCOUNT=local-storage-admin \
  -p NAMESPACE=local-storage \
ifdef::openshift-origin[]
  -p PROVISIONER_IMAGE=quay.io/external_storage/local-volume-provisioner:v1.0.1 \
endif::[]
ifndef::openshift-origin[]
  -p PROVISIONER_IMAGE=registry.access.redhat.com/openshift3/local-storage-provisioner:v3.10 \ <1>
endif::[]
  local-storage-provisioner
----
ifndef::openshift-origin[]
<1> Provide your {product-title} version number, such as `v3.10`.
+
endif::[]

. Add the necessary storage classes:
+
[source, bash]
----
$ oc create -f ./storage-class-ssd.yaml
$ oc create -f ./storage-class-hdd.yaml
----
+
For example:
+
.storage-class-ssd.yaml

[source, yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: local-ssd
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
----
+
.storage-class-hdd.yaml

[source, yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: local-hdd
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
----

See the
link:https://raw.githubusercontent.com/openshift/origin/release-3.10/examples/storage-examples/local-examples/local-storage-provisioner-template.yaml[local
storage provisioner template] for other configurable options. This template
creates a DaemonSet that runs a pod on every node. The pod watches the
directories that are specified in the ConfigMap and automatically creates PVs for
them.

The provisioner runs with root permissions because it removes all data from the
modified directories when a PV is released.

[[local-volume-adding-new-devices]]
== Adding new devices
Adding a new device is semi-automatic. The provisioner periodically checks for
new mounts in configured directories. Administrators must create a new
subdirectory, mount a device, and allow pods to use the device by
applying the SELinux label, for example:

[source, bash]
----
$ chcon -R unconfined_u:object_r:svirt_sandbox_file_t:s0 /mnt/local-storage/
----

[IMPORTANT]
====
Omitting any of these steps may result in the wrong PV being created.
====

[[local-volume-raw-block-devices]]
== Configuring raw block devices
It is possible to statically provision raw block devices using the local
volume provisioner. This feature is disabled by default and requires additional
configuration.

To configure raw block devices:

. Enable the `BlockVolume` feature gate on all masters.
Edit or create the master configuration file on all masters
(*_/etc/origin/master/master-config.yaml_* by default) and add `BlockVolume=true`
under the `apiServerArguments` and `controllerArguments` sections:
+
[source, yaml]
----
apiServerArguments:
   feature-gates:
   - BlockVolume=true
...

 controllerArguments:
   feature-gates:
   - BlockVolume=true
...
----

. Enable the feature gate on all nodes by editing the node configuration ConfigMap:
+
[source, bash]
----
$ oc edit configmap node-config-compute --namespace openshift-node
$ oc edit configmap node-config-master --namespace openshift-node
$ oc edit configmap node-config-infra --namespace openshift-node
----
+
. Ensure that all ConfigMaps contain `BlockVolume=true` in the feature gates
array of the `kubeletArguments`, for example:
+
.node configmap feature-gates setting
[source, yaml]
----
kubeletArguments:
   feature-gates:
   - RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true,BlockVolume=true
----

. Restart the master. The nodes restart automatically after the
configuration change. This may take several minutes.

[[local-volume-prepare-block-devices]]
=== Preparing raw block devices
Before you start the provisioner, link all the raw block devices that pods can use to the *_/mnt/local-storage/<storage class>_* directory structure. For example, to make directory *_/dev/dm-36_* available:

. Create a directory for the device's storage class in *_/mnt/local-storage_*:
+
[source, bash]
----
$ mkdir -p /mnt/local-storage/block-devices
----

. Create a symbolic link that points to the device:
+
[source, bash]
----
$ ln -s /dev/dm-36 dm-uuid-LVM-1234
----
+
[NOTE]
====
To avoid possible name conflicts, use the same name for the symbolic link and the link
from the *_/dev/disk/by-uuid_* or *_/dev/disk/by-id_* directory .
====

. Create or update the ConfigMap that configures the provisioner:
+
[source, yaml]
----
kind: ConfigMap
metadata:
  name: local-volume-config
data:
    storageClassMap: |
        block-devices: <1>
            hostDir:  /mnt/local-storage/block-devices <2>
            mountDir: /mnt/local-storage/block-devices <3>
----
<1> Name of the storage class.
<2> Path to the directory on the host. It must be a subdirectory of *_/mnt/local-storage_*.
<3> Path to the directory in the provisioner pod. If you use the directory structure that the host uses, which is recommended, omit the `mountDir` parameter.
. Change the `SELinux` label of the device and the *_/mnt/local-storage/_*:
+
[source, bash]
----
$ chcon -R unconfined_u:object_r:svirt_sandbox_file_t:s0 /mnt/local-storage/
$ chcon unconfined_u:object_r:svirt_sandbox_file_t:s0 /dev/dm-36
----

. Create a storage class for the raw block devices:
+
[source, yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: block-devices
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
----

The block device *_/dev/dm-36_* is now ready to be used by the provisioner and
provisioned as a PV.

[[local-volume-prepare-block-devices-deploy-provisioner]]
=== Deploying raw block device provisioners

Deploying the provisioner for raw block devices is similar to deploying the
provisioner on local volumes. There are two differences: 

. The provisioner must run in a privileged container.
. The provisioner must have access to the *_/dev_* file system from the host.

To deploy the provisioner for raw block devices:

. Download the template from the
link:https://raw.githubusercontent.com/openshift/origin/release-3.10/examples/storage-examples/local-examples/local-storage-provisioner-template.yaml[*_local-storage-provisioner-template.yaml_*] file.

. Edit the template:
.. Set the `privileged` attribute of the `securityContext` of the container spec to `true`:
+
[source, yaml]
----
...
  containers:
...
    name: provisioner
...
      securityContext:
        privileged: true
...
----

.. Mount the host *_/dev/_* file system to the container using `hostPath`:
+
[source, yaml]
----
...
  containers:
...
    name: provisioner
...
    volumeMounts:
    - mountPath: /dev
      name: dev
...
  volumes:
    - hostPath:
        path: /dev
        name: dev
...
----

. Create the template from the modified YAML file:
+
[source, bash]
----
$ oc create -f local-storage-provisioner-template.yaml
----

. Start the provisioner:
+
[source, bash]
----
$ oc new-app -p CONFIGMAP=local-volume-config \
  -p SERVICE_ACCOUNT=local-storage-admin \
  -p NAMESPACE=local-storage \
ifdef::openshift-origin[]
  -p PROVISIONER_IMAGE=quay.io/external_storage/local-volume-provisioner:v1.0.1 \
endif::[]
ifndef::openshift-origin[]
  -p
  PROVISIONER_IMAGE=registry.access.redhat.com/openshift3/local-storage-provisioner:v3.10 \
endif::[]
  local-storage-provisioner
----

[[local-volume-using-raw-block-device-pv]]
=== Using raw block device persistent volumes

To use the raw block device in the pod, create a persistent volume claim (PVC) with `volumeMode:` set to `Block` and `storageClassName` set to `block-devices`, for example:

[source, yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  storageClassName: block-devices
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
----

.Pod using the raw block device PVC

[source, yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: busybox-test
  labels:
    name: busybox-test
spec:
  restartPolicy: Never
  containers:
    - resources:
        limits :
          cpu: 0.5
      image: gcr.io/google_containers/busybox
      command:
        - "/bin/sh"
        - "-c"
        - "while true; do date; sleep 1; done"
      name: busybox
      volumeDevices:
        - name: vol
          devicePath: /dev/xvda
  volumes:
      - name: vol
        persistentVolumeClaim:
          claimName: block-pvc
----

[NOTE]
====
The volume is not mounted in the pod but is exposed as the *_/dev/xvda_* raw block device.
====
