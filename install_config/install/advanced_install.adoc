[[install-config-install-advanced-install]]
= Advanced Installation
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:
ifdef::openshift-enterprise[]
:pb-prefix: /usr/share/ansible/openshift-ansible/
endif::[]
ifdef::openshift-origin[]
:pb-prefix: ~/openshift-ansible/
endif::[]
include::_snippets/glusterfs.adoc[]

toc::[]

== Overview

A reference configuration implemented using
link:http://docs.ansible.com/ansible/[Ansible] playbooks is available as the _advanced
installation_ method for installing a {product-title} cluster. Familiarity with Ansible is
assumed, however you can use this configuration as a reference to create your
own implementation using the configuration management tool of your choosing.

[IMPORTANT]
====
While RHEL Atomic Host is supported for running containerized {product-title}
services, the advanced installation method utilizes Ansible, which is not
available in RHEL Atomic Host. The RPM-based installer must therefore be run
from
ifdef::openshift-enterprise[]
a RHEL 7 system.
endif::[]
ifdef::openshift-origin[]
a supported version of Fedora, CentOS, or RHEL.
endif::[]
The host initiating the installation does not need to be intended for inclusion
in the {product-title} cluster, but it can be. Alternatively, a
xref:running-the-advanced-installation-system-container[containerized version of the installer] is available as a system container, which can be run from a RHEL
Atomic Host system.
====

[NOTE]
====
To install {product-title} as a stand-alone registry, see
xref:../../install_config/install/stand_alone_registry.adoc#install-config-installing-stand-alone-registry[Installing a Stand-alone Registry].
====

[[advanced-before-you-begin]]
== Before You Begin

Before installing {product-title}, you must first see the
xref:../../install_config/install/prerequisites.adoc#install-config-install-prerequisites[Prerequisites]
and
xref:../../install_config/install/host_preparation.adoc#install-config-install-host-preparation[Host
Preparation] topics to prepare your hosts. This includes verifying system and
environment requirements per component type and properly installing and
configuring Docker. It also includes installing Ansible version 2.4 or later,
as the advanced installation method is based on Ansible playbooks and as such
requires directly invoking Ansible.

If you are interested in installing {product-title} using the containerized method
(optional for RHEL but required for RHEL Atomic Host), see
xref:../../install_config/install/rpm_vs_containerized.adoc#install-config-install-rpm-vs-containerized[Installing on Containerized Hosts] to ensure that you understand the differences between these
methods, then return to this topic to continue.

For large-scale installs, including suggestions for optimizing install time,
see the
xref:../../scaling_performance/install_practices.adoc#scaling-performance-install-best-practices[Scaling and Performance Guide].

After following the instructions in the
xref:../../install_config/install/prerequisites.adoc#install-config-install-prerequisites[Prerequisites]
topic and deciding between the RPM and containerized methods, you can continue
in this topic to xref:configuring-ansible[Configuring Ansible Inventory Files].

ifdef::openshift-origin[]
[[advanced-cloud-providers]]
=== Cloud installation

{product-title} VMs can be provisioned in a cloud environment.

You can use Ansible playbooks to automate defining of your cloud hosted infrastructure and
applying post-provision configuration for the supported cloud providers. Follow these advanced installation
to help you create the needed playbooks.

==== OpenStack provider

As an alternate, you can install {product-title} using the OpenStack CLI.
For more information, See the
link:https://access.redhat.com/documentation/en-us/reference_architectures/2017/html-single/deploying_and_managing_red_hat_openshift_container_platform_3.6_on_red_hat_openstack_platform_10[reference architecture] for {product-title} 3.6 and Red Hat OpenStack Platform 10.

As a prerequisite to using the OpenStack CLI, first provision VMs and configure the cloud
infrastructure, such as networking, storage, firewall, and security groups.
For information on these configuration tasks using the reference architecture,
see the
xref:../../install_config/install/prerequisites#prereq-cloud-provider-considerations[cloud provider considerations]
and link:https://github.com/openshift/openshift-ansible/tree/master/playbooks/openstack[Ansible playbooks]
to automate it. See also
xref:../../install_config/configuring_openstack#install-config-configuring-openstack[Configuring for OpenStack]
and
xref:configuring-ansible[Configuring Ansible Inventory Files].

ifdef::openshift-enterprise[]
[IMPORTANT]
====
The reference architecture for automated installations based on
link:https://docs.openstack.org/heat/latest[OpenStack Heat] templates for
link:https://access.redhat.com/documentation/en-us/reference_architectures/2017/html/deploying_red_hat_openshift_container_platform_3.4_on_red_hat_openstack_platform_10[{product-title} 3.4 on Red Hat OpenStack Platform 10]
is no longer supported. For the Red Hat OpenStack 13 release, this process is replaced with the
link:https://github.com/openshift/openshift-ansible/tree/master/playbooks/openstack[Ansible driven deployment solution].
For automated installations, follow that guide.
====
endif::openshift-enterprise[]
endif::openshift-origin[]

[[configuring-ansible]]
== Configuring Ansible Inventory Files

The *_/etc/ansible/hosts_* file is Ansible's inventory file for the playbook
used to install {product-title}. The inventory file describes the configuration
for your {product-title} cluster. You must replace the default contents of the
file with your desired configuration.

The following sections describe commonly-used variables to set in your inventory
file during an advanced installation, followed by
xref:adv-install-example-inventory-files[example inventory files] you can use as
a starting point for your installation.

Many of the Ansible variables described are optional. Accepting the default
values should suffice for development environments, but for production
environments, it is recommended you read through and become familiar with the
various options available.

The example inventories describe various environment topographies, including
xref:multiple-masters[using multiple masters for high availability]. You can
choose an example that matches your requirements, modify it to match your own
environment, and use it as your inventory file when
xref:running-the-advanced-installation[running the advanced installation].

[discrete]
[[advanced-install-image-version-policy]]
=== Image Version Policy

Images require a version number policy in order to maintain updates. See
the
xref:../../architecture/core_concepts/containers_and_images.adoc#architecture-images-tag-policy[Image
Version Tag Policy] section in the Architecture Guide for more information.

[[configuring-cluster-variables]]
=== Configuring Cluster Variables

To assign environment variables during the Ansible install that apply more
globally to your {product-title} cluster overall, indicate the desired variables in
the *_/etc/ansible/hosts_* file on separate, single lines within the *[OSEv3:vars]*
section. For example:

----
[OSEv3:vars]

openshift_master_identity_providers=[{'name': 'htpasswd_auth',
'login': 'true', 'challenge': 'true',
'kind': 'HTPasswdPasswordIdentityProvider',
'filename': '/etc/origin/master/htpasswd'}]

openshift_master_default_subdomain=apps.test.example.com
----

The following tables describe variables for use with the Ansible installer that
can be assigned cluster-wide:

[[cluster-variables-table]]
.General Cluster Variables
[cols=".^5,.^5a",options="header"]
|===

|Variable |Purpose

|`ansible_ssh_user`
|This variable sets the SSH user for the installer to use and defaults to
`root`. This user should allow SSH-based authentication
xref:../../install_config/install/host_preparation.adoc#ensuring-host-access[without requiring a password]. If
using SSH key-based authentication, then the key should be managed by an SSH
agent.

|`ansible_become`
|If `ansible_ssh_user` is not `root`, this variable must be set to `true` and
the user must be configured for passwordless `sudo`.

|`debug_level`
a|This variable sets which INFO messages are logged to the `systemd-journald.service`. Set one of the following:

* `0` to log errors and warnings only
* `2` to log normal information (This is the default level.)
* `4` to log debugging-level information
* `6` to log API-level debugging information (request / response)
* `8` to log body-level API debugging information

For more information on debug log levels, see xref:../../install_config/master_node_configuration.adoc#master-node-config-logging-levels[Configuring Logging Levels].

|`containerized`
|If set to `true`, containerized {product-title} services are run on all target master
and node hosts in the cluster instead of installed using RPM packages. If set to
`false` or unset, the default RPM method is used. RHEL Atomic Host requires the
containerized method, and is automatically selected for you based on the
detection of the *_/run/ostree-booted_* file. See
xref:../../install_config/install/rpm_vs_containerized.adoc#install-config-install-rpm-vs-containerized[Installing on
Containerized Hosts] for more details.
ifdef::openshift-enterprise[]
Containerized installations are supported starting in {product-title} 3.1.1.
endif::[]

|`openshift_clock_enabled`
| Whether to enable Network Time Protocol (NTP) on cluster nodes. `true` by default.
[IMPORTANT]
====
To prevent masters and nodes in the
cluster from going out of sync, do not change the default value of this parameter.
====


|`openshift_master_admission_plugin_config`
a|This variable sets the parameter and arbitrary JSON values as per the requirement in your inventory hosts file. For example:
----
openshift_master_admission_plugin_config={"ClusterResourceOverride":{"configuration":{"apiVersion":"v1","kind":"ClusterResourceOverrideConfig","memoryRequestToLimitPercent":"25","cpuRequestToLimitPercent":"25","limitCPUToMemoryPercent":"200"}}}
----

|`openshift_master_audit_config`
|This variable enables API service auditing. See
xref:../../install_config/master_node_configuration.adoc#master-node-config-audit-config[Audit
Configuration] for more information.

|`openshift_master_cluster_hostname`
|This variable overrides the host name for the cluster, which defaults to the
host name of the master.

|`openshift_master_cluster_public_hostname`
|This variable overrides the public host name for the cluster, which defaults to
the host name of the master.

|`openshift_master_cluster_method`
|Optional. This variable defines the HA method when deploying multiple masters.
Supports the `native` method. See xref:multiple-masters[Multiple Masters] for
more information.

|`openshift_rolling_restart_mode`
|This variable enables rolling restarts of HA masters (i.e., masters are taken
down one at a time) when
xref:../../upgrading/automated_upgrades.adoc#install-config-upgrading-automated-upgrades[running
the upgrade playbook directly]. It defaults to `services`, which allows rolling
restarts of services on the masters. It can instead be set to `system`, which
enables rolling, full system restarts and also works for single master clusters.

|`openshift_master_identity_providers`
|This variable sets the
xref:../../install_config/configuring_authentication.adoc#install-config-configuring-authentication[identity provider].
The default value is
xref:../../install_config/configuring_authentication.adoc#DenyAllPasswordIdentityProvider[Deny
All]. If you use a supported identity provider, configure {product-title} to
use it.

|`openshift_master_named_certificates`
.2+.^|These variables are used to configure xref:../../install_config/certificate_customization.adoc#install-config-certificate-customization[custom certificates] which are deployed as part of the installation. See xref:advanced-install-custom-certificates[Configuring Custom Certificates] for more information.
|`openshift_master_overwrite_named_certificates`

|`openshift_hosted_registry_cert_expire_days`
|Validity of the auto-generated registry certificate in days. Defaults to `730` (2 years).

|`openshift_ca_cert_expire_days`
|Validity of the auto-generated CA certificate in days. Defaults to `1825` (5 years).

|`openshift_node_cert_expire_days`
|Validity of the auto-generated node certificate in days. Defaults to `730` (2 years).

|`openshift_master_cert_expire_days`
|Validity of the auto-generated master certificate in days. Defaults to `730` (2 years).

|`etcd_ca_default_days`
|Validity of the auto-generated external etcd certificates in days. Controls
validity for etcd CA, peer, server and client certificates. Defaults to `1825`
(5 years).

|`os_firewall_use_firewalld`
|Set to `true` to use firewalld instead of the default iptables. Not available on RHEL Atomic Host. See the xref:advanced-install-configuring-firewalls[Configuring the Firewall] section for more information.

|`openshift_master_session_name`
.4+.^|These variables override defaults for
xref:../../install_config/configuring_authentication.adoc#session-options[session
options] in the OAuth configuration. See xref:advanced-install-session-options[Configuring Session Options] for more information.

|`openshift_master_session_max_seconds`

|`openshift_master_session_auth_secrets`

|`openshift_master_session_encryption_secrets`

|`openshift_set_node_ip`
|This variable configures `nodeIP` in the node configuration. This variable is needed in cases where it is desired for node traffic to go over an interface other than the default network interface. The host variable `openshift_ip` can also be configured on each node to set a specific IP that might not be the IP of the default route.

|`openshift_master_image_policy_config`
|Sets `imagePolicyConfig` in the master configuration. See xref:../../install_config/master_node_configuration.adoc#master-config-image-config[Image Configuration] for details.

|`openshift_router_selector`
|Default node selector for automatically deploying router pods. See
xref:configuring-node-host-labels[Configuring Node Host Labels] for details.

|`openshift_registry_selector`
|Default node selector for automatically deploying registry pods. See
xref:configuring-node-host-labels[Configuring Node Host Labels] for details.

|`openshift_template_service_broker_namespaces`
|This variable enables the template service broker by specifying one or more
namespaces whose templates will be served by the broker.

|`template_service_broker_selector`
|Default node selector for automatically deploying template service broker pods,
defaults `{"region": "infra"}`. See
xref:configuring-node-host-labels[Configuring Node Host Labels] for details.

|`osm_default_node_selector`
|This variable overrides the node selector that projects will use by default when
placing pods, which is defined by the `projectConfig.defaultNodeSelector` field
in the master configuration file. Starting in {product-title} 3.9, this defaults
to `node-role.kubernetes.io/compute=true` if undefined.

|`openshift_docker_additional_registries`
a|{product-title} adds the specified additional registry or registries to the
*docker* configuration. These are the registries to search.
If the registry required to access the registry is other than `80` include the port number required.

For example:

----
openshift_docker_additional_registries=example.com:443
----

|`openshift_docker_insecure_registries`
|{product-title} adds the specified additional insecure registry or registries to
the *docker* configuration. For any of these registries, secure sockets layer
(SSL) is not verified. Also, add these registries to `openshift_docker_additional_registries`.

|`openshift_docker_blocked_registries`
|{product-title} adds the specified blocked registry or registries to the
*docker* configuration. Block the listed registries. Setting this to `all`
blocks everything not in the other variables.

|`openshift_metrics_hawkular_hostname`
|This variable sets the host name for integration with the metrics console by
overriding `metricsPublicURL` in the master configuration for cluster metrics.
If you alter this variable, ensure the host name is accessible via your router.

|`openshift_clusterid`
|This variable is a cluster identifier unique to the AWS Availability Zone. Using this avoids potential issues in Amazon Web Service
(AWS) with multiple zones or multiple clusters. See xref:../../install_config/configuring_aws.adoc#aws-cluster-labeling[Labeling Clusters for AWS] for details.

|`openshift_image_tag`
|Use this variable to specify a container image tag to install or configure.

|`openshift_pkg_version`
|Use this variable to specify an RPM version to install or configure.

|===

[WARNING]
====
If you modify the `openshift_image_tag` or the `openshift_pkg_version` variables
after the cluster is set up, then an upgrade can be triggered, resulting in
downtime.

* If `openshift_image_tag` is set, its value is used for all hosts in
containerized environments, even those that have another version installed. If
* `openshift_pkg_version` is set, its value is used for all hosts in RPM-based
environments, even those that have another version installed.
====

[[advanced-install-networking-variables-table]]
.Networking Variables
[options="header"]
|===

|Variable |Purpose

|`openshift_master_default_subdomain`
|This variable overrides the default subdomain to use for exposed
xref:../../architecture/networking/routes.adoc#architecture-core-concepts-routes[routes].

|`os_sdn_network_plugin_name`
|This variable configures which
xref:../../architecture/networking/sdn.adoc#architecture-additional-concepts-sdn[OpenShift SDN plug-in] to
use for the pod network, which defaults to `redhat/openshift-ovs-subnet` for the
standard SDN plug-in. Set the variable to `redhat/openshift-ovs-multitenant` to
use the multitenant SDN plug-in.

|`osm_cluster_network_cidr`
|This variable overrides the SDN cluster network CIDR block. This is the network
from which pod IPs are assigned. This network block should be a private block
and must not conflict with existing network blocks in your infrastructure to
which pods, nodes, or the master may require access. Defaults to `10.128.0.0/14`
and cannot be arbitrarily re-configured after deployment, although certain
changes to it can be made in the
xref:../../install_config/configuring_sdn.adoc#configuring-the-pod-network-on-masters[SDN
master configuration].

|`openshift_portal_net`
|This variable configures the subnet in which
xref:../../architecture/core_concepts/pods_and_services.adoc#services[services]
will be created within the
xref:../../architecture/networking/sdn.adoc#architecture-additional-concepts-sdn[{product-title}
SDN]. This network block should be private and must not conflict with any
existing network blocks in your infrastructure to which pods, nodes, or the
master may require access to, or the installation will fail. Defaults to
`172.30.0.0/16`, and cannot be re-configured after deployment. If changing from the default, avoid `172.17.0.0/16`, which the *docker0* network bridge uses by default, or modify the *docker0* network.

|`osm_host_subnet_length`
|This variable specifies the size of the per host subnet allocated for pod IPs
by
xref:../../architecture/networking/sdn.adoc#sdn-design-on-masters[{product-title}
SDN]. Defaults to `9` which means that a subnet of size /23 is allocated to each
host; for example, given the default 10.128.0.0/14 cluster network, this will
allocate 10.128.0.0/23, 10.128.2.0/23, 10.128.4.0/23, and so on. This cannot be
re-configured after deployment.

|`openshift_node_proxy_mode`
|This variable specifies the
xref:../../architecture/core_concepts/pods_and_services.adoc#service-proxy-mode[service
proxy mode] to use: either `iptables` for the default, pure-`iptables`
implementation, or `userspace` for the user space proxy.

|`openshift_use_flannel`
|This variable enables *flannel* as an alternative networking layer instead of
the default SDN. If enabling *flannel*, disable the default SDN with the
`openshift_use_openshift_sdn` variable. For more information, see xref:../../install_config/configuring_sdn.adoc#using-flannel[Using Flannel].

|`openshift_use_openshift_sdn`
|Set to `false` to disable the OpenShift SDN plug-in.

|===

[[advanced-install-deployment-types]]
=== Configuring Deployment Type

Various defaults used throughout the playbooks and roles used by the installer
are based on the deployment type configuration (usually defined in an Ansible
inventory file).

ifdef::openshift-enterprise[]
Ensure the `openshift_deployment_type` parameter in your inventory file's `[OSEv3:vars]`
section is set to `openshift-enterprise` to install the {product-title} variant:

----
[OSEv3:vars]
openshift_deployment_type=openshift-enterprise
----
endif::[]
ifdef::openshift-origin[]
Ensure the `openshift_deployment_type` parameter in your inventory file's `[OSEv3:vars]`
section is set to `origin` to install the {product-title} variant:

----
[OSEv3:vars]
openshift_deployment_type=origin
----
endif::[]


[[configuring-host-variables]]
=== Configuring Host Variables

To assign environment variables to hosts during the Ansible installation, indicate
the desired variables in the *_/etc/ansible/hosts_* file after the host entry in
the *[masters]* or *[nodes]* sections. For example:

----
[masters]
ec2-52-6-179-239.compute-1.amazonaws.com openshift_public_hostname=ose3-master.public.example.com
----

The following table describes variables for use with the Ansible installer that
can be assigned to individual host entries:

[[advanced-host-variables]]
.Host Variables
[options="header"]
|===

|Variable |Purpose

|`openshift_hostname`
|This variable overrides the internal cluster host name for the system. Use this
when the system's default IP address does not resolve to the system host name.

|`openshift_public_hostname`
|This variable overrides the system's public host name. Use this for cloud
installations, or for hosts on networks using a network address translation
(NAT).

|`openshift_ip`
|This variable overrides the cluster internal IP address for the system. Use this
when using an interface that is not configured with the default
route.`openshift_ip` can be used for etcd.

|`openshift_public_ip`
|This variable overrides the system's public IP address. Use this for cloud
installations, or for hosts on networks using a network address translation
(NAT).

|`containerized`
|If set to *true*, containerized {product-title} services are run on the target master and
node hosts instead of installed using RPM packages. If set to *false* or unset,
the default RPM method is used. RHEL Atomic Host requires the containerized
method, and is automatically selected for you based on the detection of the
*_/run/ostree-booted_* file. See
xref:../../install_config/install/rpm_vs_containerized.adoc#install-config-install-rpm-vs-containerized[Installing on Containerized Hosts] for more details.
ifdef::openshift-enterprise[]
Containerized installations are supported starting in {product-title} 3.1.1.
endif::[]

|`openshift_node_labels`
|This variable adds labels to nodes during installation. See
xref:configuring-node-host-labels[Configuring Node Host Labels] for more
details.

|`openshift_node_kubelet_args`
|This variable is used to configure `kubeletArguments` on nodes, such as
arguments used in xref:../../admin_guide/garbage_collection.adoc#admin-guide-garbage-collection[container and
image garbage collection], and to
xref:../../admin_guide/manage_nodes.adoc#configuring-node-resources[specify
resources per node]. `kubeletArguments` are key value pairs that are passed
directly to the Kubelet that match the
https://kubernetes.io/docs/admin/kubelet/[Kubelet's command line
arguments]. `kubeletArguments` are not migrated or validated and may become
invalid if used. These values override other settings in node configuration
which may cause invalid configurations. Example usage:
*{'image-gc-high-threshold': ['90'],'image-gc-low-threshold': ['80']}*.

|`openshift_docker_options`
a|This variable configures additional `docker` options within
*_/etc/sysconfig/docker_*, such as options used in
xref:../../install_config/install/host_preparation.adoc#managing-docker-container-logs[Managing Container Logs].
It is recommended to use `json-file`.

The following example shows the configuration of Docker to use the `json-file` log
driver, where Docker will rotate between three 1 MB log files:
----
"--log-driver json-file --log-opt max-size=1M --log-opt max-file=3"
----
Do not use when
xref:advanced-install-docker-system-container[running `docker` as a system container].

|`openshift_schedulable`
|This variable configures whether the host is marked as a schedulable node,
meaning that it is available for placement of new pods. See
xref:marking-masters-as-unschedulable-nodes[Configuring Schedulability on Masters].

|===

[[configuring-host-port]]
=== Configuring Master API Port

To configure the default ports used by the master API, configure the following
variables in the *_/etc/ansible/hosts_* file:

[[advanced-master-ports]]
.Master API Port
[options="header"]
|===

|Variable |Purpose
|`openshift_master_api_port`
|This variable sets the port number to access the {product-title} API.
|===

For example:

----
openshift_master_api_port=3443
----

The web console port setting (`openshift_master_console_port`) must match the
API server port (`openshift_master_api_port`).

[[configuring-cluster-pre-install-checks]]
=== Configuring Cluster Pre-install Checks

Pre-install checks are a set of diagnostic tasks that run as part of the
*openshift_health_checker* Ansible role. They run prior to an Ansible
installation of {product-title}, ensure that required inventory values are set,
and identify potential issues on a host that can prevent or interfere with a
successful installation.

The following table describes available pre-install checks that will run before
every Ansible installation of {product-title}:

[[configuring-cluster-pre-install-checks-pre-install-checks]]
.Pre-install Checks
[options="header"]
|===

|Check Name |Purpose

|`memory_availability`
|This check ensures that a host has the recommended amount of memory for the
specific deployment of {product-title}. Default values have been derived from
the
xref:../../install_config/install/prerequisites.adoc#system-requirements[latest
installation documentation]. A user-defined value for minimum memory
requirements may be set by setting the `openshift_check_min_host_memory_gb`
cluster variable in your inventory file.

|`disk_availability`
|This check only runs on etcd, master, and node hosts. It ensures that the mount
path for an {product-title} installation has sufficient disk space remaining.
Recommended disk values are taken from the
xref:../../install_config/install/prerequisites.adoc#system-requirements[latest
installation documentation]. A user-defined value for minimum disk space
requirements may be set by setting `openshift_check_min_host_disk_gb` cluster
variable in your inventory file.

|`docker_storage`
|Only runs on hosts that depend on the *docker* daemon (nodes and containerized
installations). Checks that *docker*'s total usage does not exceed a
user-defined limit. If no user-defined limit is set, *docker*'s maximum usage
threshold defaults to 90% of the total size available. The threshold limit for
total percent usage can be set with a variable in your inventory file:
`max_thinpool_data_usage_percent=90`. A user-defined limit for maximum thinpool
usage may be set by setting the `max_thinpool_data_usage_percent` cluster
variable in your inventory file.

|`docker_storage_driver`
|Ensures that the *docker* daemon is using a storage driver supported by
{product-title}. If the `devicemapper` storage driver is being used, the check
additionally ensures that a loopback device is not being used. For more
information, see
link:https://docs.docker.com/storage/storagedriver/device-mapper-driver/[Docker's
Use the Device Mapper Storage Driver guide].

|`docker_image_availability`
|Attempts to ensure that images required by an {product-title} installation are
available either locally or in at least one of the configured container image
registries on the host machine.

|`package_version`
|Runs on `yum`-based systems determining if multiple releases of a required
{product-title} package are available. Having multiple releases of a package
available during an `enterprise` installation of OpenShift suggests that there
are multiple `yum` repositories enabled for different releases, which may lead
to installation problems. This check is skipped if the `openshift_release`
variable is not defined in the inventory file.

|`package_availability`
|Runs prior to non-containerized installations of {product-title}. Ensures that
RPM packages required for the current installation are available.

|`package_update`
|Checks whether a `yum` update or package installation will succeed, without
actually performing it or running `yum` on the host.
|===

To disable specific pre-install checks, include the variable
`openshift_disable_check` with a comma-delimited list of check names in your
inventory file. For example:

----
openshift_disable_check=memory_availability,disk_availability
----

[NOTE]
====
A similar set of health checks meant to run for diagnostics on existing clusters
can be found in
xref:../../admin_guide/diagnostics_tool.adoc#admin-guide-health-checks-via-ansible-playbook[Ansible-based Health Checks]. Another set of checks for checking certificate expiration can be
found in
xref:../../install_config/redeploying_certificates.adoc#install-config-redeploying-certificates[Redeploying Certificates].
====

[[advanced-install-configuring-system-containers]]
=== Configuring System Containers

System containers provide a way to containerize services that need to run before
the `docker` daemon is running. They are Docker-formatted containers that use:

- link:https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/content_management_guide/managing_ostree_content[OSTree] for storage,
- link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/managing_containers/#running_containers_with_runc[runC] for the runtime,
- link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/managing_containers/#using_systemd_with_containers[systemd] for service management, and
- link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/managing_containers/#using_skopeo_to_work_with_container_registries[skopeo] for searching.

System containers are therefore stored and run outside of the traditional
`docker` service. For more details on system container technology, see
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/managing_containers/running_system_containers[Running System Containers] in the _Red Hat Enterprise Linux Atomic Host: Managing Containers_ documentation.

You can configure your {product-title} installation to run certain components as
system containers instead of their RPM or standard containerized methods.
Currently, the `docker` and etcd components can be run as system containers in
{product-title}.

[WARNING]
====
System containers are currently OS-specific because they require specific
versions of `atomic` and systemd. For example, different system containers are
created for RHEL, Fedora, or CentOS. Ensure that the system containers you are
using match the OS of the host they will run on.
ifdef::openshift-enterprise[]
{product-title} only supports RHEL and RHEL Atomic as the host OS, so by default
system containers built for RHEL are used.
endif::[]
====

[[advanced-install-docker-system-container]]
==== Running Docker as a System Container

include::install_config/install/advanced_install.adoc[tag=syscontainers_techpreview]

The traditional method for using `docker` in an {product-title} cluster is an
xref:../../install_config/install/host_preparation.adoc#installing-docker[RPM package installation]. For Red Hat Enterprise Linux (RHEL) systems, it must be
specifically installed; for RHEL Atomic Host systems, it is provided by default.

However, you can configure your {product-title} installation to alternatively
run `docker` on node hosts as a system container. When using the system
container method, the `container-engine` container image and systemd service is
used on the host instead of the `docker` package and service.

To run `docker` as a system container:

. Because the default storage back end for Docker on RHEL 7 is a thin pool on
loopback devices, for any RHEL systems you must still configure a thin pool
logical volume for `docker` to use before running the {product-title}
installation. You can skip these steps for any RHEL Atomic Host systems.
+
For any RHEL systems, perform the steps described in the following sections:
+
--
... xref:../../install_config/install/host_preparation.adoc#installing-docker[Installing Docker]

... xref:../../install_config/install/host_preparation.adoc#configuring-docker-storage[Configuring Docker Storage]
--
After completing the storage configuration steps, you can leave the RPM
installed.

. Set the following cluster variable to `True` in your inventory file in the
`[OSEv3:vars]` section:
+
----
openshift_docker_use_system_container=True
----

When using the system container method, the following inventory variables for
`docker` are ignored:

- `docker_version`
- `docker_upgrade`

Further, the following inventory variable must not be used:

- `openshift_docker_options`

You can also force `docker` in the system container to use a specific container
registry and repository when pulling the `container-engine` image instead of
from the default `registry.access.redhat.com/openshift3/`. To do so, set the
following cluster variable in your inventory file in the `[OSEv3:vars]` section:

----
openshift_docker_systemcontainer_image_override="<registry>/<user>/<image>:<tag>"
----

[[advanced-install-etcd-system-container]]
==== Running etcd as a System Container

include::install_config/install/advanced_install.adoc[tag=syscontainers_techpreview]

When using the RPM-based installation method for {product-title}, etcd is
installed using RPM packages on any RHEL systems. When using the containerized
installation method, the `rhel7/etcd` image is used instead for RHEL or RHEL
Atomic Hosts.

However, you can configure your {product-title} installation to alternatively
run etcd as a system container. Whereas the standard containerized method uses
a systemd service named `etcd_container`, the system container method uses the
service name etcd, same as the RPM-based method. The data directory for etcd
using this method is *_/var/lib/etcd_*.

To run etcd as a system container, set the following cluster variable in your
inventory file in the `[OSEv3:vars]` section:

----
openshift_use_etcd_system_container=True
----

[[advanced-install-configuring-registry-location]]
=== Configuring a Registry Location

If you are using an image registry other than the default at
`registry.access.redhat.com`, specify the desired registry within the
*_/etc/ansible/hosts_* file.

----
oreg_url=example.com/openshift3/ose-${component}:${version}
openshift_examples_modify_imagestreams=true
----

.Registry Variables
[options="header"]
|===

|Variable |Purpose
|`*oreg_url*`
|Set to the alternate image location. Necessary if you are not using the default registry at `registry.access.redhat.com`.

|`*openshift_examples_modify_imagestreams*`
|Set to `true` if pointing to a registry other than the default. Modifies the image stream location to the value of `*oreg_url*`.

|`*openshift_docker_additional_registries*`
|Specify the additional registry or registries.
If the registry required to access the registry is other than `80` include the port number required.
|===

For example:
----
oreg_url=example.com/openshift3/ose-${component}:${version}
openshift_examples_modify_imagestreams=true
openshift_docker_additional_registries=example.com:443
----

[[advanced-install-configuring-docker-route]]
=== Configuring a Registry Route

To allow users to push and pull images to the internal Docker registry from
outside of the {product-title} cluster, configure the registry route in the
*_/etc/ansible/hosts_* file. By default, the registry route is
*_docker-registry-default.router.default.svc.cluster.local_*.


.Registry Route Variables
[options="header"]
|===

|Variable |Purpose
|`*openshift_hosted_registry_routehost*`
|Set to the value of the desired registry route. The route contains either
a name that resolves to an infrastructure node where a router manages
communication or the subdomain that you set as the default application subdomain
wildcard value. For example, if you set the `openshift_master_default_subdomain`
parameter to `apps.example.com` and `*.apps.example.com` resolves to
infrastructure nodes or a load balancer, you might use
`registry.apps.example.com` as the registry route.

|`*openshift_hosted_registry_routecertificates*`
a|Set the paths to the registry certificates. If you do not provide values for
the certificate locations, certificates are generated. You can define locations
for the following certificates:

* `*certfile*`
* `*keyfile*`
* `*cafile*`

|`*openshift_hosted_registry_routetermination*`
a| Set to one of the following values:

* Set to `reencrypt` to terminate encryption at the edge
router and re-encrypt it with a new certificate supplied by the destination.
* Set to `passthrough` to terminate encryption at
the destination. The destination is responsible for decrypting traffic.
|===

For example:
----
openshift_hosted_registry_routehost=<path>
openshift_hosted_registry_routetermination=reencrypt
openshift_hosted_registry_routecertificates= "{'certfile': '<path>/org-cert.pem', 'keyfile': '<path>/org-privkey.pem', 'cafile': '<path>/org-chain.pem'}"

----

[[advanced-install-configuring-registry-console]]
=== Configuring the Registry Console

If you are using a Cockpit registry console image other than the default or
require a specific version of the console, specify the desired registry within
the *_/etc/ansible/hosts_* file:

----
openshift_cockpit_deployer_prefix=<registry_name>/<namespace>/
openshift_cockpit_deployer_version=<cockpit_image_tag>
----

.Registry Variables
[options="header"]
|===

|Variable |Purpose
|`openshift_cockpit_deployer_prefix`
|Specify the URL and path to the directory where the image is located.

|`openshift_cockpit_deployer_version`
|Specify the Cockpit image version.
|===

For example: If your image is at
`registry.example.com/openshift3/registry-console` and you require version
3.9.3, enter:

----
openshift_cockpit_deployer_prefix='registry.example.com/openshift3/'
openshift_cockpit_deployer_version='3.9.3'
----


[[advanced-install-glusterfs-persistent-storage]]
=== Configuring {gluster} Persistent Storage

include::install_config/persistent_storage/topics/glusterfs_intro.adoc[]

Additional information and examples, including the ones below, can be found at
xref:../../install_config/persistent_storage/persistent_storage_glusterfs.adoc#install-config-persistent-storage-persistent-storage-glusterfs[Persistent Storage Using {gluster}].

[[advanced-install-containerized-glusterfs-persistent-storage]]
==== Configuring {gluster-native}

[IMPORTANT]
====
See
xref:../../install_config/install/prerequisites.adoc#prereq-glusterfs[{gluster-native} Considerations]
for specific host preparations and prerequisites.
====

include::install_config/persistent_storage/topics/glusterfs_example_basic.adoc[]

[[advanced-install-external-glusterfs-persistent-storage]]
==== Configuring {gluster-external}

include::install_config/persistent_storage/topics/glusterfs_example_basic_external.adoc[]

[[advanced-install-registry]]
=== Configuring an OpenShift Container Registry

An integrated
xref:../../architecture/infrastructure_components/image_registry.adoc#integrated-openshift-registry[OpenShift Container Registry]
can be deployed using the advanced installer.

[[advanced-install-registry-storage]]
==== Configuring Registry Storage

If no registry storage options are used, the default OpenShift Container
Registry is ephemeral and all data will be lost when the pod no longer exists.
There are several options for enabling registry storage when using the advanced
installer:

[discrete]
[[advanced-install-registry-storage-nfs-host-group]]
===== Option A: NFS Host Group

[NOTE]
====
The use of NFS for registry storage is not recommended in {product-title}.
====

When the following variables are set, an NFS volume is created during an
advanced install with the path *_<nfs_directory>/<volume_name>_* on the host
within the `[nfs]` host group. For example, the volume path using these options
would be *_/exports/registry_*:

----
[OSEv3:vars]

openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=10Gi
----

[discrete]
[[advanced-install-registry-storage-external-nfs]]
===== Option B: External NFS Host

[NOTE]
====
The use of NFS for registry storage is not recommended in {product-title}.
====

To use an external NFS volume, one must already exist with a path of
*_<nfs_directory>/<volume_name>_* on the storage host. The remote volume path
using the following options would be *_nfs.example.com:/exports/registry_*.

----
[OSEv3:vars]

openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_host=nfs.example.com
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=10Gi
----

[discrete]
[[advanced-install-cluster-registry-storage-upgrade-nfs]]
===== Upgrading or Installing {product-title} with NFS

The use of NFS for the core {product-title} components is not recommended, as NFS
(and the NFS Protocol) does not provide the proper consistency needed for the
applications that make up the {product-title} infrastructure.

As a result, the installer and update playbooks require an option to enable the use
of NFS with core infrastructure components.

----
# Enable unsupported configurations, things that will yield a partially
# functioning cluster but would not be supported for production use
#openshift_enable_unsupported_configurations=false
----

If you see the following messages when upgrading or installing an {product-title}
3.9.z cluster, then an additional step is required.

----
TASK [Run variable sanity checks] **********************************************
fatal: [host.example.com]: FAILED! => {"failed": true, "msg": "last_checked_host: host.example.com, last_checked_var: openshift_hosted_registry_storage_kind;nfs is an unsupported type for openshift_hosted_registry_storage_kind. openshift_enable_unsupported_configurations=True mustbe specified to continue with this configuration."}
----

In your Ansible inventory file, specify the following parameter:
----
[OSEv3:vars]
openshift_enable_unsupported_configurations=True
----

[discrete]
[[advanced-install-registry-storage-openstack]]
===== Option C: OpenStack Platform

An OpenStack storage configuration must already exist.

----
[OSEv3:vars]

openshift_hosted_registry_storage_kind=openstack
openshift_hosted_registry_storage_access_modes=['ReadWriteOnce']
openshift_hosted_registry_storage_openstack_filesystem=ext4
openshift_hosted_registry_storage_openstack_volumeID=3a650b4f-c8c5-4e0a-8ca5-eaee11f16c57
openshift_hosted_registry_storage_volume_size=10Gi
----

[discrete]
[[advanced-install-registry-storage-aws]]
===== Option D: AWS or Another S3 Storage Solution

The simple storage solution (S3) bucket must already exist.

----
[OSEv3:vars]

#openshift_hosted_registry_storage_kind=object
#openshift_hosted_registry_storage_provider=s3
#openshift_hosted_registry_storage_s3_accesskey=access_key_id
#openshift_hosted_registry_storage_s3_secretkey=secret_access_key
#openshift_hosted_registry_storage_s3_bucket=bucket_name
#openshift_hosted_registry_storage_s3_region=bucket_region
#openshift_hosted_registry_storage_s3_chunksize=26214400
#openshift_hosted_registry_storage_s3_rootdirectory=/registry
#openshift_hosted_registry_pullthrough=true
#openshift_hosted_registry_acceptschema2=true
#openshift_hosted_registry_enforcequota=true
----

If you are using a different S3 service, such as Minio or ExoScale, also add the
region endpoint parameter:

----
openshift_hosted_registry_storage_s3_regionendpoint=https://myendpoint.example.com/
----

[discrete]
[[advanced-install-registry-storage-glusterfs]]
===== Option E: {gluster-native}

Similar to
xref:advanced-install-containerized-glusterfs-persistent-storage[configuring {gluster-native}],
{gluster} can be configured to provide storage for an OpenShift Container
Registry during the initial installation of the cluster to offer redundant and
reliable storage for the registry.

[IMPORTANT]
====
See
xref:../../install_config/install/prerequisites.adoc#prereq-glusterfs[{gluster-native} Considerations]
for specific host preparations and prerequisites.
====

include::install_config/persistent_storage/topics/glusterfs_example_registry.adoc[]

[discrete]
[[advanced-install-registry-storage-gce]]
===== Option F: Google Cloud Storage (GCS) bucket on Google Compute Engine (GCE)

A GCS bucket must already exist.

----
[OSEv3:vars]

openshift_hosted_registry_storage_provider=gcs
openshift_hosted_registry_storage_gcs_bucket=bucket01
openshift_hosted_registry_storage_gcs_keyfile=test.key
openshift_hosted_registry_storage_gcs_rootdirectory=/registry
----

[[advanced-install-configuring-global-proxy]]
=== Configuring Global Proxy Options

If your hosts require use of a HTTP or HTTPS proxy in order to connect to
external hosts, there are many components that must be configured to use the
proxy, including masters, Docker, and builds. Node services only connect to the
master API requiring no external access and therefore do not need to be
configured to use a proxy.

In order to simplify this configuration, the following Ansible variables can be
specified at a cluster or host level to apply these settings uniformly across
your environment.

[NOTE]
====
See xref:../../install_config/build_defaults_overrides.adoc#install-config-build-defaults-overrides[Configuring
Global Build Defaults and Overrides] for more information on how the proxy
environment is defined for builds.
====

.Cluster Proxy Variables
[options="header"]
|===

|Variable |Purpose

|`*openshift_http_proxy*`
|This variable specifies the `*HTTP_PROXY*` environment variable for masters and
the Docker daemon.

|`*openshift_https_proxy*`
|This variable specifices the `*HTTPS_PROXY*` environment variable for masters
and the Docker daemon.

|`*openshift_no_proxy*`
|This variable is used to set the `*NO_PROXY*` environment variable for masters
and the Docker daemon. This value should be set to a comma separated list of
host names or wildcard host names that should not use the defined proxy. This
list will be augmented with the list of all defined {product-title} host names
by default.

|`*openshift_generate_no_proxy_hosts*`
|This boolean variable specifies whether or not the names of all defined
OpenShift hosts and `pass:[*.cluster.local]` should be automatically appended to
the `*NO_PROXY*` list. Defaults to *true*; set it to *false* to override this
option.

|`*openshift_builddefaults_http_proxy*`
|This variable defines the `*HTTP_PROXY*` environment variable inserted into
builds using the `*BuildDefaults*` admission controller. If
`*openshift_http_proxy*` is set, this variable will inherit that value; you only
need to set this if you want your builds to use a different value.

|`*openshift_builddefaults_https_proxy*`
|This variable defines the `*HTTPS_PROXY*` environment variable inserted into
builds using the `*BuildDefaults*` admission controller. If
`*openshift_https_proxy*` is set, this variable will inherit that value; you
only need to set this if you want your builds to use a different value.

|`*openshift_builddefaults_no_proxy*`
|This variable defines the `*NO_PROXY*` environment variable inserted into
builds using the `*BuildDefaults*` admission controller. If
`*openshift_no_proxy*` is set, this variable will inherit that value; you only
need to set this if you want your builds to use a different value.

|`*openshift_builddefaults_git_http_proxy*`
|This variable defines the HTTP proxy used by `git clone` operations during a
build, defined using the `*BuildDefaults*` admission controller. If
`*openshift_builddefaults_http_proxy*` is set, this variable will inherit that
value; you only need to set this if you want your `git clone` operations to use
a different value.

|`*openshift_builddefaults_git_https_proxy*`
|This variable defines the HTTPS proxy used by `git clone` operations during a
build, defined using the `*BuildDefaults*` admission controller. If
`*openshift_builddefaults_https_proxy*` is set, this variable will inherit that
value; you only need to set this if you want your `git clone` operations to use
a different value.
|===


ifdef::openshift-enterprise,openshift-origin[]
[[advanced-install-configuring-firewalls]]
=== Configuring the Firewall

[IMPORTANT]
====
If you are changing the default firewall, ensure that each host in your cluster
is using the same firewall type to prevent inconsistencies.
====

[NOTE]
====
While iptables is the default firewall, firewalld is recommended for new
installations.
====

{product-title} uses iptables as the default firewall, but you can configure
your cluster to use firewalld during the install process.

Because iptables is the default firewall, {product-title} is designed to have it
configured automatically. However, iptables rules can break {product-title} if
not configured correctly. The advantages of firewalld include allowing multiple
objects to safely share the firewall rules.

To use firewalld as the firewall for an {product-title} installation, add the
`os_firewall_use_firewalld` variable to the list of configuration variables in
the Ansible host file at install:

----
[OSEv3:vars]
os_firewall_use_firewalld=True
----
endif::[]

[[marking-masters-as-unschedulable-nodes]]
=== Configuring Schedulability on Masters

Any hosts you designate as masters during the installation process should also
be configured as nodes so that the masters are configured as part of the
xref:../../architecture/networking/network_plugins.adoc#openshift-sdn[OpenShift SDN]. You must do so by adding entries for these hosts to the `[nodes]` section:

----
[nodes]
master.example.com
----

In previous versions of {product-title}, master hosts were marked as
unschedulable nodes by default by the installer, meaning that new pods could not
be placed on the hosts. Starting with {product-title} 3.9, however, masters are
marked schedulable automatically during installation. This change is mainly so
that the web console, which used to run as part of the master itself, can
instead be run as a pod deployed to the master.

If you want to change the schedulability of a host post-installation, see
xref:../../admin_guide/manage_nodes.adoc#marking-nodes-as-unschedulable-or-schedulable[Marking Nodes as Unschedulable or Schedulable].

[[configuring-node-host-labels]]
=== Configuring Node Host Labels

You can assign
xref:../../architecture/core_concepts/pods_and_services.adoc#labels[labels] to
node hosts during the Ansible install by configuring the *_/etc/ansible/hosts_*
file. Labels are useful for determining the placement of pods onto nodes using
the
xref:../../admin_guide/scheduling/scheduler.adoc#configurable-predicates[scheduler].
Other than `region=infra` (referred to as _dedicated infrastructure nodes_ and
discussed further in xref:configuring-dedicated-infrastructure-nodes[Configuring
Dedicated Infrastructure Nodes]), the actual label names and values are
arbitrary and can be assigned however you see fit per your cluster's
requirements.

To assign labels to a node host during an Ansible install, use the
`openshift_node_labels` variable with the desired labels added to the desired
node host entry in the `[nodes]` section. In the following example, labels are
set for a region called `primary` and a zone called `east`:

----
[nodes]
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
----

Starting in {product-title} 3.9, masters are now marked as schedulable nodes by
default. As a result, the default node selector (defined in the master
configuration file's `projectConfig.defaultNodeSelector` field to determine
which node that projects will use by default when placing pods, and previously
left blank by default) is now set by default during cluster installations. It is
set to `node-role.kubernetes.io/compute=true` unless overridden using the
`osm_default_node_selector` Ansible variable.

In addition, whether `osm_default_node_selector` is set or not, the following
automatic labeling occurs for hosts defined in your inventory file during
installation:

- non-master, non-dedicated infrastructure nodes hosts (for example, the
`node1.example.com` host shown above) are labeled with
`node-role.kubernetes.io/compute=true`
- master nodes are are labeled `node-role.kubernetes.io/master=true`

This ensures that the default node selector has available nodes to choose from
when determining pod placement.

[IMPORTANT]
====
If you accept the default node selector of
`node-role.kubernetes.io/compute=true` during installation, ensure that you do
not only have dedicated infrastructure nodes as the non-master nodes defined in
your cluster. In that scenario, application pods would fail to deploy because no
nodes with the `node-role.kubernetes.io/compute=true` label would be available
to match the default node selector when scheduling pods for projects.
====

See
xref:../../admin_guide/managing_projects.adoc#setting-the-cluster-wide-default-node-selector[Setting the Cluster-wide Default Node Selector] for steps on adjusting this setting
post-installation if needed.

[[configuring-dedicated-infrastructure-nodes]]
==== Configuring Dedicated Infrastructure Nodes

It is recommended for production environments that you maintain dedicated
infrastructure nodes where the registry and router pods can run separately from
pods used for user applications.

The `openshift_router_selector` and `openshift_registry_selector` Ansible
settings determine the label selectors used when placing registry and router
pods. They are set to `region=infra` by default:

----
# default selectors for router and registry services
# openshift_router_selector='region=infra'
# openshift_registry_selector='region=infra'
----

The registry and router are only able to run on node hosts with the
`region=infra` label, which are then considered dedicated infrastructure nodes.
Ensure that at least one node host in your {product-title} environment has the
`region=infra` label. For example:

----
[nodes]
infra-node1.example.com openshift_node_labels="{'region': 'infra','zone': 'default'}"
----

[IMPORTANT]
====
If there is not a node in the `[nodes]` section that matches the selector
settings, the default router and registry will be deployed as failed with
`Pending` status.
====

If you do not intend to use {product-title} to manage the registry and router,
configure the following Ansible settings:

----
openshift_hosted_manage_registry=false
openshift_hosted_manage_router=false
----

If you are using an image registry other than the default `registry.access.redhat.com`,
you need to xref:advanced-install-configuring-registry-location[specify the desired registry]
in the  *_/etc/ansible/hosts_* file.

As described in xref:marking-masters-as-unschedulable-nodes[Configuring
Schedulability on Masters], master hosts are marked schedulable by default. If
you label a master host with `region=infra` and have no other dedicated
infrastructure nodes, the master hosts must also be marked as schedulable.
Otherwise, the registry and router pods cannot be placed anywhere:

----
[nodes]
master.example.com openshift_node_labels="{'region': 'infra','zone': 'default'}" openshift_schedulable=true
----

[[advanced-install-session-options]]
=== Configuring Session Options

xref:../../install_config/configuring_authentication.adoc#session-options[Session
options] in the OAuth configuration are configurable in the inventory file. By
default, Ansible populates a `*sessionSecretsFile*` with generated
authentication and encryption secrets so that sessions generated by one master
can be decoded by the others. The default location is
*_/etc/origin/master/session-secrets.yaml_*, and this file will only be
re-created if deleted on all masters.

You can set the session name and maximum number of seconds with
`*openshift_master_session_name*` and `*openshift_master_session_max_seconds*`:

----
openshift_master_session_name=ssn
openshift_master_session_max_seconds=3600
----

If provided, `*openshift_master_session_auth_secrets*` and
`*openshift_master_encryption_secrets*` must be equal length.

For `*openshift_master_session_auth_secrets*`, used to authenticate sessions
using HMAC, it is recommended to use secrets with 32 or 64 bytes:

----
openshift_master_session_auth_secrets=['DONT+USE+THIS+SECRET+b4NV+pmZNSO']
----

For `*openshift_master_encryption_secrets*`, used to encrypt sessions, secrets
must be 16, 24, or 32 characters long, to select AES-128, AES-192, or AES-256:

----
openshift_master_session_encryption_secrets=['DONT+USE+THIS+SECRET+b4NV+pmZNSO']
----

[[advanced-install-custom-certificates]]
=== Configuring Custom Certificates

xref:../../install_config/certificate_customization.adoc#install-config-certificate-customization[Custom serving
certificates] for the public host names of the {product-title} API and
xref:../../architecture/infrastructure_components/web_console.adoc#architecture-infrastructure-components-web-console[web console]
can be deployed during an advanced installation and are configurable in the
inventory file.

[NOTE]
====
Custom certificates should only be configured for the host name associated with
the `*publicMasterURL*` which can be set using
`*openshift_master_cluster_public_hostname*`. Using a custom serving certificate
for the host name associated with the `*masterURL*`
(*`openshift_master_cluster_hostname`*) will result in TLS errors as
infrastructure components will attempt to contact the master API using the
internal `*masterURL*` host.
====

Certificate and key file paths can be configured using the
`*openshift_master_named_certificates*` cluster variable:

----
openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key", "cafile": "/path/to/custom-ca1.crt"}]
----

File paths must be local to the system where Ansible will be run. Certificates
are copied to master hosts and are deployed within the
*_/etc/origin/master/named_certificates/_* directory.

Ansible detects a certificate's `Common Name` and `Subject Alternative Names`.
Detected names can be overridden by providing the `*"names"*` key when setting
`*openshift_master_named_certificates*`:

----
openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key", "names": ["public-master-host.com"], "cafile": "/path/to/custom-ca1.crt"}]
----

Certificates configured using `*openshift_master_named_certificates*` are cached
on masters, meaning that each additional Ansible run with a different set of
certificates results in all previously deployed certificates remaining in place
on master hosts and within the master configuration file.

If you would like `*openshift_master_named_certificates*` to be overwritten with
the provided value (or no value), specify the
`*openshift_master_overwrite_named_certificates*` cluster variable:

----
openshift_master_overwrite_named_certificates=true
----

For a more complete example, consider the following cluster variables in an
inventory file:

----
openshift_master_cluster_method=native
openshift_master_cluster_hostname=lb-internal.openshift.com
openshift_master_cluster_public_hostname=custom.openshift.com
----

To overwrite the certificates on a subsequent Ansible run, you could set the
following:

----
openshift_master_named_certificates=[{"certfile": "/root/STAR.openshift.com.crt", "keyfile": "/root/STAR.openshift.com.key", "names": ["custom.openshift.com"]}]
openshift_master_overwrite_named_certificates=true
----

[[advanced-install-config-certificate-validity]]
=== Configuring Certificate Validity

By default, the certificates used to govern the etcd, master, and kubelet expire
after two to five years. The validity (length in days until they expire) for the
auto-generated registry, CA, node, and master certificates can be configured
during installation using the following variables (default values shown):

----
[OSEv3:vars]

openshift_hosted_registry_cert_expire_days=730
openshift_ca_cert_expire_days=1825
openshift_node_cert_expire_days=730
openshift_master_cert_expire_days=730
etcd_ca_default_days=1825
----

These values are also used when
xref:../../install_config/redeploying_certificates.adoc#install-config-redeploying-certificates[redeploying certificates] via Ansible post-installation.

[[advanced-install-cluster-metrics]]
=== Configuring Cluster Metrics

Cluster metrics are not set to automatically deploy. Set the following
to enable cluster metrics when using the advanced installation method:

----
[OSEv3:vars]

openshift_metrics_install_metrics=true
----

The metrics public URL can be set during cluster
installation using the `openshift_metrics_hawkular_hostname` Ansible variable,
which defaults to:

`\https://hawkular-metrics.{{openshift_master_default_subdomain}}/hawkular/metrics`

If you alter this variable, ensure the host name is accessible via your router.

`openshift_metrics_hawkular_hostname=hawkular-metrics.{{openshift_master_default_subdomain}}`


[[advanced-install-cluster-metrics-storage]]
==== Configuring Metrics Storage

The `openshift_metrics_cassandra_storage_type` variable must be set in order to
use persistent storage for metrics. If
`openshift_metrics_cassandra_storage_type` is not set, then cluster metrics data
is stored in an `emptyDir` volume, which will be deleted when the Cassandra pod
terminates.

There are three options for enabling cluster metrics storage when using the
advanced install:

[discrete]
[[advanced-install-cluster-metrics-storage-dynamic]]
===== Option A: Dynamic

Use the following variable if your {product-title} environment supports
xref:../../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[dynamic volume provisioning] for your cloud provider:

----
[OSEv3:vars]

openshift_metrics_cassandra_storage_type=dynamic
----

[discrete]
[[advanced-install-cluster-metrics-storage-nfs-host-group]]
===== Option B: NFS Host Group

[IMPORTANT]
====
The use of NFS for metrics storage is not recommended in
{product-title}.
====

When the following variables are set, an NFS volume is created during an
advanced install with path *_<nfs_directory>/<volume_name>_* on the host within
the `[nfs]` host group. For example, the volume path using these options would
be *_/exports/metrics_*:

----
[OSEv3:vars]

openshift_metrics_storage_kind=nfs
openshift_metrics_storage_access_modes=['ReadWriteOnce']
openshift_metrics_storage_nfs_directory=/exports
openshift_metrics_storage_nfs_options='*(rw,root_squash)'
openshift_metrics_storage_volume_name=metrics
openshift_metrics_storage_volume_size=10Gi
----

[discrete]
[[advanced-install-cluster-metrics-storage-external-nfs]]
===== Option C: External NFS Host


[IMPORTANT]
====
The use of NFS for metrics storage is not recommended in
{product-title}.
====

To use an external NFS volume, one must already exist with a path of
*_<nfs_directory>/<volume_name>_* on the storage host.

----
[OSEv3:vars]

openshift_metrics_storage_kind=nfs
openshift_metrics_storage_access_modes=['ReadWriteOnce']
openshift_metrics_storage_host=nfs.example.com
openshift_metrics_storage_nfs_directory=/exports
openshift_metrics_storage_volume_name=metrics
openshift_metrics_storage_volume_size=10Gi
----

The remote volume path using the following options would be
*_nfs.example.com:/exports/metrics_*.

[discrete]
[[advanced-install-cluster-metrics-storage-upgrade-nfs]]
===== Upgrading or Installing {product-title} with NFS

The use of NFS for the core {product-title} components is not recommended, as NFS
(and the NFS Protocol) does not provide the proper consistency needed for the
applications that make up the {product-title} infrastructure.

As a result, the installer and update playbooks require an option to enable the use
of NFS with core infrastructure components.

----
# Enable unsupported configurations, things that will yield a partially
# functioning cluster but would not be supported for production use
#openshift_enable_unsupported_configurations=false
----

If you see the following messages when upgrading or installing an {product-title}
3.9.z cluster, then an additional step is required.

----
TASK [Run variable sanity checks] **********************************************
fatal: [host.example.com]: FAILED! => {"failed": true, "msg": "last_checked_host: host.example.com, last_checked_var: openshift_hosted_registry_storage_kind;nfs is an unsupported type for openshift_hosted_registry_storage_kind. openshift_enable_unsupported_configurations=True mustbe specified to continue with this configuration."}
----

In your Ansible inventory file, specify the following parameter:
----
[OSEv3:vars]
openshift_enable_unsupported_configurations=True
----

[[advanced-install-cluster-logging]]
=== Configuring Cluster Logging

Cluster logging is not set to automatically deploy by default. Set the
following to enable cluster logging when using the advanced installation method:

----
[OSEv3:vars]

openshift_logging_install_logging=true
----

[[advanced-installation-logging-storage]]
==== Configuring Logging Storage

The `openshift_logging_es_pvc_dynamic` variable must be set in order to use
persistent storage for logging. If `openshift_logging_es_pvc_dynamic` is
not set, then cluster logging data is stored in an `emptyDir` volume, which will
be deleted when the Elasticsearch pod terminates.

There are three options for enabling cluster logging storage when using the
advanced install:

[discrete]
[[advanced-installation-logging-storage-dynamic]]
===== Option A: Dynamic

Use the following variable if your {product-title} environment supports
xref:../../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[dynamic volume provisioning] for your cloud provider:

----
[OSEv3:vars]

openshift_logging_es_pvc_dynamic=true
----

[discrete]
[[advanced-installation-logging-storage-nfs-host-group]]
===== Option B: NFS Host Group

[IMPORTANT]
====
The use of NFS for logging storage is not recommended in
{product-title}.
====

When the following variables are set, an NFS volume is created during an
advanced install with path *_<nfs_directory>/<volume_name>_* on the host within
the `[nfs]` host group. For example, the volume path using these options would be
*_/exports/logging_*:

----
[OSEv3:vars]

openshift_logging_storage_kind=nfs
openshift_logging_storage_access_modes=['ReadWriteOnce']
openshift_logging_storage_nfs_directory=/exports
openshift_logging_storage_nfs_options='*(rw,root_squash)'
openshift_logging_storage_volume_name=logging
openshift_logging_storage_volume_size=10Gi
----

[discrete]
[[advanced-installation-logging-storage-external-nfs]]
===== Option C: External NFS Host

[IMPORTANT]
====
The use of NFS for logging storage is not recommended in
{product-title}.
====

To use an external NFS volume, one must already exist with a path of
*_<nfs_directory>/<volume_name>_* on the storage host.

----
[OSEv3:vars]

openshift_logging_storage_kind=nfs
openshift_logging_storage_access_modes=['ReadWriteOnce']
openshift_logging_storage_host=nfs.example.com
openshift_logging_storage_nfs_directory=/exports
openshift_logging_storage_volume_name=logging
openshift_logging_storage_volume_size=10Gi
----

The remote volume path using the following options would be
*_nfs.example.com:/exports/logging_*.

[discrete]
[[advanced-install-cluster-logging-storage-upgrade-nfs]]
===== Upgrading or Installing {product-title} with NFS

The use of NFS for the core {product-title} components is not recommended, as NFS
(and the NFS Protocol) does not provide the proper consistency needed for the
applications that make up the {product-title} infrastructure.

As a result, the installer and update playbooks require an option to enable the use
of NFS with core infrastructure components.

----
# Enable unsupported configurations, things that will yield a partially
# functioning cluster but would not be supported for production use
#openshift_enable_unsupported_configurations=false
----

If you see the following messages when upgrading or installing an {product-title}
3.9.z cluster, then an additional step is required.

----
TASK [Run variable sanity checks] **********************************************
fatal: [host.example.com]: FAILED! => {"failed": true, "msg": "last_checked_host: host.example.com, last_checked_var: openshift_hosted_registry_storage_kind;nfs is an unsupported type for openshift_hosted_registry_storage_kind. openshift_enable_unsupported_configurations=True mustbe specified to continue with this configuration."}
----

In your Ansible inventory file, specify the following parameter:
----
[OSEv3:vars]
openshift_enable_unsupported_configurations=True
----

[[enabling-service-catalog]]
=== Configuring the Service Catalog

The
xref:../../architecture/service_catalog/index.adoc#architecture-additional-concepts-service-catalog[service catalog] is enabled by default during installation. Enabling the service broker
allows service brokers to be registered with the catalog.

[NOTE]
====
To disable automatic deployment, set the following cluster variables in your
inventory file:

----
openshift_enable_service_catalog=false
ifdef::openshift-origin[]
openshift_service_catalog_image_prefix=openshift/origin-
openshift_service_catalog_image_version=latest
endif::[]
----
====

When the service catalog is enabled, the OpenShift Ansible broker and template service broker are both enabled
as well; see xref:configuring-openshift-ansible-broker[Configuring the OpenShift Ansible Broker] and xref:configuring-template-service-broker[Configuring the Template Service Broker] for more information.

[[configuring-openshift-ansible-broker]]
=== Configuring the OpenShift Ansible Broker

The
xref:../../architecture/service_catalog/ansible_service_broker.adoc#arch-ansible-service-broker[OpenShift Ansible broker] (OAB) is enabled by default during installation. However,
further configuration may be required for use.

[[configuring-oab-storage]]
==== Configuring Persistent Storage for the OpenShift Ansible Broker

The OAB deploys its own etcd instance separate from the etcd used by the rest of
the {product-title} cluster. The OAB's etcd instance requires separate storage
using persistent volumes (PVs) to function. If no PV is available, etcd will
wait until the PV can be satisfied. The OAB application will enter a `CrashLoop`
state until its etcd instance is available.

Some Ansible playbook bundles (APBs) also require a PV for their own usage in
order to deploy. For example, each of the database APBs have two plans: the
Development plan uses ephermal storage and does not require a PV, while the
Production plan is persisted and does require a PV.

[options="header"]
|===
|APB |PV Required?

|*postgresql-apb*
|Yes, but only for the Production plan

|*mysql-apb*
|Yes, but only for the Production plan

|*mariadb-apb*
|Yes, but only for the Production plan

|*mediawiki-apb*
|Yes

|===

To configure persistent storage for the OAB:

[NOTE]
====
The following example shows usage of an NFS host to provide the required PVs,
but
xref:../../install_config/persistent_storage/index.adoc#install-config-persistent-storage-index[other persistent storage providers] can be used instead.
====

. In your inventory file, add `nfs` to the `[OSEv3:children]` section to enable
the `[nfs]` group:
+
----
[OSEv3:children]
masters
nodes
nfs
----

. Add a `[nfs]` group section and add the host name for the system that will
be the NFS host:
+
----
[nfs]
master1.example.com
----

. Add the following in the `[OSEv3:vars]` section:
+
----
openshift_hosted_etcd_storage_kind=nfs
openshift_hosted_etcd_storage_nfs_options="*(rw,root_squash,sync,no_wdelay)"
openshift_hosted_etcd_storage_nfs_directory=/opt/osev3-etcd <1>
openshift_hosted_etcd_storage_volume_name=etcd-vol2 <1>
openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
openshift_hosted_etcd_storage_volume_size=1G
openshift_hosted_etcd_storage_labels={'storage': 'etcd'}

ifdef::openshift-origin[]
ansible_service_broker_image_prefix=registry.access.redhat.com/openshift3/ose-
ansible_service_broker_registry_url=registry.access.redhat.com
ansible_service_broker_registry_user=<user_name> <2>
ansible_service_broker_registry_password=<password> <2>
ansible_service_broker_registry_organization=<organization> <2>
endif::[]
----
<1> An NFS volume will be created with path `<nfs_directory>/<volume_name>` on the
host within the `[nfs]` group. For example, the volume path using these options
would be *_/opt/osev3-etcd/etcd-vol2_*.
ifdef::openshift-origin[]
<2> Only required if `ansible_service_broker_registry_url` is set to a registry that
requires authentication for pulling APBs.
endif::[]
+
These settings create a persistent volume that is attached to the OAB's etcd
instance during cluster installation.

[[configuring-oab-local-apb-devel]]
==== Configuring the OpenShift Ansible Broker for Local APB Development

In order to do xref:../../apb_devel/index.adoc#apb-devel-intro[APB development]
with the OpenShift Container Registry in conjunction with the OAB, a whitelist
of images the OAB can access must be defined. If a whitelist is not defined, the
broker will ignore APBs and users will not see any APBs available.

By default, the whitelist is empty so that a user cannot add APB images to the
broker without a cluster administrator configuring the broker. To whitelist all
images that end in `-apb`:

. In your inventory file, add the following to the `[OSEv3:vars]` section:
+
----
ansible_service_broker_local_registry_whitelist=['.*-apb$']
----

[[configuring-template-service-broker]]
=== Configuring the Template Service Broker

The
xref:../../architecture/service_catalog/template_service_broker.adoc#arch-template-service-broke[template service broker] (TSB) is enabled by default during installation.

To configure the TSB, one or more projects must be defined as the broker's
source namespace(s) for loading templates and image streams into the service
catalog. Set the desired projects by modifying the following in your inventory
file's `[OSEv3:vars]` section:

----
openshift_template_service_broker_namespaces=['openshift','myproject']
----

By default, the TSB will use the nodeselector `{"region": "infra"}` for deploying its pods.
You can modify this by setting the desired nodeselector in your inventory file's
`[OSEv3:vars]` section:

----
template_service_broker_selector={"region": "infra"}
----

[[configuring-web-console-customization]]
=== Configuring Web Console Customization

The following Ansible variables set master configuration options for customizing
the web console. See
xref:../../install_config/web_console_customization.adoc#install-config-web-console-customization[Customizing the Web Console] for more details on these customization options.

.Web Console Customization Variables
[options="header"]
|===

|Variable |Purpose

|`openshift_web_console_install`
|Determines whether to install the web console. Can be set to `true` or `false`. Defaults to `true`.

|`openshift_web_console_prefix`
|The prefix for the component images. For example, with
ifdef::openshift-origin[]
`openshift/origin-web-console:v3.9`, set prefix `openshift/origin-`.
endif::[]
ifdef::openshift-enterprise[]
`openshift3/ose-web-console:v3.9`, set prefix `openshift3/ose-`.
endif::[]

|`openshift_web_console_version`
|The version for the component images. For example, with
ifdef::openshift-origin[]
`openshift/origin-web-console:v3.9`, set version as `v3.9`.
endif::[]
ifdef::openshift-enterprise[]
`openshift3/ose-web-console:v3.9`, set prefix `openshift3/ose-`.
`openshift3/ose-web-console:v3.9.11`, set version as `v3.9.11`, or to
always get the latest 3.9 image, set `v3.9`.
endif::[]

|`openshift_master_logout_url`
|Sets `clusterInfo.logoutPublicURL` in the web console configuration. See xref:../../install_config/web_console_customization.adoc#changing-the-logout-url[Changing the Logout URL] for details. Example value: `\https://example.com/logout`

|`openshift_web_console_extension_script_urls`
|Sets `extensions.scriptURLs` in the web console configuration. See xref:../../install_config/web_console_customization.adoc#loading-custom-scripts-and-stylesheets[Loading Extension Scripts and Stylesheets] for details. Example value: `['https://example.com/scripts/menu-customization.js','https://example.com/scripts/nav-customization.js']`

|`openshift_web_console_extension_stylesheet_urls`
|Sets `extensions.stylesheetURLs` in the web console configuration. See xref:../../install_config/web_console_customization.adoc#loading-custom-scripts-and-stylesheets[Loading Extension Scripts and Stylesheets] for details. Example value: `['https://example.com/styles/logo.css','https://example.com/styles/custom-styles.css']`

|`openshift_master_oauth_template`
|Sets the OAuth template in the master configuration. See xref:../../install_config/web_console_customization.adoc#customizing-the-login-page[Customizing the Login Page] for details. Example value: `['/path/to/login-template.html']`

|`openshift_master_metrics_public_url`
|Sets `metricsPublicURL` in the master configuration. See xref:../../install_config/cluster_metrics.adoc#install-setting-the-metrics-public-url[Setting the Metrics Public URL] for details. Example value: `\https://hawkular-metrics.example.com/hawkular/metrics`

|`openshift_master_logging_public_url`
|Sets `loggingPublicURL` in the master configuration. See xref:../../install_config/aggregate_logging.adoc#aggregate-logging-kibana[Kibana] for details. Example value: `\https://kibana.example.com`

|`openshift_web_console_inactivity_timeout_minutes`
|Configurate the web console to log the user out automatically after a period of inactivity. Must be a whole number greater than or equal to 5, or 0 to disable the feature. Defaults to 0 (disabled).

|`openshift_web_console_cluster_resource_overrides_enabled`
|Boolean value indicating if the cluster is configured for overcommit. When `true`, the web console will hide fields for CPU request, CPU limit, and memory request when editing resource limits since these values should be set by the cluster resource override configuration.

|===

[[adv-install-example-inventory-files]]
== Example Inventory Files

[[single-master]]
=== Single Master Examples

You can configure an environment with a single master and multiple nodes, and
either a single or multiple number of external etcd hosts.

[NOTE]
====
Moving from a single master cluster to multiple masters after installation is
not supported.
====

[discrete]
[[single-master-multi-node-ai]]
==== Single Master, Single etcd, and Multiple Nodes

The following table describes an example environment for a single
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[master]
(with a single *etcd* on the same host), two
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
for hosting user applications, and two nodes with the `region=infra` label for hosting
xref:configuring-dedicated-infrastructure-nodes[dedicated infrastructure]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master.example.com*
|Master, etcd, and node

|*node1.example.com*
.2+.^|Node

|*node2.example.com*

|*infra-node1.example.com*
.2+.^|Node (with `region=infra` label)

|*infra-node2.example.com*
|===

You can see these example hosts present in the *[masters]*, *[etcd]*, and
*[nodes]* sections of the following example inventory file:

.Single Master, Single etcd, and Multiple Nodes Inventory File
----
# Create an OSEv3 group that contains the masters, nodes, and etcd groups
[OSEv3:children]
masters
nodes
etcd

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=root

# If ansible_ssh_user is not root, ansible_become must be set to true
#ansible_become=true

ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# host group for masters
[masters]
master.example.com

# host group for etcd
[etcd]
master.example.com

# host group for nodes, includes region info
[nodes]
master.example.com
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
----

// tag::exampleinventorynodelabel[]
[IMPORTANT]
====
See xref:configuring-node-host-labels[Configuring Node Host Labels] to ensure
you understand the default node selector requirements and node label
considerations beginning in {product-title} 3.9.
====
// end::exampleinventorynodelabel[]

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[discrete]
[[single-master-multi-etcd-multi-node-ai]]
==== Single Master, Multiple etcd, and Multiple Nodes

The following table describes an example environment for a single
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[master],
three
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[etcd]
hosts, two
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
for hosting user applications, and two nodes with the `region=infra` label for hosting
xref:configuring-dedicated-infrastructure-nodes[dedicated infrastructure]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master.example.com*
|Master and node

|*etcd1.example.com*
.3+.^|etcd

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*

|*infra-node1.example.com*
.2+.^|Node (with `region=infra` label)

|*infra-node2.example.com*
|===

You can see these example hosts present in the *[masters]*, *[nodes]*, and
*[etcd]* sections of the following example inventory file:

.Single Master, Multiple etcd, and Multiple Nodes Inventory File
----
# Create an OSEv3 group that contains the masters, nodes, and etcd groups
[OSEv3:children]
masters
nodes
etcd

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# host group for masters
[masters]
master.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# host group for nodes, includes region info
[nodes]
master.example.com
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
----

include::install_config/install/advanced_install.adoc[tag=exampleinventorynodelabel]

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[[multiple-masters]]
=== Multiple Masters Examples

You can configure an environment with multiple masters, multiple etcd hosts,
and multiple nodes. Configuring
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#high-availability-masters[multiple
masters for high availability] (HA) ensures that the cluster has no single point
of failure.

[NOTE]
====
Moving from a single master cluster to multiple masters after installation is
not supported.
====

When configuring multiple masters, the advanced installation supports the
`native` high availability (HA) method. This method leverages the native HA
master capabilities built into {product-title} and can be combined with any load
balancing solution.

If a host is defined in the *[lb]* section of the inventory file, Ansible
installs and configures HAProxy automatically as the load balancing solution. If
no host is defined, it is assumed you have pre-configured an external load
balancing solution of your choice to balance the master API (port 8443) on all
master hosts.

[NOTE]
====
This HAProxy load balancer is intended to demonstrate the API server's HA mode and is not recommended for production environments. If you are deploying to a
cloud provider, Red Hat recommends deploying a cloud-native TCP-based load
balancer or take other steps to provide a highly available load balancer.
====

For an external load balancing solution, you must have:

* A pre-created load balancer virtual IP (VIP) configured for SSL passthrough.
* A VIP listening on the port specified by the xref:advanced-master-ports[`openshift_master_api_port`]
value (8443 by default) and proxying back to all master hosts on that port.
* A domain name for VIP registered in DNS.
** The domain name will become the value of both
`openshift_master_cluster_public_hostname` and
`openshift_master_cluster_hostname` in the {product-title} installer.

See
link:https://github.com/redhat-cop/openshift-playbooks/blob/master/playbooks/installation/load_balancing.adoc[the External Load Balancer Integrations example in Github] for more information. For more on the high
availability master architecture, see
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[Kubernetes Infrastructure].

[NOTE]
====
The advanced installation method does not currently support multiple HAProxy
load balancers in an active-passive setup. See the
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Load_Balancer_Administration/ch-lvs-overview-VSA.html[Load Balancer Administration documentation] for post-installation amendments.
====

To configure multiple masters, refer to
xref:multi-masters-using-native-ha-ai[Multiple Masters with Multiple etcd]

[discrete]
[[multi-masters-using-native-ha-ai]]
==== Multiple Masters Using Native HA with External Clustered etcd

The following describes an example environment for three
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[masters]
using the `native` HA method:, one HAProxy load balancer, three
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[*etcd*]
hosts, two
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
for hosting user applications, and two nodes with the `region=infra` label for hosting
xref:configuring-dedicated-infrastructure-nodes[dedicated infrastructure]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master1.example.com*
.3+.^|Master (clustered using native HA) and node

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|HAProxy to load balance API master endpoints

|*etcd1.example.com*
.3+.^|*etcd*

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*

|*infra-node1.example.com*
.2+.^|Node (with `region=infra` label)

|*infra-node2.example.com*
|===

You can see these example hosts present in the *[masters]*, *[etcd]*, *[lb]*,
and *[nodes]* sections of the following example inventory file:

.Multiple Masters Using HAProxy Inventory File
----
# Create an OSEv3 group that contains the master, nodes, etcd, and lb groups.
# The lb group lets Ansible configure HAProxy as the load balancing solution.
# Comment lb out if your load balancer is pre-configured.
[OSEv3:children]
masters
nodes
etcd
lb

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

# Uncomment the following to enable htpasswd authentication; defaults to
# DenyAllPasswordIdentityProvider.
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# Native high availbility cluster method with optional load balancer.
# If no lb group is defined installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=openshift-internal.example.com
openshift_master_cluster_public_hostname=openshift-cluster.example.com

# apply updated node defaults
openshift_node_kubelet_args={'pods-per-core': ['10'], 'max-pods': ['250'], 'image-gc-high-threshold': ['90'], 'image-gc-low-threshold': ['80']}

# enable ntp on masters to ensure proper failover
openshift_clock_enabled=true

# host group for masters
[masters]
master1.example.com
master2.example.com
master3.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# Specify load balancer host
[lb]
lb.example.com

# host group for nodes, includes region info
[nodes]
master[1:3].example.com
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
----

include::install_config/install/advanced_install.adoc[tag=exampleinventorynodelabel]

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[discrete]
[[multi-masters-single-etcd-using-native-ha]]
==== Multiple Masters Using Native HA with Co-located Clustered etcd

The following describes an example environment for three
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[masters]
using the `native` HA method (with
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[*etcd*]
on each host), one HAProxy load balancer, two
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
for hosting user applications, and two nodes with the `region=infra` label for hosting
xref:configuring-dedicated-infrastructure-nodes[dedicated infrastructure]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master1.example.com*
.3+.^|Master (clustered using native HA) and node with etcd on each host

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|HAProxy to load balance API master endpoints

|*node1.example.com*
.2+.^|Node

|*node2.example.com*

|*infra-node1.example.com*
.2+.^|Node (with `region=infra` label)

|*infra-node2.example.com*
|===

You can see these example hosts present in the *[masters]*, *[etcd]*, *[lb]*,
and *[nodes]* sections of the following example inventory file:

----
# Create an OSEv3 group that contains the master, nodes, etcd, and lb groups.
# The lb group lets Ansible configure HAProxy as the load balancing solution.
# Comment lb out if your load balancer is pre-configured.
[OSEv3:children]
masters
nodes
etcd
lb

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

# Uncomment the following to enable htpasswd authentication; defaults to
# DenyAllPasswordIdentityProvider.
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# Native high availability cluster method with optional load balancer.
# If no lb group is defined installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=openshift-internal.example.com
openshift_master_cluster_public_hostname=openshift-cluster.example.com

# host group for masters
[masters]
master1.example.com
master2.example.com
master3.example.com

# host group for etcd
[etcd]
master1.example.com
master2.example.com
master3.example.com

# Specify load balancer host
[lb]
lb.example.com

# host group for nodes, includes region info
[nodes]
master[1:3].example.com
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
----

include::install_config/install/advanced_install.adoc[tag=exampleinventorynodelabel]

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[[running-the-advanced-installation]]
== Running the Advanced Installation

After you have xref:configuring-ansible[configured Ansible] by defining an
inventory file in *_/etc/ansible/hosts_*, you run the advanced installation
playbook via Ansible.

The installer uses modularized playbooks allowing administrators to install
specific components as needed. By breaking up the roles and playbooks, there is
better targeting of ad hoc administration tasks. This results in an increased
level of control during installations and results in time savings.

[NOTE]
====
The playbooks and their ordering are detailed below in
xref:running-the-advanced-installation-individual-components[Running Individual Component Playbooks].
====

[[running-the-advanced-installation-rpm]]
=== Running the RPM-based Installer

The RPM-based installer uses Ansible installed via RPM packages to run playbooks
and configuration files available on the local host.

[IMPORTANT]
====
Do not run OpenShift Ansible playbooks under `nohup`. Using `nohup` with the
playbooks causes file descriptors to be created and not closed. Therefore, the
system can run out of files to open and the playbook will fail.
====

To run the RPM-based installer:

////
Need to fix the following enterprise-3.9 branch incompleted step, omitting temporarily:

// tag::BZ1466783-workaround-install[]
If you are using a proxy, you must add the IP address of the etcd endpoints to
the `openshift_no_proxy` cluster variable in your inventory file.

[NOTE]
====
If you are not using a proxy, you can skip this step.
====

In {product-title}:
////

. Run the *_prequisites.yml_* playbook. This must be run only once before
deploying a new cluster. Use the following command, specifying `-i` if your
inventory file located somewhere other than *_/etc/ansible/hosts_*:
+
----
ifdef::openshift-enterprise[]
# ansible-playbook [-i /path/to/inventory] \
    /usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook [-i /path/to/inventory] \
    ~/openshift-ansible/playbooks/prerequisites.yml
endif::[]
----

. Run the *_deploy_cluster.yml_* playbook to initiate the cluster installation:
+
----
ifdef::openshift-enterprise[]
# ansible-playbook [-i /path/to/inventory] \
    /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook [-i /path/to/inventory] \
    ~/openshift-ansible/playbooks/deploy_cluster.yml
endif::[]
----

If for any reason the installation fails, before re-running the installer, see
xref:installer-known-issues[Known Issues] to check for any specific
instructions or workarounds.

[[running-the-advanced-installation-containerized]]
=== Running the Containerized Installer

The
ifdef::openshift-enterprise[]
*openshift3/ose-ansible*
endif::[]
ifdef::openshift-origin[]
*openshift/origin-ansible*
endif::[]
image is a containerized version of the {product-title} installer.
This installer image provides the same functionality as the RPM-based
installer, but it runs in a containerized environment that provides all
of its dependencies rather than being installed directly on the host.
The only requirement to use it is the ability to run a container.

[[running-the-advanced-installation-system-container]]
==== Running the Installer as a System Container

The installer image can be used as a
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/managing_containers/running_system_containers[system container].
System containers are stored and run outside of the traditional *docker* service.
This enables running the installer image from one of the target hosts without
concern for the install restarting *docker* on the host.

To use the Atomic CLI to run the installer as a run-once system container, perform the following steps as the *root* user:

. Run the *_prerequisites.yml_* playbook:
+
----
# atomic install --system \
    --storage=ostree \
    --set INVENTORY_FILE=/path/to/inventory \ <1>
    --set PLAYBOOK_FILE=/usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml \
    --set OPTS="-v" \
ifdef::openshift-enterprise[]
    registry.access.redhat.com/openshift3/ose-ansible:v3.9
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.9
endif::[]
----
<1> Specify the location on the local host for your inventory file.
+
This command runs a set of prerequiste tasks by using the inventory file
specified and the `root` user's SSH configuration.

. Run the *_deploy_cluster.yml_* playbook:
+
----
# atomic install --system \
    --storage=ostree \
    --set INVENTORY_FILE=/path/to/inventory \ <1>
    --set PLAYBOOK_FILE=/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml \
    --set OPTS="-v" \
ifdef::openshift-enterprise[]
    registry.access.redhat.com/openshift3/ose-ansible:v3.9
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.9
endif::[]
----
<1> Specify the location on the local host for your inventory file.
+
This command initiates the cluster installation by using the inventory file
specified and the `root` user's SSH configuration. It logs the output on the
terminal and also saves it in the *_/var/log/ansible.log_* file. The first time
this command is run, the image is imported into
link:https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/content_management_guide/managing_ostree_content[OSTree]
storage (system containers use this rather than *docker* daemon storage). On
subsequent runs, it reuses the stored image.
+
If for any reason the installation fails, before re-running the installer, see
xref:installer-known-issues[Known Issues] to check for any specific instructions
or workarounds.

[[running-the-advanced-installation-system-container-other-playbooks]]
==== Running Other Playbooks

You can use the `PLAYBOOK_FILE` environment variable to specify other playbooks
you want to run by using the containerized installer. The default value of the `PLAYBOOK_FILE` is
*_/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml_*, which is the
main cluster installation playbook, but you can set it to the path of another
playbook inside the container.

For example, to run the
xref:configuring-cluster-pre-install-checks[pre-install checks] playbook before
installation, use the following command:

----
# atomic install --system \
    --storage=ostree \
    --set INVENTORY_FILE=/path/to/inventory \
    --set PLAYBOOK_FILE=/usr/share/ansible/openshift-ansible/playbooks/openshift-checks/pre-install.yml \ <1>
    --set OPTS="-v" \ <2>
ifdef::openshift-enterprise[]
    registry.access.redhat.com/openshift3/ose-ansible:v3.9
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.9
endif::[]
----
<1> Set `PLAYBOOK_FILE` to the full path of the playbook starting at the
*_playbooks/_* directory. Playbooks are located in the same locations as with
the RPM-based installer.
<2> Set `OPTS` to add command line options to `ansible-playbook`.

[[running-the-advanced-installation-docker]]
==== Running the Installer as a Docker Container

The installer image can also run as a *docker* container anywhere that *docker* can run.

[WARNING]
====
This method must not be used to run the installer on one of the hosts being configured,
as the install may restart *docker* on the host, disrupting the installer container execution.
====

[NOTE]
====
Although this method and the system container method above use the same image, they
run with different entry points and contexts, so runtime parameters are not the same.
====

At a minimum, when running the installer as a *docker* container you must provide:

* SSH key(s), so that Ansible can reach your hosts.
* An Ansible inventory file.
* The location of the Ansible playbook to run against that inventory.

Here is an example of how to run an install via `docker`, which must be run by a
non-*root* user with access to `docker`:

. First, run the *_prerequisites.yml_* playbook:
+
----
$ docker run -t -u `id -u` \ <1>
    -v $HOME/.ssh/id_rsa:/opt/app-root/src/.ssh/id_rsa:Z \ <2>
    -v $HOME/ansible/hosts:/tmp/inventory:Z \ <3>
    -e INVENTORY_FILE=/tmp/inventory \ <3>
    -e PLAYBOOK_FILE=playbooks/prerequisites.yml \ <4>
    -e OPTS="-v" \ <5>
ifdef::openshift-enterprise[]
    registry.access.redhat.com/openshift3/ose-ansible:v3.9
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.9
endif::[]
----
<1> `-u `id -u`` makes the container run with the same UID as the current
user, which allows that user to use the SSH key inside the container (SSH
private keys are expected to be readable only by their owner).
<2> `-v $HOME/.ssh/id_rsa:/opt/app-root/src/.ssh/id_rsa:Z` mounts your SSH key
(`$HOME/.ssh/id_rsa`) under the container user's `$HOME/.ssh`
(*_/opt/app-root/src_* is the `$HOME` of the user in the container). If you
mount the SSH key into a non-standard location you can add an environment
variable with `-e ANSIBLE_PRIVATE_KEY_FILE=/the/mount/point` or set
`ansible_ssh_private_key_file=/the/mount/point` as a variable in the inventory
to point Ansible at it. Note that the SSH key is mounted with the `:Z` flag.
This is required so that the container can read the SSH key under its restricted
SELinux context. This also means that your original SSH key file will be
re-labeled to something like `system_u:object_r:container_file_t:s0:c113,c247`.
For more details about `:Z`, check the `docker-run(1)` man page. Keep this
in mind when providing these volume mount specifications because this might have
unexpected consequences: for example, if you mount (and therefore re-label) your
whole `$HOME/.ssh` directory it will block the host's *sshd* from accessing your
public keys to login. For this reason you may want to use a separate copy of the
SSH key (or directory), so that the original file labels remain untouched.
<3> `-v $HOME/ansible/hosts:/tmp/inventory:Z` and `-e INVENTORY_FILE=/tmp/inventory`
mount a static Ansible inventory file into the container as *_/tmp/inventory_*
and set the corresponding environment variable to point at it. As with the SSH
key, the inventory file SELinux labels may need to be relabeled by using the
`:Z` flag to allow reading in the container, depending on the existing label
(for files in a user `$HOME` directory this is likely to be needed). So again
you may prefer to copy the inventory to a dedicated location before mounting it.
The inventory file can also be downloaded from a web server if you specify the
`INVENTORY_URL` environment variable, or generated dynamically using
`DYNAMIC_SCRIPT_URL` to specify an executable script that provides a dynamic
inventory.
<4> `-e PLAYBOOK_FILE=playbooks/prerequisites.yml` specifies the playbook to run (in
this example, the prereqsuites playbook) as a relative path from the top level
directory of *openshift-ansible* content. The full path from the RPM can also be
used, as well as the path to any other playbook file in the container.
<5> `-e OPTS="-v"` supplies arbitrary command line options (in this case,
`-v` to increase verbosity) to the `ansible-playbook` command that runs
inside the container.

. Next, run the *_deploy_cluster.yml_* playbook to initiate the cluster
installation:
+
----
$ docker run -t -u `id -u` \
    -v $HOME/.ssh/id_rsa:/opt/app-root/src/.ssh/id_rsa:Z \
    -v $HOME/ansible/hosts:/tmp/inventory:Z \
    -e INVENTORY_FILE=/tmp/inventory \
    -e PLAYBOOK_FILE=playbooks/deploy_cluster.yml \
    -e OPTS="-v" \
ifdef::openshift-enterprise[]
    registry.access.redhat.com/openshift3/ose-ansible:v3.9
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.9
endif::[]
----

[[running-the-advanced-installation-individual-components]]
=== Running Individual Component Playbooks

The main installation playbook *_{pb-prefix}playbooks/deploy_cluster.yml_* runs a
set of individual component playbooks in a specific order, and the installer
reports back at the end what phases you have gone through. If the installation
fails during a phase, you are notified on the screen along with the errors from
the Ansible run.

After you resolve the issue, rather than run the entire installation over again,
you can pick up from the failed phase. You must then run each of the remaining
playbooks in order:

----
# ansible-playbook [-i /path/to/inventory] <playbook_file_location>
----

The following table is sorted in order of when each individual component
playbook is run:

.Individual Component Playbook Run Order
[options="header",cols="1,3"]
|===
|Playbook Name |File Location

|Health Check
|*_{pb-prefix}playbooks/openshift-checks/pre-install.yml_*

|etcd Install
|*_{pb-prefix}playbooks/openshift-etcd/config.yml_*

|NFS Install
|*_{pb-prefix}playbooks/openshift-nfs/config.yml_*

|Load Balancer Install
|*_{pb-prefix}playbooks/openshift-loadbalancer/config.yml_*

|Master Install
|*_{pb-prefix}playbooks/openshift-master/config.yml_*

|Master Additional Install
|*_{pb-prefix}playbooks/openshift-master/additional_config.yml_*

|Node Install
|*_{pb-prefix}playbooks/openshift-node/config.yml_*

|GlusterFS Install
|*_{pb-prefix}playbooks/openshift-glusterfs/config.yml_*

|Hosted Install
|*_{pb-prefix}playbooks/openshift-hosted/config.yml_*

|Web Console Install
|*_{pb-prefix}playbooks/openshift-web-console/config.yml_*

|Metrics Install
|*_{pb-prefix}playbooks/openshift-metrics/config.yml_*

|Logging Install
|*_{pb-prefix}playbooks/openshift-logging/config.yml_*

|Prometheus Install
|*_{pb-prefix}playbooks/openshift-prometheus/config.yml_*

|Service Catalog Install
|*_{pb-prefix}playbooks/openshift-service-catalog/config.yml_*

|Management Install
|*_{pb-prefix}playbooks/openshift-management/config.yml_*
|===

[[advanced-verifying-the-installation]]
== Verifying the Installation

After the installation completes:

// tag::verifying-the-installation[]
. Verify that the master is started and nodes
are registered and reporting in *Ready* status. _On the master host_, run the
following as root:
+
----
# oc get nodes
NAME                   STATUS    ROLES     AGE       VERSION
master.example.com     Ready     master    7h        v1.9.1+a0ce1bc657
node1.example.com      Ready     compute   7h        v1.9.1+a0ce1bc657
node2.example.com      Ready     compute   7h        v1.9.1+a0ce1bc657
----

. To verify that the web console is installed correctly, use the master host name
and the web console port number to access the web console with a web browser.
+
For example, for a master host with a host name of `master.openshift.com` and
using the default port of `8443`, the web console would be found at `\https://master.openshift.com:8443/console`.

// end::verifying-the-installation[]

[discrete]
[[verifying-multiple-etcd-hosts]]
==== Verifying Multiple etcd Hosts

If you installed multiple etcd hosts:

. First, verify that the *etcd* package, which provides the `etcdctl`
command, is installed:
+
----
# yum install etcd
----

. On a master host, verify the etcd cluster health, substituting for the FQDNs
of your etcd hosts in the following:
+
----
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/origin/master/master.etcd-ca.crt \
    --cert-file=/etc/origin/master/master.etcd-client.crt \
    --key-file=/etc/origin/master/master.etcd-client.key cluster-health
----

. Also verify the member list is correct:
+
----
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/origin/master/master.etcd-ca.crt \
    --cert-file=/etc/origin/master/master.etcd-client.crt \
    --key-file=/etc/origin/master/master.etcd-client.key member list
----

[discrete]
[[verifying-multiple-masters-haproxy]]
==== Verifying Multiple Masters Using HAProxy

If you installed multiple masters using HAProxy as a load balancer, browse to
the following URL according to your *[lb]* section definition and check
HAProxy's status:

----
http://<lb_hostname>:9000
----

You can verify your installation by consulting the
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Load_Balancer_Administration/ch-haproxy-setup-VSA.html[HAProxy
Configuration documentation].

[[uninstalling-advanced]]
== Uninstalling {product-title}

You can uninstall {product-title} hosts in your cluster by running the
*_uninstall.yml_* playbook. This playbook deletes {product-title} content
installed by Ansible, including:

- Configuration
- Containers
- Default templates and image streams
- Images
- RPM packages

The playbook will delete content for any hosts defined in the inventory file
that you specify when running the playbook. If you want to uninstall
{product-title} across all hosts in your cluster, run the playbook using the
inventory file you used when installing {product-title} initially or ran most
recently:

----
ifdef::openshift-enterprise[]
# ansible-playbook [-i /path/to/file] \
    /usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook [-i /path/to/file] \
    ~/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
----

[[uninstalling-nodes-advanced]]
=== Uninstalling Nodes

You can also uninstall node components from specific hosts using the
*_uninstall.yml_* playbook while leaving the remaining hosts and cluster alone:

[WARNING]
====
This method should only be used when attempting to uninstall specific node hosts
and not for specific masters or etcd hosts, which would require further
configuration changes within the cluster.
====

. First follow the steps in
xref:../../admin_guide/manage_nodes.adoc#deleting-nodes[Deleting Nodes] to
remove the node object from the cluster, then continue with the remaining steps
in this procedure.

. Create a different inventory file that only references those hosts. For
example, to only delete content from one node:
+
----
[OSEv3:children]
nodes <1>

[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

[nodes]
node3.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}" <2>
----
<1> Only include the sections that pertain to the hosts you are interested in
uninstalling.
<2> Only include hosts that you want to uninstall.

. Specify that new inventory file using the `-i` option when running the
*_uninstall.yml_* playbook:
+
----
ifdef::openshift-enterprise[]
# ansible-playbook -i /path/to/new/file \
    /usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook -i /path/to/new/file \
    ~/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
----

When the playbook completes, all {product-title} content should be removed from
any specified hosts.

[[installer-known-issues]]
== Known Issues

- On failover in multiple master clusters, it is possible for the controller
manager to overcorrect, which causes the system to run more pods than what was
intended. However, this is a transient event and the system does correct itself
over time. See https://github.com/kubernetes/kubernetes/issues/10030 for
details.

- On failure of the Ansible installer, you must start from a clean operating
system installation. If you are using virtual machines, start from a fresh
image. If you are using bare metal machines, see
xref:uninstalling-advanced[Uninstalling {product-title}] for instructions.

include::release_notes/ocp_3_9_release_notes.adoc[tag=BZ1558672]

== What's Next?

Now that you have a working {product-title} instance, you can:

- Deploy an xref:../../install_config/registry/index.adoc#install-config-registry-overview[integrated Docker registry].
- Deploy a xref:../../install_config/router/index.adoc#install-config-router-overview[router].
ifdef::openshift-origin[]
- xref:../../install_config/imagestreams_templates.adoc#install-config-imagestreams-templates[Populate your {product-title} installation]
with a useful set of Red Hat-provided image streams and templates.
endif::[]
