[[install-config-install-advanced-install]]
= Advanced Installation
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
A reference configuration implemented using
link:http://docs.ansible.com/ansible/[Ansible] playbooks is available as the _advanced
installation_ method for installing a {product-title} cluster. Familiarity with Ansible is
assumed, however you can use this configuration as a reference to create your
own implementation using the configuration management tool of your choosing.

[IMPORTANT]
====
While RHEL Atomic Host is supported for running containerized {product-title}
services, the advanced installation method utilizes Ansible, which is not
available in RHEL Atomic Host, and must therefore be run from
ifdef::openshift-enterprise[]
a RHEL 7 system.
endif::[]
ifdef::openshift-origin[]
a supported version of Fedora, CentOS, or RHEL.
endif::[]
The host initiating the installation does not need to be intended for inclusion
in the {product-title} cluster, but it can be.
====

ifdef::openshift-enterprise[]
Alternatively, you can use the xref:quick_install.adoc#install-config-install-quick-install[quick installation]
method if you prefer an interactive installation experience.
endif::[]

[NOTE]
====
To install {product-title} as a stand-alone registry, see
xref:../../install_config/install/stand_alone_registry.adoc#install-config-installing-stand-alone-registry[Installing a Stand-alone Registry].
====

[[advanced-before-you-begin]]
== Before You Begin

Before installing {product-title}, you must first see the xref:../../install_config/install/prerequisites.adoc#install-config-install-prerequisites[Prerequisites] topic to
prepare your hosts, which includes verifying system and environment requirements
per component type and properly installing and configuring Docker. It also
includes installing Ansible version 2.2.0 or later, as the advanced installation
method is based on Ansible playbooks and as such requires directly invoking
Ansible.

If you are interested in installing {product-title} using the containerized method
(optional for RHEL but required for RHEL Atomic Host), see
xref:../../install_config/install/rpm_vs_containerized.adoc#install-config-install-rpm-vs-containerized[Installing on Containerized Hosts] to ensure that you understand the differences between these
methods, then return to this topic to continue.

For large-scale installs, including suggestions for optimizing install time,
see the
xref:../../scaling_performance/install_practices.adoc#scaling-performance-install-best-practices[Scaling and Performance Guide].

After following the instructions in the
xref:../../install_config/install/prerequisites.adoc#install-config-install-prerequisites[Prerequisites] topic and
deciding between the RPM and containerized methods, you can continue in this
topic to xref:configuring-ansible[Configuring Ansible Inventory Files].

[[configuring-ansible]]
== Configuring Ansible Inventory Files

The *_/etc/ansible/hosts_* file is Ansible's inventory file for the playbook
used to install {product-title}. The inventory file describes the configuration
for your {product-title} cluster. You must replace the default contents of the
file with your desired configuration.

The following sections describe commonly-used variables to set in your inventory
file during an advanced installation, followed by
xref:adv-install-example-inventory-files[example inventory files] you can use as
a starting point for your installation.

Many of the Ansible variables described are optional. Accepting the default
values should suffice for development environments, but for production
environments it is recommended you read through and become familiar with the
various options available.

The example inventories describe various environment topographies, including
xref:multiple-masters[using multiple masters for high availability]. You can
choose an example that matches your requirements, modify it to match your own
environment, and use it as your inventory file when
xref:running-the-advanced-installation[running the advanced installation].

[discrete]
[[advanced-install-image-version-policy]]
=== Image Version Policy

Images require a version number policy in order to maintain updates. See
the
xref:../../architecture/core_concepts/containers_and_images.adoc#architecture-images-tag-policy[Image
Version Tag Policy] section in the Architecture Guide for more information.

[[configuring-cluster-variables]]
=== Configuring Cluster Variables

To assign environment variables during the Ansible install that apply more
globally to your {product-title} cluster overall, indicate the desired variables in
the *_/etc/ansible/hosts_* file on separate, single lines within the *[OSEv3:vars]*
section. For example:

----
[OSEv3:vars]

openshift_master_identity_providers=[{'name': 'htpasswd_auth',
'login': 'true', 'challenge': 'true',
'kind': 'HTPasswdPasswordIdentityProvider',
'filename': '/etc/origin/master/htpasswd'}]

openshift_master_default_subdomain=apps.test.example.com
----

The following table describes variables for use with the Ansible installer that
can be assigned cluster-wide:

[[cluster-variables-table]]
.Cluster Variables
[options="header"]
|===

|Variable |Purpose

|`*ansible_ssh_user*`
|This variable sets the SSH user for the installer to use and defaults to
*root*. This user should allow SSH-based authentication
xref:host_preparation.adoc#ensuring-host-access[without requiring a password]. If
using SSH key-based authentication, then the key should be managed by an SSH
agent.

|`*ansible_become*`
|If `*ansible_ssh_user*` is not *root*, this variable must be set to *true* and
the user must be configured for passwordless *sudo*.

|`*debug_level*`
a|This variable sets which INFO messages are logged to the `systemd-journald.service`. Set one of the following:

* `0` to log errors and warnings only
* `2` to log normal information (This is the default level.)
* `4` to log debugging-level information
* `6` to log API-level debugging information (request / response)
* `8` to log body-level API debugging information

For more information on debug log levels, see xref:../../install_config/master_node_configuration.adoc#master-node-config-logging-levels[Configuring Logging Levels].

|`*containerized*`
|If set to *true*, containerized {product-title} services are run on all target master
and node hosts in the cluster instead of installed using RPM packages. If set to
*false* or unset, the default RPM method is used. RHEL Atomic Host requires the
containerized method, and is automatically selected for you based on the
detection of the *_/run/ostree-booted_* file. See
xref:../../install_config/install/rpm_vs_containerized.adoc#install-config-install-rpm-vs-containerized[Installing on
Containerized Hosts] for more details.
ifdef::openshift-enterprise[]
Containerized installations are supported starting in {product-title} 3.1.1.
endif::[]

|`*openshift_master_cluster_hostname*`
|This variable overrides the host name for the cluster, which defaults to the
host name of the master.

|`*openshift_master_cluster_public_hostname*`
|This variable overrides the public host name for the cluster, which defaults to
the host name of the master.

|`*openshift_master_cluster_method*`
|Optional. This variable defines the HA method when deploying multiple masters.
Supports the `native` method. See xref:multiple-masters[Multiple Masters] for
more information.

|`*openshift_rolling_restart_mode*`
|This variable enables rolling restarts of HA masters (i.e., masters are taken
down one at a time) when
xref:../upgrading/automated_upgrades.adoc#running-the-upgrade-playbook-directly[running
the upgrade playbook directly]. It defaults to `services`, which allows rolling
restarts of services on the masters. It can instead be set to `system`, which
enables rolling, full system restarts and also works for single master clusters.

|`*os_sdn_network_plugin_name*`
|This variable configures which
xref:../../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[OpenShift SDN plug-in] to
use for the pod network, which defaults to *redhat/openshift-ovs-subnet* for the
standard SDN plug-in. Set the variable to *redhat/openshift-ovs-multitenant* to
use the multitenant plug-in.

|`*openshift_master_identity_providers*`
|This variable overrides the
xref:../../install_config/configuring_authentication.adoc#install-config-configuring-authentication[identity provider], which
defaults to
xref:../../install_config/configuring_authentication.adoc#DenyAllPasswordIdentityProvider[Deny
All].

|`*openshift_master_named_certificates*`
.2+.^|These variables are used to configure xref:../../install_config/certificate_customization.adoc#install-config-certificate-customization[custom certificates] which are deployed as part of the installation. See xref:advanced-install-custom-certificates[Configuring Custom Certificates] for more information.

|`*openshift_master_overwrite_named_certificates*`

|`*openshift_master_session_name*`
.4+.^|These variables override defaults for
xref:../../install_config/configuring_authentication.adoc#session-options[session
options] in the OAuth configuration. See xref:advanced-install-session-options[Configuring Session Options] for more information.

|`*openshift_master_session_max_seconds*`

|`*openshift_master_session_auth_secrets*`

|`*openshift_master_session_encryption_secrets*`

|`*openshift_master_portal_net*`
|This variable configures the subnet in which
xref:../../architecture/core_concepts/pods_and_services.adoc#services[services]
will be created within the
xref:../../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[{product-title}
SDN]. This network block should be private and must not conflict with any
existing network blocks in your infrastructure to which pods, nodes, or the
master may require access to, or the installation will fail. Defaults to
*172.30.0.0/16*, and *cannot* be re-configured after deployment. If changing from the default, avoid *172.17.0.0/16*, which the *docker0* network bridge uses by default, or modify the *docker0* network.

|`*openshift_master_default_subdomain*`
|This variable overrides the default subdomain to use for exposed
xref:../../architecture/core_concepts/routes.adoc#architecture-core-concepts-routes[routes].

|`*openshift_node_proxy_mode*`
|This variable specifies the
xref:../../architecture/core_concepts/pods_and_services.adoc#service-proxy-mode[service
proxy mode] to use: either *iptables* for the default, pure-*iptables*
implementation, or *userspace* for the user space proxy.

|`*osm_default_node_selector*`
|This variable overrides the node selector that projects will use by default
when placing pods.

|`*osm_cluster_network_cidr*`
| This variable overrides the
xref:../../architecture/additional_concepts/sdn.adoc#sdn-design-on-masters[SDN
cluster network] CIDR block. This is the network from which pod IPs are
assigned. This network block should be a private block and must not conflict
with existing network blocks in your infrastructure to which pods, nodes, or the
master may require access. Defaults to *10.128.0.0/14* and *cannot* be arbitrarily
re-configured after deployment, although certain changes to it can be made in
the xref:../configuring_sdn.adoc#configuring-the-pod-network-on-masters[SDN
master configuration].

|`*osm_host_subnet_length*`
|This variable specifies the size of the per host subnet allocated for pod IPs
by
xref:../../architecture/additional_concepts/sdn.adoc#sdn-design-on-masters[{product-title}
SDN]. Defaults to *9* which means that a subnet of size /23 is allocated to each
host; for example, given the default 10.128.0.0/14 cluster network, this will
allocate 10.128.0.0/23, 10.128.2.0/23, 10.128.4.0/23, and so on. This *cannot* be
re-configured after deployment.

|`*openshift_use_flannel*`
|This variable enables *flannel* as an alternative networking layer instead of
the default SDN. If enabling *flannel*, disable the default SDN with the
*openshift_use_openshift_sdn* variable. For more information, see xref:../configuring_sdn.adoc#using-flannel[Using Flannel].

|`*openshift_docker_additional_registries*`
|{product-title} adds the specified additional registry or registries to the
Docker configuration.

|`*openshift_docker_insecure_registries*`
|{product-title} adds the specified additional insecure registry or registries
to the Docker configuration.

|`*openshift_docker_blocked_registries*`
|{product-title} adds the specified blocked registry or registries to the Docker
configuration.

|`*openshift_hosted_metrics_public_url*`
|This variable sets the host name for integration with the metrics console. The
default is
`\https://hawkular-metrics.{{openshift_master_default_subdomain}}/hawkular/metrics`
If you alter this variable, ensure the host name is accessible via your router.

|`*openshift_template_service_broker_namespaces*`
|This variable enables the template service broker by specifying one of more
namespaces whose templates will be served by the broker.
|===

[[advanced-install-deployment-types]]
=== Configuring Deployment Type

Various defaults used throughout the playbooks and roles used by the installer
are based on the deployment type configuration (usually defined in an Ansible
inventory file).

ifdef::openshift-enterprise[]
Ensure the `deployment_type` parameter in your inventory file's `[OSEv3:vars]`
section is set to `openshift-enterprise` to install the {product-title} variant:

----
[OSEv3:vars]
deployment_type=openshift-enterprise
----
endif::[]
ifdef::openshift-origin[]
Ensure the `deployment_type` parameter in your inventory file's `[OSEv3:vars]`
section is set to `origin` to install the {product-title} variant:

----
[OSEv3:vars]
openshift_deployment_type=origin
----
endif::[]


[[configuring-host-variables]]
=== Configuring Host Variables

To assign environment variables to hosts during the Ansible installation, indicate
the desired variables in the *_/etc/ansible/hosts_* file after the host entry in
the *[masters]* or *[nodes]* sections. For example:

----
[masters]
ec2-52-6-179-239.compute-1.amazonaws.com openshift_public_hostname=ose3-master.public.example.com
----

The following table describes variables for use with the Ansible installer that
can be assigned to individual host entries:

[[advanced-host-variables]]
.Host Variables
[options="header"]
|===

|Variable |Purpose

|`*openshift_hostname*`
|This variable overrides the internal cluster host name for the system. Use this
when the system's default IP address does not resolve to the system host name.

|`*openshift_public_hostname*`
|This variable overrides the system's public host name. Use this for cloud
installations, or for hosts on networks using a network address translation
(NAT).

|`*openshift_ip*`
|This variable overrides the cluster internal IP address for the system. Use
this when using an interface that is not configured with the default route.

|`*openshift_public_ip*`
|This variable overrides the system's public IP address. Use this for cloud
installations, or for hosts on networks using a network address translation
(NAT).

|`*containerized*`
|If set to *true*, containerized {product-title} services are run on the target master and
node hosts instead of installed using RPM packages. If set to *false* or unset,
the default RPM method is used. RHEL Atomic Host requires the containerized
method, and is automatically selected for you based on the detection of the
*_/run/ostree-booted_* file. See
xref:../../install_config/install/rpm_vs_containerized.adoc#install-config-install-rpm-vs-containerized[Installing on Containerized Hosts] for more details.
ifdef::openshift-enterprise[]
Containerized installations are supported starting in {product-title} 3.1.1.
endif::[]

|`*openshift_node_labels*`
|This variable adds labels to nodes during installation. See
xref:configuring-node-host-labels[Configuring Node Host Labels] for more
details.

|`*openshift_node_kubelet_args*`
|This variable is used to configure `kubeletArguments` on nodes, such as
arguments used in xref:../../admin_guide/garbage_collection.adoc#admin-guide-garbage-collection[container and
image garbage collection], and to
xref:../../admin_guide/manage_nodes.adoc#configuring-node-resources[specify
resources per node]. `kubeletArguments` are key value pairs that are passed
directly to the Kubelet that match the
http://kubernetes.io/v1.1/docs/admin/kubelet.html[Kubelet's command line
arguments]. `kubeletArguments` are not migrated or validated and may become
invalid if used. These values override other settings in node configuration
which may cause invalid configurations. Example usage:
*{'image-gc-high-threshold': ['90'],'image-gc-low-threshold': ['80']}*.

|`*openshift_hosted_router_selector*`
|Default node selector for automatically deploying router pods. See
xref:configuring-node-host-labels[Configuring Node Host Labels] for details.

|`*openshift_registry_selector*`
|Default node selector for automatically deploying registry pods. See
xref:configuring-node-host-labels[Configuring Node Host Labels] for details.

|`*openshift_docker_options*`
|This variable configures additional Docker options within *_/etc/sysconfig/docker_*, such as
options used in xref:../../install_config/install/host_preparation.adoc#managing-docker-container-logs[Managing Container Logs].
Example usage: *"--log-driver json-file --log-opt max-size=1M --log-opt max-file=3"*.

|`openshift_schedulable`
|This variable configures whether the host is marked as a schedulable node,
meaning that it is available for placement of new pods. See
xref:marking-masters-as-unschedulable-nodes[Configuring Schedulability on Masters].
|===

[[advanced-install-configuring-registry-location]]
=== Configuring a Registry Location

If you are using an image registry other than the default at
`registry.access.redhat.com`, specify the desired registry within the
*_/etc/ansible/hosts_* file.

----
oreg_url=example.com/openshift3/ose-${component}:${version}
openshift_examples_modify_imagestreams=true
----

.Registry Variables
[options="header"]
|===

|Variable |Purpose
|`*oreg_url*`
|Set to the alternate image location. Necessary if you are not using the default registry at `registry.access.redhat.com`.

|`*openshift_examples_modify_imagestreams*`
|Set to `true` if pointing to a registry other than the default. Modifies the image stream location to the value of `*oreg_url*`.
|===

[[advanced-install-configuring-integrated-glusterfs-backed-registry]]
=== Configuring an Integrated GlusterFS Backed Registry

ifdef::openshift-enterprise[]
An integrated GlusterFS Registry can be configured and deployed during the initial installation
of the cluster to offer a more reliable image registry. link:https://access.redhat.com/documentation/en-us/red_hat_gluster_storage/3.1/html/container-native_storage_for_openshift_container_platform_3.4/[Red Hat Container Native Storage (CNS)] has been integrated into the {product-title} Advanced Installation procedure utilizing containerized GlusterFS storage pods to back the Docker registry.
endif::[]

ifdef::openshift-origin[]
An integrated GlusterFS Registry can be configured and deployed during the initial installation
of the cluster to offer a more reliable image registry. See link:https://github.com/gluster/gluster-kubernetes[Running Containerized GlusterFS in Kubernetes] for additional information on containerized storage using GlusterFS.
endif::[]

Some additional prerequisites and host preparation are required beyond the normal {product-title} xref:../../install_config/install/prerequisites.adoc#install-config-install-prerequisites[Prerequisites]. 

.Additional Prerequisites and Host Preparation
[options="header"]
|===

|Prerequisite |Description
|`*Storage Nodes*`
|A minimum of 3 storage nodes must be available to run GlusterFS integrated registry (3-way replication)
 
 Each storage node must have at least 1 raw block device with at least 10GB of free storage
ifdef::openshift-enterprise[]
|`*Enable Storage Repo*`
|rh-gluster-3-for-rhel-7-server-rpms will need to be enabled. 

      `# subscription-manager repos --enable=rh-gluster-3-for-rhel-7-server-rpms`
endif::[]
ifdef::openshift-enterprise[]
|`*Required Software Components*`
|Install *glusterfs-client* on each storage node. 

      `# yum install glusterfs-client -y`

endif::[]
ifdef::openshift-origin[]
|`*Required Software Components*`
|Install *glusterfs-client* or *glusterfs-fuse* packages (mount.glusterfs) on each storage node. 

endif::[]
|===


In the inventory/hosts file the following will need to be configured

.Integrated GlusterFS Registry Variables and Groups
[options="header"]
|===

|Variable |Purpose
|`*glusterfs_registry*`
|Placed in the OSEv3:children section and enables the [glusterfs_registry] group

|`*[glusterfs_registry]*`
| The group of storage nodes that will host the GlusterFS Registry along with the `glusterfs_devices` list parameter

  <hostname or ip> `glusterfs_devices`='[ "(path-to-device)", "(...)" ]'

|`*openshift_hosted_registry_storage_kind=glusterfs*`
|Placed in the OSEv3:vars section and enables GlusterFS Registry if `[glusterfs_registry]` group exists and `glusterfs_registry` group_name exists

|`*Nodes labeled with region=Infra*`
|{product-title} will install the integrated docker registry pods and the associated routers on nodes 
 containing the *infra* label. If this label does not exist on at least one node, the registry deployment will fail.

 `openshift_node_labels="{'region': 'infra'}"`

|===


Example:
----
[OSEv3:children]
masters
nodes
glusterfs_registry <1>

[OSEv3:vars]
ansible_ssh_user=root
openshift_master_default_subdomain=cloudapps.example.com
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]
openshift_hosted_registry_storage_kind=glusterfs <2>

[nodes]
192.168.10.11  openshift_schedulable=True node=True openshift_node_labels="{'region': 'infra'}" <3>
...
...

[glusterfs_registry] <4>
192.168.10.11 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
192.168.10.12 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
192.168.10.13 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'

[masters]
...
...
----
<1> OSEv3:children group_name.
<2> OSEv3:vars variable that enables GlusterFS Registry.
<3> Infrastructure label for nodes that will host registry and other infrastructure pods such as default routers, etc... 
<4> Group that lists the Registry storage nodes and the list of raw block devices.

[NOTE]
====
If no registry options are used the default {product-title} registry is ephermal and all data will be lost if the pod no longer exists.
{product-title} also supports a single node NFS integrated registry but will lack redundancy and reliability
compared with an Integrated GlusterFS Registry.
====

[[advanced-install-configuring-containerized-glusterfs-backed-persistent-storage]]
=== Configuring Containerized GlusterFS Backed Persistent Storage

Along with the ability to install and back a GlusterFS Integrated Registry as explained in the previous section, GlusterFS can also be configured and installed in the {product-title} cluster for 
xref:../../architecture/additional_concepts/storage.adoc#architecture-additional-concepts-storage[Peristent Storage] and xref:../storage_examples/gluster_dynamic_example.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[Dynamic Provisioning]. After successful installation all required components to start using and provisioning storage will exist, including Endpoints, Services and a StorageClass.

[NOTE]
====
This storage should be separate from the Integrated Docker Registry GlusterFS backed storage (if installed), and will also require additional nodes.
For example, if the Integrated Docker Registry Backed by GlusterFS is also configured, that would be a total of 6 storage nodes needed (3 for Registry and 3 for Persistent Storage).
This limitation is based on Heketi's ability to only manage a single storage pool at this time.
====

Some additional prerequisites and host preparation are required beyond the normal {product-title} xref:../../install_config/install/prerequisites.adoc#install-config-install-prerequisites[Prerequisites]. 

.Additional Prerequisites and Host Preparation
[options="header"]
|===

|Prerequisite |Description
|`*Storage Nodes*`
|A minimum of 3 storage nodes must be available to run GlusterFS integrated registry (3-way replication)
 
 Each storage node must have at least 1 raw block device with at least 10GB of free storage. However, it is
 recommended to have at least 100GB available for usage with Persistent Storage and Dynamic Provisioning.
ifdef::openshift-enterprise[]
|`*Enable Storage Repo*`
|rh-gluster-3-for-rhel-7-server-rpms will need to be enabled. 

      `# subscription-manager repos --enable=rh-gluster-3-for-rhel-7-server-rpms`
endif::[]
ifdef::openshift-enterprise[]
|`*Required Software Components*`
|Verify/Install *glusterfs-client* on each storage node. 

      `# yum install glusterfs-client -y`

endif::[]
ifdef::openshift-origin[]
|`*Required Software Components*`
|Verify/Install *glusterfs-client* or *glusterfs-fuse* packages (mount.glusterfs) on each storage node. 

 Install link:https://github.com/heketi/heketi/releases[heketi-client] on same host that will run the openshift-ansible installer (typically one of the master nodes).

endif::[]
|===


In the inventory/hosts file the following will need to be configured

.Integrated GlusterFS Persistent Storage Variables and Groups
[options="header"]
|===

|Variable |Purpose
|`*glusterfs*`
|Placed in the OSEv3:children section and enables the [glusterfs] group

|`*[glusterfs]*`
| The group of storage nodes that will host the GlusterFS storage along with the `glusterfs_devices` list parameter

  <hostname or ip> `glusterfs_devices`='[ "(path-to-device)", "(...)" ]'

|===


.Additional GlusterFS Persistent Storage Variables and Groups (Optional).
[options="header"]
|===

|Variable |Purpose
|`*openshift_storage_glusterfs_namespace*`
|Placed in the OSEv3:children section and allows users to specify a new namespace to host the storage pods otherwise they are installed
 in `default` namespace.

  `openshift_storage_glusterfs_namespace=mystorage`

|`*openshift_storage_glusterfs_name*`
| The base name used for the GlusterFS storage pods.

  `openshift_storage_glusterfs_name=scratchspace`

|===

[NOTE]
====
If `openshift_storage_glusterfs_namespace` is not set, the GlusterFS storage pods will assume `default` namespace.
====

Example:
----
[OSEv3:children]
masters
nodes
glusterfs <1>

[OSEv3:vars]
ansible_ssh_user=root
openshift_master_default_subdomain=cloudapps.example.com
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

[nodes]
192.168.10.11  openshift_schedulable=True node=True openshift_node_labels="{'region': 'infra'}" <2>
...
...

[glusterfs] <3>
192.168.10.11 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
192.168.10.12 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
192.168.10.13 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'

[masters]
...
...
----
<1> OSEv3:children group_name.
<2> Infrastructure label for nodes that will host registry and other infrastructure pods such as default routers, etc...
<3> Group that lists the Registry storage nodes and the list of raw block devices.

[NOTE]
====
As stated, both GlusterFS backed Registry and GlusterFS Persistent Storage can be installed on the same cluster using
Advanced Installation.
====


*Post Installation Configuration*

- Update the `/etc/dnsmasq.conf` on all masters to enable proper DNS communication between the masters nodes and the `Heketi` server pods routes.
  Add the following line replacing with appropriate values:
  
----
address=/.<openshift_master_default_subdomain>/<Router_IP>

eg.
   address=/.cloudapps.example.com/172.18.23.24
----   

- Restart dnsmasq Service.

----
systemctl restart dnsmasq`
----

- From a master verify that the `route` is working

----
#oc get routes
NAME                  HOST/PORT                                           PATH      SERVICES              PORT      TERMINATION   WILDCARD
heketi-scratchspace   heketi-scratchspace-default.cloudapps.example.com             heketi-scratchspace   <all>                   None

#curl http://heketi-scratchspace-default.cloudapps.example.com/hello
Hello from Heketi.

----

- Verfiy that the GlusterFS StorageClass was created

----
#oc get storageclass
NAME                  TYPE
glusterfs-storage     kubernetes.io/glusterfs

----

[NOTE]
====
Default name for the *route* will be `heketi-TBD` unless this was overridden 
using the `openshift_glusterfs_storage_name=MYNAME` variable in the ansible hosts file.
====

[NOTE]
====
After successful installation, xref:../storage_examples/gluster_dynamic_example.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[Dynamic Provisioning]
of GlusterFS volumes can occur by xref:../storage_examples/gluster_dynamic_example.html#create-a-pvc-ro-request-storage-for-your-application[creating a PVC to request storage].
 
====

[[advanced-install-configuring-global-proxy]]
=== Configuring Global Proxy Options

If your hosts require use of a HTTP or HTTPS proxy in order to connect to
external hosts, there are many components that must be configured to use the
proxy, including masters, Docker, and builds. Node services only connect to the
master API requiring no external access and therefore do not need to be
configured to use a proxy.

In order to simplify this configuration, the following Ansible variables can be
specified at a cluster or host level to apply these settings uniformly across
your environment.

[NOTE]
====
See xref:../../install_config/build_defaults_overrides.adoc#install-config-build-defaults-overrides[Configuring
Global Build Defaults and Overrides] for more information on how the proxy
environment is defined for builds.
====

.Cluster Proxy Variables
[options="header"]
|===

|Variable |Purpose

|`*openshift_http_proxy*`
|This variable specifies the `*HTTP_PROXY*` environment variable for masters and
the Docker daemon.

|`*openshift_https_proxy*`
|This variable specifices the `*HTTPS_PROXY*` environment variable for masters
and the Docker daemon.

|`*openshift_no_proxy*`
|This variable is used to set the `*NO_PROXY*` environment variable for masters
and the Docker daemon. This value should be set to a comma separated list of
host names or wildcard host names that should not use the defined proxy. This
list will be augmented with the list of all defined {product-title} host names
by default.

|`*openshift_generate_no_proxy_hosts*`
|This boolean variable specifies whether or not the names of all defined
OpenShift hosts and `pass:[*.cluster.local]` should be automatically appended to
the `*NO_PROXY*` list. Defaults to *true*; set it to *false* to override this
option.

|`*openshift_builddefaults_http_proxy*`
|This variable defines the `*HTTP_PROXY*` environment variable inserted into
builds using the `*BuildDefaults*` admission controller. If
`*openshift_http_proxy*` is set, this variable will inherit that value; you only
need to set this if you want your builds to use a different value.

|`*openshift_builddefaults_https_proxy*`
|This variable defines the `*HTTPS_PROXY*` environment variable inserted into
builds using the `*BuildDefaults*` admission controller. If
`*openshift_https_proxy*` is set, this variable will inherit that value; you
only need to set this if you want your builds to use a different value.

|`*openshift_builddefaults_no_proxy*`
|This variable defines the `*NO_PROXY*` environment variable inserted into
builds using the `*BuildDefaults*` admission controller. If
`*openshift_no_proxy*` is set, this variable will inherit that value; you only
need to set this if you want your builds to use a different value.

|`*openshift_builddefaults_git_http_proxy*`
|This variable defines the HTTP proxy used by `git clone` operations during a
build, defined using the `*BuildDefaults*` admission controller. If
`*openshift_builddefaults_http_proxy*` is set, this variable will inherit that
value; you only need to set this if you want your `git clone` operations to use
a different value.

|`*openshift_builddefaults_git_https_proxy*`
|This variable defines the HTTPS proxy used by `git clone` operations during a
build, defined using the `*BuildDefaults*` admission controller. If
`*openshift_builddefaults_https_proxy*` is set, this variable will inherit that
value; you only need to set this if you want your `git clone` operations to use
a different value.
|===


[[marking-masters-as-unschedulable-nodes]]
=== Configuring Schedulability on Masters

Any hosts you designate as masters during the installation process should also
be configured as nodes so that the masters are configured as part of the
xref:../../architecture/additional_concepts/networking.adoc#openshift-sdn[OpenShift SDN]. You must do so by adding entries for these hosts to the `[nodes]` section:

----
[nodes]
master.example.com
----

In order to ensure that your masters are not burdened with running pods, they
are automatically marked unschedulable by default by the installer, meaning that
new pods cannot be placed on the hosts. This is the same as setting the
`openshift_schedulable=false` host variable.

You can manually set a master host to schedulable during installation using the
`openshift_schedulable=true` host variable, though this is not recommended in
production environments:

----
[nodes]
master.example.com openshift_schedulable=true
----

If you want to change the schedulability of a host post-installation, see
xref:../../admin_guide/manage_nodes.adoc#marking-nodes-as-unschedulable-or-schedulable[Marking Nodes as Unschedulable or Schedulable].

[[configuring-node-host-labels]]
=== Configuring Node Host Labels

You can assign
xref:../../architecture/core_concepts/pods_and_services.adoc#labels[labels] to
node hosts during the Ansible install by configuring the *_/etc/ansible/hosts_*
file. Labels are useful for determining the placement of pods onto nodes using
the xref:../../admin_guide/scheduler.adoc#configurable-predicates[scheduler].
Other than `region=infra` (discussed in
xref:configuring-dedicated-infrastructure-nodes[Configuring Dedicated Infrastructure Nodes]), the actual label names and values are arbitrary and can
be assigned however you see fit per your cluster's requirements.

To assign labels to a node host during an Ansible install, use the
`openshift_node_labels` variable with the desired labels added to the desired
node host entry in the `[nodes]` section. In the following example, labels are
set for a region called `primary` and a zone called `east`:

----
[nodes]
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
----

[[configuring-dedicated-infrastructure-nodes]]
==== Configuring Dedicated Infrastructure Nodes

The `openshift_router_selector` and `openshift_registry_selector` Ansible
settings determine the label selectors used when placing registry and router
pods. They are set to `region=infra` by default:

----
# default selectors for router and registry services
# openshift_router_selector='region=infra'
# openshift_registry_selector='region=infra'
----

The default router and registry will be automatically deployed during
installation if nodes exist in the `[nodes]` section that match the selector
settings. For example:

----
[nodes]
infra-node1.example.com openshift_node_labels="{'region': 'infra','zone': 'default'}"
----

[IMPORTANT]
====
The registry and router are only able to run on node hosts with the
`region=infra` label. Ensure that at least one node host in your {product-title}
environment has the `region=infra` label.
====

It is recommended for production environments that you maintain dedicated
infrastructure nodes where the registry and router pods can run separately from
pods used for user applications.

As described in xref:marking-masters-as-unschedulable-nodes[Configuring
Schedulability on Masters], master hosts are marked unschedulable by default. If
you label a master host with `region=infra` and have no other dedicated
infrastructure nodes, you must also explicitly mark these master hosts as
schedulable. Otherwise, the registry and router pods cannot be placed anywhere:

----
[nodes]
master.example.com openshift_node_labels="{'region': 'infra','zone': 'default'}" openshift_schedulable=true
----

[[advanced-install-session-options]]
=== Configuring Session Options

xref:../../install_config/configuring_authentication.adoc#session-options[Session
options] in the OAuth configuration are configurable in the inventory file. By
default, Ansible populates a `*sessionSecretsFile*` with generated
authentication and encryption secrets so that sessions generated by one master
can be decoded by the others. The default location is
*_/etc/origin/master/session-secrets.yaml_*, and this file will only be
re-created if deleted on all masters.

You can set the session name and maximum number of seconds with
`*openshift_master_session_name*` and `*openshift_master_session_max_seconds*`:

----
openshift_master_session_name=ssn
openshift_master_session_max_seconds=3600
----

If provided, `*openshift_master_session_auth_secrets*` and
`*openshift_master_encryption_secrets*` must be equal length.

For `*openshift_master_session_auth_secrets*`, used to authenticate sessions
using HMAC, it is recommended to use secrets with 32 or 64 bytes:

----
openshift_master_session_auth_secrets=['DONT+USE+THIS+SECRET+b4NV+pmZNSO']
----

For `*openshift_master_encryption_secrets*`, used to encrypt sessions, secrets
must be 16, 24, or 32 characters long, to select AES-128, AES-192, or AES-256:

----
openshift_master_session_encryption_secrets=['DONT+USE+THIS+SECRET+b4NV+pmZNSO']
----

[[advanced-install-custom-certificates]]
=== Configuring Custom Certificates

xref:../../install_config/certificate_customization.adoc#install-config-certificate-customization[Custom serving
certificates] for the public host names of the {product-title} API and
xref:../../architecture/infrastructure_components/web_console.adoc#architecture-infrastructure-components-web-console[web console]
can be deployed during an advanced installation and are configurable in the
inventory file.

[NOTE]
====
Custom certificates should only be configured for the host name associated with
the `*publicMasterURL*` which can be set using
`*openshift_master_cluster_public_hostname*`. Using a custom serving certificate
for the host name associated with the `*masterURL*`
(*`openshift_master_cluster_hostname`*) will result in TLS errors as
infrastructure components will attempt to contact the master API using the
internal `*masterURL*` host.
====

Certificate and key file paths can be configured using the
`*openshift_master_named_certificates*` cluster variable:

----
openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key"}]
----

File paths must be local to the system where Ansible will be run. Certificates
are copied to master hosts and are deployed within the
*_/etc/origin/master/named_certificates/_* directory.

Ansible detects a certificate's `Common Name` and `Subject Alternative Names`.
Detected names can be overridden by providing the `*"names"*` key when setting
`*openshift_master_named_certificates*`:

----
openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key", "names": ["public-master-host.com"]}]
----

Certificates configured using `*openshift_master_named_certificates*` are cached
on masters, meaning that each additional Ansible run with a different set of
certificates results in all previously deployed certificates remaining in place
on master hosts and within the master configuration file.

If you would like `*openshift_master_named_certificates*` to be overwritten with
the provided value (or no value), specify the
`*openshift_master_overwrite_named_certificates*` cluster variable:

----
openshift_master_overwrite_named_certificates=true
----

For a more complete example, consider the following cluster variables in an
inventory file:

----
openshift_master_cluster_method=native
openshift_master_cluster_hostname=lb.openshift.com
openshift_master_cluster_public_hostname=custom.openshift.com
----

To overwrite the certificates on a subsequent Ansible run, you could set the
following:

----
openshift_master_named_certificates=[{"certfile": "/root/STAR.openshift.com.crt", "keyfile": "/root/STAR.openshift.com.key", "names": ["custom.openshift.com"]}]
openshift_master_overwrite_named_certificates=true
----

[[advanced-install-cluster-metrics]]
=== Configuring Cluster Metrics

Cluster metrics are not set to automatically deploy by default. Set the
following to enable cluster metrics when using the advanced install:

----
[OSEv3:vars]

openshift_hosted_metrics_deploy=true
----

[[advanced-install-cluster-metrics-storage]]
==== Configuring Metrics Storage

The `openshift_metrics_cassandra_storage_type` variable must be set in order to
use persistent storage for metrics. If
`openshift_metrics_cassandra_storage_type` is not set, then cluster metrics data
is stored in an `EmptyDir` volume, which will be deleted when the Cassandra pod
terminates.

There are three options for enabling cluster metrics storage when using the
advanced install:

[discrete]
[[advanced-install-cluster-metrics-storage-nfs-host-group]]
===== Option A: NFS Host Group

When the following variables are set, an NFS volume is created during an
advanced install with path *_<nfs_directory>/<volume_name>_* on the host within
the `[nfs]` host group. For example, the volume path using these options would
be *_/exports/metrics_*:

----
[OSEv3:vars]

openshift_hosted_metrics_storage_kind=nfs
openshift_hosted_metrics_storage_access_modes=['ReadWriteOnce']
openshift_hosted_metrics_storage_nfs_directory=/exports
openshift_hosted_metrics_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_metrics_storage_volume_name=metrics
openshift_hosted_metrics_storage_volume_size=10Gi
----

[discrete]
[[advanced-install-cluster-metrics-storage-external-nfs]]
===== Option B: External NFS Host

To use an external NFS volume, one must already exist with a path of
*_<nfs_directory>/<volume_name>_* on the storage host.

----
[OSEv3:vars]

openshift_hosted_metrics_storage_kind=nfs
openshift_hosted_metrics_storage_access_modes=['ReadWriteOnce']
openshift_hosted_metrics_storage_host=nfs.example.com
openshift_hosted_metrics_storage_nfs_directory=/exports
openshift_hosted_metrics_storage_volume_name=metrics
openshift_hosted_metrics_storage_volume_size=10Gi
----

The remote volume path using the following options would be
*_nfs.example.com:/exports/metrics_*.

[discrete]
[[advanced-install-cluster-metrics-storage-dynamic]]
===== Option C: Dynamic

Use the following variable if your {product-title} environment supports
xref:../../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[dynamic volume provisioning] for your cloud provider:

----
[OSEv3:vars]

openshift_metrics_cassandra_storage_type=dynamic
----

[[advanced-install-cluster-logging]]
=== Configuring Cluster Logging

Cluster logging is not set to automatically deploy by default. Set the
following to enable cluster logging when using the advanced installation method:

----
[OSEv3:vars]

openshift_hosted_logging_deploy=true
----

[[advanced-installation-logging-storage]]
==== Configuring Logging Storage

The `openshift_hosted_logging_storage_kind` variable must be set in order to use
persistent storage for logging. If `openshift_hosted_logging_storage_kind` is
not set, then cluster logging data is stored in an `EmptyDir` volume, which will
be deleted when the Elasticsearch pod terminates.

There are three options for enabling cluster logging storage when using the
advanced install:

[discrete]
[[advanced-installation-logging-storage-nfs-host-group]]
===== Option A: NFS Host Group

When the following variables are set, an NFS volume is created during an
advanced install with path *_<nfs_directory>/<volume_name>_* on the host within
the `[nfs]` host group. For example, the volume path using these options would be
*_/exports/logging_*:

----
[OSEv3:vars]

openshift_hosted_logging_storage_kind=nfs
openshift_hosted_logging_storage_access_modes=['ReadWriteOnce']
openshift_hosted_logging_storage_nfs_directory=/exports
openshift_hosted_logging_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_logging_storage_volume_name=logging
openshift_hosted_logging_storage_volume_size=10Gi
----

[discrete]
[[advanced-installation-logging-storage-external-nfs]]
===== Option B: External NFS Host

To use an external NFS volume, one must already exist with a path of
*_<nfs_directory>/<volume_name>_* on the storage host.

----
[OSEv3:vars]

openshift_hosted_logging_storage_kind=nfs
openshift_hosted_logging_storage_access_modes=['ReadWriteOnce']
openshift_hosted_logging_storage_host=nfs.example.com
openshift_hosted_logging_storage_nfs_directory=/exports
openshift_hosted_logging_storage_volume_name=logging
openshift_hosted_logging_storage_volume_size=10Gi
----

The remote volume path using the following options would be
*_nfs.example.com:/exports/logging_*.

[discrete]
[[advanced-installation-logging-storage-dynamic]]
===== Option C: Dynamic

Use the following variable if your {product-title} environment supports
xref:../../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[dynamic volume provisioning] for your cloud provider:

----
[OSEv3:vars]

openshift_hosted_logging_storage_kind=dynamic
----

[[adv-install-example-inventory-files]]
== Example Inventory Files

[[single-master]]
=== Single Master Examples

You can configure an environment with a single master and multiple nodes, and
either a single or multiple number of external *etcd* hosts.

[NOTE]
====
Moving from a single master cluster to multiple masters after installation is
not supported.
====

[discrete]
[[single-master-multi-node-ai]]
==== Single Master and Multiple Nodes

The following table describes an example environment for a single
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[master] (with *etcd* on the same host)
and two
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master.example.com*
|Master and node

|*master.example.com*
|etcd

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

You can see these example hosts present in the *[masters]* and *[nodes]*
sections of the following example inventory file:

.Single Master and Multiple Nodes Inventory File
----
# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=root

# If ansible_ssh_user is not root, ansible_become must be set to true
#ansible_become=true

ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# host group for masters
[masters]
master.example.com

# host group for etcd
[etcd]
master.example.com

# host group for nodes, includes region info
[nodes]
master.example.com
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
----

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[discrete]
[[single-master-multi-etcd-multi-node-ai]]
==== Single Master, Multiple etcd, and Multiple Nodes

The following table describes an example environment for a single
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[master],
three
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[*etcd*]
hosts, and two
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master.example.com*
|Master and node

|*etcd1.example.com*
.3+.^|*etcd*

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[NOTE]
====
When specifying multiple *etcd* hosts, external *etcd* is installed and
configured. Clustering of {product-title}'s embedded *etcd* is not supported.
====

You can see these example hosts present in the *[masters]*, *[nodes]*, and
*[etcd]* sections of the following example inventory file:

.Single Master, Multiple etcd, and Multiple Nodes Inventory File

----
# Create an OSEv3 group that contains the masters, nodes, and etcd groups
[OSEv3:children]
masters
nodes
etcd

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# host group for masters
[masters]
master.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# host group for nodes, includes region info
[nodes]
master.example.com
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
----

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[[multiple-masters]]
=== Multiple Masters Examples

You can configure an environment with multiple masters, multiple *etcd* hosts,
and multiple nodes. Configuring
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#high-availability-masters[multiple
masters for high availability] (HA) ensures that the cluster has no single point
of failure.

[NOTE]
====
Moving from a single master cluster to multiple masters after installation is
not supported.
====

When configuring multiple masters, the advanced installation supports the following high
availability (HA) method:

[cols="1,5"]
|===
|`native`
|Leverages the native HA master capabilities built into {product-title} and can be
combined with any load balancing solution. If a host is defined in the *[lb]*
section of the inventory file, Ansible installs and configures HAProxy
automatically as the load balancing solution. If no host is defined, it is
assumed you have pre-configured a load balancing solution of your choice to
balance the master API (port 8443) on all master hosts.
|===

For your pre-configured load balancing solution, you must have:

* A pre-created load balancer VIP configured for SSL passthrough.
* A domain name for VIP registered in DNS.
** The domain name will become the value of both
`openshift_master_cluster_public_hostname` and
`openshift_master_cluster_hostname` in the {product-title} installer.

See
link:https://github.com/redhat-cop/openshift-playbooks/blob/master/playbooks/installation/load_balancing.adoc[External
Load Balancer Integrations] for more information.

[NOTE]
====
For more on the high availability master architecture, see
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[Kubernetes
Infrastructure].
====

Note the following when using the `native` HA method:

- The advanced installation method does not currently support multiple HAProxy
load balancers in an active-passive setup. See the
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Load_Balancer_Administration/ch-lvs-overview-VSA.html[Load
Balancer Administration documentation] for post-installation amendments.
- In a HAProxy setup, controller manager servers run as standalone processes.
They elect their active leader with a lease stored in *etcd*. The lease
expires after 30 seconds by default. If a failure happens on an active
controller server, it will take up to this number of seconds to elect another
leader. The interval can be configured with the `*osm_controller_lease_ttl*`
variable.

To configure multiple masters, refer to the following section.

[discrete]
[[multi-masters-using-native-ha-ai]]
==== Multiple Masters with Multiple etcd

The following describes an example environment for three
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[masters],
one HAProxy load balancer, three
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[*etcd*]
hosts, and two
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
using the `native` HA method:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master1.example.com*
.3+.^|Master (clustered using native HA) and node

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|HAProxy to load balance API master endpoints

|*etcd1.example.com*
.3+.^|*etcd*

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[NOTE]
====
When specifying multiple *etcd* hosts, external *etcd* is installed and
configured. Clustering of {product-title}'s embedded *etcd* is not supported.
====

You can see these example hosts present in the *[masters]*, *[etcd]*, *[lb]*,
and *[nodes]* sections of the following example inventory file:

.Multiple Masters Using HAProxy Inventory File
====

----
# Create an OSEv3 group that contains the master, nodes, etcd, and lb groups.
# The lb group lets Ansible configure HAProxy as the load balancing solution.
# Comment lb out if your load balancer is pre-configured.
[OSEv3:children]
masters
nodes
etcd
lb

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

# Uncomment the following to enable htpasswd authentication; defaults to
# DenyAllPasswordIdentityProvider.
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# Native high availbility cluster method with optional load balancer.
# If no lb group is defined installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=openshift-cluster.example.com
openshift_master_cluster_public_hostname=openshift-cluster.example.com

# apply updated node defaults
openshift_node_kubelet_args={'pods-per-core': ['10'], 'max-pods': ['250'], 'image-gc-high-threshold': ['90'], 'image-gc-low-threshold': ['80']}

# override the default controller lease ttl
#osm_controller_lease_ttl=30

# enable ntp on masters to ensure proper failover
openshift_clock_enabled=true

# host group for masters
[masters]
master1.example.com
master2.example.com
master3.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# Specify load balancer host
[lb]
lb.example.com

# host group for nodes, includes region info
[nodes]
master[1:3].example.com
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
----
====

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[discrete]
[[multi-masters-single-etcd-using-native-ha]]
==== Multiple Masters with Master and etcd on the Same Host

The following describes an example environment for three
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[masters] with xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[*etcd*] on each host,
one HAProxy load balancer, and two
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
using the `native` HA method:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master1.example.com*
.3+.^|Master (clustered using native HA) and node with etcd on each host

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|HAProxy to load balance API master endpoints

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

You can see these example hosts present in the *[masters]*, *[etcd]*, *[lb]*,
and *[nodes]* sections of the following example inventory file:

====
----
# Create an OSEv3 group that contains the master, nodes, etcd, and lb groups.
# The lb group lets Ansible configure HAProxy as the load balancing solution.
# Comment lb out if your load balancer is pre-configured.
[OSEv3:children]
masters
nodes
etcd
lb

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
openshift_deployment_type=openshift-enterprise

# Uncomment the following to enable htpasswd authentication; defaults to
# DenyAllPasswordIdentityProvider.
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# Native high availbility cluster method with optional load balancer.
# If no lb group is defined installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=openshift-cluster.example.com
openshift_master_cluster_public_hostname=openshift-cluster.example.com

# override the default controller lease ttl
#osm_controller_lease_ttl=30

# host group for masters
[masters]
master1.example.com
master2.example.com
master3.example.com

# host group for etcd
[etcd]
master1.example.com
master2.example.com
master3.example.com

# Specify load balancer host
[lb]
lb.example.com

# host group for nodes, includes region info
[nodes]
master[1:3].example.com
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
----
====

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[[running-the-advanced-installation]]
== Running the Advanced Installation

After you have xref:configuring-ansible[configured Ansible] by defining an
inventory file in *_/etc/ansible/hosts_*, you can run the advanced installation
using the following playbook:

----
ifdef::openshift-enterprise[]
# ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook ~/openshift-ansible/playbooks/byo/config.yml
endif::[]
----

If for any reason the installation fails, before re-running the installer, see
xref:installer-known-issues[Known Issues] to check for any specific
instructions or workarounds.

[[advanced-verifying-the-installation]]
== Verifying the Installation

// tag::verifying-the-installation[]
After the installation completes:

. Verify that the master is started and nodes
are registered and reporting in *Ready* status. _On the master host_, run the
following as root:
+
----
# oc get nodes

NAME                        STATUS                     AGE
master.example.com          Ready,SchedulingDisabled   165d
node1.example.com           Ready                      165d
node2.example.com           Ready                      165d
----

. To verify that the web console is installed correctly, use the master host name
and the web console port number to access the web console with a web browser.
+
For example, for a master host with a host name of `master.openshift.com` and
using the default port of `8443`, the web console would be found at `\https://master.openshift.com:8443/console`.

. Now that the install has been verified, run the following command on each master
and node host to add the *atomic-openshift* packages back to the list of yum
excludes on the host:
+
----
# atomic-openshift-excluder exclude
----

// end::verifying-the-installation[]

[NOTE]
====
The default port for the console is `8443`. If this was changed during the installation, the port can be found at *openshift_master_console_port* in the *_/etc/ansible/hosts_* file.
====

[discrete]
[[verifying-multiple-etcd-hosts]]
==== Verifying Multiple etcd Hosts

If you installed multiple *etcd* hosts:

. On a master host, verify the *etcd* cluster health, substituting for the FQDNs
of your *etcd* hosts in the following:
+
----
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/origin/master/master.etcd-ca.crt \
    --cert-file=/etc/origin/master/master.etcd-client.crt \
    --key-file=/etc/origin/master/master.etcd-client.key cluster-health
----

. Also verify the member list is correct:
+
----
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/origin/master/master.etcd-ca.crt \
    --cert-file=/etc/origin/master/master.etcd-client.crt \
    --key-file=/etc/origin/master/master.etcd-client.key member list
----

[discrete]
[[verifying-multiple-masters-haproxy]]
==== Verifying Multiple Masters Using HAProxy

If you installed multiple masters using HAProxy as a load balancer, browse to
the following URL according to your *[lb]* section definition and check
HAProxy's status:

----
http://<lb_hostname>:9000
----

You can verify your installation by consulting the
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Load_Balancer_Administration/ch-haproxy-setup-VSA.html[HAProxy
Configuration documentation].

[[uninstalling-advanced]]
== Uninstalling {product-title}

You can uninstall {product-title} hosts in your cluster by running the
*_uninstall.yml_* playbook. This playbook deletes {product-title} content
installed by Ansible, including:

- Configuration
- Containers
- Default templates and image streams
- Images
- RPM packages

The playbook will delete content for any hosts defined in the inventory file
that you specify when running the playbook. If you want to uninstall
{product-title} across all hosts in your cluster, run the playbook using the
inventory file you used when installing {product-title} initially or ran most
recently:

----
ifdef::openshift-enterprise[]
# ansible-playbook [-i /path/to/file] \
    /usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook [-i /path/to/file] \
    ~/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
----

[[uninstalling-nodes-advanced]]
=== Uninstalling Nodes

You can also uninstall node components from specific hosts using the
*_uninstall.yml_* playbook while leaving the remaining hosts and cluster alone:

[WARNING]
====
This method should only be used when attempting to uninstall specific node hosts
and not for specific masters or etcd hosts, which would require further
configuration changes within the cluster.
====

. First follow the steps in
xref:../../admin_guide/manage_nodes.adoc#deleting-nodes[Deleting Nodes] to
remove the node object from the cluster, then continue with the remaining steps
in this procedure.

. Create a different inventory file that only references those hosts. For
example, to only delete content from one node:
+
----
[OSEv3:children]
nodes <1>

[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

[nodes]
node3.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}" <2>
----
<1> Only include the sections that pertain to the hosts you are interested in
uninstalling.
<2> Only include hosts that you want to uninstall.

. Specify that new inventory file using the `-i` option when running the
*_uninstall.yml_* playbook:
+
----
ifdef::openshift-enterprise[]
# ansible-playbook -i /path/to/new/file \
    /usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook -i /path/to/new/file \
    ~/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
----

When the playbook completes, all {product-title} content should be removed from
any specified hosts.

[[installer-known-issues]]
== Known Issues

The following are known issues for specified installation configurations.

*Multiple Masters*

- On failover, it is possible for the controller manager to overcorrect, which
causes the system to run more pods than what was intended. However, this is a
transient event and the system does correct itself over time. See
https://github.com/kubernetes/kubernetes/issues/10030 for details.

- On failure of the Ansible installer, you must start from a clean operating
system installation. If you are using virtual machines, start from a fresh
image. If you are using bare metal machines, run the following on all hosts:
+
----
# yum -y remove openshift openshift-* etcd docker docker-common

# rm -rf /etc/origin /var/lib/openshift /etc/etcd \
    /var/lib/etcd /etc/sysconfig/atomic-openshift* /etc/sysconfig/docker* \
    /root/.kube/config /etc/ansible/facts.d /usr/share/openshift
----

== What's Next?

Now that you have a working {product-title} instance, you can:

- xref:../../install_config/configuring_authentication.adoc#install-config-configuring-authentication[Configure
authentication]; by default, authentication is set to
ifdef::openshift-enterprise[]
xref:../../install_config/configuring_authentication.adoc#DenyAllPasswordIdentityProvider[Deny
All].
endif::[]
ifdef::openshift-origin[]
xref:../../install_config/configuring_authentication.adoc#AllowAllPasswordIdentityProvider[Allow
All].
endif::[]
- Deploy an xref:../registry/index.adoc#install-config-registry-overview[integrated Docker registry].
- Deploy a xref:../router/index.adoc#install-config-router-overview[router].
ifdef::openshift-origin[]
- xref:../../install_config/imagestreams_templates.adoc#install-config-imagestreams-templates[Populate your {product-title} installation]
with a useful set of Red Hat-provided image streams and templates.
endif::[]
