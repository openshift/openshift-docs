= Advanced Installation
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
For production environments, a reference configuration implemented using
http://www.ansible.com[Ansible] playbooks is available as the _advanced
installation_ method for installing OpenShift hosts. Familiarity with Ansible is
assumed, however you can use this configuration as a reference to create your
own implementation using the configuration management tool of your choosing.

While RHEL Atomic Host is supported for running containerized OpenShift
services, the advanced installation method utilizes Ansible, which is not
available in RHEL Atomic Host, and must therefore be run from
ifdef::openshift-enterprise[]
a RHEL 7 system.
endif::[]
ifdef::openshift-origin[]
a supported version of Fedora, CentOS, or RHEL.
endif::[]
The host initiating the installation does not need to be intended for inclusion
in the OpenShift cluster, but it can be.

ifdef::openshift-enterprise[]
Alternatively, you can use the link:quick_install.html[quick installation]
method if you prefer an interactive installation experience.
endif::[]

[[advanced-before-you-begin]]
== Before You Begin

Before installing OpenShift, you must first see the link:../../install_config/install/prerequisites.html[Prerequisites] topic to
prepare your hosts, which includes verifying system and environment requirements
per component type and properly installing and configuring Docker. It also
includes installing Ansible version 1.8.4 or later, as the advanced installation
method is based on Ansible playbooks and as such requires directly invoking
Ansible.

If you are interested in installing OpenShift using the containerized method
(optional for RHEL but required for RHEL Atomic Host), see
link:../../install_config/install/rpm_vs_containerized.html[RPM vs
Containerized] to ensure that you understand the differences between these
methods, then return to this topic to continue.

After following the instructions in the
link:../../install_config/install/prerequisites.html[Prerequisites] topic and
deciding between the RPM and containerized methods, you can continue in this
topic to link:#configuring-ansible[Configuring Ansible].

[[configuring-ansible]]

== Configuring Ansible

The *_/etc/ansible/hosts_* file is Ansible's inventory file for the playbook to
use during the installation. The inventory file describes the configuration for
your OpenShift cluster. You must replace the default contents of the file with
your desired configuration.

The following sections describe commonly-used variables to set in your inventory
file during an advanced installation, followed by example inventory files you
can use as a starting point for your installation. The examples describe various
environment topographies, including link:#multiple-masters[using multiple
masters for high availability]. You can choose an example that matches your
requirements, modify it to match your own environment, and use it as your
inventory file when link:#running-the-advanced-installation[running the advanced
installation].

[[configuring-host-variables]]
=== Configuring Host Variables

To assign environment variables to hosts during the Ansible installation, indicate
the desired variables in the *_/etc/ansible/hosts_* file after the host entry in
the *[masters]* or *[nodes]* sections. For example:

====
----
[masters]
ec2-52-6-179-239.compute-1.amazonaws.com openshift_public_hostname=ose3-master.public.example.com
----
====

The following table describes variables for use with the Ansible installer that
can be assigned to individual host entries:

[[advanced-host-variables]]
.Host Variables
[options="header"]
|===

|Variable |Purpose

|`*openshift_hostname*`
|This variable overrides the internal cluster host name for the system. Use this
when the system's default IP address does not resolve to the system host name.

|`*openshift_public_hostname*`
|This variable overrides the system's public host name. Use this for cloud
installations, or for hosts on networks using a network address translation
(NAT).

|`*openshift_ip*`
|This variable overrides the cluster internal IP address for the system. Use
this when using an interface that is not configured with the default route.

|`*openshift_public_ip*`
|This variable overrides the system's public IP address. Use this for cloud
installations, or for hosts on networks using a network address translation
(NAT).

|`*containerized*`
|If set to *true*, containerized OpenShift services are run on target master and
node hosts instead of installed using RPM packages. If set to *false* or unset,
the default RPM method is used. RHEL Atomic Host requires the containerized
method, and is automatically selected for you based on the detection of the
*_/run/ostree-booted_* file. See
link:../../install_config/install/rpm_vs_containerized.html[RPM vs
Containerized] for more details.
ifdef::openshift-enterprise[]
Containerized installations are supported starting in OSE 3.1.1.
endif::[]

|`*openshift_node_labels*`
|This variable adds labels to nodes during installation. See
link:#configuring-node-host-labels[Configuring Node Host Labels] for more
details.

|`*openshift_node_kubelet_args*`
|This variable is used to configure `kubeletArguments` on nodes, such as
arguments used in link:../../admin_guide/garbage_collection.html[container and
image garbage collection], and to
link:../../admin_guide/manage_nodes.html#configuring-node-resources[specify
resources per node]. `kubeletArguments` are key value pairs that are passed
directly to the Kubelet that match the
http://kubernetes.io/v1.1/docs/admin/kubelet.html[Kubelet's command line
arguments]. `kubeletArguments` are not migrated or validated and may become
invalid if used. These values override other settings in node configuration
which may cause invalid configurations. Example usage:
*{'image-gc-high-threshold': ['90'],'image-gc-low-threshold': ['80']}*.

|`*openshift_router_selector*`
|Default node selector for automatically deploying router pods. See
link:#configuring-node-host-labels[Configuring Node Host Labels] for details.

|`*openshift_registry_selector*`
|Default node selector for automatically deploying registry pods. See
link:#configuring-node-host-labels[Configuring Node Host Labels] for details.

|`*docker_log_options*`
|This variable configures container logging options that are supported by
Docker's `json-file` logging driver. See
link:../../install_config/install/prerequisites.html#managing-docker-container-logs[Managing
Docker Container Logs] for available options.

|===

[[configuring-cluster-variables]]
=== Configuring Cluster Variables

To assign environment variables during the Ansible install that apply more
globally to your OpenShift cluster overall, indicate the desired variables in
the *_/etc/ansible/hosts_* file on separate, single lines within the *[OSEv3:vars]*
section. For example:

====
----
[OSEv3:vars]

openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

osm_default_subdomain=apps.test.example.com
----
====

The following table describes variables for use with the Ansible installer that
can be assigned cluster-wide:

.Cluster Variables
[options="header", cols="1,2"]
|===

|Variable |Purpose

|`*ansible_ssh_user*`
|This variable sets the SSH user for the installer to use and defaults to
*root*. This user should allow SSH-based authentication
link:prerequisites.html#ensuring-host-access[without requiring a password]. If
using SSH key-based authentication, then the key should be managed by an SSH
agent.

|`*ansible_sudo*`
|If `*ansible_ssh_user*` is not *root*, this variable must be set to *true* and
the user must be configured for passwordless *sudo*.

|`*containerized*`
|If set to *true*, containerized OpenShift services are run on all target master
and node hosts in the cluster instead of installed using RPM packages. If set to
*false* or unset, the default RPM method is used. RHEL Atomic Host requires the
containerized method, and is automatically selected for you based on the
detection of the *_/run/ostree-booted_* file. See
link:../../install_config/install/rpm_vs_containerized.html[RPM vs
Containerized] for more details.
ifdef::openshift-enterprise[]
Containerized installations are supported starting in OSE 3.1.1.
endif::[]

|`*openshift_master_cluster_hostname*`
|This variable overrides the host name for the cluster, which defaults to the
host name of the master.

|`*openshift_master_cluster_public_hostname*`
|This variable overrides the public host name for the cluster, which defaults to
the host name of the master.

|`*openshift_master_cluster_method*`
|Optional. This variable defines the HA method when deploying multiple masters.
Can be either `native` or `pacemaker`. See link:#multiple-masters[Multiple
Masters] for more information.

|`*openshift_master_cluster_password*`
.3+.^a|These variables are only required when using the `pacemaker` HA method.

For `*openshift_master_cluster_vip*`, the virtual IP (VIP) is assigned to the
active master automatically, so the IP must be available in the cluster network.
This IP should be in the same network and able to communicate with any other
master, *etcd*, and node hosts' IP. See link:#multiple-masters[Multiple Masters]
for more information.

|`*openshift_master_cluster_vip*`

|`*openshift_master_cluster_public_vip*`

|`*os_sdn_network_plugin_name*`
|This variable configures which
link:../../architecture/additional_concepts/sdn.html[OpenShift SDN plug-in] to
use for the pod network, which defaults to *redhat/openshift-ovs-subnet* for the
standard SDN plug-in. Set the variable to *redhat/openshift-ovs-multitenant* to
use the multitenant plug-in.

|`*openshift_master_identity_providers*`
|This variable overrides the
link:../../install_config/configuring_authentication.html[identity provider], which
defaults to
link:../../install_config/configuring_authentication.html#DenyAllPasswordIdentityProvider[Deny
All].

|`*openshift_master_named_certificates*`
.2+.^|These variables are used to configure link:../../install_config/certificate_customization.html[custom certificates] which are deployed as part of the installation. See link:#advanced-install-custom-certificates[Configuring Custom Certificates] for more information.

|`*openshift_master_overwrite_named_certificates*`

|`*openshift_master_session_name*`
.4+.^|These variables override defaults for
link:../../install_config/configuring_authentication.html#session-options[session
options] in the OAuth configuration. See link:#advanced-install-session-options[Configuring Session Options] for more information.

|`*openshift_master_session_max_seconds*`

|`*openshift_master_session_auth_secrets*`

|`*openshift_master_session_encryption_secrets*`

|`*openshift_master_portal_net*`
|This variable configures the subnet in which
link:../../architecture/core_concepts/pods_and_services.html#services[services]
will be created within the
link:../../architecture/additional_concepts/sdn.html[OpenShift SDN]. This
network block should be a private block and must not conflict with any existing
network blocks in your infrastructure to which pods, nodes, or the master may
require access. Defaults to *172.30.0.0/16* and *cannot* be re-configured after
deployment.

|`*osm_default_subdomain*`
|This variable overrides the default subdomain to use for exposed
link:../../architecture/core_concepts/routes.html[routes].

|`*osm_default_node_selector*`
|This variable overrides the node selector that projects will use by default
when placing pods.

|`*osm_cluster_network_cidr*`
| This variable overrides the
link:../../architecture/additional_concepts/sdn.html#sdn-design-on-masters[SDN
cluster network] CIDR block. This is the network from which pod IPs are
assigned. This network block should be a private block and must not conflict
with existing network blocks in your infrastructure to which pods, nodes, or the
master may require access. Defaults to *10.128.0.0/14* and *cannot* be arbitrarily
re-configured after deployment, although certain changes to it can be made in
the link:../configuring_sdn.html#configuring-the-pod-network-on-masters[SDN
master configuration].

|`*osm_host_subnet_length*`
|This variable specifies the size of the per host subnet allocated for pod IPs
by
link:../../architecture/additional_concepts/sdn.html#sdn-design-on-masters[OpenShift
SDN]. Defaults to *9* which means that a subnet of size /23 is allocated to each
host; for example, given the default 10.128.0.0/14 cluster network, this will
allocate 10.128.0.0/23, 10.128.2.0/23, 10.128.4.0/23, and so on. This *cannot* be
re-configured after deployment.
|===

[[configuring-global-proxy]]

*Configuring Global Proxy Options*
If your hosts require use of a HTTP/HTTPS proxy in order to connect to external
hosts there are numerous components that need to be configured to use the proxy
including Masters, Nodes, Docker, and Builds. In order to simplify this configuration
the following ansible variables can be specified at a cluster level to apply
these settings uniformly accross your environment.

These variables can be set at a cluster level or if your infrastructure requires it
at a host level.

Please see link:../../admin_guide/build_defaults_overrides.html[Setting Build Defaults and Overrides]
for more information on how the proxy environment is defined for builds.

.Cluster Proxy Variables
[options="header"]
|===

|Variable |Purpose

|`*openshift_http_proxy*`
|This variable specifies the HTTP_PROXY environment variable for Masters, Nodes,
and the Docker Daemon.

|`*openshift_https_proxy*`
|This variable specifices the HTTPS_PROXY environment variable for Masters, Nodes,
and the Docker Daemon.

|`*openshift_no_proxy*`
|This variable is used to set the NO_PROXY environment variable for Masters, Nodes,
and the Docker Daemon. This value should be set to a comma separated list of
hostnames or wildcard hostnames that should not use the defined proxy. This list
will be augmented with the list of all defined OpenShift hostnames by default.

|`*openshift_no_proxy_auto_include_hostnames*`
|This boolean variable specifies whether or not the names of all defined OpenShift
hosts and *.cluster.local should be automatically appended to the NO_PROXY list.
Defaults to 'true', set it to 'false' to override this option.

|`*openshift_builddefaults_http_proxy*`
|This variable defines the HTTP_PROXY environment variable inserted into builds
via the BuildDefaults admission controller. If *openshift_http_proxy* is set this
variable will inherit that value, you only need to set this if you wish for your
Builds to use a different value.

|`*openshift_builddefaults_https_proxy*`
|This variable defines the HTTPS_PROXY environment variable inserted into builds
via the BuildDefaults admission controller. If *openshift_https_proxy* is set this
variable will inherit that value, you only need to set this if you wish for your
Builds to use a different value.

|`*openshift_builddefaults_no_proxy*`
|This variable defines the NO_PROXY environment variable inserted into builds via
the BuildDefaults admission controller. If *openshift_no_proxy* is set this variable
will inherit that value, you only need to set this if you wish for your Builds to
use a different value.

|`*openshift_builddefaults_git_http_proxy*`
|This variable defines the HTTP proxy used by the git clone operations during a
build defined via the BuildDefaults admission controller. If *openshift_builddefaults_http_proxy*
is set this variable will inherit that value, you only need to set this if you wish
for your git clone operations to use a different value.

|`*openshift_builddefaults_git_https_proxy*`
|This variable defines the HTTPS proxy used by the git clone operations during a
build defined via the BuildDefaults admission controller. If *openshift_builddefaults_https_proxy*
is set this variable will inherit that value, you only need to set this if you wish
for your git clone operations to use a different value.
|===

[[configuring-node-host-labels]]
=== Configuring Node Host Labels

You can assign
link:../../architecture/core_concepts/pods_and_services.html#labels[labels] to
node hosts during the Ansible install by configuring the *_/etc/ansible/hosts_*
file. Labels are useful for determining the placement of pods onto nodes using
the link:../../admin_guide/scheduler.html#configurable-predicates[scheduler].
Other than *region=infra* (discussed below), the actual label names and values
are arbitrary and can be assigned however you see fit per your cluster's
requirements.

To assign labels to a node host during an Ansible install, use the
`*openshift_node_labels*` variable with the desired labels added to the desired
node host entry in the *[nodes]* section. In the following example, labels are
set for a region called *primary* and a zone called *east*:

====
----
[nodes]
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
----
====

The `*openshift_router_selector*` and `*openshift_registry_selector*` Ansible
settings are set to *region=infra* by default:

====
----
# default selectors for router and registry services
# openshift_router_selector='region=infra'
# openshift_registry_selector='region=infra'
----
====

The default router and registry will be automatically deployed if nodes exist
that match the selector settings above. For example:

====
----
[nodes]
node1.example.com openshift_node_labels="{'region':'infra','zone':'default'}"
----
====

[[marking-masters-as-unschedulable-nodes]]
=== Marking Masters as Unschedulable Nodes

Any hosts you designate as masters during the installation process should also
be configured as nodes by adding them to the *[nodes]* section so that the
masters are configured as part of the
link:../../architecture/additional_concepts/networking.html#openshift-sdn[OpenShift
SDN].

However, in order to ensure that your masters are not burdened with running
pods, you can make them
link:../../admin_guide/manage_nodes.html#marking-nodes-as-unschedulable-or-schedulable[unschedulable]
by adding the `*openshift_scheduleable=false*` option any node that is also a
master. For example:

====
----
[nodes]
master.example.com openshift_node_labels="{'region':'infra','zone':'default'}" openshift_schedulable=false
----
====


[[advanced-install-session-options]]
=== Configuring Session Options

link:../../install_config/configuring_authentication.html#session-options[Session
options] in the OAuth configuration are configurable in the inventory file. By
default, Ansible populates a `*sessionSecretsFile*` with generated
authentication and encryption secrets so that sessions generated by one master
can be decoded by the others. The default location is
*_/etc/origin/master/session-secrets.yaml_*, and this file will only be
re-created if deleted on all masters.

You can set the session name and maximum number of seconds with
`*openshift_master_session_name*` and `*openshift_master_session_max_seconds*`:

====
----
openshift_master_session_name=ssn
openshift_master_session_max_seconds=3600
----
====

If provided, `*openshift_master_session_auth_secrets*` and
`*openshift_master_encryption_secrets*` must be equal length.

For `*openshift_master_session_auth_secrets*`, used to authenticate sessions
using HMAC, it is recommended to use secrets with 32 or 64 bytes:

====
----
openshift_master_session_auth_secrets=['DONT+USE+THIS+SECRET+b4NV+pmZNSO']
----
====

For `*openshift_master_encryption_secrets*`, used to encrypt sessions, secrets
must be 16, 24, or 32 characters long, to select AES-128, AES-192, or AES-256:

====
----
openshift_master_session_encryption_secrets=['DONT+USE+THIS+SECRET+b4NV+pmZNSO']
----
====

[[advanced-install-custom-certificates]]
=== Configuring Custom Certificates

link:../../install_config/certificate_customization.html[Custom serving
certificates] for the public host names of the OpenShift API and
link:../../architecture/infrastructure_components/web_console.html[web console]
can be deployed during an advanced installation and are configurable in the
inventory file.

[NOTE]
====
Custom certificates should only be configured for the host name associated with
the `*publicMasterURL*` which can be set using
`*openshift_master_cluster_public_hostname*`. Using a custom serving certificate
for the host name associated with the `*masterURL*`
(*`openshift_master_cluster_hostname`*) will result in TLS errors as
infrastructure components will attempt to contact the master API using the
internal `*masterURL*` host.
====

Certificate and key file paths can be configured using the
`*openshift_master_named_certificates*` cluster variable:

====
----
openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key"}]
----
====

File paths must be local to the system where Ansible will be run. Certificates
are copied to master hosts and are deployed within the
*_/etc/origin/master/named_certificates/_* directory.

Ansible detects a certificate's `Common Name` and `Subject Alternative Names`.
Detected names can be overridden by providing the `*"names"*` key when setting
`*openshift_master_named_certificates*`:

====
----
openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key", "names": ["public-master-host.com"]}]
----
====

Certificates configured using `*openshift_master_named_certificates*` are cached
on masters, meaning that each additional Ansible run with a different set of
certificates results in all previously deployed certificates remaining in place
on master hosts and within the master configuration file.

If you would like `*openshift_master_named_certificates*` to be overwritten with
the provided value (or no value), specify the
`*openshift_master_overwrite_named_certificates*` cluster variable:

====
----
openshift_master_overwrite_named_certificates=true
----
====

For a more complete example, consider the following cluster variables in an
inventory file:

====
----
openshift_master_cluster_method=native
openshift_master_cluster_hostname=lb.openshift.com
openshift_master_cluster_public_hostname=custom.openshift.com
----
====

To overwrite the certificates on a subsequent Ansible run, you could set the
following:

====
----
openshift_master_named_certificates=[{"certfile": "/root/STAR.openshift.com.crt", "keyfile": "/root/STAR.openshift.com.key"}, "names": ["custom.openshift.com"]}]
openshift_master_overwrite_named_certificates=true
----
====

[[single-master]]
== Single Master Examples

You can configure an environment with a single master and multiple nodes, and
either a single embedded *etcd* or multiple external *etcd* hosts.

[NOTE]
====
Moving from a single master cluster to multiple masters after installation is
not supported.
====

[[single-master-multi-node]]
*Single Master and Multiple Nodes*

The following table describes an example environment for a single
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[master] (with embedded *etcd*)
and two
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#node[nodes]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master.example.com*
|Master and node

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

You can see these example hosts present in the *[masters]* and *[nodes]*
sections of the following example inventory file:

.Single Master and Multiple Nodes Inventory File
====

----
# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=root

# If ansible_ssh_user is not root, ansible_sudo must be set to true
#ansible_sudo=true

ifdef::openshift-enterprise[]
deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
deployment_type=origin
endif::[]

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# host group for masters
[masters]
master.example.com

# host group for nodes, includes region info
[nodes]
master.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
----
====

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[[single-master-multi-etcd-multi-node]]
*Single Master, Multiple etcd, and Multiple Nodes*

The following table describes an example environment for a single
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[master],
three
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[*etcd*]
hosts, and two
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#node[nodes]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master.example.com*
|Master and node

|*etcd1.example.com*
.3+.^|*etcd*

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[NOTE]
====
When specifying multiple *etcd* hosts, external *etcd* is installed and
configured. Clustering of OpenShift's embedded *etcd* is not supported.
====

You can see these example hosts present in the *[masters]*, *[nodes]*, and
*[etcd]* sections of the following example inventory file:

.Single Master, Multiple etcd, and Multiple Nodes Inventory File
====

----
# Create an OSEv3 group that contains the masters, nodes, and etcd groups
[OSEv3:children]
masters
nodes
etcd

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
deployment_type=origin
endif::[]

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# host group for masters
[masters]
master.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# host group for nodes, includes region info
[nodes]
master.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
----
====

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[[multiple-masters]]
== Multiple Masters Examples

You can configure an environment with multiple masters, multiple *etcd* hosts,
and multiple nodes. Configuring
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#high-availability-masters[multiple
masters for high availability] (HA) ensures that the cluster has no single point
of failure.

[NOTE]
====
Moving from a single master cluster to multiple masters after installation is
not supported.
====

When configuring multiple masters, the advanced installation supports two high
availability (HA) methods:

.HA Master Methods
[cols="1,5"]
|===

|`native`
|Leverages the native HA master capabilities built into OpenShift and can be
combined with any load balancing solution. If a host is defined in the *[lb]*
section of the inventory file, Ansible installs and configures HAProxy
automatically as the load balancing solution. If no host is defined, it is
assumed you have pre-configured a load balancing solution of your choice to
balance the master API (port 8443) on all master hosts.

|`pacemaker`
|Configures Pacemaker as the load balancer for multiple masters.
ifdef::openshift-enterprise[]
Requires a High
Availability Add-on for Red Hat Enterprise Linux subscription, which is provided
separately from the OpenShift Enterprise subscription.
endif::[]
|===

[NOTE]
====
For more on these methods and the high availability master architecture, see
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[Kubernetes
Infrastructure].
====

To configure multiple masters, choose one of the above HA methods, and refer to
the relevant example section that follows.

[[multi-masters-using-native-ha]]
*Multiple Masters Using Native HA*

The following describes an example environment for three
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[masters],
one HAProxy load balancer, three
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[*etcd*]
hosts, and two
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#node[nodes]
using the `native` HA method:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master1.example.com*
.3+.^|Master (clustered using native HA) and node

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|HAProxy to load balance API master endpoints

|*etcd1.example.com*
.3+.^|*etcd*

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[NOTE]
====
When specifying multiple *etcd* hosts, external *etcd* is installed and
configured. Clustering of OpenShift's embedded *etcd* is not supported.
====

You can see these example hosts present in the *[masters]*, *[etcd]*, *[lb]*,
and *[nodes]* sections of the following example inventory file:

.Multiple Masters Using HAProxy Inventory File
====

----
# Create an OSEv3 group that contains the master, nodes, etcd, and lb groups.
# The lb group lets Ansible configure HAProxy as the load balancing solution.
# Comment lb out if your load balancer is pre-configured.
[OSEv3:children]
masters
nodes
etcd
lb

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
deployment_type=origin
endif::[]

# Uncomment the following to enable htpasswd authentication; defaults to
# DenyAllPasswordIdentityProvider.
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# Native high availbility cluster method with optional load balancer.
# If no lb group is defined installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=openshift-cluster.example.com
openshift_master_cluster_public_hostname=openshift-cluster.example.com

# override the default controller lease ttl
#osm_controller_lease_ttl=30

# host group for masters
[masters]
master1.example.com
master2.example.com
master3.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# Specify load balancer host
[lb]
lb.example.com

# host group for nodes, includes region info
[nodes]
master[1:3].example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
----
====

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

Note the following when using the `native` HA method:

- The advanced installation method does not currently support multiple HAProxy
load balancers in an active-passive setup. See the
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Load_Balancer_Administration/ch-lvs-overview-VSA.html[Load
Balancer Administration documentation] for post-installation amendments, or
use the `pacemaker` method if you require this capability.
- In a HAProxy setup, controller manager servers run as standalone processes.
They elect their active leader with a lease stored in *etcd*. The lease
expires after 30 seconds by default. If a failure happens on an active
controller server, it will take up to this number of seconds to elect another
leader. The interval can be configured with the `*osm_controller_lease_ttl*`
variable.

[[multi-masters-using-pacemaker]]
*Multiple Masters Using Pacemaker*

The following describes an example environment for three
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[masters],
three
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[*etcd*]
hosts, and two
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#node[nodes]
using the `pacemaker` HA method:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master1.example.com*
.3+.^|Master (clustered using Pacemaker) and node

|*master2.example.com*

|*master3.example.com*

|*etcd1.example.com*
.3+.^|*etcd*

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[NOTE]
====
When specifying multiple *etcd* hosts, external *etcd* is installed and
configured. Clustering of OpenShift's embedded *etcd* is not supported.
====

You can see these example hosts present in the *[masters]*, *[nodes]*, and
*[etcd]* sections of the following example inventory file:

.Multiple Masters Using Pacemaker Inventory File
====

----
# Create an OSEv3 group that contains the masters, nodes, and etcd groups
[OSEv3:children]
masters
nodes
etcd

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
deployment_type=origin
endif::[]

# Uncomment the following to enable htpasswd authentication; defaults to
# DenyAllPasswordIdentityProvider.
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# Pacemaker high availability cluster method.
# Pacemaker HA environment must be able to self provision the
# configured VIP. For installation openshift_master_cluster_hostname
# must resolve to the configured VIP.
openshift_master_cluster_method=pacemaker
openshift_master_cluster_password=openshift_cluster
openshift_master_cluster_vip=192.168.133.25
openshift_master_cluster_public_vip=192.168.133.25
openshift_master_cluster_hostname=openshift-cluster.example.com
openshift_master_cluster_public_hostname=openshift-cluster.example.com

# override the default controller lease ttl
#osm_controller_lease_ttl=30

# host group for masters
[masters]
master1.example.com
master2.example.com
master3.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# host group for nodes, includes region info
[nodes]
master[1:3].example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
----
====

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

Note the following when using this configuration:

- Installing multiple masters with Pacemaker requires that you
link:#configuring-fencing[configure a fencing device] after running the
installer.
- When specifying multiple masters, the installer handles creating and starting
the HA cluster. If during that process the `pcs status` command indicates that
an HA cluster already exists, the installer skips HA cluster configuration.

[[running-the-advanced-installation]]
== Running the Advanced Installation

After you have link:#configuring-ansible[configured Ansible] by defining an
inventory file in *_/etc/ansible/hosts_*, you can run the advanced installation
using the following playbook:

----
ifdef::openshift-enterprise[]
# ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook ~/openshift-ansible/playbooks/byo/config.yml
endif::[]
----

If for any reason the installation fails, before re-running the installer, see
link:#installer-known-issues[Known Issues] to check for any specific
instructions or workarounds.


[[configuring-fencing]]
== Configuring Fencing

If you installed OpenShift using a link:#multiple-masters[configuration for
multiple masters] with Pacemaker as a load balancer, you must configure a
fencing device. See
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Reference/ch-fencing-HAAR.html[Fencing:
Configuring STONITH] in the High Availability Add-on for Red Hat Enterprise
Linux documentation for instructions, then continue to
link:#advanced-verifying-the-installation[Verifying the Installation].

[[advanced-verifying-the-installation]]
== Verifying the Installation

// tag::verifying-the-installation[]
After the installation completes, verify that the master is started and nodes
are registered and reporting in *Ready* status. *On the master host*, run the
following as root:

====
----
# oc get nodes

NAME                      LABELS                                                                     STATUS
master.example.com        kubernetes.io/hostname=master.example.com,region=infra,zone=default        Ready,SchedulingDisabled
node1.example.com         kubernetes.io/hostname=node1.example.com,region=primary,zone=east          Ready
node2.example.com         kubernetes.io/hostname=node2.example.com,region=primary,zone=west          Ready
----
====
// end::verifying-the-installation[]

*Multiple etcd Hosts*

If you installed multiple *etcd* hosts:

. On a master host, verify the *etcd* cluster health, substituting for the FQDNs
of your *etcd* hosts in the following:
+
====
----
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/origin/master/master.etcd-ca.crt \
    --cert-file=/etc/origin/master/master.etcd-client.crt \
    --key-file=/etc/origin/master/master.etcd-client.key cluster-health
----
====

. Also verify the member list is correct:
+
====
----
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/origin/master/master.etcd-ca.crt \
    --cert-file=/etc/origin/master/master.etcd-client.crt \
    --key-file=/etc/origin/master/master.etcd-client.key member list
----
====

*Multiple Masters Using Pacemaker*

If you installed multiple masters using Pacemaker as a load balancer:

. On a master host, determine which host is currently running as the active
master:
+
----
# pcs status
----

. After determining the active master, put the specified host into standby mode:
+
----
# pcs cluster standby <host1_name>
----

. Verify the master is now running on another host:
+
----
# pcs status
----

. After verifying the master is running on another node, re-enable the host on standby for normal operation by running:
+
----
# pcs cluster unstandby <host1_name>
----

Red Hat recommends that you also verify your installation by consulting the
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html-single/High_Availability_Add-On_Reference/index.html[High
Availability Add-on for Red Hat Enterprise Linux documentation].

*Multiple Masters Using HAProxy*

If you installed multiple masters using HAProxy as a load balancer, browse to
the following URL according to your *[lb]* section definition and check
HAProxy's status:

----
http://<lb_hostname>:9000
----

You can verify your installation by consulting the
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Load_Balancer_Administration/ch-haproxy-setup-VSA.html[HAProxy
Configuration documentation].

[[adding-nodes-advanced]]
== Adding Nodes to an Existing Cluster

After your cluster is installed, you can install additional nodes and add them
to your cluster by running the *_scaleup.yml_* playbook. This playbook queries
the master, generates and distributes new certificates for the new nodes, then
runs the configuration playbooks on the new nodes only.

ifdef::openshift-enterprise[]
This process is similar to re-running the installer in the
link:../../install_config/install/quick_install.html#adding-nodes-or-reinstalling-quick[quick
installation method to add nodes], however you have more configuration options
available when using the advanced method and running the playbooks directly.
endif::[]

You must have an existing inventory file (for example, *_/etc/ansible/hosts_*)
that is representative of your current cluster configuration in order to run the
*_scaleup.yml_* playbook.
ifdef::openshift-enterprise[]
If you previously used the `atomic-openshift-installer` command to run your
installation, you can check *_~/.config/openshift/.ansible/hosts_* for the last
inventory file that the installer generated and use or modify that as needed as
your inventory file. You must then specify the file location with `-i` when
calling `ansible-playbook` later.
endif::[]

To add nodes to an existing cluster:

. Ensure you have the latest playbooks by updating the *atomic-openshift-utils*
package:
+
----
# yum update atomic-openshift-utils
----

. Edit your *_/etc/ansible/hosts_* file and add `new_nodes` to the
*[OSEv3:children]* section:
+
====
----
[OSEv3:children]
masters
nodes
new_nodes
----
====

. Then, create a *[new_nodes]* section much like the existing *[nodes]* section,
specifying host information for any new nodes you want to add. For example:
+
====
----
[nodes]
master[1:3].example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"

[new_nodes]
node3.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
----
====
+
See link:#advanced-host-variables[Configuring Host Variables] for more options.

. Now run the *_scaleup.yml_* playbook. If your inventory file is located
somewhere other than the default *_/etc/ansible/hosts_*, specify the location
with the `-i option`:
+
----
# ansible-playbook [-i /path/to/file] \
    /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/scaleup.yml
----

. After the playbook completes successfully,
link:#advanced-verifying-the-installation[verify the installation].

. Finally, move any hosts you had defined in the *[new_nodes]* section up into
the *[nodes]* section (but leave the *[new_nodes]* section definition itself in
place) so that subsequent runs using this inventory file are aware of the nodes
but do not handle them as new nodes. For example:
+
====
----
[nodes]
master[1:3].example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
node3.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"

[new_nodes]
----
====

[[uninstalling-advanced]]
== Uninstalling {product-title}

You can uninstall {product-title} hosts in your cluster by running the
*_uninstall.yml_* playbook. This playbook deletes {product-title} content
installed by Ansible, including:

- Configuration
- Containers
- Default templates and image streams
- Images
- RPM packages

The playbook will delete content for any hosts defined in the inventory file
that you specify when running the playbook. If you want to uninstall
{product-title} across all hosts in your cluster, run the playbook using the
inventory file you used when installing {product-title} initially or ran most
recently:

----
ifdef::openshift-enterprise[]
# ansible-playbook [-i /path/to/file] \
    /usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook [-i /path/to/file] \
    ~/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
----

[[uninstalling-nodes-advanced]]
=== Uninstalling Nodes

You can also uninstall node components from specific hosts using the
*_uninstall.yml_* playbook while leaving the remaining hosts and cluster alone:

[WARNING]
====
This method should only be used when attempting to uninstall specific node hosts
and not for specific masters or etcd hosts, which would require further
configuration changes within the cluster.
====

. First follow the steps in
link:../../admin_guide/manage_nodes.html#deleting-nodes[Deleting Nodes] to
remove the node object from the cluster, then continue with the remaining steps
in this procedure.

. Create a different inventory file that only references those hosts. For
example, to only delete content from one node:
+
====
----
[OSEv3:children]
nodes <1>

[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
deployment_type=origin
endif::[]

[nodes]
node3.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}" <2>
----
<1> Only include the sections that pertain to the hosts you are interested in
uninstalling.
<2> Only include hosts that you want to uninstall.
====

. Specify that new inventory file using the `-i` option when running the
*_uninstall.yml_* playbook:
+
----
ifdef::openshift-enterprise[]
# ansible-playbook -i /path/to/new/file \
    /usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook -i /path/to/new/file \
    ~/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
----

When the playbook completes, all {product-title} content should be removed from
any specified hosts.

[[installer-known-issues]]
== Known Issues

The following are known issues for specified installation configurations.

*Multiple Masters*

- On failover, it is possible for the controller manager to overcorrect, which
causes the system to run more pods than what was intended. However, this is a
transient event and the system does correct itself over time. See
https://github.com/GoogleCloudPlatform/kubernetes/issues/10030 for details.

- On failure of the Ansible installer, you must start from a clean operating
system installation. If you are using virtual machines, start from a fresh
image. If you are use bare metal machines:
+
. Run the following on a master host with Pacemaker:
+
----
# pcs cluster destroy --all
----
+
. Then, run the following on all node hosts:
+
----
# yum -y remove openshift openshift-* etcd docker

# rm -rf /etc/origin /var/lib/openshift /etc/etcd \
    /var/lib/etcd /etc/sysconfig/atomic-openshift* /etc/sysconfig/docker* \
    /root/.kube/config /etc/ansible/facts.d /usr/share/openshift
----

== What's Next?

Now that you have a working OpenShift instance, you can:

- link:../../install_config/configuring_authentication.html[Configure
authentication]; by default, authentication is set to
ifdef::openshift-enterprise[]
link:../../install_config/configuring_authentication.html#DenyAllPasswordIdentityProvider[Deny
All].
endif::[]
ifdef::openshift-origin[]
link:../../install_config/configuring_authentication.html#AllowAllPasswordIdentityProvider[Allow
All].
endif::[]
- Deploy an link:docker_registry.html[integrated Docker registry].
- Deploy a link:deploy_router.html[router].
ifdef::openshift-origin[]
- link:../../install_config/imagestreams_templates.html[Populate your OpenShift installation]
with a useful set of Red Hat-provided image streams and templates.
endif::[]
