[[install-config-install-prerequisites]]
= Prerequisites
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

ifdef::atomic-registry[]
[NOTE]
====
While {product-title} is based on OpenShift, some of these topics are irrelevant
to an {product-title} deployment. The following is provided for reference.
====
endif::[]

ifdef::openshift-origin[]
{product-title}
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#architecture-infrastructure-components-kubernetes-infrastructure[infrastructure
components] can be installed across multiple hosts. The following sections
outline the system requirements and instructions for preparing your environment
and hosts before installing {product-title}.
endif::[]

ifdef::openshift-enterprise[]
{product-title}
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#architecture-infrastructure-components-kubernetes-infrastructure[infrastructure
components] can be installed across multiple hosts. The following sections
outline the system requirements and instructions for preparing your environment
and hosts before installing {product-title}.
endif::[]

[[prerequisites-planning]]
== Planning

For production environments, several factors that can influence installation
must be considered prior to deployment:

* What is the number of required hosts required to run the cluster?
* How many pods are required in your cluster?
* Is xref:../../admin_guide/high_availability.adoc#admin-guide-high-availability[high availability] required?
High availability is recommended for fault tolerance.
* Which installation type do you want to use:
xref:../../install_config/install/rpm_vs_containerized.adoc#install-config-install-rpm-vs-containerized[RPM or
containerized]?


[[system-requirements]]

== System Requirements

ifdef::openshift-enterprise[]
You must have an active {product-title} subscription on your Red Hat
account to proceed. If you do not, contact your sales representative for more
information.

[IMPORTANT]
====
{product-title} 3.2 requires Docker 1.9.1, and supports Docker 1.10 as of
xref:../../release_notes/ose_3_2_release_notes.adoc#ose-3-2-1-1[{product-title}
3.2.1].
====
endif::[]

The system requirements vary per host type:

[cols="1,7"]
|===
|xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[Masters]
a|- Physical or virtual system, or an instance running on a public or private IaaS.
ifdef::openshift-origin[]
- Base OS: Fedora 21, CentOS 7.1, or RHEL 7.1 or later with "Minimal"
installation option, or RHEL Atomic Host 7.2.4 or later.
endif::[]
ifdef::openshift-enterprise[]
- Base OS: RHEL 7.1 or later with "Minimal" installation option, or RHEL Atomic
Host 7.2.4 or later.
endif::[]
- 2 vCPU.
- Minimum 8 GB RAM.
- Minimum 30 GB hard disk space for the file system containing *_/var/_*.

|xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[Nodes]
a| - Physical or virtual system, or an instance running on a public or private IaaS.
ifdef::openshift-origin[]
- Base OS: Fedora 21, CentOS 7.1, or RHEL 7.1 or later with "Minimal"
installation option, or RHEL Atomic Host 7.2.4 or later.
endif::[]
ifdef::openshift-enterprise[]
- Base OS: RHEL 7.1 or later with "Minimal" installation option, or RHEL Atomic
Host 7.2.4 or later.
endif::[]
- NetworkManager 1.0 or later
- 1 vCPU.
- Minimum 8 GB RAM.
- Minimum 15 GB hard disk space for the file system containing *_/var/_*.
- An additional minimum 15 GB unallocated space to be used for Docker's storage
back end; see xref:configuring-docker-storage[Configuring Docker Storage]
below.
|===

[IMPORTANT]
====
{product-title} only supports servers with x86_64 architecture.
====

[NOTE]
====
Meeting the *_/var/_* file system sizing requirements in RHEL Atomic Host
requires making changes to the default configuration. See
https://access.redhat.com/documentation/en/red-hat-enterprise-linux-atomic-host/version-7/getting-started-with-containers/#managing_storage_in_red_hat_enterprise_linux_atomic_host[Managing
Storage in Red Hat Enterprise Linux Atomic Host] for instructions on configuring
this during or after installation.
====

[[host-recommendations]]
=== Host Recommendations
The following apply to production environments. Test or sample environments will
function with the minimum requirements.

Master Hosts::
In a highly available {product-title} cluster with external etcd, a master host
should have 1 CPU core and 1.5 GB of memory, on top of the defaults in the table
above, for each 1000 pods. Therefore, the recommended size of master host in an
{product-title} cluster of 2000 pods would be 2 CPU cores and 5 GB of RAM, in
addition to the minimum requirements for a master host of 2 CPU cores and 8 GB of RAM.

When planning an environment with multiple masters, a minimum of three etcd
hosts as well as a load-balancer between the master hosts, is required.


Node Hosts::
The size of a node host depends on the expected size of its workload. As an
{product-title} cluster administrator, you will need to calculate the expected
workload, then add about 10% for overhead. For production environments, allocate
enough resources so that node host failure does not affect your maximum
capacity.

Use the above with the following table to plan the maximum loads for nodes and
pods:

[cols="4,2",options="header"]
|===
|Host |Sizing Recommendation

|Maximum nodes per cluster |300

|Maximum pods per nodes |110
|===

[IMPORTANT]
====
Oversubscribing the physical resources on a node affects resource guarantees the
Kubernetes scheduler makes during pod placement. Learn what measures you can
take to xref:../../admin_guide/overcommit.adoc#disabling-swap-memory[avoid memory swapping].
====

[[configuring-core-usage]]

=== Configuring Core Usage

By default, {product-title} masters and nodes use all available cores in the
system they run on. You can choose the number of cores you want {product-title}
to use by setting the https://golang.org/pkg/runtime/[`*GOMAXPROCS*` environment
variable].

For example, run the following before starting the server to make
{product-title} only run on one core:

====
----
# export GOMAXPROCS=1
----
====

ifdef::openshift-origin[]
Alternatively, if you plan to
xref:../../getting_started/administrators.adoc#running-in-a-docker-container[run
OpenShift in a container], add `-e GOMAXPROCS=1` to the `docker run`
command when launching the server.
endif::[]

[[security-warning]]

=== Security Warning

{product-title} runs
xref:../../architecture/core_concepts/containers_and_images.adoc#containers[containers] on your hosts, and in some cases, such as build operations and the
registry service, it does so using privileged containers. Furthermore, those
containers access your host's Docker daemon and perform `docker build` and
`docker push` operations. As such, you should be aware of the inherent security
risks associated with performing `docker run` operations on arbitrary images as
they effectively have root access.

For more information, see these articles:

- http://opensource.com/business/14/7/docker-security-selinux
- https://docs.docker.com/engine/security/security/

To address these risks, {product-title} uses
xref:../../architecture/additional_concepts/authorization.adoc#security-context-constraints[security
context constraints] that control the actions that pods can perform and what it
has the ability to access.

[[envirornment-requirements]]

== Environment Requirements

The following must be set up in your environment before {product-title} can be
installed.

[[prereq-dns]]
=== DNS

A fully functional DNS environment is a requirement for {product-title} to work
correctly. Adding entries into the *_/etc/hosts_* file is not enough, because
that file is not copied into containers running on the platform.

To configure the {product-title} DNS environment:

- xref:../../install_config/install/prerequisites.adoc#dns-config-prereq[Complete DNS configuration]
- (Optionally) xref:../../install_config/install/prerequisites.adoc#wildcard-dns-prereq[configure a wildcard for the router]

Key components of {product-title} run themselves inside of containers. By
default, these containers receive their *_/etc/resolv.conf_* DNS configuration
file from their host. {product-title} then inserts one DNS value into the pods
(above the node's nameserver values). That value is defined in the
*_/etc/origin/node/node-config.yaml_* file by the `*dnsIP*` parameter, which by
default is set to the address of the host node because the host is using
*dnsmasq*. If the `*dnsIP*` parameter is omitted from the *_node-config.yaml_*
file, then the value defaults to the kubernetes service IP, which is the first
nameserver in the pod's *_/etc/resolv.conf_* file.

As of {product-title}
ifdef::openshift-enterprise[]
3.2,
endif::[]
ifdef::openshift-origin[]
1.2,
endif::[]
*dnsmasq* is automatically configured on all masters and nodes. The pods use the
nodes as their DNS, and the nodes forward the requests. By default, *dnsmasq*
is configured on the nodes to listen on port 53, therefore the nodes cannot run
any other type of DNS application.

[NOTE]
====
Previously, in {product-title}
ifdef::openshift-enterprise[]
3.1,
endif::[]
ifdef::openshift-origin[]
1.1,
endif::[]
a DNS server could not be installed on a master node, because it ran its own
internal DNS server. Now, with master nodes using *dnsmasq*, SkyDNS is now
configured to listen on port 8053 so that *dnsmasq* can run on the masters. Note
that these DNS changes (*dnsmasq* configured on nodes and the SkyDNS port
change) only apply to new installations of {product-title} 3.2. Clusters
upgraded to {product-title}
ifdef::openshift-enterprise[]
3.2
endif::[]
ifdef::openshift-origin[]
1.2
endif::[]
from a previous version do not currently have these changes applied during the
upgrade process.
====

[NOTE]
====
*NetworkManager* is required on the nodes in order to populate *dnsmasq* with
the DNS IP addresses.
====

If you do not have a properly functioning DNS environment, you could experience failure with:

- Product installation via the reference Ansible-based scripts
- Deployment of the infrastructure containers (registry, routers)
- Access to the {product-title} web console, because it is not accessible via IP address alone


[[dns-config-prereq]]
*Configuring a DNS Environment*

To properly configure your DNS environment for {product-title}:

. Check the contents of *_/etc/resolv.conf_*:
+
----
$ cat /etc/resolv.conf
# Generated by NetworkManager
search ose3.example.com
nameserver 10.64.33.1
# nameserver updated by /etc/NetworkManager/dispatcher.d/99-origin-dns.sh
----
. Ensure that the DNS servers listed in *_/etc/resolv.conf_* are able to resolve to the addresses of all the masters and nodes in your {product-title} environment:
+
----
$ dig <node_hostname> @<IP_address> +short
----
+
For example:
+
----
$ dig node1.ose3.example.com @10.64.33.1 +short
10.64.33.156
$ dig master.ose3.example.com @10.64.33.1 +short
10.64.33.37
----
. If DHCP is:
+
- Disabled, then configure your network interface to be static, and add DNS nameservers to NetworkManager.
- Enabled, then the NetworkManager dispatch script automatically configures DNS based on the DHCP configuration. Optionally, you can add a value to `*dnsIP*` in the *_node-config.yaml_* file to prepend the pod's *_resolv.conf_* file. The second nameserver is then defined by the host's first nameserver. By default, this will be the IP address of the node host.
+
[NOTE]
====
For most configurations, do not set the `*openshift_dns_ip*` option during the
advanced installation of {product-title} (using Ansible), because this option
overrides the default IP address set by `*dnsIP*`.

Instead, allow the installer to configure each node to use *dnsmasq* and forward
requests to SkyDNS or the external DNS provider. If you do set the
`*openshift_dns_ip*` option, then it should be set either with a DNS IP that
queries SkyDNS first, or to the SkyDNS service or endpoint IP (the Kubernetes
service IP).
====


[[dns-config-prereq-disabling-dnsmasq]]
==== Disabling dnsmasq

If you want to disable *dnsmasq* (for example, if your *_/etc/resolv.conf_* is
managed by a configuration tool other than NetworkManager), then set
`openshift_use_dnsmasq` to *false* in the Ansible playbook.

However, certain containers do not properly move to the next nameserver when the
first issues *SERVFAIL*. Red Hat Enterprise Linux (RHEL)-based containers do not
suffer from this, but certain versions of *uclibc* and *musl* do.

[[wildcard-dns-prereq]]
*Configuring Wildcard DNS*

Optionally, configure a wildcard for the router to use, so that you do not need
to update your DNS configuration when new routes are added.

A wildcard for a DNS zone must ultimately resolve to the IP address of the
{product-title} xref:../../architecture/core_concepts/routes.adoc#routers[router].

For example, create a wildcard DNS entry for *cloudapps* that has a low
time-to-live value (TTL) and points to the public IP address of the host where
the router will be deployed:

----
*.cloudapps.example.com. 300 IN  A 192.168.133.2
----

In almost all cases, when referencing VMs you must use host names, and the host
names that you use must match the output of the `hostname -f` command on each
node.

[WARNING]
====
In your *_/etc/resolv.conf_* file on each node host, ensure that the DNS server
that has the wildcard entry is not listed as a nameserver or that the wildcard
domain is not listed in the search list. Otherwise, containers managed by
{product-title} may fail to resolve host names properly.
====


[[run-dns-diagnostics]]
*Running Diagnostics*

To explore your DNS setup and run specific DNS queries, you can use the `host` and `dig` commands (part of the `bind-utils` package). For example, you can query a specific DNS server, or check if recursion is involved.

----
$ host `hostname`
ose3-master.example.com has address 172.16.25.41

$ dig ose3-node1.example.com  +short
172.16.25.45
----


[[prereq-network-access]]
=== Network Access

A shared network must exist between the master and node hosts. If you plan to
configure
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#high-availability-masters[multiple
masters for high-availability] using the xref:advanced_install.adoc#install-config-install-advanced-install[advanced
installation method], you must also select an IP to be configured as your
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master-components[virtual
IP] (VIP) during the installation process. The IP that you select must be
routable between all of your nodes, and if you configure using a FQDN it should
resolve on all nodes.

[[prereq-networkmanager]]

*NetworkManager*

NetworkManager, a program for providing detection and configuration for systems
to automatically connect to the network, is required.

[[required-ports]]

*Required Ports*

{product-title} infrastructure components communicate with each other using
ports, which are communication endpoints that are identifiable for specific
processes or services. Ensure the following ports required by {product-title}
are open between hosts, for example if you have a firewall in your environment.
Some ports are optional depending on your configuration and usage.

.Node to Node
[cols='2,1,8']
|===
| *4789*
|UDP
|Required for SDN communication between pods on separate hosts.
|===

.Nodes to Master
[cols='2,1,8']
|===
| *53* or *8053*
|TCP/UDP
|Required for DNS resolution of cluster services (SkyDNS).
ifdef::openshift-origin[]
Installations prior to 1.2 or environments upgraded to 1.2 use port 53.
endif::[]
ifdef::openshift-enterprise[]
Installations prior to 3.2 or environments upgraded to 3.2 use port 53.
endif::[]
New installations will use 8053 by default so that *dnsmasq* may be configured.

| *4789*
|UDP
|Required for SDN communication between pods on separate hosts.

| *443* or *8443*
|TCP
|Required for node hosts to communicate to the master API, for the node hosts to
post back status, to receive tasks, and so on.
|===

.Master to Node
[cols='2,1,8']
|===
| *4789*
|UDP
|Required for SDN communication between pods on separate hosts.

| *10250*
|TCP
|The master proxies to node hosts via the Kubelet for `oc` commands.
|===

[NOTE]
====
In the following table,
*(L)* indicates the marked port is also used in _loopback mode_,
enabling the master to communicate with itself.

In a single-master cluster:

- Ports marked with *(L)* must be open.
- Ports not marked with *(L)* need not be open.

In a multiple-master cluster, all the listed ports must be open.
====

.Master to Master
[cols='2,1,8']
|===
| *53 (L)* or *8053* (L)
|TCP/UDP
|Required for DNS resolution of cluster services (SkyDNS).
ifdef::openshift-origin[]
Installations prior to 1.2 or environments upgraded to 1.2 use port 53.
endif::[]
ifdef::openshift-enterprise[]
Installations prior to 3.2 or environments upgraded to 3.2 use port 53.
endif::[]
New installations will use 8053 by default so that *dnsmasq* may be configured.

| *2049* (L)
|TCP/UDP
|Required when provisioning an NFS host as part of the installer.

| *2379*
|TCP
|Used for standalone etcd (clustered) to accept changes in state.

| *2380*
|TCP
|etcd requires this port be open between masters for leader election and peering
connections when using standalone etcd (clustered).

| *4001 (L)*
|TCP
|Used for embedded etcd (non-clustered) to accept changes in state.

| *4789 (L)*
|UDP
|Required for SDN communication between pods on separate hosts.

|===

.External to Load Balancer
[cols='2,1,8']
|===
| *9000*
|TCP
|If you choose the `*native*` HA method, optional to allow access to the HAProxy statistics page.

|===


.External to Master
[cols='2,1,8']
|===
| *443* or *8443*
|TCP
|Required for node hosts to communicate to the master API, for node hosts to
post back status, to receive tasks, and so on.
|===

.IaaS Deployments
[cols='2,1,8']
|===
| *22*
|TCP
| Required for SSH by the installer or system administrator.

| *53* or *8053*
|TCP/UDP
|Required for DNS resolution of cluster services (SkyDNS).
ifdef::openshift-origin[]
Installations prior to 1.2 or environments upgraded to 1.2 use port 53.
endif::[]
ifdef::openshift-enterprise[]
Installations prior to 3.2 or environments upgraded to 3.2 use port 53.
endif::[]
New installations will use 8053 by default so that *dnsmasq* may be configured.
Only required to be internally open on master hosts.

| *80* or *443*
|TCP
| For HTTP/HTTPS use for the router. Required to be externally open on node hosts, especially on nodes running the router.

| *1936*
|TCP
| For router statistics use. Required to be open when running the template
router to access statistics, and can be open externally or internally to
connections depending on if you want the statistics to be expressed publicly.

| *4001*
|TCP
| For embedded etcd (non-clustered) use. Only required to be internally open on
the master host. *4001* is for server-client connections.

| *2379* and *2380*
|TCP
| For standalone etcd use. Only required to be internally open on the master host.
*2379* is for server-client connections. *2380* is for server-server
connections, and is only required if you have clustered etcd.

| *4789*
|UDP
| For VxLAN use ({product-title} SDN). Required only internally on node hosts.

| *8443*
|TCP
| For use by the {product-title} web console, shared with the API server.

| *10250*
|TCP
| For use by the Kubelet. Required to be externally open on nodes.
|===

*Notes*

* In the above examples, port *4789* is used for User Datagram Protocol (UDP).
* When deployments are using the SDN, the pod network is accessed via a service proxy, unless it is accessing the registry from the same node the registry is deployed on.
* {product-title} internal DNS cannot be received over SDN. Depending on the detected values of `*openshift_facts*`, or if the `*openshift_ip*` and `*openshift_public_ip*` values are overridden, it will be the computed value of `*openshift_ip*`. For non-cloud deployments, this will default to the IP address associated with the default route on the master host. For cloud deployments, it will default to the IP address associated with the first internal interface as defined by the cloud metadata.
* The master host uses port *10250* to reach the nodes and does not go over SDN. It depends on the target host of the deployment and uses the computed values of `*openshift_hostname*` and `*openshift_public_hostname*`.

.Aggregated Logging
[cols='2,1,8']
|===
| *9200*
|TCP
|For Elasticsearch API use. Required to be internally open on any infrastructure
nodes so Kibana is able to retrieve logs for display. It can be externally
opened for direct access to Elasticsearch by means of a route. The route can be
created using `oc expose`.

| *9300*
|TCP
|For Elasticsearch inter-cluster use. Required to be internally open on any
infrastructure node so the members of the Elasticsearch cluster may communicate
with each other.
|===

[[prereq-git]]

=== Git Access

You must have either Internet access and a GitHub account, or read and write
access to an internal, HTTP-based Git server

[[prereq-persistent-storage]]

=== Persistent Storage

The Kubernetes
xref:../../architecture/additional_concepts/storage.adoc#architecture-additional-concepts-storage[persistent volume]
framework allows you to provision an {product-title} cluster with persistent storage
using networked storage available in your environment. This can be done after
completing the initial {product-title} installation depending on your application
needs, giving users a way to request those resources without having any
knowledge of the underlying infrastructure.

The xref:../../install_config/index.adoc#install-config-index[Installation and Configuration Guide]
provides instructions for cluster administrators on provisioning an {product-title}
cluster with persistent storage using
xref:../../install_config/persistent_storage/persistent_storage_nfs.adoc#install-config-persistent-storage-persistent-storage-nfs[NFS],
xref:../../install_config/persistent_storage/persistent_storage_glusterfs.adoc#install-config-persistent-storage-persistent-storage-glusterfs[GlusterFS],
xref:../../install_config/persistent_storage/persistent_storage_ceph_rbd.adoc#install-config-persistent-storage-persistent-storage-ceph-rbd[Ceph
RBD],
xref:../../install_config/persistent_storage/persistent_storage_cinder.adoc#install-config-persistent-storage-persistent-storage-cinder[OpenStack
Cinder],
xref:../../install_config/persistent_storage/persistent_storage_aws.adoc#install-config-persistent-storage-persistent-storage-aws[AWS Elastic Block Store (EBS)],
xref:../../install_config/persistent_storage/persistent_storage_gce.adoc#install-config-persistent-storage-persistent-storage-gce[GCE
Persistent Disks], and
xref:../../install_config/persistent_storage/persistent_storage_iscsi.adoc#install-config-persistent-storage-persistent-storage-iscsi[iSCSI].

[[prereq-selinux]]

=== SELinux

Security-Enhanced Linux (SELinux) must be enabled on all of the servers before
installing {product-title} or the installer will fail. Also, configure
`*SELINUXTYPE=targeted*` in the *_/etc/selinux/config_* file:

----
# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=enforcing
# SELINUXTYPE= can take one of these three values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected.
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted
----

[[prereq-cloud-provider-considerations]]

=== Cloud Provider Considerations

*Set up the Security Group*

When installing on AWS or OpenStack, ensure that you set up the appropriate
security groups. These are some ports that you should have in your security
groups, without which the installation will fail. You may need more depending on
the cluster configuration you want to install. For more information and to
adjust your security groups accordingly, see xref:required-ports[Required Ports]
for more information.


[cols="1,2"]
|===
|*All {product-title} Hosts*
a|- tcp/22 from host running the installer/Ansible

|*etcd Security Group*
a|- tcp/2379 from masters
- tcp/2380 from etcd hosts

|*Master Security Group*
a|- tcp/8443 from 0.0.0.0/0
ifdef::openshift-origin[]
- tcp/53 from all {product-title} hosts for environments installed prior to or upgraded to 1.2
- udp/53 from all {product-title} hosts for environments installed prior to or upgraded to 1.2
- tcp/8053 from all {product-title} hosts for new environments installed with 1.2
- udp/8053 from all {product-title} hosts for new environments installed with 1.2
endif::[]
ifdef::openshift-enterprise[]
- tcp/53 from all {product-title} hosts for environments installed prior to or upgraded to 3.2
- udp/53 from all {product-title} hosts for environments installed prior to or upgraded to 3.2
- tcp/8053 from all {product-title} hosts for new environments installed with 3.2
- udp/8053 from all {product-title} hosts for new environments installed with 3.2
endif::[]

|*Node Security Group*
a|- tcp/10250 from masters
- udp/4789 from nodes

|*Infrastructure Nodes*
(ones that can host the {product-title} router)
a|- tcp/443 from 0.0.0.0/0
- tcp/80 from 0.0.0.0/0

|===

If configuring ELBs for load balancing the masters and/or routers, you also need
to configure Ingress and Egress security groups for the ELBs appropriately.

*Override Detected IP Addresses and Host Names*

Some deployments require that the user override the detected host names and IP
addresses for the hosts. To see the default values, run the `*openshift_facts*`
playbook:

====
----
# ansible-playbook playbooks/byo/openshift_facts.yml
----
====

Now, verify the detected common settings. If they are not what you expect them
to be, you can override them.

The
xref:../../install_config/install/advanced_install.adoc#configuring-ansible[Advanced
Installation] topic discusses the available Ansible variables in greater detail.

[cols="1,2",options="header"]
|===
|Variable |Usage

|`*hostname*`
a| - Should resolve to the internal IP from the instances themselves.
- `*openshift_hostname*` overrides.

|`*ip*`
a| - Should be the internal IP of the instance.
- `*openshift_ip*` will overrides.

|`*public_hostname*`
a| - Should resolve to the external IP from hosts outside of the cloud.
- Provider `*openshift_public_hostname*` overrides.

|`*public_ip*`
a| - Should be the externally accessible IP associated with the instance.
- `*openshift_public_ip*` overrides.

|`*use_openshift_sdn*`
a| - Should be true unless the cloud is GCE.
- `*openshift_use_openshift_sdn*` overrides.

|===

[WARNING]
====
If `*openshift_hostname*` is set to a value other than the metadata-provided
`*private-dns-name*` value, the native cloud integration for those providers
will no longer work.
====

In AWS, situations that require overriding the variables include:

[cols="1,2"options="header"]
|===
|Variable |Usage

|`*hostname*`
a|The user is installing in a VPC that is not configured for both `*DNS hostnames*` and `*DNS resolution*`.

|`*ip*`
a|Possibly if they have multiple network interfaces configured and they want to
use one other than the default. You must first set
`*openshift_set_node_ip*` to `True`. Otherwise, the SDN would attempt to
use the `*hostname*` setting or try to resolve the host name for the IP.

|`*public_hostname*`
a| - A master instance where the VPC subnet is not configured for `*Auto-assign
Public IP*`. For external access to this master, you need to have an ELB or
other load balancer configured that would provide the external access needed, or
you need to connect over a VPN connection to the internal name of the host.
- A master instance where metadata is disabled.
- This value is not actually used by the nodes.

|`*public_ip*`
a| - A master instance where the VPC subnet is not configured for `*Auto-assign Public IP*`.
- A master instance where metadata is disabled.
- This value is not actually used by the nodes.

|===

If setting `*openshift_hostname*` to something other than the metadata-provided
`*private-dns-name*` value, the native cloud integration for those providers
will no longer work.

For EC2 hosts in particular, they must be deployed in a VPC that has both
`*DNS host names*` and `*DNS resolution*` enabled, and `*openshift_hostname*`
should not be overridden.

*Post-Installation Configuration for Cloud Providers*

Following the installation process, you can configure {product-title} for
xref:../../install_config/configuring_aws.adoc#install-config-configuring-aws[AWS],
xref:../../install_config/configuring_openstack.adoc#install-config-configuring-openstack[OpenStack], or
xref:../../install_config/configuring_gce.adoc#install-config-configuring-gce[GCE].

[[host-preparation]]

== Host Preparation

Before installing {product-title}, you must first prepare each host per the following.

ifdef::openshift-origin[]
[NOTE]
====
If you are using https://www.vagrantup.com[Vagrant] to run {product-title}, you
do not need to go through the following sections. These changes are only
necessary when you are setting up the host yourself. If you are using Vagrant,
see the
https://github.com/openshift/origin/blob/master/CONTRIBUTING.html#develop-on-virtual-machine-using-vagrant[Contributing
Guide], then you can skip directly to trying out the
xref:../../getting_started/administrators.adoc#try-it-out[sample applications].
====
endif::[]

ifdef::openshift-enterprise[]


[[software-prerequisites]]

=== Software Prerequisites

*Installing an Operating System*

A base installation of RHEL 7.1 or later or RHEL Atomic Host 7.2.4 or later is
required for master and node hosts. See the following documentation for the
respective installation instructions, if required:

- https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Installation_Guide/index.html[Red
Hat Enterprise Linux 7 Installation Guide]
- https://access.redhat.com/documentation/en/red-hat-enterprise-linux-atomic-host/version-7/installation-and-configuration-guide/[Red
Hat Enterprise Linux Atomic Host 7 Installation and Configuration Guide]

*Registering the Hosts*

Each host must be registered using Red Hat Subscription Manager (RHSM) and have
an active {product-title} subscription attached to access the required
packages.

. On each host, register with RHSM:
+
----
# subscription-manager register --username=<user_name> --password=<password>
----

. List the available subscriptions:
+
----
# subscription-manager list --available
----

. In the output for the previous command, find the pool ID for an {product-title} subscription and attach it:
+
----
# subscription-manager attach --pool=<pool_id>
----
+
[NOTE]
====
When finding the pool ID, the related subscription name might include either
"OpenShift Enterprise" or "OpenShift Container Platform", due to the product
name change introduced with version 3.3.
====

. Disable all repositories and enable only the required ones:
+
----
# subscription-manager repos --disable="*"
# subscription-manager repos \
    --enable="rhel-7-server-rpms" \
    --enable="rhel-7-server-extras-rpms" \
    --enable="rhel-7-server-ose-3.2-rpms"
----
endif::[]

*Managing Packages*

For RHEL 7 systems:

. Install the following base packages:
+
----
# yum install wget git net-tools bind-utils iptables-services bridge-utils bash-completion
----

. Update the system to the latest packages:
+
----
# yum update
----

ifdef::openshift-enterprise[]
. Install the following package, which provides {product-title} utilities and pulls in
other tools required by the
xref:../../install_config/install/quick_install.adoc#install-config-install-quick-install[quick] and
xref:../../install_config/install/advanced_install.adoc#install-config-install-advanced-install[advanced installation]
methods, such as Ansible and related configuration files:
+
----
# yum install atomic-openshift-utils
----

. Install the following **-excluder* packages on each RHEL 7 system, which helps
ensure your systems stay on the correct versions of *atomic-openshift* and
*docker* packages when you are not trying to upgrade, according to the
{product-title} version:
+
----
# yum install atomic-openshift-excluder atomic-openshift-docker-excluder
----

. The **-excluder* packages add entries to the `exclude` directive in the host's
*_/etc/yum.conf_* file when installed. Run the following command on each host to
remove the *atomic-openshift* packages from the list for the duration of the
installation.
+
----
# atomic-openshift-excluder unexclude
----
endif::[]

For RHEL Atomic Host 7 systems:

. Ensure the host is up to date by upgrading to the latest Atomic tree if one is
available:
+
----
# atomic host upgrade
----

. After the upgrade is completed and prepared for the next boot, reboot the
host:
+
----
# systemctl reboot
----


ifdef::openshift-origin[]
[[preparing-for-advanced-installations-origin]]

*Preparing for Advanced Installations*

If you plan to use the
xref:../../install_config/install/advanced_install.adoc#install-config-install-advanced-install[advanced installation]
method, you must install Ansible and clone the *openshift-ansible* repository from
GitHub, which provides the required playbooks and configuration files.

For convenience, the following steps are provided if you want to use EPEL as a
package source for Ansible:

. Install the EPEL repository:
+
----
# yum -y install \
    https://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm
----

. Disable the EPEL repository globally so that it is not accidentally used during
later steps of the installation:
+
----
# sed -i -e "s/^enabled=1/enabled=0/" /etc/yum.repos.d/epel.repo
----

. Install the packages for Ansible:
+
----
# yum -y --enablerepo=epel install ansible pyOpenSSL
----

To clone the *openshift-ansible* repository:

----
# cd ~
# git clone https://github.com/openshift/openshift-ansible
# cd openshift-ansible
----

[NOTE]
====
Be sure to stay on the *master* branch of the *openshift-ansible* repository
when running an advanced installation.
====
endif::[]

[[installing-docker]]

*Installing Docker*

At this point, you should install Docker on all master and node hosts. This
allows you to configure your xref:configuring-docker-storage[Docker storage
options] before installing {product-title}.

. For RHEL 7 systems, install Docker 1.10.
+
[NOTE]
====
On RHEL Atomic Host 7 systems, Docker should already be installed, configured,
and running by default.
====
+
ifdef::openshift-enterprise[]
The *atomic-openshift-docker-excluder* package that was installed in
xref:software-prerequisites[Software Prerequisites] should ensure that the
correct version of Docker is installed in this step:
+
endif::[]
----
# yum install docker
----
+
After the package installation is complete, verify that version 1.10.3 was
installed:
+
----
# docker version
----

. Edit the *_/etc/sysconfig/docker_* file and add `--insecure-registry
172.30.0.0/16` to the `*OPTIONS*` parameter. For example:
+
----
OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'
----
+
If using the
xref:../../install_config/install/quick_install.adoc#install-config-install-quick-install[Quick
Installation] method, you can easily script a complete installation from a
kickstart or cloud-init setup, change the default configuration file:
+
----
# sed -i '/OPTIONS=.*/c\OPTIONS="--selinux-enabled --insecure-registry 172.30.0.0/16"' \
/etc/sysconfig/docker
----
+
[[NOTE]]
====
The
xref:../../install_config/install/advanced_install.adoc#install-config-install-advanced-install[Advanced
Installation] method automatically changes *_/etc/sysconfig/docker_*.
====
+
The `--insecure-registry` option instructs the Docker daemon to trust any Docker
registry on the indicated subnet, rather than
xref:docker_registry.adoc#securing-the-registry[requiring a certificate].
+
[IMPORTANT]
====
172.30.0.0/16 is the default value of the `*servicesSubnet*` variable in the
*_master-config.yaml_* file. If this has changed, then the `--insecure-registry`
value in the above step should be adjusted to match, as it is indicating the
subnet for the registry to use. Note that the `*openshift_master_portal_net*`
variable can be set in the Ansible inventory file and used during the
xref:advanced_install.adoc#configuring-ansible[advanced installation]
method to modify the `*servicesSubnet*` variable.
====
+
[NOTE]
====
After the initial {product-title} installation is complete, you can choose to
xref:docker_registry.adoc#securing-the-registry[secure the integrated Docker
registry], which involves adjusting the `--insecure-registry` option
accordingly.
====

[[configuring-docker-storage]]

=== Configuring Docker Storage

Docker containers and the images they are created from are stored in Docker's
storage back end. This storage is ephemeral and separate from any
xref:../../dev_guide/persistent_volumes.adoc#dev-guide-persistent-volumes[persistent storage] allocated to
meet the needs of your applications.

*For RHEL Atomic Host*

The default storage back end for Docker on RHEL Atomic Host is a thin pool
logical volume, which is supported for production environments. You must ensure
that enough space is allocated for this volume per the Docker storage
requirements mentioned in
xref:../../install_config/install/prerequisites.adoc#system-requirements[System
Requirements].

If you do not have enough allocated, see
https://access.redhat.com/documentation/en/red-hat-enterprise-linux-atomic-host/version-7/getting-started-with-containers/#managing_storage_with_docker_formatted_containers[Managing
Storage with Docker Formatted Containers] for details on using
*docker-storage-setup* and basic instructions on storage management in RHEL
Atomic Host.

*For RHEL*

The default storage back end for Docker on RHEL 7 is a thin pool on loopback
devices, which is not supported for production use and only appropriate for
proof of concept environments. For production environments, you must create a
thin pool logical volume and re-configure Docker to use that volume.

You can use the *docker-storage-setup* script included with Docker to create a
thin pool device and configure Docker's storage driver. This can be done after
installing Docker and should be done before creating images or containers. The
script reads configuration options from the
*_/etc/sysconfig/docker-storage-setup_* file and supports three options for
creating the logical volume:

- *Option A)* Use an additional block device.
- *Option B)* Use an existing, specified volume group.
- *Option C)* Use the remaining free space from the volume group where your root
file system is located.

Option A is the most robust option, however it requires adding an additional
block device to your host before configuring Docker storage. Options B and C
both require leaving free space available when provisioning your host.

. Create the *docker-pool* volume using one of the following three options:

** [[docker-storage-a]]*Option A) Use an additional block device.*
+
In *_/etc/sysconfig/docker-storage-setup_*, set *DEVS* to the path of the block
device you wish to use. Set *VG* to the volume group name you wish to create;
*docker-vg* is a reasonable choice. For example:
+
====
----
# cat <<EOF > /etc/sysconfig/docker-storage-setup
DEVS=/dev/vdc
VG=docker-vg
EOF
----
====
+
Then run *docker-storage-setup* and review the output to ensure the
*docker-pool* volume was created:
+
====
----
# docker-storage-setup                                                                                                                                                                                                                                [5/1868]
0
Checking that no-one is using this disk right now ...
OK

Disk /dev/vdc: 31207 cylinders, 16 heads, 63 sectors/track
sfdisk:  /dev/vdc: unrecognized partition table type

Old situation:
sfdisk: No partitions found

New situation:
Units: sectors of 512 bytes, counting from 0

   Device Boot    Start       End   #sectors  Id  System
/dev/vdc1          2048  31457279   31455232  8e  Linux LVM
/dev/vdc2             0         -          0   0  Empty
/dev/vdc3             0         -          0   0  Empty
/dev/vdc4             0         -          0   0  Empty
Warning: partition 1 does not start at a cylinder boundary
Warning: partition 1 does not end at a cylinder boundary
Warning: no primary partition is marked bootable (active)
This does not matter for LILO, but the DOS MBR will not boot this disk.
Successfully wrote the new partition table

Re-reading the partition table ...

If you created or changed a DOS partition, /dev/foo7, say, then use dd(1)
to zero the first 512 bytes:  dd if=/dev/zero of=/dev/foo7 bs=512 count=1
(See fdisk(8).)
  Physical volume "/dev/vdc1" successfully created
  Volume group "docker-vg" successfully created
  Rounding up size to full physical extent 16.00 MiB
  Logical volume "docker-poolmeta" created.
  Logical volume "docker-pool" created.
  WARNING: Converting logical volume docker-vg/docker-pool and docker-vg/docker-poolmeta to pool's data and metadata volumes.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  Converted docker-vg/docker-pool to thin pool.
  Logical volume "docker-pool" changed.
----
====

** [[docker-storage-b]]*Option B) Use an existing, specified volume group.*
+
In *_/etc/sysconfig/docker-storage-setup_*, set *VG* to the desired volume
group. For example:
+
====
----
# cat <<EOF > /etc/sysconfig/docker-storage-setup
VG=docker-vg
EOF
----
====
+
Then run *docker-storage-setup* and review the output to ensure the
*docker-pool* volume was created:
+
====
----
# docker-storage-setup
  Rounding up size to full physical extent 16.00 MiB
  Logical volume "docker-poolmeta" created.
  Logical volume "docker-pool" created.
  WARNING: Converting logical volume docker-vg/docker-pool and docker-vg/docker-poolmeta to pool's data and metadata volumes.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  Converted docker-vg/docker-pool to thin pool.
  Logical volume "docker-pool" changed.
----
====

** [[docker-storage-c]]*Option C) Use the remaining free space from the volume
 group where your root file system is located.*
+
Verify that the volume group where your root file system resides has the desired
free space, then run *docker-storage-setup* and review the output to ensure the
*docker-pool* volume was created:
+
====
----
# docker-storage-setup
  Rounding up size to full physical extent 32.00 MiB
  Logical volume "docker-poolmeta" created.
  Logical volume "docker-pool" created.
  WARNING: Converting logical volume rhel/docker-pool and rhel/docker-poolmeta to pool's data and metadata volumes.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  Converted rhel/docker-pool to thin pool.
  Logical volume "docker-pool" changed.
----
====

. Verify your configuration. You should have a *dm.thinpooldev* value in the
*_/etc/sysconfig/docker-storage_* file and a *docker-pool* logical volume:
+
====
----
# cat /etc/sysconfig/docker-storage
DOCKER_STORAGE_OPTIONS=--storage-opt dm.fs=xfs --storage-opt
dm.thinpooldev=/dev/mapper/docker--vg-docker--pool

# lvs
  LV          VG   Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  docker-pool rhel twi-a-t---  9.29g             0.00   0.12
----
====
+
[IMPORTANT]
====
Before using Docker or {product-title}, verify that the *docker-pool* logical volume
is large enough to meet your needs. The *docker-pool* volume should be 60% of
the available volume group and will grow to fill the volume group via LVM
monitoring.
====

. Check if Docker is running:
+
----
# systemctl is-active docker
----

. If Docker has not yet been started on the host, enable and start the service:
+
----
# systemctl enable docker
# systemctl start docker
----
+
If Docker is already running, re-initialize Docker:
+
[WARNING]
====
This will destroy any Docker containers or images currently on the host.
====
+
----
# systemctl stop docker
# rm -rf /var/lib/docker/*
# systemctl restart docker
----
+
If there is any content in *_/var/lib/docker/_*, it must be deleted. Files
will be present if Docker has been used prior to the installation of {product-title}.

[[reconfiguring-docker-storage]]
*Reconfiguring Docker Storage*

Should you need to reconfigure Docker storage after having created the
*docker-pool*, you should first remove the *docker-pool* logical volume. If you
are using a dedicated volume group, you should also remove the volume group and
any associated physical volumes before reconfiguring *docker-storage-setup*
according to the instructions above.

See
link:https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Logical_Volume_Manager_Administration/index.html[Logical
Volume Manager Administration] for more detailed information on LVM management.

[[managing-docker-container-logs]]
*Managing Docker Container Logs*

Sometimes a container's log file (the
*_/var/lib/docker/containers/<hash>/<hash>-json.log_* file on the node where the
container is running) can increase to a problematic size. You can manage this by
configuring Docker's `json-file` logging driver to restrict the size and number
of log files.

[options="header"]
|===

|Option |Purpose

|`--log-opt max-size`
|Sets the size at which a new log file is created.

|`--log-opt max-file`
|Sets the file on each host to configure the options.
|===

For example, to set the maximum file size to 1MB and always keep the last three
log files, edit the *_/etc/sysconfig/docker_* file to configure `max-size=1M`
and `max-file=3`:
====
----
OPTIONS='--insecure-registry=172.30.0.0/16 --selinux-enabled --log-opt max-size=1M --log-opt max-file=3'
----
====

Next, restart the Docker service:
----
# systemctl restart docker
----

[[viewing-available-container-logs]]
*Viewing Available Container Logs*

Container logs are stored in the *_/var/lib/docker/containers/<hash>/_*
directory on the node where the container is running. For example:
====
----
# ls -lh /var/lib/docker/containers/f088349cceac173305d3e2c2e4790051799efe363842fdab5732f51f5b001fd8/
total 2.6M
-rw-r--r--. 1 root root 5.6K Nov 24 00:12 config.json
-rw-r--r--. 1 root root 649K Nov 24 00:15 f088349cceac173305d3e2c2e4790051799efe363842fdab5732f51f5b001fd8-json.log
-rw-r--r--. 1 root root 977K Nov 24 00:15 f088349cceac173305d3e2c2e4790051799efe363842fdab5732f51f5b001fd8-json.log.1
-rw-r--r--. 1 root root 977K Nov 24 00:15 f088349cceac173305d3e2c2e4790051799efe363842fdab5732f51f5b001fd8-json.log.2
-rw-r--r--. 1 root root 1.3K Nov 24 00:12 hostconfig.json
drwx------. 2 root root    6 Nov 24 00:12 secrets
----
====

See Docker's documentation for additional information on how to
http://docs.docker.com/engine/reference/logging/overview/#the-json-file-options[Configure
Logging Drivers].

[[ensuring-host-access]]

=== Ensuring Host Access

ifdef::openshift-origin[]
The xref:advanced_install.adoc#install-config-install-advanced-install[advanced installation] method requires
endif::[]
ifdef::openshift-enterprise[]
The xref:quick_install.adoc#install-config-install-quick-install[quick] and xref:advanced_install.adoc#install-config-install-advanced-install[advanced
installation] methods require
endif::[]
a user that has access to all hosts. If you want to run the installer as a
non-root user, passwordless *sudo* rights must be configured on each destination
host.

For example, you can generate an SSH key on the host where you will invoke the
installation process:

----
# ssh-keygen
----

Do *not* use a password.

An easy way to distribute your SSH keys is by using a `bash` loop:

----
# for host in master.example.com \
    node1.example.com \
    node2.example.com; \
    do ssh-copy-id -i ~/.ssh/id_rsa.pub $host; \
    done
----

Modify the host names in the above command according to your configuration.

[[setting-global-proxy]]
== Setting Global Proxy Values

The {product-title} installer uses the proxy settings
in the *_/etc/environment _* file.

Ensure the following domain suffixes and IP addresses are in the *_/etc/environment_* file in the `no_proxy` parameter:

* Master and node host names (domain suffix).
* Other internal host names (domain suffix).
* Etcd IP addresses (must be IP addresses and not host names, as *etcd* access is done by IP address).
* Docker registry IP address.
* Kubernetes IP address, by default 172.30.0.1. Must be the value set in the
xref:../../install_config/install/advanced_install.adoc#advanced-install-networking-variables-table[`openshift_portal_net`] parameter in the 
Ansible inventory file, by default *_/etc/ansible/hosts_*.
* Kubernetes internal domain suffix: `cluster.local`. 
* Kubernetes internal domain suffix: `.svc`.

The following example assumes `http_proxy` and `https_proxy` values are set:

----
no_proxy=.internal.example.com,10.0.0.1,10.0.0.2,10.0.0.3,.cluster.local,.svc,localhost,127.0.0.1,172.30.0.1
----

[NOTE]
====
Because `noproxy` does not support CIDR, you can use domain suffixes.
====

== What's Next?

ifdef::openshift-enterprise[]
If you are interested in installing {product-title} using the containerized method
(optional for RHEL but required for RHEL Atomic Host), see
xref:../../install_config/install/rpm_vs_containerized.adoc#install-config-install-rpm-vs-containerized[RPM vs
Containerized] to ensure that you understand the differences between these
methods.

When you are ready to proceed, you can install {product-title} using the
xref:quick_install.adoc#install-config-install-quick-install[quick installation] or
xref:advanced_install.adoc#install-config-install-advanced-install[advanced installation] method.
endif::[]

ifdef::openshift-origin[]
If you are interested in installing {product-title} using the containerized method
(optional for Fedora, CentOS, or RHEL but required for RHEL Atomic Host), see
xref:../../install_config/install/rpm_vs_containerized.adoc#install-config-install-rpm-vs-containerized[RPM vs
Containerized] to ensure that you understand the differences between the
installation methods. Then continue with your chosen installation method.

If you came here from xref:../../getting_started/administrators.adoc#getting-started-administrators[Getting
Started for Administrators], you can now continue there by choosing an
xref:../../getting_started/administrators.adoc#installation-methods[installation
method]. Alternatively, you can install {product-title} using the
xref:advanced_install.adoc#install-config-install-advanced-install[advanced installation] method.
endif::[]
