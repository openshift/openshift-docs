[[install-config-install-prerequisites]]
= Prerequisites
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

ifdef::atomic-registry[]
[NOTE]
====
While {product-title} is based on OpenShift, some of these topics are irrelevant
to an {product-title} deployment. The following is provided for reference.
====
endif::[]

ifdef::openshift-origin[]
OpenShift
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html[infrastructure
components] can be installed across multiple hosts. The following sections
outline the system requirements and instructions for preparing your environment
and hosts before installing OpenShift.
endif::[]

ifdef::openshift-enterprise[]
OpenShift
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html[infrastructure
components] can be installed across multiple hosts. The following sections
outline the system requirements and instructions for preparing your environment
and hosts before installing OpenShift.
endif::[]

[[system-requirements]]

== System Requirements

ifdef::openshift-enterprise[]
You must have an active OpenShift Enterprise subscription on your Red Hat
account to proceed. If you do not, contact your sales representative for more
information.

[IMPORTANT]
====
OpenShift Enterprise (OSE) 3.x requires Red Hat Enterprise Linux (RHEL) 7.1 or
higher. OSE 3.1.x requires Docker 1.8.2 or higher. Upgrade the Docker packages
in RHEL 7.1 before installing or upgrading to OSE 3.1.
====
endif::[]

The system requirements vary per host type:

[cols="1,7"]
|===
|link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[Masters]
a|- Physical or virtual system, or an instance running on a public IaaS
ifdef::openshift-origin[]
- Base OS: Fedora 21, CentOS 7.1, or RHEL 7.1 or later with "Minimal"
installation option
endif::[]
ifdef::openshift-enterprise[]
- Base OS: RHEL 7.1 or later with "Minimal"
installation option
endif::[]
- 2 vCPU
- Minimum 8 GB RAM
- Minimum 30 GB hard disk space

|link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#node[Nodes]
a| - Physical or virtual system, or an instance running on a public IaaS
ifdef::openshift-origin[]
- Base OS: Fedora 21, CentOS 7.1, or RHEL 7.1 or later with "Minimal"
installation option
endif::[]
ifdef::openshift-enterprise[]
- Base OS: RHEL 7.1 or later with "Minimal"
installation option
endif::[]
- 1 vCPU
- Minimum 8 GB RAM
- Minimum 15 GB hard disk space
- An additional minimum 15 GB unallocated space to be configured using
*docker-storage-setup*; see link:#configuring-docker-storage[Configuring
Docker Storage] below.

|===

[[configuring-core-usage]]

=== Configuring Core Usage

By default, OpenShift masters and nodes use all available cores in the system they run on.
You can choose the number of cores you want OpenShift to use by setting the
https://golang.org/pkg/runtime/[`*GOMAXPROCS*` environment variable].

For example, run the following before starting the server to make OpenShift only
run on one core:

====
----
# export GOMAXPROCS=1
----
====

ifdef::openshift-origin[]
Alternatively, if you plan to
link:../../getting_started/administrators.html#running-in-a-docker-container[run
OpenShift in a Docker container], add `-e GOMAXPROCS=1` to the `docker run`
command when launching the server.
endif::[]

[[security-warning]]

=== Security Warning

OpenShift runs
link:../../architecture/core_concepts/containers_and_images.html#containers[Docker
containers] on your hosts, and in some cases, such as build operations and the
registry service, it does so using privileged containers. Furthermore, those
containers access your host's Docker daemon and perform `docker build` and
`docker push` operations. As such, you should be aware of the inherent security
risks associated with performing `docker run` operations on arbitrary images as
they effectively have root access.

For more information, see these articles:

- http://opensource.com/business/14/7/docker-security-selinux
- https://docs.docker.com/articles/security/

To address these risks, OpenShift uses
link:../../architecture/additional_concepts/authorization.html#security-context-constraints[security
context constraints] that control the actions that pods can perform and what it
has the ability to access.

[[envirornment-requirements]]

== Environment Requirements

The following must be set up in your environment before OpenShift can be
installed.

[[prereq-dns]]

=== DNS

A wildcard for a DNS zone must ultimately resolve to the IP address of the
OpenShift link:../../architecture/core_concepts/routes.html#routers[router].

For example, create a wildcard DNS entry for *cloudapps*, or something similar,
that has a low TTL and points to the public IP address of the host where the
router will be deployed:

----
*.cloudapps.example.com. 300 IN  A 192.168.133.2
----

In almost all cases, when referencing VMs you must use host names, and the host
names that you use must match the output of the `hostname -f` command on each
node.

[[prereq-network-access]]

=== Network Access

A shared network must exist between the master and node hosts. If you plan to
configure
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#high-availability-masters[multiple
masters for high-availability] using the link:advanced_install.html[advanced
installation method], you must also select an IP to be configured as your
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master-components[virtual
IP] (VIP) during the installation process. The IP that you select must be
routable between all of your nodes, and if you configure using a FQDN it should
resolve on all nodes.

[[required-ports]]

*Required Ports*

OpenShift infrastructure components communicate with each other using ports,
which are communication endpoints that are identifiable for specific processes
or services. Ensure the following ports required by OpenShift are open between hosts,
for example if you have a firewall in your environment. Some ports are optional
depending on your configuration and usage.

.Node to Node
[cols='2,8']
|===
| *4789*
|Required for SDN communication between pods on separate hosts.
|===

.Nodes to Master
[cols='2,8']
|===
| *53*
|Required for SDN communication between pods on separate hosts.

| *4789*
|Required for SDN communication between pods on separate hosts.

| *443* or *8443*
|Required for node hosts to communicate to the master API, for the node hosts to
post back status, to receive tasks, and so on.
|===

.Master to Node
[cols='2,8']
|===
| *4789*
|Required for SDN communication between pods on separate hosts.

| *10250*
|The master proxies to node hosts via the Kubelet for `oc` commands.
|===

.Master to Master
[cols='2,8']
|===
| *53*
|Provides DNS services.

| *2379*
|Used for standalone etcd (clustered) to accept changes in state.

| *2380*
|etcd requires this port be open between masters for leader election and peering
connections when using standalone etcd (clustered).

| *4001*
|Used for embedded etcd (non-clustered) to accept changes in state.

| *4789*
|Required for SDN communication between pods on separate hosts.


|===

.External to Master
[cols='2,8']
|===
| *443* or *8443*
|Required for node hosts to communicate to the master API, for node hosts to
post back status, to receive tasks, and so on.
|===

.IaaS Deployments
[cols='2,8']
|===
| *22*
| Required for SSH by the installer or system administrator.

| *53*
| For SkyDNS use. Only required to be internally open on master hosts.

| *80* or *443*
| For HTTP/HTTPS use for the router. Required to be externally open on node hosts, especially on nodes running the router.

| *1936*
| For router statistics use. Required to be open when running the template
router to access statistics, and can be open externally or internally to
connections depending on if you want the statistics to be expressed publicly.

| *4001*
| For embedded etcd (non-clustered) use. Only required to be internally open on
the master host. *4001* is for server-client connections.

| *2379* and *2380*
| For standalone etcd use. Only required to be internally open on the master host.
*2379* is for server-client connections. *2380* is for server-server
connections, and is only required if you have clustered etcd.

| *4789*
| For VxLAN use (OpenShift SDN). Required only internally on node hosts.

| *8443*
| For use by the OpenShift web console, shared with the API server.

| *10250*
| For use by the Kubelet. Required to be externally open on nodes.

| *24224*
| For use by Fluentd. Required to be open on master hosts for internal
connections to node hosts.
|===

*Notes*

* In the above examples, port *4789* is used for User Datagram Protocol (UDP).
* When deployments are using the SDN, the pod network is accessed via a service proxy, unless it is accessing the registry from the same node the registry is deployed on.
* OpenShift internal DNS cannot be received over SDN. Depending on the detected values of `*openshift_facts*`, or if the `*openshift_ip*` and `*openshift_public_ip*` values are overridden, it will be the computed value of `*openshift_ip*`. For non-cloud deployments, this will default to the IP address associated with the default route on the master host. For cloud deployments, it will default to the IP address associated with the first internal interface as defined by the cloud metadata.
* The master host uses port *10250* to reach the nodes and does not go over SDN. It depends on the target host of the deployment and uses the computed values of `*openshift_hostname*` and `*openshift_public_hostname*`.

[[prereq-git]]

=== Git Access

You must have either Internet access and a GitHub account, or read and write
access to an internal, HTTP-based Git server.

[[prereq-persistent-storage]]

=== Persistent Storage

The Kubernetes
link:../../architecture/additional_concepts/storage.html[persistent volume]
framework allows you to provision an OpenShift cluster with persistent storage
using networked storage available in your environment. This can be done after
completing the initial OpenShift installation depending on your application
needs, giving users a way to request those resources without having any
knowledge of the underlying infrastructure.

The link:../../install_config/index.html[Installation and Configuration Guide]
provides instructions for cluster administrators on provisioning an OpenShift
cluster with persistent storage using
link:../../install_config/persistent_storage/persistent_storage_nfs.html[NFS],
link:../../install_config/persistent_storage/persistent_storage_glusterfs.html[GlusterFS],
link:../../install_config/persistent_storage/persistent_storage_ceph_rbd.html[Ceph
RBD],
link:../../install_config/persistent_storage/persistent_storage_cinder.html[OpenStack
Cinder],
link:../../install_config/persistent_storage/persistent_storage_aws.html[AWS Elastic Block Store (EBS)],
link:../../install_config/persistent_storage/persistent_storage_gce.html[GCE
Persistent Disks], and
link:../../install_config/persistent_storage/persistent_storage_iscsi.html[iSCSI].

[[host-preparation]]

== Host Preparation

Before installing OpenShift, you must first prepare each host per the following.

ifdef::openshift-origin[]
[NOTE]
====
If you are using https://www.vagrantup.com[Vagrant] to run OpenShift Origin, you
do not need to go through the following sections. These changes are only
necessary when you are setting up the host yourself. If you are using Vagrant,
see the
https://github.com/openshift/origin/blob/master/CONTRIBUTING.adoc#develop-on-virtual-machine-using-vagrant[Contributing
Guide], then you can skip directly to trying out the
link:../../getting_started/administrators.html#try-it-out[sample applications].
====
endif::[]

ifdef::openshift-enterprise[]

[[software-prerequisites]]

=== Software Prerequisites

*Installing Red Hat Enterprise Linux 7*

A base installation of Red Hat Enterprise Linux (RHEL) 7.1 or later is required
for master or node hosts. See the
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Installation_Guide/index.html[Red
Hat Enterprise Linux 7 Installation Guide] for more information.

*Registering the Hosts*

Each host must be registered using Red Hat Subscription Manager (RHSM) and have
an active OpenShift Enterprise subscription attached to access the required
packages.

. On each host, register with RHSM:
+
----
# subscription-manager register --username=<user_name> --password=<password>
----

. List the available subscriptions:
+
----
# subscription-manager list --available
----

. In the output for the previous command, find the pool ID for an OpenShift
Enterprise subscription and attach it:
+
----
# subscription-manager attach --pool=<pool_id>
----
+
If you plan to configure
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#high-availability-masters[multiple
masters] with the link:advanced_install.html[advanced installation] using the
`pacemaker` HA method, you must also attach a subscription for
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Overview/index.html[High
Availability Add-on for Red Hat Enterprise Linux]:
+
----
# subscription-manager attach --pool=<pool_id_for_rhel_ha>
----
+
[NOTE]
====
The High Availability Add-on for Red Hat Enterprise Linux subscription is
provided separately from the OpenShift Enterprise subscription.
====

. Disable all repositories and enable only the required ones:
+
----
# subscription-manager repos --disable="*"
# subscription-manager repos \
    --enable="rhel-7-server-rpms" \
    --enable="rhel-7-server-extras-rpms" \
    --enable="rhel-7-server-ose-3.1-rpms"
----
+
If you plan to use the `pacemaker` HA method, enable the following repository as
well:
+
----
# subscription-manager repos \
    --enable="rhel-ha-for-rhel-7-server-rpms"
----
endif::[]

*Managing Packages*

. Install the following base packages:
+
----
# yum install wget git net-tools bind-utils iptables-services bridge-utils bash-completion
----

. Update the system to the latest packages:
+
----
# yum update
----

ifdef::openshift-enterprise[]
. Install the following package, which provides OpenShift utilities and pulls in
other tools required by the
link:../../install_config/install/quick_install.html[quick] and
link:../../install_config/install/advanced_install.html[advanced installation]
methods, such as Ansible and related configuration files:
+
----
# yum install atomic-openshift-utils
----
endif::[]

ifdef::openshift-origin[]
[[preparing-for-advanced-installations-origin]]

*Preparing for Advanced Installations*

If you plan to use the
link:../../install_config/install/advanced_install.html[advanced installation]
method, you must install Ansible and clone the *openshift-ansible* repository from
GitHub, which provides the required playbooks and configuration files.

For convenience, the following steps are provided if you want to use EPEL as a
package source for Ansible:

. Install the EPEL repository:
+
----
# yum -y install \
    https://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm
----

. Disable the EPEL repository globally so that it is not accidentally used during
later steps of the installation:
+
----
# sed -i -e "s/^enabled=1/enabled=0/" /etc/yum.repos.d/epel.repo
----

. Install the packages for Ansible:
+
----
# yum -y --enablerepo=epel install ansible
----

To clone the *openshift-ansible* repository:

----
# cd ~
# git clone https://github.com/openshift/openshift-ansible
# cd openshift-ansible
----

[NOTE]
====
Be sure to stay on the *master* branch of the *openshift-ansible* repository
when running an advanced installation.
====
endif::[]

*Installing Docker*

Docker version 1.8.2 or later must be installed and running on master and node
hosts before installing OpenShift.

. Install Docker:
+
----
# yum install docker
----

. Edit the *_/etc/sysconfig/docker_* file and add `--insecure-registry
172.30.0.0/16` to the `*OPTIONS*` parameter. For example:
+
----
OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'
----
+
The `--insecure-registry` option instructs the Docker daemon to trust any Docker
registry on the indicated subnet, rather than
link:docker_registry.html#securing-the-registry[requiring a certificate].
+
After installing OpenShift, you can choose to
link:docker_registry.html#securing-the-registry[secure the integrated Docker
registry], which involves adjusting the `--insecure-registry` option
accordingly.
+
[IMPORTANT]
172.30.0.0/16 is the default value of the `*servicesSubnet*` variable in the
*_master-config.yaml_* file. If this has changed, then the `--insecure-registry`
value in the above step should be adjusted to match, as it is indicating the
subnet for the registry to use. Note that the `*openshift_master_portal_net*`
variable can be set in the Ansible inventory file and used during the
link:advanced_install.html#configuring-ansible[advanced installation]
method to modify the `*servicesSubnet*` variable.


[[configuring-docker-storage]]

=== Configuring Docker Storage

Docker containers and the images they are created from are stored in Docker's
storage back end. This storage is ephemeral and separate from any
link:../../dev_guide/persistent_volumes.html[persistent storage] allocated to
meet the needs of your applications.

The default storage back end is a thin pool on loopback devices which is not
supported for production use and only appropriate for proof of concept
environments. For production environments, you must create a thin pool logical
volume and re-configure Docker to use that volume.

You can use the *docker-storage-setup* script included with Docker to create a
thin pool device and configure Docker's storage driver. This can be done after
installing Docker and should be done before creating images or containers. The
script reads configuration options from the
*_/etc/sysconfig/docker-storage-setup_* file and supports three options for
creating the logical volume:

- *Option A)* Use an additional block device.
- *Option B)* Use an existing, specified volume group.
- *Option C)* Use the remaining free space from the volume group where your root
file system is located.

Option A is the most robust option, however it requires adding an additional
block device to your host before configuring Docker storage. Options B and C
both require leaving free space available when provisioning your host.

[NOTE]
====
See the https://access.redhat.com/articles/1492923[Managing Storage with Docker
Formatted Containers on Red Hat Enterprise Linux and Red Hat Enterprise Linux
Atomic Host] Knowledgebase article for more details about *docker-storage-setup*
and basic instructions on storage management in Red Hat Enterprise Linux 7.
====

. Create the *docker-pool* volume using one of the following three options:

** [[docker-storage-a]]*Option A) Use an additional block device.*
+
In *_/etc/sysconfig/docker-storage-setup_*, set *DEVS* to the path of the block
device you wish to use. Set *VG* to the volume group name you wish to create;
*docker-vg* is a reasonable choice. For example:
+
====
----
# cat <<EOF > /etc/sysconfig/docker-storage-setup
DEVS=/dev/vdc
VG=docker-vg
EOF
----
====
+
Then run *docker-storage-setup* and review the output to ensure the
*docker-pool* volume was created:
+
====
----
# docker-storage-setup                                                                                                                                                                                                                                [5/1868]
0
Checking that no-one is using this disk right now ...
OK

Disk /dev/vdc: 31207 cylinders, 16 heads, 63 sectors/track
sfdisk:  /dev/vdc: unrecognized partition table type

Old situation:
sfdisk: No partitions found

New situation:
Units: sectors of 512 bytes, counting from 0

   Device Boot    Start       End   #sectors  Id  System
/dev/vdc1          2048  31457279   31455232  8e  Linux LVM
/dev/vdc2             0         -          0   0  Empty
/dev/vdc3             0         -          0   0  Empty
/dev/vdc4             0         -          0   0  Empty
Warning: partition 1 does not start at a cylinder boundary
Warning: partition 1 does not end at a cylinder boundary
Warning: no primary partition is marked bootable (active)
This does not matter for LILO, but the DOS MBR will not boot this disk.
Successfully wrote the new partition table

Re-reading the partition table ...

If you created or changed a DOS partition, /dev/foo7, say, then use dd(1)
to zero the first 512 bytes:  dd if=/dev/zero of=/dev/foo7 bs=512 count=1
(See fdisk(8).)
  Physical volume "/dev/vdc1" successfully created
  Volume group "docker-vg" successfully created
  Rounding up size to full physical extent 16.00 MiB
  Logical volume "docker-poolmeta" created.
  Logical volume "docker-pool" created.
  WARNING: Converting logical volume docker-vg/docker-pool and docker-vg/docker-poolmeta to pool's data and metadata volumes.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  Converted docker-vg/docker-pool to thin pool.
  Logical volume "docker-pool" changed.
----
====

** [[docker-storage-b]]*Option B) Use an existing, specified volume group.*
+
In *_/etc/sysconfig/docker-storage-setup_*, set *VG* to the desired volume
group. For example:
+
====
----
# cat <<EOF > /etc/sysconfig/docker-storage-setup
VG=docker-vg
EOF
----
====
+
Then run *docker-storage-setup* and review the output to ensure the
*docker-pool* volume was created:
+
====
----
# docker-storage-setup
  Rounding up size to full physical extent 16.00 MiB
  Logical volume "docker-poolmeta" created.
  Logical volume "docker-pool" created.
  WARNING: Converting logical volume docker-vg/docker-pool and docker-vg/docker-poolmeta to pool's data and metadata volumes.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  Converted docker-vg/docker-pool to thin pool.
  Logical volume "docker-pool" changed.
----
====

** [[docker-storage-c]]*Option C) Use the remaining free space from the volume
 group where your root file system is located.*
+
Verify that the volume group where your root file system resides has the desired
free space, then run *docker-storage-setup* and review the output to ensure the
*docker-pool* volume was created:
+
====
----
# docker-storage-setup
  Rounding up size to full physical extent 32.00 MiB
  Logical volume "docker-poolmeta" created.
  Logical volume "docker-pool" created.
  WARNING: Converting logical volume rhel/docker-pool and rhel/docker-poolmeta to pool's data and metadata volumes.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  Converted rhel/docker-pool to thin pool.
  Logical volume "docker-pool" changed.
----
====

. Verify your configuration. You should have a *dm.thinpooldev* value in the
*_/etc/sysconfig/docker-storage_* file and a *docker-pool* logical volume:
+
====
----
# cat /etc/sysconfig/docker-storage
DOCKER_STORAGE_OPTIONS=--storage-opt dm.fs=xfs --storage-opt
dm.thinpooldev=/dev/mapper/docker--vg-docker--pool

# lvs
  LV          VG   Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  docker-pool rhel twi-a-t---  9.29g             0.00   0.12
----
====
+
[IMPORTANT]
====
Before using Docker or OpenShift, verify that the *docker-pool* logical volume
is large enough to meet your needs. The *docker-pool* volume should be 60% of
the available volume group and will grow to fill the volume group via LVM
monitoring.
====

. Re-initialize Docker.
+
[WARNING]
====
This will destroy any Docker containers or images currently on the host.
====
+
----
# systemctl stop docker
# rm -rf /var/lib/docker/*
# systemctl restart docker
----

[[reconfiguring-docker-storage]]
*Reconfiguring Docker Storage*

Should you need to reconfigure Docker storage after having created the
*docker-pool*, you should first remove the *docker-pool* logical volume. If you
are using a dedicated volume group, you should also remove the volume group and
any associated physical volumes before reconfiguring *docker-storage-setup*
according to the instructions above.

See
link:https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Logical_Volume_Manager_Administration/index.html[Logical
Volume Manager Administration] for more detailed information on LVM management.

[[ensuring-host-access]]

== Ensuring Host Access

ifdef::openshift-origin[]
The link:advanced_install.html[advanced installation] method requires
endif::[]
ifdef::openshift-enterprise[]
The link:quick_install.html[quick] and link:advanced_install.html[advanced
installation] methods require
endif::[]
a user that has access to all hosts. If you want to run the installer as a
non-root user, passwordless *sudo* rights must be configured on each destination
host.

For example, you can generate an SSH key on the host where you will invoke the
installation process:

----
# ssh-keygen
----

Do *not* use a password.

An easy way to distribute your SSH keys is by using a `bash` loop:

----
# for host in master.example.com \
    node1.example.com \
    node2.example.com; \
    do ssh-copy-id -i ~/.ssh/id_rsa.pub $host; \
    done
----

Modify the host names in the above command according to your configuration.

== What's Next?

ifdef::openshift-enterprise[]
Now that your environment and hosts are properly set up, you can install
OpenShift Enterprise using the link:quick_install.html[quick installation] or
link:advanced_install.html[advanced installation] method.
endif::[]

ifdef::openshift-origin[]
If you came here from link:../../getting_started/administrators.html[Getting
Started for Administrators], you can now continue there by choosing an
link:../../getting_started/administrators.html#installation-methods[installation
method]. Alternatively, you can install OpenShift using the
link:advanced_install.html[advanced installation] method.
endif::[]
