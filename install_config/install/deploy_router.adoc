[[install-config-install-deploy-router]]
= Deploying a Router
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
The {product-title} xref:../../architecture/core_concepts/routes.adoc#architecture-core-concepts-routes[router] is
the ingress point for all external traffic destined for
xref:../../architecture/core_concepts/pods_and_services.adoc#services[services]
in your {product-title} installation. {product-title} provides and supports the
following two router plug-ins:

- The
xref:../../architecture/core_concepts/routes.adoc#haproxy-template-router[HAProxy
template router] is the default plug-in. It uses the
ifdef::openshift-enterprise[]
*openshift3/ose-haproxy-router*
endif::[]
ifdef::openshift-origin[]
*openshift/origin-haproxy-router*
endif::[]
 image to run an HAProxy instance alongside the template router plug-in inside a
container on {product-title}. It currently supports HTTP(S) traffic and TLS-enabled
traffic via SNI. The router's container listens on the host network interface,
unlike most containers that listen only on private IPs. The router proxies
external requests for route names to the IPs of actual pods identified by the
service associated with the route.

- The xref:../../architecture/core_concepts/routes.adoc#f5-router[F5 router]
integrates with an existing *F5 BIG-IP®* system in your environment to
synchronize routes. *F5 BIG-IP®* version 11.4 or newer is required in order to
have the F5 iControl REST API.

ifdef::openshift-enterprise[]
[NOTE]
====
The F5 router plug-in is available starting in {product-title} 3.0.2.
====
endif::[]

[[creating-the-router-service-account]]

ifdef::openshift-enterprise[]
== Router Service Account
Before deploying an {product-title} cluster, you must have a service account for the
router. Starting in {product-title} 3.1, a router
xref:../../admin_guide/service_accounts.adoc#admin-guide-service-accounts[service account]
is automatically created during a quick or advanced installation (previously, this required manual creation). This service account has permissions to a
xref:../../architecture/additional_concepts/authorization.adoc#security-context-constraints[security context constraint]
(SCC) that allows it to specify host ports.
endif::[]

ifdef::openshift-origin[]
== Creating the Router Service Account
You must first create a
xref:../../admin_guide/service_accounts.adoc#admin-guide-service-accounts[service account]
for the router before deploying. This service account must have permissions to a
xref:../../architecture/additional_concepts/authorization.adoc#security-context-constraints[security
context constraint] (SCC) that allows it to specify host ports.

To create a service account named *router* in the *default* namespace:

====
----
$ oc create serviceaccount router -n default
----
====

To add a privileged SCC to the *router* service account in the *default* namespace:

====
----
$ oadm policy add-scc-to-user privileged system:serviceaccount:default:router
----
====

Use of labels requires `cluster-reader` permission. So if the router definition sets
`NAMESPACE_LABELS` then you need to give it the following permission.

====
----
$ oadm policy add-cluster-role-to-user cluster-reader system:serviceaccount:default:router
----
====
endif::[]

[[haproxy-router]]
== Deploying the Default HAProxy Router
The `oadm router` command is provided with the administrator CLI to simplify the
tasks of setting up routers in a new installation.
This command creates the service and deployment configuration objects.
Just about every form of
communication between {product-title} components is secured by TLS and uses various
certificates and authentication methods. Use the `--credentials` option to
specify what credentials the router should use to contact the master.

[IMPORTANT]
====
Routers directly attach to port 80 and 443 on all interfaces on a host. Restrict
routers to hosts where port 80/443 is available and not being consumed by
another service, and set this using node selectors and the
xref:../../admin_guide/scheduler.adoc#admin-guide-scheduler[scheduler configuration]. As an example, you can
achieve this by dedicating infrastructure nodes to run services such as routers.
====

[IMPORTANT]
====
It is recommended to use separate distinct *openshift-router* credentials
with your router. The credentials can be provided using the `--credentials`
flag to the `oadm router` command. Alternatively, the default cluster
administrator credentials can be used from the `$KUBECONFIG` environment
variable.

ifdef::openshift-enterprise[]
----
$ oadm router --dry-run --service-account=router \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' //<1>
----
endif::[]
ifdef::openshift-origin[]
----
$ oadm router --dry-run --service-account=router \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} //<1>
----
endif::[]
<1> `--credentials` is the path to the
xref:../../cli_reference/manage_cli_profiles.adoc#cli-reference-manage-cli-profiles[CLI configuration file]
for the *openshift-router*.
ifdef::openshift-origin[]
It is recommended using an *openshift-router* specific profile with
appropriate permissions.
endif::[]
====

[IMPORTANT]
====
Router pods created using `oadm router` have default resource requests
that a node must satisfy for the router pod to be deployed. In an
effort to increase the reliability of infrastructure components, the default
resource requests are used to increase the QoS tier of the router pods above
pods without resource requests. The default values represent the observed minimum
resources required for a basic router to be deployed and can be edited in the
routers deployment configuration and you may want to increase them based on the
load of the router.
====

ifdef::openshift-enterprise[]
The default router service account, named *router*, is automatically created during quick and advanced installations. To verify that this account already exists:
endif::[]
ifdef::openshift-origin[]
First, ensure you have xref:creating-the-router-service-account[created the
router service account] before deploying a router.

To check if a default router, named *router*, already exists:
endif::[]

ifdef::openshift-enterprise[]
----
$ oadm router --dry-run \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oadm router --dry-run --service-account=router \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"}
----
endif::[]

To see what the default router would look like if created:

ifdef::openshift-enterprise[]
----
$ oadm router -o yaml \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oadm router -o yaml --service-account=router \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"}
----
endif::[]

To create a router if it does not exist:

ifdef::openshift-enterprise[]
----
$ oadm router <router_name> --replicas=<number> \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oadm router <router_name> --replicas=<number> \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} \
    --service-account=router
----
endif::[]

Multiple instances are created on different hosts according to the
xref:../../admin_guide/scheduler.adoc#admin-guide-scheduler[scheduler policy].

To use a different router image and view the router configuration that would be used:

ifdef::openshift-enterprise[]
----
$ oadm router <router_name> -o <format> --images=<image> \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oadm router <router_name> -o <format> --images=<image> \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} \
    --service-account=router
----
endif::[]

For example:

ifdef::openshift-enterprise[]
====
----
$ oadm router region-west -o yaml --images=myrepo/somerouter:mytag \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm router region-west -o yaml --images=myrepo/somerouter:mytag \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} \
    --service-account=router
----
====
endif::[]

=== High Availability
You can xref:../../admin_guide/high_availability.adoc#admin-guide-high-availability[set up a highly-available
router] on your {product-title} cluster using IP failover.

[[customizing-the-router-service-ports]]
=== Customizing the Router Service Ports
You can customize the service ports that a template router binds to by setting
the environment variables `*ROUTER_SERVICE_HTTP_PORT*` and
`*ROUTER_SERVICE_HTTPS_PORT*`. This can be done by creating a template router,
then editing its deployment configuration.

The following example creates a router deployment with `0` replicas and
customizes the router service HTTP and HTTPS ports, then scales it
appropriately (to `1` replica).

====
----
$ oadm router --replicas=0 --ports='10080:10080,10443:10443' //<1>
$ oc env dc/router ROUTER_SERVICE_HTTP_PORT=10080  \
                   ROUTER_SERVICE_HTTPS_PORT=10443
$ oc scale dc/router --replicas=1
----
<1> Ensures exposed ports are appropriately set for routers that use the
    container networking mode `--host-network=false`.
====

[IMPORTANT]
====
If you do customize the template router service ports, you will also need to
ensure that the nodes where the router pods run have those custom ports opened
in the firewall (either via *Ansible* or *iptables*, or any other custom method
that you use via *firewall-cmd*).
====

The following is an example using *iptables* to open the custom router service
ports.

====
----
$ iptables -A INPUT -p tcp --dport 10080 -j ACCEPT
$ iptables -A INPUT -p tcp --dport 10443 -j ACCEPT
----
====


[[working-with-multiple-routers]]
=== Working With Multiple Routers

An administrator can create multiple routers with the same definition
that will all serve the same set of routes. By having different groups
of routers with different namespace or route selectors, they can vary
the routes that the router will serve.

Multiple routers can be grouped to distribute routing load in the cluster
and separate tenants to different routers or shards.  Each router or shard in the
group handles routes based on the selectors in the router.  The administrator
can create shards over the whole cluster using `ROUTE_LABELS`.  The user can
create shards over a namespace (project) by using `NAMESPACE_LABELS`.


[[adding-nodeselector-to-a-deployment]]
=== Adding a Node Selector to a Deployment Configuration

You can make specific routers deploy on specific nodes by labeling the nodes and
adding a node selector to the router deployment controller.

Add a label to the desired node.
====
----
$ oc label node 10.254.254.28 "router=first"
----
====
Add a node selector to the deployment configuration.
====
----
$ oc edit dc <deploymentConfigName>
----
====
Find the section “spec” under “template” and add node selector section.
Also, add the nodeSelector.
====
----
  template:
    metadata:
      creationTimestamp: null
      labels:
        router: router1
    spec:
      nodeSelector:
        router: "first"
----
====
When the edit is saved the deployment is updated.

[[using-router-shards]]
=== Using Router Shards

Each xref:../../architecture/core_concepts/projects_and_users.adoc#projects[project has its own namespace].
By default, a router selects routes from all the namespaces it has
access to. The access controls are based on the service account that the
router is run with.

Using `NAMESPACE_LABELS` and/or `ROUTE_LABELS`, a router can filter out the
namespaces and/or routes that it should service. This enables you to
partition routes amongst multiple router deployments effectively
distributing the set of routes.

Example: A router deployment `finops-router` is run with route selector
        `NAMESPACE_LABELS="name in (finance, ops)"`
        and a router deployment `dev-router` is run with route selector
        `NAMESPACE_LABELS="name=dev"`

If all routes are in the 3 namespaces `finance`, `ops` or `dev`,
then this could effectively distribute our routes across two
router deployments.

In the above scenario, sharding becomes a special case of partitioning
with no overlapping sets. Routes are divided amongst multiple router shards.

The criteria for route selection governs how the routes are distributed - it
is possible to have routes that overlap accross multiple router deployments.

Example: In addition to the `finops-router` and `dev-router` in the example
         above, we also have an `devops-router` which is run with a route
         selector `NAMESPACE_LABELS="name in (dev, ops)"`.

The routes in namespaces `dev` or `ops` now are serviced by two different
router deployments. This becomes a case where we have partitioned the
routes with an overlapping set.


In addition, this enables us to create more complex routing rules ala
divert high priority traffic to the dedicated `finops-router` but send
the lower priority ones to the `devops-router`.

`NAMESPACE_LABELS` allows filtering the projects to service and selecting
all the routes from those projects. But we may want to partition routes
based on other criteria in the routes themselves. The `ROUTE_LABELS`
selector allows you to slice-and-dice the routes themselves.

Example: A router deployment `prod-router` is run with route selector
        `ROUTE_LABELS="mydeployment=prod"`
        and a router deployment `devtest-router` is run with route selector
        `ROUTE_LABELS="mydeployment in (dev, test)"`

Example assumes you have all the routes you wish to serviced tagged with a
label `"mydeployment=<tag>"`.

[[creating-router-shards]]
=== Creating Router Shards

Router sharding lets you select how routes are distributed among a set of
routers. As above the routers are created on different nodes.

Router sharding is
xref:../../architecture/core_concepts/routes.adoc#router-sharding[based on labels];
you set labels on the routes in the pool,
and express the desired subset of those routes for the router to serve
with a selection expression via the `oc env` command.

The rest of this section describes an extended example.
Suppose there are 26 routes, named `a` -- `z`,
in the pool, with various labels:

.Possible labels on routes in the pool
----
sla=high       geo=east     hw=modest     dept=finance
sla=medium     geo=west     hw=strong     dept=dev
sla=low                                   dept=ops
----

These labels express the concepts:
service level agreement, geographical location,
hardware requirements, and department.
The routes in the pool can have at most one label from each column.
Some routes may have other labels, entirely, or none at all.

[options="header",cols="1,1,1,1,1,3"]
|===
|Name(s) |SLA |Geo |HW |Dept |Other Labels

|`a`
|`high`
|`east`
|`modest`
|`finance`
|`type=static`

|`b`
|
|`west`
|`strong`
|
|`type=dynamic`

|`c`, `d`, `e`
|`low`
|
|`modest`
|
|`type=static`

|`g` -- `k`
|`medium`
|
|`strong`
|`dev`
|

|`l` -- `s`
|`high`
|
|`modest`
|`ops`
|

|`t` -- `z`
|
|`west`
|
|
|`type=dynamic`

|===

Here is a convenience script *_mkshard_*  that
ilustrates how `oadm router`, `oc env`, and `oc scale`
work together to make a router shard.

====
[source,bash]
----
#!/bin/bash
# Usage: mkshard ID SELECTION-EXPRESSION
id=$1
sel="$2"
router=router-shard-$id           //<1>
oadm router $router --replicas=0  //<2>
dc=dc/router-shard-$id            //<3>
oc env   $dc ROUTE_LABELS="$sel"  //<4>
oc scale $dc --replicas=3         //<5>
----
<1> The created router has name `router-shard-<id>`.
<2> Specify no scaling for now.
<3> The deployment configuration for the router.
<4> Set the selection expression using `oc env`.
    The selection expression is the value of
    the `ROUTE_LABELS` environment variable.
<5> Scale it up.
====

Running *_mkshard_* several times creates several routers:

[options="header",cols="2,3,2"]
|===
|Router |Selection Expression |Routes

|`router-shard-1`
|`sla=high`
|`a`, `l` -- `s`

|`router-shard-2`
|`geo=west`
|`b`, `t` -- `z`

|`router-shard-3`
|`dept=dev`
|`g` -- `k`

|===


[[modifying-router-shards]]
=== Modifying Router Shards

Because a router shard is a construct
xref:../../architecture/core_concepts/routes.adoc#router-sharding[based on labels],
you can modify either the labels (via
xref:../../cli_reference/basic_cli_operations.adoc#application-modification-cli-operations[`oc label`])
or the selection expression.

This section extends the example started in the
xref:creating-router-shards[Creating Router Shards] section,
demonstrating how to change the selection expression.

Here is a convenience script *_modshard_* that modifies
an existing router to use a new selection expression:

====
[source,bash]
----
#!/bin/bash
# Usage: modshard ID SELECTION-EXPRESSION...
id=$1
shift
router=router-shard-$id       //<1>
dc=dc/$router                 //<2>
oc scale $dc --replicas=0     //<3>
oc env   $dc "$@"             //<4>
oc scale $dc --replicas=3     //<5>
----
<1> The modified router has name `router-shard-<id>`.
<2> The deployment configuration where the modifications occur.
<3> Scale it down.
<4> Set the new selection expression using `oc env`.
    Unlike `mkshard` from the
    xref:creating-router-shards[Creating Router Shards]
    section, the selection expression specified as the
    non-`ID` arguments to `modshard` must include the
    environment variable name as well as its value.
<5> Scale it back up.
====

[NOTE]
====
In `modshard`, the `oc scale` commands are not necessary if the
xref:../../dev_guide/deployments.adoc#strategies[deployment strategy]
for `router-dhsard-<id>` is `Rolling`.
====

For example, to expand the department for `router-shard-3`
to include `ops` as well as `dev`:

----
$ modshard 3 ROUTE_LABELS='dept in (dev, ops)'
----

The result is that `router-shard-3` now selects routes `g` -- `s`
(the combined sets of `g` -- `k` and `l` -- `s`).

This example takes into account that
there are only three departments in this example scenario,
and specifies a department to leave out of the shard,
thus achieving the same result as the preceding example:

----
$ modshard 3 ROUTE_LABELS='dept != finanace'
----

This example specifies shows three comma-separated qualities,
and results in only route `b` being selected:

----
$ modshard 3 ROUTE_LABELS='hw=strong,type=dynamic,geo=west'
----

Similarly to `ROUTE_LABELS`, which involve a route's labels,
you can select routes based on the labels of the route's namespace labels,
with the `NAMESPACE_LABELS` environment variable.
This example modifies `router-shard-3` to serve
routes whose namespace has the label `frequency=weekly`:

----
$ modshard 3 NAMESPACE_LABELS='frequency=weekly'
----

The last example combines `ROUTE_LABELS` and `NAMESPACE_LABELS`
to select routes with label `sla=low` and
whose namespace has the label `frequency=weekly`:

----
$ modshard 3 \
    NAMESPACE_LABELS='frequency=weekly' \
    ROUTE_LABELS='sla=low'
----

[[using-namespace-router-shards]]
=== Using Namespace Router Shards

Each
xref:../../architecture/core_concepts/projects_and_users.adoc#projects[project has its own namespace].
The routes for a project can be handled by a selected router by using
`NAMESPACE_LABELS`.  The router is given a selector for a `NAMESPACE_LABELS`
label and the project that wants to use the router applies the `NAMESPACE_LABELS`
label to its namespace.

Because `NAMESPACE_LABELS` labels are used, the router service account associated with the
router must have `cluster-reader` permission. This permits the router to read the labels
that are applied to the namespaces. Multiple routers can be deployed using the
same service account.

Add `cluster-reader` to the router service account, in this case the default route:
====
----
$ oadm policy add-cluster-role-to-user cluster-reader system:serviceaccount:default:router
----
====
Now create and label the router:
====
----
$ oadm router ...  --service-account=router
$ oc env dc/router NAMESPACE_LABELS="router=r1"
----
====
Because the router has a selector for a namespace, the router will handle
routes for that namespace.  So, for example:
====
----
$ oc label namespace default "router=r1"
----
====
Now create routes in the default namespace, and the route is
available in the default router:
====
----
$ oc create -f route1.yaml
----
====
Now create a new project (namespace) and create a route, route2.
====
----
$ oc new-project p1
$ oc create -f route2.yaml
----
====
And notice the route is not available in your router.
Now label namespace p1 with "router=r1"
====
----
$ oc label namespace p1 "router=r1"
----
====
Which makes the route available to the router.

Note that removing the label from the namespace won't have immediate effect
(as we don't see the updates in the router), so if you redeploy/start a new
router pod, you should see the unlabelled effects.
====
----
$ oc scale dc/router --replicas=0 && oc scale dc/router --replicas=1
----
====


[[customizing-the-default-routing-subdomain]]
=== Customizing the Default Routing Subdomain
You can customize the default routing subdomain by modifying the master
configuration file. Routes that do not specify a host name would have one
generated using this default routing subdomain.

[[modifying-the-master-configuration-file]]
==== Modifying the Master Configuration file
You can customize the suffix used as the default routing subdomain for your
environment using the
xref:../../install_config/master_node_configuration.adoc#master-configuration-files[master
configuration file] (the *_/etc/origin/master/master-config.yaml_* file by
default).

The following example shows how you can set the configured suffix to
*v3.openshift.test*:

====
----
routingConfig:
  subdomain: v3.openshift.test
----
====

[NOTE]
====
This change requires a restart of the master if it is running.
====

With the {product-title} master(s) running the above configuration, the
xref:../../architecture/core_concepts/routes.adoc#route-hostnames[generated host
name] for the example of a route named *no-route-hostname* without a
host name added to a namespace *mynamespace* would be:

====
----
no-route-hostname-mynamespace.v3.openshift.test
----
====

[[forcing-route-hostnames-to-a-custom-routing-subdomain]]
=== Forcing Route Host Names to a Custom Routing Subdomain
If an administrator wants to restrict all routes to a specific routing
subdomain, they can pass the `--force-subdomain` option to the `oadm
router` command. This forces the router to override any host names specified in
a route and generate one based on the template provided to the
`--force-subdomain` option.

The following example runs a router, which overrides the route host names using
a custom subdomain template `${name}-${namespace}.apps.example.com`.

====
----
$ oadm router --force-subdomain='${name}-${namespace}.apps.example.com'
----
====

[[using-wildcard-certificates]]
=== Using Wildcard Certificates

A TLS-enabled route that does not include a certificate uses the router's
default certificate instead. In most cases, this certificate should be provided
by a trusted certificate authority, but for convenience you can use the
{product-title} CA to create the certificate. For example:

====
----
$ CA=/etc/origin/master
$ oadm ca create-server-cert --signer-cert=$CA/ca.crt \
      --signer-key=$CA/ca.key --signer-serial=$CA/ca.serial.txt \
      --hostnames='*.cloudapps.example.com' \
      --cert=cloudapps.crt --key=cloudapps.key
----
====

The router expects the certificate and key to be in PEM format in a single
file:

====
----
$ cat cloudapps.crt cloudapps.key $CA/ca.crt > cloudapps.router.pem
----
====

From there you can use the `--default-cert` flag:

====
----
$ oadm router --default-cert=cloudapps.router.pem --service-account=router \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"}
----
====

[NOTE]
====
Browsers only consider wildcards valid for subdomains one
level deep. So in this example, the certificate would be valid for
_a.cloudapps.example.com_ but not for _a.b.cloudapps.example.com_.
====

[[using-secured-routes]]

=== Using Secured Routes

Currently, password protected key files are not supported. HAProxy prompts
for a password upon starting and does not have a way to automate this process.
To remove a passphrase from a keyfile, you can run:

----
# openssl rsa -in <passwordProtectedKey.key> -out <new.key>
----

Here is an example of how to use a secure edge terminated route with TLS
termination occurring on the router before traffic is proxied to the
destination. The secure edge terminated route specifies the TLS certificate
and key information. The TLS certificate is served by the router front end.

First, start up a router instance:

----
# oadm router --replicas=1 --service-account=router  \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"}
----

Next, create a private key, csr and certificate for our edge secured route.
The instructions on how to do that would be specific to your certificate
authority and provider. For a simple self-signed certificate for a domain
named `www.example.test`, see the example shown below:

----
# sudo openssl genrsa -out example-test.key 2048
#
# sudo openssl req -new -key example-test.key -out example-test.csr  \
  -subj "/C=US/ST=CA/L=Mountain View/O=OS3/OU=Eng/CN=www.example.test"
#
# sudo openssl x509 -req -days 366 -in example-test.csr  \
      -signkey example-test.key -out example-test.crt
----

Generate a route using the above certificate and key.

----
$ oc create route edge --service=my-service \
    --hostname=www.example.test \
    --key=example-test.key --cert=example-test.crt
route "my-service" created
----

Look at its definition.

----
$ oc get route/my-service -o yaml
apiVersion: v1
kind: Route
metadata:
  name:  my-service
spec:
  host: www.example.test
  to:
    kind: Service
    name: my-service
  tls:
    termination: edge
    key: |
      -----BEGIN PRIVATE KEY-----
      [...]
      -----END PRIVATE KEY-----
    certificate: |
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
----

Make sure your DNS entry for `www.example.test` points to your router
instance(s) and the route to your domain should be available.
The example below uses curl along with a local resolver to simulate the
DNS lookup:

----
# routerip="4.1.1.1"  #  replace with IP address of one of your router instances.
# curl -k --resolve www.example.test:443:$routerip https://www.example.test/
----


[[using-the-container-network-stack]]

=== Using the Container Network Stack

The {product-title} router runs inside a Docker container and the default behavior is
to use the network stack of the host (i.e., the node where the router container
runs). This default behavior benefits performance because network traffic from
remote clients does not need to take multiple hops through user space to reach
the target service and container.

Additionally, this default behavior enables the router to get the actual source
IP address of the remote connection rather than getting the node's IP address.
This is useful for defining ingress rules based on the originating IP,
supporting sticky sessions, and monitoring traffic, among other uses.

This host network behavior is controlled by the `--host-network` router command
line option, and the default behaviour is the equivalent of using
`--host-network=true`. If you wish to run the router with the container network
stack, use the `--host-network=false` option when creating the router. For
example:

ifdef::openshift-enterprise[]
====
----
$ oadm router \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router \
    --host-network=false
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm router \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} \
    --service-account=router \
    --host-network=false
----
====
endif::[]

Internally, this means the router container must publish the 80 and 443
ports in order for the external network to communicate with the router.

[NOTE]
====
Running with the container network stack means that the router sees the source
IP address of a connection to be the NATed IP address of the node, rather than
the actual remote IP address.
====

[NOTE]
====
On {product-title} clusters using
xref:../../architecture/additional_concepts/sdn.adoc#network-isolation-multitenant[multi-tenant
network isolation], routers on a non-default namespace with the
`--host-network=false` option will load all routes in the cluster, but routes
across the namespaces will not be reachable due to network isolation. With the
`--host-network=true` option, routes bypass the container network and it can
access any pod in the cluster. If isolation is needed in this case, then do not
add routes across the namespaces.
====


[[exposing-the-router-metrics]]

=== Expose Router metrics

The {product-title} router can optionally be configured to run a sidecar container
in the router pod. Depending on the image run in the sidecar container, this
provides a mechanism to expose or publish router metrics for consumption by
external metrics collection and/or aggregation systems (e.g. Prometheus, statsd).

The `--metrics-image` and `--expose-metrics` flags control this
above-mentioned behaviour. Depending on the router implementation in use,
the image is appropriately setup and the metrics sidecar container is
started up when the router is deployed.

The HAProxy based router implementation defaults to using the
`prom/haproxy-exporter` image to run as a sidecar container, which can then
be used as a metrics datasource by the Prometheus server.

[NOTE]
====
The `--metrics-image` flag allows you to override the defaults for HAProxy
based router implementations and in the case of custom implementations
enables the image to use for a custom metrics exporter (or publisher).
====


ifdef::openshift-enterprise[]
====
----
$ sudo docker pull prom/haproxy-exporter

$ oadm router \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router --expose-metrics

$ # Same as above - the command arguments show the --metrics-image usage.
$ oadm router \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router --expose-metrics  \
    --metrics-image=prom/haproxy-exporter
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ sudo docker pull prom/haproxy-exporter

$ oadm router \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} \
    --service-account=router --expose-metrics

$ # Same as above - the command arguments show the --metrics-image usage.
$ oadm router \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} \
    --service-account=router --expose-metrics  \
    --metrics-image=prom/haproxy-exporter
----
====
endif::[]


Once the haproxy-exporter containers (and your HAProxy router) are started
up, you can now point Prometheus at the sidecar container (on port 9101 on
the node where the haproxy-exporter container is running).

An example prometheus config and its usage is show below.

====
----
$ haproxy_exporter_ip="<enter-ip-address-or-hostname>"
$ cat > haproxy-scraper.yml  <<CFGEOF
---
global:
  scrape_interval: "60s"
  scrape_timeout:  "10s"
  # external_labels:
    # source: openshift-router

scrape_configs:
  - job_name:  "haproxy"
    target_groups:
      - targets:
        - "${haproxy_exporter_ip}:9101"
CFGEOF

$ #  And start prometheus as you would normally using the above config file.
$ echo "  - Example:  prometheus -config.file=haproxy-scraper.yml "
$ echo "              or you can start it as a container on {product-title}!!

$ echo "  - Once the prometheus server is up, view the {product-title} HAProxy "
$ echo "    router metrics at: http://<ip>:9090/consoles/haproxy.html "

----
====


=== Preventing Connection Failures During Restarts

If you connect to the router while the proxy is reloading, there is a
small chance that your connection will end up in the wrong network
queue and be dropped.  The issue is being addressed.  In the meantime,
it is possible to work around the problem by installing *_iptables_*
rules to prevent connections during the reload window.  However, doing
so means that the router needs to run with elevated privilege so that
it can manipulate *_iptables_* on the host.  It also means that
connections that happen during the reload are temporarily ignored and
will have to retransmit their connection start, lengthening the time
it takes to connect, but preventing connection failure.

Due to these issues, do not enable this option most of the time.
However, if you decide you must try to prevent this case from
happening, you can make the router use *_iptables_* by changing the
service account, and setting an environment variable on the router.

*Use a Privileged SCC*

When creating the router, allow it to use the privileged SCC.  That
this gives the router user the ability to create containers with root
privileges on the nodes.
----
$ oadm policy add-scc-to-user privileged -z router
----

*Patch the Router Deployment Configuration to Create a Privileged Container*

Now that the router use can create privileged containers, make the
router deployment configuration use the power so that the router can
set the iptables rules it needs.  This patch changes the router
deployment configuration so that the containter that is created runs
as root.
----
$ oc patch dc router -p '{"spec":{"template":{"spec":{"containers":[{"name":"router","securityContext":{"privileged":true}}]}}}}'
----

*Tell the Router to Use iptables*

Set the option on the router deployment configuration.  If you used a
non-default name for the router, you will have to change *_dc/router_*
accordingly):

====
----
oc set env dc/router -c router DROP_SYN_DURING_RESTART=true
----
====


[[deploying-customized-router]]
== Deploying a Customized HAProxy Router

The HAProxy router is based on a
link:http://golang.org/pkg/text/template/[*golang* template] that
generates the HAProxy configuration file from a list of routes. If you
want a customized template router to meet your needs, you can customize
the template file, build a new Docker image, and run a customized router.
Alternatively you can use a xref:../../dev_guide/configmaps.adoc#dev-guide-configmaps[configMap].

One common case for this might be implementing new features within the
application back ends. For example, it might be desirable in a highly-available
setup to xref:using-stick-tables[use stick-tables] that synchronizes between
peers. The router plug-in provides all the facilities necessary to make this
customization.

You can obtain a new *_haproxy-config.template_* file from the latest router
image by running:

----
ifdef::openshift-enterprise[]
# docker run --rm --interactive=true --tty --entrypoint=cat \
    registry.access.redhat.com/openshift3/ose-haproxy-router:v3.0.2.0 haproxy-config.template
endif::[]
ifdef::openshift-origin[]
# docker run --rm --interactive=true --tty --entrypoint=cat \
    openshift/origin-haproxy-router haproxy-config.template
endif::[]
----

Save this content to a file for use as the basis of your customized template.

[[using-configmap-replace-template]]

=== Using a ConfigMap to Replace the Router Configuration Template

A xref:../../dev_guide/configmaps.adoc#dev-guide-configmaps[configMap] permits you to customize
the router instance without rebuilding the router image. The
*_haproxy-config.template_*, *_reload-haproxy_* and other scripts can be modified
as well as creating and modifying router environment variables.

The following example shows how to use a custom *_haproxy-config.template_* in
your router.

First get a copy of the *_haproxy-config.template_* that you want to modify as
xref:deploying-customized-router[described above]. Modify the
*_haproxy-config.template_* as desired.

Create a configmap:
====
----
$ oc create configmap customrouter --from-file=haproxy-config.template
----
====
The configMap contains a copy of your modified *_haproxy-config.template_* file.
The router dc is next modified to mount the configMap as a file and the
`TEMPLATE_FILE` environment variable is modified to point to it. This can be
done using "oc env" and "oc volume" or alternaitvely by editing the router dc.

====
----
$ oc env dc/router TEMPLATE_FILE=/var/lib/haproxy/conf/custom/haproxy-config.template
$ oc volume dc/router --add --overwrite --name=config-volume \
  --mount-path=/var/lib/haproxy/conf/custom \
  --source='{"configMap": { "name": "customrouter"}}'
----
====

Alternatively, edit the router dc to mention the configMap in all the needed places.
In the `*spec.container.env*` section add the `TEMPLATE_FILE` environment variable to
point to the mounted *_haproxy-config.template_*. Add the `*spec.container.volumeMounts*`
to create the needed mount point. Add a new `*spec.volumes*` mentioning the configMap.

====
----
$ oc edit dc router
...
        - name: STATS_USERNAME
          value: admin
        - name: TEMPLATE_FILE
          value: /var/lib/haproxy/conf/custom/haproxy-config.template
        image: openshift/origin-haproxy-routerp
...
        terminationMessagePath: /dev/termination-log
        volumeMounts:
        - mountPath: /var/lib/haproxy/conf/custom
          name: config-volume
      dnsPolicy: ClusterFirst
...
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          name: customrouter
        name: config-volume
  test: false
...
----
====
Save the changes and exit the editor.  This restarts the router.

[[using-stick-tables]]

=== Using Stick Tables

The following example customization can be used in a
xref:../../admin_guide/high_availability.adoc#configuring-a-highly-available-routing-service[highly-available
routing setup] to use stick-tables that synchronize between peers.

*Adding a Peer Section*

In order to synchronize stick-tables amongst peers you must a define a peers
section in your HAProxy configuration. This section determines how HAProxy will
identify and connect to peers. The plug-in provides data to the template under
the `*.PeerEndpoints*` variable to allow you to easily identify members of the
router service. You may add a peer section to the *_haproxy-config.template_*
file inside the router image by adding:

====
----
{{ if (len .PeerEndpoints) gt 0 }}
peers openshift_peers
  {{ range $endpointID, $endpoint := .PeerEndpoints }}
    peer {{$endpoint.TargetName}} {{$endpoint.IP}}:1937
  {{ end }}
{{ end }}
----
====

*Changing the Reload Script*

When using stick-tables, you have the option of telling HAProxy what it should
consider the name of the local host in the peer section. When creating
endpoints, the plug-in attempts to set the `*TargetName*` to the value of the
endpoint's `*TargetRef.Name*`. If `*TargetRef*` is not set, it will set the
`*TargetName*` to the IP address. The `*TargetRef.Name*` corresponds with the
Kubernetes host name, therefore you can add the `-L` option to the
`reload-haproxy` script to identify the local host in the peer section.

====
----
peer_name=$HOSTNAME <1>

if [ -n "$old_pid" ]; then
  /usr/sbin/haproxy -f $config_file -p $pid_file -L $peer_name -sf $old_pid
else
  /usr/sbin/haproxy -f $config_file -p $pid_file -L $peer_name
fi
----
<1> Must match an endpoint target name that is used in the peer section.
====

*Modifying Back Ends*

Finally, to use the stick-tables within back ends, you can modify the HAProxy
configuration to use the stick-tables and peer set. The following is an example
of changing the existing back end for TCP connections to use stick-tables:

====
----

            {{ if eq $cfg.TLSTermination "passthrough" }}
backend be_tcp_{{$cfgIdx}}
  balance leastconn
  timeout check 5000ms
  stick-table type ip size 1m expire 5m{{ if (len $.PeerEndpoints) gt 0 }} peers openshift_peers {{ end }}
  stick on src
                {{ range $endpointID, $endpoint := $serviceUnit.EndpointTable }}
  server {{$endpointID}} {{$endpoint.IP}}:{{$endpoint.Port}} check inter 5000ms
                {{ end }}
            {{ end }}
----
====

After this modification, you can xref:rebuilding-your-router[rebuild your router].
[[rebuilding-your-router]]

=== Rebuilding Your Router

After you have made any desired modifications to the template, such as the
example xref:using-stick-tables[stick tables] customization, you must rebuild
your router for your changes to go in effect:

. https://access.redhat.com/articles/881893#createimage[Rebuild the Docker
image to include your customized template.]
. xref:docker_registry.adoc#access[Push the resulting image to your repository].
. Create the router specifying your new image, either:
.. in the pod's object definition directly, or
.. by adding the `--images=<repo>/<image>:<tag>` flag to the `oadm router`
command when
xref:../../admin_guide/high_availability.adoc#configuring-a-highly-available-routing-service[creating
a highly-available routing service].

[[deploying-the-f5-router]]

== Deploying the F5 Router

ifdef::openshift-enterprise[]
[NOTE]
====
The F5 router plug-in is available starting in {product-title} 3.0.2.
====
endif::[]

The F5 router plug-in is provided as a Docker image and run as a pod, just like
the xref:haproxy-router[default HAProxy router]. Deploying the F5 router is
done similarly as well, using the `oadm router` command but providing additional
flags (or environment variables) to specify the following parameters for the *F5
BIG-IP®* host:

[[f5-router-flags]]
[cols="1,4"]
|===
|Flag |Description

|`--type=f5-router`
|Specifies that an F5 router should be launched (the default `--type` is
*haproxy-router*).

|`--external-host`
|Specifies the *F5 BIG-IP®* host's management interface's host name or IP
address.

|`--external-host-username`
|Specifies the *F5 BIG-IP®* user name (typically *admin*).

|`--external-host-password`
|Specifies the *F5 BIG-IP®* password.

|`--external-host-http-vserver`
|Specifies the name of the F5 virtual server for HTTP connections.

|`--external-host-https-vserver`
|Specifies the name of the F5 virtual server for
HTTPS connections.

|`--external-host-private-key`
|Specifies the path to the SSH private key file for the *F5 BIG-IP®* host.
Required to upload and delete key and certificate files for routes.

|`--external-host-insecure`
|A Boolean flag that indicates that the F5 router should skip strict certificate
verification with the *F5 BIG-IP®* host.

|`--external-host-partition-path`
|Specifies the *F5 BIG-IP®* xref:f5-router-partition-paths[partition path] (the default is */Common*).
|===

As with the HAProxy router, the `oadm router` command creates the service and
deployment configuration objects, and thus the replication controllers and
pod(s) in which the F5 router itself runs. The replication controller restarts
the F5 router in case of crashes. Because the F5 router is only watching routes
and endpoints and configuring *F5 BIG-IP®* accordingly, running the F5 router in
this way along with an appropriately configured *F5 BIG-IP®* deployment should
satisfy high-availability requirements.

To deploy the F5 router:

. First,
xref:../../install_config/routing_from_edge_lb.adoc#establishing-a-tunnel-using-a-ramp-node[establish
a tunnel using a ramp node], which allows for the routing of traffic to pods
through the xref:../../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[{product-title} SDN].
ifdef::openshift-origin[]
. Ensure you have xref:creating-the-router-service-account[created the router
service account].
endif::[]
. Run the `oadm router` command with the xref:f5-router-flags[appropriate
flags]. For example:
+
ifdef::openshift-enterprise[]
====
----
$ oadm router \
    --type=f5-router \
    --external-host=10.0.0.2 \
    --external-host-username=admin \
    --external-host-password=mypassword \
    --external-host-http-vserver=ose-vserver \
    --external-host-https-vserver=https-ose-vserver \
    --external-host-private-key=/path/to/key \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \//<1>
    --service-account=router
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm router \
    --type=f5-router \
    --external-host=10.0.0.2 \
    --external-host-username=admin \
    --external-host-password=mypassword \
    --external-host-http-vserver=ose-vserver \
    --external-host-https-vserver=https-ose-vserver \
    --external-host-private-key=/path/to/key \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} \//<1>
    --service-account=router
----
====
endif::[]
<1> `--credentials` is the path to the
xref:../../cli_reference/manage_cli_profiles.adoc#cli-reference-manage-cli-profiles[CLI configuration file]
for the *openshift-router*. It is recommended using an *openshift-router*
specific profile with appropriate permissions.

[[f5-router-partition-paths]]
=== F5 Router Partition Paths
Partition paths allow you to store your {product-title} routing configuration in
a custom *F5 BIG-IP®* administrative partition, instead of the default */Common*
partition. You can use custom administrative partitions to secure *F5 BIG-IP®*
environments. This means that an {product-title}-specific configuration stored
in *F5 BIG-IP®* system objects reside within a logical container, allowing
administrators to define access control policies on that specific administrative
partition.

See the
link:https://support.f5.com/kb/en-us/products/big-ip_ltm/manuals/product/tmos_management_guide_10_0_0/tmos_partitions.html[*F5 BIG-IP®* documentation] for more information about administrative partitions.

Use the `--external-host-partition-path` flag when
xref:deploying-the-f5-router[deploying the F5 router] to specify a partition
path:
====
----
$ oadm router --external-host-partition-path=/OpenShift/zone1 ...
----
====

== What's Next?

If you deployed an HAProxy router, you can learn more about
xref:../../admin_guide/router.adoc#admin-guide-router[monitoring the router].

If you have not yet done so, you can:

- xref:../../install_config/configuring_authentication.adoc#install-config-configuring-authentication[Configure
authentication]; by default, authentication is set to
ifdef::openshift-enterprise[]
xref:../../install_config/configuring_authentication.adoc#DenyAllPasswordIdentityProvider[Deny
All].
endif::[]
ifdef::openshift-origin[]
xref:../../install_config/configuring_authentication.adoc#AllowAllPasswordIdentityProvider[Allow
All].
endif::[]
- Deploy an xref:docker_registry.adoc#install-config-install-docker-registry[integrated Docker registry].
ifdef::openshift-origin[]
- xref:../../install_config/imagestreams_templates.adoc#install-config-imagestreams-templates[Populate your {product-title} installation]
with a useful set of Red Hat-provided image streams and templates.
endif::[]
