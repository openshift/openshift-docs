[[install-config-install-deploy-router]]
= Deploying a Router
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
The OpenShift link:../../architecture/core_concepts/routes.html[router] is the
ingress point for all external traffic destined for
link:../../architecture/core_concepts/pods_and_services.html#services[services]
in your OpenShift installation. OpenShift provides and supports the following
two router plug-ins:

- The
link:../../architecture/core_concepts/routes.html#haproxy-template-router[HAProxy
template router] is the default plug-in. It uses the
ifdef::openshift-enterprise[]
*openshift3/ose-haproxy-router*
endif::[]
ifdef::openshift-origin[]
*openshift/origin-haproxy-router*
endif::[]
 image to run an HAProxy instance alongside the template router plug-in inside a
container on OpenShift. It currently supports HTTP(S) traffic and TLS-enabled
traffic via SNI. The router's container listens on the host network interface,
unlike most containers that listen only on private IPs. The router proxies
external requests for route names to the IPs of actual pods identified by the
service associated with the route.

- The link:../../architecture/core_concepts/routes.html#f5-router[F5 router]
integrates with an existing *F5 BIG-IP®* system in your environment to
synchronize routes. *F5 BIG-IP®* version 11.4 or newer is required in order to
have the F5 iControl REST API.

ifdef::openshift-enterprise[]
[NOTE]
====
The F5 router plug-in is available starting in OpenShift Enterprise 3.0.2.
====
endif::[]

[[creating-the-router-service-account]]

== Creating the Router Service Account

ifdef::openshift-enterprise[]
Starting in OpenShift Enterprise 3.0.1.0, you
endif::[]
ifdef::openshift-origin[]
You
endif::[]
must first create a
link:../../architecture/core_concepts/projects_and_users.html[service account]
for the router before deploying. This service account must have permissions to a
link:../../architecture/additional_concepts/authorization.html#security-context-constraints[security
context constraint] (SCC) that allows it to specify host ports.

Create a service account, for example named *router*:

====
----
$ echo \
    '{"kind":"ServiceAccount","apiVersion":"v1","metadata":{"name":"router"}}' \
    | oc create -f -
----
====

Edit the *privileged* SCC:

====
----
$ oc edit scc privileged
----
====

Add the *router* service account in the form of
*system:serviceaccount:<project>:<name>* to the `*users*` section:

====
----
...
users:
- system:serviceaccount:openshift-infra:build-controller
- system:serviceaccount:default:router
----
====

[[haproxy-router]]
== Deploying the Default HAProxy Router
The `oadm router` command is provided with the administrator CLI to simplify the
tasks of setting up routers in a new installation. Just about every form of
communication between OpenShift components is secured by TLS and uses various
certificates and authentication methods. Use the `--credentials` option to
specify what credentials the router should use to contact the master.

[IMPORTANT]
====
Routers directly attach to port 80 and 443 on all interfaces on a host. Restrict
routers to hosts where port 80/443 is available and not being consumed by
another service, and set this using node selectors and the
link:../../admin_guide/scheduler.html[scheduler configuration]. As an example, you can
achieve this by dedicating infrastructure nodes to run services such as routers.
====

[IMPORTANT]
====
It is recommended to use separate distinct *openshift-router* credentials
with your router. The credentials can be provided using the `--credentials`
flag to the `oadm router` command. Alternatively, the default cluster
administrator credentials can be used from the `$KUBECONFIG` environment
variable.

ifdef::openshift-enterprise[]
----
$ oadm router --dry-run --service-account=router \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' //<1>
----
endif::[]
ifdef::openshift-origin[]
----
$ oadm router --dry-run --service-account=router \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} //<1>
----
endif::[]
<1> `--credentials` is the path to the
link:../../cli_reference/manage_cli_profiles.html[CLI configuration file]
for the *openshift-router*.
ifdef::openshift-origin[]
It is recommended using an *openshift-router* specific profile with
appropriate permissions.
endif::[]
====

First, ensure you have link:#creating-the-router-service-account[created the
router service account] before deploying a router.

To check if a default router, named *router*, already exists:

ifdef::openshift-enterprise[]
----
$ oadm router --dry-run \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oadm router --dry-run --service-account=router \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"}
----
endif::[]

To see what the default router would look like if created:

ifdef::openshift-enterprise[]
----
$ oadm router -o yaml \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oadm router -o yaml --service-account=router \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"}
----
endif::[]

To create a router if it does not exist:

ifdef::openshift-enterprise[]
----
$ oadm router <router_name> --replicas=<number> \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oadm router <router_name> --replicas=<number> \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} \
    --service-account=router
----
endif::[]

Multiple instances are created on different hosts according to the
link:../../admin_guide/scheduler.html[scheduler policy].

To use a different router image and view the router configuration that would be used:

ifdef::openshift-enterprise[]
----
$ oadm router <router_name> -o <format> --images=<image> \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oadm router <router_name> -o <format> --images=<image> \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} \
    --service-account=router
----
endif::[]

For example:

ifdef::openshift-enterprise[]
====
----
$ oadm router region-west -o yaml --images=myrepo/somerouter:mytag \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm router region-west -o yaml --images=myrepo/somerouter:mytag \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} \
    --service-account=router
----
====
endif::[]

=== High Availability
You can link:../../admin_guide/high_availability.html[set up a highly-available
router] on your OpenShift cluster using IP failover.

[[customizing-the-default-routing-subdomain]]

=== Customizing the Default Routing Subdomain

You can customize the suffix used as the default routing subdomain for your
environment using the
link:../../install_config/master_node_configuration.html#master-configuration-files[master
configuration file] (the *_/etc/origin/master/master-config.yaml_* file by
default). The following example shows how you can set the configured suffix to
*v3.openshift.test*:

.Master Configuration Snippet
====

----
routingConfig:
  subdomain: v3.openshift.test
----
====

[NOTE]
====
This change requires a restart of the master if it is running.
====

With the OpenShift master(s) running the above configuration, the
link:../../architecture/core_concepts/routes.html#route-hostnames[generated host
name] for the example of a host added to a namespace *mynamespace* would be:

.Generated Host Name
====

----
myroute-mynamespace.v3.openshift.test
----
====

[[using-wildcard-certificates]]

=== Using Wildcard Certificates

A TLS-enabled route that does not include a certificate uses the router's
default certificate instead. In most cases, this certificate should be provided by a
trusted certificate authority, but for convenience you can use the OpenShift CA
to create the certificate. For example:

====
----
$ CA=/etc/origin/master
$ oadm ca create-server-cert --signer-cert=$CA/ca.crt \
      --signer-key=$CA/ca.key --signer-serial=$CA/ca.serial.txt \
      --hostnames='*.cloudapps.example.com' \
      --cert=cloudapps.crt --key=cloudapps.key
----
====

The router expects the certificate and key to be in PEM format in a single
file:

====
----
$ cat cloudapps.crt cloudapps.key $CA/ca.crt > cloudapps.router.pem
----
====

From there you can use the `--default-cert` flag:

====
----
$ oadm router --default-cert=cloudapps.router.pem --service-account=router \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"}
----
====

[NOTE]
====
Browsers only consider wildcards valid for subdomains one
level deep. So in this example, the certificate would be valid for
_a.cloudapps.example.com_ but not for _a.b.cloudapps.example.com_.
====

[[using-secured-routes]]

=== Using Secured Routes

Currently, password protected key files are not supported. HAProxy prompts
for a password upon starting and does not have a way to automate this process.
To remove a passphrase from a keyfile, you can run:

----
# openssl rsa -in <passwordProtectedKey.key> -out <new.key>
----

Here is an example of how to use a secure edge terminated route with TLS
termination occurring on the router before traffic is proxied to the
destination. The secure edge terminated route specifies the TLS certificate
and key information. The TLS certificate is served by the router front end.

First, start up a router instance:

----
# oadm router --replicas=1 --service-account=router  \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"}
----

Next, create a private key, csr and certificate for our edge secured route.
The instructions on how to do that would be specific to your certificate
authority and provider. For a simple self-signed certificate for a domain
named `www.example.test`, see the example shown below:

----
# sudo openssl genrsa -out example-test.key 2048
#
# sudo openssl req -new -key example-test.key -out example-test.csr  \
  -subj "/C=US/ST=CA/L=Mountain View/O=OS3/OU=Eng/CN=www.example.test"
#
# sudo openssl x509 -req -days 366 -in example-test.csr  \
      -signkey example-test.key -out example-test.crt
----

Generate a route configuration file using the above certificate and key.
Make sure to replace servicename `my-service` with the name of your service.

----
# servicename="my-service"
# echo "
apiVersion: v1
kind: Route
metadata:
  name:  secured-edge-route
spec:
  host: www.example.test
  to:
    kind: Service
    name: $servicename
  tls:
    termination: edge
    key: |
$(openssl rsa -in example-test.key | sed 's/^/      /')
    certificate: |
$(openssl x509 -in example-test.crt | sed 's/^/      /')

" > example-test-route.yaml
----

Finally add the route to OpenShift (and the router) via:

----
# oc create -f example-test-route.yaml
----

Make sure your DNS entry for `www.example.test` points to your router
instance(s) and the route to your domain should be available.
The example below uses curl along with a local resolver to simulate the
DNS lookup:

----
# routerip="4.1.1.1"  #  replace with IP address of one of your router instances.
# curl -k --resolve www.example.test:443:$routerip https://www.example.test/
----


[[using-the-container-network-stack]]

=== Using the Container Network Stack

The OpenShift router runs inside a Docker container and the default behavior is
to use the network stack of the host (i.e., the node where the router container
runs). This default behavior benefits performance because network traffic from
remote clients does not need to take multiple hops through user space to reach
the target service and container.

Additionally, this default behavior enables the router to get the actual source
IP address of the remote connection rather than getting the node's IP address.
This is useful for defining ingress rules based on the originating IP,
supporting sticky sessions, and monitoring traffic, among other uses.

This host network behavior is controlled by the `--host-network` router command
line option, and the default behaviour is the equivalent of using
`--host-network=true`. If you wish to run the router with the container network
stack, use the `--host-network=false` option when creating the router. For
example:

ifdef::openshift-enterprise[]
====
----
$ oadm router \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \
    --service-account=router \
    --host-network=false
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm router \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} \
    --service-account=router \
    --host-network=false
----
====
endif::[]

Internally, this means the router container must publish the 80 and 443
ports in order for the external network to communicate with the router.

[NOTE]
====
Running with the container network stack means that the router sees the source
IP address of a connection to be the NATed IP address of the node, rather than
the actual remote IP address.
====

[NOTE]
====
On OpenShift clusters using
link:../../architecture/additional_concepts/sdn.html#network-isolation-multitenant[multi-tenant
network isolation], routers on a non-default namespace with the
`--host-network=false` option will load all routes in the cluster, but routes
across the namespaces will not be reachable due to network isolation. With the
`--host-network=true` option, routes bypass the container network and it can
access any pod in the cluster. If isolation is needed in this case, then do not
add routes across the namespaces.
====

== Deploying a Customized HAProxy Router

The HAProxy router is based on a
link:http://golang.org/pkg/text/template/[*golang* template] that
generates the HAProxy configuration file from a list of routes. If you
want a customized template router to meet your needs, you can customize
the template file, build a new Docker image, and run a customized router.

One common case for this might be implementing new features within the
application back ends. For example, it might be desirable in a highly-available
setup to link:#using-stick-tables[use stick-tables] that synchronizes between
peers. The router plug-in provides all the facilities necessary to make this
customization.

You can obtain a new *_haproxy-config.template_* file from the latest router
image by running:

----
ifdef::openshift-enterprise[]
# docker run --rm --interactive=true --tty --entrypoint=cat \
    registry.access.redhat.com/openshift3/ose-haproxy-router:v3.0.2.0 haproxy-config.template
endif::[]
ifdef::openshift-origin[]
# docker run --rm --interactive=true --tty --entrypoint=cat \
    openshift/origin-haproxy-router haproxy-config.template
endif::[]
----

Save this content to a file for use as the basis of your customized template.

[[using-stick-tables]]

=== Using Stick Tables

The following example customization can be used in a
link:../../admin_guide/high_availability.html#configuring-a-highly-available-routing-service[highly-available
routing setup] to use stick-tables that synchronize between peers.

*Adding a Peer Section*

In order to synchronize stick-tables amongst peers you must a define a peers
section in your HAProxy configuration. This section determines how HAProxy will
identify and connect to peers. The plug-in provides data to the template under
the `*.PeerEndpoints*` variable to allow you to easily identify members of the
router service. You may add a peer section to the *_haproxy-config.template_*
file inside the router image by adding:

====
----
{{ if (len .PeerEndpoints) gt 0 }}
peers openshift_peers
  {{ range $endpointID, $endpoint := .PeerEndpoints }}
    peer {{$endpoint.TargetName}} {{$endpoint.IP}}:1937
  {{ end }}
{{ end }}
----
====

*Changing the Reload Script*

When using stick-tables, you have the option of telling HAProxy what it should
consider the name of the local host in the peer section. When creating
endpoints, the plug-in attempts to set the `*TargetName*` to the value of the
endpoint's `*TargetRef.Name*`. If `*TargetRef*` is not set, it will set the
`*TargetName*` to the IP address. The `*TargetRef.Name*` corresponds with the
Kubernetes host name, therefore you can add the `-L` option to the
`reload-haproxy` script to identify the local host in the peer section.

====
----
peer_name=$HOSTNAME <1>

if [ -n "$old_pid" ]; then
  /usr/sbin/haproxy -f $config_file -p $pid_file -L $peer_name -sf $old_pid
else
  /usr/sbin/haproxy -f $config_file -p $pid_file -L $peer_name
fi
----
<1> Must match an endpoint target name that is used in the peer section.
====

*Modifying Back Ends*

Finally, to use the stick-tables within back ends, you can modify the HAProxy
configuration to use the stick-tables and peer set. The following is an example
of changing the existing back end for TCP connections to use stick-tables:

====
----

            {{ if eq $cfg.TLSTermination "passthrough" }}
backend be_tcp_{{$cfgIdx}}
  balance leastconn
  timeout check 5000ms
  stick-table type ip size 1m expire 5m{{ if (len $.PeerEndpoints) gt 0 }} peers openshift_peers {{ end }}
  stick on src
                {{ range $endpointID, $endpoint := $serviceUnit.EndpointTable }}
  server {{$endpointID}} {{$endpoint.IP}}:{{$endpoint.Port}} check inter 5000ms
                {{ end }}
            {{ end }}
----
====

After this modification, you can link:#rebuilding-your-router[rebuild your router].
[[rebuilding-your-router]]

=== Rebuilding Your Router

After you have made any desired modifications to the template, such as the
example link:#using-stick-tables[stick tables] customization, you must rebuild
your router for your changes to go in effect:

. https://access.redhat.com/articles/881893#createimage[Rebuild the Docker
image to include your customized template.]
. link:docker_registry.html#access[Push the resulting image to your repository].
. Create the router specifying your new image, either:
.. in the pod's object definition directly, or
.. by adding the `--images=<repo>/<image>:<tag>` flag to the `oadm router`
command when
link:../../admin_guide/high_availability.html#configuring-a-highly-available-routing-service[creating
a highly-available routing service].

[[deploying-the-f5-router]]

== Deploying the F5 Router

ifdef::openshift-enterprise[]
[NOTE]
====
The F5 router plug-in is available starting in OpenShift Enterprise 3.0.2.
====
endif::[]

The F5 router plug-in is provided as a Docker image and run as a pod, just like
the link:#haproxy-router[default HAProxy router]. Deploying the F5 router is
done similarly as well, using the `oadm router` command but providing additional
flags (or environment variables) to specify the following parameters for the *F5
BIG-IP®* host:

[[f5-router-flags]]
[cols="1,4"]
|===
|Flag |Description

|`--type=f5-router`
|Specifies that an F5 router should be launched (the default `--type` is
*haproxy-router*).

|`--external-host`
|Specifies the *F5 BIG-IP®* host's management interface's host name or IP
address.

|`--external-host-username`
|Specifies the *F5 BIG-IP®* user name (typically *admin*).

|`--external-host-password`
|Specifies the *F5 BIG-IP®* password.

|`--external-host-http-vserver`
|Specifies the name of the F5 virtual server for HTTP connections.

|`--external-host-https-vserver`
|Specifies the name of the F5 virtual server for
HTTPS connections.

|`--external-host-private-key`
|Specifies the path to the SSH private key file for the *F5 BIG-IP®* host.
Required to upload and delete key and certificate files for routes.

|`--external-host-insecure`
|A Boolean flag that indicates that the F5 router should skip strict certificate
verification with the *F5 BIG-IP®* host.

|`--external-host-partition-path`
|Specifies the *F5 BIG-IP®* link:#f5-router-partition-paths[partition path] (the default is */Common*).
|===

As with the HAProxy router, the `oadm router` command creates the service and
deployment configuration objects, and thus the replication controllers and
pod(s) in which the F5 router itself runs. The replication controller restarts
the F5 router in case of crashes. Because the F5 router is only watching routes
and endpoints and configuring *F5 BIG-IP®* accordingly, running the F5 router in
this way along with an appropriately configured *F5 BIG-IP®* deployment should
satisfy high-availability requirements.

To deploy the F5 router:

. First,
xref:../../install_config/routing_from_edge_lb.adoc#establishing-a-tunnel-using-a-ramp-node[establish
a tunnel using a ramp node], which allows for the routing of traffic to pods
through the xref:../../architecture/additional_concepts/sdn.adoc[OpenShift SDN].
ifdef::openshift-origin[]
. Ensure you have link:#creating-the-router-service-account[created the router
service account].

. Run the `oadm router` command with the link:#f5-router-flags[appropriate
flags]. For example:
+
ifdef::openshift-enterprise[]
====
----
$ oadm router \
    --type=f5-router \
    --external-host=10.0.0.2 \
    --external-host-username=admin \
    --external-host-password=mypassword \
    --external-host-http-vserver=ose-vserver \
    --external-host-https-vserver=https-ose-vserver \
    --external-host-private-key=/path/to/key \
    --credentials='/etc/origin/master/openshift-router.kubeconfig' \//<1>
    --service-account=router
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm router \
    --type=f5-router \
    --external-host=10.0.0.2 \
    --external-host-username=admin \
    --external-host-password=mypassword \
    --external-host-http-vserver=ose-vserver \
    --external-host-https-vserver=https-ose-vserver \
    --external-host-private-key=/path/to/key \
    --credentials=${ROUTER_KUBECONFIG:-"$KUBECONFIG"} \//<1>
    --service-account=router
----
====
endif::[]
<1> `--credentials` is the path to the
link:../../cli_reference/manage_cli_profiles.html[CLI configuration file]
for the *openshift-router*. It is recommended using an *openshift-router*
specific profile with appropriate permissions.


=== F5 Router Partition Paths
Partition paths allow you to store your OpenShift routing configuration in a
custom *F5 BIG-IP®* administrative partition instead of the
default */Common* partition.  Custom administrative partitions can be used
to secure *F5 BIG-IP®* environments so that OpenShift specific
configuration stored in *F5 BIG-IP®* system objects resides within a
logical container.  This allows administrators to define access control
policies on that specific administrative partition. To learn more about
administrative partitions, please refer to the link:https://support.f5.com/kb/en-us/products/big-ip_ltm/manuals/product/tmos_management_guide_10_0_0/tmos_partitions.html[*F5 BIG-IP®* documentation].

Use the `--external-host-partition-path` flag when
link:#deploying-the-f5-router[deploying the F5 router] to specify a partition path:
====
----
$ oadm router --external-host-partition-path=/OpenShift/zone1 ...
----
====

== What's Next?

If you deployed an HAProxy router, you can learn more about
link:../../admin_guide/router.html[monitoring the router].

If you have not yet done so, you can:

- link:../../install_config/configuring_authentication.html[Configure
authentication]; by default, authentication is set to
link:../../install_config/configuring_authentication.html#DenyAllPasswordIdentityProvider[Deny
All].
- Deploy an link:docker_registry.html[integrated Docker registry].
- link:first_steps.html[Populate your OpenShift installation] with a useful set
of Red Hat-provided image streams and templates.
