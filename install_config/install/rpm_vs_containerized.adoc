[[install-config-install-rpm-vs-containerized]]
= Installing on Containerized Hosts
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== RPM vs Containerized Installation

You can opt to install {product-title} using the RPM or containerized package
method. Either installation method results in a working environment, but the
choice comes from the operating system and how you choose to update your hosts.

[IMPORTANT]
====
The default method for installing {product-title} on
ifdef::openshift-origin[]
Fedora, CentOS, or RHEL
endif::[]
ifdef::openshift-enterprise[]
Red Hat Enterprise Linux (RHEL)
endif::[]
uses RPMs. When targeting a Red Hat Atomic Host system, the
containerized method is the only available option, and is automatically selected
for you based on the detection of the *_/run/ostree-booted_* file.
====

When using RPMs, all services are installed and updated via package management
from an outside source. These modify a host's existing configuration within the
same user space. Alternatively, containerized installs instead are a complete,
all-in-one resource using container images and its own operating system within
the container. Any updated, newer containers replace any existing ones on your
host. Choosing one method over the other depends on how you choose to update
{product-title} in the future.

The following table outlines further differences between the RPM and
Containerized methods:

[cols="h,2*",options="header"]
|===
| |RPM  |Containerized

|Installation Method |Packages via `yum` |Container images via `docker`
|Service Management |`systemd` |`docker` and `systemd` units
|Operating System | Red Hat Enterprise Linux | Red Hat Enterprise Linux or Red Hat Atomic Host
|===

[[install-config-install-install-methods-containerized]]
== Install Methods for Containerized Hosts

As with the RPM installation, you can choose between the xref:../../install_config/install/quick_install.adoc#install-config-install-quick-install[quick] and xref:../../install_config/install/quick_install.adoc#defining-an-installation-configuration-file[advanced] install methods for the containerized install.

For the quick installation method, you can choose between the RPM or
containerized method on a per host basis during the interactive installation, or
set the values manually in an
xref:../../install_config/install/quick_install.adoc#defining-an-installation-configuration-file[installation
configuration file].

For the advanced installation method, you can set the Ansible variable
`*containerized=true*` in an
xref:../../install_config/install/advanced_install.adoc#configuring-ansible[inventory
file] on a cluster-wide or per host basis.

For the xref:../../install_config/install/disconnected_install.adoc#install-config-install-disconnected-install[disconnected
installation method], to install the etcd container, you can set the Ansible
variable `osm_etcd_image` to be the fully qualified name of the etcd image on
your local registry, for example, `registry.example.com/rhel7/etcd`.

[[containerized-required-images]]
== Required Images

Containerized installations make use of the following images:

ifdef::openshift-origin[]
- *openshift/origin*
- *openshift/node* (*node* + *openshift-sdn* + *openvswitch* RPM for client tools)
- *openshift/openvswitch* (CentOS 7 + *openvswitch* RPM, runs *ovsdb* and *ovsctl* processes)
- *registry.access.redhat.com/rhel7/etcd*
endif::[]
ifdef::openshift-enterprise[]
- *openshift3/ose*
- *openshift3/node*
- *openshift3/openvswitch*
- *registry.access.redhat.com/rhel7/etcd*

By default, all of the above images are pulled from the Red Hat Registry at
https://registry.access.redhat.com[registry.access.redhat.com].
endif::[]

If you need to use a private registry to pull these images during the
installation, you can specify the registry information ahead of time. For the
advanced installation method, you can set the following Ansible variables in
your inventory file, as required:

----
openshift_docker_additional_registries=<registry_hostname>
openshift_docker_insecure_registries=<registry_hostname>
openshift_docker_blocked_registries=<registry_hostname>
----

ifdef::openshift-enterprise[]
For the quick installation method, you can export the following environment
variables on each target host:

----
# export OO_INSTALL_ADDITIONAL_REGISTRIES=<registry_hostname>
# export OO_INSTALL_INSECURE_REGISTRIES=<registry_hostname>
----


[IMPORTANT]
====
Blocked Docker registries cannot currently be specified using the quick
installation method.
====
endif::[]

The configuration of additional, insecure, and blocked Docker registries occurs
at the beginning of the installation process to ensure that these settings are
applied before attempting to pull any of the required images.

[[containerized-cli-wrappers]]
== CLI Wrappers

When using containerized installations, a CLI wrapper script is deployed on each
master at *_/usr/local/bin/openshift_*. The following set of symbolic links are
also provided to ease administrative tasks:

|===
|Symbolic Link |Usage

|*_/usr/local/bin/oc_*
|Developer CLI

|*_/usr/local/bin/oadm_*
|Administrative CLI

|*_/usr/local/bin/kubectl_*
|Kubernetes CLI
|===

The wrapper spawns a new container on each invocation, so you may notice
it run slightly slower than native CLI operations.

The wrapper scripts mount a limited subset of paths:

- *_~/.kube_*
- *_/etc/origin/_*
- *_/tmp/_*

Be mindful of this when passing in files to be processed by the `oc` or `oadm`
commands. You may find it easier to redirect the input, for example:

----
# oc create -f - < my-file.json
----

[NOTE]
====
The wrapper is intended only to be used to bootstrap an environment. You should
install the CLI tools on another host after you have granted *cluster-admin*
privileges to a user. See
xref:../../admin_guide/manage_rbac.adoc#managing-role-bindings[Managing
Role Bindings] and xref:../../cli_reference/get_started_cli.adoc#cli-reference-get-started-cli[Get Started
with the CLI] for more information.
====

[[containerized-starting-and-stopping-containers]]
== Starting and Stopping Containers

The installation process creates relevant *systemd* units which can be used to
start, stop, and poll services using normal *systemctl* commands. For
containerized installations, these unit names match those of an RPM
installation, with the exception of the *etcd* service which is named
*etcd_container*.

This change is necessary as currently RHEL Atomic Host ships with the *etcd*
package installed as part of the operating system, so a containerized version is
used for the {product-title} installation instead. The installation process
disables the default *etcd* service. 

[NOTE]
====
The *etcd* package is slated to be removed from RHEL Atomic Host in the future.
====

[[containerized-file-paths]]
== File Paths

All {product-title} configuration files are placed in the same locations during
containerized installation as RPM based installations and will survive *os-tree*
upgrades.

However,
xref:../../install_config/imagestreams_templates.adoc#install-config-imagestreams-templates[the default image stream and template files]
are installed at *_/etc/origin/examples/_* for
containerized installations rather than the standard
*_/usr/share/openshift/examples/_*, because that directory is read-only on RHEL
Atomic Host.

[[containerized-storage-requirements]]
== Storage Requirements

RHEL Atomic Host installations normally have a very small root file system.
However, the etcd, master, and node containers persist data in the *_/var/lib/_*
directory. Ensure that you have enough space on the root file system before
installing {product-title}. See the
xref:../../install_config/install/prerequisites.adoc#system-requirements[System
Requirements] section for details.

[[containerized-openvswitch-sdn-initialization]]
== Open vSwitch SDN Initialization

OpenShift SDN initialization requires that the Docker bridge be
reconfigured and that Docker is restarted. This complicates the situation when
the node is running within a container. When using the Open vSwitch (OVS) SDN,
you will see the node start, reconfigure Docker, restart Docker (which restarts
all containers), and finally start successfully.

In this case, the node service may fail to start and be restarted a few times,
because the master services are also restarted along with Docker. The current
implementation uses a workaround which relies on setting the `*Restart=always*`
parameter in the Docker based *systemd* units.
