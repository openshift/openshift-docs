At this point, you have a dynamically created GlusterFS volume bound to a PVC.
You can now utilize this PVC in a pod. In this example, we use a simple
NGINX pod.

. Create the pod object definition:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    name: nginx-pod
spec:
  containers:
  - name: nginx-pod
    image: nginx
    ports:
    - name: web
      containerPort: 80
    volumeMounts:
    - name: gluster-vol1
      mountPath: /usr/share/nginx/html
      readOnly: false
  volumes:
  - name: gluster-vol1
    persistentVolumeClaim:
      claimName: gluster1 <1>
----
<1> The name of the PVC created in the previous step.

. From the {product-title} master host, create the pod:
+
----
# oc create -f nginx-pod.yaml
pod "nginx-pod" created
----

. View the pod. Give it a few minutes, as it might need to download the image if
it does not already exist:
+
----
# oc get pods -o wide
NAME                               READY     STATUS    RESTARTS   AGE       IP               NODE
nginx-pod                          1/1       Running   0          9m        10.38.0.0        node1
----

. `oc exec` into the container and create an *_index.html_* file in the
`mountPath` definition of the pod:
+
----
$ oc exec -ti nginx-pod /bin/sh
$ cd /usr/share/nginx/html
$ echo 'Hello World from GlusterFS!!!' > index.html
$ ls
index.html
$ exit
----

. Now `curl` the URL of the pod:
+
----
# curl http://10.38.0.0
Hello World from GlusterFS!!!
----

. Delete the pod, recreate it, and wait for it to come up:
+
----
# oc delete pod nginx-pod
pod "nginx-pod" deleted
# oc create -f nginx-pod.yaml
pod "nginx-pod" created
# oc get pods -o wide
NAME                               READY     STATUS    RESTARTS   AGE       IP               NODE
nginx-pod                          1/1       Running   0          9m        10.37.0.0        node1
----

. Now `curl` the pod again and it should still have the same data as before.
Note that its IP address may have changed:
+
----
# curl http://10.37.0.0
Hello World from GlusterFS!!!
----

. Check that the *_index.html_* file was written to GlusterFS storage by doing
the following on any of the nodes:
+
----
$ mount | grep heketi
/dev/mapper/VolGroup00-LogVol00 on /var/lib/heketi type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
/dev/mapper/vg_f92e09091f6b20ab12b02a2513e4ed90-brick_1e730a5462c352835055018e1874e578 on /var/lib/heketi/mounts/vg_f92e09091f6b20ab12b02a2513e4ed90/brick_1e730a5462c352835055018e1874e578 type xfs (rw,noatime,seclabel,nouuid,attr2,inode64,logbsize=256k,sunit=512,swidth=512,noquota)
/dev/mapper/vg_f92e09091f6b20ab12b02a2513e4ed90-brick_d8c06e606ff4cc29ccb9d018c73ee292 on /var/lib/heketi/mounts/vg_f92e09091f6b20ab12b02a2513e4ed90/brick_d8c06e606ff4cc29ccb9d018c73ee292 type xfs (rw,noatime,seclabel,nouuid,attr2,inode64,logbsize=256k,sunit=512,swidth=512,noquota)

$ cd /var/lib/heketi/mounts/vg_f92e09091f6b20ab12b02a2513e4ed90/brick_d8c06e606ff4cc29ccb9d018c73ee292/brick
$ ls
index.html
$ cat index.html
Hello World from GlusterFS!!!
----
