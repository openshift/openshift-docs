= Volume Security
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
This topic provides a general guide on pod security context as it relates to
volume security. For information on pod security context in general, please see
link:../../admin_guide/manage_scc.html[Managing Security Context Constraints (SCC)]
and
link:../../architecture/additional_concepts/authorization.html#security-context-constraints[SCC concepts].

As a quick background, the Openshift
link:../../architecture/additional_concepts/storage.html[persistent volume]
framework allows administrators to provision a cluster with persistent storage
and gives users a way to request those resources without requiring knowledge of
the underlying infrastructure. Persistent volumes (PV) are global resources and
not tied to a specific project (namespace). 
link:../../architecture/additional_concepts/storage.html#persistent-volume-claims[Persistent
volume claims (PVC)] are local to the project, and multiple PVCs within the same project
can bind to the same PV. However, once a PVC binds to a PV, that PV cannot be bound by a
claim outside of the first claim's namespace. If the underlying storage needs to be accessed
by multiple projects then each project needs its own PV, which can point to the same physical
storage. In this sense, a bound PV is tied to a namespace. For a detailed PV and PVC example,
see the guide for
https://github.com/openshift/origin/tree/master/examples/wordpress[WordPress andMySQL using NFS].

Accessing persistent storage requires coordination between the cluster (and/or storage)
administrator and the end-developer. The cluster admin creates persistent volumes (PVs),
which abstract the underlying physical storage. The developer creates pods and,
optionally, persistent volume claims (PVCs), which bind to PVs, based on matching
criteria, such as capacity. For the admin, granting pods access to PVs involves knowing
the group ids and/or user id assigned to the actual storage, selinux considerations, and,
finally, ensuring that these ids are allowed in the range of legal ids defined for the
project and/or the SCC that matches the requirements of the pod.

Group ids, the user id, and selinux values are defined in the `*SecurityContext*` stanza
in a pod definition. Group ids are global to the pod and apply to all containers defined
in the pod. User ids can also be global, or specific to each container. Four stanzas
control access to volumes:
link:#fsgroup[`fsGroup`],
link:#supplemental-groups[`supplementalGroups`],
link:#user-id[`runAsUser`], and
link:#selinux[`SELinuxOptions`].

[[scc]]
==== SCCs, Defaults, and Allowed Ranges

(*Note:* "project" and "namespace" are user interchangeably)

SCCs influence whether or not a pod is given a default user id, fsgroup id, supplemental
group id, and selinux label; and, whether or not ids supplied in the pod spec (or in the
image) will be validated against a range of allowable ids. If validation is required and
fails then the pod will also fail.

SCCs define strategies, such as `RunAsUser`, `supplementalGroups`, and `fsGroup`. These
strategies are used to help decide if the pod is authorized. Strategy values set to
_RunAsAny_ are essentially stating that the pod can do what it wants regarding that
strategy. Authorization is skipped for that strategy and no Openshift default is produced
based on that strategy. So ids and selinux labels in the resulting container are based on
container defaults and not on Openshift policies. Here's a quick summary of _RunAsAny_:

- any id defined in the pod spec (or image) is allowed,
- absence of an id in the pod spec (and in the image) results in the container assigning an id,
which is 0 (root) for docker,
- no selinux labels are defined, so docker will assign a unique (and generally not useful) label.

For these reasons, SCCs with _RunAsAny_ for id related strategies should be protected so
that ordinary developers do not have access to the SCC.

On the other hand, SCC strategies set to _MustRunAs_ or _MustRunAsRange_ trigger id validation
(for id related strategies), and cause default values to be supplied by Openshift to the
container (when those values are not supplied directly in the pod spec or image).

SCCs may define the range of allowed ids (user, groups). If range checking is required
(e.g., _MustRunAs_) and the allowable range is not defined in the SCC, then the project
determines the id range. So, projects support ranges of allowable ids; however, unlike SCCs,
projects do not define strategies, such as `runAsUser`. Allowable ranges are interesting
not only because they define the boundaries for container IDs, but also because the
minimum value in the range becomes the *default* value for the id in question. For instance,
if the SCC id strategy value is _MustRunAs_, and the minimum value of an id range is 100,
and the id is absent from the pod spec, then 100 is provided as the default for this id.

As part of pod authorization, the SCCs available to a pod are examined (roughly, in priority
order followed by most restrictive) to best match the requests of the pod. A SCC's strategy's
type of _RunAsAny_ is less restrictive; wheras a type of _MustRunAs_ is more restrictive. All
of these strategies are evaluated. To see which SCC was assigned to a pod use the `oc get pod`
command, below:
```
# oc get pod <pod-name> -o yaml 
...
metadata:
  annotations:
    openshift.io/scc: nfs-scc <1>
  name: nfs-pod1 <2>
  namespace: default <3>
...
```
<1> this is the name of the SCC that the pod used (in this case, a custom SCC)
<2> this is the name of the pod
<3> this is the name of the project

It can be inobvious which SCC was matched by a pod, so the command above can be very useful
in understanding the UID, supplemental groups, and selinux re-labeling in a live container.

There are currently six predefined SCCs, seen below:
```
# oc get scc
NAME               PRIV      CAPS      HOSTDIR   SELINUX     RUNASUSER          FSGROUP    SUPGROUP    PRIORITY
anyuid             false     []        false     MustRunAs   RunAsAny           RunAsAny   RunAsAny    10
hostaccess         false     []        true      MustRunAs   MustRunAsRange     RunAsAny   RunAsAny    <none>
hostmount-anyuid   false     []        true      MustRunAs   RunAsAny           RunAsAny   RunAsAny    <none>
nonroot            false     []        false     MustRunAs   MustRunAsNonRoot   RunAsAny   RunAsAny    <none>
privileged         true      []        true      RunAsAny    RunAsAny           RunAsAny   RunAsAny    <none>
restricted         false     []        false     MustRunAs   MustRunAsRange     RunAsAny   RunAsAny    <none>
```
*Note:* Any SCC with a strategy set to _RunAsAny_ allows arbitrary values for that strategy to be
defined in the pod spec (and/or image). When this applies to the user id (`runAsUser`) it is prudent
to restrict access to the SCC to prevent a container from being able to run as root.

Since often a pod matches the _restricted_ SCC it is worth knowing the security this entails.
The following observations apply to the _restricted_ SCC:

* it constrains user ids due to the `runAsUser` strategy set to _MustRunAsRange_. This forces user
id validation.
* since a range of allowable user ids is not defined in the SCC (see `oc export scc restricted`
for more details), the namespace's `openshift.io/sa.scc.uid-range`) range will be used for range
checking and for a default id, if needed.
* it causes a default user id to be produced when a user id is not specified in the pod spec
(again due to `runAsUser` set to _MustRunAsRange_).
* requires a selinux label (`seLinuxContext` set to _MustRunAs_) which uses the namespace's
default MCS label.
* allows arbitrary supplemental group ids since no range checking is required. This is a result
of both the `supplementalGroups` and `fsGroup` strategies being set to _RunAsAny_.
* produces no default supplemental groups for the running pod, again due to _RunAsAny_ for the
two group strategies above. Therefore, if no groups are defined in the pod spec (or in the
image), the container(s) will have no supplemental groups predefined.

Below is the _default_ namespace and a custom SCC, shown to summarize the interactions of
the SCC and the namespace:
```
oc export ns default <1>
...
metadata:
  annotations: <2>
    openshift.io/sa.scc.mcs: s0:c1,c0 <3>
    openshift.io/sa.scc.supplemental-groups: 1000000000/10000 <4>
    openshift.io/sa.scc.uid-range: 1000000000/10000 <5>
...

# oc export scc a-custom-scc
...
fsGroup:
  type: MustRunAs <6>
  ranges:
  - min: 5000
    max: 6000
runAsUser:
  type: MustRunAsRange <7>
  uidRangeMin: 99
  uidRangeMax: 199
seLinuxContext: <8>
  type: MustRunAs 
  SELinuxOptions: <9>
    user: <selinux-user-name>
    role: ...
    type: ...
    level: ...
supplementalGroups:
  type: MustRunAs <6>
  ranges:
  - min: 5000
    max: 6000
```
<1> "default" is the (unfortunate) name of the project.
<2> recall that defaults are *only* produced when the corresponding SCC stragtegy is *not* _RunAsAny_.
<3> this is the selinux default when not defined in the pod spec or in the SCC.
<4> this is the range of allowable group ids. Id validation only occurs when the SCC stragtegy is
*not* _RunAsAny_. There can be more than one range specified, separated by commas. Two range formats
are supported: 1) _M/N_, where M is the starting id and N is the count, so the range becomes M through,
and including, M+N-1. 2) _M-N_, M is again the starting id and N is the ending id. The default group id
is the starting id in the first range, 1000000000 in the this namespace. If the SCC did not define a
minimum group id then the namespace's default id is applied.
<5> same as (4) but for user ids. Also, only a single range of user ids is supported.
<6> _MustRunAs_ enforces group id range checking and provides the container's groups default. Based
on this SCC definition, the default is 5000 (the min id value). If the range was omitted from the
SCC then the default would be 1000000000, from the namespace. The other supported type, _RunAsAny_,
does not perform range checking, thus allowing any group id, and produces no default groups.
<7> _MustRunAsRange_ enforces user id range checking and provides a UID default. Based on this SCC
the default UID is 99, the min value. If the min/max range were omitted from the SCC, the default
user id would be 1000000000, derived from the namespace. _MustRunAsNonRoot_ and _RunAsAny_ are the
other supported types.
<8> when set to _MustRunAs_, the container is created with the SCC's selinux options, or the
MCS default defined in the namespace. A type of _RunAsAny_ indicates that selinux context is not
required, and if not defined in the pod, is not set in the container.
<9> The selinux user name, role name, type, and labels can be defined here.

[[supplemental-groups]]
== Supplemental Groups
*Note:* the link:#scc[SCC overview], above, should be read before working with supplemental
groups.

Supplemental groups are regular Linux groups. When a process runs in Linux, it has a UID,
a GID, and one or more supplemental groups. These attributes can be set for a container's
main process. The `supplementalGroups` ids are typically used for controlling access to
_shared_ storage, such as NFS and GlusterFS; whereas, link:#fsgroup[fsGroup] is used for
controlling access to _block_ storage, such as Ceph-RBD and iSCSI.

[[nfs-example]]
For example, consider the following NFS export:
====
----
#on an openshift node:
#(Note: showmount needs access to the ports used by rpcbind and rpc.mount on the nfs server)
showmount -e <nfs-server-ip-or-hostname>
Export list for f21-nfs.vm:
/opt/nfs  *

#on the nfs server:
# cat /etc/exports
/opt/nfs *(rw,sync,no_root_squash)
...

# ls -lZ /opt/nfs -d
drwxrws---. nobody 5555 unconfined_u:object_r:usr_t:s0   /opt/nfs

# id nobody
uid=99(nobody) gid=99(nobody) groups=99(nobody)
----
====

The _/opt/nfs/_ export is accessible by UID *99* and the group *5555*. In general, containers
should not run as root, so, in this NFS example, containers which are not run as UID *99* or
are not members the group *5555* will not be able to access the NFS export.

Often, the SCC matching the pod does not allow an arbitrary user id to be specified, thus
using supplemental groups is a more flexible way to grant storage access to a pod. For example,
to grant NFS access to the export above, the group *5555* can be defined in the pod spec, as
shown below (fragment):
```
apiVersion: v1
kind: Pod
...
spec:
  containers:
  - name: ...
    volumeMounts: 
    - name: nfs <1>
      mountPath: /usr/share/... <2>
  securityContext: <3>
    supplementalGroups: [5555] <4>
  volumes:
  - name: nfs <1>
    nfs:
      server: <nfs-server-ip-or-host>
      path: /opt/nfs <5>
```
<1> name of the volume mount, must match the name in the `volumes` section.
<2> nfs export path as seen in the container.
<3> pod global security context: applies to all containers in pod. Note: each container can also define its
`securityContext`; however, group ids are global to the pod, and cannot be defined for individual containers.
<4> supplemental groups, which is an array of ids, is set to 5555. This grants group access to the export.
<5> actual nfs export path on the nfs server.

All containers in the above pod (assuming the matching SCC or project allows the group *5555*) will be
members of the group *5555*, and will have access to the volume, regardless of the container's user id.
However, the assumption above is critical. Sometimes, the SCC does not define a range of allowable group
ids but requires group id validation (due to `supplementalGroups` set to _MustRunAs_; note this is
not the case for the _restricted_ SCC). And, the namespace will not likely allow a group id of 5555
(unless the project has been customized for access to this NFS export). So, in this scenario, the above
pod will fail because its group id of *5555* is not within the SCC's or the namespace's range of allowed
group ids. 

[[scc-supplemental-groups]]
==== Supplemental Groups and Custom SCCs
To remedy this situation a custom SCC can be created such that a min and max group id are defined,
id range checking is enforced, and the group id of 5555 is allowed. It is considered a better
practice to create new SCCs versus modifying a predefined SCC, or changing the range of allowed
ids in the predefined projects. 

The easiest way to create a new SCC is to export an existing SCC and customize the yaml file to 
meet the requirements of the new SCC. For example:
```
# oc export SCC restricted >new-scc.yaml <1>
##edit new-scc.yaml file
# oc create -f new-scc.yaml <2>
```
<1> use the _restricted_ SCC as a template for the new SCC.
<2> instantiate the new SCC

*Note:* the `oc edit scc` command can be used to modify an instantiated SCC.

Here is a fragment of a new SCC named "nfs-scc":
```
# oc export scc nfs-scc 
allowHostDirVolumePlugin: false  #the allow* bools are the same as for the "restricted" scc
...
kind: SecurityContextConstraints
metadata:
  ...
  name: nfs-scc <1>
priority: 9 <2>
...
supplementalGroups:
  type: MustRunAs <3>
  ranges:
  -  min: 5000 <4>
     max: 6000
...
```
<1> the name of the new SCC.
<2> numerically larger numbers have greater priority, nil or omitted is the lowest priority.
Higher priority SCCs sort before lower pri SCCs and thus have a better chance of matching a new pod
<3> `supplementalGroups` is a strategy and it is set to _MustRunAs_, which means group id checking
is required.
<4> multiple ranges are supported. The allowed group id range here is 5000-5999, with the default
supplemental group being 5000.

When the same pod shown above runs against this new SCC (assuming, of course, the pod has access
to the new SCC), it will start because the group *5555*, supplied in the pod spec, is now allowed
by the custom SCC.

[[fsgroup]]
== FS Group
*Note:* the link:#scc[SCC overview], above, should be read before working with fs group.

`*fsGroup*` defines a pod's "file system group" id, which gets added to the container's supplemental
groups. As mentioned link:#supplemental-groups[above], the `supplementalGroups` id applies to shared
storage; whereas, the `fsGroup` id is used for block storage.

Block storage, such as Ceph-RBD, iSCSI, and various cloud storage, is typically dedicated to a single
pod which has requested the block storage volume, either directly or via a persistent volume claim (PVC).
Unlike shared storage, block storage is *_taken over_* by a pod, meaning that user and group ids supplied
in the pod spec (or image) are applied to the actual, physical block device. Typically, block storage is
not shared. Sharing block storage requires that all pods in the namespace define the same group or user
ids, so that when a pod "takes over" the block device, it is still accessible to the other pods. 

A `fsGroup` definition is shown below in the pod spec fragment:
```
kind: Pod
...
spec:
  containers:
  - name: ...
  securityContext: <1>
    fsGroup: 5555 <2>
  ...
```
<1> like with `supplementalGroups`, `fsGroup` must be defined globally to the pod, not per container.
<2> 5555 will become the group id for the volume's group permissions and for all new files created in
the volume.

As is true with `supplementalGroups`, all containers in the above pod (assuming the matching SCC or
project allows the group *5555*) will be members of the group *5555*, and will have access to the
block volume, regardless of the container's user id. If the pod matches the _restricted_ SCC, whose
`fsGroup` strategy is _RunAsAny_, then any `fsGroup` id (including 5555) will be accepted. However,
if the SCC has its `fsGroup` strategy set to _MustRunAs_, and 5555 is not in the allowable range of
fs group ids, then the pod will fail to run.

[[scc-fsgroup]]
==== FS Groups and Custom SCCs
To remedy this situation a custom SCC can be created such that a min and max group id are defined,
id range checking is enforced, and the group id of 5555 is allowed. It is considered a better
practice to create new SCCs versus modifying a predefined SCC, or changing the range of allowed
ids in the predefined projects.

Here is a fragment of a new SCC:
```
# oc export scc <new-scc>
...
kind: SecurityContextConstraints
...
fsGroup:
  type: MustRunAs <1>
  ranges: <2>
  - max: 6000
    min: 5000 <3>
...
```
<1> _MustRunAs_ triggers group id range checking; whereas, _RunAsAny_ does not require range checking.
<2> the range of allowed group ids is 5000 through, and including, 5999. Multiple ranges are supported.
The allowed group id range here is 5000-5999, with the default fs group being 5000.
<3> the min value (or the entire range) can be omitted from the SCC and, thus range checking and generating
a default value will defer to the namespace's `openshift.io/sa.scc.supplemental-groups` range. `fsGroup`
and `supplementalGroups` use the same group field in the namespace (there is not a separate range for fs
group).

When the pod shown above runs against this new SCC (assuming, of course, the pod has access to
the new SCC), it will start because the group *5555*, supplied in the pod spec, is allowed by the
custom SCC. Additionally, the pod will "take over" the block device, so when the block storage is
viewed by a process outside of the pod, it will actually have 5555 as its group permissions.

Currently the list of volumes which support block ownership (block) management include:

* AWS Elastic Block Store
* OpenStack Cinder
* GCE Persistent Disk
* iSCI
* emptyDir
* Ceph RBD
* gitRepo

[[user-id]]
== User ID
User ids can be defined in the container image or in the pod spec. In the pod spec, a single user
id can be define global to all containers, or specific to individual containers (or both). A user
id is supplied as shown in the pod fragement below:
[[pod-user-id-99]]
```
spec:
  containers: <1>
  - name: ...
    securityContext:
      runAsUser: 99  #nobody
```
<1> id 99 is container specific. Specifying `securityContext` outside of the container spec makes
the id global to all containers in the pod.

Similar to group ids, user ids may be validated according to policies set in the SCC and/or
namespace. If the SCC's `runAsUser` strategy is set to _RunAsAny_ then any user id defined in
the pod spec or in the image is allowed.

*Note:* this means a UID of 0 is allowed!

If no user id is supplied in the pod spec (or image), then 0 (root) becomes the default user id.

If, instead, the `runAsUser` strategy is set to _MustRunAsRange_ then a supplied user id will
be validated against a range of allowed ids. If the supplied id is outside of this range the
pod will fail to start. If there is no supplied user id (in the pod or image) then the default
id is the minimum value of the range of allowable user ids.

The range of allowed user ids can be defined in the SCC and/or in the namespace, as shown below:
```
# oc export scc <scc-name> <1>
...
runAsUser: <2>
  type: MustRunAsRange <3>
  uidRangeMax: 100
  uidRangeMin: 88 <4>
...
# oc export ns default <5>
...
kind: Namespace
metadata:
  annotations:
    ...
    openshift.io/sa.scc.uid-range: 1000000000/10000 <6>
...
```
<1> the name of an existing or custom SCC.
<2> straegy for determining user id rules.
<3> _MustRunAsRange_ means that a supplied user id must be within a designated range else the pod
will fail. And, if there is no supplied user id then a default user id, which is the min id in the
allowed range, will be provided. _RunAsAny_ has no restrictions on user id
and must be carefully guarded, else it will be easy for containers to run as root.
<4> the min uid value in the SCC becomes the default. If a min value is not defined in the SCC then
the namespace is used.
<5> "default" is the (unfortunate) name of the current project.
<6> this range is interpreted as allowing user ids between 1000000000 through and including 1000009999.
This range also defines the default user id when no id is supplied in the pod and no range is defined
in the SCC. In this case, the default UID is 1000000000, the min value of the namespace's range.

As an example, using the _restricted_ SCC and the _default_ namespace, here are the user ID default
and allowed values:
```
# oc get scc restricted 
NAME         PRIV      CAPS      HOSTDIR   SELINUX     RUNASUSER        FSGROUP    SUPGROUP   PRIORITY
restricted   false     []        false     MustRunAs   MustRunAsRange   RunAsAny   RunAsAny   <none>
```

The _restricted_ SCC requires user id checking (RUNASUSER set to _MustRunAsRange_), but supplies
no user id range (the min/max values are not visible in `oc get scc` above, but are shown in
`oc export scc restricted`). Therefore, the user id's allowable range comes from the _default_
namespace, shown above, and is 1000000000 - 1000009999 (inclusive).

Getting back to the link:#nfs-example[NFS example], the container needs it's UID set to 99,
which is shown in the link:#pod-user-id-99[pod fragement above].

Assuming the _default_ project and the _restricted_ SCC, the pod's requested user id of 99
will, unfortunately, *not* be allowed and therefore the pod will fail. The pod fails because:

- it requests 99 as its user id,
- all SCCs available to the pod are examined (roughly in priority order followed by most restrictive)
to see which SCC will allow a user id of 99 (actually, all policies of the SCCs are checked but the 
focus here is on user id),
- since all available SCCs use _MustRunAsRange_ for their `*runAsUser*` strategy, uid range checking
is required, 
- 99 is not included in the SCC or namespace's user id range, so the pod fails.

To fix this situation:

- the _restricted_ SCC could be modified to include 99 within the min and max user ids
(*not* recommended),
- the _restricted_ SCC could be modified to use _RunAsAny_ for the `*runAsUser*` value,
thus eliminating id range checking (*not* recommended -- containers can run as root),
- a new SCC could be created with the appropriate user id range (recommended),
- the _default_ project's UID range could be changed to allow a user id of 99.
(not generally advisable since only a single range of user ids can be specified),
- a new project could be created with the appropriate user id range defined (not covered here).

===== Custom SCC for UserID:
It's generally considered a good practice to *not* modify the predefined SCCs. The preferred approach
is to create a custom SCC that better fits an organization's security needs, or create a new project
that supports the desired user ids. See
link:../../dev_guide/projects.html#create-a-project[projects] on creating a new project.

A custom SCC can be created such that a min and max user id is defined, UID range
checking is still enforced, and the UID of 99 will be allowed. Here is an example:
```
# oc export scc nfs-scc 
allowHostDirVolumePlugin: false  #the allow* bools are the same as for the restricted scc
...
kind: SecurityContextConstraints
metadata:
  ...
  name: nfs-scc <1>
priority: 9 <2>
requiredDropCapabilities: null
runAsUser:
  type: MustRunAsRange <3>
  uidRangeMax: 99 <4>
  uidRangeMin: 99
...
```
<1> the name of this new SCC is "nfs-scc"
<2> numerically larger numbers have greater priority, nil or omitted is the lowest priority.
Higher priority SCCs sort before lower pri SCCs and thus have a better chance of matching a new pod
<3> `*runAsUser*` is a strategy and it is set to _MustRunAsRange_, which means uid range checking is 
enforced
<4> the uid range is 99-99 (a range of one value).

Now, using `runAsUser: 99`, shown in the pod fragment above, the pod to matches the new nfs-scc and is
able to run with a UID of 99.

[[selinux]]
== SELinuxOptions

SELinux labels can be defined in the pod's `*securityContext*` stanza using `level`,
shown in the pod spec fragment below:
```
...
 securityContext: <1>
    seLinuxOptions:
      level: s0:c1,c0 <2>
...
```
<1> `level` can be defined globally for the entire pod, or specifically to each container.
<2> in addition to `level`, `user`, `role`, `type` can also be specified.

Here are fragements from a SCC and from the _default_ project:
```
... #SCC
seLinuxContext:
  type: MustRunAs <1>
...
# oc export ns default 
...
metadata:
  annotations:
    openshift.io/sa.scc.mcs: s0:c1,c0 <2>
...
```
<1> _MustRunAs_ should be used in the SCCs so that volume relabeling can be performed by the container.
<2> if the label is not provided in the pod or in the SCC then the default comes from the namespace.

All predefined SCCs, except for the _privileged_ SCC, set the `seLinuxContext:` to _MustRunAs_.
This forces poda to use MCS labels, which can be defined in the pod spec, or provided as a default.

...
An SELinuxContext strategy of MustRunAs with no level set. Admission looks for the openshift.io/sa.scc.mcs annotation to populate the level.

The SCC determines whether or not to require a selinux label and can provide a default label.
If `*seLinuxContext*` in the SCC is set to _MustRunAs_, and the pod (or image) does not
define a label then a default, either from the SCC itself or the namespace, is used. If
`seLinuxContext`  is set to _RunAsAny_ then no default labels are provided, so the container
determines the final label. In the case of docker, the container will use a unique MCS label,
which will not likely match the label on existing storage mounts.

Additionally, volumes which support SELinux management,
will be relabeled so that they are accessible by the specified label and,
depending on how exclusionary the label is, only that label.

This means two things for unprivileged containers:

* the volume will be given a `*type*` which is accessible by unprivileged containers.
Usually *svirt_sandbox_file_t*.
* If a `*level*` is specified, the volume will be labeled with the given MCS label.

[NOTE]
====
Level and MCS label are used interchangeably in this topic.
====

For your volume to be accessible by your pod, the pod must have both categories
of the volume. So a pod with *s0:c1,c2* will be able to access volumes with
*s0,c1,c2*, and a volume with *s0* will be accessible by all pods.

[WARNING]
====
Hard coding MCS labels into your pod definition makes it easy for others to
determine what MCS label is needed to access the same volume as the defined pod.
So it is especially important to rely on the MCS labels allocated by OpenShift
and to use this feature with care.
====

SELinux options are specified as follows:

====
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: ebs-web
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
      volumeMounts:
          - name: ebs-volume
            mountPath: "/usr/share/nginx/html"
  securityContext:
    seLinuxOptions:
    level: "s0:c123,c456"
  volumes:
    - name: ebs-volume
      awsElasticBlockStore:
      volumeID: <VOLUME ID>
----
====

Currently the list of volumes which support SELinux management includes:

* AWS Elastic Block Store
* OpenStack Cinder
* GCE Persistent Disk
* iSCSI
* emptyDir
* Ceph RBD
* gitRepo

GlusterFS and NFS do not support SELinux management.

== UID and GID Management with NFS and GlusterFS
As mentioned above, link:persistent_storage_nfs.html[NFS] and
link:persistent_storage_glusterfs.html[GlusterFS] do not support ownership
management. This is because they do not allow `chown` and `chmod` on the client
side. As a result, when you are using NFS and GlusterFS, you must set the
appropriate ownership on the server side, then use `*supplementalGroups*` to
match the group. You can also use `*runAsUser*` to match the user ID.

However, there are a few caveats in this setup that you should be aware of.

=== NFS root_squash Option
NFS usually runs with *root_squash* as a default option. This option tells the
NFS server to squash any attempt to do something using UID 0 to *nfsnobody*. So
if you have a container which is running as *root* and it tries to create a
file, the file will be owned by the *nfsnobody* user.

=== NFS all_squash Option
If the NFS server you are using was set up with the *all_squash* option turned
on, you will not be able to create files which are owned by an arbitrary user or
group. All files will end up being owned by *nfsnobody*.

=== Applications With Strict UID Requirements
Certain applications, such as MySQL, and PostgreSQL, double-check the ownership
of the files they create, and they require that the files be owned by the
application's configured user ID. An application like this cannot be run on an
NFS server which enables *all_squash*, for example, so you would have to turn
that off.

=== NetApp NFSv4 vs NFSv3
NetApp NFSv4 by default enables the *all_squash* option.
https://library.netapp.com/ecmdocs/ECMP1196993/html/GUID-24367A9F-E17B-4725-ADC1-02D86F56F78E.html[This
can be turned off]. However, if you are using NFSv4, NetApp will require that
you setup an authentication system and export `*AUTH_SYSTEM*`. With NFSv3, the
`*AUTH_SYSTEM*` requirement is not strict.
