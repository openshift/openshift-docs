= Persistent Storage Using NFS
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

OpenShift clusters can be provisioned with
link:../../architecture/additional_concepts/storage.html[persistent storage] using NFS
Some familiarity with Openshift,
link:../../architecture/additional_concepts/storage.html#persistent-volume-claims[persistent volume claims (PVCs)],
and https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch-nfs.html[NFS] is beneficial.

The Openshift persistent volume (PV) framework allows administrators to provision a cluster
with persistent storage and gives developers the ability to request those resources without
having specific knowledge of the underlying infrastructure. Persistent volumes are not bound
to a single project; they can be shared across the entire OpenShift cluster. Persistent volume
claims (PVCs), however, are specific to a project and are created and used by developers.

(*Note:* "project" and "namespace" are used interchangeably throughout this document)

== Provisioning

Storage must exist in the underlying infrastructure before it can be mounted as a volume in
OpenShift. To provision NFS volumes a list of NFS servers and export paths are all that is required.

*Note:*
Each NFS volume must be mountable by all schedulable nodes in the cluster.

A PV spec file is usually defined before creating the PV:
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0001 <1>
spec:
  capacity:
    storage: 5Gi <2>
  accessModes:
  - ReadWriteOnce <3>
  nfs: <4>
    path: /tmp <5>
    server: 172.17.0.2 <6>
  persistentVolumeReclaimPolicy: Recycle <7>
```
<1> the name of the volume. This is the PV identify in various `oc ... pod` commands.
<2> the amount of storage allocated to this volume.
<3> though this appears to be related to controlling access to the volume, in fact, it
is actually used similarly to labels and used to match a PVC to a PV. Currently, no
access rules are enforced based on the `accessModes`.
<4> this defines the volume type being used, in this case the *nfs* plug-in.
<5> the path that is exported by the NFS server.
<6> the host name or IP address of the NFS server.
<7> defines what happens to a volume when released from its claim. Valid options
are *Retain* (default) and *Recycle*. See
link:#nfs-reclaiming-resources[Reclaiming Resources].

Save the definition to a file, for example *_nfs-pv.yaml_*, and create the persistent
volume:
```
# oc create -f nfs-pv.yaml
persistentvolume "pv0001" created
```

Verify that the persistent volume was created:
```
# oc get pv
NAME                     LABELS    CAPACITY     ACCESSMODES   STATUS      CLAIM     REASON    AGE
pv0001                   <none>    5368709120   RWO           Available                       31s
```

The next step can be to create a persistent volume claim (PVC) which will bind to the new PV.
This is easy to do, as shown in the "nfs-claim.yaml example below:
```
#claim yaml file: "nfs-claim.yaml"

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-claim1
spec:
  accessModes:
    - ReadWriteOnce <1>
  resources:
    requests:
      storage: 1Gi <2>

# oc create -f nfs-claim.yaml
```
<1> as mentioned above for PVs, the `accessModes` do not enforce security, but rather act as
labels to match a PV to a PVC.
<2> this claim will look for PVs offering 1Gi or greater capacity.

*Note:* PVs, and PVCs are not necessary, just convenient, and make sharing a volume across a
project simpler. The NFS specific information contained in the PV spec can also be defined
directly in a pod spec.

[[nfs-enforcing-disk-quotas]]
=== Enforcing Disk Quotas
Disk partitions can be used to enforce disk quotas and size constraints. Each partition
can be its own export. Each export is one persistent volume (PV). Openshift enforces
unique names for PVs, but the uniqueness of the NFS volume's server and path is up to
the administrator.

Enforcing quotas in this way allows the developer to request persistent storage
by a specific amount (e.g, 10Gi) and be matched with a corresponding volume of
equal or greater capacity.

[[nfs-volume-security]]
== Volume Security
This section covers NFS volume security, including matching permissions and selinux considerations. 
The reader is expected to understand the basics of POSIX permissions, process UIDs, supplemental groups, and selinux.

*Note:* The
link:pod_security_context.html[pod and storage security] document should be read before
implementing NFS volumes.

Developers request NFS storage by referencing, in the `volumes` section of their
pod spec, either a _PersistentVolumeClaim_ by name, or the NFS volume plug-in directly.

The *_/etc/exports_* file, on the NFS server, contains the accessible NFS directories.
The target NFS directory has the familiar POSIX owner and group ids. The Openshift NFS
plug-in mounts the container's NFS directory with the same POSIX ownership and permissions
found on the exported NFS directory. However, the container is not run with its effective
UID equal to the owner of the NFS mount, which is the desired behavior.

As an example, if the target NFS directory appears as:
[[nfs-export]]
```
#on the nfs server:
# ls -lZ /opt/nfs -d
drwxrws---. nobody 5555 unconfined_u:object_r:usr_t:s0   /opt/nfs

# id nobody
uid=99(nobody) gid=99(nobody) groups=99(nobody)
```
then the container will need to match selinux labels, and either run with a UID of 99
(_nobody_ owner), or with 5555 in its supplemental groups, in order to access the directory.

*Note:* the owner id of 99 is used as an example. Even though NFS's _root_squash_ maps *_root_* (0) to *_nobody_* (99), NFS exports can have arbitrary owner ids.
 Owner 99 is not required for NFS exports.

[[supplemental-groups]]
=== Group IDs
*Note:* it's generally preferable to use supplemental group ids to gain access to persistent storage vs. using link:#user-ids[user ids].

The recommended way to handle NFS access (assuming it's not a choice to change permissions on the NFS export)
is to use supplemental groups. Supplemental groups in Openshift are used for shared storage, of which
NFS is an exmaple. In contrast, block storage, such as Ceph RBD or iSCSI, use the `fsGroup` SCC strategy
and  the `fsGroup` value in the pod's `securityContext`.

*Note:* supplemental groups are covered in the
link:pod_security_context.adoc#supplemental-groups[pod and storage security] document, which should be read prior to setting up NFS persistent storage.

Since the group id on the link:#nfs-export[target NFS directory], shown above, is 5555,
the pod can define that group id using `suplementalGroups` under pod's global
`securityContext` definition. For example:
```
spec:
  containers:
    - name: ...
      #runAsUser: 99 from above has been commented out here
  securityContext: <1>
    supplementalGroups: [5555] #an array of GIDs defined globally for the pod
```
<1> securityContext must be defined globally to the pod, not under a specific container.

Assuming no custom SCCs that might satisfy the pod's requirements, the pod will likely match the _restricted_ SCC. This SCC has the `supplementalGroups` strategy set to _RunAsAny, meaning that any supplied group id will be accepted without range checking.

So, the above pod will pass admissions and will be launched. However, if group id range checking is desired, a custom SCC, as described in 
link:pod_security_context#scc-supplemental-groups[pod security and custom SCCs], is the preferred solution.
A custom SCC can be created such that a min and max group ids are defined, group id range checking is enforced, and a group id of 5555 will be allowed.

[[user-ids]]
=== User IDs
*Note:* it's generally preferable to use link:#supplemental-groups[supplemental group ids] to gain access to persistent storage vs. using user ids.

User ids can be defined in the container image or in the pod spec. The
link:pod_security_context.html#user-id[pod and storage security] document covers
controlling storage access based on user ids, and should be read prior to setting up NFS persistent storage.

In the link:#nfs-export[NFS export], above, the container needs it's UID set to 99 
(ignoring group ids for the moment), so the following fragement can be added to the pod spec:
```
spec:
  containers: <1>
  - name: ...
    securityContext:
      runAsUser: 99  #nobody
```
<1> pods contain a `securtityContext` specific to each container (shown above), and a global
`securityContext` which applies to all containers defined in the pod.

Assuming the _default_ project and the _restricted_ SCC, the pod's requested user id of 99
will, unfortunately, *not* be allowed, and therefore the pod will fail. The pod fails because:

- it requests 99 as its user id,
- all SCCs available to the pod are examined to see which SCC will allow a user id of 99
(actually, all policies of the SCCs are checked but the  focus here is on user id),
- since all available SCCs use _MustRunAsRange_ for their `runAsUser` strategy, uid
range checking is required, 
- 99 is not included in the SCC or namespace's user id range, so the pod fails.

It's generally considered a good practice to *not* modify the predefined SCCs. The preferred 
way to fix this situation is to create a custom SCC, as described in 
link:pod_security_context#scc-runasuser[pod security and custom SCCs].
A custom SCC can be created such that a min and max user ids are defined, UID range checking is still enforced, and the UID of 99 will be allowed.

=== SELinux
The link:pod_security_context.html#selinux[pod and storage security] document covers controlling storage access in conjunction with using selinux.

==== SELinux and NFS Export Settings
By default, SELinux does not allow writing from a pod to a remote NFS server.
The NFS volume mounts correctly, but is read-only.

To enable writing to NFS volumes with SELinux enforcing on each node, run:

----
# setsebool -P virt_use_nfs 1
----
The `-P` option makes the bool persistent between reboots.

The *virt_use_nfs* boolean is defined by the *_docker-selinux_* package. If an 
error is seen indicating that this bool is not defined, then ensure that this 
package has been installed.

Additionally, in order to enable arbitrary container users to read and write the
volume, each exported volume on the NFS server should conform to the following:

- Each export must be:
+
----
/<example_fs> *(rw,root_squash)
----
- The firewall must be configured to allow traffic to the mount point. The default port is 2049:
+
----
# iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT
----
- The NFS export and directory must be set up so that it is accessible by the target pods. 
Either set the export to be owned by the container's primary UID, or supply 
the pod group access using `suppplementalGroups`, as shown in the 
link:#supplemental-groups[supplemental groups] section above. See also, 
link:pod_security_context.html[Volume Security] for additional pod security information.

[[nfs-reclaiming-resources]]
== Reclaiming Resources
NFS implements the Openshift *Recyclable* plug-in interface. Automatic
processes handle reclamation tasks based on policies set on each persistent
volume.

By default, persistent volumes are set to *Retain*. NFS volumes which are set to
*Recycle* are scrubbed (i.e., `rm -rf` is run on the volume) after being
released from their claim (i.e, after the user's `*PersistentVolumeClaim*` bound
to the volume is deleted). Once recycled, the NFS volume can be bound to a new
claim.

[[nfs-automation]]
== Automation
As discussed, clusters can be provisioned with persistent storage using NFS in
the following way:

- Disk partitions can be used to link:#nfs-enforcing-disk-quotas[enforce storage
quotas].
- Security can be enforced by link:#nfs-volume-security[restricting volumes] to
the namespace that has a claim to them.
- link:#nfs-reclaiming-resources[Reclamation of discarded resources] can be
configured for each persistent volume.

They are many ways that you can use scripts to automate the above tasks. You can
use an
link:https://github.com/openshift/openshift-ansible/tree/master/roles/kube_nfs_volumes[example
Ansible playbook] to help you get started.

[[nfs-additional-config-and-troubleshooting]]

== Additional Configuration and Troubleshooting
Depending on what version of NFS is being used and how it is configured, there 
configuration steps needed for proper export and security mapping.  Below are s
may apply:

`*NFSv4 mount incorrectly shows all files with ownership of nobody:nobody*`

- Could be attributed to the ID mapping settings (/etc/idmapd.conf) on your NFS
- See link:https://access.redhat.com/solutions/33455[this Red Hat Solution arti

`*To disable ID mapping on NFSv4*`

- On both the NFS client and server:
----
# echo 'Y' > /sys/module/nfsd/parameters/nfs4_disable_idmapping
----
