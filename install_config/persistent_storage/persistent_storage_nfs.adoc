= Persistent Storage Using NFS
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
You can provision your OpenShift cluster with
link:../../architecture/additional_concepts/storage.html[persistent storage] using
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch-nfs.html[NFS].
Some familiarity with Kubernetes and NFS is assumed.

The Kubernetes
link:../../architecture/additional_concepts/storage.html[persistent volume]
framework allows administrators to provision a cluster with persistent storage
and gives users a way to request those resources without having any knowledge of
the underlying infrastructure. Persistent volumes are not bound to a single
project or namespace; they can be shared across the OpenShift cluster.
link:../../architecture/additional_concepts/storage.html#persistent-volume-claims[Persistent
volume claims], however, are specific to a project or namespace and can be
requested by users.

For a detailed example, see the guide for
https://github.com/openshift/origin/tree/master/examples/wordpress[WordPress and
MySQL using NFS].

[IMPORTANT]
====
High-availability of storage in the infrastructure is left to the underlying
storage provider.
====

[[nfs-provisioning]]

== Provisioning
Storage must exist in the underlying infrastructure before it can be mounted as
a volume in OpenShift. To provision NFS volumes in OpenShift, all that is
required is a distinct list of NFS servers and paths and the
`*PersistentVolume*` API.

You must define your persistent volume in an object definition before creating
it in OpenShift:

.Persistent Volume Object Definition Using NFS
====

[source,yaml]
----
apiVersion: "v1"
kind: "PersistentVolume"
metadata:
  name: "pv0001" <1>
spec:
  capacity:
    storage: "5Gi" <2>
  accessModes:
    - "ReadWriteOnce" <3>
  nfs: <4>
    path: "/tmp" <5>
    server: "172.17.0.2" <6>
  persistentVolumeReclaimPolicy: "Recycle" <7>
----
<1> The name of the volume. This will be how it is identified via
link:../../architecture/additional_concepts/storage.html[persistent volume
claims] or from pods.
<2> The amount of storage allocated to this volume.
<3> This is the policy for restricting read/writes to a shared volume.
<4> This defines the volume type being used, in this case the *nfs* plug-in.
<5> The path that is exported by the NFS server.
<6> The host name or IP address of the NFS server.
<7> Defines what happens to a volume when released from its claim. Valid options
are *Retain* (default) and *Recycle*. See
link:#nfs-reclaiming-resources[Reclaiming Resources].
====

Save your definition to a file, for example *_nfs-pv.yaml_*, and create the
persistent volume:

====
----
# oc create -f nfs-pv.yaml
persistentvolume "pv0001" created
----
====

Verify that the persistent volume was created:

====
----
# oc get pv
NAME                     LABELS    CAPACITY     ACCESSMODES   STATUS      CLAIM     REASON    AGE
pv0001                   <none>    5368709120   RWO           Available                       31s
----
====

Users can then link:../../dev_guide/persistent_volumes.html[request storage
using persistent volume claims], which can now utilize your new persistent
volume.

[[nfs-enforcing-disk-quotas]]

=== Enforcing Disk Quotas
Use disk partitions to enforce disk quotas and size constraints. Each partition
can be its own export. Each export is one persistent volume. Kubernetes enforces
unique names for persistent volumes, but the uniqueness of the NFS volume's
server and path is up to the administrator.

Enforcing quotas in this way allows the end user to request persistent storage
by a specific amount (e.g, 10Gi) and be matched with a corresponding volume of
equal or greater capacity.

[[nfs-volume-security]]

=== Volume Security
Users request storage with a `*PersistentVolumeClaim*`. This claim only lives in
the user's namespace and can only be referenced by a pod within that same
namespace. Any attempt to access a persistent volume across a namespace causes
the pod to fail.

Each NFS volume must be mountable by all nodes in the cluster.

[[nfs-reclaiming-resources]]

== Reclaiming Resources
NFS implements the Kubernetes *Recyclable* plug-in interface. Automatic
processes handle reclamation tasks based on policies set on each persistent
volume.

By default, persistent volumes are set to *Retain*. NFS volumes which are set to
*Recycle* are scrubbed (i.e., `rm -rf` is run on the volume) after being
released from their claim (i.e, after the user's `*PersistentVolumeClaim*` bound
to the volume is deleted). Once recycled, the NFS volume can be bound to a new
claim.

[[nfs-automation]]

== Automation
As discussed, clusters can be provisioned with persistent storage using NFS in
the following way:

- Disk partitions can be used to link:#nfs-enforcing-disk-quotas[enforce storage
quotas].
- Security can be enforced by link:#nfs-volume-security[restricting volumes] to
the namespace that has a claim to them.
- link:#nfs-reclaiming-resources[Reclamation of discarded resources] can be
configured for each persistent volume.

They are many ways that you can use scripts to automate the above tasks. You can
use an
link:https://github.com/openshift/openshift-ansible/tree/master/roles/kube_nfs_volumes[example
Ansible playbook] to help you get started.

[[selinux-and-nfs-export-settings]]

== SELinux and NFS Export Settings
By default, SELinux does not allow writing from a pod to a remote NFS server.
The NFS volume mounts correctly, but is read-only.

To enable writing to NFS volumes with SELinux enforcing on each node, run:

----
# setsebool -P virt_use_nfs 1
----

The `-P` option makes the bool persistent between reboots.

Additionally, in order to enable arbitrary container users to read and write the
volume, each exported volume on the NFS server itself should conform to the
following:

- Each export must be:
+
----
/<example_fs> *(rw,root_squash)
----
- The firewall must be configured to allow traffic to the mount point, for NFSv4 the default port is 2049 (nfs).  
  For NFSv3 there are three ports that need to be configured: 2049 (nfs), 20048 (mountd) and 111 (port mapper).
+
NFSv4:
+
----
# iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT
----
+
NFSv3:
+
----
# iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT
# iptables -I INPUT 1 -p tcp --dport 20048 -j ACCEPT
# iptables -I INPUT 1 -p tcp --dport 111 -j ACCEPT
----

- The NFS export and directory must be set up so that it is accessible by your
pods. Either set the export to be owned by the container's primary UID, or give
your pod group based access using `*SupplementalGroups*`. See
link:pod_security_context.html[Volume Security] for more information.


[[nfs-additional-config-and-troubleshooting]]

== Additional Configuration and Troubleshooting
Depending on what version of NFS is being used and how it is configured, there may be additional
configuration steps needed for proper export and security mapping.  Below are some links and topics that
may apply:

`*NFSv4 mount incorrectly shows all files with ownership of nobody:nobody*`

- Could be attributed to the ID mapping settings (/etc/idmapd.conf) on your NFS client and server, as NFSv4 utilizes ID mapping to ensure permissions are set properly on exported shares, if the domains of the client and server do not match then the permissions are mapped to nobody:nobody
- See link:https://access.redhat.com/solutions/33455[this Red Hat Solution article] for more information on how to diagnose and resolve this issue.

`*To disable ID mapping on NFSv4*`

- On both the NFS client and server:
----
# echo 'Y' > /sys/module/nfsd/parameters/nfs4_disable_idmapping
----

