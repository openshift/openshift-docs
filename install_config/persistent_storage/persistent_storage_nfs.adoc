= Persistent Storage Using NFS
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
You can provision your OpenShift cluster with
link:../../architecture/additional_concepts/storage.html[persistent storage] using
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch-nfs.html[NFS].
Some familiarity with Kubernetes and NFS is assumed.

The Kubernetes
link:../../architecture/additional_concepts/storage.html[persistent volume (PV)]
framework allows administrators to provision a cluster with persistent storage
and gives users a way to request those resources without having any knowledge of
the underlying infrastructure. Persistent volumes are not bound to a single
project or namespace; they can be shared across the OpenShift cluster.
link:../../architecture/additional_concepts/storage.html#persistent-volume-claims[Persistent
volume claims (PVC)], however, are specific to a project or namespace and can be
requested by users.

For a detailed example, see the guide for
https://github.com/openshift/origin/tree/master/examples/wordpress[WordPress and
MySQL using NFS].

[IMPORTANT]
====
High-availability of storage in the infrastructure is left to the underlying
storage provider.
====

[[nfs-provisioning]]

== Provisioning
Storage must exist in the underlying infrastructure before it can be mounted as
a volume in OpenShift. To provision NFS volumes in OpenShift, all that is
required is a distinct list of NFS servers and paths and the
`*PersistentVolume*` API.

You must define your persistent volume in an object definition before creating
it in OpenShift:

.Persistent Volume Object Definition Using NFS
====

[source,yaml]
----
apiVersion: "v1"
kind: "PersistentVolume"
metadata:
  name: "pv0001" <1>
spec:
  capacity:
    storage: "5Gi" <2>
  accessModes:
    - "ReadWriteOnce" <3>
  nfs: <4>
    path: "/tmp" <5>
    server: "172.17.0.2" <6>
  persistentVolumeReclaimPolicy: "Recycle" <7>
----
<1> The name of the volume. This will be how it is identified via
link:../../architecture/additional_concepts/storage.html[persistent volume
claims] or from pods.
<2> The amount of storage allocated to this volume.
<3> This is the policy for restricting read/writes to a shared volume.
<4> This defines the volume type being used, in this case the *nfs* plug-in.
<5> The path that is exported by the NFS server.
<6> The host name or IP address of the NFS server.
<7> Defines what happens to a volume when released from its claim. Valid options
are *Retain* (default) and *Recycle*. See
link:#nfs-reclaiming-resources[Reclaiming Resources].
====

Save your definition to a file, for example *_nfs-pv.yaml_*, and create the
persistent volume:

====
----
# oc create -f nfs-pv.yaml
persistentvolume "pv0001" created
----
====

Verify that the persistent volume was created:

====
----
# oc get pv
NAME                     LABELS    CAPACITY     ACCESSMODES   STATUS      CLAIM     REASON    AGE
pv0001                   <none>    5368709120   RWO           Available                       31s
----
====

Users can then link:../../dev_guide/persistent_volumes.html[request storage
using persistent volume claims (PVC)], which can now utilize your new persistent
volume.

[[nfs-enforcing-disk-quotas]]

=== Enforcing Disk Quotas
Use disk partitions to enforce disk quotas and size constraints. Each partition
can be its own export. Each export is one persistent volume. Kubernetes enforces
unique names for persistent volumes, but the uniqueness of the NFS volume's
server and path is up to the administrator.

Enforcing quotas in this way allows the end user to request persistent storage
by a specific amount (e.g, 10Gi) and be matched with a corresponding volume of
equal or greater capacity.

[[nfs-volume-security]]

=== Volume Security
This section covers pieces of Security Context Constraints (SCC), which are descr
ibed elsewhere, 
see link:../../admin_guide/manage_scc.adoc[managing SCCs] and link:../../architec
ture/additional_concepts/authorization.adoc#security-context-constraints[SCC conc
epts].
The reader is expected to understand the basics of POSIX permissions, process UIDs,
supplemental groups, and selinux.

Developers request NFS storage by referencing, in the `volumes:` section of their
 pod spec,
either a _PersistentVolumeClaim_ by name, or the NFS volume plugin directly. Alth
ough a Persistent
Volume resides in a global namespace, a persistent volume claim (PVC) lives in th
e user's namespace
(project), and the claim can only be referenced by a pod within that same namespa
ce.

The *_/etc/exports_* file, on the NFS server, contains the accessible NFS directo
ries. The target
NFS directory, which is referenced by the pod in either the `path:` section of th
e PV spec or
directly in the pod spec, has the familiar POSIX owner and group ids. The Openshift NFS plugin
mounts the container's NFS directory with the same POSIX ownership and permissions found on the
target NFS directory. However, the container is not run with its effective UID equal to the owner
of the NFS mount, which is the desired behavior.

As an example, if the target NFS directory appears as:
```
#on the nfs server:
# ls -lZ /opt/nfs -d
drwxrws---. nobody 5555 unconfined_u:object_r:usr_t:s0   /opt/nfs

# id nobody
uid=99(nobody) gid=99(nobody) groups=99(nobody)
```
then the container will need to match selinux labels, and either run with a UID of 99 (_nobody_ owner),
or with 5555 in its supplemental groups in order to access the directory.

==== User IDs:
User ids can be defined in the container image or in the pod spec. If a user id is supplied and
the matching SCC's `runAsUser:` strategy is _MustRunAsRange_ then that id will be validated against
the min and max user ids defined in that SCC. If min/max user ids are not defined in the SCC then
the user id is validated against the namespace's `openshift.io/sa.scc.uid-range` value. On the
other hand, if the user id is omitted then the default UID becomes the matching SCC's `runAsUser:`
strategy's `uidRangeMin:` value. Or, if a min value is not specified in the SCC, then the first
number in the namespace's `openshift.io/sa.scc.uid-range` becomes the default user id.

As an example, using the _restricted_ SCC and the _default_ namespace, here are the user ID default
and allowed values:
```
# oc get scc restricted 
NAME         PRIV      CAPS      HOSTDIR   SELINUX     RUNASUSER        FSGROUP    SUPGROUP   PRIORITY
restricted   false     []        false     MustRunAs   MustRunAsRange   RunAsAny   RunAsAny   <none>
                                                        <1>
```
<1> _MustRunAsRange_ enforces UID checking. In comparison, a value of _RunAsAny_ would not trigger UID
range checking and thus would accept any user id.

So, the _restricted_ SCC requires user id checking, but supplies no user id range (the id min/max values,
are not visible in `oc get scc` above, but are shown in `oc export scc restricted`). Therefore, the
user id range must come from the _default_ namespace, seen below:
```
# oc export ns default
...
kind: Namespace
metadata:
  annotations:
    ...
    openshift.io/sa.scc.uid-range: 1000000000/10000 <1>
...
```
<1> this range is interpreted as allowing user ids between 1000000000 through and including 1000009999.
If no user id is specified then the default user id will be the min value of 1000000000.

Getting back to the NFS example above: the container needs it's UID to be 99 (ignoring group ids for
the moment), so the following fragement can be added to the pod spec:
```
spec:
  containers: <1>
  - name: ...
    securityContext:
      runAsUser: 99  #nobody
```
<1> *NOTE:* pods contain a `securtityContext:` specific to each container (shown above) and a global
`securityContext:` which applies to all containers defined in the pod.

Aassuming the _default_ project and the _restricted_ SCC above, the pod's requested user id of 99
will, unfortunetely, *not* be allowed and therefore the pod will fail. The pod fails because:

- it requests 99 as its user id,
- all SCCs available to the pod are examined (roughly in priority order followed by most restrictive)
to see which SCC will allow a user id of 99 (actually, all policies of the SCCs are checked but the 
focus here is on user id),
- since all available SCCs use _MustRunAsRange_ for their `runAsUser:` strategy, uid range checking is 
required, 
- 99 is not included in the SCC or namespace's user id range, so the pod fails.

To fix this situation:

- the _restricted_ SCC could be modified to include 99 within the min and max user ids
(*not* recommended),
- the _restricted_ SCC could be modified to use _RunAsAny_ for the `runAsUser:` value,
thus eliminating id range checking (*not* recommended -- containers can run as root),
- a new SCC could be created with the appropriate user id range (recommended),
- a new SCC could be created with the `runAsUser:` strategy set to _RunAsAny_
(*caution:* need to be mindful of containers being able to run as root),
- the _default_ project's UID range could be changed to allow a user id of 99.
(not generally advisable since only a single range of user ids can be specified),
- a new project could be created with the appropriate user id range defined (not covered here).

====== Custom SCC for UserID:
It's generally considered a good practice to *not* modify the predefined SCCs. The preferred approach
is to create a custom SCC that better fits an organization's security needs, or create a new project
that supports the desired user ids. See
link:../../dev_guide/projects.adoc#create-a-project[projects] on creating a new project.

A custom SCC can be created such that a min and max user id is defined, UID range
checking is still enforced, and the UID of 99 will be allowed. Here is an example:
```
# oc export scc nfs-scc 
allowHostDirVolumePlugin: false  #the allow* bools are the same as for the restricted scc
...
kind: SecurityContextConstraints
metadata:
  ...
  name: nfs-scc <1>
priority: 9 <2>
requiredDropCapabilities: null
runAsUser:
  type: MustRunAsRange <3>
  uidRangeMax: 99 <4>
  uidRangeMin: 99
...
```
<1> the name of this new SCC is "nfs-scc"
<2> numerically larger numbers have greater priority, nil or omitted is the lowest priority.
Higher priority SCCs sort before lower pri SCCs and thus have a better chance of matching a new pod
<3> `runAsUser:` is a strategy and it is set to _MustRunAsRange_, which means uid range checking is 
enforced
<4> the uid range is 99-99 (a range of one value).

ow, using `runAsUser: 99`, shown in the pod fragment above, the pod to matches the new nfs-scc and is
able to run with a UID of 99.

===== Group IDs:
Another way to handle NFS access (assuming it's not a choice to change permissions on the NFS mount)
is to use supplemental groups. Supplemental groups in Openshift are used for shared storage, of which
NFS is an exmaple. In contrast, block storage, such as Ceph RBD or iSCSI, use the `fsGroup:` SCC strategy
and  the `fsGroup:` value in the pod's `securityContext:`. Since the group id on the target NFS directory,
shown above, is 5555, the pod can define that group id using `suplementalGroups:` under pod's global
`securityContext:` definition. For example:
```
spec:
  containers:
    - name: ...
      #runAsUser: 99 from above has been commented out here
  securityContext: <1>
    supplementalGroups: [5555] #an array of GIDs defined globally for the pod
```
<1> securityContext here is defined globally to the pod, not under a specific container

Since group id is the focus here, it's worth seeing the ranges defined for the _default_
project:
```
# oc export ns default 
...
metadata:
  annotations:
    ...
    openshift.io/sa.scc.supplemental-groups: 1000000000/10000 <1>
    openshift.io/sa.scc.uid-range: 1000000000/10000
...
```
<1> this is the preallocated range for the group ids. Additionally, the min value of the 
range (1000000000) will be the GID default when a group id is not specified in the pod or image.
The suggestion below does not modify the project's allowed group ids, but that could be an
option for some project admins.

Supplemental groups and ranges work a bit differently from a user id and its single range
(assume the _default_ namespace and the "nfs-scc" SCC are still being used):

- there can be more than one range of allowed group ids defined in the SCC and/or namespace.
- the "nfs-sec" SCC (which has its `supplementalGroups:` strategy set as _MustRunAs_) will
not satisfy the pod's requirements. This is due to the pod defining a group id but "nfs-sec"
does not contain any group id ranges. Since "nfs-scc" is not the last SCC to be examined (its
priority is 9 compared to nil for the other SCCs), the remaining SCCs are examined.
- the _restricted_ SCC, which typically is the last SCC used to attempt to statisy a pod's 
requirements, has its `supplementalGroups:` strategy set to _RunAsAny_, and therefore statisfies
the pod, and, thus, the pod will start. Contrasted to user ids, the _restricted_ SCC's `runAsUser:`
strategy is set to _MustRunAsRange_, which means that _restricted_ will not satisfy the pod's 
requirements (and neither will the other predefined SCCs available to the pod) and, thus, the
pod will fail to start. If the _restricted_ SCC were edited (not recommended) to change
`supplementalGroups:` from _RunAsAny_ to _MustRunAs_, then the pod would not match the constraints
of _restricted_ either and, thus, would fail. Ths scenario is analogous to the behavior seen
when a user id of 99 was defined in the pod and the _restricted_ SCC was evaluated.

====== Custom SCC for GroupID:
If GID range checking is desired, and none of the predefined SCCs are to be edited, then the
new "nfs-scc" can be modified to support this reqirement, as seen below:
```
#after oc edit scc nfs-scc
...
# oc export scc nfs-scc 
...
groups:
- system:authenticated
kind: SecurityContextConstraints
metadata:
...
  name: nfs-scc
priority: 9
runAsUser:
  type: MustRunAsRange <1>
seLinuxContext:
  type: MustRunAs
supplementalGroups:
  ranges: <3>
  - max: 6000
    min: 5000
  type: MustRunAs <2>
```
<1> no change for user but the user id is no longer defined in the pod spec, so the default value is used
<2> _MustRunAs_ triggers gid range checking
<3> the min and max values are defined in the SCC, therefore the SCC statisfies the need for range checking
and thus the namespace'a `openshift.io/sa.scc.supplemental-groups` range is not needed.

Only the "nfs-scc" has been changed. The pod spec (fragment shown just above) does not need any changes, 
and the _default_ namespace also remains the same (its original settings). After creating the pod:
```
# oc create -f nfs-pod.yaml
...
# oc get pod nfs-pod1 -0 yaml
...
metadata:
  annotations:
    openshift.io/scc: nfs-scc <1>
  name: nfs-pod1
  namespace: default <2>
...
spec:
  containers:
    ...
    securityContext:
      runAsUser: 1000000000 <3>
 ...
 securityContext:
    seLinuxOptions:
     level: s0:c1,c0
    supplementalGroups:
    - 5555 <3>
...
# oc rsh nfs-pod1 id
uid=1000000000 gid=0(root) groups=5555 <3>

```
<1> the "nfs-scc" matched the pod, which was the goal
<2> the namespace (project) is still _default_
<3> verification that the running container has the default user id (1000000000)
and supplemental groups of 5555

==== SELinux
All predefined SCCs, except for the _privileged_ SCC, set the `seLinuxContext:` to _MustRunAs_.
This forces the pod to use a selinux policy, which can be defined in the pod spec. See the
fragment below:
```
spec:
  containers:
  - name: ...
  securityContext:
    type: MustRunAs
    SELinuxOptions: <1>
      user: selinux-user-name
      role: selinux-role-name
      type: selinux-type-label
      level: selinux-level
```
<1> selinux policy is defined in the context of a container, but it can also be set global to the pod.

If selinux is not defined in the pod spec then it defaults to the selinux policy defined in the pod's
matching SCC, or, if absent in the SCC, then the namespace's `sa.scc.mcs:` value is used. 

...
An SELinuxContext strategy of MustRunAs with no level set. Admission looks for the openshift.io/sa.scc.mcs annotation to populate the level.
...
ausearch -m avc --start recent
...
# setsebool -P virt_sandbox_use_nfs on
The virt_sandbox_use_nfs boolean is defined by the docker-selinux package. If you get an error saying it is not defined, please ensure that this package is installed.
...


*NOTE:*
The `accessModes:` section of the PV and the PVC provide no access
enforcement. They are used similarly to labels and match a PVC to a PV, nothing more. For example,
the NFS PV's `accessModes:` can be set to _ReadOnlyMany_ yet the container, depending on its user
and group ids could have write access to that PV.

*NOTE:*
Each NFS volume must be mountable by all nodes in the cluster.

[[nfs-reclaiming-resources]]

== Reclaiming Resources
NFS implements the Kubernetes *Recyclable* plug-in interface. Automatic
processes handle reclamation tasks based on policies set on each persistent
volume.

By default, persistent volumes are set to *Retain*. NFS volumes which are set to
*Recycle* are scrubbed (i.e., `rm -rf` is run on the volume) after being
released from their claim (i.e, after the user's `*PersistentVolumeClaim*` bound
to the volume is deleted). Once recycled, the NFS volume can be bound to a new
claim.

[[nfs-automation]]

== Automation
As discussed, clusters can be provisioned with persistent storage using NFS in
the following way:

- Disk partitions can be used to link:#nfs-enforcing-disk-quotas[enforce storage
quotas].
- Security can be enforced by link:#nfs-volume-security[restricting volumes] to
the namespace that has a claim to them.
- link:#nfs-reclaiming-resources[Reclamation of discarded resources] can be
configured for each persistent volume.

They are many ways that you can use scripts to automate the above tasks. You can
use an
link:https://github.com/openshift/openshift-ansible/tree/master/roles/kube_nfs_volumes[example
Ansible playbook] to help you get started.

[[selinux-and-nfs-export-settings]]

== SELinux and NFS Export Settings
By default, SELinux does not allow writing from a pod to a remote NFS server.
The NFS volume mounts correctly, but is read-only.

To enable writing to NFS volumes with SELinux enforcing on each node, run:

----
# setsebool -P virt_use_nfs 1
----

The `-P` option makes the bool persistent between reboots.

Additionally, in order to enable arbitrary container users to read and write the
volume, each exported volume on the NFS server itself should conform to the
following:

- Each export must be:
+
----
/<example_fs> *(rw,root_squash)
----
- The firewall must be configured to allow traffic to the mount point. The default port is 2049:
+
----
# iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT
----
- The NFS export and directory must be set up so that it is accessible by your
pods. Either set the export to be owned by the container's primary UID, or give
your pod group based access using `*SuppplementalGroups*`. See
link:pod_security_context.html[Volume Security] for more information.
