[[install-config-persistent-storage-persistent-storage-nfs]]
= Persistent Storage Using NFS
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

{product-title} clusters can be provisioned with
xref:../../architecture/additional_concepts/storage.adoc#architecture-additional-concepts-storage[persistent storage]
using NFS. Persistent volumes (PVs) and persistent volume claims (PVCs) provide
a convenient method for sharing a volume across a project. While the
NFS-specific information contained in a PV definition could also be defined
directly in a pod definition, doing so does not create the volume as a distinct
cluster resource, making the volume more susceptible to conflicts.

This topic covers the specifics of using the NFS persistent storage type. Some
familiarity with {product-title} and
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch-nfs.html[NFS]
is beneficial. See the
xref:../../architecture/additional_concepts/storage.adoc#architecture-additional-concepts-storage[Persistent Storage]
concept topic for details on the {product-title} persistent volume (PV) framework in
general.

[[nfs-provisioning]]
== Provisioning

Storage must exist in the underlying infrastructure before it can be mounted as
a volume in {product-title}. To provision NFS volumes, a list of NFS servers and
export paths are all that is required.

You must first create an object definition for the PV:

.PV Object Definition Using NFS
====
[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0001 <1>
spec:
  capacity:
    storage: 5Gi <2>
  accessModes:
  - ReadWriteOnce <3>
  nfs: <4>
    path: /tmp <5>
    server: 172.17.0.2 <6>
  persistentVolumeReclaimPolicy: Recycle <7>
----
<1> The name of the volume. This is the PV identity in various `oc <command>
pod` commands.
<2> The amount of storage allocated to this volume.
<3> Though this appears to be related to controlling access to the volume, it is
actually used similarly to labels and used to match a PVC to a PV. Currently, no
access rules are enforced based on the `*accessModes*`.
<4> The volume type being used, in this case the *nfs* plug-in.
<5> The path that is exported by the NFS server.
<6> The host name or IP address of the NFS server.
<7> The reclaim policy for the PV. This defines what happens to a volume when released
from its claim. Valid options are *Retain* (default) and *Recycle*. See
xref:nfs-reclaiming-resources[Reclaiming Resources].
====

[NOTE]
====
Each NFS volume must be mountable by all schedulable nodes in the cluster.
====

Save the definition to a file, for example *_nfs-pv.yaml_*, and create the PV:

====
----
$ oc create -f nfs-pv.yaml
persistentvolume "pv0001" created
----
====

Verify that the PV was created:

====
----
# oc get pv
NAME                     LABELS    CAPACITY     ACCESSMODES   STATUS      CLAIM     REASON    AGE
pv0001                   <none>    5368709120   RWO           Available                       31s
----
====

The next step can be to create a PVC, which binds to the new PV:

.PVC Object Definition
====
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-claim1
spec:
  accessModes:
    - ReadWriteOnce <1>
  resources:
    requests:
      storage: 1Gi <2>
----
<1> As mentioned above for PVs, the `*accessModes*` do not enforce security, but
rather act as labels to match a PV to a PVC.
<2> This claim looks for PVs offering *1Gi* or greater capacity.
====

Save the definition to a file, for example *_nfs-claim.yaml_*, and create the
PVC:

====
----
# oc create -f nfs-claim.yaml
----
====

[[nfs-enforcing-disk-quotas]]
== Enforcing Disk Quotas

You can use disk partitions to enforce disk quotas and size constraints. Each
partition can be its own export. Each export is one PV. {product-title} enforces
unique names for PVs, but the uniqueness of the NFS volume's server and path is
up to the administrator.

Enforcing quotas in this way allows the developer to request persistent storage
by a specific amount (for example, 10Gi) and be matched with a corresponding
volume of equal or greater capacity.

[[nfs-volume-security]]
== NFS Volume Security

This section covers NFS volume security, including matching permissions and
SELinux considerations. The user is expected to understand the basics of POSIX
permissions, process UIDs, supplemental groups, and SELinux.

[NOTE]
====
See the full
xref:../../install_config/persistent_storage/pod_security_context.adoc#install-config-persistent-storage-pod-security-context[Volume
Security] topic before implementing NFS volumes.
====

Developers request NFS storage by referencing, in the `*volumes*` section of
their pod definition, either a PVC by name or the NFS volume plug-in directly.

The *_/etc/exports_* file on the NFS server contains the accessible NFS
directories. The target NFS directory has POSIX owner and group IDs. The
{product-title} NFS plug-in mounts the container's NFS directory with the same
POSIX ownership and permissions found on the exported NFS directory. However,
the container is not run with its effective UID equal to the owner of the NFS
mount, which is the desired behavior.

As an example, if the target NFS directory appears on the NFS server as:

[[nfs-export]]
====
----
# ls -lZ /opt/nfs -d
drwxrws---. nfsnobody 5555 unconfined_u:object_r:usr_t:s0   /opt/nfs

# id nfsnobody
uid=65534(nfsnobody) gid=65534(nfsnobody) groups=65534(nfsnobody)
----
====

Then the container must match SELinux labels, and either run with a UID of
*65534* (*nfsnobody* owner) or with *5555* in its supplemental groups in order
to access the directory.

[NOTE]
====
The owner ID of 65534 is used as an example. Even though NFS's *root_squash*
maps *root* (0) to *nfsnobody* (65534), NFS exports can have arbitrary owner
IDs. Owner 65534 is not required for NFS exports.
====

[[nfs-supplemental-groups]]
=== Group IDs

The recommended way to handle NFS access (assuming it is not an option to change
permissions on the NFS export) is to use supplemental groups. Supplemental
groups in {product-title} are used for shared storage, of which NFS is an
example. In contrast, block storage, such as Ceph RBD or iSCSI, use the
*fsGroup* SCC strategy and  the *fsGroup* value in the pod's
`*securityContext*`.

[NOTE]
====
It is generally preferable to use supplemental group IDs to gain access to
persistent storage versus using xref:nfs-user-ids[user IDs]. Supplemental
groups are covered further in the full
xref:../../install_config/persistent_storage/pod_security_context.adoc#supplemental-groups[Volume Security] topic.
====

Because the group ID on the xref:nfs-export[example target NFS directory] shown
above is 5555, the pod can define that group ID using `*supplementalGroups*`
under the pod-level `*securityContext*` definition. For example:

====
----
spec:
  containers:
    - name:
    ...
  securityContext: <1>
    supplementalGroups: [5555] <2>
----
<1> `*securityContext*` must be defined at the pod level, not under a specific container.
<2> An array of GIDs defined for the pod. In this case, there is one element in the array;
additional GIDs would be comma-separated.
====

Assuming there are no custom SCCs that might satisfy the pod's requirements, the
pod likely matches the *restricted* SCC. This SCC has the
`*supplementalGroups*` strategy set to *RunAsAny*, meaning that any supplied
group ID is accepted without range checking.

As a result, the above pod passes admissions and is launched. However,
if group ID range checking is desired, a custom SCC, as described in
xref:../../install_config/persistent_storage/pod_security_context.adoc#scc-supplemental-groups[pod security and custom
SCCs], is the preferred solution. A custom SCC can be created such that minimum
and maximum group IDs are defined, group ID range checking is enforced, and a
group ID of 5555 is allowed.

[NOTE]
====
To use a custom SCC, you must first add it to the appropriate service
account. For example, use the `default` service account in the given project
unless another has been specified on the pod specification. See
xref:../../admin_guide/manage_scc.adoc#add-scc-to-user-group-project[Add an SCC to a User, Group, or Project] for details.
====

[[nfs-user-ids]]
=== User IDs

User IDs can be defined in the container image or in the pod definition. The
full xref:../../install_config/persistent_storage/pod_security_context.adoc#user-id[Volume Security] topic covers
controlling storage access based on user IDs, and should be read prior to
setting up NFS persistent storage.

[NOTE]
====
It is generally preferable to use xref:nfs-supplemental-groups[supplemental
group IDs] to gain access to persistent storage versus using user IDs.
====

In the xref:nfs-export[example target NFS directory] shown above, the container
needs its UID set to 65534 (ignoring group IDs for the moment), so the following
can be added to the pod definition:

====
[source,yaml]
----
spec:
  containers: <1>
  - name:
  ...
    securityContext:
      runAsUser: 65534 <2>
----
<1> Pods contain a `*securityContext*` specific to each container (shown here) and
a pod-level `*securityContext*` which applies to all containers defined in the pod.
<2> 65534 is the *nfsnobody* user.
====

Assuming the *default* project and the *restricted* SCC, the pod's requested
user ID of 65534 is not allowed, and therefore the pod fails. The pod fails for
the following reasons:

- It requests 65534 as its user ID.
- All SCCs available to the pod are examined to see which SCC allows a user ID
of 65534 (actually, all policies of the SCCs are checked but the focus here is
on user ID).
- Because all available SCCs use *MustRunAsRange* for their `*runAsUser*`
strategy, UID range checking is required.
- 65534 is not included in the SCC or project's user ID range.

It is generally considered a good practice not to modify the predefined SCCs.
The preferred way to fix this situation is to create a custom SCC, as described
in the full xref:../../install_config/persistent_storage/pod_security_context.adoc#scc-runasuser[Volume Security] topic.
A custom SCC can be created such that minimum and maximum user IDs are defined,
UID range checking is still enforced, and the UID of 65534 is allowed.

[NOTE]
====
To use a custom SCC, you must first add it to the appropriate service
account. For example, use the `default` service account in the given project
unless another has been specified on the pod specification. See
xref:../../admin_guide/manage_scc.adoc#add-scc-to-user-group-project[Add an SCC to a User, Group, or Project] for details.
====

[[nfs-selinux]]
=== SELinux

[NOTE]
====
See the full xref:../../install_config/persistent_storage/pod_security_context.adoc#selinuxoptions[Volume Security]
topic for information on controlling storage access in conjunction with using
SELinux.
====

By default, SELinux does not allow writing from a pod to a remote NFS server.
The NFS volume mounts correctly, but is read-only.

To enable writing to NFS volumes with SELinux enforcing on each node, run:

----
# setsebool -P virt_use_nfs 1
----

The `-P` option above makes the bool persistent between reboots.

The *virt_use_nfs* boolean is defined by the *_docker-selinux_* package. If an
error is seen indicating that this bool is not defined, ensure this package has
been installed.

[[nfs-export-settings]]
=== Export Settings

In order to enable arbitrary container users to read and write the volume, each
exported volume on the NFS server should conform to the following conditions:

* Each export must be:
+
----
/<example_fs> *(rw,root_squash)
----
+

* The firewall must be configured to allow traffic to the mount point.
** For NFSv4, configure the default port `2049` (*nfs*) and port `111` (*portmapper*).
+
.NFSv4
----
# iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT
# iptables -I INPUT 1 -p tcp --dport 111 -j ACCEPT
----

** For NFSv3, there are three ports to configure:
`2049` (*nfs*), `20048` (*mountd*), and `111` (*portmapper*).
+
.NFSv3
----
# iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT
# iptables -I INPUT 1 -p tcp --dport 20048 -j ACCEPT
# iptables -I INPUT 1 -p tcp --dport 111 -j ACCEPT
----

* The NFS export and directory must be set up so that it is accessible by the
target pods. Either set the export to be owned by the container's primary UID,
or supply the pod group access using `*supplementalGroups*`, as shown in
xref:nfs-supplemental-groups[Group IDs] above. See the full
xref:../../install_config/persistent_storage/pod_security_context.adoc#install-config-persistent-storage-pod-security-context[Volume Security] topic for additional pod
security information as well.

[[nfs-reclaiming-resources]]
== Reclaiming Resources
NFS implements the {product-title} *Recyclable* plug-in interface. Automatic
processes handle reclamation tasks based on policies set on each persistent
volume.

By default, PVs are set to *Retain*. NFS volumes which are set to
*Recycle* are scrubbed (i.e., `rm -rf` is run on the volume) after being
released from their claim (i.e, after the user's `*PersistentVolumeClaim*` bound
to the volume is deleted). Once recycled, the NFS volume can be bound to a new
claim.

Once claim to a PV is released (that is, the PVC is deleted), the PV object
should not be re-used. Instead, a new PV should be created with the same basic
volume details as the original.

For example, the administrator creates a PV named `nfs1`:

----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs1
spec:
  capacity:
    storage: 1Mi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 192.168.1.1
    path: "/"
----

The user creates `PVC1`, which binds to `nfs1`. The user then deletes `PVC1`,
releasing claim to `nfs1`, which causes `nfs1` to be `Released`.  If the
administrator wishes to make the same NFS share available, they should create a new PV
with the same NFS server details, but a different PV name:

----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs2
spec:
  capacity:
    storage: 1Mi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 192.168.1.1
    path: "/"
----

Deleting the original PV and re-creating it with the same name is discouraged.
Attempting to manually change the status of a PV from `Released` to `Available`
causes errors and potential data loss.

[NOTE]
====
A PV with retention policy of `Recycle` scrubs (`rm -rf`) the data and marks it
as `Available` for claim. The `Recycle` retention policy is deprecated starting
in {product-title} 3.6 and should be avoided. Anyone using recycler should use
dynamic provision and volume deletion instead.
====

[[nfs-automation]]
== Automation
Clusters can be provisioned with persistent storage using NFS in the following
ways:

- xref:nfs-enforcing-disk-quotas[Enforce storage quotas] using disk partitions.
- Enforce security by xref:nfs-volume-security[restricting volumes] to the
project that has a claim to them.
- Configure xref:nfs-reclaiming-resources[reclamation of discarded resources] for
each PV.

They are many ways that you can use scripts to automate the above tasks. You can
use an
link:https://github.com/openshift/openshift-ansible/tree/master/roles/openshift_node_certificates[example
Ansible playbook] to help you get started.

[[nfs-additional-config-and-troubleshooting]]
== Additional Configuration and Troubleshooting

Depending on what version of NFS is being used and how it is configured, there
may be additional configuration steps needed for proper export and security
mapping. The following are some that may apply:

[cols="1,2"]
|===

|NFSv4 mount incorrectly shows all files with ownership of *nobody:nobody*
a|- Could be attributed to the ID mapping settings (/etc/idmapd.conf) on your NFS
- See https://access.redhat.com/solutions/33455[this Red Hat Solution].

|Disabling ID mapping on NFSv4
a|- On both the NFS client and server, run:
+
----
# echo 'Y' > /sys/module/nfsd/parameters/nfs4_disable_idmapping
----
|===
