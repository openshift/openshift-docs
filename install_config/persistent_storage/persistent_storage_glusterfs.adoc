= Persistent Storage Using GlusterFS
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

OpenShift clusters can be provisioned with
link:../../architecture/additional_concepts/storage.html[persistent storage]
using GlusterFS. Persistent volumes (PVs) and persistent volume claims (PVCs) provide
a convenient method for sharing a volume across a project. While the
GlusterFS-specific information contained in a PV definition could also be defined
directly in a pod definition, doing so does not create the volume as a distinct
cluster resource, making the volume more susceptible to conflicts.

This topic covers the specifics of using the GlusterFS persistent storage type. Some
familiarity with OpenShift and
https://access.redhat.com/documentation/en-US/Red_Hat_Storage/3/html/Administration_Guide/index.html[GlusterFS]
is beneficial. See the
link:../../architecture/additional_concepts/storage.html[Persistent Storage]
concept topic for details on the OpenShift persistent volume (PV) framework in general.

[[gfs-provisioning]]
== Provisioning

Storage must exist in the underlying infrastructure before it can be mounted as
a volume in OpenShift. To provision GlusterFS volumes the following information
 is required:

- A distinct list of servers (IP addresses) in the Gluster cluster, to be defined
as endpoints.
- A service, to persist the endpoints (optional).
- Existing Gluster volume name to be referenced in the persistent volume object.

You must also ensure that *glusterfs-fuse* is installed on *_each_* schedulable OpenShift
node in the cluster:

----
# yum install glusterfs-fuse
----

[NOTE]
====
OpenShift nodes can also host a gluster node (referrfed to as _hyperconverged_ storage);
however, performance may be less predictible and harder to manage in this setup.
====

[[creating-gluster-endpoints]]
=== Creating Gluster Endpoints

In an endpoints definition, the storage administrator must define the GlusterFS cluster
as `*EndPoints*` and include the IP addresses of your Gluster servers. The
port value can be any numeric value within the accepted range of ports
(and is ignored). It is recommended, but not required, that you also create a
link:../../architecture/core_concepts/pods_and_services.html#services[service]
that persists the endpoints.

Before defining the endpoints, first define the following service:

.Gluster Service Definition
====
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: glusterfs-cluster <1>
spec:
  ports:
  - port: 1
----
<1> this name must be defined in the endpoints definition to match the endpoints to this service
====

Save the service definition to a file, for example *_gluster-service.yaml_*,
then create the service:

====
----
$ oc create -f gluster-service.yaml
service "glusterfs-cluster" created
----
====

Verify that the service was created:

====
----
# oc get services
NAME                       CLUSTER_IP       EXTERNAL_IP   PORT(S)    SELECTOR        AGE
glusterfs-cluster          172.30.205.34    <none>        1/TCP      <none>          44s
----
====

Next, define the Gluster endpoints. For example:

.Gluster Endpoints Definition
====
[source,yaml]
----
apiVersion: v1
kind: Endpoints
metadata:
  name: glusterfs-cluster <1>
subsets:
  - addresses:
      - ip: 192.168.122.221 <2>
    ports:
      - port: 1
  - addresses:
      - ip: 192.168.122.222 <2>
    ports:
      - port: 1 <3>
----
<1> This name must match the service name.
<2> The `*ip*` values must be the actual IP addresses of a Gluster server, not
fully-qualified host names. This requirement could change in future releases.
<3> The port number is ignored.
====

Save the endpoints definition to a file, for example
*_gluster-endpoints.yaml_*, then create the endpoints:

====
----
$ oc create -f gluster-endpoints.yaml
endpoints "glusterfs-cluster" created
----
====

Verify that the endpoints were created:

====
----
$ oc get endpoints
NAME                ENDPOINTS                             AGE
docker-registry     10.1.0.3:5000                         4h
glusterfs-cluster   192.168.122.221:1,192.168.122.222:1   11s
kubernetes          172.16.35.3:8443                      4d
----
====

[[gfs-creating-persistent-volume]]
=== Creating the Persistent Volume

The storage admin next defines the persistent volume (PV) in an object definition before creating
it in OpenShift:

.Persistent Volume Object Definition Using GlusterFS
====

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: gluster-default-volume <1>
spec:
  capacity:
    storage: 2Gi <2>
  accessModes: <3>
    - ReadWriteMany
  glusterfs: <4>
    endpoints: glusterfs-cluster <5>
    path: myVol1 <6>
    readOnly: false
  persistentVolumeReclaimPolicy: Recycle
----
<1> The name of the volume. This is how it is identified via
link:../../architecture/additional_concepts/storage.html[persistent volume
claims] or from pods.
<2> The amount of storage allocated to this volume.
<3> `accessModes` are used as labels to match a PV and a PVC. They currently
do not define any form of access control.
<4> The volume type being used, in this case the *glusterfs*
plug-in.
<5> The endpoints name that defines the Gluster cluster.
created in link:#creating-gluster-endpoints[Creating Gluster Endpoints].
<6> The Gluster volume that will be accessed, as shown in the `gluster volume status`
command.
====

Save your definition to a file, for example *_gluster-pv.yaml_*, and create the
persistent volume:

====
----
# oc create -f gluster-pv.yaml
persistentvolume "gluster-default-volume" created
----
====

Verify that the persistent volume was created:

====
----
# oc get pv
NAME                     LABELS    CAPACITY     ACCESSMODES   STATUS      CLAIM     REASON    AGE
gluster-default-volume   <none>    2147483648   RWX           Available                       2s
----
====

[[gfs-creating-pvc]]
=== Creating the Persistent Volume Claim
Developers request GlusterFS storage by referencing, in the `*volumes*` section
of their pod spec, either a
link:../../dev_guide/persistent_volumes.html[`*PersistentVolumeClaim*`] by name,
or the gluster volume plug-in directly. A PVC exists only in the user's project
and can only be referenced only by pods within that same project. Any attempt to
access a PV across a project causes the pod to fail.

The next step is to create a persistent volume claim (PVC) which will bind
to the new PV:

.PVC Object Definition
====
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gluster-claim
spec:
  accessModes:
  - ReadWriteMany <1>
  resources:
     requests:
       storage: 1Gi <2>
----
<1> As mentioned above for PVs, the `*accessModes*` do not enforce security, but
rather act as labels to match a PV to a PVC.
<2> This claim will look for PVs offering *1Gi* or greater capacity.
====

Save the definition to a file, for example *_gluster-claim.yaml_*, and create the
PVC:

====
----
# oc create -f gluster-claim.yaml
----
====

[NOTE]
====
PVs and PVCs are not necessary, just convenient, and make sharing a volume
across a project simpler. The gluster-specific information contained in the PV
definition can also be defined directly in a pod spec.
====

[[gluster-volume-security]]
== Volume Security

This section covers Gluster volume security, including matching permissions and
SELinux considerations. The reader is expected to understand the basics of POSIX
permissions, process UIDs, supplemental groups, and SELinux.

[NOTE]
====
See
link:../../install_config/persistent_storage/pod_security_context.html[Volume
Security] before implementing gluster volumes.
====

Assume that the target gluster volume, "_HadoopVol_", is mounted under
_/mnt/glusterfs/_, with the following POSIX permissions and selinux labels:

[[gfs-mount]]
====
----
# ls -lZ /mnt/glusterfs/
drwxrwx---. yarn hadoop system_u:object_r:fusefs_t:s0    HadoopVol

# id yarn
uid=592(yarn) gid=590(hadoop) groups=590(hadoop)
----
====

In order to access the "_HadoopVol_" volume, containers must match the SELinux
label, and either run with a UID of 592, or with 590 in their supplemental groups.
The OpenShift GlusterFS plug-in mounts the volume in the container with the same
POSIX ownership and permissions found on the target gluster mount, namely the owner
will be *592* and group id will be *590*. However, the container is not run with its
effective UID equal to 592, nor with its GID equal to 590, which is the desired behavior.
Instead, container's UID and supplemental groups are determined by Security Context
Constraints (SCCs) and the user's project's defaults.

[[gfs-supplemental-groups]]
=== Group IDs

The recommended way to handle gluster volume access (assuming it is not an option to
change permissions on the gluster mount) is to use supplemental groups. Supplemental
groups in OpenShift are used for shared storage, of which GlusterFS is an example. In
contrast, block storage, such as Ceph RBD or iSCSI, use the *fsGroup* SCC strategy and
the *fsGroup* value in the pod's `*securityContext*`.

[NOTE]
====
It is generally preferable to use supplemental group IDs to gain access to
persistent storage versus using link:#gfs-user-ids[user IDs]. Supplemental
groups are covered further in
link:pod_security_context.html#supplemental-groups[Volume Security].
====

Because the group ID on the link:#gfs-mount[example target gluster mount] shown
above is 590, the pod can define that group ID using `*supplementalGroups*`
under the pod-level `*securityContext*` definition. For example:

====
----
spec:
  containers:
    - name:
    ...
  securityContext: <1>
    supplementalGroups: [590] <2>
----
<1> `*securityContext*` must be defined at the pod level, not under a specific container.
<2> An array of GIDs defined atr the pod level.
====

Assuming there are no custom SCCs that might satisfy the pod's requirements, the
pod will likely match the *restricted* SCC. This SCC has the
`*supplementalGroups*` strategy set to *RunAsAny*, meaning that any supplied
group IDs will be accepted without range checking.

As a result, the above pod will pass admissions and will be launched. However,
if group ID range checking is desired, a custom SCC, as described in
link:pod_security_context#scc-supplemental-groups[pod security and custom SCCs],
is the preferred solution. A custom SCC can be created such that minimum and
maximum group IDs are defined, group ID range checking is enforced, and a group
ID of 590 is allowed.

[[gfs-user-ids]]
=== User IDs

User IDs can be defined in the container image or in the pod definition.
link:pod_security_context.html#user-id[Volume Security] covers controlling
storage access based on user IDs, and should be read prior to setting up NFS
persistent storage.

[NOTE]
====
It is generally preferable to use link:#gfs-supplemental-groups[supplemental
group IDs] to gain access to persistent storage versus using user IDs.
====

In the link:#gfs-mount[example target gluster mount] shown above, the container
needs its UID set to 592 (ignoring group IDs for the moment), so the following
can be added to the pod definition:

====
[source,yaml]
----
spec:
  containers: <1>
  - name:
  ...
    securityContext:
      runAsUser: 592 <2>
----
<1> Pods contain a `*securtityContext*` specific to each container (shown here)
and a pod-level `*securityContext*` which applies to all containers defined in the pod.
<2> the uid defined on the gluser mount.
====

Assuming the *default* project and the *restricted* SCC, the pod's requested
user ID of 592 will, unfortunately, not be allowed, and therefore the pod will
fail. The pod fails because of the following:

- It requests 592 as its user ID.
- All SCCs available to the pod are examined to see which SCC will allow a user ID
of 592 (actually, all policies of the SCCs are checked but the focus here is on
user ID).
- Because all available SCCs use *MustRunAsRange* for their `*runAsUser*`
strategy, UID range checking is required.
- 592 is not included in the SCC or project's user ID range.

It is generally considered a good practice not to modify the predefined SCCs.
The preferred way to fix this situation is to create a custom SCC, as described
in link:pod_security_context.html#scc-runasuser[Volume Security]. A custom SCC
can be created such that minimum and maximum user IDs are defined, UID range
checking is still enforced, and the UID of 592 will be allowed.

[[selinux]]
=== SELinux

[NOTE]
====
See link:pod_security_context.html#selinux[Volume Security] for information on
controlling storage access in conjunction with using SELinux.
====

By default, SELinux does not allow writing from a pod to a remote Gluster
server.

To enable writing to GlusterFS volumes with SELinux enforcing on each node, run:

----
$ sudo setsebool -P virt_sandbox_use_fusefs on
----

[NOTE]
====
The `virt_sandbox_use_fusefs` boolean is defined by the *docker-selinux*
package. If you get an error saying it is not defined, please ensure that this
package is installed.
====

The `-P` option makes the bool persistent between reboots.

