[[install-config-cluster-metrics]]
= Enabling Cluster Metrics
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

The
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#kubelet[kubelet]
exposes metrics that can be collected and stored in back-ends by
link:https://github.com/GoogleCloudPlatform/heapster[Heapster].

As an {product-title} administrator, you can view a cluster's metrics from all
containers and components in one user interface. These metrics are also used by
xref:../dev_guide/pod_autoscaling.adoc#dev-guide-pod-autoscaling[horizontal pod
autoscalers] in order to determine when and how to scale.

This topic describes using
link:https://github.com/hawkular/hawkular-metrics[Hawkular Metrics] as a metrics
engine which stores the data persistently in a
link:http://cassandra.apache.org/[Cassandra] database. When this is configured,
CPU, memory and network-based metrics are viewable from the {product-title} web
console and are available for use by
xref:../dev_guide/pod_autoscaling.adoc#dev-guide-pod-autoscaling[horizontal pod
autoscalers].

Heapster retrieves a list of all nodes from the master server, then contacts
each node individually through the `/stats` endpoint. From there, Heapster
scrapes the metrics for CPU, memory and network usage, then exports them into
Hawkular Metrics.

The storage volume metrics available on the kubelet are not available through
the `/stats` endpoint, but are available through the `/metrics` endpoint. See
{product-title} via Prometheus for detailed information.

Browsing individual pods in the web console displays separate sparkline charts
for memory and CPU. The time range displayed is selectable, and these charts
automatically update every 30 seconds. If there are multiple containers on the
pod, then you can select a specific container to display its metrics.

If xref:../admin_guide/limits.adoc#admin-guide-limits[resource limits] are
defined for your project, then you can also see a donut chart for each pod. The
donut chart displays usage against the resource limit. For example: `145
Available of 200 MiB`, with the donut chart showing `55 MiB Used`.

ifdef::openshift-origin[]
For more information about the metrics integration, refer to the
link:https://github.com/openshift/origin-metrics[Origin Metrics] GitHub project.
endif::[]

[[cluster-metrics-before-you-begin]]
== Before You Begin

ifdef::openshift-origin[]
[WARNING]
====
If your {product-title} installation was originally performed on a version
previous to v1.0.8, even if it has since been updated to a newer version, follow
the instructions for node certificates outlined in
xref:../upgrading/manual_upgrades.adoc#manual-updating-master-and-node-certificates[Updating
Master and Node Certificates]. If the node certificate does not contain the IP
address of the node, then Heapster fails to retrieve any metrics.
====
endif::[]

An Ansible playbook is available to deploy and upgrade cluster metrics. You
should familiarize yourself with the
xref:../install/index.adoc#install-planning[Installing Clusters] guide. This
provides information for preparing to use Ansible and includes information about
configuration. Parameters are added to the Ansible inventory file to configure
various areas of cluster metrics.

The following describe the various areas and the parameters that can be added to
the Ansible inventory file in order to modify the defaults:

- xref:../install_config/cluster_metrics.adoc#metrics-namespace[Metrics Project]
- xref:../install_config/cluster_metrics.adoc#metrics-data-storage[Metrics Data Storage]

[[metrics-namespace]]
== Metrics Project

The components for cluster metrics must be deployed to the *openshift-infra*
project in order for autoscaling to work.
xref:../dev_guide/pod_autoscaling.adoc#dev-guide-pod-autoscaling[Horizontal pod
autoscalers] specifically use this project to discover the Heapster service and
use it to retrieve metrics. The metrics project can be changed by adding
`openshift_metrics_project` to the inventory file.

[[metrics-data-storage]]
== Metrics Data Storage

You can store the metrics data to either
xref:../architecture/additional_concepts/storage.adoc#architecture-additional-concepts-storage[persistent
storage] or to a temporary xref:../dev_guide/volumes.adoc#dev-guide-volumes[pod
volume].

[[metrics-persistent-storage]]
=== Persistent Storage

Running {product-title} cluster metrics with persistent storage means that your
metrics are stored to a
xref:../architecture/additional_concepts/storage.adoc#persistent-volumes[persistent
volume] and are able to survive a pod being restarted or recreated. This is ideal
if you require your metrics data to be guarded from data loss. For production
environments it is highly recommended to configure persistent storage for your
metrics pods.

The size requirement of the Cassandra storage is dependent on the number of
pods. It is the administrator's responsibility to ensure that the size
requirements are sufficient for their setup and to monitor usage to ensure that
the disk does not become full. The size of the persisted volume claim is
specified with the `openshift_metrics_cassandra_pvc_size`
xref:../install_config/cluster_metrics.adoc#metrics-ansible-variables[ansible
variable] which is set to 10 GB by default.

If you would like to use xref:../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[dynamically provisioned persistent volumes] set the `openshift_metrics_cassandra_storage_type`
xref:../install_config/cluster_metrics.adoc#metrics-ansible-variables[variable]
to `dynamic` in the inventory file.

[[capacity-planning-for-openshift-metrics]]
=== Capacity Planning for Cluster Metrics

After running the `openshift_metrics` Ansible role, the output of `oc get pods`
should resemble the following:

----
 # oc get pods -n openshift-infra
 NAME                                READY             STATUS      RESTARTS          AGE
 hawkular-cassandra-1-l5y4g          1/1               Running     0                 17h
 hawkular-metrics-1t9so              1/1               Running     0                 17h
 heapster-febru                      1/1               Running     0                 17h
----

{product-title} metrics are stored using the Cassandra database, which is
deployed with settings of `openshift_metrics_cassandra_limits_memory: 2G`; this
value could be adjusted further based upon the available memory as determined by
the Cassandra start script. This value should cover most {product-title} metrics
installations, but using environment variables you can modify the `MAX_HEAP_SIZE`
along with heap new generation size, `HEAP_NEWSIZE`, in the
ifdef::openshift-origin[]
link:https://github.com/openshift/origin-metrics/blob/master/cassandra/Dockerfile[Cassandra Dockerfile]
endif::openshift-origin[]
ifdef::openshift-enterprise[]
Cassandra Dockerfile
endif::openshift-enterprise[]
prior to deploying cluster metrics.

By default, metrics data is stored for seven days. After seven days, Cassandra
begins to purge the oldest metrics data. Metrics data for deleted pods and
projects is not automatically purged; it is only removed once the data is more
than seven days old.

.Data Accumulated by 10 Nodes and 1000 Pods
====
In a test scenario including 10 nodes and 1000 pods, a 24 hour period
accumulated 2.5 GB of metrics data. Therefore, the capacity planning formula for
metrics data in this scenario is:

(((2.5 × 10^9^) ÷ 1000) ÷ 24) ÷ 10^6^ = ~0.125 MB/hour per pod.
====

.Data Accumulated by 120 Nodes and 10000 Pods
====
In a test scenario including 120 nodes and 10000 pods, a 24 hour period
accumulated 25 GB of metrics data. Therefore, the capacity planning formula for
metrics data in this scenario is:

(((11.410 × 10^9^) ÷ 1000) ÷ 24) ÷ 10^6^ = 0.475 MB/hour
====

|===
| |1000 pods| 10000 pods

|Cassandra storage data accumulated over 24 hours (default metrics parameters)
|2.5 GB
|11.4 GB
|===

ifdef::openshift-origin[]
These two test cases are presented on the following graph:

image::https://raw.githubusercontent.com/ekuric/openshift/master/metrics/1_10kpods.png[1000 pods versus 10000 pods monitored during 24 hours]
endif::openshift-origin[]

If the default value of 7 days for `openshift_metrics_duration` and 30 seconds for
`openshift_metrics_resolution` are preserved, then weekly storage requirements for the Cassandra pod would be:

|===
| |1000 pods | 10000 pods

|Cassandra storage data accumulated over seven days (default metrics parameters)
|20 GB
|90 GB
|===

In the previous table, an additional 10 percent was added to the expected
storage space as a buffer for unexpected monitored pod usage.

[WARNING]
====
If the Cassandra persisted volume runs out of sufficient space, then data loss
occurs.
====

For cluster metrics to work with persistent storage, ensure that the persistent
volume has the *ReadWriteOnce* access mode. If this mode is not active, then the
persistent volume claim cannot locate the persistent volume, and Cassandra fails
to start.

To use persistent storage with the metric components, ensure that a
xref:../architecture/additional_concepts/storage.adoc#persistent-volumes[persistent volume] of
sufficient size is available. The creation of
xref:../architecture/additional_concepts/storage.adoc#persistent-volume-claims[persistent volume claims] is handled by
the OpenShift Ansible `openshift_metrics` role.

{product-title} metrics also supports dynamically-provisioned persistent volumes.
To use this feature with {product-title} metrics, it is necessary to set the value
of `openshift_metrics_cassandra_storage_type` to `dynamic`.
You can use EBS, GCE, and Cinder storage back-ends to
xref:../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[dynamically provision persistent volumes].

For information on configuring the performance and scaling the cluster metrics
pods, see the
xref:../scaling_performance/scaling_cluster_metrics.adoc#scaling-performance-cluster-metrics[Scaling
Cluster Metrics] topic.

[discrete]
[[cluster-metrics-known-issues-and-limitations]]
==== Known Issues and Limitations

Testing found that the `heapster` metrics component is capable of handling up to
25,000 pods. If the amount of pods exceed that number, Heapster begins to fall
behind in metrics processing, resulting in the possibility of metrics graphs no
longer appearing. Work is ongoing to increase the number of pods that Heapster
can gather metrics on, as well as upstream development of alternate
metrics-gathering solutions.

[[metrics-non-persistent-storage]]
=== Non-Persistent Storage

Running {product-title} cluster metrics with non-persistent storage means that
any stored metrics are deleted when the pod is deleted. While it is much
easier to run cluster metrics with non-persistent data, running with
non-persistent data does come with the risk of permanent data loss. However,
metrics can still survive a container being restarted.

In order to use non-persistent storage, you must set the
`openshift_metrics_cassandra_storage_type`
xref:../install_config/cluster_metrics.adoc#metrics-ansible-variables[variable]
to `emptyDir` in the inventory file.

[NOTE]
====
When using non-persistent storage, metrics data is written to
*_/var/lib/origin/openshift.local.volumes/pods_* on the node where the Cassandra
pod runs Ensure *_/var_* has enough free space to accommodate metrics
storage.
====

[[metrics-ansible-role]]
== Metrics Ansible Role

The {product-title} Ansible `openshift_metrics` role configures and deploys all of the
metrics components using the variables from the
xref:../install/configuring_inventory_file.adoc#configuring-ansible[Configuring
Ansible] inventory file.

[[metrics-ansible-variables]]
=== Specifying Metrics Ansible Variables

The `openshift_metrics` role included with OpenShift Ansible defines the tasks
to deploy cluster metrics. The following is a list of role variables that can be
added to your inventory file if it is necessary to override them.

.Ansible Variables
[options="header"]
|===

|Variable |Description

|`openshift_metrics_install_metrics`
|Deploy metrics if `true`. Otherwise, undeploy.

|`openshift_metrics_start_cluster`
|Start the metrics cluster after deploying the components.

|`openshift_metrics_image_prefix`
|The prefix for the component images. With
ifdef::openshift-origin[]
`openshift/origin-metrics-cassandra:v1.3`, set prefix `openshift/origin-`.
endif::[]
ifdef::openshift-enterprise[]
`openshift3/ose-metrics-cassandra:v3.9`, set prefix `openshift/ose-`.
endif::[]

|`openshift_metrics_image_version`
|The version for the component images. For example, with
ifdef::openshift-origin[]
`openshift/origin-metrics-cassandra:v1.3`, set version  as `v1.3`.
endif::[]
ifdef::openshift-enterprise[]
`openshift3/ose-metrics-cassandra:v3.9.11`, set version as `v3.9.11`, or to
always get the latest 3.9 image, set `v3.9`.
endif::[]

|`openshift_metrics_startup_timeout`
|The time, in seconds, to wait until Hawkular Metrics and Heapster start up
before attempting a restart.

|`openshift_metrics_duration`
|The number of days to store metrics before they are purged.

|`openshift_metrics_resolution`
|The frequency that metrics are gathered. Defined as a number and time
identifier: seconds (s), minutes (m), hours (h).

|`openshift_metrics_cassandra_pvc_prefix`
|The persistent volume claim prefix created for Cassandra. A serial number is
appended to the prefix starting from 1.

|`openshift_metrics_cassandra_pvc_size`
|The persistent volume claim size for each of the Cassandra nodes.

|`openshift_metrics_cassandra_storage_type`
|Use `emptyDir` for ephemeral storage (for testing); `pv` for persistent volumes,
which need to be created before the installation; or `dynamic` for dynamic
persistent volumes.

|`openshift_metrics_cassandra_replicas`
|The number of Cassandra nodes for the metrics stack. This value dictates the
number of Cassandra replication controllers.

|`openshift_metrics_cassandra_limits_memory`
|The memory limit for the Cassandra pod. For example, a value of `2Gi` would
limit Cassandra to 2 GB of memory. This value could be further adjusted by the
start script based on available memory of the node on which it is scheduled.

|`openshift_metrics_cassandra_limits_cpu`
|The CPU limit for the Cassandra pod. For example, a value of `4000m` (4000
millicores) would limit Cassandra to 4 CPUs.

|`openshift_metrics_cassandra_requests_memory`
|The amount of memory to request for Cassandra pod. For example, a value of
`2Gi` would request 2 GB of memory.

|`openshift_metrics_cassandra_requests_cpu`
|The CPU request for the Cassandra pod. For example, a value of `4000m` (4000
millicores) would request 4 CPUs.

|`openshift_metrics_cassandra_storage_group`
|The supplemental storage group to use for Cassandra.

|`openshift_metrics_cassandra_nodeselector`
|Set to the desired, existing
xref:../admin_guide/scheduling/node_selector.adoc#admin-guide-sched-selector[node selector] to ensure that
pods are placed onto nodes with specific labels. For example,
`{"region":"infra"}`.

|`openshift_metrics_hawkular_ca`
|An optional certificate authority (CA) file used to sign the Hawkular certificate.

|`openshift_metrics_hawkular_cert`
|The certificate file used for re-encrypting the route to Hawkular metrics. The
certificate must contain the host name used by the route. If unspecified, the
default router certificate is used.

|`openshift_metrics_hawkular_key`
|The key file used with the Hawkular certificate.

|`openshift_metrics_hawkular_limits_memory`
|The amount of memory to limit the Hawkular pod. For example, a value of `2Gi`
would limit the Hawkular pod to 2 GB of memory. This value could be further
adjusted by the start script based on available memory of the node on which it
is scheduled.

|`openshift_metrics_hawkular_limits_cpu`
|The CPU limit for the Hawkular pod. For example, a value of `4000m` (4000
millicores) would limit the Hawkular pod to 4 CPUs.

|`openshift_metrics_hawkular_replicas`
|The number of replicas for Hawkular metrics.

|`openshift_metrics_hawkular_requests_memory`
|The amount of memory to request for the Hawkular pod. For example, a value of
`2Gi` would request 2 GB of memory.

|`openshift_metrics_hawkular_requests_cpu`
|The CPU request for the Hawkular pod. For example, a value of `4000m` (4000
millicores) would request 4 CPUs.

|`openshift_metrics_hawkular_nodeselector`
|Set to the desired, existing
xref:../admin_guide/scheduling/node_selector.adoc#admin-guide-sched-selector[node selector] to ensure that
pods are placed onto nodes with specific labels. For example,
`{"region":"infra"}`.

|`openshift_metrics_heapster_allowed_users`
|A comma-separated list of CN to accept. By default, this is set to allow the
OpenShift service proxy to connect. Add `system:master-proxy` to the list when
overriding in order to allow
xref:../dev_guide/pod_autoscaling.adoc#dev-guide-pod-autoscaling[horizontal pod
autoscaling] to function properly.

|`openshift_metrics_heapster_limits_memory`
|The amount of memory to limit the Heapster pod. For example, a value of `2Gi`
would limit the Heapster pod to 2 GB of memory.

|`openshift_metrics_heapster_limits_cpu`
|The CPU limit for the Heapster pod. For example, a value of `4000m` (4000
millicores) would limit the Heapster pod to 4 CPUs.

|`openshift_metrics_heapster_requests_memory`
|The amount of memory to request for Heapster pod. For example, a value of `2Gi`
would request 2 GB of memory.

|`openshift_metrics_heapster_requests_cpu`
|The CPU request for the Heapster pod. For example, a value of `4000m` (4000
millicores) would request 4 CPUs.

|`openshift_metrics_heapster_standalone`
|Deploy only Heapster, without the Hawkular Metrics and Cassandra components.

|`openshift_metrics_heapster_nodeselector`
|Set to the desired, existing
xref:../admin_guide/scheduling/node_selector.adoc#admin-guide-sched-selector[node selector] to ensure that
pods are placed onto nodes with specific labels. For example,
`{"region":"infra"}`.

|`openshift_metrics_install_hawkular_agent`
|Set to `true` to install the Hawkular OpenShift Agent (HOSA). Set to `false` to
remove the HOSA from an installation. HOSA can be used to collect custom
metrics from your pods. This component is currently in
Technology Preview and is not installed by default.

|`openshift_metrics_hawkular_hostname`
|Set when executing the `openshift_metrics` Ansible role, since it uses the host
name for the Hawkular Metrics xref:../architecture/networking/routes.adoc#architecture-core-concepts-routes[route].
This value should correspond to a fully qualified domain name.
|===

[NOTE]
====
The Hawkular {product-title} Agent on {product-title} is a Technology Preview feature
only.
ifdef::openshift-enterprise[]
Technology Preview features are not
supported with Red Hat production service level agreements (SLAs), might not be
functionally complete, and Red Hat does not recommend to use them for
production. These features provide early access to upcoming product features,
enabling customers to test functionality and provide feedback during the
development process.

For more information on Red Hat Technology Preview features support scope, see
https://access.redhat.com/support/offerings/techpreview/.
endif::[]
====

See xref:../dev_guide/compute_resources.adoc#dev-compute-resources[Compute
Resources] for further discussion on how to specify requests and limits.

If you are using
xref:../install_config/cluster_metrics.adoc#metrics-persistent-storage[persistent
storage] with Cassandra, it is the administrator's responsibility to set a
sufficient disk size for the cluster using the `openshift_metrics_cassandra_pvc_size` variable.
It is also the administrator's responsibility to monitor disk usage to make sure
that it does not become full.

[WARNING]
====
Data loss results if the Cassandra persisted volume runs out of sufficient space.
====

All of the other variables are optional and allow for greater customization.
For instance, if you have a custom install in which the Kubernetes master is not
available under `\https://kubernetes.default.svc:443` you can specify the value
to use instead with the `openshift_metrics_master_url` parameter. To deploy a specific version
of the metrics components, modify the `openshift_metrics_image_version` variable.

[WARNING]
====
It is highly recommended to not use *latest* for the
*openshift_metrics_image_version*. The *latest* version corresponds to the very
latest version available and can cause issues if it brings in a newer version
not meant to function on the version of {product-title} you are currently
running.
====

[[metrics-using-secrets]]
=== Using Secrets

The {product-title} Ansible `openshift_metrics` role auto-generates self-signed certificates for use between its
components and generates a
xref:../architecture/networking/routes.adoc#secured-routes[re-encrypting route] to expose
the Hawkular Metrics service. This route is what allows the web console to access the Hawkular Metrics
service.

In order for the browser running the web console to trust the connection through
this route, it must trust the route's certificate. This can be accomplished by
xref:metrics-using-secrets-byo-certs[providing your own certificates] signed by
a trusted Certificate Authority. The `openshift_metrics` role allows you to
specify your own certificates, which it then uses when creating the route.

The router's default certificate are used if you do not provide your own.

[[metrics-using-secrets-byo-certs]]
==== Providing Your Own Certificates

To provide your own certificate, which is used by the
xref:../architecture/networking/routes.adoc#secured-routes[re-encrypting
route], you can set the `openshift_metrics_hawkular_cert`,
`openshift_metrics_hawkular_key`, and `openshift_metrics_hawkular_ca`
xref:../install_config/cluster_metrics.adoc#metrics-ansible-variables[variables]
in your inventory file.

The `hawkular-metrics.pem` value needs to contain the certificate in its *_.pem_*
format. You may also need to provide the certificate for the Certificate Authority
which signed this *_pem_* file via the `hawkular-metrics-ca.cert` secret.

For more information, see the
xref:../architecture/networking/routes.adoc#secured-routes[re-encryption
route documentation].

[[deploying-the-metrics-components]]
== Deploying the Metric Components

Because deploying and configuring all the metric components is handled with
{product-title} Ansible, you can deploy everything in one step.

The following examples show you how to deploy metrics with and without
persistent storage using the default parameters.

[IMPORTANT]
====
The host that you run the Ansible playbook on must have at least 75MiB of free
memory per host in the inventory.
====

[IMPORTANT]
====
In accordance with upstream Kubernetes rules, metrics can be collected only on the default interface of `eth0`.
====

.Deploying with Persistent Storage
====
The following command sets the Hawkular Metrics route to use
*hawkular-metrics.example.com* and is deployed using persistent storage.

You must have a persistent volume of sufficient size available.

----
$ ansible-playbook [-i </path/to/inventory>] <OPENSHIFT_ANSIBLE_DIR>/playbooks/openshift-metrics/config.yml \
   -e openshift_metrics_install_metrics=True \
   -e openshift_metrics_hawkular_hostname=hawkular-metrics.example.com \
   -e openshift_metrics_cassandra_storage_type=pv
----
====

.Deploying without Persistent Storage
====
The following command sets the Hawkular Metrics route to use
*hawkular-metrics.example.com* and deploy without persistent storage.

----
$ ansible-playbook [-i </path/to/inventory>] <OPENSHIFT_ANSIBLE_DIR>/playbooks/openshift-metrics/config.yml \
   -e openshift_metrics_install_metrics=True \
   -e openshift_metrics_hawkular_hostname=hawkular-metrics.example.com
----
====

[WARNING]
====
Because this is being deployed without persistent storage, metric data loss
can occur.
====

[[metrics-diagnostics]]
=== Metrics Diagnostics

The are some diagnostics for metrics to assist in evaluating the state of the
metrics stack. To execute diagnostics for metrics:

----
$ oc adm diagnostics MetricsApiProxy
----

[[install-setting-the-metrics-public-url]]
== Setting the Metrics Public URL

The {product-title} web console uses the data coming from the Hawkular Metrics
service to display its graphs. The URL for accessing the Hawkular Metrics
service must be configured with the `metricsPublicURL` option in the
xref:../install_config/web_console_customization.adoc#install-config-web-console-customization[master
webconsole-config configmap file].
This URL corresponds to the route created with the
`openshift_metrics_hawkular_hostname` inventory variable used during the
xref:../install_config/cluster_metrics.adoc#deploying-the-metrics-components[deployment]
of the metrics components.

[NOTE]
====
You must be able to resolve the `openshift_metrics_hawkular_hostname` from the browser
accessing the console.
====

For example, if your `openshift_metrics_hawkular_hostname` corresponds to
`hawkular-metrics.example.com`, then you must make the following change in the
webconsole-config configmap file:

====
[source,yaml]
----
clusterInfo:
  ...
  metricsPublicURL: "https://hawkular-metrics.example.com/hawkular/metrics"
----
====

Once you have updated and saved the webconsole-config configmap file, you must
restart your {product-title} instance.

When your {product-title} server is back up and running, metrics are displayed on the pod overview pages.

[CAUTION]
====
If you are using self-signed certificates, remember that the Hawkular Metrics
service is hosted under a different host name and uses different certificates
than the console. You may need to explicitly open a browser tab to the value
specified in `metricsPublicURL` and accept that certificate.

To avoid this issue, use certificates which are configured to be acceptable by
your browser.
====

[[cluster-metrics-accessing-hawkular-metrics-directly]]
== Accessing Hawkular Metrics Directly

To access and manage metrics more directly, use the
link:https://github.com/openshift/origin-metrics/blob/master/docs/hawkular_metrics.adoc#accessing-metrics-using-hawkular-metrics[Hawkular
Metrics API].

[NOTE]
====
When accessing Hawkular Metrics from the API, you are only able to perform
reads. Writing metrics is disabled by default. If you want individual
users to also be able to write metrics, you must set the
`openshift_metrics_hawkular_user_write_access`
xref:../install_config/cluster_metrics.adoc#metrics-ansible-variables[variable]
to *true*.

However, it is recommended to use the default configuration and only have
metrics enter the system via Heapster. If write access is enabled, any user
can write metrics to the system, which can affect performance and
cause Cassandra disk usage to unpredictably increase.
====

The link:http://www.hawkular.org/docs/rest/rest-metrics.html[Hawkular Metrics documentation]
covers how to use the API, but there are a few differences when dealing with the
version of Hawkular Metrics configured for use on {product-title}:

[[cluster-metrics-openshift-projects-and-hawkular-tenants]]
=== {product-title} Projects and Hawkular Tenants

Hawkular Metrics is a multi-tenanted application. It is configured so that a
project in {product-title} corresponds to a tenant in Hawkular Metrics.

As such, when accessing metrics for a project named *MyProject* you must set the
link:http://www.hawkular.org/docs/rest/rest-metrics.html#_tenant_header[*Hawkular-Tenant*]
header to *MyProject*.

There is also a special tenant named *_system* which contains system level
metrics. This requires either a *cluster-reader* or *cluster-admin* level
privileges to access.

[[cluster-metrics-authorization]]
=== Authorization

The Hawkular Metrics service authenticates the user against {product-title}
to determine if the user has access to the project it is trying to access.

Hawkular Metrics accepts a bearer token from the client and verifies that token
with the {product-title} server using a *SubjectAccessReview*. If the user has
proper read privileges for the project, they are allowed to read the metrics
for that project. For the *_system* tenant, the user requesting to read from
this tenant must have *cluster-reader* permission.

When accessing the Hawkular Metrics API, you must pass a bearer token in the
*Authorization* header.

ifdef::openshift-origin[]
[[cluster-metrics-accessing-heapster-directly]]
== Accessing Heapster Directly

Heapster is configured to only be accessible via the API proxy. Accessing
Heapster requires either a cluster-reader or cluster-admin privileges.

For example, to access the Heapster *validate* page, you need to access it
using something similar to:

----
$ curl -H "Authorization: Bearer XXXXXXXXXXXXXXXXX" \
       -X GET https://${KUBERNETES_MASTER}/api/v1/proxy/namespaces/openshift-infra/services/https:heapster:/validate
----

For more information about Heapster and how to access its APIs, refer the
link:https://github.com/kubernetes/heapster/[Heapster] project.
endif::[]

[[metrics-scaling-metrics-pods]]
== Scaling {product-title} Cluster Metrics Pods

Information about scaling cluster metrics capabilities is available in the
xref:../scaling_performance/scaling_cluster_metrics.adoc#cluster-metrics-scaling-openshift-metrics-pods[Scaling and
Performance Guide].

[[metrics-cleanup]]
== Cleanup

You can remove everything deployed by the {product-title} Ansible `openshift_metrics` role
by performing the following steps:

----
$ ansible-playbook [-i </path/to/inventory>] <OPENSHIFT_ANSIBLE_DIR>/playbooks/openshift-metrics/config.yml \
   -e openshift_metrics_install_metrics=False
----

[[openshift-prometheus]]
== Prometheus on {product-title}

Prometheus is a stand-alone, open source systems monitoring and alerting
toolkit. You can use Prometheus to visualize metrics and alerts for {product-title}
system resources.

[IMPORTANT]
====
Prometheus on {product-title} is a Technology Preview feature only.
ifdef::openshift-enterprise[]
Technology Preview features are not supported with Red Hat production service
level agreements (SLAs), might not be functionally complete, and Red Hat does
not recommend to use them for production. These features provide early access to
upcoming product features, enabling customers to test functionality and provide
feedback during the development process.

For more information on Red Hat Technology Preview features support scope, see
https://access.redhat.com/support/offerings/techpreview/.
endif::[]
====

[[openshift-prometheus-roles]]
=== Setting Prometheus Role Variables

The Prometheus role creates:

* The `openshift-metrics` namespace.
* Prometheus `clusterrolebinding` and service account.
* Prometheus pod with Prometheus behind OAuth proxy, Alertmanager, and
Alert Buffer as a stateful set.
* Prometheus and `prometheus-alerts` ConfigMaps.
* Prometheus and Prometheus Alerts services and direct routes.

Prometheus deployment is disabled by default, enable it by setting
`openshift_hosted_prometheus_deploy` to `true`. For example:

----
# openshift_hosted_prometheus_deploy=true
----

Set the following role variables to install and configure Prometheus.

.Prometheus Variables
[options="header"]
|===

|Variable |Description

|`openshift_prometheus_namespace`
|Project namespace where the components are deployed. Default set to
`openshift-metrics`. For example, `openshift_prometheus_namespace=${USER_PROJECT}`.

|`openshift_prometheus_node_selector`
|Selector for the nodes on which Prometheus is deployed.

|`openshift_prometheus_storage_kind`
|Set to create PV for Prometheus. For example,
`openshift_prometheus_storage_kind=nfs`.

|`openshift_prometheus_alertmanager_storage_kind`
|Set to create PV for Alertmanager. For example,
`openshift_prometheus_alertmanager_storage_kind=nfs`.

|`openshift_prometheus_alertbuffer_storage_kind`
|Set to create PV for Alert Buffer. For example,
`openshift_prometheus_alertbuffer_storage_kind=nfs`.

|`openshift_prometheus_storage_type`
|Set to create PVC for Prometheus. For example,
`openshift_prometheus_storage_type=pvc`.

|`openshift_prometheus_alertmanager_storage_type`
|Set to create PVC for Alertmanager. For example,
`openshift_prometheus_alertmanager_storage_type=pvc`.

|`openshift_prometheus_alertbuffer_storage_type`
|Set to create PVC for Alert Buffer. For example,
`openshift_prometheus_alertbuffer_storage_type=pvc`.

|`openshift_prometheus_additional_rules_file`
|Additional Prometheus rules file. Set to `null` by default.

|===

[[openshift-prometheus-deploy]]
=== Deploying Prometheus Using Ansible Installer

[IMPORTANT]
====
The host that you run the Ansible playbook on must have at least 75MiB of free
memory per host in the inventory.
====

The Ansible Installer is the default method of deploying Prometheus.

Add label to your node:
----
# Inventory file
openshift_prometheus_namespace=openshift-metrics

openshift_prometheus_node_selector={"region":"infra"}
----

Run the playbook:
----
$ ansible-playbook -vvv -i ${INVENTORY_FILE} playbooks/openshift-prometheus/config.yml
----

[[openshift-prometheus-additional-deploy]]
==== Additional Methods for Deploying Prometheus

*Deploy Using Node-Selector*

Label the node on which you want to deploy Prometheus:
----
# oadm label node/$NODE ${KEY}=${VALUE}
----

Deploy Prometheus with Ansible and container resources:
----
# Inventory file
openshift_prometheus_namespace=openshift-metrics

# Set node selector for prometheus
openshift_prometheus_node_selector={"${KEY}":"${VALUE}"}
----

Run the playbook:
----
$ ansible-playbook -vvv -i ${INVENTORY_FILE} playbooks/openshift-prometheus/config.yml
----

*Deploy Using a Non-default Namespace*

Identify your namespace:
----
# Inventory file
openshift_prometheus_node_selector={"region":"infra"}

# Set non-default openshift_prometheus_namespace
openshift_prometheus_namespace=${USER_PROJECT}
----

Run the playbook:
----
$ ansible-playbook -vvv -i ${INVENTORY_FILE} playbooks/openshift-prometheus/config.yml
----

[[openshift-prometheus-web]]
==== Accessing the Prometheus Web UI

The Prometheus server automatically exposes a Web UI at `localhost:9090`. You
can access the Prometheus Web UI with the `view` role.

[[openshift-prometheus-config]]
==== Configuring Prometheus for {product-title}
//
// Example Prometheus rules file:
// ----
// # additional prometheus rules file
// openshift_prometheus_additional_rules_file: null
//
// # All the required exports
// openshift_prometheus_pv_exports:
// -  prometheus
// -  prometheus-alertmanager
// -  prometheus-alertbuffer
// # PV template files and their created object names
//   openshift_prometheus_pv_data:
//   -  pv_name: prometheus
//      pv_template: prom-pv-server.yml
//      pv_label: Prometheus Server PV
//   -  pv_name: prometheus-alertmanager
//      pv_template: prom-pv-alertmanager.yml
//      pv_label: Prometheus Alertmanager PV
//   -  pv_name: prometheus-alertbuffer
//      pv_template: prom-pv-alertbuffer.yml
//      pv_label: Prometheus Alert Buffer PV
//
// # Hostname/IP of the NFS server.
// openshift_prometheus_storage_host
// openshift_prometheus_alertmanager_storage_host
// openshift_prometheus_alertbuffer_storage_host
//
// # storage
// openshift_prometheus_storage_type: pvc
// openshift_prometheus_pvc_name: prometheus
// openshift_prometheus_pvc_size: 10G
// openshift_prometheus_pvc_access_modes: [ReadWriteOnce]
// openshift_prometheus_pvc_pv_selector: {}
//
// openshift_prometheus_alertmanager_storage_type: pvc
// openshift_prometheus_alertmanager_pvc_name: prometheus-alertmanager
// openshift_prometheus_alertmanager_pvc_size: 10G
// openshift_prometheus_alertmanager_pvc_size: 10G
// openshift_prometheus_alertmanager_pvc_access_modes: [ReadWriteOnce]
// openshift_prometheus_alertmanager_pvc_pv_selector: {}
// openshift_prometheus_cpu_limit: null
// openshift_prometheus_memory_limit: null
// openshift_prometheus_cpu_requests: null
// openshift_prometheus_memory_requests: null
// openshift_prometheus_alertmanager_cpu_limit: null
// openshift_prometheus_alertmanager_memory_limit: null
// openshift_prometheus_alertmanager_cpu_requests: null
// openshift_prometheus_alertmanager_memory_requests: null
// openshift_prometheus_alertbuffer_cpu_limit: null
// openshift_prometheus_alertbuffer_memory_limit: null
// openshift_prometheus_alertbuffer_cpu_requests: null
// openshift_prometheus_alertbuffer_memory_requests: null
// openshift_prometheus_oauth_proxy_cpu_limit: null
// openshift_prometheus_oauth_proxy_memory_limit: null
// openshift_prometheus_oauth_proxy_cpu_requests: null
// openshift_prometheus_oauth_proxy_memory_requests: null
// ----

*Prometheus Storage Related Variables*

With each Prometheus component (including Prometheus, Alertmanager, Alert
Buffer, and OAuth proxy) you can set the PV claim by setting corresponding
role variable, for example:

----
openshift_prometheus_storage_type: pvc
openshift_prometheus_alertmanager_pvc_name: alertmanager
openshift_prometheus_alertbuffer_pvc_size: 10G
openshift_prometheus_pvc_access_modes: [ReadWriteOnce]
----

*Prometheus Alert Rules File Variable*

You can add an external file with alert rules by setting the path to an
additional rules variable:

----
openshift_prometheus_additional_rules_file: <PATH>
----

The file content should be in Prometheus Alert rules format. The following
example sets a rule to send an alert when one of the cluster nodes is down:

----
groups:
- name: example-rules
  interval: 30s # defaults to global interval
  rules:
  - alert: Node Down
    expr: up{job="kubernetes-nodes"} == 0
    annotations:
      miqTarget: "ContainerNode"
      severity: "HIGH"
      message: "{{ '{{' }}{{ '$labels.instance' }}{{ '}}' }} is down"
----

*Prometheus Variables to Control Resource Limits*

With each Prometheus component (including Prometheus, Alertmanager,
Alert Buffer, and OAuth proxy) you can specify CPU, memory limits, and requests
by setting the corresponding role variable, for example:

----
openshift_prometheus_alertmanager_limits_memory: 1Gi
openshift_prometheus_oauth_proxy_cpu_requests: 100m
----

For more detailed information, see
link:https://github.com/openshift/openshift-ansible/tree/master/roles/openshift_prometheus#openshift-prometheus[OpenShift
Prometheus].

[NOTE]
====
Once `openshift_metrics_project: openshift-infra` is installed, metrics can be
gathered from the `http://${POD_IP}:7575/metrics` endpoint.
====

[[openshift-prometheus-kubernetes-metrics]]
=== {product-title} Metrics via Prometheus

The state of a system can be gauged by the metrics that it emits. This section
describes current and proposed metrics that identify the health of the storage subsystem and
cluster.

[[k8s-current-metrics]]
==== Current Metrics

This section describes the metrics currently emitted from Kubernetes’s storage subsystem.

*Cloud Provider API Call Metrics*

This metric reports the time and count of success and failures of all
cloudprovider API calls. These metrics include `aws_attach_time` and
`aws_detach_time`. The type of emitted metrics is a histogram, and hence,
Prometheus also generates sum, count, and bucket metrics for these metrics.

.Example summary of cloudprovider metrics from GCE:
----
cloudprovider_gce_api_request_duration_seconds { request = "instance_list"}
cloudprovider_gce_api_request_duration_seconds { request = "disk_insert"}
cloudprovider_gce_api_request_duration_seconds { request = "disk_delete"}
cloudprovider_gce_api_request_duration_seconds { request = "attach_disk"}
cloudprovider_gce_api_request_duration_seconds { request = "detach_disk"}
cloudprovider_gce_api_request_duration_seconds { request = "list_disk"}
----

.Example summary of cloudprovider metrics from AWS:
----
cloudprovider_aws_api_request_duration_seconds { request = "attach_volume"}
cloudprovider_aws_api_request_duration_seconds { request = "detach_volume"}
cloudprovider_aws_api_request_duration_seconds { request = "create_tags"}
cloudprovider_aws_api_request_duration_seconds { request = "create_volume"}
cloudprovider_aws_api_request_duration_seconds { request = "delete_volume"}
cloudprovider_aws_api_request_duration_seconds { request = "describe_instance"}
cloudprovider_aws_api_request_duration_seconds { request = "describe_volume"}
----

See
link:https://github.com/kubernetes/community/blob/master/contributors/design-proposals/cloud-provider/cloudprovider-storage-metrics.md[Cloud
Provider (specifically GCE and AWS) metrics for Storage API calls] for more
information.

*Volume Operation Metrics*

These metrics report time taken by a storage operation once started. These
metrics keep track of operation time at the plug-in level, but do not include
time taken by `goroutine` to run or operation to be picked up from the internal
queue. These metrics are a type of histogram.

.Example summary of available volume operation metrics
----
storage_operation_duration_seconds { volume_plugin = "aws-ebs", operation_name = "volume_attach" }
storage_operation_duration_seconds { volume_plugin = "aws-ebs", operation_name = "volume_detach" }
storage_operation_duration_seconds { volume_plugin = "glusterfs", operation_name = "volume_provision" }
storage_operation_duration_seconds { volume_plugin = "gce-pd", operation_name = "volume_delete" }
storage_operation_duration_seconds { volume_plugin = "vsphere", operation_name = "volume_mount" }
storage_operation_duration_seconds { volume_plugin = "iscsi" , operation_name = "volume_unmount" }
storage_operation_duration_seconds { volume_plugin = "aws-ebs", operation_name = "unmount_device" }
storage_operation_duration_seconds { volume_plugin = "cinder" , operation_name = "verify_volumes_are_attached" }
storage_operation_duration_seconds { volume_plugin = "<n/a>" , operation_name = "verify_volumes_are_attached_per_node" }
----

See
link:https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/volume-metrics.md[Volume
operation metrics] for more information.

*Volume Stats Metrics*

These metrics typically report usage stats of PVC (such as used space versus available space). The type of metrics emitted is gauge.

.Volume Stats Metrics
|===
|Metric|Type|Labels/tags

|volume_stats_capacityBytes
|Gauge
|namespace,persistentvolumeclaim,persistentvolume=

|volume_stats_usedBytes
|Gauge
|namespace=<persistentvolumeclaim-namespace>
persistentvolumeclaim=<persistentvolumeclaim-name>
persistentvolume=<persistentvolume-name>

|volume_stats_availableBytes
|Gauge
|namespace=<persistentvolumeclaim-namespace>
persistentvolumeclaim=<persistentvolumeclaim-name>
persistentvolume=

|volume_stats_InodesFree
|Gauge
|namespace=<persistentvolumeclaim-namespace>
persistentvolumeclaim=<persistentvolumeclaim-name>
persistentvolume=<persistentvolume-name>

|volume_stats_Inodes
|Gauge
|namespace=<persistentvolumeclaim-namespace>
persistentvolumeclaim=<persistentvolumeclaim-name>
persistentvolume=<persistentvolume-name>

|volume_stats_InodesUsed
|Gauge
|namespace=<persistentvolumeclaim-namespace>
persistentvolumeclaim=<persistentvolumeclaim-name>
persistentvolume=<persistentvolume-name>
|===

[[openshift-prometheus-undeploy]]
=== Undeploying Prometheus

To undeploy Prometheus, run:

----
$ ansible-playbook -vvv -i ${INVENTORY_FILE} playbooks/openshift-prometheus/config.yml -e openshift_prometheus_state=absent
----
