[[install-config-cluster-metrics]]
= Enabling Cluster Metrics
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

The
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#kubelet[kubelet]
exposes metrics that can be collected and stored in back-ends by
link:https://github.com/GoogleCloudPlatform/heapster[Heapster].

As an {product-title} administrator, you can view a cluster's metrics from all
containers and components in one user interface.  These metrics are also
used by xref:../dev_guide/pod_autoscaling.adoc#dev-guide-pod-autoscaling[horizontal pod autoscalers]
in order to determine when and how to scale.

This topic describes using
link:https://github.com/hawkular/hawkular-metrics[Hawkular Metrics] as a metrics
engine which stores the data persistently in a
link:http://cassandra.apache.org/[Cassandra] database. When this is configured,
CPU, memory and network-based metrics are viewable from the {product-title} web console
and are available for use by xref:../dev_guide/pod_autoscaling.adoc#dev-guide-pod-autoscaling[horizontal
pod autoscalers].

Heapster retrieves a list of all nodes from the master server, then contacts
each node individually through the `/stats` endpoint. From there, Heapster
scrapes the metrics for CPU, memory and network usage, then exports them into Hawkular
Metrics.

Browsing individual pods in the web console displays separate sparkline charts
for memory and CPU. The time range displayed is selectable, and these charts
automatically update every 30 seconds. If there are multiple containers on the
pod, then you can select a specific container to display its metrics.

If xref:../admin_guide/limits.adoc#admin-guide-limits[resource limits] are defined for your
project, then you can also see a donut chart for each pod. The donut chart
displays usage against the resource limit. For example: `145 Available of 200
MiB`, with the donut chart showing `55 MiB Used`.

ifdef::openshift-origin[]
For more information about the metrics integration, please refer to the
link:https://github.com/openshift/origin-metrics[Origin Metrics] GitHub project.
endif::[]

[[cluster-metrics-before-you-begin]]
== Before You Begin

ifdef::openshift-origin[]
[WARNING]
====
If your {product-title} installation was originally performed on a version
previous to v1.0.8, even if it has since been updated to a newer version, follow
the instructions for node certificates outlined in
xref:../install_config/upgrading/manual_upgrades.adoc#manual-updating-master-and-node-certificates[Updating
Master and Node Certificates]. If the node certificate does not contain the IP
address of the node, then Heapster will fail to retrieve any metrics.
====
endif::[]

An Ansible playbook is available to deploy cluster metrics.  You should familiarize yourself with the
xref:../install_config/install/advanced_install.adoc[advanced installation and configuration] section.
This provides information for prepairing to use Ansible and includes information about configuration.
Parameters are added to the Ansible inventory file to configure various areas of cluster metrics.

The following describe the various areas and the parameters that can be added to the Ansible
inventory file in order to modify the defaults:

- xref:../install_config/cluster_metrics.adoc#metrics-namespace[Metrics Project]
- xref:../install_config/cluster_metrics.adoc#metrics-data-storage[Metrics Data Storage]

[[metrics-namespace]]
== Metrics Project

The components for cluster metrics must be deployed to the *openshift-infra*
project in order for autoscaling to work. xref:../dev_guide/pod_autoscaling.adoc#dev-guide-pod-autoscaling[Horizontal pod
autoscalers] specifically use this project to discover the Heapster service and use it to retrieve metrics.  The metrics
project can be changed by adding `*openshift_metrics_project*` to the inventory file.

[[metrics-data-storage]]
== Metrics Data Storage

You can store the metrics data to either
xref:../architecture/additional_concepts/storage.adoc#architecture-additional-concepts-storage[persistent storage] or to
a temporary xref:../dev_guide/volumes.adoc#dev-guide-volumes[pod volume].

[[metrics-persistent-storage]]
=== Persistent Storage

Running {product-title} cluster metrics with persistent storage means that
your metrics will be stored to a
xref:../architecture/additional_concepts/storage.adoc#persistent-volumes[persistent
volume] and be able to survive a pod being restarted or recreated. This is
ideal if you require your metrics data to be guarded from data loss.  For production environments it is highly recommended
to configure persistent storage for your metrics pods.

The size requirement of the Cassandra storage is dependent on the number of
pods. It is the administrator's responsibility to ensure that the size
requirements are sufficient for their setup and to monitor usage to ensure that
the disk does not become full.  The size of the persisted volume is specified with the `*openshift_metrics_cassandra_pv_size*`
xref:../install_config/cluster_metrics.adoc#metrics-ansible-variables[ansible variable] which is set to 10 GB by default.

If you would like to use xref:../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[dynamically provisioned persistent volumes]
set the `*openshift_metrics_cassandra_storage_type*`
xref:../install_config/cluster_metrics.adoc#metrics-ansible-variable[variable]
to `*dynamic*` in the inventory file.

[[capacity-planning-for-openshift-metrics]]
=== Capacity Planning for Cluster Metrics

After running the `*openshift_metrics*` Ansible role, the output of `oc get pods` should resemble the following:

====
----
 # oc get pods -n openshift-infra
 NAME                                READY             STATUS      RESTARTS          AGE
 hawkular-cassandra-1-l5y4g          1/1               Running     0                 17h
 hawkular-metrics-1t9so              1/1               Running     0                 17h
 heapster-febru                      1/1               Running     0                 17h
----
====

{product-title} metrics are stored using the Cassandra database, which is
deployed with settings of `*openshift_metrics_cassandra_limits_memory*: 2G`; this value could
be adjusted further based upon the avialable memory as determined by the Cassandra start script.
This value should cover most  {product-title} metrics installations, but you
can modify, via environment variables, `*MAX_HEAP_SIZE*` along with heap new generation
size (`*HEAP_NEWSIZE*`) in the
ifdef::openshift-origin[]
link:https://github.com/openshift/origin-metrics/blob/master/cassandra/Dockerfile[Cassandra Dockerfile]
endif::openshift-origin[]
ifdef::openshift-enterprise[]
Cassandra Dockerfile
endif::openshift-enterprise[]
prior to deploying cluster metrics.

By default, metrics data is stored for 7 days. You can configure this by setting the `*openshift_metrics_duration*`
variable in your hosts file which will update the upon deployment
ifdef::openshift-origin[]
link:https://github.com/openshift/origin-metrics/blob/master/metrics.yaml[*_metrics.yaml_* configuration file].
endif::openshift-origin[]
ifdef::openshift-enterprise[]
*_metrics.yaml_* configuration file.
endif::openshift-enterprise[]
After 7 days, Cassandra begins to purge the oldest metrics data.
Metrics data for deleted pods and projects is not automatically
purged; it is only removed once the data is 7 days old.

.Data Accumulated by 10 Nodes and 1000 Pods
====
In a test scenario including 10 nodes and 1000 pods, a 24 hour period
accumulated 2.5GB of metrics data. Therefore, the capacity planning formula for
metrics data in this scenario is:

(((2.5 × 10^9^) ÷ 1000) ÷ 24) ÷ 10^6^ = ~0.125 MB/hour per pod.
====

.Data Accumulated by 120 Nodes and 10000 Pods
====
In a test scenario including 120 nodes and 10000 pods, a 24 hour period
accumulated 25GB of metrics data. Therefore, the capacity planning formula for
metrics data in this scenario is:

(((11.410 × 10^9^) ÷ 1000) ÷ 24) ÷ 10^6^ = 0.475 MB/hour
====

|===
| |1000 pods| 10000 pods

|Cassandra storage data accumulated over 24 hours (default metrics parameters)
|2.5GB
|11.4GB
|===

ifdef::openshift-origin[]
These two test cases are presented on the following graph:

image::https://raw.githubusercontent.com/ekuric/openshift/master/metrics/1_10kpods.png[1000 pods vs 10000 pods monitored during 24 hours]
endif::openshift-origin[]

If the default value of 7 days for `*openshift_metrics_duration*` and 10 seconds for
`*openshift_metrics_resolution*` are preserved, then weekly storage requirements for the Cassandra pod would be:

|===
| |1000 pods | 10000 pods

|Cassandra storage data accumulated over 7 days (default metrics parameters)
|20GB
|90GB
|===

In the previous table, an additional 10% was added to the expected storage space
as a buffer for unexpected monitored pod usage.

[WARNING]
====
If the Cassandra persisted volume runs out of sufficient space, then data loss
will occur.
====

For cluster metrics to work with persistent storage, ensure that the persistent
volume has the *ReadWriteOnce* access mode. If this mode is not active, then the persistent volume claim
cannot locate the persistent volume, and Cassandra fails to start.

To use persistent storage with the metric components, ensure that a
xref:../architecture/additional_concepts/storage.adoc#persistent-volumes[persistent volume] of
sufficient size is available. The creation of
xref:../architecture/additional_concepts/storage.adoc#persistent-volume-claims[persistent volume claims] is handled by
the OpenShift Ansible `*openshift_metrics*` role.

{product-title} metrics also supports
ifdef::openshift-origin[]
link:https://github.com/openshift/origin-metrics/blob/master/metrics.yaml#L130[dynamically-provisioned persistent volumes].
endif::openshift-origin[]
ifdef::openshift-enterprise[]
dynamically-provisioned persistent volumes.
endif::openshift-enterprise[]
To use this feature with {product-title} metrics, it is necessary to set the value
of `*openshift_metrics_cassandra_storage_type*` to `*dynamic*`.
You can use EBS, GCE, and Cinder storage back-ends to
xref:../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[dynamically provision persistent volumes].

[[metrics-non-persistent-storage]]
=== Non-Persistent Storage

Running {product-title} cluster metrics with non-persistent storage means that
any stored metrics will be deleted when the pod is deleted. While it is much
easier to run cluster metrics with non-persistent data, running with
non-persistent data does come with the risk of permanent data loss. However,
metrics can still survive a container being restarted.

In order to use non-persistent storage, you must set the
`*openshift_metrics_cassandra_storage_type*`
xref:../install_config/cluster_metrics.adoc#metrics-ansible-variables[variable] to `*emptydir*`
in the inventory file.

[NOTE]
====
When using non-persistent storage, metrics data will be written to
*_/var/lib/origin/openshift.local.volumes/pods_* on the node where the Cassandra
pod is running. Ensure *_/var_* has enough free space to accommodate metrics
storage.
====

[[metrics-ansible-role]]
== Metrics Ansible Role

The OpenShift Ansible `*openshift_metrics*` role configures and deploys all of the
metrics components using the variables from the xref:../install_config/install/advanced_install.adoc[inventory file].

[[metrics-using-secrets]]
=== Using Secrets

The OpenShift Ansible `*openshift_metrics*` role will auto-generate self-signed certificates for use between its
components and will generate a
xref:../architecture/core_concepts/routes.adoc#secured-routes[re-encrypting route] to expose
the Hawkular Metrics service. This route is what allows the web console to access the Hawkular Metrics
service.

In order for the browser running the web console to trust the connection through this route,
it must trust the route's certificate. This can be accomplished by
xref:metrics-using-secrets-byo-certs[providing your own certificates] signed by a trusted
Certificate Authority. The `*openshift_metrics*` role allows you to specify your own certificates
which it will then use when creating the route.

The router's default certificate are used if you do not provide your own.

[[metrics-using-secrets-byo-certs]]
==== Providing Your Own Certificates

To provide your own certificate which will be used by the
xref:../architecture/core_concepts/routes.adoc#secured-routes[re-encrypting route],
you can set the 'cert, key, and ca' xref:../install_config/cluster_metrics.adoc#metrics-ansible-variables[variables]
in your inventory file.

The `hawkular-metrics.pem` value needs to contain the certificate in its *_.pem_*
format. You may also need to provide the certificate for the Certificate Authority
which signed this *_pem_* file via the `hawkular-metrics-ca.cert` secret.

For more information, please see the
xref:../architecture/core_concepts/routes.adoc#secured-routes[re-encryption
route documentation].

[[metrics-ansible-variables]]
==== Metrics Ansible Variables

The `openshift_metrics` role included with OpenShift Ansible defines the tasks to deploy
cluster metrics.  Following is a list of role variables that can
be added to your inventory file if it is necessary to override them.

.Ansible Variables
[options="header"]
|===

|Variable |Description

|`*openshift_metrics_install_metrics*`
|Deploy metrics if `true`.  Otherwise undeploy

|`*openshift_metrics_start_cluster*`
|Start the metrics cluster after deploying the components.

|`*openshift_metrics_image_prefix*`
|The prefix for the component images. e.g for "openshift/origin-metrics-cassandra:v1.3", set prefix "openshift/origin-".

|`*openshift_metrics_image_version*`
|The version for the component images. e.g. for "openshift/origin-metrics-cassandra:v1.3", set version "v1.3"

|`*openshift_metrics_startup_timeout*`
|The time in seconds to wait until Hawkular Metrics and Heapster starts up before attempting a restart.

|`*openshift_metrics_duration*`
| The number of days to store metrics before they are purged

|`*openshift_metrics_resolution*`
| The frequency that metrics are gathered defined as a number and time identifier(s,m,h)
(e.g. 15s for 15 seconds).

|`*openshift_metrics_cassandra_pv_prefix*`
|The persistent volume claim prefix created for Cassandra. A serial number is appended to the prefix starting
  from 1.

|`*openshift_metrics_cassandra_pv_size*`
| The persistent volume size for each of the Cassandra  nodes.

|`*openshift_metrics_cassandra_storage_type*`
| Use `emptydir` for ephemeral storage (for  testing), `pv` to use persistent volumes
(which need to be created before the installation) or `dynamic` for dynamic persistent volumes.

|`*openshift_metrics_cassandra_replicas*`
| The number of Cassandra nodes for the metrics stack.  There will be this many
Cassandra replication controllers.

|`*openshift_metrics_cassandra_limits_memory*`
| The amount of memory to limit the Cassandra pod (e.g. 2G).  This value could be further adjusted
by the start script based on available memory of the node on which it is scheduled.

|`*openshift_metrics_cassandra_limits_cpu*`
| The CPU limit for the Cassandra pod.

|`*openshift_metrics_cassandra_replicas*`
|The number of replicas for Cassandra

|`*openshift_metrics_cassandra_request_memory*`
| The amount of memory to request for Cassandra pod (e.g. 1.5G).

|`*openshift_metrics_cassandra_request_cpu*`
| The CPU request for the Cassandra pod

|`*openshift_metrics_hawkular_ca*`
| An optional certificate file:0 used to sign the Hawkular certificate

|`*openshift_metrics_hawkular_cert*`
|The certificate file used for re-encrypting the route to Hawkular metrics.  The certificate must contain
the hostname used by the route. The default router certificate is used if unspecified.

|`*openshift_metrics_hawkular_key*`
| The key file used with the Hawkular certificate

|`*openshift_metrics_hawkular_limits_memory*`
| The amount of memory to limit the Hawkular pod (e.g. 2G).  This value could be further adjusted
by the start script based on available memory of the node on which it is scheduled.

|`*openshift_metrics_hawkular_limits_cpu*`
| The CPU limit for the Hawkular pod.

|`*openshift_metrics_hawkular_replicas*`
| The number of replicas for Hawkular metrics

|`*openshift_metrics_hawkular_request_memory*`
| The amount of memory to request for Hawkular pod (e.g. 1.5G).

|`*openshift_metrics_hawkular_request_cpu*`
| The CPU request for the Hawkular pod

|`*openshift_metrics_heapster_allowed_users*`
| A comma-separated list of CN to accept.  By  default, this is set to allow the
OpenShift service proxy to connect.  *Note:* Add `system:master-proxy` to the list
when overriding in order to allow xref:../dev_guide/pod_autoscaling.adoc#dev-guide-pod-autoscaling[horizontal pod autoscaling]
to function properly.

|`*openshift_metrics_heapster_limits_memory*`
| The amount of memory to limit the Heapster pod (e.g. 2G).

|`*openshift_metrics_heapster_limits_cpu*`
| The CPU limit for the Heapster pod.

|`*openshift_metrics_heapster_request_memory*`
| The amount of memory to request for Heapster pod (e.g. 1.5G).

|`*openshift_metrics_heapster_request_cpu*`
| The CPU request for the Heapster pod

|`*openshift_metrics_heapster_standalone*`
|Deploy only heapster, without the Hawkular Metrics and Cassandra components.

|===

The only required variable is `*openshift_metrics_hawkular_hostname*`. This value is
required when executing the `*openshift_metrics*` Ansible role because it uses the host name for the
Hawkular Metrics xref:../architecture/core_concepts/routes.adoc#architecture-core-concepts-routes[route]. This
value should correspond to a fully qualified domain name. You must know
the value of `*openshift_metrics_hawkular_hostname*` when
xref:../install_config/cluster_metrics.adoc#configuring-openshift-metrics[configuring the console] for metrics access.

If you are using
xref:../install_config/cluster_metrics.adoc#metrics-persistent-storage[persistent
storage] with Cassandra, it is the administrator's responsibility to set a
sufficient disk size for the cluster using the `*openshift_metrics_cassandra_pv_size*` variable.
It is also the administrator's responsibility to monitor disk usage to make sure
that it does not become full.

[WARNING]
====
Data loss will result if the Cassandra persisted volume runs out of sufficient space.
====

All of the other variables are optional and allow for greater customization.
For instance, if you have a custom install in which the Kubernetes master is not
available under *_https://kubernetes.default.svc:443_* you can specify the value
to use instead with the `*openshift_metrics_master_url*` parameter. To deploy a specific version
of the metrics components, modify the `*openshift_metrics_image_version*` variable.

[WARNING]
====
It is highly recommended to not use *latest* for the *openshift_metrics_image_version*. The *latest*
version corresponds to the very latest version available and can cause issues if it brings in a
newer version not meant to function on the version of {product-title} you are currently running.
====

[[deploying-the-metrics-components]]
== Deploying the Metric Components

Because deploying and configuring all the metric components is handled with OpenShift Ansible,
you can deploy everything in one step.

The following examples show you how to deploy metrics with and without
persistent storage using the default parameters.

.Deploying with Persistent Storage
====
The following command sets the Hawkular Metrics route to use
*hawkular-metrics.example.com* and is deployed using persistent storage.

You must have a persistent volume of sufficient size available.

----
$ ansible-playbook <OPENSHIFT_ANSIBLE_DIR>/common/openshift-cluster/openshift_metrics.yml \
   -e openshift_metrics_install_metrics=True \
   -e openshift_metrics_hawkular_hostname=hawkular-metrics.example.com \
   -e openshift_metrics_cassandra_storage_type=pv
----
====

.Deploying without Persistent Storage
====
The following command sets the Hawkular Metrics route to use
*hawkular-metrics.example.com* and deploy without persistent storage.

----
$ ansible-playbook <OPENSHIFT_ANSIBLE_DIR>/common/openshift-cluster/openshift_metrics.yml \
   -e openshift_metrics_install_metrics=True \
   -e openshift_metrics_hawkular_hostname=hawkular-metrics.example.com
----
====

[WARNING]
====
Because this is being deployed without persistent storage, metric data loss
can occur.
====

[[metrics-diagnostics]]
=== Metrics Diagnostics

The are some diagnostics for metrics to assist in evaluating the state of the
metrics stack.  To execute diagnostics for metrics:

----
$ oadm diagnostics MetricsApiProxy
----

[[configuring-openshift-metrics]]
== Configuring {product-title}

The {product-title} web console uses the data coming from the Hawkular Metrics
service to display its graphs. The URL for accessing the Hawkular Metrics
service must be configured via the `*metricsPublicURL*` option in the
xref:../install_config/master_node_configuration.adoc#master-configuration-files[master
configuration file] (*_/etc/origin/master/master-config.yaml_*). This URL
corresponds to the route created with the `*openshift_metrics_hawkular_hostname*`
inventory variable used during the
xref:../install_config/cluster_metrics.adoc#deploying-the-metrics-components[deployment]
of the metrics components.

[NOTE]
====
You must be able to resolve the `*openshift_metrics_hawkular_hostname*` from the browser
accessing the console.
====

For example, if your `*openshift_metrics_hawkular_hostname*` corresponds to
`hawkular-metrics.example.com`, then you must make the following change in the
*_master-config.yaml_* file:

====
[source,yaml,]
----
  assetConfig:
    ...
    metricsPublicURL: "https://hawkular-metrics.example.com/hawkular/metrics"
----
====

Once you have updated and saved the *_master-config.yaml_* file, you must
restart your {product-title} instance.

When your {product-title} server is back up and running, metrics will be
displayed on the pod overview pages.

[CAUTION]
====
If you are using self-signed certificates, remember that the Hawkular Metrics
service is hosted under a different host name and uses different certificates
than the console. You may need to explicitly open a browser tab to the value
specified in `*metricsPublicURL*` and accept that certificate.

To avoid this issue, use certificates which are configured to be acceptable by
your browser.
====

[[cluster-metrics-accessing-hawkular-metrics-directly]]
== Accessing Hawkular Metrics Directly

To access and manage metrics more directly, use the Hawkular Metrics API.

[NOTE]
====
When accessing Hawkular Metrics via the API, you will only be able to perform
reads. Writing metrics has been disabled by default. If you want for individual
users to also be able to write metrics, you must set the
`*openshift_metrics_hawkular_user_write_access*`
xref:../install_config/cluster_metrics.adoc#metrics-ansible-variables[variable]
to *true*.

However, it is recommended to use the default configuration and only have
metrics enter the system via Heapster. If write access is enabled, any user
will be able to write metrics to the system, which can affect performance and
cause Cassandra disk usage to unpredictably increase.
====

The link:http://www.hawkular.org/docs/rest/rest-metrics.html[Hawkular Metrics documentation]
covers how to use the API, but there are a few differences when dealing with the
version of Hawkular Metrics configured for use on {product-title}:

[[cluster-metrics-openshift-projects-and-hawkular-tenants]]
=== {product-title} Projects and Hawkular Tenants

Hawkular Metrics is a multi-tenanted application. It is configured so that a
project in {product-title} corresponds to a tenant in Hawkular Metrics.

As such, when accessing metrics for a project named *MyProject* you must set the
link:http://www.hawkular.org/docs/rest/rest-metrics.html#_tenant_header[*Hawkular-Tenant*]
header to *MyProject*.

There is also a special tenant named *_system* which contains system level
metrics. This requires either a *cluster-reader* or *cluster-admin* level
privileges to access.

[[cluster-metrics-authorization]]
=== Authorization

The Hawkular Metrics service will authenticate the user against {product-title}
to determine if the user has access to the project it is trying to access.

Hawkular Metrics accepts a bearer token from the client and verifies that token
with the {product-title} server using a *SubjectAccessReview*. If the user has
proper read privileges for the project, they are allowed to read the metrics
for that project. For the *_system* tenant, the user requesting to read from
this tenant must have *cluster-reader* permission.

When accessing the Hawkular Metrics API, you must pass a bearer token in the
*Authorization* header.

ifdef::openshift-origin[]
[[cluster-metrics-accessing-heapster-directly]]
== Accessing Heapster Directly

Heapster has been configured to be only accessible via the
xref:../rest_api/kubernetes_v1.adoc#proxy-get-requests-to-service[API proxy].
Accessing it will required either a cluster-reader or cluster-admin privileges.

For example, to access the Heapster *validate* page, you need to access it
using something similar to:

----
$ curl -H "Authorization: Bearer XXXXXXXXXXXXXXXXX" \
       -X GET https://${KUBERNETES_MASTER}/api/v1/proxy/namespaces/openshift-infra/services/https:heapster:/validate
----

For more information about Heapster and how to access its APIs, please refer the
link:https://github.com/kubernetes/heapster/[Heapster] project.
endif::[]

[[cluster-metrics-scaling-openshift-metrics-pods]]
== Scaling {product-title} Metrics Pods

One set of metrics pods (Cassandra/Hawkular/Heapster) is able to monitor at
least 10,000 pods.

[CAUTION]
====
Pay attention to system load on nodes where {product-title} metrics pods run.
Use that information to determine if it is necessary to scale out a number of
{product-title} metrics pods and spread the load across multiple {product-title}
nodes. Scaling {product-title} metrics heapster pods is not recommended.
====

[[cluster-metrics-scaling-pods-prereqs]]
=== Prerequisites

If persistent storage was used to deploy {product-title} metrics, then you must
xref:../dev_guide/persistent_volumes.adoc#dev-guide-persistent-volumes[create a persistent volume (PV)]
for the new Cassandra pod to use before you can scale out the number of
{product-title} metrics Cassandra pods. However, if Cassandra was deployed with
dynamically provisioned PVs, then this step is not necessary.

[[cluster-metrics-scaling-pods-cassandra]]
=== Scaling the Cassandra Components

The Cassandra nodes use persistent storage, therefore scaling up or down is not possible with replication controllers.

Scaling a Cassandra cluster requires you to modify the `*openshift_metrics_cassandra_replicas*` variable and
re-running the xref:../install_config/cluster_metrics.adoc#deploying-the-metrics-components[deployment].
By default, the Cassandra cluster is a single-node cluster.
ifdef::openshift-origin[]

To deploy more nodes, provision storage if `*openshift_metrics_cassandra_replicas*` equals `pv` and
increase the `*openshift_metrics_cassandra_replicas*` value.
endif::openshift-origin[]

ifdef::openshift-enterprise[]
To scale out the number of {product-title} metrics hawkular pods to two
replicas, run:

----
# oc scale -n openshift-infra --replicas=2 rc hawkular-metrics
----

or update your inventory file and re-run the xref:../install_config/cluster_metrics.adoc#deploying-the-metrics-components[deployment].

endif::openshift-enterprise[]

[NOTE]
====
If you add a new node to a Cassandra cluster, the data stored in the cluster
rebalances across the cluster. The same thing happens If you remove a node from
the Cluster.
====

ifdef::openshift-enterprise[]
[[cluster-metrics-horizontal-pod-autoscaling]]
== Horizontal Pod Autoscaling

{product-title} version 3.3 does not provide
xref:../dev_guide/pod_autoscaling.adoc#dev-guide-pod-autoscaling[Horizontal
Pod Autoscaling (HPA)] support for metrics pods and scaling metrics pods.
endif::[]

[[metrics-cleanup]]
== Cleanup

You can remove everything deployed by the OpenShift Ansible `*openshift_metrics*` role
by performing the following steps:

----
$ ansible-playbook <OPENSHIFT_ANSIBLE_DIR>/common/openshift-cluster/openshift_metrics.yml \
   -e openshift_metrics_install_metrics=False
----
