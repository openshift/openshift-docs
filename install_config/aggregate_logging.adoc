= Aggregating Container Logs
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

OpenShift cluster administrators can aggregate logs and enable application
developers to view them for a range of OpenShift services using the EFK stack,
which is a modified version of the
https://www.elastic.co/videos/introduction-to-the-elk-stack[ELK stack]. The EFK
stack can be useful for viewing logs aggregated from hosts and applications,
whether coming from multiple containers or even deleted pods.

Three components make up the EFK logging stack:

* https://www.elastic.co/products/elasticsearch[Elasticsearch]: An object store where all logs are stored.
* http://www.fluentd.org/architecture[Fluentd]: Gathers logs from nodes and feeds them to Elasticsearch.
* https://www.elastic.co/guide/en/kibana/current/introduction.html[Kibana]: A web UI for Elasticsearch.

Once deployed in a cluster, the stack aggregates logs from all nodes and
projects into Elasticsearch, and provides a Kibana UI to view any logs. Cluster
administrators can view all logs, but application developers can only view logs
for the project the have permission to view.

[NOTE]
====
link:../install_config/install/prerequisites.html#managing-docker-container-logs[Managing
Docker Container Logs] discusses the use of `json-file` logging driver options
to manage container logs when they become full.
====

== Pre-deployment Configuration

Deploying the EFK stack requires:

* A *logging-deployer* secret
* A *logging-deployer* service account with access to the secret and creation privileges
* Sufficient volumes for Elasticsearch cluster storage
* A router deployment for serving cluster-defined routes

The deployer generates the objects necessary for communication within the
cluster, and defines secrets and templates to implement aggregated logging.

. Create a new project. Once implemented in a single project, the EFK stack will
collect logs for every project within your OpenShift cluster. The examples in
this topic use `logging` as an example project:
+
====
----
$ oadm new-project logging --node-selector label=value
----
====
+
[NOTE]
====
Using link:../admin_guide/managing_projects.html#using-node-selectors[node
selectors] is optional. These can be used to restrict the placement of pods onto
nodes in which the logging infrastructure deploys.

Restricting the project will also restrict where Fluentd can be deployed.
It is instead recommended to specify node selectors on your Kibana and
Elasticsearch Deployment Configs once created.

See link:#logging-node-selector[Node Selector] for examples on how to do this.
====
+
Ensure that you switch to your new project before moving on to the next step. In
this example, the project name is called `logging`:
+
====
----
$ oc project logging
----
====
+
. Create a link:../dev_guide/secrets.html[secret] for the deployer. The secret
can be configured specifically for your instance of the deployer.
+
You can supply the following files when creating a new secret:
+
[cols="2",options="header"]
|===
|File Name
|Description

|*_kibana.crt_*
|A browser-facing certificate for the Kibana server.

|*_kibana.key_*
|A key to be used with the Kibana certificate.

|*_kibana-ops.crt_*
|A browser-facing certificate for the Ops Kibana server.

|*_kibana-ops.key_*
|A key to be used with the Ops Kibana certificate.

|*_server-tls.json_*
|JSON TLS options to override the Kibana server defaults. Refer to
https://nodejs.org/api/tls.html#tls_tls_connect_options_callback[Node.JS] docs
for available options.

|*_ca.crt_*
|A certificate for a CA that will be used to sign all certificates generated by
the deployer.

|*_ca.key_*
|A matching CA key.
|===
+
For example:
+
----
$ oc secrets new logging-deployer \
   kibana.crt=/path/to/cert kibana.key=/path/to/key
----
+
If a certificate file is not passed as a secret, the deployer will generate a
self-signed certificate instead. However, a secret is still needed for
certificates to be generated. In this case, you can create a "dummy" secret that
does not specify a certificate value:
+
----
$ oc secrets new metrics-deployer nothing=/dev/null
----

. Create the deployer link:../admin_guide/service_accounts.html[service
account]:
+
====
----
$ oc create -f - <<API
apiVersion: v1
kind: ServiceAccount
metadata:
  name: logging-deployer
secrets:
- name: logging-deployer
API
----
====
+
Then, enable the *logging-deployer* service account to edit the project
contents. By default service accounts are not allowed to do this. The following
example uses the `logging` project. Be sure to use the project created above in
the following example:
+
====
----
$ oc policy add-role-to-user edit \
            system:serviceaccount:default:logging-deployer
----
====

. Enable the Fluentd service account, which the deployer will create, that
requires special privileges to operate Fluentd. Add the service account user to
the security context. Be sure to use the project created above in the following
example:
+
====
----
$ oadm policy add-scc-to-user  \
    privileged system:serviceaccount:logging:aggregated-logging-fluentd
----
====
+
Give the Fluentd service account permission to read labels from all pods. Be
sure to use the project created above in the following example:
+
====
----
$ oadm policy add-cluster-role-to-user cluster-reader \
    system:serviceaccount:logging:aggregated-logging-fluentd
----
====

== Deploying the EFK Stack

The EFK stack is deployed link:../dev_guide/templates.html[using a template].
Ansible-based installs should create the *_logging-deployer-template_* template in the *_openshift_* project.
ifdef::openshift-enterprise[]
Otherwise, the template can be created with the following command:

----
$ oc create -n openshift -f /usr/share/openshift/examples/infrastructure-templates/enterprise/logging-deployer.yaml
----
endif::openshift-enterprise[]
ifdef::openshift-origin[]
Otherwise, the template can be created with the following command:

----
$ oc create -n openshift -f https://github.com/openshift/openshift-ansible/blob/master/roles/openshift_examples/files/examples/infrastructure-templates/origin/logging-deployer.yaml
----
endif::openshift-origin[]

. Run the deployer, specifying at least the parameters in the following example (more are described in the table below):
+
====
----
$ oc process logging-deployer-template -n openshift \
           -v KIBANA_HOSTNAME=kibana.example.com,ES_CLUSTER_SIZE=1,PUBLIC_MASTER_URL=https://localhost:8443 \
           | oc create -f -
----
====
+
Be sure to replace at least `*KIBANA_HOSTNAME*` and `*PUBLIC_MASTER_URL*` with
values relevant to your deployment.
+
The available parameters are:
+
[cols="3,7",options="header"]
|===
|Variable Name
|Description

|`*KIBANA_HOSTNAME*`
|(Required with the `oc process` command) The external host name for web clients
to reach Kibana.

|`*PUBLIC_MASTER_URL*`
|(Required with the `oc process` command) The external URL for the master. For
OAuth use.

|`*ES_CLUSTER_SIZE*`
|(Required with the `oc process` command) The amount of instances of
Elasticsearch to deploy. Redundancy requires at least three, and more can be
used for scaling.

|`*IMAGE_PREFIX*`
|The prefix for logging component images. For example, setting the prefix to
*openshift/origin-* creates *openshift/origin-logging-deployer:v1.1*.

|`*IMAGE_VERSION*`
|The version for logging component images. For example, setting the version to
*v1.1* creates *openshift/origin-logging-deployer:v1.1*.

|`*ES_INSTANCE_RAM*`
|Amount of RAM to reserve per Elasticsearch instance. The default is 8G (for 8GB), and it
must be at least 512M. Possible suffixes are G,g,M,m.

|`*ENABLE_OPS_CLUSTER*`
|If set to `*true*`, configures a second Elasticsearch cluster and Kibana for
operations logs.

|`*KIBANA_OPS_HOSTNAME*`, `*ES_OPS_INSTANCE_RAM*`, `*ES_OPS_CLUSTER_SIZE*`
|Variables for the operations log cluster.
|===
+
[NOTE]
====
When setting the `*ENABLE_OPS_CLUSTER*` parameter to `*true*`, Fluentd splits
logs between the Elasticsearch cluster and a cluster reserved for operations
logs. This means a second Elasticsearch and Kibana are deployed. The deployments
are distinguishable by the *-ops* included in their names.
====
+
Running the deployer creates a deployer pod and prints its name. Wait until the
pod is running. This can take up to a few minutes to retrieve the deployer image
from the registry. You can watch its process with:
+
----
$ oc get pod/<pod_name> -w
----
+
If it seems to be taking too long, you can retrieve more details about the pod
and any associated events with:
+
----
$ oc describe pod/<pod_name>
----
+
When it runs, you can check the logs of the resulting pod to see if the
deployment was successful:
+
----
$ oc logs -f <pod_name>
----

. As a cluster administrator, deploy a template that is created by the deployer:
+
====
----
$ oc process logging-support-template | oc create -f -
----
====

== Post-deployment Configuration

=== Elasticsearch

All pods created from an OpenShift deployment share the storage volumes
specified for the deployment. However, Elasticsearch pods cannot share storage.
The ability to specify multiple volumes to be allocated to each instance in a
deployment is currently being completed. Currently, multiple deployments are
used in order to scale Elasticsearch. To view all current deployments used by
Elasticsearch:

====
----
$ oc get dc --selector logging-infra=elasticsearch
----
====

////
To scale Elasticsearch deployments, create and add more deployments, being aware
of the cluster parameter restrictions. The deployer uses a template to create
Elasticsearch deployments. These deployments will be named differently, but will
all have the 'service/logging-es-cluster' prefix:

====
----
$ oc process logging-es-template | oc create -f -
----
====
////

*Persistent Elasticsearch Storage*

The deployer creates an ephemeral deployment in which all of a pod's data is
lost upon restart. For production, persistent storage is recommended. The
following example specifies a persistent storage volume for the Elasticsearch
deployment. You can use the `oc volume` command to add a created volume to a
deployment:

====
----
$ oc volume dc/logging-es-rca2m9u8 \
          --add --overwrite --name=elasticsearch-storage \
          --type=hostPath --path=/path/to/storage
----
====

[NOTE]
====
Allowing the pods to mount host volumes as above usually requires adding the
`aggregated-logging-elasticsearch` service account to the privileged SCC,
similarly as for Fluentd above.
====

You can use any volume type, such as
link:../install_config/persistent_storage/persistent_storage_nfs.html[NFS].

[[logging-node-selector]]
*Node Selector*

Because Elasticsearch can use a lot of resources, all members of a cluster
should have low latency network connections to each other. Ensure this by
directing the instances to dedicated nodes, or a dedicated region within your
cluster, using a
link:../admin_guide/managing_projects.html#using-node-selectors[node selector].

To configure a node selector, edit each deployment configuration and add the
`*nodeSelector*` parameter to specify the label of the desired nodes:

====
----
apiVersion: v1
kind: DeploymentConfig
spec:
  template:
    spec:
      nodeSelector:
        nodelabel: logging-es-node-1
----
====

Alternatively you can use the `oc patch` command:
====
----
$ oc patch dc/logging-es-<unique_name> \
   -p '{"spec":{"template":{"spec":{"nodeSelector":{"nodeLabel":"logging-es-node-1"}}}}}'
----
====


=== Fluentd

Once Elasticsearch is running, scale Fluentd to every node to feed logs into
Elasticsearch. The following example is for an OpenShift instance with three
nodes:

====
----
$ oc scale dc/logging-fluentd --replicas=3
$ oc scale rc/logging-fluentd-1 --replicas=3
----
====

You will need to scale Fluentd if nodes are added or subtracted.

If the logging project was created using a node selector, then scaling Fluentd
will fail. The scheduler will try to place all Fluentd replicas onto the same
node. To fix this, edit the project:

====
----
$ oc edit project logging
----
====

And remove the entry for the node selector:

====
----
...
openshift.io/node-selector: region=infra
...
----
====

=== Kibana

To access the Kibana console from the OpenShift web console, add the
`loggingPublicURL` parameter in the *_/etc/origin/master/master-config.yaml_*
file, with the URL of the Kibana console (the `*KIBANA_HOSTNAME*` parameter).
The value must be an HTTPS URL:

====
----
...
assetConfig:
  ...
  loggingPublicURL: "https://kibana.example.com"
...
----
====

Setting the `loggingPublicURL` parameter creates a *View Archive* button on the
OpenShift web console under the *Browse* -> *Pods* -> *<pod_name>* -> *Logs*
tab. This links to the Kibana console.

You can scale the Kibana deployment as usual for redundancy:

====
----
$ oc scale dc/logging-kibana --replicas=2
$ oc scale rc/logging-kibana-1 --replicas=2
----
====

You can see the UI by visiting the site specified at the `*KIBANA_HOSTNAME*`
variable.

See the https://www.elastic.co/guide/en/kibana/4.1/discover.html[Kibana
documentation] for more information on Kibana.

=== Cleanup

After deployment, the deployer can be removed:

----
$ oc delete sa/logging-deployer secret/logging-deployer
----

Similarly, you can remove everything generated during the deployment that will
not destroy the project:

----
$ oc delete all --selector logging-infra=kibana
$ oc delete all --selector logging-infra=fluentd
$ oc delete all --selector logging-infra=elasticsearch
$ oc delete all,sa,oauthclient --selector logging-infra=support
$ oc delete secret logging-fluentd logging-elasticsearch \
    logging-es-proxy logging-kibana logging-kibana-proxy \
    logging-kibana-ops-proxy
----

== Troubleshooting Kibana

Using the Kibana console with OpenShift can cause problems that are easily
solved, but are not accompanied with useful error messages. Check the following
troubleshooting sections if you are experiencing any problems when deploying
Kibana on OpenShift:

*Login Loop*

The OAuth2 proxy on the Kibana console must share a secret with the master
host's OAuth2 server. If the secret is not identical on both servers, it can
cause a login loop where you are continuously redirected back to the Kibana
login page.

To fix this issue, delete the current oauthclient, and create a new one, using the
same template as before:

====
----
$ oc delete oauthclient/kibana-proxy
$ oc process logging-support-template | oc create -f -
----
====

*Cryptic Error When Viewing the Console*

When attempting to visit the Kibana console, you may instead receive a browser
error:

====
----
{"error":"invalid_request","error_description":"The request is missing a required parameter,
 includes an invalid parameter value, includes a parameter more than once, or is otherwise malformed."}
----
====

This can be caused by a mismatch between the OAuth2 client and server. The
return address for the client must be in a whitelist so the server can securely
redirect back after logging in.

Fix this issue by replacing the OAuth client entry:

====
----
$ oc delete oauthclient/kibana-proxy
$ oc process logging-support-template | oc create -f -
----
====

If the problem persists, check that you are accessing Kibana at a URL listed in
the OAuth client. This issue can be caused by accessing the URL at a forwarded
port, such as 1443 instead of the standard 443 HTTPS port. You can adjust the
server whitelist by editing the OAuth client:

====
----
$ oc edit oauthclient/kibana-proxy
----
====

*503 Error When Viewing the Console*

If you receive a proxy error when viewing the Kibana console, it could be caused
by one of two issues.

First, Kibana may not be recognizing pods. If Elasticsearch is slow in starting
up, Kibana may timeout trying to reach it. Check whether the relevant service
has any endpoints:

====
----
$ oc describe service logging-kibana
Name:                   logging-kibana
[...]
Endpoints:              <none>
----
====

If any Kibana pods are live, endpoints will be listed. If they are not, check
the state of the Kibana pods and deployment. You may need to scale the
deployment down and back up again.

The second possible issue may be caused if the route for accessing the Kibana
service is masked. This can happen if you perform a test deployment in one
project, then deploy in a different project without completely removing the
first deployment. When multiple routes are sent to the same destination, the
default router will only route to the first created. Check the problematic route
to see if it is defined in multiple places:

====
----
$ oc get route  --all-namespaces --selector logging-infra=support
----
====
