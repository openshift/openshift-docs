= Aggregating Container Logs
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

OpenShift cluster administrators can aggregate logs and enable application
developers to view them for a range of OpenShift services using the EFK stack,
which is a modified version of the
https://www.elastic.co/videos/introduction-to-the-elk-stack[ELK stack]. The EFK
stack can be useful for viewing logs aggregated from hosts and applications,
whether coming from multiple containers or even deleted pods.

These components make up the EFK logging stack:

* https://www.elastic.co/products/elasticsearch[Elasticsearch]: An object store where all logs are stored.
* http://www.fluentd.org/architecture[Fluentd]: Gathers logs from nodes and feeds them to Elasticsearch.
* https://www.elastic.co/guide/en/kibana/current/introduction.html[Kibana]: A web UI for Elasticsearch.
ifdef::openshift-origin[]
* https://www.elastic.co/guide/en/elasticsearch/client/curator/current/about.html[Curator]: Removes old logs from Elasticsearch.
endif::openshift-origin[]

Once deployed in a cluster, the stack aggregates logs from all nodes and
projects into Elasticsearch, and provides a Kibana UI to view any logs. Cluster
administrators can view all logs, but application developers can only view logs
for projects they have permission to view.

[NOTE]
====
link:../install_config/install/prerequisites.html#managing-docker-container-logs[Managing
Docker Container Logs] discusses the use of `json-file` logging driver options
to manage container logs and prevent filling node disks.
====

== Pre-deployment Configuration

The deployer for the EFK stack generates the objects necessary to
implement aggregated logging, including secrets for secure communication
within the cluster.

Before running the deployer, the following are required:

* A router deployment for serving cluster-defined routes
* Sufficient volumes for Elasticsearch cluster storage (one per replica)
* A project, typically reserved just for this deployment
* A *logging-deployer* secret
* A *logging-deployer* service account with access to the secret and creation privileges

Perform the following steps to prepare for deployment:

. Ensure that you have deployed a router for the cluster.
. Ensure that you have the necessary storage for Elasticsearch.
ifdef::openshift-enterprise[]
. Ansible-based installs should create the *logging-deployer-template*
template in the *openshift* project.
Otherwise you can create it with the following command:
+
====
----
$ oc create -n openshift -f \
    /usr/share/openshift/examples/infrastructure-templates/enterprise/logging-deployer.yaml
----
====
endif::openshift-enterprise[]
ifdef::openshift-origin[]
. If your installation did not create templates in the *openshift* namespace, the
*logging-deployer-template* and *logging-deployer-account-template* templates
may not exist. In that case you can create them with the following command:
+
====
----
$ oc create -n openshift -f \
    https://raw.githubusercontent.com/openshift/origin-aggregated-logging/master/deployment/deployer.yaml
----
====
endif::openshift-origin[]
. Create a new project. Once implemented in a single project, the EFK stack will
collect logs for every project within your OpenShift cluster. The examples in
this topic use *logging* as an example project:
+
====
----
$ oadm new-project logging --node-selector=""
$ oc project logging
----
====
+
[NOTE]
====
Specifying a non-empty
link:../admin_guide/managing_projects.html#using-node-selectors[node
selector] on the project is not recommended, as this would restrict
where Fluentd can be deployed. Instead, specify node selectors for the
deployer to be applied on your other deployment configurations.
====

. Create a link:../dev_guide/secrets.html[secret] for the deployer. The secret
can be configured specifically for your instance of the deployer.
+
You can supply the following files when creating a new secret:
+
[cols="2",options="header"]
|===
|File Name
|Description

|*_kibana.crt_*
|A browser-facing certificate for the Kibana server.

|*_kibana.key_*
|A key to be used with the Kibana certificate.

|*_kibana-ops.crt_*
|A browser-facing certificate for the Ops Kibana server.

|*_kibana-ops.key_*
|A key to be used with the Ops Kibana certificate.

|*_server-tls.json_*
|JSON TLS options to override the Kibana server defaults. Refer to
https://nodejs.org/api/tls.html#tls_tls_connect_options_callback[Node.JS] docs
for available options.

|*_ca.crt_*
|A certificate for a CA that will be used to sign all certificates generated by
the deployer.

|*_ca.key_*
|A matching CA key.
|===
+
For example:
+
----
$ oc secrets new logging-deployer \
   kibana.crt=/path/to/cert kibana.key=/path/to/key
----
+
If a certificate file is not passed as a secret, the deployer will generate a
self-signed certificate instead. However, a secret is still required for
the deployer to run. In this case, you can create a "dummy" secret that
does not specify a certificate value:
+
----
$ oc secrets new logging-deployer nothing=/dev/null
----

ifdef::openshift-enterprise[]
. Create the deployer link:../admin_guide/service_accounts.html[service
account]:
+
====
----
$ oc create -f - <<API
apiVersion: v1
kind: ServiceAccount
metadata:
  name: logging-deployer
secrets:
- name: logging-deployer
API
----
====
endif::openshift-enterprise[]
ifdef::openshift-origin[]
. Create the deployer link:../admin_guide/service_accounts.html[service
account] and custom roles:
+
====
----
$ oc process logging-deployer-account-template -n openshift \
     | oc create -f -
----
====
endif::openshift-origin[]
+
Then, enable the *logging-deployer* service account to create the objects
needed for a logging deployment:
ifdef::openshift-enterprise[]
+
====
----
$ oc policy add-role-to-user edit --serviceaccount logging-deployer
----
====
endif::openshift-enterprise[]
ifdef::openshift-origin[]
+
====
----
$ oc policy add-role-to-user edit --serviceaccount logging-deployer
$ oc policy add-role-to-user daemonset-admin --serviceaccount logging-deployer
$ oadm policy add-cluster-role-to-user oauth-editor \
       system:serviceaccount:logging:logging-deployer <1>
----
<1> Use the new project you created earlier (e.g., *logging*) when specifying
this service account.
====
endif::openshift-origin[]

. Enable the Fluentd service account, which the deployer will create, that
requires special privileges to operate Fluentd. Add the service account user to
the security context:
+
====
----
$ oadm policy add-scc-to-user  \
    privileged system:serviceaccount:logging:aggregated-logging-fluentd <1>
----
<1> Use the new project you created earlier (e.g., *logging*) when specifying
this service account.
====
+
Give the Fluentd service account permission to read labels from all pods:
+
====
----
$ oadm policy add-cluster-role-to-user cluster-reader \
    system:serviceaccount:logging:aggregated-logging-fluentd <1>
----
<1> Use the new project you created earlier (e.g., *logging*) when specifying
this service account.
====

== Deploying the EFK Stack

The EFK stack is deployed link:../dev_guide/templates.html[using a template].

. Run the deployer, specifying at least the parameters in the following example (more are described in the table below):
+
====
----
$ oc process logging-deployer-template -n openshift \
           -v KIBANA_HOSTNAME=kibana.example.com,ES_CLUSTER_SIZE=1,PUBLIC_MASTER_URL=https://localhost:8443 \
           | oc create -f -
----
====
+
Be sure to replace at least `*KIBANA_HOSTNAME*` and `*PUBLIC_MASTER_URL*` with
values relevant to your deployment.
+
The available parameters are:
+
[cols="3,7",options="header"]
|===
|Variable Name
|Description

|`*PUBLIC_MASTER_URL*`
|(Required with the `oc process` command) The external URL for the master. For
OAuth use.

|`*ENABLE_OPS_CLUSTER*`
|If set to `*true*`, configures a second Elasticsearch cluster and Kibana for
operations logs. Fluentd splits
logs between the main cluster and a cluster reserved for operations
logs (which consists of *_/var/log/messages_* on nodes and the logs from the
projects *default*, *openshift*, and *openshift-infra*).
This means a second Elasticsearch and Kibana are deployed. The deployments
are distinguishable by the *-ops* included in their names and have parallel
deployment options listed below.

|`*KIBANA_HOSTNAME*`, `*KIBANA_OPS_HOSTNAME*`
|(Required with the `oc process` command) The external host name for web clients
to reach Kibana.

|`*ES_CLUSTER_SIZE*`, `*ES_OPS_CLUSTER_SIZE*`
|(Required with the `oc process` command) The number of instances of
Elasticsearch to deploy. Redundancy requires at least three, and more can be
used for scaling.

|`*ES_INSTANCE_RAM*`, `*ES_OPS_INSTANCE_RAM*`
|Amount of RAM to reserve per Elasticsearch instance. The default is 8GB, and it
must be at least 512MB.

|`*ES_NODE_QUORUM*`, `*ES_OPS_NODE_QUORUM*`
|The quorum required to elect a new master. Should be more than half the intended cluster size.

|`*ES_RECOVER_AFTER_NODES*`, `*ES_OPS_RECOVER_AFTER_NODES*`
|When restarting the cluster, require this many nodes to be present before starting recovery.
Defaults to one less than the cluster size to allow for one missing node.

|`*ES_RECOVER_EXPECTED_NODES*`, `*ES_OPS_RECOVER_EXPECTED_NODES*`
|When restarting the cluster, wait for this number of nodes to be present before starting recovery.
By default, the same as the cluster size.

|`*ES_RECOVER_AFTER_TIME*`, `*ES_OPS_RECOVER_AFTER_TIME*`
|When restarting the cluster, this is a timeout for waiting for the expected number of nodes to be present.
Defaults to "5m".

ifdef::openshift-origin[]
|`*ES_NODESELECTOR*`, `*ES_OPS_NODESELECTOR*`
| A node selector that specifies which nodes are eligible targets
for deploying Elasticsearch instances. This can be used to place
these instances on nodes reserved and/or optimized for running them.
For example, the selector could be `*node-type=infrastructure*`. At least
one active node must have this label before Elasticsearch will deploy.

|`*KIBANA_NODESELECTOR*`, `*KIBANA_OPS_NODESELECTOR*`, `*CURATOR_NODESELECTOR*`
| A node selector that specifies which nodes are eligible targets
for deploying Kibana or Curator instances.

|`*FLUENTD_NODESELECTOR*`
| A node selector that specifies which nodes are eligible targets
for deploying Fluentd instances. Defaults to "logging-infra-fluentd=true".

|`*IMAGE_PREFIX*`
|The prefix for logging component images. For example, setting the prefix to
*openshift/origin-* creates *openshift/origin-logging-deployer:v1.1*.

|`*IMAGE_VERSION*`
|The version for logging component images. For example, setting the version to
*v1.1* creates *openshift/origin-logging-deployer:v1.1*.
endif::openshift-origin[]
ifdef::openshift-enterprise[]
|`*IMAGE_PREFIX*`
|The prefix for logging component images. For example, setting the prefix to
*registry.access.redhat.com/openshift3/ose-* creates *registry.access.redhat.com/openshift3/ose-logging-deployer:latest*.

|`*IMAGE_VERSION*`
|The version for logging component images. For example, setting the version to
*v3.1* creates *registry.access.redhat.com/openshift3/ose-logging-deployer:v3.1*.
endif::openshift-enterprise[]
|===
+
Running the deployer creates a deployer pod and prints its name. Wait until the
pod is running. This can take up to a few minutes to retrieve the deployer image
from the registry. You can watch its process with:
+
----
$ oc get pod/<pod_name> -w
----
+
If it seems to be taking too long to start, you can retrieve more details about the pod
and any associated events with:
+
----
$ oc describe pod/<pod_name>
----
+
When it runs, you can check the logs of the resulting pod to see if the
deployment was successful:
+
----
$ oc logs -f <pod_name>
----

ifdef::openshift-enterprise[]
. As a cluster administrator, deploy a template that is created by the deployer:
+
====
----
$ oc process logging-support-template | oc create -f -
----
====
endif::openshift-enterprise[]

== Post-deployment Configuration

=== Elasticsearch

In any production employment, Elasticsearch should be deployed with a
cluster size of at least three for resiliency to node failures. Each instance
requires individual storage, but an OpenShift deployment can only provide
volumes shared by all its pods, so the Elasticsearch cluster cannot be
implemented with a single deployment. The EFK deployer instead creates one
deployment per instance. To view all current deployments of Elasticsearch:

====
----
$ oc get dc --selector logging-infra=elasticsearch
----
====

*Persistent Elasticsearch Storage*

The deployer creates an ephemeral deployment in which all of a pod's
data is lost upon restart. For production usage, persistent storage is
required. You can use the `oc volume` command to add a created volume
to each deployment. The following example specifies a volume for an
Elasticsearch instance (using a
link:../architecture/additional_concepts/storage.html#persistent-volume-claims[PersistentVolumeClaim]):

====
----
$ oc volume dc/logging-es-rca2m9u8 \
          --add --overwrite --name=elasticsearch-storage \
          --type=persistentVolumeClaim --claim-name=logging-es-1
----
====

[NOTE]
====
Any available volume type can be used, such as a host-mount, but the
recommended volume type is a PersistentVolumeClaim.
====

ifdef::openshift-enterprise[]
[[logging-node-selector]]
*Node Selector*

Because Elasticsearch can use a lot of resources, all members of a cluster
should have low latency network connections to each other. Ensure this by
directing the instances to dedicated nodes, or a dedicated region within your
cluster, using a
link:../admin_guide/managing_projects.html#using-node-selectors[node selector].

To configure a node selector, edit each deployment configuration and add the
`*nodeSelector*` parameter to specify the label of the desired nodes:

====
----
apiVersion: v1
kind: DeploymentConfig
spec:
  template:
    spec:
      nodeSelector:
        nodelabel: logging-es-node-1
----
====

Alternatively you can use the `oc patch` command:
====
----
$ oc patch dc/logging-es-<unique_name> \
   -p '{"spec":{"template":{"spec":{"nodeSelector":{"nodeLabel":"logging-es-node-1"}}}}}'
----
====
endif::openshift-enterprise[]

[[scaling-elasticsearch]]
*Changing the Scale of Elasticsearch*

To scale Elasticsearch up, create more deployments. The deployer provides
a template to create Elasticsearch deployments:

====
----
$ oc process logging-es-template | oc create -f -
----
====

These deployments will be named differently, but all will have the
*logging-es* prefix. You should be aware of the cluster
parameters (described in the deployer parameters) based on cluster size
that may need corresponding adjustment in the template as well as
existing deployments.


=== Fluentd

ifdef::openshift-enterprise[]
Once Elasticsearch is running, scale Fluentd to every node to feed logs into
Elasticsearch. The following example is for an OpenShift instance with three
nodes:

====
----
$ oc scale dc/logging-fluentd --replicas=3
----
====

You will need to scale Fluentd if nodes are added or subtracted.
endif::openshift-enterprise[]

ifdef::openshift-origin[]
Once Elasticsearch is running, label nodes to enable Fluentd to run on them
and feed logs to Elasticsearch. Use the `*FLUENTD_NODESELECTOR*` given to
the deployer (if different) in the command below:

====
----
$ oc label nodes --all logging-infra-fluentd=true
----
====

endif::openshift-origin[]

=== Kibana

To access the Kibana console from the OpenShift web console, add the
`loggingPublicURL` parameter in the *_/etc/origin/master/master-config.yaml_*
file, with the URL of the Kibana console (the `*KIBANA_HOSTNAME*` parameter).
The value must be an HTTPS URL:

====
----
...
assetConfig:
  ...
  loggingPublicURL: "https://kibana.example.com"
...
----
====

Setting the `loggingPublicURL` parameter creates a *View Archive* button on the
OpenShift web console under the *Browse* -> *Pods* -> *<pod_name>* -> *Logs*
tab. This links to the Kibana console.

You can scale the Kibana deployment as usual for redundancy:

====
----
$ oc scale dc/logging-kibana --replicas=2
----
====

You can see the UI by visiting the site specified at the `*KIBANA_HOSTNAME*`
variable.

See the https://www.elastic.co/guide/en/kibana/4.1/discover.html[Kibana
documentation] for more information on Kibana.

=== Cleanup

You can remove everything generated during the deployment while
leaving other project contents intact:

----
$ oc delete all --selector logging-infra=kibana
ifdef::openshift-enterprise[]
$ oc delete all --selector logging-infra=fluentd
endif::openshift-enterprise[]
ifdef::openshift-origin[]
$ oc delete all,daemonsets --selector logging-infra=fluentd
endif::openshift-origin[]
$ oc delete all --selector logging-infra=elasticsearch
$ oc delete all --selector logging-infra=curator
$ oc delete all,sa,oauthclient --selector logging-infra=support
$ oc delete secret logging-fluentd logging-elasticsearch \
    logging-es-proxy logging-kibana logging-kibana-proxy \
    logging-kibana-ops-proxy
----


== Upgrading the EFK Stack

###  OpenShift Origin 1.1 to 1.2 / OpenShift Enterprise 3.1 to 3.2


If you need to upgrade your EFK stack with new images, you'll need to take the
following steps to safely upgrade with minimal disruption to your log data.

Scale down your Fluentd instances to 0.

    $ oc scale dc/logging-fluentd --replicas=0

Wait until they have properly terminated, this gives them time to properly
flush their current buffer and send any logs they were processing to
Elasticsearch. This helps prevent loss of data.

You can scale down your Kibana instances at this time as well.

    $ oc scale dc/logging-kibana --replicas=0
    $ oc scale dc/logging-kibana-ops --replicas=0 (if applicable)

Once your Fluentd and Kibana pods are confirmed to be terminated we can safely
scale down the Elasticsearch pods.

    $ oc scale dc/logging-es-{unique_name} --replicas=0
    $ oc scale dc/logging-es-ops-{unique_name} --replicas=0 (if applicable)

Once your ES pods are confirmed to be terminated we can now pull in the latest
EFK images to use as described link:../install_config/upgrading/manual_upgrades.html#importing-the-latest-images[here],
replacing the default namespace with the namespace where logging was installed.

With the latest images in your repository we can now rerun the deployer to generate
any missing or changed features.

Be sure to delete your oauth client

    $ oc delete oauthclient --selector logging-infra=support

Then proceed to follow the same steps as done previously for using the deployer.
After the deployer completes, re-attach your persistent volumes you were using
previously.  Next, we want to scale ES back up incrementally so that the cluster
has time to rebuild.

    $ oc scale dc/logging-es-{unique_name} --replicas=1

We want to tail the logs of the resulting pod to ensure that it was able to recover
its indices correctly and that there were no errors.  If that is successful, we
can then do the same for the operations cluster if one was previously used.

Once all ES nodes have recovered their indices, we can then scale it back up to
the size it was prior to doing maintenance. It is recommended to check the logs
of the ES members to verify that they have correctly joined the cluster and
recovered.

We can now scale Kibana and Fluentd back up to their previous state.  Since Fluentd
was shut down and allowed to push its remaining records to ES in the previous
steps it can now pick back up from where it left off with no loss of logs -- so long
as the log files that were not read in are still available on the node.

With this latest version, Kibana will display indices differently now in order
to prevent users from being able to access the logs of previously created
projects that have been deleted.

Due to this change your old logs will not appear automatically. To migrate your
old indices to the new format, rerun the deployer with `-v MODE=migrate` in addition
to your prior flags. This should be run while your ES cluster is running as the
script will need to connect to it to make changes.
Note: This only impacts non-operations logs, operations logs will appear the
same as in previous versions. There should be minimal performance impact to ES
while running this and it will not perform an install.

== Troubleshooting Kibana

Using the Kibana console with OpenShift can cause problems that are easily
solved, but are not accompanied with useful error messages. Check the following
troubleshooting sections if you are experiencing any problems when deploying
Kibana on OpenShift:

*Login Loop*

The OAuth2 proxy on the Kibana console must share a secret with the master
host's OAuth2 server. If the secret is not identical on both servers, it can
cause a login loop where you are continuously redirected back to the Kibana
login page.

To fix this issue, delete the current oauthclient, and create a new one, using the
same template as before:

====
----
$ oc delete oauthclient/kibana-proxy
$ oc process logging-support-template | oc create -f -
----
====

*Cryptic Error When Viewing the Console*

When attempting to visit the Kibana console, you may instead receive a browser
error:

====
----
{"error":"invalid_request","error_description":"The request is missing a required parameter,
 includes an invalid parameter value, includes a parameter more than once, or is otherwise malformed."}
----
====

This can be caused by a mismatch between the OAuth2 client and server. The
return address for the client must be in a whitelist so the server can securely
redirect back after logging in.

Fix this issue by replacing the OAuth client entry:

====
----
$ oc delete oauthclient/kibana-proxy
$ oc process logging-support-template | oc create -f -
----
====

If the problem persists, check that you are accessing Kibana at a URL listed in
the OAuth client. This issue can be caused by accessing the URL at a forwarded
port, such as 1443 instead of the standard 443 HTTPS port. You can adjust the
server whitelist by editing the OAuth client:

====
----
$ oc edit oauthclient/kibana-proxy
----
====

*503 Error When Viewing the Console*

If you receive a proxy error when viewing the Kibana console, it could be caused
by one of two issues.

First, Kibana may not be recognizing pods. If Elasticsearch is slow in starting
up, Kibana may timeout trying to reach it. Check whether the relevant service
has any endpoints:

====
----
$ oc describe service logging-kibana
Name:                   logging-kibana
[...]
Endpoints:              <none>
----
====

If any Kibana pods are live, endpoints will be listed. If they are not, check
the state of the Kibana pods and deployment. You may need to scale the
deployment down and back up again.

The second possible issue may be caused if the route for accessing the Kibana
service is masked. This can happen if you perform a test deployment in one
project, then deploy in a different project without completely removing the
first deployment. When multiple routes are sent to the same destination, the
default router will only route to the first created. Check the problematic route
to see if it is defined in multiple places:

====
----
$ oc get route  --all-namespaces --selector logging-infra=support
----
====
