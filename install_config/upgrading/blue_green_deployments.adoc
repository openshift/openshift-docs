[[upgrading-blue-green-deployments]]
= Blue-Green Deployments
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

[IMPORTANT]
====
An etcd performance issue has been discovered on new and upgraded
{product-title} 3.4 clusters. See the following Knowledgebase Solution for
further details:

https://access.redhat.com/solutions/2916381[] +
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1415839[*BZ#1415839*])
====

[NOTE]
====
This topic serves as an alternative approach for host upgrades to the in-place
upgrade method.
====

The
xref:../../install_config/upgrading/blue_green_deployments.adoc#upgrading-blue-green-deployments[blue-green deployment] upgrade method follows a similar flow to the in-place method:
masters and etcd servers are still upgraded first, however a parallel
environment is created for new hosts instead of upgrading them in-place.

This method allows administrators to switch traffic from the old set of hosts
(e.g., the "blue" deployment) to the new set (e.g., the "green" deployment)
after the new deployment has been verified. If a problem is detected, it is also
then easy to rollback to the old deployment quickly.

While blue-green is a proven and valid strategy for deploying just about any
software, there are always trade-offs. Not all environments have the same uptime
requirements or the resources to properly perform blue-green deployments. In an
{product-title} environment, the most suitable candidate for blue-green
deployments are the Openshift node hosts. All user processes run on these systems and even
critical pieces of {product-title} infrastructure are self-hosted on these resources.
Uptime is most important for these workloads and the additional complexity of
blue-green deployments can be justified.

The exact implementation of this approach varies based on your requirements.
Often the main challenge is having the excess capacity to facilitate such an
approach.

ifdef::openshift-enterprise[]
Another lesser challenge is that the administrator must temporarily share the
Red Hat software entitlements between the blue-green deployments or provide
access to the installation content by means of a system such as Red Hat
Satellite. This can be accomplished by sharing the consumer ID from the previous
host.
endif::openshift-enterprise[]

[[blue-green-deployments-preparing-for-upgrade]]
== Preparing for Upgrade

ifdef::openshift-enterprise[]
. On the old host:
+
----
# subscription-manager identity | grep system
system identity: 6699375b-06db-48c4-941e-689efd6ce3aa
----

. On the new host:
+
----
# subscription-manager register --consumerid=6699375b-06db-48c4-941e-689efd6ce3aa
----
+
[IMPORTANT]
====
After a successful deployment, remember to unregister the old host with
`subscription-manager clean` to prevent the environment from being out of
compliance.
====
endif::openshift-enterprise[]

. xref:../../install_config/upgrading/manual_upgrades.adoc#upgrading-masters[After
the master and etcd servers have been upgraded], you must ensure that your
current hosts are labeled either blue or green. In this example, the
current installation will be blue and the new environment will be green. Ensure
that all hosts have appropriate labels.  In some deployments, masters might also
be running as Openshift nodes.  Additional labels will assist in management of clusters,
specifically labeling hosts as their types(eg. type=master or type=node) should assist
management. Ensure labels on Openshift hosts are set in your current installation:
+
----
$ oc label node -l type=node  color=blue
----
+
In the case of hosts requiring the uptime guarantees of a blue-green deployment,
the `-l` flag can be used to match a subset of the environment using a selector.

. Create the new green environment for any hosts that are to be replaced by
xref:../../install_config/adding_hosts_to_existing_cluster.adoc#adding-nodes-advanced[adding an equal
number of new nodes] to the existing cluster. Ansible can apply the
`*color=green*` label using the `*openshift_node_labels*` variable for each
node.

. In order to delay workload scheduling until the nodes are
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[healthy],
be sure to set the `*openshift_schedulable=false*` variable. After the green
nodes are in *Ready* state, they can be made schedulable.

. Blue nodes are disabled so that no new pods are run on them:
+
----
# oadm manage-node --schedulable=true --selector=color=green
# oadm manage-node --schedulable=false --selector=color=blue
----

A common practice is to scale the registry and router pods until they are
migrated to the new or green Openshift nodes. For these pods, a _canary_ deployment approach is
commonly used. Scaling them up will make them immediately active on the new
nodes. Pointing the deployment configuration to the new image initiates a
rolling update. However, because of node anti-affinity, and the fact that the
blue nodes are still unschedulable, the deployments to the old nodes will fail.
At this point, the registry and router deployments can be scaled down to the
original number of pods. At any given point, the original number of pods is
still available so no capacity is lost and downtime should be avoided.

[[blue-green-deployments-new-node-verification]]
== New Node Verification

Before proceeding with Openshift node preparation, verify that your new hosts are in a healthy
state. Peform the following checklist:

- Verify that new hosts show up inside of Openshift and are ready.
----
oc get nodes
ip-172-31-49-10.ec2.internal    Ready                      3d
----
- Verify that the Openshift node has proper labels.
----
oc get nodes -o wide --show-labels
ip-172-31-49-10.ec2.internal    Ready                      4d        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.large,beta.kubernetes.io/os=linux,color=green,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1c,hostname=openshift-cluster-1d005,kubernetes.io/hostname=ip-172-31-49-10.ec2.internal,region=us-east-1,type=infra
----
- Perform a diagnostic check
----
oadm diagnostics
[Note] Determining if client configuration exists for client/cluster diagnostics
Info:  Successfully read a client config file at '/root/.kube/config'
Info:  Using context for cluster-admin access: 'default/internal-api-upgradetest-openshift-com:443/system:admin'
[Note] Performing systemd discovery

[Note] Running diagnostic: ConfigContexts[default/api-upgradetest-openshift-com:443/system:admin]
       Description: Validate client config context is complete and has connectivity
...
         [Note] Running diagnostic: CheckExternalNetwork
              Description: Check that external network is accessible within a pod

       [Note] Running diagnostic: CheckNodeNetwork
              Description: Check that pods in the cluster can access its own node.

       [Note] Running diagnostic: CheckPodNetwork
              Description: Check pod to pod communication in the cluster. In case of ovs-subnet network plugin, all pods
should be able to communicate with each other and in case of multitenant network plugin, pods in non-global projects
should be isolated and pods in global projects should be able to access any pod in the cluster and vice versa.

       [Note] Running diagnostic: CheckServiceNetwork
              Description: Check pod to service communication in the cluster. In case of ovs-subnet network plugin, all
pods should be able to communicate with all services and in case of multitenant network plugin, services in non-global
projects should be isolated and pods in global projects should be able to access any service in the cluster.
...
----

[[blue-green-deployments-warming-the-new-nodes]]
== Warming the New Nodes

In order for pods to be migrated from the blue environment to the green, the
images must be pulled. Network latency and load on the registry can cause delays
if there is not sufficient capacity built in to the environment. Often, the best
way to minimize impact to the running system is to trigger new pod deployments that
will land on the new nodes. Accomplish this by importing new image streams.

A major release of {product-title} is the motivation for a blue-green
deployment. At that time, new image streams become available for users of
Source-to-Image (S2I). Upon import, any builds or deployments configured with
*ImageChangeTriggers* are automatically created.

[NOTE]
====
To continue with the upgrade process,
xref:../../install_config/upgrading/manual_upgrades.adoc#updating-the-default-image-streams-and-templates[update
the default image streams and templates] and
xref:../../install_config/upgrading/manual_upgrades.adoc#importing-the-latest-images[import
the latest images].
====

It is important to realize that this process can trigger a large number of
builds. The good news is that the builds are performed on the green nodes and,
therefore, do not impact any traffic on the blue deployment.

To monitor build progress across all namespaces (projects) in the cluster:

----
$ oc get events -w --all-namespaces
----

In large environments, builds rarely completely stop. However, you should see a
large increase and decrease caused by the administrative import.

Another benefit of triggering the builds is that it does a fairly good job of
fetching the majority of the ancillary images to all Openshift nodes such as the various
build images, the pod infrastructure image, and deployers. Everything else can
be moved over using Openshift node evacuation and will proceed more quickly as a result.

[[blue-green-deployments-node-evacuation]]
== Node Evacuation and Decommission

For larger deployments, it is possible to have other labels that help
determine how evacuation can be coordinated. The most conservative approach
for avoiding downtime is to evacuate one Openshift node at a time. If services are
composed of pods using zone anti-affinity, then an entire zone can be
evacuated at once. It is important to ensure that the storage volumes used are
available in the new zone as this detail can vary among cloud providers.

ifdef::openshift-origin[]
In OpenShift Origin 1.2 and later,
endif::[]
ifdef::openshift-enterprise[]
In OpenShift Enterprise 3.2 and later,
endif::[]
an Openshift node evacuation is triggered whenever the service is stopped. Openshift node
labeling is very important and can cause issues if nodes are mislabled or commands
are run on nodes with generalized labels. Exercise caution if Openshift master
hosts are also labeled with 'color=blue'.  Achieve manual evacuation and deletion
of all blue nodes at once by:

----
# oadm manage-node --selector=color=blue --evacuate
# oc delete node --selector=color=blue
----
Or, filter out masters before running the delete commands. Verify the list of
old or blue Openshift nodes are as expected by running:
----
oc get nodes -o go-template='{{ range .items }}{{ if and (eq .metadata.labels.foo "bar") (ne .metadata.labels.type "master") }}{{ .metadata.name }}{{ "\n" }}{{end}}{{ end }}');
----
Once the list is determined to be the blue or old Openshift nodes, run:
----
for i in $(oc get nodes -o go-template='{{ range .items }}{{ if and (eq .metadata.labels.color "blue") (ne .metadata.labels.type "master") }}{{ .metadata.name }}{{ "\n" }}{{end}}{{ end }}');
do
    oc delete node $i
done
----

Once these hosts no longer contain pods and have been removed from Openshift
they are safe to power off. As a safety precaution, leaving the hosts around
for a short period of time can prove beneficial if the upgrade has issues. Ensure
that any desired scripts or files are captured before terminating these hosts.
After a determined time period and capacity is not an issue, remove these hosts.
