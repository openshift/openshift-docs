[[install-config-upgrading-automated-upgrades]]
= Performing Automated In-place Cluster Upgrades
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

If you installed using the
xref:../../install_config/install/advanced_install.adoc#install-config-install-advanced-install[advanced installation]
and the inventory file that was used is available, you can use the upgrade
playbook to automate the OpenShift cluster upgrade process.
ifdef::openshift-enterprise[]
If you installed using the
xref:../../install_config/install/quick_install.adoc#install-config-install-quick-install[quick installation] method
and a *_~/.config/openshift/installer.cfg.yml_* file is available, you can use
the installer to perform the automated upgrade.
endif::[]

The automated upgrade performs the following steps for you:

* Applies the latest configuration.
* Upgrades and restart master services.
* Upgrades and restart node services.
* Applies the latest cluster policies.
* Updates the default router if one exists.
* Updates the default registry if one exists.
* Updates default image streams and InstantApp templates.

[IMPORTANT]
====
Ensure that you have met all
xref:../../install_config/install/prerequisites.adoc#install-config-install-prerequisites[prerequisites]
before proceeding with an upgrade. Failure to do so can result in a failed
upgrade.
====

ifdef::openshift-origin[]
[[running-upgrade-playbooks]]
== Running Upgrade Playbooks

Ensure that you have the latest *openshift-ansible* code checked out:

----
# cd ~/openshift-ansible
# git pull https://github.com/openshift/openshift-ansible master
----

Then run one of the following upgrade playbooks utilizing the inventory file you
used during the advanced installation. If your inventory file is located
somewhere other than the default *_/etc/ansible/hosts_*, add the `-i` flag to
specify the location.

[[upgrading-to-openshift-origin-1-1]]
=== Upgrading to OpenShift Origin 1.1

To upgrade from OpenShift Origin 1.0 to 1.1, run the following playbook:

----
# ansible-playbook \
    [-i </path/to/inventory/file>] \
    playbooks/byo/openshift-cluster/upgrades/v3_0_to_v3_1/upgrade.yml
----

[NOTE]
====
The *_v3_0_to_v3_1_* in the above path is a reference to the related OpenShift
Enterprise versions, however it is also the correct playbook to use when
upgrading from OpenShift Origin 1.0 to 1.1.
====

When the upgrade finishes, a recommendation will be printed to reboot all hosts.
After rebooting, continue to Updating Master and Node
Certificates.

[[upgrading-to-openshift-origin-1-1-z-releases]]
=== Upgrading to OpenShift Origin 1.1.z Releases

To upgrade an existing OpenShift Origin 1.1 cluster to the latest 1.1.z release,
run the following playbook:

----
# ansible-playbook \
    [-i </path/to/inventory/file>] \
    playbooks/byo/openshift-cluster/upgrades/v3_1_minor/upgrade.yml
----

[NOTE]
====
The *v3_1_minor* in the above path is a reference to the related OpenShift
Enterprise versions, however it is also the correct playbook to use when
upgrading from OpenShift Origin 1.1 to the latest 1.1.z release.
====

When the upgrade finishes, a recommendation will be printed to reboot all hosts.
After rebooting, continue to xref:verifying-the-upgrade[Verifying the Upgrade].
endif::[]

ifdef::openshift-enterprise[]
[[preparing-for-an-automated-upgrade]]
== Preparing for an Automated Upgrade

[NOTE]
====
Before upgrading your cluster to {product-title} 3.3, the cluster must be
already upgraded to the
link:https://docs.openshift.com/enterprise/3.2/release_notes/ose_3_2_release_notes.html#ose-32-asynchronous-errata-updates[latest asynchronous release of version 3.2]. Cluster upgrades cannot span more than one
minor version at a time, so if your cluster is at version 3.0 or 3.1, you must
first upgrade incrementally (e.g., 3.0 to 3.1, then 3.1 or 3.2).
====

If you are upgrading from version 3.2 to 3.3, on each master and node host you
must manually disable the 3.2 channel and enable the 3.3 channel:

====
----
# subscription-manager repos --disable="rhel-7-server-ose-3.2-rpms" \
    --enable="rhel-7-server-ose-3.3-rpms"\
    --enable="rhel-7-server-extras-rpms"
# yum clean all
----
====

For any upgrade path, always ensure that you have the latest version of the
*atomic-openshift-utils* package, which should also update the
*openshift-ansible-** packages:

----
# yum update atomic-openshift-utils
----

Lastly, you must be logged in as a cluster administrative user on the master
host for the upgrade to succeed:
----
$ oc login
----

There are two methods for running the automated upgrade:
xref:upgrading-using-the-installation-utility-to-upgrade[using the installer]
or xref:running-the-upgrade-playbook-directly[running the upgrade playbook
directly]. Choose and follow one method.

[[upgrading-using-the-installation-utility-to-upgrade]]
== Using the Installer to Upgrade

If you installed {product-title} using the
xref:../../install_config/install/quick_install.adoc#install-config-install-quick-install[quick installation] method,
you should have an installation configuration file located at
*_~/.config/openshift/installer.cfg.yml_*. The installer requires this file to
start an upgrade.

The installer supports upgrading between minor versions of {product-title}
(one minor version at a time, e.g., 3.2 to 3.3) as well as between
xref:../../release_notes/index.adoc#release-notes-index[asynchronous errata updates] within a minor version (e.g., 3.3.z).

If you have an older format installation configuration file in
*_~/.config/openshift/installer.cfg.yml_* from an installation of a previous
cluster version, the installer will attempt to upgrade the file to the new supported
format. If you do not have an installation configuration file of any format, you
can
xref:../../install_config/install/quick_install.adoc#defining-an-installation-configuration-file[create one manually].

To start the upgrade, run the installer with the `upgrade` subcommand:

----
# atomic-openshift-installer upgrade
----

Then, follow the on-screen instructions to upgrade to the latest release.

// tag::automated_upgrade_after_reboot[]
When the upgrade finishes, a recommendation will be printed to reboot all hosts.
After rebooting, if there are no features enabled, you can
xref:verifying-the-upgrade[verify the upgrade]. Otherwise, the next step depends
on what features are enabled.

[cols="1,4"]
|===
| Feature | Next Step

| Aggregated Logging
| xref:automated-upgrading-efk-logging-stack[Upgrade the EFK logging stack.]

| Cluster Metrics
| xref:automated-upgrading-cluster-metrics[Upgrade cluster metrics.]
|===
// end::automated_upgrade_after_reboot[]

[[running-the-upgrade-playbook-directly]]
== Running the Upgrade Playbook Directly

Alternatively, you can run the upgrade playbook with Ansible directly, similar
to the advanced installation method, if you have an inventory file.

[[upgrading-to-openshift-enterprise-3-3]]
=== Upgrading to OpenShift Enterprise 3.3

Before running the upgrade, first ensure the `*deployment_type*` parameter in
your inventory file is set to `openshift-enterprise`.

If you have multiple masters configured and want to enable rolling, full system
restarts of the hosts, you can set the `*openshift_rolling_restart_mode*`
parameter in your inventory file to `system`. Otherwise, the default value
*services* performs rolling service restarts on HA masters, but does not reboot
the systems. See
xref:../install/advanced_install.adoc#configuring-cluster-variables[Configuring
Cluster Variables] for details.

Then, run the *_v3_3_* upgrade playbook. If your inventory file is
located somewhere other than the default *_/etc/ansible/hosts_*, add the `-i`
flag to specify the location. If you previously used the
`atomic-openshift-installer` command to run your installation, you can check
*_~/.config/openshift/.ansible/hosts_* for the last inventory file that was
used, if needed.

----
# ansible-playbook [-i </path/to/inventory/file>] \
    /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/upgrades/v3_3/upgrade.yml
----

include::install_config/upgrading/automated_upgrades.adoc[tag=automated_upgrade_after_reboot]

[[upgrading-to-openshift-enterprise-3-3-asynchronous-releases]]
=== Upgrading to OpenShift Enterprise 3.3 Asynchronous Releases

To apply
xref:../../release_notes/index.adoc#release-notes-index[asynchronous errata updates] to an existing {product-title} 3.3 cluster, first upgrade the
*atomic-openshift-utils* package on the Red Hat Enterprise Linux 7 system where
you will be running Ansible:

----
# yum update atomic-openshift-utils
----

Then, run the same *_v3_3_* upgrade playbook that is used for
xref:upgrading-to-openshift-enterprise-3-3[upgrading to {product-title} 3.3 from
3.2]. If your inventory file is located somewhere other than the default
*_/etc/ansible/hosts_*, add the `-i` flag to specify the location. If you
previously used the `atomic-openshift-installer` command to run your
installation, you can check *_~/.config/openshift/.ansible/hosts_* for the last
inventory file that was used, if needed.

----
# ansible-playbook [-i </path/to/inventory/file>] \
    /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/upgrades/v3_3/upgrade.yml
----

include::install_config/upgrading/automated_upgrades.adoc[tag=automated_upgrade_after_reboot]
endif::[]

ifdef::openshift-origin[]
:sect: automated
include::install_config/upgrading/manual_upgrades.adoc[tag=30to31updatingcerts]
endif::[]

[[automated-upgrading-efk-logging-stack]]
== Upgrading the EFK Logging Stack

If you have previously xref:../../install_config/aggregate_logging.adoc#install-config-aggregate-logging[deployed
the EFK logging stack] and want to upgrade to the latest logging component
images, the steps must be performed manually as shown in
xref:../../install_config/upgrading/manual_upgrades.adoc#manual-upgrading-efk-logging-stack[Manual
Upgrades].

[[automated-upgrading-cluster-metrics]]
== Upgrading Cluster Metrics

If you have previously
xref:../../install_config/cluster_metrics.adoc#install-config-cluster-metrics[deployed cluster metrics],
you must manually
xref:../../install_config/upgrading/manual_upgrades.adoc#manual-upgrading-cluster-metrics[update]
to the latest metric components.

[[verifying-the-upgrade]]
== Verifying the Upgrade

To verify the upgrade, first check that all nodes are marked as *Ready*:

====
----
# oc get nodes
NAME                        STATUS                     AGE
master.example.com          Ready,SchedulingDisabled   165d
node1.example.com           Ready                      165d
node2.example.com           Ready                      165d
----
====

Then, verify that you are running the expected versions of the *docker-registry*
and *router* images, if deployed:

====
----
ifdef::openshift-enterprise[]
# oc get -n default dc/docker-registry -o json | grep \"image\"
    "image": "openshift3/ose-docker-registry:v3.3.0.31",
# oc get -n default dc/router -o json | grep \"image\"
    "image": "openshift3/ose-haproxy-router:v3.3.0.31",
endif::[]
ifdef::openshift-origin[]
# oc get -n default dc/docker-registry -o json | grep \"image\"
    "image": "openshift/origin-docker-registry:v1.0.6",
# oc get -n default dc/router -o json | grep \"image\"
    "image": "openshift/origin-haproxy-router:v1.0.6",
endif::[]
----
====

ifdef::openshift-origin[]
If you upgraded from Origin 1.0 to Origin 1.1, verify in your old
*_/etc/sysconfig/openshift-master_* and *_/etc/sysconfig/openshift-node_* files
that any custom configuration is added to your new
*_/etc/sysconfig/origin-master_* and *_/etc/sysconfig/origin-node_* files.
endif::[]

After upgrading, you can use the diagnostics tool on the master to look for
common issues:

====
----
# oadm diagnostics
...
[Note] Summary of diagnostics execution:
[Note] Completed with no errors or warnings seen.
----
====
