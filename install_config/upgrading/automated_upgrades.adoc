[[install-config-upgrading-automated-upgrades]]
= Performing Automated Cluster Upgrades
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

ifdef::openshift-enterprise[]
Starting with OpenShift 3.0.2,
endif::[]
ifdef::openshift-origin[]
Starting with Origin 1.0.6,
endif::[]
if you installed using the
xref:../../install_config/install/advanced_install.adoc#install-config-install-advanced-install[advanced installation]
and the inventory file that was used is available, you can use the upgrade
playbook to automate the OpenShift cluster upgrade process.
ifdef::openshift-enterprise[]
If you installed using the
xref:../../install_config/install/quick_install.adoc#install-config-install-quick-install[quick installation] method
and a *_~/.config/openshift/installer.cfg.yml_* file is available, you can use
the installer to perform the automated upgrade.
endif::[]

The automated upgrade performs the following steps for you:

* Applies the latest configuration.
* Upgrades and restart master services.
* Upgrades and restart node services.
* Applies the latest cluster policies.
* Updates the default router if one exists.
* Updates the default registry if one exists.
* Updates default image streams and InstantApp templates.

ifdef::openshift-origin[]
[[running-upgrade-playbooks]]
== Running Upgrade Playbooks

Ensure that you have the latest *openshift-ansible* code checked out:

----
# cd ~/openshift-ansible
# git pull https://github.com/openshift/openshift-ansible master
----

Then run one of the following upgrade playbooks utilizing the inventory file you
used during the advanced installation. If your inventory file is located
somewhere other than the default *_/etc/ansible/hosts_*, add the `-i` flag to
specify the location.

[[upgrading-to-openshift-origin-1-1]]
=== Upgrading to OpenShift Origin 1.1

To upgrade from OpenShift Origin 1.0 to 1.1, run the following playbook:

----
# ansible-playbook \
    [-i </path/to/inventory/file>] \
    playbooks/byo/openshift-cluster/upgrades/v3_0_to_v3_1/upgrade.yml
----

[NOTE]
====
The *_v3_0_to_v3_1_* in the above path is a reference to the related OpenShift
Enterprise versions, however it is also the correct playbook to use when
upgrading from OpenShift Origin 1.0 to 1.1.
====

When the upgrade finishes, a recommendation will be printed to reboot all hosts.
After rebooting, continue to Updating Master and Node
Certificates.

[[upgrading-to-openshift-origin-1-1-z-releases]]
=== Upgrading to OpenShift Origin 1.1.z Releases

To upgrade an existing OpenShift Origin 1.1 cluster to the latest 1.1.z release,
run the following playbook:

----
# ansible-playbook \
    [-i </path/to/inventory/file>] \
    playbooks/byo/openshift-cluster/upgrades/v3_1_minor/upgrade.yml
----

[NOTE]
====
The *v3_1_minor* in the above path is a reference to the related OpenShift
Enterprise versions, however it is also the correct playbook to use when
upgrading from OpenShift Origin 1.1 to the latest 1.1.z release.
====

When the upgrade finishes, a recommendation will be printed to reboot all hosts.
After rebooting, continue to xref:verifying-the-upgrade[Verifying the Upgrade].
endif::[]

ifdef::openshift-enterprise[]
[[preparing-for-an-automated-upgrade]]
== Preparing for an Automated Upgrade

[NOTE]
====
If you are on OpenShift Enterprise 3.0, you must first upgrade to 3.1 before
upgrading to 3.2. Further, if you are currently using the Pacemaker HA method,
you must first upgrade to the native HA method before upgrading to 3.2, as the
Pacemaker method is no longer supported starting with 3.2. See the
https://docs.openshift.com/enterprise/3.1/install_config/upgrading/index.html[OpenShift
Enterprise 3.1 upgrade documentation] for instructions.
====

If you are upgrading from OpenShift Enterprise 3.1 to 3.2, on each master and
node host you must manually disable the 3.1 channel and enable the 3.2 channel:

====
----
# subscription-manager repos --disable="rhel-7-server-ose-3.1-rpms" \
    --enable="rhel-7-server-ose-3.2-rpms"\
    --enable="rhel-7-server-extras-rpms"
# yum clean all
----
====

For any upgrade path, always ensure that you have the latest version of the
*atomic-openshift-utils* package, which should also update the
*openshift-ansible-** packages:

----
# yum update atomic-openshift-utils
----

There are two methods for running the automated upgrade:
xref:upgrading-using-the-installation-utility-to-upgrade[using the installer]
or xref:running-the-upgrade-playbook-directly[running the upgrade playbook
directly]. Choose and follow one method.

[[upgrading-using-the-installation-utility-to-upgrade]]
== Using the Installer to Upgrade

If you installed OpenShift using the
xref:../../install_config/install/quick_install.adoc#install-config-install-quick-install[quick installation] method,
you should have an installation configuration file located at
*_~/.config/openshift/installer.cfg.yml_*. The installer requires this file to
start an upgrade.

[NOTE]
====
The installer currently only supports upgrading between minor versions of
OpenShift Enterprise: 3.0 to 3.1, or 3.1 to 3.2. See
xref:upgrading-to-openshift-enterprise-3-2-asynchronous-releases[Upgrading to
OpenShift Enterprise 3.2 Asynchronous Releases] for instructions on using
Ansible directly for upgrades within a minor version.
====

If you have an older format installation configuration file in
*_~/.config/openshift/installer.cfg.yml_* from an existing OpenShift Enterprise
3.0 or 3.1 installation, the installer will attempt to upgrade the file to the
new supported format. If you do not have an installation configuration file of
any format, you can
xref:../../install_config/install/quick_install.adoc#defining-an-installation-configuration-file[create
one manually].

To start the upgrade, run the installer with the `upgrade` subcommand:

----
# atomic-openshift-installer upgrade
----

Then, follow the on-screen instructions to upgrade to the latest release. When
the upgrade finishes, a recommendation will be printed to reboot all hosts.
After rebooting, continue to
xref:automated-upgrading-efk-logging-stack[Upgrading the EFK Logging Stack] if
you have aggregated logging enabled, otherwise proceed to
xref:verifying-the-upgrade[Verifying the Upgrade].

[[running-the-upgrade-playbook-directly]]
== Running the Upgrade Playbook Directly

Alternatively, you can run the upgrade playbook with Ansible directly, similar
to the advanced installation method, if you have an inventory file.

[[upgrading-to-openshift-enterprise-3-2]]
=== Upgrading to OpenShift Enterprise 3.2

Before running the upgrade, first ensure the `*deployment_type*` parameter in
your inventory file is set to `openshift-enterprise`.

If you have multiple masters configured and want to enable rolling, full system
restarts of the hosts, you can set the `*openshift_rolling_restart_mode*`
parameter in your inventory file to `system`. Otherwise, the default value
*services* performs rolling service restarts on HA masters, but does not reboot
the systems. See
xref:../install/advanced_install.adoc#configuring-cluster-variables[Configuring
Cluster Variables] for details.

Then, run the *_v3_1_to_v3_2_* upgrade playbook. If your inventory file is
located somewhere other than the default *_/etc/ansible/hosts_*, add the `-i`
flag to specify the location. If you previously used the
`atomic-openshift-installer` command to run your installation, you can check
*_~/.config/openshift/.ansible/hosts_* for the last inventory file that was
used, if needed.

----
# ansible-playbook [-i </path/to/inventory/file>] \
    /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/upgrades/v3_1_to_v3_2/upgrade.yml
----

When the upgrade finishes, a recommendation will be printed to reboot all hosts.
After rebooting, if there are no features enabled, you can
xref:verifying-the-upgrade[verify the upgrade].
Otherwise, the next step depends on what features are enabled.

[cols="1,4"]
|===
| Feature | Next Step

| Aggregated Logging
| xref:automated-upgrading-efk-logging-stack[Upgrade the EFK logging stack.]

| Cluster Metrics
| xref:automated-upgrading-cluster-metrics[Upgrade cluster metrics.]
|===

[[upgrading-to-openshift-enterprise-3-2-asynchronous-releases]]
=== Upgrading to OpenShift Enterprise 3.2 Asynchronous Releases

Currently, you must use the
xref:../../install_config/upgrading/manual_upgrades.adoc#install-config-upgrading-manual-upgrades[manual cluster upgrade
steps] to apply
xref:../../release_notes/ose_3_2_release_notes.adoc#ose-32-asynchronous-errata-updates[asynchronous
errata updates] released for OpenShift Enterprise 3.2, such as 3.2.1.4. An
automated playbook is in development, and this section will be updated with
instructions when it is available.

However, if you are upgrading from {product-title} 3.1, you can use the
*_v3_1_to_v3_2_* upgrade playbook as described in
xref:upgrading-to-openshift-enterprise-3-2[Upgrading to OpenShift Enterprise
3.2] to upgrade all the way to the latest asynchronous release at once.
endif::[]

ifdef::openshift-origin[]
:sect: automated
include::install_config/upgrading/manual_upgrades.adoc[tag=30to31updatingcerts]
endif::[]

[[automated-upgrading-efk-logging-stack]]
== Upgrading the EFK Logging Stack

If you have previously xref:../../install_config/aggregate_logging.adoc#install-config-aggregate-logging[deployed
the EFK logging stack] and want to upgrade to the latest logging component
images, the steps must be performed manually as shown in
xref:../../install_config/upgrading/manual_upgrades.adoc#manual-upgrading-efk-logging-stack[Manual
Upgrades].

[[automated-upgrading-cluster-metrics]]
== Upgrading Cluster Metrics

If you have previously
xref:../../install_config/cluster_metrics.adoc#install-config-cluster-metrics[deployed cluster metrics],
you must manually
xref:../../install_config/upgrading/manual_upgrades.adoc#manual-upgrading-cluster-metrics[update]
to the latest metric components.

[[verifying-the-upgrade]]
== Verifying the Upgrade

To verify the upgrade, first check that all nodes are marked as *Ready*:

====
----
# oc get nodes
NAME                 LABELS                                                                STATUS
master.example.com   kubernetes.io/hostname=master.example.com,region=infra,zone=default   Ready
node1.example.com    kubernetes.io/hostname=node1.example.com,region=primary,zone=east     Ready
----
====

Then, verify that you are running the expected versions of the *docker-registry*
and *router* images, if deployed:

====
----
ifdef::openshift-enterprise[]
# oc get -n default dc/docker-registry -o json | grep \"image\"
    "image": "openshift3/ose-docker-registry:v3.2.1.4",
# oc get -n default dc/router -o json | grep \"image\"
    "image": "openshift3/ose-haproxy-router:v3.2.1.4",
endif::[]
ifdef::openshift-origin[]
# oc get -n default dc/docker-registry -o json | grep \"image\"
    "image": "openshift/origin-docker-registry:v1.0.6",
# oc get -n default dc/router -o json | grep \"image\"
    "image": "openshift/origin-haproxy-router:v1.0.6",
endif::[]
----
====

ifdef::openshift-origin[]
If you upgraded from Origin 1.0 to Origin 1.1, verify in your old
*_/etc/sysconfig/openshift-master_* and *_/etc/sysconfig/openshift-node_* files
that any custom configuration is added to your new
*_/etc/sysconfig/origin-master_* and *_/etc/sysconfig/origin-node_* files.
endif::[]

After upgrading, you can use the diagnostics tool on the master to look for
common issues:

====
----
# oadm diagnostics
...
[Note] Summary of diagnostics execution:
[Note] Completed with no errors or warnings seen.
----
====
