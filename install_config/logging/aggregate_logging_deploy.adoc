[[install-config-aggregate-logging-configure]]
= Deploying Aggregated Logging
{product-author}
{product-version}
ifdef::openshift-enterprise[]
:latest-tag: v3.10.14
endif::[]
ifdef::openshift-origin[]
:latest-tag: v3.10.14
endif::[]
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]
{nbsp} +

[[aggregate-logging-pre-deployment-configuration]]
== Pre-deployment Configuration

. An Ansible playbook is available to deploy and upgrade aggregated logging. You
should familiarize yourself with the
xref:../..install/index.adoc#install-planning[Installing Clusters] guide. This
provides information for preparing to use Ansible and includes information about
configuration. Parameters are added to the Ansible inventory file to configure
various areas of the EFK stack.
. Review the xref:../..install_config/logging/aggregate_logging_sizing.adoc#install-config-aggregate-logging-sizing[sizing guidelines]
to determine how best to configure your deployment.
. Ensure that you have deployed a router for the cluster.
. Ensure that you have
xref:../..install_config/persistent_storage/index.adoc#install-config-persistent-storage-index[the
necessary storage] for Elasticsearch. Note that each Elasticsearch replica
requires its own storage volume. See
xref:aggregated-elasticsearch[Elasticsearch] for more information.
. Choose a project. Once deployed, the EFK stack collects logs for every
project within your {product-title} cluster. The examples in this section use the
default project *logging*. The Ansible playbook creates the project for you
if it does not already exist. You will only need to create a project if you want
to specify a node-selector on it. Otherwise, the `openshift-logging` role will
create a project.
+
----
$ oc adm new-project logging --node-selector=""
$ oc project logging
----
+
[NOTE]
====
Specifying an empty
xref:../..admin_guide/managing_projects.adoc#using-node-selectors[node
selector] on the project is recommended, as Fluentd should be deployed
throughout the cluster and any selector would restrict where it is
deployed. To control component placement, specify node selectors per component to
be applied to their deployment configurations.
====

[[aggregate-logging-ansible-variables]]
== Specifying Logging Ansible Variables

Parameters for the EFK deployment may be specified to the
 xref:../..install/configuring_inventory_file.adoc#configuring-ansible[inventory host file]
to override the
 https://github.com/openshift/openshift-ansible/blob/master/roles/openshift_logging/defaults/main.yml[defaults]. Read the
xref:aggregated-elasticsearch[Elasticsearch]
and the xref:aggregated-fluentd[Fluentd] sections
before choosing parameters:

[NOTE]
====
By default the Elasticsearch service uses port 9300 for TCP communication
between nodes in a cluster.
====

[cols="3,7",options="header"]
|===
|Parameter
|Description

|`openshift_logging_use_ops`
|If set to `true`, configures a second Elasticsearch cluster and Kibana for
operations logs. Fluentd splits logs between the main cluster and a cluster
reserved for operations logs, which consists of the logs from the projects
*default*, *openshift*, and *openshift-infra*, as well as Docker, OpenShift, and
system logs from the journal. This means a second Elasticsearch cluster and
Kibana are deployed. The deployments are distinguishable by the *-ops* suffix
included in their names and have parallel deployment options listed below and
described in
xref:../..install_config/logging/aggregate_logging_understanding.adoc#aggregate-logging-creating-the-curator-configuration[Creating the Curator Configuration].

|`openshift_logging_master_url`
|The URL for the Kubernetes master, this does not need to be public facing but
should be accessible from within the cluster. For example,
https://<PRIVATE-MASTER-URL>:8443.

|`openshift_logging_master_public_url`
|The public facing URL for the Kubernetes master. This is used for Authentication
redirection by the Kibana proxy. For example,
https://<CONSOLE-PUBLIC-URL-MASTER>:8443.

|`openshift_logging`
|The namespace where Aggregated Logging is deployed.

|`openshift_logging_install_logging`
|Set to `true` to install logging. Set to `false` to uninstall logging.

|`openshift_logging_purge_logging`
|The common uninstall keeps PVC to prevent unwanted data loss during
reinstalls. To ensure that the Ansible playbook completely and irreversibly
removes all logging persistent data including PVC, set
`openshift_logging_install_logging` to 'false' to trigger uninstallation and
`openshift_logging_purge_logging` to 'true'. The default is set to 'false'.

|`openshift_logging_install_eventrouter`
|Coupled with `openshift_logging_install_logging`. When both are set to 'true',
*eventrouter* will be installed. When both are 'false', *eventrouter* will be
uninstalled.

|`openshift_logging_eventrouter_image_prefix`
|The prefix for the *eventrouter* logging image.

|`openshift_logging_eventrouter_image_version`
|The image version for the logging *eventrouter*.

|`openshift_logging_eventrouter_sink`
|Select a sink for *eventrouter*, supported `stdout` and `glog`. The default is set
to `stdout`.

|`openshift_logging_eventrouter_nodeselector`
|A map of labels, such as `"node":"infra"`,`"region":"west"`, to select the nodes
where the pod will land.

|`openshift_logging_eventrouter_replicas`
|The default is set to '1'.

|`openshift_logging_eventrouter_cpu_limit`
|The minimum amount of CPU to allocate to *eventrouter*. The default is set to '100m'.

|`openshift_logging_eventrouter_memory_limit`
|The memory limit for *eventrouter* pods. The default is set to '128Mi'.

|`openshift_logging_eventrouter_namespace`
|The namespace where *eventrouter* is deployed. The default is set to 'default'.

|`openshift_logging_image_pull_secret`
|Specify the name of an existing pull
secret to be used for pulling component images from an authenticated registry.

|`openshift_logging_curator_default_days`
|The default minimum age (in days) Curator uses for deleting log records.

|`openshift_logging_curator_run_hour`
|The hour of the day Curator will run.

|`openshift_logging_curator_run_minute`
| The minute of the hour Curator will run.

|`openshift_logging_curator_run_timezone`
|The timezone Curator uses for figuring out its run time. Provide the
timezone as a string in the tzselect(8) or timedatectl(1) "Region/Locality"
format, for example `America/New_York` or `UTC`.

|`openshift_logging_curator_script_log_level`
|The script log level for Curator.

|`openshift_logging_curator_log_level`
|The log level for the Curator process.

|`openshift_logging_curator_cpu_limit`
|The amount of CPU to allocate to Curator.

|`openshift_logging_curator_memory_limit`
|The amount of memory to allocate to Curator.

|`openshift_logging_curator_nodeselector`
|A node selector that specifies
which nodes are eligible targets for deploying Curator instances.

|`openshift_logging_curator_ops_cpu_limit`
|Equivalent to `openshift_logging_curator_cpu_limit` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_curator_ops_memory_limit`
|Equivalent to `openshift_logging_curator_memory_limit` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_kibana_hostname`
|The external host name for web clients to reach Kibana.

|`openshift_logging_kibana_cpu_limit`
|The amount of CPU to allocate to Kibana.

|`openshift_logging_kibana_memory_limit`
|The amount of memory to allocate to Kibana.

|`openshift_logging_kibana_proxy_debug`
|When `true`, set the Kibana Proxy log level to DEBUG.

|`openshift_logging_kibana_proxy_cpu_limit`
|The amount of CPU to allocate to Kibana proxy.

|`openshift_logging_kibana_proxy_memory_limit`
|The amount of memory to allocate to Kibana proxy.

|`openshift_logging_kibana_replica_count`
|The number of to which Kibana should be scaled up.

|`openshift_logging_kibana_nodeselector`
|A node selector that specifies
which nodes are eligible targets for deploying Kibana instances.

|`openshift_logging_kibana_env_vars`
|A map of environment variables to add to the Kibana deployment configuration.
For example, {"ELASTICSEARCH_REQUESTTIMEOUT":"30000"}.

|`openshift_logging_kibana_key`
|The public facing key to use when creating
the Kibana route.

|`openshift_logging_kibana_cert`
|The cert that matches
the key when creating the Kibana route.

|`openshift_logging_kibana_ca`
|Optional. The CA to goes with the key and cert used when creating the Kibana
route.

|`openshift_logging_kibana_ops_hostname`
|Equivalent to `openshift_logging_kibana_hostname` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_kibana_ops_cpu_limit`
|Equivalent to `openshift_logging_kibana_cpu_limit` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_kibana_ops_memory_limit`
|Equivalent to `openshift_logging_kibana_memory_limit` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_kibana_ops_proxy_debug`
|Equivalent to `openshift_logging_kibana_proxy_debug` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_kibana_ops_proxy_cpu_limit`
|Equivalent to `openshift_logging_kibana_proxy_cpu_limit` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_kibana_ops_proxy_memory_limit`
|Equivalent to `openshift_logging_kibana_proxy_memory_limit` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_kibana_ops_replica_count`
|Equivalent to `openshift_logging_kibana_replica_count` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_allow_external`
|Set to `true` to expose Elasticsearch as a reencrypt route. Set to `false` by
default.

|`openshift_logging_es_hostname`
|The external-facing hostname to use for the route and the TLS server
certificate. The default is set to `es`.

For example, if `openshift_master_default_subdomain` is set to `=example.test`,
then the default value of `openshift_logging_es_hostname` will be
`es.example.test`.

|`openshift_logging_es_cert`
|The location of the certificate Elasticsearch uses for the external TLS server
cert. The default is a generated cert.

|`openshift_logging_es_key`
|The location of the key Elasticsearch uses for the external TLS server cert.
The default is a generated key.

|`openshift_logging_es_ca_ext`
|The location of the CA cert Elasticsearch uses for the external TLS
server cert. The default is the internal CA.

|`openshift_logging_es_ops_allow_external`
|Set to `true` to expose Elasticsearch as a reencrypt route. Set to `false` by
defaut.

|`openshift_logging_es_ops_hostname`
|The external-facing hostname to use for the route and the TLS server certificate.
The default is set to `es-ops`.

For example, if `openshift_master_default_subdomain` is set to `=example.test`,
then the default value of `openshift_logging_es_ops_hostname` will be
`es-ops.example.test`.

|`openshift_logging_es_ops_cert`
|The location of the certificate Elasticsearch uses for the external TLS server
cert. The default is a generated cert.

|`openshift_logging_es_ops_key`
|The location of the key Elasticsearch uses for the external TLS server cert.
The default is a generated key.

|`openshift_logging_es_ops_ca_ext`
|The location of the CA cert Elasticsearch uses for the external TLS
server cert. The default is the internal CA.

|`openshift_logging_fluentd_nodeselector`
|A node selector that specifies which nodes are eligible targets
for deploying Fluentd instances.
Any node where Fluentd should run (typically, all) must have this label
before Fluentd is able to run and collect logs.

When scaling up the Aggregated Logging cluster after installation,
the `openshift_logging` role labels nodes provided by
`openshift_logging_fluentd_hosts` with this node selector.

As part of the installation, it is recommended that you add the Fluentd node
selector label to the list of persisted
xref:../..install/configuring_inventory_file.adoc#configuring-node-host-labels[node labels].

|`openshift_logging_fluentd_cpu_limit`
|The CPU limit for Fluentd pods.

|`openshift_logging_fluentd_memory_limit`
|The memory limit for Fluentd pods.

|`openshift_logging_fluentd_journal_read_from_head`
|Set to `true` if Fluentd should read from the head of Journal when first
starting up, using this may cause a delay in ES receiving current log records.

|`openshift_logging_fluentd_hosts`
|List of nodes that should be labeled for Fluentd to be deployed. The default is
to label all nodes with ['--all']. The null value is
`openshift_logging_fluentd_hosts={}`.
To spin up Fluentd pods update the daemonset's `nodeSelector` to a valid label. For
example, ['host1.example.com', 'host2.example.com'].

|`openshift_logging_fluentd_audit_container_engine`
|When `openshift_logging_fluentd_audit_container_engine` is set to `true`, the
audit log of the container engine is collected and stored in ES. Enabling this
variable allows the EFK to watch the specified audit log file or the
default `/var/log/audit.log` file, collects audit information for the container
engine for the platform, then puts it into Kibana.

|`openshift_logging_fluentd_audit_file`
|Location of audit log file. The default is `/var/log/audit/audit.log`. Enabling this
variable allows the EFK to watch the specified audit log file or the
default `/var/log/audit.log` file, collects audit information for the container
engine for the platform, then puts it into Kibana.

|`openshift_logging_fluentd_audit_pos_file`
|Location of the Fluentd `in_tail` position file for the audit log file. The default is
`/var/log/audit/audit.log.pos`. Enabling this
variable allows the EFK to watch the specified audit log file or the
default `/var/log/audit.log` file, collects audit information for the container
engine for the platform, then puts it into Kibana.

|`openshift_logging_es_host`
|The name of the ES service where Fluentd should send logs.

|`openshift_logging_es_port`
|The port for the ES service where Fluentd should send logs.

|`openshift_logging_es_ca`
|The location of the CA Fluentd uses to communicate with `openshift_logging_es_host`.

|`openshift_logging_es_client_cert`
|The location of the client certificate Fluentd uses for `openshift_logging_es_host`.

|`openshift_logging_es_client_key`
|The location of the client key Fluentd uses for `openshift_logging_es_host`.

|`openshift_logging_es_cluster_size`
|Elasticsearch replicas to deploy. Redundancy requires at least three or more.

|`openshift_logging_es_cpu_limit`
|The amount of CPU limit for the ES cluster.

|`openshift_logging_es_memory_limit`
|Amount of RAM to reserve per Elasticsearch instance. It
must be at least 512M. Possible suffixes are G,g,M,m.

|`openshift_logging_es_number_of_replicas`
|The number of replica shards per primary shard for every new index. Defaults to '0'. A minimum of `1` is advisable for production clusters.

|`openshift_logging_es_number_of_shards`
|The number of primary shards for every new index created in ES. Defaults to '1'.

|`openshift_logging_es_pv_selector`
|A key/value map added to a PVC in order to select specific PVs.

|`openshift_logging_es_pvc_dynamic`
|To dynamically provision the backing storage, set the parameter value to `true`.
When set to `true`, the storageClass spec is omitted from the PVC definition.
If you set a `openshift_logging_es_pvc_storage_class_name` parameter value,
its value overrides the value of the the `openshift_logging_es_pvc_dynamic` parameter.

|`openshift_logging_es_pvc_storage_class_name`
|To use a non-default storage class, specify the storage class name, such as
`glusterprovisioner` or `cephrbdprovisioner`. After you specify
the storage class name, dynamic volume provisioning is active regardless of the
openshift_logging_es_pvc_dynamic value.

|`openshift_logging_es_pvc_size`
|Size of the persistent volume claim to
create per Elasticsearch instance. For example, 100G. If omitted, no PVCs are
created and ephemeral volumes are used instead.

|`openshift_logging_es_pvc_prefix`
a|Prefix for the names of persistent volume claims to be used as storage for
Elasticsearch instances. A number is appended per instance, such as
*logging-es-1*. If they do not already exist, they are created with size
`_es-pvc-size_`.

When `openshift_logging_es_pvc_prefix` is set, and:

* `openshift_logging_es_pvc_dynamic`=`true`, the value for `openshift_logging_es_pvc_size` is optional.
* `openshift_logging_es_pvc_dynamic`=`false`, the value for `openshift_logging_es_pvc_size` must be set.

|`openshift_logging_es_recover_after_time`
|The amount of time ES will wait before it tries to recover.

|`openshift_logging_es_storage_group`
|Number of a supplemental group ID for access to Elasticsearch storage volumes.
Backing volumes should allow access by this group ID.

|`openshift_logging_es_nodeselector`
|A node selector specified as a map that determines which nodes are eligible targets
for deploying Elasticsearch instances. This can be used to place
these instances on nodes reserved or optimized for running them.
For example, the selector could be `{"node-type":"infrastructure"}`. At least
one active node must have this label before Elasticsearch will deploy.

|`openshift_logging_es_ops_allow_cluster_reader`
|Set to `true` if cluster-reader role is allowed to read operation logs.

|`openshift_logging_es_ops_host`
|Equivalent to `openshift_logging_es_host` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_port`
|Equivalent to `openshift_logging_es_port` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_ca`
|Equivalent to `openshift_logging_es_ca` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_client_cert`
|Equivalent to `openshift_logging_es_client_cert` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_client_key`
|Equivalent to `openshift_logging_es_client_key` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_cluster_size`
|Equivalent to `openshift_logging_es_cluster_size` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_cpu_limit`
|Equivalent to `openshift_logging_es_cpu_limit` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_memory_limit`
|Equivalent to `openshift_logging_es_memory_limit` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_pv_selector`
|Equivalent to `openshift_logging_es_pv_selector` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_pvc_dynamic`
|Equivalent to `openshift_logging_es_pvc_dynamic` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_pvc_size`
|Equivalent to `openshift_logging_es_pvc_size` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_pvc_prefix`
|Equivalent to `openshift_logging_es_pvc_prefix` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_recover_after_time`
|Equivalent to `openshift_logging_es_recovery_after_time` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_storage_group`
|Equivalent to `openshift_logging_es_storage_group` for Ops cluster
when `openshift_logging_use_ops` is set to `true`.

|`openshift_logging_es_ops_nodeselector`
|A node selector that specifies which nodes are eligible targets
for deploying Elasticsearch instances. This can be used to place
these instances on nodes reserved or optimized for running them.
For example, the selector could be `node-type=infrastructure`. At least
one active node must have this label before Elasticsearch will deploy.

|`openshift_logging_elasticsearch_kibana_index_mode`
|The default value, `unique`, allows users to each have their own Kibana index. In
this mode, their saved queries, visualizations, and dashboards are not shared.

You may also set the value `shared_ops`. In this mode, all operations users
share a Kibana index which allows each operations user to see the same
queries, visualizations, and dashboards.

|`openshift_logging_kibana_ops_nodeselector`
|A node selector that specifies which nodes are eligible targets
for deploying Kibana instances.

|`openshift_logging_curator_ops_nodeselector`
|A node selector that specifies which nodes are eligible targets
for deploying Curator instances.

|===


[[logging-custom-certificates]]
*Custom Certificates*

You can specify custom certificates using the following inventory variables
instead of relying on those generated during the deployment process. These
certificates are used to encrypt and secure communication between a user's
browser and Kibana. The security-related files will be generated if they are not
supplied.

[cols="3,7",options="header"]
|===
|File Name
|Description

|`openshift_logging_kibana_cert`
|A browser-facing certificate for the Kibana server.

|`openshift_logging_kibana_key`
|A key to be used with the browser-facing Kibana certificate.

|`openshift_logging_kibana_ca`
|The absolute path on the control node to the CA file to use
for the browser facing Kibana certs.

|`openshift_logging_kibana_ops_cert`
|A browser-facing certificate for the Ops Kibana server.

|`openshift_logging_kibana_ops_key`
|A key to be used with the browser-facing Ops Kibana certificate.

|`openshift_logging_kibana_ops_ca`
|The absolute path on the control node to the CA file to use
for the browser facing ops Kibana certs.
|===

[[deploying-the-efk-stack]]
== Deploying the EFK Stack

The EFK stack is deployed using an Ansible playbook to the EFK components. Run the playbook from the default OpenShift Ansible location
using the default
xref:../..install/configuring_inventory_file.adoc#configuring-ansible[inventory] file.

ifdef::openshift-origin[]
----
$ ansible-playbook playbooks/openshift-logging/config.yml
----
endif::openshift-origin[]

ifdef::openshift-enterprise[]
----
$ ansible-playbook [-i </path/to/inventory>] \
    /usr/share/ansible/openshift-ansible/playbooks/openshift-logging/config.yml
----
endif::openshift-enterprise[]

Running the playbook deploys all resources needed to support the stack; such as
Secrets, ServiceAccounts, and DeploymentConfigs. The playbook waits to deploy
the component pods until the stack is running. If the wait steps fail, the
deployment could still be successful; it may be retrieving the component images
from the registry which can take up to a few minutes. You can watch the
process with:

----
$ oc get pods -w
----

They will eventually enter *Running* status. For additional details about the status of the pods during deployment by retrieving
associated events:

----
$ oc describe pods/<pod_name>
----

Check the logs if the pods do not run successfully:

----
$ oc logs -f <pod_name>
----

[[aggregate-logging-cleanup]]
== Cleanup

Remove everything generated during the deployment.

ifdef::openshift-origin[]
----
$ ansible-playbook playbooks/openshift-logging/config.yml \
    -e openshift_logging_install_logging=False
----
endif::openshift-origin[]

ifdef::openshift-enterprise[]
----
$ ansible-playbook [-i </path/to/inventory>] \
    /usr/share/ansible/openshift-ansible/playbooks/openshift-logging/config.yml \
    -e openshift_logging_install_logging=False
----
endif::openshift-enterprise[]



