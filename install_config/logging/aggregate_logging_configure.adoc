[[install-config-aggregate-logging-configure-understanding]]
= Understanding and Adjusting the Deployment
{product-author}
{product-version}
ifdef::openshift-enterprise[]
:latest-tag: v3.10.14
endif::[]
ifdef::openshift-origin[]
:latest-tag: v3.10.14
endif::[]
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

This section describes adjustments that you can make to deployed components.

[[aggregated-ops]]
=== Ops Cluster

[NOTE]
====
The logs for the *default*, *openshift*, and *openshift-infra* projects are
automatically aggregated and grouped into the *.operations* item in the Kibana
interface.

The project where you have deployed the EFK stack (*logging*, as documented
here) is _not_ aggregated into *.operations* and is found under its ID.
====

If you set `openshift_logging_use_ops` to *true* in your inventory file, Fluentd is
configured to split logs between the main Elasticsearch cluster and another
cluster reserved for operations logs, which are defined as node system logs and
the projects *default*, *openshift*, and *openshift-infra*. Therefore, a
separate Elasticsearch cluster, a separate Kibana, and a separate Curator are
deployed to index, access, and manage operations logs. These deployments are set
apart with names that include `-ops`. Keep these separate deployments in mind if
you enable this option. Most of the following discussion also applies to the
operations cluster if present, just with the names changed to include `-ops`.

[[aggregated-elasticsearch]]
=== Elasticsearch

A highly-available environment requires at least three replicas of
Elasticsearch; each on a different host. Elasticsearch replicas require their
own storage, but an {product-title} deployment configuration shares storage
volumes between all its pods. So, when scaled up, the EFK deployer ensures each
replica of Elasticsearch has its own deployment configuration.

It is possible to scale your cluster up after creation by modifying the
`openshift_logging_es_cluster_size` in the inventory file and re-running the
logging playbook. Additional clustering parameters can be modified and are
described in xref:../install_config/logging/aggregate_logging_deploy.adoc#aggregate-logging-ansible-variables[Specifying Logging Ansible Variables].

Refer to
link:https://www.elastic.co/guide/en/elasticsearch/guide/current/hardware.html[Elastic's
documentation] for considerations involved in choosing storage and
network location as directed below.

*Viewing all Elasticsearch Deployments*

To view all current Elasticsearch deployments:

====
----
$ oc get dc --selector logging-infra=elasticsearch
----
====

[[logging-node-selector]]
*Node Selector*

Because Elasticsearch can use a lot of resources, all members of a cluster
should have low latency network connections to each other and to any remote
storage. Ensure this by directing the instances to dedicated nodes, or a
dedicated region within your cluster, using a
xref:../admin_guide/managing_projects.adoc#using-node-selectors[node selector].

To configure a node selector, specify the `openshift_logging_es_nodeselector`
configuration option in the inventory file. This applies to all Elasticsearch
deployments; if you need to individualize the node selectors, you must manually
edit each deployment configuration after deployment. The node selector is
specified as a python compatible dict. For example, `{"node-type":"infra",
"region":"east"}`.

[[aggregated-logging-persistent-storage]]
*Persistent Elasticsearch Storage*

By default, the `openshift_logging` Ansible role creates an ephemeral
deployment in which all of a pod's data is lost upon restart. For production
usage, specify a persistent storage volume for each Elasticsearch deployment
configuration. You can create the necessary
xref:../architecture/additional_concepts/storage.adoc#persistent-volume-claims[persistent
volume claims] before deploying or have them created for you. The PVCs must be
named to match the `openshift_logging_es_pvc_prefix` setting, which defaults to
`logging-es`; each PVC name will have a sequence number added to it: `logging-es-0`,
`logging-es-1`, `logging-es-2`, and so on. If a PVC needed for the deployment
exists already, it is used; if not, and `openshift_logging_es_pvc_size` has been
specified, it is created with a request for that size.

[WARNING]
====
Using NFS storage as a volume or a persistent volume (or via NAS such as
Gluster) is not supported for Elasticsearch storage, as Lucene relies on file
system behavior that NFS does not supply. Data corruption and other problems can
occur. If NFS storage is a requirement, you can allocate a large file on a
volume to serve as a storage device and mount it locally on one host.
For example, if your NFS storage volume is mounted at *_/nfs/storage_*:

----
$ truncate -s 1T /nfs/storage/elasticsearch-1
$ mkfs.xfs /nfs/storage/elasticsearch-1
$ mount -o loop /nfs/storage/elasticsearch-1 /usr/local/es-storage
$ chown 1000:1000 /usr/local/es-storage
----

Then, use *_/usr/local/es-storage_* as a host-mount as described below.
Use a different backing file as storage for each Elasticsearch replica.

This loopback must be maintained manually outside of {product-title}, on the
node. You must not maintain it from inside a container.
====

It is possible to use a local disk volume (if available) on each
node host as storage for an Elasticsearch replica. Doing so requires
some preparation as follows.

. The relevant service account must be given the privilege to mount and edit a
local volume:
+
====
----
$ oc adm policy add-scc-to-user privileged  \
       system:serviceaccount:logging:aggregated-logging-elasticsearch <1>
----
<1> Use the project you created earlier (for example, *logging*) when running the
logging playbook.
====

. Each Elasticsearch replica definition must be patched to claim that privilege,
for example:
+
----
$ for dc in $(oc get deploymentconfig --selector logging-infra=elasticsearch -o name); do
    oc scale $dc --replicas=0
    oc patch $dc \
       -p '{"spec":{"template":{"spec":{"containers":[{"name":"elasticsearch","securityContext":{"privileged": true}}]}}}}'
  done
----

. The Elasticsearch replicas must be located on the correct nodes to use the local
storage, and should not move around even if those nodes are taken down for a
period of time. This requires giving each Elasticsearch replica a node selector
that is unique to a node where an administrator has allocated storage for it. To
configure a node selector, edit each Elasticsearch deployment configuration and
add or edit the *nodeSelector* section to specify a unique label that you have
applied for each desired node:
+
====
----
apiVersion: v1
kind: DeploymentConfig
spec:
  template:
    spec:
      nodeSelector:
        logging-es-node: "1" <1>
----
<1> This label should uniquely identify a replica with a single node that bears that
label, in this case `logging-es-node=1`. Use the `oc label` command to apply
labels to nodes as needed.

To automate applying the node selector you can instead use the `oc patch` command:

----
$ oc patch dc/logging-es-<suffix> \
   -p '{"spec":{"template":{"spec":{"nodeSelector":{"logging-es-node":"1"}}}}}'
----
====

. Once these steps are taken, a local host mount can be applied to each replica
as in this example (where we assume storage is mounted at the same path on each node):
+
----
$ for dc in $(oc get deploymentconfig --selector logging-infra=elasticsearch -o name); do
    oc set volume $dc \
          --add --overwrite --name=elasticsearch-storage \
          --type=hostPath --path=/usr/local/es-storage
    oc rollout latest $dc
    oc scale $dc --replicas=1
  done
----

[[scaling-elasticsearch]]
*Changing the Scale of Elasticsearch*

If you need to scale up the number of Elasticsearch instances your cluster uses,
it is not as simple as scaling up an Elasticsearch deployment configuration.
This is due to the nature of persistent volumes and how Elasticsearch is
configured to store its data and recover the cluster. Instead, scaling up
requires creating a deployment configuration for each Elasticsearch cluster
node.

The simplest way to change the scale of Elasticsearch is to modify the inventory
host file and re-run the logging playbook as described previously. Assuming you
have supplied persistent storage for the deployment, this should not be
disruptive.

[NOTE]
====
Resizing an Elasticsearch cluster using the logging playbook is only possible when
the new `openshift_logging_es_cluster_size` value is higher than the current number
of Elasticsearch nodes (scaled up) in the cluster.
====

If you do not wish to reinstall, for instance because you have made
customizations that you would like to preserve, then it is possible to add new
Elasticsearch deployment configurations to the cluster using a template supplied
by the deployer. This requires a more complicated procedure however.

[[expose-elasticsearch-as-route]]
*Expose Elasticsearch as a Route*

By default, Elasticsearch deployed with OpenShift aggregated logging is not
accessible from outside the logging cluster. You can enable a route for external
access to Elasticsearch for those tools that want to access its data.

You have access to Elasticsearch using your OpenShift token, and
you can provide the external Elasticsearch and Elasticsearch Ops
hostnames when creating the server certificate (similar to Kibana).

. To access Elasticsearch as a reencrypt route, define the following variables:
+
----
openshift_logging_es_allow_external=True
openshift_logging_es_hostname=elasticsearch.example.com
----

. Run the following Ansible playbook:
+
----
$ ansible-playbook [-i </path/to/inventory>] \
    /usr/share/ansible/openshift-ansible/playbooks/openshift-logging/config.yml
----

. To log in to Elasticsearch remotely, the request must contain three HTTP headers:
+
----
Authorization: Bearer $token
X-Proxy-Remote-User: $username
X-Forwarded-For: $ip_address
----

. You must have access to the project in order to be able to access to the
logs. For example:
+
----
$ oc login <user1>
$ oc new-project <user1project>
$ oc new-app <httpd-example>
----

. You need to get the token of this ServiceAccount to be used in the request:
+
----
$ token=$(oc whoami -t)
----

. Using the token previously configured, you should be able access Elasticsearch
through the exposed route:
+
----
$ curl -k -H "Authorization: Bearer $token" -H "X-Proxy-Remote-User: $(oc whoami)" -H "X-Forwarded-For: 127.0.0.1" https://es.example.test/project.my-project.*/_search?q=level:err | python -mjson.tool
----

[[aggregated-fluentd]]
=== Fluentd

Fluentd is deployed as a DaemonSet that deploys replicas according to a node
label selector, which you can specify with the inventory parameter
`openshift_logging_fluentd_nodeselector` and the default is `logging-infra-fluentd`.
As part of the OpenShift cluster installation, it is recommended that you add the
Fluentd node selector to the list of persisted
xref:../install/configuring_inventory_file.adoc#configuring-node-host-labels[node labels].

Fluentd uses `journald` as the system log source. These are log messages from
the operating system, the container runtime, and OpenShift.

The available container runtimes provide minimal information to identify the
source of log messages. Log collection and normalization of logs can occur after
a pod is deleted and additional metadata cannot be retrieved from the
API server, such as labels or annotations.

If a pod with a given name and namespace is deleted before the log collector
finishes processing logs, there might not be a way to distinguish the log messages
from a similarly named pod and namespace. This can cause logs to be indexed and
annotated to an index that is not owned by the user who deployed the pod.

[IMPORTANT]
====
The available container runtimes provide minimal information to identify the
source of log messages and do not guarantee unique individual log
messages or that these messages can be traced to their source.
====

Clean installations of {product-title} 3.9 use `json-file` as the default log
driver, but environments upgraded from {product-title} 3.7 will maintain their
existing `journald` log driver configuration. It is recommended to use the
`json-file` log driver. See xref:fluentd-update-source[Changing the Aggregated
Logging Driver] for instructions to change your existing log driver
configuration to `json-file`.

[[fluentd-external-log-aggregator]]
*Configuring Fluentd to Send Logs to an External Log Aggregator*

You can configure Fluentd to send a copy of its logs to an external log
aggregator, and not the default Elasticsearch, using the `secure-forward`
plug-in. From there, you can further process log records after the locally
hosted Fluentd has processed them.

ifdef::openshift-origin[]
The `secure-forward` plug-in is provided with the Fluentd image as of v1.4.0.
endif::openshift-origin[]

The logging deployment provides a `secure-forward.conf` section in the Fluentd configmap
for configuring the external aggregator:

----
<store>
@type secure_forward
self_hostname pod-${HOSTNAME}
shared_key thisisasharedkey
secure yes
enable_strict_verification yes
ca_cert_path /etc/fluent/keys/your_ca_cert
ca_private_key_path /etc/fluent/keys/your_private_key
ca_private_key_passphrase passphrase
<server>
  host ose1.example.com
  port 24284
</server>
<server>
  host ose2.example.com
  port 24284
  standby
</server>
<server>
  host ose3.example.com
  port 24284
  standby
</server>
</store>
----

This can be updated using the `oc edit` command:

----
$ oc edit configmap/logging-fluentd
----

Certificates to be used in `secure-forward.conf` can be added to the existing
secret that is mounted on the Fluentd pods. The `your_ca_cert` and
`your_private_key` values must match what is specified in `secure-forward.conf`
in `configmap/logging-fluentd`:

----
$ oc patch secrets/logging-fluentd --type=json \
  --patch "[{'op':'add','path':'/data/your_ca_cert','value':'$(base64 /path/to/your_ca_cert.pem)'}]"
$ oc patch secrets/logging-fluentd --type=json \
  --patch "[{'op':'add','path':'/data/your_private_key','value':'$(base64 /path/to/your_private_key.pem)'}]"
----

[NOTE]
====
Replace `your_private_key` with a generic name. This is a link to the JSON path,
not a path on your host system.
====

When configuring the external aggregator, it must be able to accept messages
securely from Fluentd.

If the external aggregator is another Fluentd server, it must have the
`fluent-plugin-secure-forward` plug-in installed and make use of the input
plug-in it provides:

----
<source>
  @type secure_forward

  self_hostname ${HOSTNAME}
  bind 0.0.0.0
  port 24284

  shared_key thisisasharedkey

  secure yes
  cert_path        /path/for/certificate/cert.pem
  private_key_path /path/for/certificate/key.pem
  private_key_passphrase secret_foo_bar_baz
</source>
----

You can find further explanation of how to set up the
`fluent-plugin-secure-forward` plug-in in the
link:https://github.com/tagomoris/fluent-plugin-secure-forward[`fluent-plugin-secure-forward` repository].

*Reducing the Number of Connections from Fluentd to the API Server*

[IMPORTANT]
====
`mux` is a Technology Preview feature only.
ifdef::openshift-enterprise[]
Technology Preview features are not supported with Red Hat production service
level agreements (SLAs), might not be functionally complete, and Red Hat does
not recommend to use them for production. These features provide early access to
upcoming product features, enabling customers to test functionality and provide
feedback during the development process.

For more information on Red Hat Technology Preview features support scope, see
https://access.redhat.com/support/offerings/techpreview/.
endif::[]
====

`mux` is a Secure Forward listener service.

[cols="3,7",options="header"]
|===
|Parameter
|Description

| `openshift_logging_use_mux`
|The default is set to `False`. If set to `True`,
a service called `mux` is deployed. This service acts as a Fluentd
`secure_forward` aggregator for the node agent Fluentd daemonsets running in the
cluster. Use `openshift_logging_use_mux` to reduce the number of connections to
the OpenShift API server, and configure each node in Fluentd to send raw logs to
`mux` and turn off the Kubernetes metadata plug-in. This requires the use of
`openshift_logging_mux_client_mode`.

|`openshift_logging_mux_client_mode`
|Values for `openshift_logging_mux_client_mode` are `minimal` and `maximal`, and
there is no default. `openshift_logging_mux_client_mode` causes the Fluentd node
agent to send logs to mux rather than directly to Elasticsearch. The value
`maximal` means that Fluentd does as much processing as possible at the node
before sending the records to `mux`. The `maximal` value is recommended for
using `mux`. The value `minimal` means that Fluentd does no processing at all,
and sends the raw logs to `mux` for processing. It is not recommended to use the
`minimal` value.

| `openshift_logging_mux_allow_external`
|The default is set to `False`. If set to `True`, the `mux` service is
deployed, and it is configured to allow Fluentd clients running outside of
the cluster to send logs using `secure_forward`. This allows OpenShift logging
to be used as a central logging service for clients other than OpenShift, or
other OpenShift clusters.

| `openshift_logging_mux_hostname`
|The default is `mux` plus `openshift_master_default_subdomain`. This is the
hostname `external_clients` will use to connect to `mux`, and is used in the
TLS server cert subject.

| `openshift_logging_mux_port`
|24284

| `openshift_logging_mux_cpu_limit`
|500M

| `openshift_logging_mux_memory_limit`
|1Gi

| `openshift_logging_mux_default_namespaces`
|The default is `mux-undefined`. The first value in the list is the namespace to
use for undefined projects, followed by any additional namespaces to create by
default. Usually, you do not need to set this value.

| `openshift_logging_mux_namespaces`
|The default value is empty, allowing for additional namespaces to create for
external `mux` clients to associate with their logs. You will need to set this
value.
|===


[[fluentd-throttling]]
*Throttling logs in Fluentd*

For projects that are especially verbose, an administrator can throttle down the
rate at which the logs are read in by Fluentd before being processed.

[WARNING]
====
Throttling can contribute to log aggregation falling behind for the configured
projects; log entries can be lost if a pod is deleted before Fluentd catches up.
====

[NOTE]
====
Throttling does not work when using the systemd journal as the log
source. The throttling implementation depends on being able to throttle the
reading of the individual log files for each project. When reading from the
journal, there is only a single log source, no log files, so no file-based
throttling is available. There is not a method of restricting the log
entries that are read into the Fluentd process.
====

To tell Fluentd which projects it should be restricting, edit the throttle
configuration in its ConfigMap after deployment:

----
$ oc edit configmap/logging-fluentd
----

The format of the *_throttle-config.yaml_* key is a YAML file that contains
project names and the desired rate at which logs are read in on each
node. The default is 1000 lines at a time per node. For example:

====
----
logging:
  read_lines_limit: 500

test-project:
  read_lines_limit: 10

.operations:
  read_lines_limit: 100
----
====

When you make changes to any part of the EFK stack, specifically Elasticsearch
or Fluentd, you should first scale Elasicsearch down to zero and scale Fluentd
so it does not match any other nodes. Then, make the changes and scale
Elasicsearch and Fluentd back.

To scale Elasicsearch to zero:
----
$ oc scale --replicas=0 dc/<ELASTICSEARCH_DC>
----

Change nodeSelector in the daemonset configuration to match zero:

.Get the fluentd node selector:
----
$ oc get ds logging-fluentd -o yaml |grep -A 1 Selector
     nodeSelector:
       logging-infra-fluentd: "true"
----

.Use the `oc patch` command to modify the daemonset nodeSelector:
----
$ oc patch ds logging-fluentd -p '{"spec":{"template":{"spec":{"nodeSelector":{"nonexistlabel":"true"}}}}}'
----

.Get the fluentd node selector:
----
$ oc get ds logging-fluentd -o yaml |grep -A 1 Selector
     nodeSelector:
       "nonexistlabel: "true"
----

Scale Elastcsearch back up from zero:
----
$ oc scale --replicas=# dc/<ELASTICSEARCH_DC>
----

Change nodeSelector in the daemonset configuration back to
logging-infra-fluentd: "true".

Use the `oc patch` command to modify the daemonset nodeSelector:
----
oc patch ds logging-fluentd -p '{"spec":{"template":{"spec":{"nodeSelector":{"logging-infra-fluentd":"true"}}}}}'
----

[[aggregate-logging-kibana]]
=== Kibana

To access the Kibana console from the {product-title} web console, add the
`loggingPublicURL` parameter in the
xref:../install_config/web_console_customization.adoc#install-config-web-console-customization[master
webconsole-config configmap file], with the URL of the Kibana console (the
`kibana-hostname` parameter). The value must be an HTTPS URL:

====
----
...
clusterInfo:
  ...
  loggingPublicURL: "https://kibana.example.com"
...
----
====

Setting the `loggingPublicURL` parameter creates a *View Archive* button on the
{product-title} web console under the *Browse* -> *Pods* -> *<pod_name>* ->
*Logs* tab. This links to the Kibana console.

You can scale the Kibana deployment as usual for redundancy:

====
----
$ oc scale dc/logging-kibana --replicas=2
----
====

[NOTE]
====
To ensure the scale persists across multiple executions of the logging playbook,
make sure to update the `openshift_logging_kibana_replica_count` in the inventory file.
====


You can see the user interface by visiting the site specified by the
`openshift_logging_kibana_hostname` variable.

See the link:https://www.elastic.co/guide/en/kibana/4.5/discover.html[Kibana
documentation] for more information on Kibana.

[[kibana-visualizations-dashboard]]
*Kibana Visualize*

Kibana Visualize enables you to create visualizations and dashboards for
monitoring container and pod logs allows administrator users (`cluster-admin` or
`cluster-reader`) to view logs by deployment, namespace, pod, and container.

Kibana Visualize exists inside the Elasticsearch and ES-OPS
pod, and must be run inside those pods. To load dashboards and other Kibana UI
objects, you must first log into Kibana as the user you want to add the
dashboards to, then log out. This will create the necessary per-user
configuration that the next step relies on. Then, run:

----
$ oc exec <$espod> -- es_load_kibana_ui_objects <user-name>
----

Where `$espod` is the name of any one of your Elasticsearch pods.

[[configuring-curator]]
=== Curator

Curator allows administrators to configure scheduled Elasticsearch maintenance
operations to be performed automatically on a per-project basis. It is scheduled
to perform actions daily based on its configuration. Only one Curator pod is
recommended per Elasticsearch cluster. Curator is configured via a YAML
configuration file with the following structure:

[NOTE]
====
The time zone is set based on the {product-title} master node.
====

----
$PROJECT_NAME:
  $ACTION:
    $UNIT: $VALUE

$PROJECT_NAME:
  $ACTION:
    $UNIT: $VALUE
 ...

----

The available parameters are:

[cols="3,7",options="header"]
|===
|Variable Name
|Description

|`PROJECT_NAME`
|The actual name of a project, such as *myapp-devel*. For {product-title} *operations*
logs, use the name `.operations` as the project name.

|`ACTION`
|The action to take, currently only `delete` is allowed.

|`UNIT`
|One of `days`, `weeks`, or `months`.

|`VALUE`
|An integer for the number of units.

|`.defaults`
|Use `.defaults` as the `$PROJECT_NAME` to set the defaults for projects that are
not specified.

|`runhour`
|(Number) the hour of the day in 24-hour format at which to run the Curator jobs. For
use with `.defaults`.

|`runminute`
|(Number) the minute of the hour at which to run the Curator jobs. For use with `.defaults`.

|`.regex`
|The list of regular expressions that match project names.

|`pattern`
|The valid and properly escaped regular expression pattern enclosed by single
quotation marks.

|===

For example, to configure Curator to:

- Delete indices in the *myapp-dev* project older than `1 day`
- Delete indices in the *myapp-qe* project older than `1 week`
- Delete *operations* logs older than `8 weeks`
- Delete all other projects indices after they are `31 days` old
- Run the Curator jobs at midnight every day
- Delete indices older than 1 day that are matched by the '^project\..+\-dev.*$' regex
- Delete indices older than 2 days that are matched by the '^project\..+\-test.*$' regex

Use:

----
config.yaml: |
  myapp-dev:
    delete:
      days: 1

  myapp-qe:
    delete:
      weeks: 1

  .operations:
    delete:
      weeks: 8

  .defaults:
    delete:
      days: 31

  .regex:
    - pattern: '^project\..+\-dev\..*$'
      delete:
        days: 1
    - pattern: '^project\..+\-test\..*$'
      delete:
        days: 2
----

[IMPORTANT]
====
When you use `months` as the `$UNIT` for an operation, Curator starts counting at
the first day of the current month, not the current day of the current month.
For example, if today is April 15, and you want to delete indices that are 2 months
older than today (delete: months: 2), Curator does not delete indices that are dated
older than February 15; it deletes indices older than February 1. That is, it
goes back to the first day of the current month, then goes back two whole months
from that date. If you want to be exact with Curator, it is best to use days
(for example, `delete: days: 30`).
====

[[aggregate-logging-using-curator-actions-file]]
==== Using the Curator Actions File

Setting the {product-title} custom configuration file format ensures internal
indices are not mistakenly deleted.

To use the *actions file*, add an exclude rule to your Curator configuration to
retain these indices. You must manually add all of the required patterns.

----
actions.yaml: |
actions:

    action: delete_indices
    description: be careful!
    filters:
    - exclude: false
      kind: regex
      filtertype: pattern
      value: '^project\.myapp\..*$'
    - direction: older
      filtertype: age
      source: name
      timestring: '%Y.%m.%d'
      unit_count: 7
      unit: days
    options:
      continue_if_exception: false
      timeout_override: '300'
      ignore_empty_list: true

    action: delete_indices
    description: be careful!
    filters:
    - exclude: false
      kind: regex
      filtertype: pattern
      value: '^\.operations\..*$'
    - direction: older
      filtertype: age
      source: name
      timestring: '%Y.%m.%d'
      unit_count: 56
      unit: days
    options:
      continue_if_exception: false
      timeout_override: '300'
      ignore_empty_list: true

    action: delete_indices
    description: be careful!
    filters:
    - exclude: true
      kind: regex
      filtertype: pattern
      value: '^project\.myapp\..*$|^\.operations\..*$|^\.searchguard\..*$|^\.kibana$'
    - direction: older
      filtertype: age
      source: name
      timestring: '%Y.%m.%d'
      unit_count: 30
      unit: days
    options:
      continue_if_exception: false
      timeout_override: '300'
      ignore_empty_list: true
----


[[aggregate-logging-creating-the-curator-configuration]]
==== Creating the Curator Configuration

The `openshift_logging` Ansible role provides a ConfigMap from which Curator
reads its configuration. You may edit or replace this ConfigMap to reconfigure
Curator. Currently the `logging-curator` ConfigMap is used to configure both
your ops and non-ops Curator instances. Any `.operations` configurations are
in the same location as your application logs configurations.

. To create the Curator configuration, edit the configuration in the deployed
ConfigMap:
+
----
$ oc edit configmap/logging-curator
----

.. For scripted deployments, copy the configuration file that was created by the
installer and create your new {product-title} custom configuration:
+
----
$ oc extract configmap/logging-curator --keys=curator5.yaml,config.yaml --to=/my/config
  edit /my/config/curator5.yaml
  edit /my/config/config.yaml
$ oc delete configmap logging-curator ; sleep 1
$ oc create configmap logging-curator \
    --from-file=curator5.yaml=/my/config/curator5.yaml \
    --from-file=config.yaml=/my/config/config.yaml \
    ; sleep 1
----

.. Alternatively, if you are using the *actions file*:
+
----
$ oc extract configmap/logging-curator --keys=curator5.yaml,actions.yaml --to=/my/config
  edit /my/config/curator5.yaml
  edit /my/config/actions.yaml
$ oc delete configmap logging-curator ; sleep 1
$ oc create configmap logging-curator \
    --from-file=curator5.yaml=/my/config/curator5.yaml \
    --from-file=actions.yaml=/my/config/actions.yaml \
    ; sleep 1
----

The next scheduled job use this configuration.

