[[install-config-downgrade]]
= Downgrading OpenShift
{product-author}
{product-version}
:icons: font
:experimental:
:toc: macro
:toc-title:
:prewrap!:
:description: Manual steps to revert {product-title} to a previous version following an upgrade.
:keywords: yum

toc::[]

== Overview

Following an {product-title}
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[upgrade],
it may be desirable in extreme cases to downgrade your cluster to a previous
version. The following sections outline the required steps for each system in a
cluster to perform such a downgrade for the {product-title} 3.6 to 3.5 downgrade
path.

[WARNING]
====
These steps are currently only supported for
xref:../install_config/install/rpm_vs_containerized.adoc#install-config-install-rpm-vs-containerized[RPM-based
installations] of {product-title} and assumes downtime of the entire cluster.
====

[[downgrade-verifying-backups]]
== Verifying Backups

The Ansible playbook used during the
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[upgrade
process] should have created a backup of the *_master-config.yaml_* file and the
etcd data directory. Ensure these exist on your masters and etcd members:

----
/etc/origin/master/master-config.yaml.<timestamp>
/var/lib/etcd/openshift-backup-pre-upgrade-<timestamp>
----

Also, back up the *_node-config.yaml_* file on each node (including masters,
which have the node component on them) with a timestamp:

----
/etc/origin/node/node-config.yaml.<timestamp>
----

If you are using an external etcd cluster (versus the single embedded etcd), the
backup is likely created on all etcd members, though only one is required for
the recovery process.

The RPM downgrade process in a later step should create *_.rpmsave_* backups of
the following files, but it may be a good idea to keep a separate copy
regardless:

----
/etc/sysconfig/atomic-openshift-master
/etc/sysconfig/atomic-openshift-master-api
/etc/sysconfig/atomic-openshift-master-controller
/etc/etcd/etcd.conf <1>
----
<1> Only required if using external etcd.

[[downgrade-shutting-down-the-cluster]]
== Shutting Down the Cluster

On all masters, nodes, and etcd members (if using an external etcd cluster),
ensure the relevant services are stopped.

----
# systemctl stop atomic-openshift-master-api atomic-openshift-master-controllers
----

On all master and node hosts:

----
# systemctl stop atomic-openshift-node
----

On any external etcd hosts:

----
# systemctl stop etcd
----

[[downgrade-removing-rpms]]
== Removing RPMs

. The **-excluder* packages add entries to the exclude directive in the hostâ€™s
*_/etc/yum.conf_* file when installed. Run the following command on each host to
remove the `atomic-openshift-*` and `docker` packages from the exclude list:
+
----
# atomic-openshift-excluder unexclude
# atomic-openshift-docker-excluder unexclude
----

. On all masters, nodes, and etcd members (if using an external etcd cluster),
remove the following packages:
+
----
# yum remove atomic-openshift \
    atomic-openshift-clients \
    atomic-openshift-node \
    atomic-openshift-master-api \
    atomic-openshift-master-controllers \
    openvswitch \
    atomic-openshift-sdn-ovs \
    tuned-profiles-atomic-openshift-node\
    atomic-openshift-excluder\
    atomic-openshift-docker-excluder
----

. If you are using external etcd, also remove the *etcd* package:
+
----
# yum remove etcd
----
+
If using the embedded etcd, leave the *etcd* package installed. It is required
for running the `etcdctl` command to issue operations in later steps.

[[downgrade-docker]]
== Downgrading Docker

Both {product-title} 3.5 and 3.6 require Docker 1.12, so Docker does not need to
be downgraded.

[[downgrade-reinstalling-rpms]]
== Reinstalling RPMs

Disable the {product-title} 3.6 repositories, and re-enable the 3.5
repositories:

----
# subscription-manager repos \
    --disable=rhel-7-server-ose-3.6-rpms \
    --enable=rhel-7-server-ose-3.5-rpms
----

On each master, install the following packages:

----
# yum install atomic-openshift \
    atomic-openshift-clients \
    atomic-openshift-node \
    atomic-openshift-master-api \
    atomic-openshift-master-controllers \
    openvswitch \
    atomic-openshift-sdn-ovs \
    tuned-profiles-atomic-openshift-node \
    atomic-openshift-excluder \
    atomic-openshift-docker-excluder
----

On each node, install the following packages:

----
# yum install atomic-openshift \
    atomic-openshift-node \
    openvswitch \
    atomic-openshift-sdn-ovs \
    tuned-profiles-atomic-openshift-node \
    atomic-openshift-excluder \
    atomic-openshift-docker-excluder
----

If using an external etcd cluster, install the following package on each etcd
member:

----
# yum install etcd
----

[[downgrade-restore-etcd]]
== Restoring etcd

See
xref:../admin_guide/backup_restore.adoc#admin-guide-backup-and-restore[Backup
and Restore].

[[downgrade-bringing-openshift-services-back-online]]
== Bringing {product-title} Services Back Online

See xref:../admin_guide/backup_restore.adoc#bringing-openshift-services-back-online[Backup
and Restore].

[[verifying-the-downgrade]]
== Verifying the Downgrade

. To verify the downgrade, first check that all nodes are marked as *Ready*:
+
----
# oc get nodes
NAME                        STATUS                     AGE
master.example.com          Ready,SchedulingDisabled   165d
node1.example.com           Ready                      165d
node2.example.com           Ready                      165d
----

. Then, verify that you are running the expected versions of the *docker-registry*
and *router* images, if deployed:
+
----
ifdef::openshift-enterprise[]
# oc get -n default dc/docker-registry -o json | grep \"image\"
    "image": "openshift3/ose-docker-registry:v3.5.5.31",
# oc get -n default dc/router -o json | grep \"image\"
    "image": "openshift3/ose-haproxy-router:v3.5.5.31",
----

. You can use the
xref:../admin_guide/diagnostics_tool.adoc#admin-guide-diagnostics-tool[diagnostics
tool] on the master to look for common issues and provide suggestions:
+
----
# oc adm diagnostics
...
[Note] Summary of diagnostics execution:
[Note] Completed with no errors or warnings seen.
----
