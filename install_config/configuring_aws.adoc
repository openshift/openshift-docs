[[install-config-configuring-aws]]
= Configuring for AWS
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
{product-title} can be configured to access an
link:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html[AWS EC2
infrastructure], including
xref:../install_config/persistent_storage/persistent_storage_aws.adoc#install-config-persistent-storage-persistent-storage-aws[using AWS
volumes as persistent storage] for application data. After AWS is configured
properly, some additional configurations will need to be completed on the
{product-title} hosts.


[[configuring-aws-variables]]
== Configuring AWS Variables

To set the required AWS variables, create a *_/etc/aws/aws.conf_* file with the
following contents on all of your {product-title} hosts, both masters and nodes:

====
----
[Global]
Zone = us-east-1c <1>
----
<1> This is the Availability Zone of your AWS Instance and where your EBS Volume
resides; this information is obtained from the AWS Managment Console.
====

[[aws-configuring-masters]]
== Configuring Masters

Edit or
xref:../install_config/master_node_configuration.adoc#creating-new-configuration-files[create]
the master configuration file on all masters
(*_/etc/origin/master/master-config.yaml_* by default) and update the contents
of the `*apiServerArguments*` and `*controllerArguments*` sections:

====
[source,yaml]
----
kubernetesMasterConfig:
  ...
  apiServerArguments:
    cloud-provider:
      - "aws"
    cloud-config:
      - "/etc/aws/aws.conf"
  controllerArguments:
    cloud-provider:
      - "aws"
    cloud-config:
      - "/etc/aws/aws.conf"
----
====

[[aws-configuring-nodes]]
== Configuring Nodes

Edit or
xref:../install_config/master_node_configuration.adoc#creating-new-configuration-files[create]
the node configuration file on all nodes (*_/etc/origin/node/node-config.yaml_*
by default) and update the contents of the `*kubeletArguments*` section:

====
[source,yaml]
----
kubeletArguments:
  cloud-provider:
    - "aws"
  cloud-config:
    - "/etc/aws/aws.conf"
----
====

[[aws-setting-key-value-access-pairs]]
== Setting Key Value Access Pairs

Make sure the following environment variables are set in the
ifdef::openshift-enterprise[]
*_/etc/sysconfig/atomic-openshift-master_* file on masters and the
*_/etc/sysconfig/atomic-openshift-node_* file on nodes:
endif::[]
ifdef::openshift-origin[]
*_/etc/sysconfig/origin-master_* file on masters and the
*_/etc/sysconfig/origin-node_* file on nodes:
endif::[]

====
----
AWS_ACCESS_KEY_ID=<key_ID>
AWS_SECRET_ACCESS_KEY=<secret_key>
----
====

[NOTE]
====
Access keys are obtained when setting up your AWS IAM user.
====

[[aws-applying-configuration-changes]]
== Applying Configuration Changes

Start or restart {product-title} services on all master and node hosts to apply your
configuration changes:

ifdef::openshift-enterprise[]
----
# systemctl restart atomic-openshift-master
# systemctl restart atomic-openshift-node
----
endif::[]
ifdef::openshift-origin[]
----
# systemctl restart origin-master
# systemctl restart origin-node
----
endif::[]

Switching from not using a cloud provider to using a cloud provider produces an
error message. Adding the cloud provider tries to delete the node because the
node switches from using the *hostname* as the `*externalID*` (which would have
been the case when no cloud provider was being used) to using the AWS
`*instance-id*` (which is what the AWS cloud provider specifies). To resolve
this issue:

.  Log in to the CLI as a cluster administrator.
.  Delete the nodes:
+
----
$ oc delete node <node_name>
----
.  On each node host, restart the *atomic-openshift-node* service.
.  Add back any xref:../admin_guide/manage_nodes.adoc#updating-labels-on-nodes[labels on each node] that you previously had.
