[[install-config-configuring-aws]]
= Configuring for Amazon Web Services (AWS)
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
{product-title} can be configured to access an
link:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html[AWS EC2 infrastructure], including
xref:../install_config/persistent_storage/persistent_storage_aws.adoc#install-config-persistent-storage-persistent-storage-aws[using AWS
volumes as persistent storage] for application data. After AWS is configured
properly, some additional configurations will need to be completed on the
{product-title} hosts.

[[configuring-aws-permissions]]
== Permissions
Configuring AWS for {product-title} requires the following permissions:

.Master Permissions
[cols='1,2']
|===

| Elastic Compute Cloud(EC2)
|`ec2:DescribeVolume`, `ec2:CreateVolume`, `ec2:CreateTags`,
`ec2:DescribeInstance`, `ec2:AttachVolume`, `ec2:DetachVolume`,
`ec2:DeleteVolume`, `ec2:DescribeSubnets`, `ec2:CreateSecurityGroup`,
`ec2:DescribeSecurityGroups`, `ec2:DescribeRouteTables`,
`ec2:AuthorizeSecurityGroupIngress`

| Elastic Load Balancing
| `elasticloadbalancing:DescribeTags`,
`elasticloadbalancing:CreateLoadBalancerListeners`,
`elasticloadbalancing:ConfigureHealthCheck`,
`elasticloadbalancing:DeleteLoadBalancerListeners`,
`elasticloadbalancing:RegisterInstancesWithLoadBalancer`,
`elasticloadbalancing:DescribeLoadBalancers`,
`elasticloadbalancing:CreateLoadBalancer`,
`elasticloadbalancing:DeleteLoadBalancer`,
`elasticloadbalancing:ModifyLoadBalancerAttributes`,
`elasticloadbalancing:DescribeLoadBalancerAttributes`

|===

.Node Permissions
[cols='1,2']
|===

| Elastic Compute Cloud(EC2)
| `ec2:DescribeInstance*`

|===

[IMPORTANT]
====
* Every master, node, and subnet must have the `KubernetesCluster: value` tag.
* One security group, preferably the one linked to the nodes, must have the
`KubernetesCluster: value` tag.
 ** Do not tag all security groups with the `KubernetesCluster: value` tag or the
Elastic Load Balancing (ELB) will not be able to create a load balancer.

====

[[configuring-aws-variables]]
== Configuring AWS Variables

To set the required AWS variables, create a *_/etc/aws/aws.conf_* file with the
following contents on all of your {product-title} hosts, both masters and nodes:


----
[Global]
Zone = us-east-1c <1>
----
<1> This is the Availability Zone of your AWS Instance and where your EBS Volume
resides; this information is obtained from the AWS Managment Console.


[[aws-configuring-masters]]
== Configuring {product-title} Masters for AWS

You can set the AWS configuration on your {product-title} master hosts in two ways:

- xref:aws-configuring-masters-ansible[using Ansible and the advanced installation tool]
- xref:aws-configuring-masters-manually[manually, by modifying the *_master-config.yaml_* file]

[[aws-configuring-masters-ansible]]
=== Configuring {product-title} for AWS with Ansible

During
xref:../install_config/install/advanced_install.adoc#install-config-install-advanced-install[advanced installations],
AWS can be configured using
xref:../install_config/install/advanced_install.adoc#advanced-install-configuring-global-proxy[the `*openshift_cloudprovider_aws_access_key*`, `*openshift_cloudprovider_aws_secret_key*`, `*openshift_cloudprovider_kind*`, `*openshift_clusterid*` parameters], which are configurable in the inventory file.

.Example AWS Configuration with Ansible

----
# Cloud Provider Configuration
#
# Note: You may make use of environment variables rather than store
# sensitive configuration within the ansible inventory.
# For example:
#openshift_cloudprovider_aws_access_key="{{ lookup('env','AWS_ACCESS_KEY_ID') }}"
#openshift_cloudprovider_aws_secret_key="{{ lookup('env','AWS_SECRET_ACCESS_KEY') }}"
#
#openshift_clusterid=unique_identifier_per_availablility_zone
#
# AWS (Using API Credentials)
#openshift_cloudprovider_kind=aws
#openshift_cloudprovider_aws_access_key=aws_access_key_id
#openshift_cloudprovider_aws_secret_key=aws_secret_access_key
#
# AWS (Using IAM Profiles)
#openshift_cloudprovider_kind=aws
# Note: IAM roles must exist before launching the instances.
----

[NOTE]
====
When Ansible configures AWS, the following files are created for you:

- *_/etc/aws/aws.conf_*
- *_/etc/origin/master/master-config.yaml_*
- *_/etc/origin/node/node-config.yaml_*
- *_/etc/sysconfig/atomic-openshift-master_*
- *_/etc/sysconfig/atomic-openshift-master-api_*
- *_/etc/sysconfig/atomic-openshift-master-controllers_*
- *_/etc/sysconfig/atomic-openshift-node_*
====

[[aws-configuring-masters-manually]]
=== Manually Configuring {product-title} Masters for AWS

Edit or
xref:../install_config/master_node_configuration.adoc#creating-new-configuration-files[create]
the master configuration file on all masters
(*_/etc/origin/master/master-config.yaml_* by default) and update the contents
of the `*apiServerArguments*` and `*controllerArguments*` sections:

[source,yaml]
----
kubernetesMasterConfig:
  ...
  apiServerArguments:
    cloud-provider:
      - "aws"
    cloud-config:
      - "/etc/aws/aws.conf"
  controllerArguments:
    cloud-provider:
      - "aws"
    cloud-config:
      - "/etc/aws/aws.conf"
----

Currently, the `nodeName` *must* match the instance name in AWS in order
for the cloud provider integration to work properly.  The name must also be
RFC1123 compliant.

[IMPORTANT]
====
When triggering a containerized installation, only the directories of
*_/etc/origin_* and *_/var/lib/origin_* are mounted to the master and node
container. Therefore, *_aws.conf_* should be in *_/etc/origin/_* instead of
*_/etc/_*.
====

[[aws-configuring-nodes]]
=== Manually Configuring {product-title} Nodes for AWS

Edit or
xref:../install_config/master_node_configuration.adoc#creating-new-configuration-files[create]
the node configuration file on all nodes (*_/etc/origin/node/node-config.yaml_*
by default) and update the contents of the `*kubeletArguments*` section:

[source,yaml]
----
kubeletArguments:
  cloud-provider:
    - "aws"
  cloud-config:
    - "/etc/aws/aws.conf"
----

[IMPORTANT]
====
When triggering a containerized installation, only the directories of
*_/etc/origin_* and *_/var/lib/origin_* are mounted to the master and node
container. Therefore, *_aws.conf_* should be in *_/etc/origin/_* instead of
*_/etc/_*.
====

[[aws-setting-key-value-access-pairs]]
== Setting Key Value Access Pairs

Make sure the following environment variables are set in the
ifdef::openshift-enterprise[]
*_/etc/sysconfig/atomic-openshift-master-api_* file and
*_/etc/sysconfig/atomic-openshift-master-containers_* file on masters and the
*_/etc/sysconfig/atomic-openshift-node_* file on nodes:
endif::[]
ifdef::openshift-origin[]
*_/etc/sysconfig/origin-master-api_* file and
*_/etc/sysconfig/origin-master-containers_* file on masters and the
*_/etc/sysconfig/origin-node_* file on nodes:
endif::[]

----
AWS_ACCESS_KEY_ID=<key_ID>
AWS_SECRET_ACCESS_KEY=<secret_key>
----

[NOTE]
====
Access keys are obtained when setting up your AWS IAM user.
====

[[aws-applying-configuration-changes]]
== Applying Configuration Changes

Start or restart {product-title} services on all master and node hosts to apply your
configuration changes:

ifdef::openshift-enterprise[]
----
$ systemctl restart atomic-openshift-master-api atomic-openshift-master-controllers
$ systemctl restart atomic-openshift-node
----
endif::[]
ifdef::openshift-origin[]
----
$ systemctl restart origin-master-api origin-master-controllers
$ systemctl restart origin-node
----
endif::[]

Switching from not using a cloud provider to using a cloud provider produces an
error message. Adding the cloud provider tries to delete the node because the
node switches from using the *hostname* as the `*externalID*` (which would have
been the case when no cloud provider was being used) to using the AWS
`*instance-id*` (which is what the AWS cloud provider specifies). To resolve
this issue:

.  Log in to the CLI as a cluster administrator.
. Check and backup existing node labels:
+
[source, bash]
----
$ oc describe node <node_name> | grep -Poz '(?s)Labels.*\n.*(?=Taints)'
----
.  Delete the nodes:
+
[source, bash]
----
$ oc delete node <node_name>
----
.  On each node host, restart the {product-title} service.
+
ifdef::openshift-enterprise[]
----
$ systemctl restart atomic-openshift-node
----
endif::[]
ifdef::openshift-origin[]
----
$ systemctl restart origin-node
----
endif::[]
.  Add back any xref:../admin_guide/manage_nodes.adoc#updating-labels-on-nodes[labels on each node] that you previously had.

[[aws-cluster-labeling]]
== Labeling Clusters for AWS
Starting with {product-title} version 3.7 of the `atomic-openshift-installer`,
if you configured AWS provider credentials, you must also ensure that all
instances are labeled. This topic describes how to label an existing
{product-title} cluster running on AWS.

To correctly identify which resources are associated with a cluster, tag
resources with the key `kubernetes.io/cluster/<name>,Value=<clusterid>`, where:

* `<name>` is a unique name for the cluster.
* `<clusterid>` is a cluster identifier unique to the AWS Availability Zone.

Tagging all resources with the `kubernetes.io/cluster/<name>,Value=<clusterid>`
tag avoids potential issues with multiple zones or multiple clusters.

[NOTE]
====
In versions prior to {product-title} version 3.6, this was
`Key=KubernetesCluster,Value=clusterid`.
====

See xref:../architecture/core_concepts/pods_and_services.adoc#labels[Pods and
Services] to learn more about labeling and tagging in {product-title}.

[[aws-resources-that-need-tags]]
=== Resources That Need Tags
There are four types of resources that need to be tagged:

* Instances
* Security Groups
* Load Balancers
* EBS Volumes

[[aws-tagging-an-existing-cluster]]
=== Tagging an Existing Cluster

A cluster uses the value of the `kubernetes.io/cluster/<name>,Value=<clusterid>` tag to determine which
resources belong to the cluster. Therefore, you must tag all resources with the
key `kubernetes.io/cluster/<name>,Value=<clusterid>` and have the same value for that key.

. Tag all instances with `kubernetes.io/cluster/<name>,Value=<clusterid>` and a value to be used as the cluster ID.
. Tag any security groups with `kubernetes.io/cluster/<name>,Value=<clusterid>` and the same value used for the instances.
. Tag any load balancers with `kubernetes.io/cluster/<name>,Value=<clusterid>` and the same value used for the instances.
. Tag all EBS volumes with `kubernetes.io/cluster/<name>,Value=<clusterid>` and the same value used for the instances. The EBS Volumes that need to be tagged can found with:
+
[source,bash]
----
$ oc get pv -o json|jq '.items[].spec.awsElasticBlockStore.volumeID'
----

. Restart the master services on the master and the node service on all nodes:
+
[source,bash]
----
$ systemctl restart atomic-openshift-master-api atomic-openshift-master-controller
$ systemctl restart atomic-openshift-node
----
