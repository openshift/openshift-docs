[[install-config-configuring-aws]]
= Configuring for AWS
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
{product-title} can be configured to access an
link:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html[AWS EC2 infrastructure], including
xref:../install_config/persistent_storage/persistent_storage_aws.adoc#install-config-persistent-storage-persistent-storage-aws[using AWS
volumes as persistent storage] for application data. After AWS is configured
properly, some additional configurations will need to be completed on the
{product-title} hosts.

[[configuring-aws-variables]]
== Configuring AWS Variables

To set the required AWS variables, create a *_/etc/aws/aws.conf_* file with the
following contents on all of your {product-title} hosts, both masters and nodes:


----
[Global]
Zone = us-east-1c <1>
----
<1> This is the Availability Zone of your AWS Instance and where your EBS Volume
resides; this information is obtained from the AWS Managment Console.


[[aws-configuring-masters]]
== Configuring {product-title} Masters for AWS

You can set the AWS configuration on your {product-title} master hosts in two ways:

- xref:aws-configuring-masters-ansible[using Ansible and the advanced installation tool]
- xref:aws-configuring-masters-manually[manually, by modifying the *_master-config.yaml_* file]

[[aws-configuring-masters-ansible]]
=== Configuring {product-title} for AWS with Ansible

During
xref:../install_config/install/advanced_install.adoc#install-config-install-advanced-install[advanced installations],
AWS can be configured using
xref:../install_config/install/advanced_install.adoc#advanced-install-configuring-global-proxy[the `*openshift_cloudprovider_aws_access_key*`, `*openshift_cloudprovider_aws_secret_key*`, and `*openshift_cloudprovider_kind*` parameters], which are configurable in the inventory file.

.Example AWS Configuration with Ansible

----
# Cloud Provider Configuration
#
# Note: You may make use of environment variables rather than store
# sensitive configuration within the ansible inventory.
# For example:
#openshift_cloudprovider_aws_access_key="{{ lookup('env','AWS_ACCESS_KEY_ID') }}"
#openshift_cloudprovider_aws_secret_key="{{ lookup('env','AWS_SECRET_ACCESS_KEY') }}"
#
# AWS (Using API Credentials)
#openshift_cloudprovider_kind=aws
#openshift_cloudprovider_aws_access_key=aws_access_key_id
#openshift_cloudprovider_aws_secret_key=aws_secret_access_key
#
# AWS (Using IAM Profiles)
#openshift_cloudprovider_kind=aws
# Note: IAM roles must exist before launching the instances.
----

[NOTE]
====
When Ansible configures AWS, the following files are created for you:

- *_/etc/aws/aws.conf_*
- *_/etc/origin/master/master-config.yaml_*
- *_/etc/origin/node/node-config.yaml_*
- *_/etc/sysconfig/atomic-openshift-master_*
- *_/etc/sysconfig/atomic-openshift-node_*
====

[[aws-configuring-masters-manually]]
=== Manually Configuring {product-title} Masters for AWS

Edit or
xref:../install_config/master_node_configuration.adoc#creating-new-configuration-files[create]
the master configuration file on all masters
(*_/etc/origin/master/master-config.yaml_* by default) and update the contents
of the `*apiServerArguments*` and `*controllerArguments*` sections:

[source,yaml]
----
kubernetesMasterConfig:
  ...
  apiServerArguments:
    cloud-provider:
      - "aws"
    cloud-config:
      - "/etc/aws/aws.conf"
  controllerArguments:
    cloud-provider:
      - "aws"
    cloud-config:
      - "/etc/aws/aws.conf"
----

[IMPORTANT]
====
When triggering a containerized installation, only the directories of
*_/etc/origin_* and *_/var/lib/origin_* are mounted to the master and node
container. Therefore, *_aws.conf_* should be in *_/etc/origin/_* instead of
*_/etc/_*.
====

[[aws-configuring-nodes]]
=== Manually Configuring {product-title} Nodes for AWS

Edit or
xref:../install_config/master_node_configuration.adoc#creating-new-configuration-files[create]
the node configuration file on all nodes (*_/etc/origin/node/node-config.yaml_*
by default) and update the contents of the `*kubeletArguments*` section:

[source,yaml]
----
kubeletArguments:
  cloud-provider:
    - "aws"
  cloud-config:
    - "/etc/aws/aws.conf"
----

[IMPORTANT]
====
When triggering a containerized installation, only the directories of
*_/etc/origin_* and *_/var/lib/origin_* are mounted to the master and node
container. Therefore, *_aws.conf_* should be in *_/etc/origin/_* instead of
*_/etc/_*.
====

[[aws-setting-key-value-access-pairs]]
== Setting Key Value Access Pairs

Make sure the following environment variables are set in the
ifdef::openshift-enterprise[]
*_/etc/sysconfig/atomic-openshift-master_* file on masters and the
*_/etc/sysconfig/atomic-openshift-node_* file on nodes:
endif::[]
ifdef::openshift-origin[]
*_/etc/sysconfig/origin-master_* file on masters and the
*_/etc/sysconfig/origin-node_* file on nodes:
endif::[]

----
AWS_ACCESS_KEY_ID=<key_ID>
AWS_SECRET_ACCESS_KEY=<secret_key>
----

[NOTE]
====
Access keys are obtained when setting up your AWS IAM user.
====

[[aws-applying-configuration-changes]]
== Applying Configuration Changes

Start or restart {product-title} services on all master and node hosts to apply your
configuration changes:

ifdef::openshift-enterprise[]
----
$ systemctl restart atomic-openshift-master
$ systemctl restart atomic-openshift-node
----
endif::[]
ifdef::openshift-origin[]
----
$ systemctl restart origin-master
$ systemctl restart origin-node
----
endif::[]

Switching from not using a cloud provider to using a cloud provider produces an
error message. Adding the cloud provider tries to delete the node because the
node switches from using the *hostname* as the `*externalID*` (which would have
been the case when no cloud provider was being used) to using the AWS
`*instance-id*` (which is what the AWS cloud provider specifies). To resolve
this issue:

.  Log in to the CLI as a cluster administrator.
. Check and backup existing node labels:
+
[source, bash]
----
$ oc describe node <node_name> | grep -Poz '(?s)Labels.*\n.*(?=Taints)'
----
.  Delete the nodes:
+
[source, bash]
----
$ oc delete node <node_name>
----
.  On each node host, restart the {product-title} service.
+
ifdef::openshift-enterprise[]
----
$ systemctl restart atomic-openshift-node
----
endif::[]
ifdef::openshift-origin[]
----
$ systemctl restart origin-node
----
endif::[]
.  Add back any xref:../admin_guide/manage_nodes.adoc#updating-labels-on-nodes[labels on each node] that you previously had.
