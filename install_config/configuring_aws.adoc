[[install-config-configuring-aws]]
= Configuring for Amazon Web Services (AWS)
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
{product-title} can be configured to access an
link:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html[AWS EC2
infrastructure], including
xref:../install_config/persistent_storage/persistent_storage_aws.adoc#install-config-persistent-storage-persistent-storage-aws[using
AWS volumes as persistent storage] for application data. After you configure AWS, some additional configurations must be completed on the
{product-title} hosts.

[[configuring-aws-permissions]]
== Permissions
Configuring AWS for {product-title} requires the following permissions:

.Master Permissions
[cols='1,2']
|===

| Elastic Compute Cloud(EC2)
|`ec2:DescribeVolume`, `ec2:CreateVolume`, `ec2:CreateTags`,
`ec2:DescribeInstance`, `ec2:AttachVolume`, `ec2:DetachVolume`,
`ec2:DeleteVolume`, `ec2:DescribeSubnets`, `ec2:CreateSecurityGroup`,
`ec2:DescribeSecurityGroups`, `ec2:DescribeRouteTables`,
`ec2:AuthorizeSecurityGroupIngress`, `ec2:RevokeSecurityGroupIngress`, `ec2:DeleteSecurityGroup`

| Elastic Load Balancing
| `elasticloadbalancing:DescribeTags`,
`elasticloadbalancing:CreateLoadBalancerListeners`,
`elasticloadbalancing:ConfigureHealthCheck`,
`elasticloadbalancing:DeleteLoadBalancerListeners`,
`elasticloadbalancing:RegisterInstancesWithLoadBalancer`,
`elasticloadbalancing:DescribeLoadBalancers`,
`elasticloadbalancing:CreateLoadBalancer`,
`elasticloadbalancing:DeleteLoadBalancer`,
`elasticloadbalancing:ModifyLoadBalancerAttributes`,
`elasticloadbalancing:DescribeLoadBalancerAttributes`

|===

.Node Permissions
[cols='1,2']
|===

| Elastic Compute Cloud(EC2)
| `ec2:DescribeInstance*`

|===

[IMPORTANT]
====
* Every master host, node host, and subnet must have the `kubernetes.io/cluster/<name>,Value=<clusterid>` tag.
* One security group, preferably the one linked to the nodes, must have the
`kubernetes.io/cluster/<name>,Value=<clusterid>` tag.
 ** Do not tag all security groups with the `kubernetes.io/cluster/<name>,Value=<clusterid>` tag or the
Elastic Load Balancing (ELB) will not be able to create a load balancer.

====

[[configuring-a-security-group-aws]]
== Configuring a Security Group
When installing {product-title} on AWS, ensure that you set up the appropriate
security groups.
include::install_config/topics/configuring_a_security_group.adoc[]

[[overriding-detected-ip-addresses-host-names-aws]]
=== Overriding Detected IP Addresses and Host Names
In AWS, situations that require overriding the variables include:

[cols="1,2"options="header"]
|===
|Variable |Usage

|`*hostname*`
a|The user is installing in a VPC that is not configured for both `*DNS hostnames*` and `*DNS resolution*`.

|`*ip*`
a|You have multiple network interfaces configured and want to
use one other than the default. You must also set the
`*openshift_set_node_ip*` parameter to `True`, or the SDN attempts to
use the `*hostname*` setting or tries to resolve the host name for the IP address.

|`*public_hostname*`
a| - A master instance where the VPC subnet is not configured for `*Auto-assign
Public IP*`. For external access to this master, you need to have an ELB or
other load balancer configured that would provide the external access needed, or
you need to connect over a VPN connection to the internal name of the host.
- A master instance where metadata is disabled.
- This value is not actually used by the nodes.

|`*public_ip*`
a| - A master instance where the VPC subnet is not configured for `*Auto-assign Public IP*`.
- A master instance where metadata is disabled.
- This value is not actually used by the nodes.

|===

[WARNING]
====
If `*openshift_hostname*` is set to a value other than the metadata-provided
`*private-dns-name*` value, the native cloud integration for those providers
will no longer work.
====

For EC2 hosts in particular, they must be deployed in a VPC that has both
`*DNS host names*` and `*DNS resolution*` enabled, and `*openshift_hostname*`
should not be overridden.

[[configuring-aws-variables]]
== Configuring AWS Variables

To set the required AWS variables, create a *_/etc/origin/cloudprovider/aws.conf_* file with the
following contents on all of your {product-title} hosts, both masters and nodes:


----
[Global]
Zone = us-east-1c <1>
----
<1> This is the Availability Zone of your AWS Instance and where your EBS Volume
resides; this information is obtained from the AWS Managment Console.


[[aws-configuring-masters]]
== Configuring {product-title} Masters for AWS

You can set the AWS configuration on your {product-title} master hosts in two ways:

- xref:aws-configuring-masters-ansible[using Ansible]
- xref:aws-configuring-masters-manually[manually, by modifying the *_master-config.yaml_* file]

[[aws-configuring-masters-ansible]]
=== Configuring {product-title} for AWS with Ansible

During cluster installations,  AWS can be configured using the
`openshift_cloudprovider_aws_access_key`,
`openshift_cloudprovider_aws_secret_key`, `openshift_cloudprovider_kind`,
`openshift_clusterid` parameters, which are configurable in the
xref:../install/configuring_inventory_file.adoc#configuring-ansible[inventory file].

.Example AWS Configuration with Ansible

----
# Cloud Provider Configuration
#
# Note: You may make use of environment variables rather than store
# sensitive configuration within the ansible inventory.
# For example:
#openshift_cloudprovider_aws_access_key="{{ lookup('env','AWS_ACCESS_KEY_ID') }}"
#openshift_cloudprovider_aws_secret_key="{{ lookup('env','AWS_SECRET_ACCESS_KEY') }}"
#
#openshift_clusterid=unique_identifier_per_availablility_zone
#
# AWS (Using API Credentials)
#openshift_cloudprovider_kind=aws
#openshift_cloudprovider_aws_access_key=aws_access_key_id
#openshift_cloudprovider_aws_secret_key=aws_secret_access_key
#
# AWS (Using IAM Profiles)
#openshift_cloudprovider_kind=aws
# Note: IAM roles must exist before launching the instances.
----

[NOTE]
====
When Ansible configures AWS, the following files are created for you:

- *_/etc/origin/cloudprovider/aws.conf_*
- *_/etc/origin/master/master-config.yaml_*
- *_/etc/origin/node/node-config.yaml_*
- *_/etc/sysconfig/atomic-openshift-master-api_*
- *_/etc/sysconfig/atomic-openshift-master-controllers_*
- *_/etc/sysconfig/atomic-openshift-node_*
====

[[aws-configuring-masters-manually]]
=== Manually Configuring {product-title} Masters for AWS

Edit or
xref:../install_config/master_node_configuration.adoc#creating-new-configuration-files[create]
the master configuration file on all masters
(*_/etc/origin/master/master-config.yaml_* by default) and update the contents
of the `*apiServerArguments*` and `*controllerArguments*` sections:

[source,yaml]
----
kubernetesMasterConfig:
  ...
  apiServerArguments:
    cloud-provider:
      - "aws"
    cloud-config:
      - "/etc/origin/cloudprovider/aws.conf"
  controllerArguments:
    cloud-provider:
      - "aws"
    cloud-config:
      - "/etc/origin/cloudprovider/aws.conf"
----

Currently, the `nodeName` *must* match the instance name in AWS in order
for the cloud provider integration to work properly.  The name must also be
RFC1123 compliant.

[IMPORTANT]
====
When triggering a containerized installation, only the directories of
*_/etc/origin_* and *_/var/lib/origin_* are mounted to the master and node
container. Therefore, *_aws.conf_* should be in *_/etc/origin/_* instead of
*_/etc/_*.
====

[[aws-configuring-nodes]]
=== Manually Configuring {product-title} Nodes for AWS

Edit or
xref:../install_config/master_node_configuration.adoc#creating-new-configuration-files[create]
the node configuration file on all nodes (*_/etc/origin/node/node-config.yaml_*
by default) and update the contents of the `*kubeletArguments*` section:

[source,yaml]
----
kubeletArguments:
  cloud-provider:
    - "aws"
  cloud-config:
    - "/etc/origin/cloudprovider/aws.conf"
----

[IMPORTANT]
====
When triggering a containerized installation, only the directories of
*_/etc/origin_* and *_/var/lib/origin_* are mounted to the master and node
container. Therefore, *_aws.conf_* should be in *_/etc/origin/_* instead of
*_/etc/_*.
====

[[aws-setting-key-value-access-pairs]]
== Setting Key Value Access Pairs

Make sure the following environment variables are set in the
ifdef::openshift-enterprise[]
*_/etc/sysconfig/atomic-openshift-master-api_* file and
*_/etc/sysconfig/atomic-openshift-master-controllers_* file on masters and the
*_/etc/sysconfig/atomic-openshift-node_* file on nodes:
endif::[]
ifdef::openshift-origin[]
*_/etc/sysconfig/origin-master-api_* file and
*_/etc/sysconfig/origin-master-controllers_* file on masters and the
*_/etc/sysconfig/origin-node_* file on nodes:
endif::[]

----
AWS_ACCESS_KEY_ID=<key_ID>
AWS_SECRET_ACCESS_KEY=<secret_key>
----

[NOTE]
====
Access keys are obtained when setting up your AWS IAM user.
====

[[aws-applying-configuration-changes]]
== Applying Configuration Changes
include::install_config/topics/applying_configuration_changes.adoc[]

[[aws-cluster-labeling]]
== Labeling Clusters for AWS
Starting with {product-title} version 3.7 of the `atomic-openshift-installer`,
if you configured AWS provider credentials, you must also ensure that all
hosts are labeled.

To correctly identify which resources are associated with a cluster, tag
resources with the key `kubernetes.io/cluster/<clusterid>`, where:

* `<clusterid>` is a unique name for the cluster.

Set the corresponding value to `owned` if the node belongs exclusively to the 
cluster or to `shared` if it is a resource shared with other systems.

Tagging all resources with the `kubernetes.io/cluster/<clusterid>,Value=(owned|shared)`
tag avoids potential issues with multiple zones or multiple clusters.

[NOTE]
====
In versions prior to {product-title} version 3.6, this was
`Key=KubernetesCluster,Value=clusterid`.
====

See xref:../architecture/core_concepts/pods_and_services.adoc#labels[Pods and
Services] to learn more about labeling and tagging in {product-title}.

[[aws-resources-that-need-tags]]
=== Resources That Need Tags
There are four types of resources that need to be tagged:

* Instances
* Security Groups
* Load Balancers
* EBS Volumes

[[aws-tagging-an-existing-cluster]]
=== Tagging an Existing Cluster

A cluster uses the value of the
`kubernetes.io/cluster/<clusterid>,Value=(owned|shared)` tag to determine which
resources belong to the AWS cluster. This means that all relevant resources must
be labeled with the `kubernetes.io/cluster/<clusterid>,Value=(owned|shared)` 
tag using the same values for that key. These resources include:

* All hosts.
* All relevant load balancers to be used in the AWS instances.
* All EBS volumes. The EBS Volumes that need to be tagged can found with:
+
[source,bash]
----
$ oc get pv -o json|jq '.items[].spec.awsElasticBlockStore.volumeID'
----

* All relevant security groups to be used with the AWS instances.
+
[NOTE]
====
Do not tag all existing security groups with the
`kubernetes.io/cluster/<name>,Value=<clusterid>` tag, or the Elastic Load
Balancing (ELB) will not be able to create a load balancer.
====

After tagging any resources, restart the master services on the master and the node service on all nodes. See the xref:aws-applying-configuration-changes[Applying Configuration Section].
