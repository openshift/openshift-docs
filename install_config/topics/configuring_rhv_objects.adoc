////
Module included in the following assemblies:

install_config/configuring_rhv.adoc
////

[id='configuring-rhv-objects_{context}']
= Configuring Red Hat Virtualization

To integrate {product-title} with Red Hat Virtualization, perform the following actions as part of your xref:../install/host_preparation.adoc#install-config-install-host-preparation[host preparation].

== Creating the bastion virtual machine

The bastion virtual machine is used to install {product-title} on Red Hat Virtualization.

.Procedure

. Log in to the Manager machine using SSH.
. Create a temporary bastion installation directory, for example, *_/bastion_installation_*, for the installation files.
. Create an encrypted file, *_/bastion_installation/secure_vars.yaml_*:
+
[options="nowrap" subs="+quotes,verbatim"]
----
# ansible-vault create secure_vars.yaml
----

. Add the following variables to *_secure_vars.yaml_*:
+
[options="nowrap" subs="+quotes,verbatim"]
----
engine_password: <Manager_password>
bastion_root_password: <bastion_root_password>
rh_password: <Red_Hat_Subscription_Manager_password>
root_password: <VM_root_password>
----

. Save the file and record the vault password.

. Create the installation playbook, *_/bastion_installation/create-bastion-machine-playbook.yaml_*, and update its parameters:
+
[source,yml]
----
---
- name: Create a bastion machine
  hosts: localhost
  connection: local
  gather_facts: false

  vars:
    engine_url: https://_Manager_FQDN_/ovirt-engine/api
    engine_user: <admin@internal>
    engine_password: "{{ engine_password }}"
    engine_cafile: /etc/pki/ovirt-engine/ca.pem

    qcow_url: <1>
    template_cluster: Default
    template_name: rhelguest7
    template_memory: 4GiB
    template_cpu: 2
    wait_for_ip: True

    vms:
      name: rhel-bastion
      cluster: Default
      profile:
        cores: 2
        template: "{{ template_name }}"
        root_password: "{{ root_password }}"
        ssh_key: "{{ lookup('file', '~/.ssh/id_rsa.pub') }}"
        state: running
    cloud_init:
      custom_script: |
        rh_subscription:
          username: <Red_Hat_Subscription_Manager_username>
          password: "{{ rh_password }}"
          auto-attach: True
          # 'rhel-7-server-rhv-4.2-manager-rpms' supports RHV 4.2 and 4.3
          enable-repo: ['rhel-7-server-rpms', 'rhel-7-server-extras-rpms', 'rhel-7-server-ansible-2.7-rpms', 'rhel-7-server-ose-3.11-rpms', 'rhel-7-server-supplementary-rpms', 'rhel-7-server-rhv-4.2-manager-rpms']
        packages:
          - ansible
          - ovirt-ansible-roles
          - openshift-ansible
          - ovirt-engine-sdk-python
  roles:
    - oVirt.image-template
    - oVirt.vm-infra
- name: post installation tasks on the bastion machine
  hosts: rhel-bastion
  tasks:
    - name: create ovirt-engine pki dir
      file:
        state: directory
        dest: /etc/pki/ovirt-engine/
    - name: Copy the engine ca cert to the bastion machine
      copy:
        src: /etc/pki/ovirt-engine/ca.pem
        dest: /etc/pki/ovirt-engine/ca.pem
    - name: Copy the secured vars to the bastion machine
      copy:
        src: secure_vars.yaml
        dest: secure_vars.yaml
----

<1> `<qcow_url>` is the URL for downloading the *_RHEL-guest-image_*. You can copy the URL from link:https://access.redhat.com/downloads/content/69/ver=/rhel---7/latest/x86_64/product-software[]:
.. In the *Product Software* tab, locate the *_Red Hat Enterprise Linux KVM Guest Image_*.
.. Right-click the *Download Now* button and copy the URL.
+
[NOTE]
====
This link expires after some time. Save the link when you are ready to run the playbook.
====

. Create the bastion virtual machine:
+
[options="nowrap" subs="+quotes,verbatim"]
----
# ansible-playbook -i localhost create-bastion-machine-playbook.yaml -e @secure_vars.yaml --ask-vault-pass
----

. Verify that the bastion virtual machine, *_rhel-bastion_*, was created successfully in the Administration Portal.

== Installing {product-title} on Red Hat Virtualization

You can install {product-title} using the bastion virtual machine:

. Log in to *_rhel-bastion_*.

. Create *_vars.yaml_* with the following content and update the parameters:
+
[source,yml]
----
---
# For more comprehensive list of arguments please see
# openshift_ovirt role args   - https://github.com/openshift/openshift-ansible/tree/master/roles/openshift_ovirt#role-variables
# openshift installation args - https://github.com/openshift/openshift-ansible/tree/master/inventory
engine_url: https://<Manager_VQDN>/ovirt-engine/api
engine_user: admin@internal
engine_password: "{{ engine_password }}"
engine_insecure: false
engine_cafile: '/etc/pki/ovirt-engine/ca.pem'

openshift_ovirt_vm_manifest:
  - name: 'master'
    count: 1
    profile: 'master_vm'
  - name: 'node'
    count: 0
    profile: 'node_vm'
  - name: 'lb'
    count: 0
    profile: 'node_vm'

# Set 'openshift_ovirt_all_in_one' to 'false' to install the master and node VMs separately.
openshift_ovirt_all_in_one: true
openshift_ovirt_cluster: Default
openshift_ovirt_data_store: data
openshift_ovirt_ssh_key: "{{ lookup('file', 'id_rsa.pub') }}"

public_hosted_zone:
# Uncomment to disable install-time checks, for smaller scale installations
#openshift_disable_check: memory_availability,disk_availability,docker_image_availability

qcow_url: <1>
image_path: /var/tmp
template_name: rhel7
template_cluster: "{{ openshift_ovirt_cluster }}"
template_memory: 4GiB
template_cpu: 1
template_disk_storage: "{{ openshift_ovirt_data_store }}"
template_disk_size: 100GiB
template_nics:
  - name: nic1
    profile_name: ovirtmgmt
    interface: virtio

debug_vm_create: true
wait_for_ip: true
vm_infra_wait_for_ip_retries: 30
vm_infra_wait_for_ip_delay: 20

openshift_ovirt_vm_profile:
  master_vm:
    cluster: "{{ openshift_ovirt_cluster }}"
    template: "{{ template_name }}"
    memory: "{{ vm_memory | default('16GiB') }}"
    cores: "{{ vm_cores | default(4) }}"
    high_availability: true
    state: running
    cloud_init:
      root_password: {{ root_password }}
      authorized_ssh_keys: "{{ openshift_ovirt_ssh_key }}"
      custom_script: "{{ cloud_init_script_master }}"

##########################
# Cloud Init Script
##########################
# Use the following if RHEL 7.4 (or earlier) VMs are being created on a RHV 4.2 (or later) engine
#    - sed -i 's@^# device =.*@device = /dev/virtio-ports/ovirt-guest-agent.0@' /etc/ovirt-guest-agent.conf
#    - sed -i 's@com.redhat.rhevm.vdsm@ovirt-guest-agent.0@' /etc/udev/rules.d/55-ovirt-guest-agent.rules
#    - 'udevadm trigger --subsystem-match="virtio-ports"'

cloud_init_script_master: |
  packages:
    - ovirt-guest-agent
  runcmd:
    - sed -i 's/# ignored_nics =.*/ignored_nics = docker0, tun0 /' /etc/ovirt-guest-agent.conf
    - systemctl enable ovirt-guest-agent
    - systemctl start ovirt-guest-agent
  power_state:
    mode: reboot
    message: cloud init finished - boot and install openshift
    condition: True
----

<1> `<qcow_url>` is the URL for downloading the *_RHEL-guest-image_*. You can copy the URL from link:https://access.redhat.com/downloads/content/69/ver=/rhel---7/latest/x86_64/product-software[]:
.. In the *Product Software* tab, locate the *_Red Hat Enterprise Linux KVM Guest Image_*.
.. Right-click the *Download Now* button and copy the URL.
+
[NOTE]
====
This link expires after some time. Save the link when you are ready to run the playbook.
====

. Create *_install_okd.yaml_* with the following content:
+
[source,yml]
----
---
- name: Openshift Origin on oVirt
  hosts: localhost
  connection: local
  gather_facts: false

  vars_files:
    - vars.yaml
    - secure_vars.yaml

  pre_tasks:
    - ovirt_auth:
        url:      "{{ engine_url }}"
        username: "{{ engine_user }}"
        password: "{{ engine_password }}"
        insecure: "{{ engine_insecure }}"
        ca_file:  "{{ engine_username | default(omit) }}"

  roles:
    - role: openshift_ovirt

- import_playbook: setup_dns.yaml
- import_playbook: playbooks/prerequisites.yml
- import_playbook: playbooks/openshift-node/network_manager.yml
- import_playbook: playbooks/deploy_cluster.yml
----

. Create *_setup_dns.yaml_* with the following content:
+
[source,yml]
----
- hosts: masters
  strategy: free
  tasks:
    - shell: "echo {{ ansible_default_ipv4.address }} {{ inventory_hostname }} etcd.{{ inventory_hostname.split('.', 1)[1] }} openshift-master.{{ inventory_hostname.split('.', 1)[1] }} openshift-public-master.{{ inventory_hostname.split('.', 1)[1] }} docker-registry-default.apps.{{ inventory_hostname.split('.', 1)[1] }} webconsole.openshift-web-console.svc registry-console-default.apps.{{ inventory_hostname.split('.', 1)[1] }} >> /etc/hosts"
      when: openshift_ovirt_all_in_one is defined | ternary((openshift_ovirt_all_in_one | bool), false)
----

. Create an Ansible inventory file, *_/etc/ansible/openshift_3_11.hosts_*, with the following content:
+
[source,yaml]
----
[workstation]
localhost ansible_connection=local

[all:vars]
openshift_ovirt_dns_zone="{{ public_hosted_zone }}"
openshift_ovirt_data_store=filedomain2
openshift_ovirt_ssh_key="{{ lookup('file', '~/.ssh/id_rsa.pub') }}"
openshift_ovirt_cluster=Default
openshift_web_console_install=true
openshift_master_overwrite_named_certificates=true
openshift_master_cluster_hostname="openshift-master.{{ public_hosted_zone }}"
openshift_master_cluster_public_hostname="openshift-public-master.{{ public_hosted_zone }}"
openshift_master_default_subdomain="{{ public_hosted_zone }}"
openshift_public_hostname="{{openshift_master_cluster_public_hostname}}"
openshift_deployment_type=openshift-enterprise
openshift_service_catalog_image_version="{{ openshift_image_tag }}"

[OSEv3:vars]
# General variables
debug_level=1
containerized=False
ansible_ssh_user=root
os_firewall_use_firewalld=True
openshift_enable_excluders=false
openshift_install_examples=false
openshift_clock_enabled=true
openshift_debug_level="{{ debug_level }}"
openshift_node_debug_level="{{ node_debug_level | default(debug_level,true) }}"
osn_storage_plugin_deps=[]
openshift_master_bootstrap_auto_approve=true
openshift_master_bootstrap_auto_approver_node_selector={"node-role.kubernetes.io/master":"true"}
osm_controller_args={"experimental-cluster-signing-duration": ["20m"]}
osm_default_node_selector="node-role.kubernetes.io/compute=true"
openshift_enable_service_catalog=False

# Docker
#container_runtime_docker_storage_setup_device=/dev/vdb
container_runtime_docker_storage_type=overlay2
openshift_docker_use_system_container=False

# ANSIBLE BROKER
ansible_service_broker_etcd_image_prefix=quay.io/coreos/
ansible_service_broker_registry_type=quay
ansible_service_broker_registry_name=quay.io
ansible_service_broker_registry_url=https://quay.io
ansible_service_broker_registry_user=
ansible_service_broker_registry_password=
ansible_service_broker_registry_organization=
ansible_service_broker_registry_tag=latest
ansible_service_broker_registry_whitelist=[.*-apb$]
ansible_service_broker_registry_blacklist=[.*automation-broker-apb$]

[OSEv3:children]
nodes
masters
etcd
lb

[masters]
[nodes]
[etcd]
[lb]
----

. Export the environment variables and run the {product-title} installation playbook:
+
[options="nowrap" subs="+quotes,verbatim"]
----
# export ANSIBLE_ROLES_PATH="/usr/share/ansible/roles/:/usr/share/ansible/openshift-ansible/roles"
# export ANSIBLE_JINJA2_EXTENSIONS="jinja2.ext.do"
# ansible-playbook -i /etc/ansible/openshift_3_11.hosts install_okd.yaml -e @vars.yaml -e @secure_args.yaml --ask-vault-pass
----

. Create DNS entries for the routers. Provide entries for all infrastructure instances and configure a round-robin strategy so that the router can pass traffic to applications.

. Create a DNS entry for the {product-title} web console. Specify the IP address of the load balancer node.

. Continue to install the cluster following the xref:../install/running_install.adoc#install-running-installation-playbooks[Installing {product-title}] steps. During that process, make any changes to your inventory file that your cluster needs.
