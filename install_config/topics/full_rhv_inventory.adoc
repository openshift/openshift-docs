////
Module included in the following assemblies:

install_config/configuring_rhv.adoc
////

= Configuring {product-title} for Red Hat Virtualization

You configure {product-title} for Red Hat Virtualization by modifying the
Ansible inventory file before you install the cluster.

// Check whether this needs to be updated

. Modify the Ansible inventory file, located at
*_/etc/ansible/hosts_* by default, to use the following YAML sections:
+
[source,yaml]
----
[OSEv3:children]
nodes
masters
etcd
glusterfs_registry
lb

[OSEv3:vars]
# General variables
ansible_ssh_user=root
openshift_deployment_type=openshift-enterprise
oreg_auth_user=service_account <1>
oreg_auth_password=service_account_token <1>
openshift_master_cluster_method=native
debug_level=2
openshift_debug_level="{{ debug_level }}"
openshift_node_debug_level="{{ node_debug_level | default(debug_level, true) }}"
openshift_enable_service_catalog=False

app_dns_prefix=apps
public_hosted_zone=example.com
load_balancer_hostname=lb.{{public_hosted_zone}}
openshift_master_cluster_hostname="{{ load_balancer_hostname }}"
openshift_master_cluster_public_hostname="{{ load_balancer_hostname }}"
openshift_master_default_subdomain="{{ app_dns_prefix }}.{{ public_hosted_zone }}"

# Pod Networking
os_sdn_network_plugin_name=redhat/openshift-ovs-networkpolicy

# Registry
openshift_hosted_registry_storage_kind=glusterfs

# Authentication (example here creates one user, myuser with password changeme)
openshift_master_identity_providers="[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'True', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]"
openshift_master_htpasswd_users={'myuser': '$apr1$zAhyA9Ko$rBxBOwAwwtRuuaw8OtCwH0'}

# Docker and extra file system setup
container_runtime_docker_storage_setup_device=/dev/vdb
container_runtime_docker_storage_type=overlay2
openshift_docker_use_system_container=False
openshift_node_local_quota_per_fsgroup=512Mi <2>
openshift_use_system_containers=False

[masters]
master0.example.com
master1.example.com
master2.example.com

[etcd]
master0.example.com
master1.example.com
master2.example.com

[infras]
infra0.example.com
infra1.example.com
infra2.example.com

[glusterfs_registry]
infra0.example.com glusterfs_devices="['/dev/vdd']"
infra1.example.com glusterfs_devices="['/dev/vdd']"
infra2.example.com glusterfs_devices="['/dev/vdd']"

[lb]
lb.example.com

[nodes]
master0.example.com openshift_node_group_name=node-config-master
master1.example.com openshift_node_group_name=node-config-master
master2.example.com openshift_node_group_name=node-config-master
infra0.example.com openshift_node_group_name=node-config-infra
infra1.example.com openshift_node_group_name=node-config-infra
infra2.example.com openshift_node_group_name=node-config-infra
app0.example.com openshift_node_group_name=node-config-compute
app1.example.com openshift_node_group_name=node-config-compute
app2.example.com openshift_node_group_name=node-config-compute
----
<1> If you use a container registry that requires authentication, such as the
default container image registry, specify the credentials for that account. See
xref:../install_config/configuring_red_hat_registry.html#install-config-configuring-red-hat-registry[Accessing and Configuring the Red Hat Registry].
<2>  If you use the `openshift_node_local_quota_per_fsgroup` parameter, you must
specify the partition or LVM to use for the directory of
`/var/lib/origin/openshift.local.volumes`. The partition must be
mounted with the option of `gquota` in fstab.
+
--
This inventory file uses the following nodes and disks:

* One load balancer instance
* Three master instances
** Extra disks attached: 15 GB for the container image registry, 30 GB for local volume storage, and 25 GB for etcd
* Three infrastructure instances
** Extra disks attached: 15 GB for Docker, 30 GB for local volume storage, and,
because this cluster uses GlusterFS for persistent storage, 25 GB for a GlusterFS registry
* One or more application instance
** Extra disks attached: 15 GB for Docker, 30 GB for local volume storage
--
. Continue to install the cluster following the xref:../install/running_install.adoc#install-running-installation-playbooks[Installing {product-title}]
steps. During that process, make any changes to your inventory file that your
cluster needs.
