// Module included in the following assemblies:
//
// * install_config/configuring_for_vsphere.adoc

[id='vsphere-configuring-masters-manual_{context}']
= Option 2: Manually configuring {product-title} for vSphere

== Manually configuring master hosts for vSphere

Perform the following on all master hosts.

.Procedure

. Edit the master configuration file at *_/etc/origin/master/master-config.yaml_*
by default on all masters and update the contents of the `apiServerArguments`
and `controllerArguments` sections:
+
[source,yaml]
----
kubernetesMasterConfig:
  ...
  apiServerArguments:
    cloud-provider:
      - "vsphere"
    cloud-config:
      - "/etc/origin/cloudprovider/vsphere.conf"
  controllerArguments:
    cloud-provider:
      - "vsphere"
    cloud-config:
      - "/etc/origin/cloudprovider/vsphere.conf"
----
+
[IMPORTANT]
====
When triggering a containerized installation, only the *_/etc/origin_* and
*_/var/lib/origin_* directories are mounted to the master and node container.
Therefore, *_master-config.yaml_* must be in *_/etc/origin/master_* rather than
*_/etc/_*.
====

. When you configure {product-title} for vSphere using Ansible, the
*_/etc/origin/cloudprovider/vsphere.conf_* file is created automatically.
Because you are manually configuring {product-title} for vSphere, you must
create the file. Before you create the file, decide if you want multiple vCenter
zones or not.
+
The cluster installation process configures single-zone or single vCenter by default.
However, deploying {product-title} in vSphere on different zones can be helpful to avoid
single-point-of-failures, but creates the need for shared storage across zones.
If an {product-title} node host goes down in zone "A" and the pods
should be moved to zone "B".
See https://kubernetes.io/docs/setup/best-practices/multiple-zones/[Running in multiple zones] in the Kubernetes documentation for more information.

** To configure a single vCenter server, use the following format for the
*_/etc/origin/cloudprovider/vsphere.conf_* file:
+
[subs=+quotes]
----
[Global] <1>
        user = "myusername" <2>
        password = "mypassword" <3>
        port = "443" <4>
        insecure-flag = "1" <5>
        datacenters = "mydatacenter" <6>

[VirtualCenter "10.10.0.2"] <7>
        user = "myvCenterusername"
        password = "password"

[Workspace] <8>
        server = "10.10.0.2" <9>
        datacenter = "mydatacenter"
        folder = "path/to/vms" <10>
        default-datastore = "shared-datastore" <11>
        resourcepool-path = "myresourcepoolpath" <12>

[Disk]
        scsicontrollertype = pvscsi <13>

[Network]
        public-network = "VM Network" <14>
----
<1> Any properties set in the `[Global]` section are used for all specified vcenters unless overriden by the settings in the individual `[VirtualCenter]` sections.
<2> vCenter username for the vSphere cloud provider.
<3> vCenter password for the specified user.
<4> Optional. Port number for the vCenter server. Defaults to port `443`.
<5> Set to `1` if the vCenter uses a self-signed certificate.
<6> Name of the data center on which Node VMs are deployed.
<7> Override specific `[Global]` properties for this Virtual Center. Possible setting scan be `[Port]`, `[user]`, `[insecure-flag]`, `[datacenters]`. Any settings not specified are pulled from the `[Global]` section.
<8> Set any properties used for various vSphere Cloud Provider functionality. For example, dynamic provisioning, Storage Profile Based Volume provisioning, and others.
<9> IP Address or FQDN for the vCenter server.
<10> Path to the VM directory for node VMs.
<11> Set to the name of the datastore to use for provisioning volumes using the storage classes or dynamic provisioning. Prior to {product-title} 3.9, if the datastore was located in a storage directory or is a member of a datastore cluster, the full path was required.
<12> Optional. Set to the path to the resource pool where dummy VMs for Storage Profile Based volume provisioning must be created.
<13> Type of SCSI controller the VMDK will be attached to the VM as.
<14> Set to the network port group for vSphere to access the node, which is called VM Network by default. This is the node host's ExternalIP that is registered with Kubernetes.

** To configure a multiple vCenter servers, use the following format for the
*_/etc/origin/cloudprovider/vsphere.conf_* file:
+
[subs=+quotes]
----
[Global] <1>
        user = "myusername" <2>
        password = "mypassword" <3>
        port = "443" <4>
        insecure-flag = "1" <5>
        datacenters = "us-east, us-west" <6>

[VirtualCenter "10.10.0.2"] <7>
        user = "myvCenterusername"
        password = "password"

[VirtualCenter "10.10.0.3"]
        port = "448"
        insecure-flag = "0"

[Workspace] <8>
        server = "10.10.0.2" <9>
        datacenter = "mydatacenter"
        folder = "path/to/vms" <10>
        default-datastore = "shared-datastore" <11>
        resourcepool-path = "myresourcepoolpath" <12>

[Disk]
        scsicontrollertype = pvscsi <13>

[Network]
        public-network = "VM Network" <14>
----
<1> Any properties set in the `[Global]` section are used for all specified vcenters unless overriden by the settings in the individual `[VirtualCenter]` sections.
<2> vCenter username for the vSphere cloud provider.
<3> vCenter password for the specified user.
<4> Optional. Port number for the vCenter server. Defaults to port `443`.
<5> Set to `1` if the vCenter uses a self-signed certificate.
<6> Name of the data centers on which Node VMs are deployed.
<7> Override specific `[Global]` properties for this Virtual Center. Possible setting scan be `[Port]`, `[user]`, `[insecure-flag]`, `[datacenters]`. Any settings not specified are pulled from the `[Global]` section.
<8> Set any properties used for various vSphere Cloud Provider functionality. For example, dynamic provisioning, Storage Profile Based Volume provisioning, and others.
<9> IP Address or FQDN for the vCenter server where the Cloud Provider communicates.
<10> Path to the VM directory for node VMs.
<11> Set to the name of the datastore to use for provisioning volumes using the storage classes or dynamic provisioning. Prior to {product-title} 3.9, if the datastore was located in a storage directory or is a member of a datastore cluster, the full path was required.
<12> Optional. Set to the path to the resource pool where dummy VMs for Storage Profile Based volume provisioning must be created.
<13> Type of SCSI controller the VMDK will be attached to the VM as.
<14> Set to the network port group for vSphere to access the node, which is called VM Network by default. This is the node host's ExternalIP that is registered with Kubernetes.
+
. Restart the {product-title} host services:
+
[source,bash]
----
# master-restart api
# master-restart controllers
# systemctl restart atomic-openshift-node
----

== Manually configuring node hosts for vSphere

Perform the following on all node hosts.

.Procedure

To configure the {product-title} nodes for vSphere:

. Edit the appropriate xref:../admin_guide/manage_nodes.adoc#modifying-nodes[node
configuration map] and update the contents of the `*kubeletArguments*`
section:
+
[source,yaml]
----
kubeletArguments:
  cloud-provider:
    - "vsphere"
  cloud-config:
    - "/etc/origin/cloudprovider/vsphere.conf"
----
+
[IMPORTANT]
====
The `nodeName` must match the VM name in vSphere in order
for the cloud provider integration to work properly. The name must also be
RFC1123 compliant.
====

. Restart the {product-title} services on all nodes.
+
[source,bash]
----
# systemctl restart atomic-openshift-node
----

[[vsphere-applying-configuration-changes]]
== Applying Configuration Changes
