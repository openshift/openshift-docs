[[inventory_provision]]
=== Preparing the Inventory for Provisioning

With the installation of the `openshift-ansible` package complete via our
previous steps, there resides a
`sample-inventory` directory that we will copy to our `cloud-user` home directory
of the deployment host.

On the deployment host,

----
$ cp -r /usr/share/ansible/openshift-ansible/playbooks/openstack/sample-inventory/ ~/inventory
----

Within this inventory directory, the _all.yml_ file contains all the different
parameters that must be set in to order to achieve successful provisioning of
the RHOCP instances. The _OSEv3.yml_ file contains some references required by
the _all.yml_ file and all the available {product-title} cluster parameters
that you can customize.

[[all-yaml-file]]
==== OpenShiftSDN All YAML file

The _all.yml_ file has many options that can be modified to meet your specific needs.
The information gathered in this file is for the provisioning portion of the instances
required for a successful deployment of {product-title}. It
is important to review these carefully. This document will provide a condensed
version of the All YAML file and focus on the most critical parameters that need to
be set for a successful deployment.


----
$ cat ~/inventory/group_vars/all.yml
---
openshift_openstack_clusterid: "openshift"
openshift_openstack_public_dns_domain: *"example.com"*
openshift_openstack_dns_nameservers: *["10.19.115.228"]*
openshift_openstack_public_hostname_suffix: "-public"
openshift_openstack_nsupdate_zone: "{{ openshift_openstack_public_dns_domain }}"

openshift_openstack_keypair_name: *"openshift"*
openshift_openstack_external_network_name: *"public"*

openshift_openstack_default_image_name: *"rhel75"*

## Optional (Recommended) - This removes the need for floating IPs
## on the OpenShift Cluster nodes
openshift_openstack_node_subnet_name: *<deployment-subnet-name>*
openshift_openstack_router_name: *<deployment-router-name>*
openshift_openstack_master_floating_ip: *false*
openshift_openstack_infra_floating_ip: *false*
openshift_openstack_compute_floating_ip: *false*
## End of Optional Floating IP section

openshift_openstack_num_masters: *3*
openshift_openstack_num_infra: *3*
openshift_openstack_num_cns: *0*
openshift_openstack_num_nodes: *2*

openshift_openstack_master_flavor: *"m1.master"*
openshift_openstack_default_flavor: *"m1.node"*

openshift_openstack_use_lbaas_load_balancer: *true*

openshift_openstack_docker_volume_size: "15"

# # Roll-your-own DNS
*openshift_openstack_external_nsupdate_keys:*
  public:
    *key_secret: '/alb8h0EAFWvb4i+CMA12w=='*
    *key_name: "update-key"*
    *key_algorithm: 'hmac-md5'*
    *server: '<ip-of-DNS>'*
  private:
    *key_secret: '/alb8h0EAFWvb4i+CMA12w=='*
    *key_name: "update-key"*
    *key_algorithm: 'hmac-md5'*
    *server: '<ip-of-DNS>'*

ansible_user: openshift

## cloud config
openshift_openstack_disable_root: true
openshift_openstack_user: openshift
----

NOTE: Due to using an external DNS server, the private and public sections use
the public IP address of the DNS server as the DNS server does not reside in the
OpenStack environment.

The values above that are enclosed by asterisks (*) require modification based
upon your OpenStack environment and DNS server.

In order to properly modify the DNS portion of the All YAML file, login to the DNS
server and perform the following commands to capture the key name,
key algorithm and key secret:

----
$ ssh <ip-of-DNS>
$ sudo -i
# cat /etc/named/<key-name.key>
key "update-key" {
	algorithm hmac-md5;
	secret "/alb8h0EAFWvb4i+CMA02w==";
};

----

NOTE: The key name may vary and the above is only an example.


[[kuryr-all-yaml-file]]
==== KuryrSDN All YAML file

The following _all.yml_ file enables Kuryr SDN instead of the default
OpenShiftSDN. Note that the example below is a condensed version and it is
important to review the default template carefully.

----
$ cat ~/inventory/group_vars/all.yml
---
openshift_openstack_clusterid: "openshift"
openshift_openstack_public_dns_domain: *"example.com"*
openshift_openstack_dns_nameservers: *["10.19.115.228"]*
openshift_openstack_public_hostname_suffix: "-public"
openshift_openstack_nsupdate_zone: "{{ openshift_openstack_public_dns_domain }}"

openshift_openstack_keypair_name: *"openshift"*
openshift_openstack_external_network_name: *"public"*

openshift_openstack_default_image_name: *"rhel75"*

## Optional (Recommended) - This removes the need for floating IPs
## on the OpenShift Cluster nodes
openshift_openstack_node_subnet_name: *<deployment-subnet-name>*
openshift_openstack_router_name: *<deployment-router-name>*
openshift_openstack_master_floating_ip: *false*
openshift_openstack_infra_floating_ip: *false*
openshift_openstack_compute_floating_ip: *false*
## End of Optional Floating IP section

openshift_openstack_num_masters: *3*
openshift_openstack_num_infra: *3*
openshift_openstack_num_cns: *0*
openshift_openstack_num_nodes: *2*

openshift_openstack_master_flavor: *"m1.master"*
openshift_openstack_default_flavor: *"m1.node"*

## Kuryr configuration
openshift_use_kuryr: True
openshift_use_openshift_sdn: False
use_trunk_ports: True
os_sdn_network_plugin_name: cni
openshift_node_proxy_mode: userspace
kuryr_openstack_pool_driver: nested
openshift_kuryr_precreate_subports: 5

kuryr_openstack_public_net_id: *<public_ID>*

# To disable namespace isolation, comment out the next 2 lines
openshift_kuryr_subnet_driver: namespace
openshift_kuryr_sg_driver: namespace
# If you enable namespace isolation, `default` and `openshift-monitoring` become the
# global namespaces. Global namespaces can access all namespaces. All
# namespaces can access global namespaces.
# To make other namespaces global, include them here:
kuryr_openstack_global_namespaces: default,openshift-monitoring

# If OpenStack cloud endpoints are accessible over HTTPS, provide the CA certificate
kuryr_openstack_ca: *<path-to-ca-certificate>*

openshift_master_open_ports:
- service: dns tcp
  port: 53/tcp
- service: dns udp
  port: 53/udp
openshift_node_open_ports:
- service: dns tcp
  port: 53/tcp
- service: dns udp
  port: 53/udp

# To set the pod network CIDR range, uncomment the following property and set its value:
#
# openshift_openstack_kuryr_pod_subnet_prefixlen: 24
#
# The subnet prefix length value must be smaller than the CIDR value that is
# set in the inventory file as openshift_openstack_kuryr_pod_subnet_cidr.
# By default, this value is /24.

# openshift_portal_net is the range that OpenShift services and their associated Octavia
# load balancer VIPs use. Amphora VMs use Neutron ports in the range that is defined by
# openshift_openstack_kuryr_service_pool_start and openshift_openstack_kuryr_service_pool_end.
#
# The value of openshift_portal_net in the OSEv3.yml file must be within the range that is
# defined by openshift_openstack_kuryr_service_subnet_cidr. This range must be half
# of openshift_openstack_kuryr_service_subnet_cidr's range. This practice ensures that
# openshift_portal_net does not overlap with the range that load balancers' VMs use, which is
# defined by openshift_openstack_kuryr_service_pool_start and openshift_openstack_kuryr_service_pool_end.
#
# For reference only, copy the value in the next line from OSEv3.yml:
# openshift_portal_net: *"172.30.0.0/16"*

openshift_openstack_kuryr_service_subnet_cidr: *"172.30.0.0/15"*
openshift_openstack_kuryr_service_pool_start: *"172.31.0.1"*
openshift_openstack_kuryr_service_pool_end: *"172.31.255.253"*

# End of Kuryr configuration

openshift_openstack_use_lbaas_load_balancer: *true*

openshift_openstack_docker_volume_size: "15"

# # Roll-your-own DNS
*openshift_openstack_external_nsupdate_keys:*
  public:
    *key_secret: '/alb8h0EAFWvb4i+CMA12w=='*
    *key_name: "update-key"*
    *key_algorithm: 'hmac-md5'*
    *server: '<ip-of-DNS>'*
  private:
    *key_secret: '/alb8h0EAFWvb4i+CMA12w=='*
    *key_name: "update-key"*
    *key_algorithm: 'hmac-md5'*
    *server: '<ip-of-DNS>'*

ansible_user: openshift

## cloud config
openshift_openstack_disable_root: true
openshift_openstack_user: openshift
----

[NOTE]
====
If you are using namespace isolation, the Kuryr-controller creates a new Neutron
network and subnet for each namespace.
====

[NOTE]
====
Network policies and nodeport services are not supported when Kuryr SDN is
enabled.
====

[NOTE]
====
If Kuryr is enabled, {product-title} services are implemented through OpenStack Octavia Amphora VMs.

Octavia does not support UDP load balancing. Services that expose UDP ports are not supported.
====


[[kuryr-global-namespace-security-groups]]
===== Configuring global namespace access

The `kuryr_openstack_global_namespace` parameter contains a list that defines
global namespaces.
By default, only the `default` and `openshift-monitoring` namespaces are included in
this list.

If you are upgrading from a previous z-release of {product-title} {product-version}, note that access to other
namespaces from global namespaces is controlled by the security group `*-allow_from_default`.

Although the `remote_group_id rule` can control access to other namespaces from global namespaces, using it can
cause scaling and connectivity problems. To avoid these problems, switch from
using `remote_group_id` at `*_allow_from_default` to `remote_ip_prefix`:

. From a command line, retrieve your networks' `subnetCIDR` value:
+
----
$ oc get kuryrnets ns-default -o yaml | grep subnetCIDR
  subnetCIDR: 10.11.13.0/24
----

. Create TCP and UDP rules for this range:
+
----
$ openstack security group rule create --remote-ip 10.11.13.0/24 --protocol tcp openshift-ansible-openshift.example.com-allow_from_default
$ openstack security group rule create --remote-ip 10.11.13.0/24 --protocol udp openshift-ansible-openshift.example.com-allow_from_default
----

. Remove the security group rule that uses `remote_group_id`:
+
----
$ openstack security group show *-allow_from_default | grep remote_group_id
$ openstack security group rule delete REMOTE_GROUP_ID
----

[[all_yml]]
.Description of Variables in the All YAML file
|===
|Variable |Description

|openshift_openstack_clusterid |Cluster identification name

|openshift_openstack_public_dns_domain |Public DNS domain name
|openshift_openstack_dns_nameservers | IP of DNS nameservers
|openshift_openstack_public_hostname_suffix | Adds a suffix to the node hostname in the DNS record for both public and private
|openshift_openstack_nsupdate_zone | Zone to be updated with OCP instance IPs
|openshift_openstack_keypair_name | Keypair name used to log in to OCP instances
|openshift_openstack_external_network_name| OpenStack public network name
|openshift_openstack_default_image_name | OpenStack image used for OCP instances
|openshift_openstack_num_masters | Number of master nodes to deploy
|openshift_openstack_num_infra | Number of infrastructure nodes to deploy
|openshift_openstack_num_cns | Number of container native storage nodes to deploy
|openshift_openstack_num_nodes | Number of application nodes to deploy
|openshift_openstack_master_flavor| Name of the OpenStack flavor used for master instances
|openshift_openstack_default_flavor| Name of the Openstack flavor used for all instances, if specific flavor not specified.
|openshift_openstack_use_lbaas_load_balancer | Boolean value enabling Octavia load balancer (Octavia must be installed)
|openshift_openstack_docker_volume_size | Minimum size of the Docker volume (required variable)
|openshift_openstack_external_nsupdate_keys | Updating the DNS with the instance IP addresses
|ansible_user| Ansible user used to deploy {product-title}. "openshift" is the required name and must not be changed.
|openshift_openstack_disable_root| Boolean value that disables root access
|openshift_openstack_user| OCP instances created with this user
|openshift_openstack_node_subnet_name | Name of existing OpenShift subnet to use for deployment. This should be the same subnet name used for your deployment host.
|openshift_openstack_router_name | Name of existing OpenShift router to use for deployment. This should be the same router name used for your deployment host.
|openshift_openstack_master_floating_ip | Default is `true`. Must set to `false` if you do not want floating IPs assigned to master nodes.
|openshift_openstack_infra_floating_ip | Default is `true`. Must set to `false` if you do not want floating IPs assigned to infrastructure nodes.
|openshift_openstack_compute_floating_ip | Default is `true`. Must set to `false` if you do not want floating IPs assigned to compute nodes.
|openshift_use_openshift_sdn | Must set to `false` if you want to disable openshift-sdn
|openshift_use_kuryr | Must set to `true` if you want to enable kuryr sdn
|use_trunk_ports | Must be set to `true` to create the OpenStack VMs with trunk ports (required by kuryr)
|os_sdn_network_plugin_name | selection of the SDN behavior. Must set to `cni` for kuryr
|openshift_node_proxy_mode | Must set to `userspace` for Kuryr
|openshift_master_open_ports | Ports to be opened on the VMs when using Kuryr
|kuryr_openstack_public_net_id | Need by Kuryr. ID of the public OpenStack network from where FIPs are obtained
|openshift_kuryr_subnet_driver | Kuryr Subnet driver. Must be `namespace` for creating a subnet per namespace
|openshift_kuryr_sg_driver | Kuryr Security Group driver. Must be `namespace` for namespace isolation
|kuryr_openstack_global_namespaces | Global namespaces to use for namespace isolation. The default values are `default`, `openshift-monitoring`.
|kuryr_openstack_ca | Path to the CA certificate of the cloud. Required if OpenStack cloud endpoints are accessible over HTTPS.
|===

==== OSEv3 YAML file

The OSEv3 YAML file specifies all the different parameters and customizations
relating the installation of OpenShift.

Below is a condensed version of the file with all required variables for a
successful deployment. Additional variables may be required depending on what
customization is required for your specific {product-title} deployment.


[subs=+quotes]
----
*$ cat ~/inventory/group_vars/OSEv3.yml*
---

openshift_deployment_type: openshift-enterprise
openshift_release: v3.11
oreg_url: registry.access.redhat.com/openshift3/ose-${component}:${version}
openshift_examples_modify_imagestreams: true
oreg_auth_user: <oreg_auth_user>
oreg_auth_password: <oreg_auth_pw>
# The following is required if you want to deploy the Operator Lifecycle Manager (OLM)
openshift_additional_registry_credentials: [{'host':'registry.connect.redhat.com','user':'REGISTRYCONNECTUSER','password':'REGISTRYCONNECTPASSWORD','test_image':'mongodb/enterprise-operator:0.3.2'}]

openshift_master_default_subdomain: "apps.{{ (openshift_openstack_clusterid|trim == '') | ternary(openshift_openstack_public_dns_domain, openshift_openstack_clusterid + '.' + openshift_openstack_public_dns_domain) }}"

openshift_master_cluster_public_hostname: "console.{{ (openshift_openstack_clusterid|trim == '') | ternary(openshift_openstack_public_dns_domain, openshift_openstack_clusterid + '.' + openshift_openstack_public_dns_domain) }}"

*#OpenStack Credentials:*
openshift_cloudprovider_kind: openstack
openshift_cloudprovider_openstack_auth_url: "{{ lookup('env','OS_AUTH_URL') }}"
openshift_cloudprovider_openstack_username: "{{ lookup('env','OS_USERNAME') }}"
openshift_cloudprovider_openstack_password: "{{ lookup('env','OS_PASSWORD') }}"
openshift_cloudprovider_openstack_tenant_name: "{{ lookup('env','OS_PROJECT_NAME') }}"
openshift_cloudprovider_openstack_blockstorage_version: v2
openshift_cloudprovider_openstack_domain_name: "{{ lookup('env','OS_USER_DOMAIN_NAME') }}"
openshift_cloudprovider_openstack_conf_file: <path_to_local_openstack_configuration_file>

*#Use Cinder volume for Openshift registry:*
openshift_hosted_registry_storage_kind: openstack
openshift_hosted_registry_storage_access_modes: ['ReadWriteOnce']
openshift_hosted_registry_storage_openstack_filesystem: xfs
openshift_hosted_registry_storage_volume_size: 30Gi


openshift_hosted_registry_storage_openstack_volumeID: d65209f0-9061-4cd8-8827-ae6e2253a18d
openshift_hostname_check: false
ansible_become: true

*#Setting SDN (defaults to ovs-networkpolicy) not part of OSEv3.yml*
#For more info, on which to choose, visit:
#https://docs.openshift.com/container-platform/3.11/architecture/networking/sdn.html#overview
networkPluginName: redhat/ovs-networkpolicy
#networkPluginName: redhat/ovs-multitenant

*#Configuring identity providers with Ansible*
#For initial cluster installations, the Deny All identity provider is configured
#by default. It is recommended to be configured with either htpasswd
#authentication, LDAP authentication, or Allowing all authentication (not recommended)
#For more info, visit:
#https://docs.openshift.com/container-platform/3.10/install_config/configuring_authentication.html#identity-providers-ansible
#Example of Allowing All
#openshift_master_identity_providers: [{'name': 'allow_all', 'login': 'true', 'challenge': 'true', 'kind': 'AllowAllPasswordIdentityProvider'}]


*#Optional Metrics (uncomment below lines for installation)*

#openshift_metrics_install_metrics: true
#openshift_metrics_cassandra_storage_type: dynamic
#openshift_metrics_storage_volume_size: 25Gi
#openshift_metrics_cassandra_nodeselector: {"node-role.kubernetes.io/infra":"true"}
#openshift_metrics_hawkular_nodeselector: {"node-role.kubernetes.io/infra":"true"}
#openshift_metrics_heapster_nodeselector: {"node-role.kubernetes.io/infra":"true"}

*#Optional Aggregated Logging (uncomment below lines for installation)*

#openshift_logging_install_logging: true
#openshift_logging_es_pvc_dynamic: true
#openshift_logging_es_pvc_size: 30Gi
#openshift_logging_es_cluster_size: 3
#openshift_logging_es_number_of_replicas: 1
#openshift_logging_es_nodeselector: {"node-role.kubernetes.io/infra":"true"}
#openshift_logging_kibana_nodeselector: {"node-role.kubernetes.io/infra":"true"}
#openshift_logging_curator_nodeselector: {"node-role.kubernetes.io/infra":"true"}

----

For further details on any of the variables listed, see
link:https://github.com/openshift/openshift-ansible/blob/master/inventory/hosts.example[an example OpenShift-Ansible host inventory].


=== OpenStack Prerequisites Playbook

The {product-title} Ansible Installer provides a playbook to ensure all the provisioning
steps of the OpenStack instances have been met.

Prior to running the playbook, ensure to source the RC file

----
$ source path/to/examplerc
----

Via the `ansible-playbook` command on the deployment host, ensure all the
prerequisites are met using `prerequisites.yml` playbook:

[subs=+quotes]
----
$  ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openstack/openshift-cluster/prerequisites.yml
----

Once the prerequisite playbook completes successfully, run the provision playbook
as follows:

----
$ ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openstack/openshift-cluster/provision.yml
----

[IMPORTANT]

====
If _provision.yml_ prematurely errors, check if the status of the
OpenStack stack and wait for it finish

----
$ watch openstack stack list
+--------------------------------------+-------------------+--------------------+----------------------+--------------+
| ID                                   | Stack Name        | Stack Status       | Creation Time        | Updated Time |
+--------------------------------------+-------------------+--------------------+----------------------+--------------+
| 87cb6d1c-8516-40fc-892b-49ad5cb87fac | openshift-cluster | CREATE_IN_PROGRESS | 2018-08-20T23:44:46Z | None         |
+--------------------------------------+-------------------+--------------------+----------------------+--------------+

----

If the stack shows a `CREATE_IN_PROGRESS`, wait for the stack to complete with a
final result such as `CREATE_COMPLETE`. If the stack does complete successfully,
re-run the _provision.yml_ playbook for it to finish all the additional required
steps.

If the stack shows a `CREATE_FAILED`, make sure to run the following command to
see what caused the errors:

----
$ openstack stack failures list openshift-cluster
----
====


[[stack-name]]
=== Stack Name Configuration

By default, the Heat stack that is created by OpenStack for the {product-title}
cluster is named `openshift-cluster`. If you want to use a different
name then you must set the `OPENSHIFT_CLUSTER` environment variable before
running the playbooks:

----
$ export OPENSHIFT_CLUSTER=openshift.example.com
----

If you use a non-default stack name and run the openshift-ansible playbooks to
update your deployment, you must set `OPENSHIFT_CLUSTER` to your stack name to
avoid errors.
