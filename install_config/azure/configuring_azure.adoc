[[install-config-configuring-azure]]
= Configuring for Azure
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
{product-title} can be configured to access an
link:https://azure.microsoft.com/en-us/services/storage/disks/[Azure
infrastructure], including
xref:../install_config/persistent_storage/persistent_storage_azure.adoc#install-config-persistent-storage-persistent-storage-azure[using
Azure disk as persistent storage] for application data. After Azure is
configured properly, some additional configurations need to be completed on the
{product-title} hosts.

[[azure-configuration-file]]
== The Azure Configuration File

Configuring {product-title} for Azure requires the *_/etc/azure/azure.conf_* file, on each node host.

If the file does not exist, create it, and add the following:

----
tenantId: <> <1>
subscriptionId: <> <2>
aadClientId: <> <3>
aadClientSecret: <> <4>
aadTenantId: <> <5>
resourceGroup: <> <6>
location: <> <7>
----
<1> The AAD tenant ID for the subscription that the cluster is deployed in.
<2> The Azure subscription ID that the cluster is deployed in.
<3> The client ID for an AAD application with RBAC access to talk to Azure RM APIs.
<4> The client secret for an AAD application with RBAC access to talk to Azure RM APIs.
<5> Ensure this is the same as tenant ID (optional).
<6> The Azure Resource Group name that Azure VM belongs to.
<7> The compact style Azure region, for example `southeastasia` (optional).

[[azure-configuring-masters]]
== Configuring Masters

Edit or
xref:../install_config/master_node_configuration.adoc#creating-new-configuration-files[create] the
master configuration file on all masters
(*_/etc/origin/master/master-config.yaml_* by default) and update the
contents of the `apiServerArguments` and `controllerArguments` sections:

====
[source,yaml]
----
kubernetesMasterConfig:
  ...
  apiServerArguments:
    cloud-provider:
      - "azure"
    cloud-config:
      - "/etc/azure/azure.conf"
  controllerArguments:
    cloud-provider:
      - "azure"
    cloud-config:
      - "/etc/azure/azure.conf"
----
====

[IMPORTANT]
====
When triggering a containerized installation, only the *_/etc/origin_* and
*_/var/lib/origin_* directories are mounted to the master and node container.
Therefore, *_master-config.yaml_* should be in *_/etc/origin/master_* instead of
*_/etc/_*.
====

[[azure-configuring-nodes]]
== Configuring Nodes

. Edit or
xref:../install_config/master_node_configuration.adoc#creating-new-configuration-files[create]
the node configuration file on all nodes (*_/etc/origin/node/node-config.yaml_*
by default) and update the contents of the `kubeletArguments` section:
+
====
[source,yaml]
----
kubeletArguments:
  cloud-provider:
    - "azure"
  cloud-config:
    - "/etc/azure/azure.conf"

----
====
+
[IMPORTANT]
====
When triggering a containerized installation, only the *_/etc/origin_* and
*_/var/lib/origin_* directories are mounted to the master and node container.
Therefore, *_node-config.yaml_* should be in *_/etc/origin/node_* instead of
*_/etc/_*.
====

[[azure-applying-configuration-changes]]
== Applying Configuration Changes

Start or restart {product-title} services on all master and node hosts to apply your
configuration changes:

ifdef::openshift-enterprise[]
----
# systemctl restart atomic-openshift-master-api atomic-openshift-master-controller
# systemctl restart atomic-openshift-node
----
endif::[]
ifdef::openshift-origin[]
----
# systemctl restart origin-master-api origin-master-controllers
# systemctl restart origin-node
----
endif::[]

Switching from not using a cloud provider to using a cloud provider produces an
error message. Adding the cloud provider tries to delete the node because the
node switches from using the *hostname* as the `*externalID*` (which would have
been the case when no cloud provider was being used) to using the Azure
`*instance-id*` (which is what the Azure cloud provider specifies). To resolve
this issue:

.  Log in to the CLI as a cluster administrator.
. Check and backup existing node labels:
+
[source, bash]
----
$ oc describe node <node_name> | grep -Poz '(?s)Labels.*\n.*(?=Taints)'
----

.  Delete the nodes:
+
[source, bash]
----
$ oc delete node <node_name>
----
.  On each node host, restart the {product-title} service.
+
ifdef::openshift-enterprise[]
----
$ systemctl restart atomic-openshift-node
----
endif::[]
ifdef::openshift-origin[]
----
$ systemctl restart origin-node
----
endif::[]
.  Add back any xref:../admin_guide/manage_nodes.adoc#updating-labels-on-nodes[labels on each node] that you previously had.
