[id='configuring-persistent-storage']
= Configuring persistent storage

Running cluster monitoring with persistent storage means that your metrics are stored to a persistent volume and can survive a pod being restarted or recreated. This is ideal if you require your metrics or alerting data to be guarded from data loss. For production environments it is highly recommended to configure persistent storage.

.Additional resources

* See link:../architecture/additional_concepts/storage.adoc#persistent-volumes[Persistent Storage] for details.

== Determining how much storage is necessary

How much storage you need depends on the number of pods. It is administrator's responsibility to dedicate sufficient storage to ensure that the disk does not become full.

.Additional resources

* For information on system requirements for persistent storage, see link:../scaling_performance/scaling_cluster_monitoring.adoc#capacity-planning-for-cluster-monitoring-operator[Capacity Planning for Cluster Monitoring Operator].

== Enabling persistent storage

By default, persistent storage is disabled for both Prometheus time-series data and for Alertmanager notifications and silences. You can configure the cluster to persistently store any one of them or both.

.Procedure

* To enable persistent storage of Prometheus time-series data, set this variable to `true` in the Ansible inventory file:
+
`openshift_cluster_monitoring_operator_prometheus_storage_enabled`

* To enable persistent storage of Alertmanager notifications and silences, set this variable to `true` in the Ansible inventory file:
+
`openshift_cluster_monitoring_operator_alertmanager_storage_enabled`

== Setting persistent storage size

You can specify the size of the persistent volume claim for Prometheus and Alertmanager.

.Procedure

Change these Ansible variables:

* `openshift_cluster_monitoring_operator_prometheus_storage_capacity` (default: 50Gi)
* `openshift_cluster_monitoring_operator_alertmanager_storage_capacity` (default: 2Gi)

Each of these variables applies only if its corresponding `storage_enabled` variable is set to `true`.

== Allocating enough persistent volumes

Unless you use dynamically-provisioned storage, make sure you have a persistent volume (PV) ready to be claimed by the PVC, one PV for each replica. Prometheus has two replicas and Alertmanager has three replicas, which amounts to five PVs.

== Enabling dynamically-provisioned storage

Instead of statically-provisioned storage, you can use dynamically-provisioned storage.

.Procedure

To enable dynamic storage for Prometheus and Alertmanager, set the following parameters to `true` in the Ansible inventory file:

* `openshift_cluster_monitoring_operator_prometheus_storage_enabled`   (Default: false)
* `openshift_cluster_monitoring_operator_alertmanager_storage_enabled` (Default: false)

After you enable dynamic storage, you can also set the `storageclass` for the persistent volume claim for each component in the following parameters in the Ansible inventory file:

* `openshift_cluster_monitoring_operator_prometheus_storage_class_name`   (default: "")
* `openshift_cluster_monitoring_operator_alertmanager_storage_class_name` (default: "")

Each of these variables applies only if its corresponding `storage_enabled` variable is set to `true`.

.Additional resources

* See https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/[Dynamic Volume Provisioning] for details.
