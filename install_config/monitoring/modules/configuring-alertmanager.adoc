[id='configuring-alertmanager']
= Configuring Alertmanager
:data-uri:
:icons:
:experimental:
:prewrap!:

The Alertmanager manages incoming alerts, including silencing, inhibition, aggregation, and sending out notifications through methods such as email, PagerDuty, and HipChat.

The default configuration of the {product-title} Monitoring Alertmanager cluster is:

----
  global:
    resolve_timeout: 5m
  route:
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 12h
    receiver: default
    routes:
    - match:
        alertname: DeadMansSwitch
      repeat_interval: 5m
      receiver: deadmansswitch
  receivers:
  - name: default
  - name: deadmansswitch
----

You can overwrite this configuration.

.Procedure

Put the new configuration into the Ansible variable `openshift_cluster_monitoring_operator_alertmanager_config` from the `openshift_cluster_monitoring_operator` role.

For example, the following listing configures link:https://www.pagerduty.com/[PagerDuty] for notifications. See link:https://www.pagerduty.com/docs/guides/prometheus-integration-guide/[the PagerDuty documentation for Alertmanager] to learn how to retrieve the `service_key`.

----
openshift_cluster_monitoring_operator_alertmanager_config: |+
  global:
    resolve_timeout: 5m
  route:
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 12h
    receiver: default
    routes:
    - match:
        alertname: DeadMansSwitch
      repeat_interval: 5m
      receiver: deadmansswitch
    - match:
        service: example-app
      routes:
      - match:
          severity: critical
        receiver: team-frontend-page
  receivers:
  - name: default
  - name: deadmansswitch
  - name: team-frontend-page
    pagerduty_configs:
    - service_key: "<key>"
----

The sub-route matches only on alerts that have a severity of `critical`, and sends them via the receiver called `team-frontend-page`. As the name indicates, someone should be paged for alerts that are critical. See https://prometheus.io/docs/alerting/configuration/[Alertmanager configuration] for configuring alerting through different alert receivers.

[[dead-mans-switch]]
== Dead man's switch

{product-title} Monitoring ships with a "Dead man's switch" to ensure the availability of the monitoring infrastructure.

The "Dead man's switch" is a simple Prometheus alerting rule that always triggers. The Alertmanager continuously sends notifications for the dead man's switch to the notification provider that supports this functionality. This also ensures that communication between the Alertmanager and the notification provider is working.

This mechanism is supported by PagerDuty to issue alerts when the monitoring system itself is down.

[[dead-mans-switch-pagerduty]]
== Dead man's switch PagerDuty

https://www.pagerduty.com/[PagerDuty] supports "Dead man's switch" through an integration called https://deadmanssnitch.com/[Dead Man's Snitch]. You can enable it.

.Procedure

Add a `PagerDuty` configuration to the default `deadmansswitch` receiver.

For example, you can configure Dead Man's Snitch to page the operator if the "Dead man's switch" alert is silent for 15 minutes. With the default Alertmanager configuration, the Dead man's switch alert is repeated every five minutes. If Dead Man's Snitch triggers after 15 minutes, it indicates that the notification has been unsuccessful at least twice.

.Additional resources

* To learn how to add a `PagerDuty` configuration to the default `deadmansswitch` receiver, see link:configuring-alertmanager.adoc[Configuring Alertmanager].
* To learn how to configure Dead Man's Snitch for PagerDuty, see https://www.pagerduty.com/docs/guides/dead-mans-snitch-integration-guide/[Dead Manâ€™s Snitch Integration Guide].

== Grouping alerts

_FIXME get missing info and convert to procedure_

Once alerts are firing against the Alertmanager, it must be configured to know how to logically group them.

For this example a new route will be added to reflect alert routing of the "frontend" team.

First, add new routes. Multiple routes may be added beneath the original route, typically to define the receiver for the notification. The following example uses a matcher to ensure that only alerts coming from the service `example-app` are used.

  global:
    resolve_timeout: 5m
  route:
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 12h
    receiver: default
    routes:
    - match:
        alertname: DeadMansSwitch
      repeat_interval: 5m
      receiver: deadmansswitch
    - match:
        service: example-app
      routes:
      - match:
          severity: critical
        receiver: team-frontend-page
  receivers:
  - name: default
  - name: deadmansswitch

The sub-route matches only on alerts that have a severity of `critical`, and sends them via the receiver called `team-frontend-page`. As the name indicates, someone should be paged for alerts that are critical.

[[alerting-rules]]
== Alerting rules
:data-uri:
:icons:
:experimental:
:prewrap!:

{product-title} Cluster Monitoring ships with the following alerting rules configured by default. Currently you cannot add custom alerting rules.

Some alerting rules have identical names. This is intentional. They are alerting about the same event with different thresholds, with different severity, or both. With the inhibition rules, the lower severity is inhibited when the higher severity is firing.

For more details on the alerting rules, see the link:https://github.com/openshift/cluster-monitoring-operator/blob/master/assets/prometheus-k8s/rules.yaml[configuration file].

[options="header"]
|===
|Alert|Severity|Description
|`ClusterMonitoringOperatorErrors`|`critical`|Cluster Monitoring Operator is experiencing _X_% errors.
|`AlertmanagerDown`|`critical`|Alertmanager has disappeared from Prometheus target discovery.
|`ClusterMonitoringOperatorDown`|`critical`|ClusterMonitoringOperator has disappeared from Prometheus target discovery.
|`KubeAPIDown`|`critical`|KubeAPI has disappeared from Prometheus target discovery.
|`KubeControllerManagerDown`|`critical`|KubeControllerManager has disappeared from Prometheus target discovery.
|`KubeSchedulerDown`|`critical`|KubeScheduler has disappeared from Prometheus target discovery.
|`KubeStateMetricsDown`|`critical`|KubeStateMetrics has disappeared from Prometheus target discovery.
|`KubeletDown`|`critical`|Kubelet has disappeared from Prometheus target discovery.
|`NodeExporterDown`|`critical`|NodeExporter has disappeared from Prometheus target discovery.
|`PrometheusDown`|`critical`|Prometheus has disappeared from Prometheus target discovery.
|`PrometheusOperatorDown`|`critical`|PrometheusOperator has disappeared from Prometheus target discovery.
|`KubePodCrashLooping`|`critical`|_Namespace/Pod_ (_Container_) is restarting _times_ / second
|`KubePodNotReady`|`critical`|_Namespace/Pod_ is not ready.
|`KubeDeploymentGenerationMismatch`|`critical`|Deployment _Namespace/Deployment_ generation mismatch
|`KubeDeploymentReplicasMismatch`|`critical`|Deployment _Namespace/Deployment_ replica mismatch
|`KubeStatefulSetReplicasMismatch`|`critical`|StatefulSet _Namespace/StatefulSet_ replica mismatch
|`KubeStatefulSetGenerationMismatch`|`critical`|StatefulSet _Namespace/StatefulSet_ generation mismatch
|`KubeDaemonSetRolloutStuck`|`critical`|Only _X_% of desired pods scheduled and ready for daemon set _Namespace/DaemonSet_
|`KubeDaemonSetNotScheduled`|`warning`|A number of pods of daemonset _Namespace/DaemonSet_ are not scheduled.
|`KubeDaemonSetMisScheduled`|`warning`|A number of pods of daemonset _Namespace/DaemonSet_ are running where they are not supposed to run.
|`KubeCronJobRunning`|`warning`|CronJob _Namespace/CronJob_ is taking more than 1h to complete.
|`KubeJobCompletion`|`warning`|Job _Namespaces/Job_ is taking more than 1h to complete.
|`KubeJobFailed`|`warning`|Job _Namespaces/Job_ failed to complete.
|`KubeCPUOvercommit`|`warning`|Overcommited CPU resource requests on Pods, cannot tolerate node failure.
|`KubeMemOvercommit`|`warning`|Overcommited Memory resource requests on Pods, cannot tolerate node failure.
|`KubeCPUOvercommit`|`warning`|Overcommited CPU resource request quota on Namespaces.
|`KubeMemOvercommit`|`warning`|Overcommited Memory resource request quota on Namespaces.
|`alerKubeQuotaExceeded`|`warning`|_X_% usage of _Resource_ in namespace _Namespace_.
|`KubePersistentVolumeUsageCritical`|`critical`|The persistent volume claimed by _PersistentVolumeClaim_ in namespace _Namespace_ has _X_% free.
|`KubePersistentVolumeFullInFourDays`|`critical`|Based on recent sampling, the persistent volume claimed by _PersistentVolumeClaim_ in namespace _Namespace_ is expected to fill up within four days. Currently _X_ bytes are available.
|`KubeNodeNotReady`|`warning`|_Node_ has been unready for more than an hour
|`KubeVersionMismatch`|`warning`|There are _X_ different versions of Kubernetes components running.
|`KubeClientErrors`|`warning`|Kubernetes API server client '_Job/Instance_' is experiencing _X_% errors.'
|`KubeClientErrors`|`warning`|Kubernetes API server client '_Job/Instance_' is experiencing _X_ errors / sec.'
|`KubeletTooManyPods`|`warning`|Kubelet _Instance_ is running _X_ pods, close to the limit of 110.
|`KubeAPILatencyHigh`|`warning`|The API server has a 99th percentile latency of _X_ seconds for _Verb_ _Resource_.
|`KubeAPILatencyHigh`|`critical`|The API server has a 99th percentile latency of _X_ seconds for _Verb_ _Resource_.
|`KubeAPIErrorsHigh`|`critical`|API server is erroring for _X_% of requests.
|`KubeAPIErrorsHigh`|`warning`|API server is erroring for _X_% of requests.
|`KubeClientCertificateExpiration`|`warning`|Kubernetes API certificate is expiring in less than 7 days.
|`KubeClientCertificateExpiration`|`critical`|Kubernetes API certificate is expiring in less than 1 day.
|`AlertmanagerConfigInconsistent`|`critical`|Summary: Configuration out of sync. Description: The configuration of the instances of the Alertmanager cluster `_Service_` are out of sync.
|`AlertmanagerFailedReload`|`warning`|Summary: Alertmanager's configuration reload failed. Description: Reloading Alertmanager's configuration has failed for _Namespace/Pod_.
|`TargetDown`|`warning`|Summary: Targets are down. Description: _X_% of _Job_ targets are down.
|`DeadMansSwitch`|`none`|Summary: Alerting DeadMansSwitch. Description: This is a DeadMansSwitch meant to ensure that the entire Alerting pipeline is functional.
|`NodeDiskRunningFull`|`warning`|Device _Device_ of node-exporter _Namespace/Pod_ is running full within the next 24 hours.
|`NodeDiskRunningFull`|`critical`|Device _Device_ of node-exporter _Namespace/Pod_ is running full within the next 2 hours.
|`PrometheusConfigReloadFailed`|`warning`|Summary: Reloading Prometheus' configuration failed. Description: Reloading Prometheus' configuration has failed for _Namespace/Pod_
|`PrometheusNotificationQueueRunningFull`|`warning`|Summary: Prometheus' alert notification queue is running full. Description: Prometheus' alert notification queue is running full for _Namespace/Pod_
|`PrometheusErrorSendingAlerts`|`warning`|Summary: Errors while sending alert from Prometheus. Description: Errors while sending alerts from Prometheus _Namespace/Pod_ to Alertmanager _Alertmanager_
|`PrometheusErrorSendingAlerts`|`critical`|Summary: Errors while sending alerts from Prometheus. Description: Errors while sending alerts from Prometheus _Namespace/Pod_ to Alertmanager _Alertmanager_
|`PrometheusNotConnectedToAlertmanagers`|`warning`|Summary: Prometheus is not connected to any Alertmanagers. Description: Prometheus _Namespace/Pod_ is not connected to any Alertmanagers
|`PrometheusTSDBReloadsFailing`|`warning`|Summary: Prometheus has issues reloading data blocks from disk. Description: _Job_ at _Instance_ had _X_ reload failures over the last four hours.
|`PrometheusTSDBCompactionsFailing`|`warning`|Summary: Prometheus has issues compacting sample blocks. Description: _Job_ at _Instance_ had _X_ compaction failures over the last four hours.
|`PrometheusTSDBWALCorruptions`|`warning`|Summary: Prometheus write-ahead log is corrupted. Description: _Job_ at _Instance_ has a corrupted write-ahead log (WAL).
|`PrometheusNotIngestingSamples`|`warning`|Summary: Prometheus isn't ingesting samples. Description: Prometheus _Namespace/Pod_ isn't ingesting samples.
|`PrometheusTargetScrapesDuplicate`|`warning`|Summary: Prometheus has many samples rejected. Description: _Namespace/Pod_ has many samples rejected due to duplicate timestamps but different values
|`EtcdInsufficientMembers`|`critical`|Etcd cluster "_Job_": insufficient members (_X_).
|`EtcdNoLeader`|`critical`|Etcd cluster "_Job_": member _Instance_ has no leader.
|`EtcdHighNumberOfLeaderChanges`|`warning`|Etcd cluster "_Job_": instance _Instance_ has seen _X_ leader changes within the last hour.
|`EtcdHighNumberOfFailedGRPCRequests`|`warning`|Etcd cluster "_Job_": _X_% of requests for _GRPC_Method_ failed on etcd instance _Instance_.
|`EtcdHighNumberOfFailedGRPCRequests`|`critical`|Etcd cluster "_Job_": _X_% of requests for _GRPC_Method_ failed on etcd instance _Instance_.
|`EtcdGRPCRequestsSlow`|`critical`|Etcd cluster "_Job_": gRPC requests to _GRPC_Method_ are taking _X_s on etcd instance _Instance_.
|`EtcdMemberCommunicationSlow`|`warning`|Etcd cluster "_Job_": member communication with _To_ is taking _X_s on etcd instance _Instance_.
|`EtcdHighNumberOfFailedProposals`|`warning`|Etcd cluster "_Job_": _X_ proposal failures within the last hour on etcd instance _Instance_.
|`EtcdHighFsyncDurations`|`warning`|Etcd cluster "_Job_": 99th percentile fync durations are _X_s on etcd instance _Instance_.
|`EtcdHighCommitDurations`|`warning`|Etcd cluster "_Job_": 99th percentile commit durations _X_s on etcd instance _Instance_.
|`FdExhaustionClose`|`warning`|_Job_ instance _Instance_ will exhaust its file descriptors soon
|`FdExhaustionClose`|`critical`|_Job_ instance _Instance_ will exhaust its file descriptors soon
|===
