[[configuring-openshift-cluster-monitoring]]
= Configuring {product-title} cluster monitoring
:data-uri:
:icons:
:experimental:
:prewrap!:

The {product-title} Ansible `openshift_cluster_monitoring_operator` role configures and deploys the Cluster Monitoring Operator using the variables from the xref:../install/configuring_inventory_file.adoc#configuring-ansible[inventory file].

.Ansible variables
[options="header"]
|===

|Variable |Description

|`openshift_cluster_monitoring_operator_install`
| Deploy the Cluster Monitoring Operator if `true`. Otherwise, undeploy. This variable is set to `true` by default.

|`openshift_cluster_monitoring_operator_prometheus_storage_capacity`
| The persistent volume claim size for each of the Prometheus instances. This variable applies only if `openshift_cluster_monitoring_operator_prometheus_storage_enabled` is set to `true`. Defaults to `50Gi`.

|`openshift_cluster_monitoring_operator_alertmanager_storage_capacity`
| The persistent volume claim size for each of the Alertmanager instances. This variable applies only if `openshift_cluster_monitoring_operator_alertmanager_storage_enabled` is set to `true`. Defaults to `2Gi`.

|`openshift_cluster_monitoring_operator_node_selector`
| Set to the desired, existing xref:../admin_guide/scheduling/node_selector.adoc#admin-guide-sched-selector[node selector] to ensure that pods are placed onto nodes with specific labels. Defaults to `node-role.kubernetes.io/infra=true`.

|`openshift_cluster_monitoring_operator_alertmanager_config`
| Configures Alertmanager.

|`openshift_cluster_monitoring_operator_prometheus_storage_enabled`
| Enable persistent storage of Prometheus' time-series data. This variable is set to `false` by default.

|`openshift_cluster_monitoring_operator_alertmanager_storage_enabled`
| Enable persistent storage of Alertmanager notifications and silences. This variable is set to `false` by default.

|`openshift_cluster_monitoring_operator_prometheus_storage_class_name`
| If you enabled the `openshift_cluster_monitoring_operator_prometheus_storage_enabled` option, set a specific StorageClass to ensure that pods are configured to use the `PVC` with that `storageclass`. Defaults to `none`, which applies the default storage class name.

|`openshift_cluster_monitoring_operator_alertmanager_storage_class_name`
| If you enabled the `openshift_cluster_monitoring_operator_alertmanager_storage_enabled` option, set a specific StorageClass to ensure that pods are configured to use the `PVC` with that `storageclass`. Defaults to `none`, which applies the default storage class name.

|===

[[monitoring-prerequisites]]
== Monitoring prerequisites

The monitoring stack imposes additional resource requirements. See xref:../scaling_performance/scaling_cluster_monitoring.adoc#cluster-monitoring-recommendations-for-OCP[computing resources recommendations] for details.

== Installing monitoring stack

The Monitoring stack is installed with {product-title} by default. You can prevent it from being installed. To do that, set this variable to `false` in the Ansible inventory file:

`openshift_cluster_monitoring_operator_install`

== Persistent storage

Running cluster monitoring with persistent storage means that your metrics are stored to a xref:../architecture/additional_concepts/storage.adoc#persistent-volumes[persistent volume] and can survive a pod being restarted or recreated. This is ideal if you require your metrics or alerting data to be guarded from data loss. For production environments it is highly recommended to configure persistent storage.

=== Enabling persistent storage

By default, persistent storage is disabled for both Prometheus time-series data and for Alertmanager notifications and silences. You can configure the cluster to persistently store any one of them or both.

* To enable persistent storage of Prometheus time-series data, set this variable to `true` in the Ansible inventory file:
+
`openshift_cluster_monitoring_operator_prometheus_storage_enabled`

* To enable persistent storage of Alertmanager notifications and silences, set this variable to `true` in the Ansible inventory file:
+
`openshift_cluster_monitoring_operator_alertmanager_storage_enabled`

=== Determining how much storage is necessary

How much storage you need depends on the number of pods. It is administrator's responsibility to dedicate sufficient storage to ensure that the disk does not become full. For information on system requirements for persistent storage, see xref:../scaling_performance/scaling_cluster_monitoring.adoc#capacity-planning-for-cluster-monitoring-operator[Capacity Planning for Cluster Monitoring Operator].

=== Setting persistent storage size

To specify the size of the persistent volume claim for Prometheus and Alertmanager, change these Ansible variables:

* `openshift_cluster_monitoring_operator_prometheus_storage_capacity` (default: 50Gi)
* `openshift_cluster_monitoring_operator_alertmanager_storage_capacity` (default: 2Gi)

Each of these variables applies only if its corresponding `storage_enabled` variable is set to `true`.

=== Allocating enough persistent volumes

Unless you use dynamically-provisioned storage, you need to make sure you have a persistent volume (PV) ready to be claimed by the PVC, one PV for each replica. Prometheus has two replicas and Alertmanager has three replicas, which amounts to five PVs.

=== Enabling dynamically-provisioned storage

Instead of statically-provisioned storage, you can use dynamically-provisioned storage. See https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/[Dynamic Volume Provisioning] for details.

To enable dynamic storage for Prometheus and Alertmanager, set the following parameters to `true` in the Ansible inventory file:

* `openshift_cluster_monitoring_operator_prometheus_storage_enabled`   (Default: false)
* `openshift_cluster_monitoring_operator_alertmanager_storage_enabled` (Default: false)

After you enable dynamic storage, you can also set the `storageclass` for the persistent volume claim for each component in the following parameters in the Ansible inventory file:

* `openshift_cluster_monitoring_operator_prometheus_storage_class_name`   (default: "")
* `openshift_cluster_monitoring_operator_alertmanager_storage_class_name` (default: "")

Each of these variables applies only if its corresponding `storage_enabled` variable is set to `true`.

[[supported-configuration]]
=== Supported configuration

The supported way of configuring {product-title} Monitoring is by configuring it using the options described in xref:#configuring-openshift-cluster-monitoring[Configuring OpenShift cluster monitoring]. Beyond those explicit configuration options, it is possible to inject additional configuration into the stack. However this is unsupported, as configuration paradigms might change across Prometheus releases, and such cases can only be handled gracefully if all configuration possibilities are controlled.

Explicitly unsupported cases include:

* Creating additional `ServiceMonitor` objects in the `openshift-monitoring` namespace, thereby extending the targets the cluster monitoring Prometheus instance scrapes. This can cause collisions and load differences that cannot be accounted for, therefore the Prometheus setup can be unstable.
* Creating additional `ConfigMap` objects, that cause the cluster monitoring Prometheus instance to include additional alerting and recording rules. Note that this behavior is known to cause a breaking behavior if applied, as Prometheus 2.0 will ship with a new rule file syntax.
