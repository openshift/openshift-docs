[id='configuring-persistent-storage_{context}']
= Configuring persistent storage

Running cluster monitoring with persistent storage means that your metrics are stored to a persistent volume (PV) and can survive a pod being restarted or recreated. This is ideal if you require your metrics or alerting data to be guarded from data loss. For production environments, it is highly recommended to configure persistent storage. Because of the high IO demands, it is advantageous to use local storage.

[id="persistent-storage-prerequisites_{context}"]
== Persistent storage prerequisites

* Dedicate sufficient local persistent storage to ensure that the disk does not become full. How much storage you need depends on the number of pods.

* Make sure you have a persistent volume (PV) ready to be claimed by the persistent volume claim (PVC), one PV for each replica. Because Prometheus has two replicas and Alertmanager has three replicas, you need five PVs to support the entire monitoring stack. The PVs should be available from the Local Storage Operator. This does not apply if you enable dynamically provisioned storage.

* Use the block type of storage.

[NOTE]
====
If you use a local volume for persistent storage, do not use a raw block volume, which is described with `volumeMode: block` in the `LocalVolume` object. Prometheus cannot use raw block volumes.
====

[id='persistent-storage-configuring-persistent-claim_{context}']
== Configuring a local persistent volume claim

For monitoring components to use a persistent volume (PV), you must configure a persistent volume claim (PVC).

.Prerequisites

* *If you are configuring core {product-title} monitoring components*:
** You have access to the cluster as a user with the `cluster-admin` role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
* *If you are configuring components that monitor user-defined projects*:
** You have access to the cluster as a user with the `cluster-admin` role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `ConfigMap` object:
** *To configure a PVC for a component that monitors core {product-title} projects*:
.. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Add your PVC configuration for the component under `data/config.yaml`:
+
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    <component>:
      volumeClaimTemplate:
        spec:
          storageClassName: <storage_class>
          resources:
            requests:
              storage: <amount_of_storage>
----
+
See the link:https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims[Kubernetes documentation on PersistentVolumeClaims] for information on how to specify `volumeClaimTemplate`.
+
The following example configures a PVC that claims local persistent storage for the Prometheus instance that monitors core {product-title} components:
+
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    *prometheusK8s*:
      volumeClaimTemplate:
        spec:
          storageClassName: *local-storage*
          resources:
            requests:
              storage: *40Gi*
----
+
In the above example, the storage class created by the Local Storage Operator is called `local-storage`.
+
The following example configures a PVC that claims local persistent storage for Alertmanager:
+
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    *alertmanagerMain*:
      volumeClaimTemplate:
        spec:
          storageClassName: *local-storage*
          resources:
            requests:
              storage: *10Gi*
----

** *To configure a PVC for a component that monitors user-defined projects*:
.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Add your PVC configuration for the component under `data/config.yaml`:
+
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    <component>:
      volumeClaimTemplate:
        spec:
          storageClassName: <storage_class>
          resources:
            requests:
              storage: <amount_of_storage>
----
+
See the link:https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims[Kubernetes documentation on PersistentVolumeClaims] for information on how to specify `volumeClaimTemplate`.
+
The following example configures a PVC that claims local persistent storage for the Prometheus instance that monitors user-defined projects:
+
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    *prometheus*:
      volumeClaimTemplate:
        spec:
          storageClassName: *local-storage*
          resources:
            requests:
              storage: *40Gi*
----
+
In the above example, the storage class created by the Local Storage Operator is called `local-storage`.
+
The following example configures a PVC that claims local persistent storage for Thanos Ruler:
+
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    *thanosRuler*:
      volumeClaimTemplate:
        spec:
          storageClassName: *local-storage*
          resources:
            requests:
              storage: *10Gi*
----
+
[NOTE]
====
Storage requirements for the `thanosRuler` component depend on the number of rules that are evaluated and how many samples each rule generates.
====

. Save the file to apply the changes. The pods affected by the new configuration are restarted automatically and the new storage configuration is applied.
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====
+
[WARNING]
====
When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.
====

[id="modifying-retention-time-for-prometheus-metrics-data_{context}"]
== Modifying the retention time for Prometheus metrics data

By default, the {product-title} monitoring stack configures the retention time for Prometheus data to be 15 days. You can modify the retention time for the Prometheus instance that monitors user-defined projects, to change how soon the data is deleted.

.Prerequisites

* You have access to the cluster as a user with the `dedicated-admin` role.
* You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. To modify the retention time for the Prometheus instance that monitors user-defined projects, edit the `ConfigMap` object:
.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Add your retention time configuration under `data.config.yaml`:
+
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      retention: <time_specification>
----
+
Substitute `<time_specification>` with a number directly followed by `ms` (milliseconds), `s` (seconds), `m` (minutes), `h` (hours), `d` (days), `w` (weeks), or `y` (years).
+
The following example sets the retention time to 24 hours for the Prometheus instance that monitors user-defined projects:
+
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      retention: 24h
----

. Save the file to apply the changes. The pods affected by the new configuration are restarted automatically.
+
[WARNING]
====
When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.
====
