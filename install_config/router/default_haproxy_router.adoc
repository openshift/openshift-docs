[[install-config-router-default-haproxy]]
= Using the Default HAProxy Router
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

The `oc adm router` command is provided with the administrator CLI to simplify
the tasks of setting up routers in a new installation. The `oc
adm router` command creates the service and deployment configuration objects.
Use the `--service-account` option to specify the service account the router
will use to contact the master.

The
xref:../../install_config/router/index.adoc#creating-the-router-service-account[router
service account] can be created in advance or created by the `oc adm router
--service-account` command.

Every form of communication between {product-title} components is secured by TLS
and uses various certificates and authentication methods. The
`--default-certificate` .pem format file can be supplied or one is created by
the `oc adm router` command. When
xref:../../architecture/networking/routes.adoc#[routes] are created, the user
can provide route certificates that the router will use when handling the route.

[IMPORTANT]
====
When deleting a router, ensure the deployment configuration, service, and secret
are deleted as well.
====

Routers are deployed on specific nodes. This makes it easier for the cluster
administrator and external network manager to coordinate which IP address will
run a router and which traffic the router will handle. The routers are deployed
on specific nodes by using xref:adding-nodeselector-to-a-deployment[node
selectors].

[IMPORTANT]
====
Routers use host networking by default, and they directly attach to port 80 and
443 on all interfaces on a host. Restrict routers to hosts where ports 80/443
are available and not being consumed by another service, and set this using
xref:adding-nodeselector-to-a-deployment[node selectors] and the
xref:../../admin_guide/scheduling/scheduler.adoc#admin-guide-scheduler[scheduler
configuration]. As an example, you can achieve this by dedicating infrastructure
nodes to run services such as routers.
====

[IMPORTANT]
====
It is recommended to use separate distinct *openshift-router* service account
with your router. This can be provided using the `--service-account` flag to the
`oc adm router` command.

ifdef::openshift-enterprise[]
----
$ oc adm router --dry-run --service-account=router //<1>
----
endif::[]
ifdef::openshift-origin[]
----
$ oc adm router --dry-run --service-account=router //<1>
----
endif::[]
<1> `--service-account` is the name of a xref:../../admin_guide/service_accounts.adoc#admin-guide-service-accounts[service account]
for the *openshift-router*.
ifdef::openshift-origin[]
It is recommended using an *openshift-router* specific service account with
appropriate permissions.
endif::[]
====

[IMPORTANT]
====
Router pods created using `oc adm router` have default resource requests
that a node must satisfy for the router pod to be deployed. In an
effort to increase the reliability of infrastructure components, the default
resource requests are used to increase the QoS tier of the router pods above
pods without resource requests. The default values represent the observed minimum
resources required for a basic router to be deployed and can be edited in the
routers deployment configuration and you may want to increase them based on the
load of the router.
====

[[deploy-router-create-router]]
== Creating a Router

If the router does
not exist, run the following to create a router:

ifdef::openshift-enterprise[]
----
$ oc adm router <router_name> --replicas=<number> --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oc adm router <router_name> --replicas=<number> --service-account=router
----
endif::[]

`--replicas` is usually `1` unless a
xref:../../admin_guide/high_availability.adoc#[high availability] configuration
is  being created.

To find the host IP address of the router:

----
$ oc get po <router-pod>  --template={{.status.hostIP}}
----

You can also xref:creating-router-shards[use router shards] to ensure that the
router is filtered to specific namespaces or routes, or
xref:../../architecture/networking/routes.adoc#env-variables[set any
environment variables] after router creation. In this case create a router for each shard.

[[basic-router-commands]]
== Other Basic Router Commands

[[deploy-router-check-default]]
Checking the Default Router::

ifdef::openshift-enterprise[]
The default router service account, named *router*, is automatically created
during cluster installations. To verify that this account already exists:
endif::[]
ifdef::openshift-origin[]
First, ensure you have xref:../../install_config/router/index.adoc#creating-the-router-service-account[created the
router service account] before deploying a router.

To check if a default router, named *router*, already exists:
endif::[]

ifdef::openshift-enterprise[]
----
$ oc adm router --dry-run --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oc adm router --dry-run --service-account=router
----
endif::[]

[[deploy-router-viewing-default]]
Viewing the Default Router::

To see what the default router would look like if created:

ifdef::openshift-enterprise[]
----
$ oc adm router --dry-run -o yaml --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oc adm router --dry-run -o yaml --service-account=router
----
endif::[]

[[deploy-router-to-labeled-nodes]]
Deploying the Router to a Labeled Node::

To deploy the router to any node(s) that match a specified
xref:../../admin_guide/manage_nodes.adoc#updating-labels-on-nodes[node label]:

ifdef::openshift-enterprise[]
----
$ oc adm router <router_name> --replicas=<number> --selector=<label> \
    --service-account=router
----

For example, if you want to create a router named `router` and have it placed on a node labeled with `node-role.kubernetes.io/infra=true`:

----
$ oc adm router router --replicas=1 --selector='node-role.kubernetes.io/infra=true' \
  --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oc adm router <router_name> --replicas=<number> --selector=<label> \
    --service-account=router
----

For example, if you want to create a router named `router` and have it placed on a node labeled with `node-role.kubernetes.io/infra=true`:

----
$ oc adm router router --replicas=1 --selector='node-role.kubernetes.io/infra=true' \
  --service-account=router
----
endif::[]

During cluster installation, the `openshift_router_selector` and
`openshift_registry_selector` Ansible settings are set to `node-role.kubernetes.io/infra=true` by
default. The default router and registry will only be automatically deployed if
a node exists that matches the `node-role.kubernetes.io/infra=true` label.

For information on updating labels, see
xref:../../admin_guide/manage_nodes.adoc#updating-labels-on-nodes[Updating Labels
on Nodes].

Multiple instances are created on different hosts according to the
xref:../../admin_guide/scheduling/scheduler.adoc#admin-guide-scheduler[scheduler policy].

[[deploy-router-different-image]]
Using a Different Router Image::

To use a different router image and view the router configuration that would be used:

ifdef::openshift-enterprise[]
----
$ oc adm router <router_name> -o <format> --images=<image> \
    --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oc adm router <router_name> -o <format> --images=<image> \
    --service-account=router
----
endif::[]

For example:

ifdef::openshift-enterprise[]
----
$ oc adm router region-west -o yaml --images=myrepo/somerouter:mytag \
    --service-account=router
----
endif::[]
ifdef::openshift-origin[]
----
$ oc adm router region-west -o yaml --images=myrepo/somerouter:mytag \
    --service-account=router
----
endif::[]

[[install-haproxy-filtering-routes]]
== Filtering Routes to Specific Routers

Using the
xref:../../architecture/networking/routes.adoc#env-variables[`ROUTE_LABELS`]
environment variable, you can filter routes so that they are used only by
specific routers.

For example, if you have multiple routers, and 100 routes, you can attach labels
to the routes so that a portion of them are handled by one router, whereas the
rest are handled by another.

. After xref:../../install_config/router/index.adoc#install-config-router-overview[creating a router], use the `ROUTE_LABELS` environment variable to tag the router:
+
----
$ oc set env dc/<router=name>  ROUTE_LABELS="key=value"
----

. Add the label to the desired routes:
+
----
oc label route <route=name> key=value
----

. To verify that the label has been attached to the route, check the route configuration:
+
----
$ oc describe route/<route_name>
----

[[concurrent-connections]]
Setting the Maximum Number of Concurrent Connections::

The router can handle a maximum number of 20000 connections by default. You can
change that limit depending on your needs. Having too few connections prevents
the health check from working, which causes unnecessary restarts. You need to
configure the system to support the maximum number of connections. The limits
shown in `'sysctl fs.nr_open'` and `'sysctl fs.file-max'` must be large enough.
Otherwise, HAproxy will not start.

When the router is created, the `--max-connections=` option sets the desired
limit:

----
$ oc adm router --max-connections=10000   ....
----

Edit the xref:../../architecture/networking/routes.adoc#env-variables[`ROUTER_MAX_CONNECTIONS`] environment variable in the router's
deployment configuration to change the value. The router pods are restarted with
the new value. If `ROUTER_MAX_CONNECTIONS` is not present, the default value of
20000, is used.

[NOTE]
====
A connection includes the frontend and internal backend. This counts as two
connections. Be sure to set `ROUTER_MAX_CONNECTIONS` to double than the number
of connections you intend to create.
====

[[bind-strict-sni]]
== HAProxy Strict SNI

The
xref:../../architecture/networking/routes.adoc#strict-sni[HAProxy `strict-sni`] can be
controlled through the `ROUTER_STRICT_SNI` environment variable in the router's
deployment configuration. It can also be set when the router is created by using the
`--strict-sni` command line option.

----
$ oc adm router --strict-sni
----

[[bind-ciphers]]
== TLS Cipher Suites

Set the router xref:../../architecture/networking/routes.adoc#ciphers[cipher suite] using the `--ciphers` option when creating a router:

----
$ oc adm router --ciphers=modern   ....
----

The values are: `modern`, `intermediate`, or `old`, with `intermediate` as the
default. Alternatively, a set of ":" separated ciphers can be provided. The
ciphers must be from the set displayed by:

----
$ openssl ciphers
----

Alternatively, use the `ROUTER_CIPHERS` environment variable for an existing
router.

[[highly-available-routers]]
== Highly-Available Routers

You can
xref:../../admin_guide/high_availability.adoc#admin-guide-high-availability[set
up a highly-available router] on your {product-title} cluster using IP failover.
This setup has multiple replicas on different nodes so the failover software can
switch to another replica if the current one fails.

[[customizing-the-router-service-ports]]
== Customizing the Router Service Ports
You can customize the service ports that a template router binds to by setting
the environment variables
xref:../../architecture/networking/routes.adoc#env-variables[`*ROUTER_SERVICE_HTTP_PORT*`]
and
xref:../../architecture/networking/routes.adoc#env-variables[`*ROUTER_SERVICE_HTTPS_PORT*`].
This can be done by creating a template router, then editing its deployment
configuration.

The following example creates a router deployment with `0` replicas and
customizes the router service HTTP and HTTPS ports, then scales it
appropriately (to `1` replica).

----
$ oc adm router --replicas=0 --ports='10080:10080,10443:10443' //<1>
$ oc set env dc/router ROUTER_SERVICE_HTTP_PORT=10080  \
                   ROUTER_SERVICE_HTTPS_PORT=10443
$ oc scale dc/router --replicas=1
----
<1> Ensures exposed ports are appropriately set for routers that use the
    container networking mode `--host-network=false`.

[IMPORTANT]
====
If you do customize the template router service ports, you will also need to
ensure that the nodes where the router pods run have those custom ports opened
in the firewall (either via Ansible or `iptables`, or any other custom method
that you use via `firewall-cmd`).
====

The following is an example using `iptables` to open the custom router service
ports.

----
$ iptables -A INPUT -p tcp --dport 10080 -j ACCEPT
$ iptables -A INPUT -p tcp --dport 10443 -j ACCEPT
----

[[working-with-multiple-routers]]
== Working With Multiple Routers

An administrator can create multiple routers with the same definition to serve
the same set of routes. Each router will be on a different
xref:../../install_config/router/default_haproxy_router.adoc#adding-nodeselector-to-a-deployment[node]
and will have a different IP address. The network administrator will need to get
the desired traffic to each node.

Multiple routers can be grouped to distribute routing load in the cluster and
separate tenants to different routers or
xref:../../architecture/networking/routes.adoc#router-sharding[shards]. Each
router or shard in the group admits routes based on the selectors in the router.
An administrator can create shards over the whole cluster using
xref:../../architecture/networking/routes.adoc#env-variables[`ROUTE_LABELS`].
A user can create shards over a namespace (project) by using
xref:../../architecture/networking/routes.adoc#env-variables[`NAMESPACE_LABELS`].


[[adding-nodeselector-to-a-deployment]]
== Adding a Node Selector to a Deployment Configuration

Making specific routers deploy on specific nodes requires two steps:

1. Add a
xref:../../admin_guide/manage_nodes.adoc#updating-labels-on-nodes[label]
to the desired node:
+
----
$ oc label node 10.254.254.28 "router=first"
----

2. Add a node selector to the router deployment configuration:
+
----
$ oc edit dc <deploymentConfigName>
----
+
Add the `template.spec.nodeSelector` field with a key and value
corresponding to the label:
+
----
...
  template:
    metadata:
      creationTimestamp: null
      labels:
        router: router1
    spec:
      nodeSelector:      <1>
        router: "first"
...
----
<1> The key and value are `router` and `first`, respectively,
corresponding to the `router=first` label.

[[using-router-shards]]
== Using Router Shards

xref:../../architecture/networking/routes.adoc#router-sharding[_Router
sharding_] uses
xref:../../architecture/networking/routes.adoc#env-variables[`NAMESPACE_LABELS`]
and
xref:../../architecture/networking/routes.adoc#env-variables[`ROUTE_LABELS`], to
filter router namespaces and routes. This enables you to distribute subsets of
routes over multiple router deployments. By using non-overlapping subsets, you
can effectively partition the set of routes. Alternatively, you can define
shards comprising overlapping subsets of routes.

By default, a router selects all routes from all
xref:../../architecture/core_concepts/projects_and_users.adoc#projects[projects
(namespaces)]. Sharding involves adding labels to routes or namespaces and label
selectors to routers. Each router shard comprises the routes that are selected
by a specific set of label selectors or belong to the namespaces that are
selected by a specific set of label selectors.

[NOTE]
====
The router service account
must have the [`cluster reader`] permission set to allow access to labels in other namespaces.
====

*Router Sharding and DNS*

Because an external DNS server is needed to route requests to the desired shard,
the administrator is responsible for making a separate DNS entry
for each router in a project. A router will not forward unknown routes to another router.

Consider the following example:

* Router A lives on host 192.168.0.5 and has routes with `*.foo.com`.
* Router B lives on host 192.168.1.9 and has routes with `*.example.com`.

Separate DNS entries must resolve *.foo.com to the node hosting Router A and *.example.com to the node hosting Router B:

* `*.foo.com A IN 192.168.0.5`
* `*.example.com A IN 192.168.1.9`

*Router Sharding Examples*

This section describes router sharding using namespace and route labels.

[[using-router-shards-namespace]]
.Router Sharding Based on Namespace Labels
image::router_sharding_namespace_labels.png[Router Sharding Based on Namespace Labels]

. Configure a router with a namespace label selector:
+
----
$ oc set env dc/router NAMESPACE_LABELS="router=r1"
----

. Because the router has a selector on the namespace, the router will handle
routes only for matching namespaces. In order to make this selector match
a namespace, label the namespace accordingly:
+
----
$ oc label namespace default "router=r1"
----

. Now, if you create a route in the default namespace, the route is
available in the default router:
+
----
$ oc create -f route1.yaml
----

. Create a new project (namespace) and create a route, `route2`:
+
----
$ oc new-project p1
$ oc create -f route2.yaml
----
+
Notice the route is not available in your router.

. Label namespace `p1` with `router=r1`
+
----
$ oc label namespace p1 "router=r1"
----

Adding this label makes the route available in the router.

Example:: A router deployment `finops-router` is configured with the label
selector `NAMESPACE_LABELS="name in (finance, ops)"`, and a router deployment
`dev-router` is configured with the label selector
`NAMESPACE_LABELS="name=dev"`.
+
If all routes are in namespaces labeled `name=finance`, `name=ops`, and
`name=dev`, then this configuration effectively distributes your routes between
the two router deployments.
+
In the above scenario, sharding becomes a special case of partitioning,
with no overlapping subsets. Routes are divided between router shards.
+
The criteria for route selection govern how the routes are distributed. It is
possible to have overlapping subsets of routes across router deployments.

Example:: In addition to `finops-router` and `dev-router` in the example
above, you also have `devops-router`, which is configured with a label selector
`NAMESPACE_LABELS="name in (dev, ops)"`.
+
The routes in namespaces labeled `name=dev` or `name=ops` now are serviced by
two different router deployments. This becomes a case in which you have defined
overlapping subsets of routes, as illustrated in the procedure in xref:using-router-shards-namespace[Router Sharding Based on Namespace Labels].
+
In addition, this enables you to create more complex routing rules, allowing the
diversion of higher priority traffic to the dedicated `finops-router` while sending
lower priority traffic to `devops-router`.

[[using-router-shards-route]]
.Router Sharding Based on Route Labels

`NAMESPACE_LABELS` allows filtering of the projects to service and selecting all
the routes from those projects, but you may want to partition routes based on
other criteria associated with the routes themselves. The `ROUTE_LABELS`
selector allows you to slice-and-dice the routes themselves.

Example:: A router deployment `prod-router` is configured with the label selector
`ROUTE_LABELS="mydeployment=prod"`, and a router deployment `devtest-router` is
configured with the label selector `ROUTE_LABELS="mydeployment in (dev, test)"`.
+
This configuration partitions routes between the two router deployments
according to the routes' labels, irrespective of their namespaces.
+
The example assumes you have all the routes you want to be serviced tagged with
a label `"mydeployment=<tag>"`.

[[creating-router-shards]]
=== Creating Router Shards

This section describes an advanced example of router sharding.
Suppose there are 26 routes, named `a` -- `z`,
with various labels:

.Possible labels on routes
----
sla=high       geo=east     hw=modest     dept=finance
sla=medium     geo=west     hw=strong     dept=dev
sla=low                                   dept=ops
----

These labels express the concepts including service level agreement,
geographical location, hardware requirements, and department. The routes can
have at most one label from each column. Some routes may have other labels or no
labels at all.

[options="header",cols="1,1,1,1,1,3"]
|===
|Name(s) |SLA |Geo |HW |Dept |Other Labels

|`a`
|`high`
|`east`
|`modest`
|`finance`
|`type=static`

|`b`
|
|`west`
|`strong`
|
|`type=dynamic`

|`c`, `d`, `e`
|`low`
|
|`modest`
|
|`type=static`

|`g` -- `k`
|`medium`
|
|`strong`
|`dev`
|

|`l` -- `s`
|`high`
|
|`modest`
|`ops`
|

|`t` -- `z`
|
|`west`
|
|
|`type=dynamic`

|===

Here is a convenience script *_mkshard_*  that
illustrates how `oc adm router`, `oc set env`, and `oc scale`
can be used together to make a router shard.

[source,bash]
----
#!/bin/bash
# Usage: mkshard ID SELECTION-EXPRESSION
id=$1
sel="$2"
router=router-shard-$id           //<1>
oc adm router $router --replicas=0  //<2>
dc=dc/router-shard-$id            //<3>
oc set env   $dc ROUTE_LABELS="$sel"  //<4>
oc scale $dc --replicas=3         //<5>
----
<1> The created router has name `router-shard-<id>`.
<2> Specify no scaling for now.
<3> The deployment configuration for the router.
<4> Set the selection expression using `oc set env`.
    The selection expression is the value of
    the `ROUTE_LABELS` environment variable.
<5> Scale it up.

Running *_mkshard_* several times creates several routers:

[options="header",cols="2,3,2"]
|===
|Router |Selection Expression |Routes

|`router-shard-1`
|`sla=high`
|`a`, `l` -- `s`

|`router-shard-2`
|`geo=west`
|`b`, `t` -- `z`

|`router-shard-3`
|`dept=dev`
|`g` -- `k`

|===


[[modifying-router-shards]]
=== Modifying Router Shards

Because a router shard is a construct
xref:../../architecture/networking/routes.adoc#router-sharding[based on labels],
you can modify either the labels (via
xref:../../cli_reference/basic_cli_operations.adoc#oc-label[`oc label`])
or the selection expression (via xref:../../cli_reference/basic_cli_operations.adoc#oc-set-env[`oc set env`]).

This section extends the example started in the
xref:creating-router-shards[Creating Router Shards] section,
demonstrating how to change the selection expression.

Here is a convenience script *_modshard_* that modifies
an existing router to use a new selection expression:

[source,bash]
----
#!/bin/bash
# Usage: modshard ID SELECTION-EXPRESSION...
id=$1
shift
router=router-shard-$id       //<1>
dc=dc/$router                 //<2>
oc scale $dc --replicas=0     //<3>
oc set env   $dc "$@"             //<4>
oc scale $dc --replicas=3     //<5>
----
<1> The modified router has name `router-shard-<id>`.
<2> The deployment configuration where the modifications occur.
<3> Scale it down.
<4> Set the new selection expression using `oc set env`.
    Unlike `mkshard` from the
    xref:creating-router-shards[Creating Router Shards]
    section, the selection expression specified as the
    non-`ID` arguments to `modshard` must include the
    environment variable name as well as its value.
<5> Scale it back up.

[NOTE]
====
In `modshard`, the `oc scale` commands are not necessary if the
xref:../../dev_guide/deployments/deployment_strategies.adoc#strategies[deployment strategy]
for `router-shard-<id>` is `Rolling`.
====

For example, to expand the department for `router-shard-3`
to include `ops` as well as `dev`:

----
$ modshard 3 ROUTE_LABELS='dept in (dev, ops)'
----

The result is that `router-shard-3` now selects routes `g` -- `s`
(the combined sets of `g` -- `k` and `l` -- `s`).

This example takes into account that
there are only three departments in this example scenario,
and specifies a department to leave out of the shard,
thus achieving the same result as the preceding example:

----
$ modshard 3 ROUTE_LABELS='dept != finance'
----

This example specifies three comma-separated qualities,
and results in only route `b` being selected:

----
$ modshard 3 ROUTE_LABELS='hw=strong,type=dynamic,geo=west'
----

Similarly to `ROUTE_LABELS`, which involves a route's labels,
you can select routes based on the labels of the route's namespace
using the `NAMESPACE_LABELS` environment variable.
This example modifies `router-shard-3` to serve
routes whose namespace has the label `frequency=weekly`:

----
$ modshard 3 NAMESPACE_LABELS='frequency=weekly'
----

The last example combines `ROUTE_LABELS` and `NAMESPACE_LABELS`
to select routes with label `sla=low` and
whose namespace has the label `frequency=weekly`:

----
$ modshard 3 \
    NAMESPACE_LABELS='frequency=weekly' \
    ROUTE_LABELS='sla=low'
----

[[finding-router-hostname]]
== Finding the Host Name of the Router

When exposing a service, a user can use the same route from the DNS name that
external users use to access the application. The network administrator of the
external network must make sure the host name resolves to the name of a router
that has admitted the route. The user can set up their DNS with a CNAME that
points to this host name. However, the user may not know the host name of the
router. When it is not known, the cluster administrator can provide it.

The cluster administrator can use the `--router-canonical-hostname` option with
the router's canonical host name when creating the router. For example:

----
# oc adm router myrouter --router-canonical-hostname="rtr.example.com"
----

This creates the `ROUTER_CANONICAL_HOSTNAME` environment variable in the router's
deployment configuration containing the host name of the router.

For routers that already exist, the cluster administrator can edit the router's
deployment configuration and add the `ROUTER_CANONICAL_HOSTNAME` environment
variable:

----
spec:
  template:
    spec:
      containers:
        - env:
          - name: ROUTER_CANONICAL_HOSTNAME
            value: rtr.example.com
----

The `ROUTER_CANONICAL_HOSTNAME` value is displayed in the route status for all
routers that have admitted the route. The route status is refreshed every time
the router is reloaded.

When a user creates a route, all of the active routers evaluate the route and,
if conditions are met, admit it. When a router that defines the
`ROUTER_CANONICAL_HOSTNAME` environment variable admits the route, the router
places the value in the `routerCanonicalHostname` field in the route status. The
user can examine the route status to determine which, if any, routers have
admitted the route, select a router from the list, and find the
host name of the router to pass along to the network administrator.

----
status:
  ingress:
    conditions:
      lastTransitionTime: 2016-12-07T15:20:57Z
      status: "True"
      type: Admitted
      host: hello.in.mycloud.com
      routerCanonicalHostname: rtr.example.com
      routerName: myrouter
      wildcardPolicy: None
----

`oc describe` inclues the host name when available:

----
$ oc describe route/hello-route3
...
Requested Host: hello.in.mycloud.com exposed on router myroute (host rtr.example.com) 12 minutes ago
----

Using the above information, the user can ask the DNS administrator to set up a
CNAME from the route's host, `hello.in.mycloud.com`, to the router's canonical
hostname, `rtr.example.com`. This results in any traffic to
`hello.in.mycloud.com` reaching the user's application.


[[customizing-the-default-routing-subdomain]]
== Customizing the Default Routing Subdomain
You can customize the suffix used as the default routing subdomain for your
environment by modifying the
xref:../../install_config/master_node_configuration.adoc#master-configuration-files[master
configuration file] (the *_/etc/origin/master/master-config.yaml_* file by
default). Routes that do not specify a host name would have one generated using
this default routing subdomain.

The following example shows how you can set the configured suffix
to *v3.openshift.test*:

----
routingConfig:
  subdomain: v3.openshift.test
----

[NOTE]
====
This change requires a restart of the master if it is running.
====

With the {product-title} master(s) running the above configuration, the
xref:../../architecture/networking/routes.adoc#route-hostnames[generated host
name] for the example of a route named *no-route-hostname* without a
host name added to a namespace *mynamespace* would be:

----
no-route-hostname-mynamespace.v3.openshift.test
----

[[forcing-route-hostnames-to-a-custom-routing-subdomain]]
== Forcing Route Host Names to a Custom Routing Subdomain
If an administrator wants to restrict all routes to a specific routing
subdomain, they can pass the `--force-subdomain` option to the `oc adm
router` command. This forces the router to override any host names specified in
a route and generate one based on the template provided to the
`--force-subdomain` option.

The following example runs a router, which overrides the route host names using
a custom subdomain template `${name}-${namespace}.apps.example.com`.

----
$ oc adm router --force-subdomain='${name}-${namespace}.apps.example.com'
----

[[using-wildcard-certificates]]
== Using Wildcard Certificates

A TLS-enabled route that does not include a certificate uses the router's
default certificate instead. In most cases, this certificate should be provided
by a trusted certificate authority, but for convenience you can use the
{product-title} CA to create the certificate. For example:

----
$ CA=/etc/origin/master
$ oc adm ca create-server-cert --signer-cert=$CA/ca.crt \
      --signer-key=$CA/ca.key --signer-serial=$CA/ca.serial.txt \
      --hostnames='*.cloudapps.example.com' \
      --cert=cloudapps.crt --key=cloudapps.key
----

[NOTE]
====
The `oc adm ca create-server-cert` command generates a certificate that is valid
for two years. This can be altered with the `--expire-days` option, but for
security reasons, it is recommended to not make it greater than this value.

Run `oc adm` commands only from the first master listed in the Ansible host inventory file,
by default *_/etc/ansible/hosts_*.
====

The router expects the certificate and key to be in PEM format in a single
file:

----
$ cat cloudapps.crt cloudapps.key $CA/ca.crt > cloudapps.router.pem
----

From there you can use the `--default-cert` flag:

----
$ oc adm router --default-cert=cloudapps.router.pem --service-account=router
----

[NOTE]
====
Browsers only consider wildcards valid for subdomains one
level deep. So in this example, the certificate would be valid for
_a.cloudapps.example.com_ but not for _a.b.cloudapps.example.com_.
====

[[manually-redeploy-certs]]
== Manually Redeploy Certificates

To manually redeploy the router certificates:

. Check to see if a secret containing the default router certificate was added to the router:
+
----
$ oc set volume dc/router

deploymentconfigs/router
  secret/router-certs as server-certificate
    mounted at /etc/pki/tls/private
----
+
If the certificate is added, skip the following step and overwrite the secret.

. Make sure that you have a default certificate directory set for the following variable `DEFAULT_CERTIFICATE_DIR`:
+
----
$ oc set env dc/router --list

DEFAULT_CERTIFICATE_DIR=/etc/pki/tls/private
----
+
If not, create the directory using the following command:
+
----
$ oc set env dc/router DEFAULT_CERTIFICATE_DIR=/etc/pki/tls/private
----

. Export the certificate to PEM format:
+
----
$ cat custom-router.key custom-router.crt custom-ca.crt > custom-router.crt
----

. Overwrite or create a router certificate secret:
+
If the certificate secret was added to the router, overwrite the secret. If not, create a new secret.
+
To overwrite the secret, run the following command:
+
----
$ oc create secret generic router-certs --from-file=tls.crt=custom-router.crt --from-file=tls.key=custom-router.key --type=kubernetes.io/tls -o json | oc replace -f -
----
+
To create a new secret, run the following commands:
+
----
$ oc create secret generic router-certs --from-file=tls.crt=custom-router.crt --from-file=tls.key=custom-router.key --type=kubernetes.io/tls

$ oc set volume dc/router --add --mount-path=/etc/pki/tls/private --secret-name='router-certs' --name router-certs
----

. Deploy the router.
+
----
$ oc rollout latest dc/router
----

[[using-secured-routes]]
== Using Secured Routes

Currently, password protected key files are not supported. HAProxy prompts
for a password upon starting and does not have a way to automate this process.
To remove a passphrase from a keyfile, you can run:

----
# openssl rsa -in <passwordProtectedKey.key> -out <new.key>
----

Here is an example of how to use a secure edge terminated route with TLS
termination occurring on the router before traffic is proxied to the
destination. The secure edge terminated route specifies the TLS certificate
and key information. The TLS certificate is served by the router front end.

First, start up a router instance:

----
# oc adm router --replicas=1 --service-account=router
----

Next, create a private key, csr and certificate for our edge secured route.
The instructions on how to do that would be specific to your certificate
authority and provider. For a simple self-signed certificate for a domain
named `www.example.test`, see the example shown below:

----
# sudo openssl genrsa -out example-test.key 2048
#
# sudo openssl req -new -key example-test.key -out example-test.csr  \
  -subj "/C=US/ST=CA/L=Mountain View/O=OS3/OU=Eng/CN=www.example.test"
#
# sudo openssl x509 -req -days 366 -in example-test.csr  \
      -signkey example-test.key -out example-test.crt
----

Generate a route using the above certificate and key.

----
$ oc create route edge --service=my-service \
    --hostname=www.example.test \
    --key=example-test.key --cert=example-test.crt
route "my-service" created
----

Look at its definition.

----
$ oc get route/my-service -o yaml
apiVersion: v1
kind: Route
metadata:
  name:  my-service
spec:
  host: www.example.test
  to:
    kind: Service
    name: my-service
  tls:
    termination: edge
    key: |
      -----BEGIN PRIVATE KEY-----
      [...]
      -----END PRIVATE KEY-----
    certificate: |
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
----

Make sure your DNS entry for `www.example.test` points to your router
instance(s) and the route to your domain should be available.
The example below uses curl along with a local resolver to simulate the
DNS lookup:

----
# routerip="4.1.1.1"  #  replace with IP address of one of your router instances.
# curl -k --resolve www.example.test:443:$routerip https://www.example.test/
----

[[using-wildcard-routes]]
== Using Wildcard Routes (for a Subdomain)

The HAProxy router has support for wildcard routes, which are enabled by setting
the `ROUTER_ALLOW_WILDCARD_ROUTES` environment variable to `true`. Any routes
with a wildcard policy of `Subdomain` that pass the router admission checks will
be serviced by the HAProxy router. Then, the HAProxy router exposes the
associated service (for the route) per the route's wildcard policy.

[IMPORTANT]
====
To change a route's wildcard policy, you must remove the route and recreate it
with the updated wildcard policy. Editing only the route's wildcard policy in a
route's *_.yaml_* file does not work.
====


----
$ oc adm router --replicas=0 ...
$ oc set env dc/router ROUTER_ALLOW_WILDCARD_ROUTES=true
$ oc scale dc/router --replicas=1
----

xref:../../install_config/web_console_customization.adoc#web-console-enable-wildcard-routes[Learn how to configure the web console for wildcard routes].

.Using a Secure Wildcard Edge Terminated Route
This example reflects TLS termination occurring on the router before traffic is
proxied to the destination. Traffic sent to any hosts in the subdomain
`example.org` (`*.example.org`) is proxied to the exposed service.

The secure edge terminated route specifies the TLS certificate and key
information. The TLS certificate is served by the router front end for all hosts
that match the subdomain (`*.example.org`).

. Start up a router instance:
+
----
$ oc adm router --replicas=0 --service-account=router
$ oc set env dc/router ROUTER_ALLOW_WILDCARD_ROUTES=true
$ oc scale dc/router --replicas=1
----

. Create a private key, certificate signing request (CSR), and certificate for the
edge secured route.
+
The instructions on how to do this are specific to your certificate authority
and provider. For a simple self-signed certificate for a domain named
`*.example.test`, see this example:
+
----
# sudo openssl genrsa -out example-test.key 2048
#
# sudo openssl req -new -key example-test.key -out example-test.csr  \
  -subj "/C=US/ST=CA/L=Mountain View/O=OS3/OU=Eng/CN=*.example.test"
#
# sudo openssl x509 -req -days 366 -in example-test.csr  \
      -signkey example-test.key -out example-test.crt
----

. Generate a wildcard route using the above certificate and key:
+
----
$ cat > route.yaml  <<REOF
apiVersion: v1
kind: Route
metadata:
  name:  my-service
spec:
  host: www.example.test
  wildcardPolicy: Subdomain
  to:
    kind: Service
    name: my-service
  tls:
    termination: edge
    key: "$(perl -pe 's/\n/\\n/' example-test.key)"
    certificate: "$(perl -pe 's/\n/\\n/' example-test.cert)"
REOF
$ oc create -f route.yaml
----
+
Ensure your DNS entry for `*.example.test` points to your router instance(s) and
the route to your domain is available.
+
This example uses `curl` with a local resolver to simulate the DNS lookup:
+
----
# routerip="4.1.1.1"  #  replace with IP address of one of your router instances.
# curl -k --resolve www.example.test:443:$routerip https://www.example.test/
# curl -k --resolve abc.example.test:443:$routerip https://abc.example.test/
# curl -k --resolve anyname.example.test:443:$routerip https://anyname.example.test/
----

For routers that allow wildcard routes (`ROUTER_ALLOW_WILDCARD_ROUTES` set to
`true`), there are some caveats to the ownership of a subdomain associated with
a wildcard route.

Prior to wildcard routes, ownership was based on the claims made for a host name
with the namespace with the oldest route winning against any other claimants.
For example, route `r1` in namespace `ns1` with a claim for `one.example.test`
would win over another route `r2` in namespace `ns2` for the same host name
`one.example.test` if route `r1` was older than route `r2`.

In addition, routes in other namespaces were allowed to claim non-overlapping
hostnames. For example, route `rone` in namespace `ns1` could claim
`www.example.test` and another route `rtwo` in namespace `d2` could claim
`c3po.example.test`.

This is still the case if there are _no_ wildcard routes claiming that same
subdomain (`example.test` in the above example).

However, a wildcard route needs to claim all of the host names within a
subdomain (host names of the form `\*.example.test`). A wildcard route's claim
is allowed or denied based on whether or not the oldest route for that subdomain
(`example.test`) is in the same namespace as the wildcard route. The oldest
route can be either a regular route or a wildcard route.

For example, if there is already a route `eldest` that exists in the `ns1`
namespace that claimed a host named `owner.example.test` and, if at a later
point in time, a new wildcard route `wildthing` requesting for routes in that
subdomain (`example.test`) is added, the claim by the wildcard route will _only_
be allowed if it is the same namespace (`ns1`) as the owning route.

The following examples illustrate various scenarios in which claims for wildcard
routes will succeed or fail.

In the example below, a router that allows wildcard routes will allow
non-overlapping claims for hosts in the subdomain `example.test` as long as a
wildcard route has not claimed a subdomain.

----
$ oc adm router ...
$ oc set env dc/router ROUTER_ALLOW_WILDCARD_ROUTES=true

$ oc project ns1
$ oc expose service myservice --hostname=owner.example.test
$ oc expose service myservice --hostname=aname.example.test
$ oc expose service myservice --hostname=bname.example.test

$ oc project ns2
$ oc expose service anotherservice --hostname=second.example.test
$ oc expose service anotherservice --hostname=cname.example.test

$ oc project otherns
$ oc expose service thirdservice --hostname=emmy.example.test
$ oc expose service thirdservice --hostname=webby.example.test
----

In the example below, a router that allows wildcard routes will not allow the
claim for `owner.example.test` or `aname.example.test` to succeed since the
owning namespace is `ns1`.

----
$ oc adm router ...
$ oc set env dc/router ROUTER_ALLOW_WILDCARD_ROUTES=true

$ oc project ns1
$ oc expose service myservice --hostname=owner.example.test
$ oc expose service myservice --hostname=aname.example.test

$ oc project ns2
$ oc expose service secondservice --hostname=bname.example.test
$ oc expose service secondservice --hostname=cname.example.test

$ # Router will not allow this claim with a different path name `/p1` as
$ # namespace `ns1` has an older route claiming host `aname.example.test`.
$ oc expose service secondservice --hostname=aname.example.test --path="/p1"

$ # Router will not allow this claim as namespace `ns1` has an older route
$ # claiming host name `owner.example.test`.
$ oc expose service secondservice --hostname=owner.example.test

$ oc project otherns

$ # Router will not allow this claim as namespace `ns1` has an older route
$ # claiming host name `aname.example.test`.
$ oc expose service thirdservice --hostname=aname.example.test
----

In the example below, a router that allows wildcard routes will allow the claim
for ``\*.example.test` to succeed since the owning namespace is `ns1` and the
wildcard route belongs to that same namespace.

----
$ oc adm router ...
$ oc set env dc/router ROUTER_ALLOW_WILDCARD_ROUTES=true

$ oc project ns1
$ oc expose service myservice --hostname=owner.example.test

$ # Reusing the route.yaml from the previous example.
$ # spec:
$ #   host: www.example.test
$ #   wildcardPolicy: Subdomain

$ oc create -f route.yaml   #  router will allow this claim.
----

In the example below, a router that allows wildcard routes will not allow
the claim for ``\*.example.test` to succeed since the owning namespace is `ns1`
and the wildcard route belongs to another namespace `cyclone`.

----
$ oc adm router ...
$ oc set env dc/router ROUTER_ALLOW_WILDCARD_ROUTES=true

$ oc project ns1
$ oc expose service myservice --hostname=owner.example.test

$ # Switch to a different namespace/project.
$ oc project cyclone

$ # Reusing the route.yaml from a prior example.
$ # spec:
$ #   host: www.example.test
$ #   wildcardPolicy: Subdomain

$ oc create -f route.yaml   #  router will deny (_NOT_ allow) this claim.
----

Similarly, once a namespace with a wildcard route claims a subdomain, only
routes within that namespace can claim any hosts in that same subdomain.

In the example below, once a route in namespace `ns1` with a wildcard route
claims subdomain `example.test`, only routes in the namespace `ns1` are allowed
to claim any hosts in that same subdomain.

----
$ oc adm router ...
$ oc set env dc/router ROUTER_ALLOW_WILDCARD_ROUTES=true

$ oc project ns1
$ oc expose service myservice --hostname=owner.example.test

$ oc project otherns

$ # namespace `otherns` is allowed to claim for other.example.test
$ oc expose service otherservice --hostname=other.example.test

$ oc project ns1

$ # Reusing the route.yaml from the previous example.
$ # spec:
$ #   host: www.example.test
$ #   wildcardPolicy: Subdomain

$ oc create -f route.yaml   #  Router will allow this claim.

$ #  In addition, route in namespace otherns will lose its claim to host
$ #  `other.example.test` due to the wildcard route claiming the subdomain.

$ # namespace `ns1` is allowed to claim for deux.example.test
$ oc expose service mysecondservice --hostname=deux.example.test

$ # namespace `ns1` is allowed to claim for deux.example.test with path /p1
$ oc expose service mythirdservice --hostname=deux.example.test --path="/p1"

$ oc project otherns

$ # namespace `otherns` is not allowed to claim for deux.example.test
$ # with a different path '/otherpath'
$ oc expose service otherservice --hostname=deux.example.test --path="/otherpath"

$ # namespace `otherns` is not allowed to claim for owner.example.test
$ oc expose service yetanotherservice --hostname=owner.example.test

$ # namespace `otherns` is not allowed to claim for unclaimed.example.test
$ oc expose service yetanotherservice --hostname=unclaimed.example.test
----

In the example below,  different scenarios are shown, in which the owner routes
are deleted and ownership is passed within and across namespaces. While a route
claiming host `eldest.example.test` in the namespace `ns1` exists, wildcard
routes in that namespace can claim subdomain `example.test`. When the route for
host `eldest.example.test` is deleted, the next oldest route
`senior.example.test` would become the oldest route and would not affect any
other routes. Once the route for host `senior.example.test` is deleted, the next
oldest route `junior.example.test` becomes the oldest route and block the
wildcard route claimant.

----
$ oc adm router ...
$ oc set env dc/router ROUTER_ALLOW_WILDCARD_ROUTES=true

$ oc project ns1
$ oc expose service myservice --hostname=eldest.example.test
$ oc expose service seniorservice --hostname=senior.example.test

$ oc project otherns

$ # namespace `otherns` is allowed to claim for other.example.test
$ oc expose service juniorservice --hostname=junior.example.test

$ oc project ns1

$ # Reusing the route.yaml from the previous example.
$ # spec:
$ #   host: www.example.test
$ #   wildcardPolicy: Subdomain

$ oc create -f route.yaml   #  Router will allow this claim.

$ #  In addition, route in namespace otherns will lose its claim to host
$ #  `junior.example.test` due to the wildcard route claiming the subdomain.

$ # namespace `ns1` is allowed to claim for dos.example.test
$ oc expose service mysecondservice --hostname=dos.example.test

$ # Delete route for host `eldest.example.test`, the next oldest route is
$ # the one claiming `senior.example.test`, so route claims are unaffacted.
$ oc delete route myservice

$ # Delete route for host `senior.example.test`, the next oldest route is
$ # the one claiming `junior.example.test` in another namespace, so claims
$ # for a wildcard route would be affected. The route for the host
$ # `dos.example.test` would be unaffected as there are no other wildcard
$ # claimants blocking it.
$ oc delete route seniorservice
----

[[using-the-container-network-stack]]
== Using the Container Network Stack

The {product-title} router runs inside a container and the default behavior is
to use the network stack of the host (i.e., the node where the router container
runs). This default behavior benefits performance because network traffic from
remote clients does not need to take multiple hops through user space to reach
the target service and container.

Additionally, this default behavior enables the router to get the actual source
IP address of the remote connection rather than getting the node's IP address.
This is useful for defining ingress rules based on the originating IP,
supporting sticky sessions, and monitoring traffic, among other uses.

This host network behavior is controlled by the `--host-network` router command
line option, and the default behaviour is the equivalent of using
`--host-network=true`. If you wish to run the router with the container network
stack, use the `--host-network=false` option when creating the router. For
example:

ifdef::openshift-enterprise[]
----
$ oc adm router --service-account=router --host-network=false
----
endif::[]
ifdef::openshift-origin[]
----
$ oc adm router --service-account=router --host-network=false
----
endif::[]

Internally, this means the router container must publish the 80 and 443
ports in order for the external network to communicate with the router.

[NOTE]
====
Running with the container network stack means that the router sees the source
IP address of a connection to be the NATed IP address of the node, rather than
the actual remote IP address.
====

[NOTE]
====
On {product-title} clusters using
xref:../../architecture/networking/sdn.adoc#network-isolation-multitenant[multi-tenant
network isolation], routers on a non-default namespace with the
`--host-network=false` option will load all routes in the cluster, but routes
across the namespaces will not be reachable due to network isolation. With the
`--host-network=true` option, routes bypass the container network and it can
access any pod in the cluster. If isolation is needed in this case, then do not
add routes across the namespaces.
====

[[using-the-dynamic-configuration-manager]]
== Using the Dynamic Configuration Manager

The HAProxy router supports a dynamic configuration manager to bring certain
types of routes online without requiring any haproxy reload downtime.
The dynamic configuration manager support can be enabled by setting
the `ROUTER_HAPROXY_CONFIG_MANAGER` environment variable to `true`.
When the dynamic configuration manager is enabled, it will handle any route and
endpoint lifecyle events such as route and endpoint `addition|deletion|update`.
In the previous releases if any of these events caused the configuration to
change, it resulted in the router requiring a full configuration re-write and a
haproxy process reload.

The dynamic configuration internally uses the haproxy socket and configuration
API with a pool of pre-allocated routes and backend servers to dynamically
achieve the same results without requiring a haproxy process reload.
In the event the dynamic configuration manager is unable to configure haproxy
dynamically, it forces a configuration re-write and a haproxy process reload.
This can occur in cases like if a newly added route contains custom annotations
that would cause a drift from the standard configuration (custom timeouts) or
if the route requires custom TLS configuration.
The pre-allocated pool of routes is created using route blueprints - the default
set of blueprints supports unsecured routes, edge secured routes (without any
custom TLS configuration) and passthrough routes.
`re-encypt` routes require custom TLS configuration information and so are not
supported out of the box. However that said, the blueprints that the dynamic
configuration manager uses can be extended by setting the `ROUTER_BLUEPRINT_ROUTE_NAMESPACE`
and optionally the `ROUTER_BLUEPRINT_ROUTE_LABELS` environment variable values.
All routes (or the routes matching the route labels) in the blueprint route
namespace will be processed as custom blueprints similar to the default set of
blueprints. This enables defining certain types of routes such as `re-encypt`
routes or routes that use custom annotations or routes with custom TLS
configuration as blueprints.

Assuming you have a project with some routes, as below,
====
----
$ oc new-project blues
$ oc create route edge edge-blueprint-1 --key=/path/to/key.pem \
      --cert=/path/to/cert.pem --service=<service> --port=8443
$ oc label route edge-blueprint-1 type=3oclock-blues
$ # create 3 different routes (all have the label type=3oclock-blues)
$ oc create -f reencrypt-blueprint.yaml
$ oc create -f annotated-edge-blueprint.yaml
$ oc create -f annotated-unsecured-blueprint.json
$ oc label route annotated-unsecured-blueprint type-
----
====

When you create a router, as below,
----
$ oc adm router
$ oc set env dc/router ROUTER_HAPROXY_CONFIG_MANAGER=true      \
                       ROUTER_BLUEPRINT_ROUTE_NAMESPACE=blues  \
                       ROUTER_BLUEPRINT_ROUTE_LABELS="type=3oclock-blues"
----
====
all routes in the namespace or project `blues` with label `type=3oclock-blues`
will be processed and used as custom blueprints.
In the above example, the `annotated-unsecured-blueprint` will *_not_* be used
as a blueprint route since we explicitly removed the label from that route.

Note here that you can also `add|update|remove` blueprints on-the-fly by
managing the routes as you would normally in that namespace `blues`.
The dynamic configuration manager watches for changes to routes in the
namespace `blues` similar to how the router watches for `routes` and `services`.

The pool sizes of the pre-allocated routes and backend servers can be controlled
by the `ROUTER_BLUEPRINT_ROUTE_POOL_SIZE` (default: 10) and
`ROUTER_MAX_DYNAMIC_SERVERS` (default: 5) environment variables.
And you can also control how often changes made by the dynamic configuration
manager are committed to disk (haproxy configuration is re-written and the
haproxy process is reloaded). The default is 1 hour (3600 seconds) or when the
dynamic configuration manager runs out of pool space - which ever is sooner.
The `COMMIT_INTERVAL` environment variable controls this setting.

Example::
====
----
$ oc set env dc/router -c router ROUTER_BLUEPRINT_ROUTE_POOL_SIZE=20  \
      ROUTER_MAX_DYNAMIC_SERVERS=3 COMMIT_INTERVAL=6h
----
====
The above example increases the pool size for each blueprint route to 20,
reduces the number of dynamic servers to 3 and increases the commit interval
to 6 hours.


[[exposing-the-router-metrics]]
== Exposing Router Metrics

The
xref:../../architecture/networking/assembly_available_router_plugins.adoc#haproxy-metrics[HAProxy router metrics]
are, by default, exposed or published in
link:https://prometheus.io/docs/concepts/data_model/[Prometheus format]
for consumption by external metrics collection and aggregation systems (e.g. Prometheus, statsd).
Metrics are also available directly from the
link:https://cbonte.github.io/haproxy-dconv/1.5/configuration.html#9[HAProxy router] in its own HTML format for viewing in a browser
or CSV download.
These metrics include the HAProxy native metrics and some controller metrics.

When you create a router using the following command, {product-title} makes metrics available in Prometheus format on the stats port, by default 1936.

----
$ oc adm router --service-account=router
----

* To extract the raw statistics in Prometheus format run the following command:
+
----
curl <user>:<password>@<router_IP>:<STATS_PORT>
----
+
For example:
+
----
$ curl admin:sLzdR6SgDJ@10.254.254.35:1936/metrics
----
+
You can get the information you need to access the metrics from the router service annotations:
+
----
$ oc edit router service <router-service-name>

apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/port: "1936"
    prometheus.io/scrape: "true"
    prometheus.openshift.io/password: IImoDqON02
    prometheus.openshift.io/username: admin
----
+
The `prometheus.io/port` is the stats port, by default 1936. You might need to configure your firewall to permit access.
Use the previous user name and password to access the metrics. The path is *_/metrics_*.
+
----
$ curl <user>:<password>@<router_IP>:<STATS_PORT>
for example:
$ curl admin:sLzdR6SgDJ@10.254.254.35:1936/metrics
...
# HELP haproxy_backend_connections_total Total number of connections.
# TYPE haproxy_backend_connections_total gauge
haproxy_backend_connections_total{backend="http",namespace="default",route="hello-route"} 0
haproxy_backend_connections_total{backend="http",namespace="default",route="hello-route-alt"} 0
haproxy_backend_connections_total{backend="http",namespace="default",route="hello-route01"} 0
...
# HELP haproxy_exporter_server_threshold Number of servers tracked and the current threshold value.
# TYPE haproxy_exporter_server_threshold gauge
haproxy_exporter_server_threshold{type="current"} 11
haproxy_exporter_server_threshold{type="limit"} 500
...
# HELP haproxy_frontend_bytes_in_total Current total of incoming bytes.
# TYPE haproxy_frontend_bytes_in_total gauge
haproxy_frontend_bytes_in_total{frontend="fe_no_sni"} 0
haproxy_frontend_bytes_in_total{frontend="fe_sni"} 0
haproxy_frontend_bytes_in_total{frontend="public"} 119070
...
# HELP haproxy_server_bytes_in_total Current total of incoming bytes.
# TYPE haproxy_server_bytes_in_total gauge
haproxy_server_bytes_in_total{namespace="",pod="",route="",server="fe_no_sni",service=""} 0
haproxy_server_bytes_in_total{namespace="",pod="",route="",server="fe_sni",service=""} 0
haproxy_server_bytes_in_total{namespace="default",pod="docker-registry-5-nk5fz",route="docker-registry",server="10.130.0.89:5000",service="docker-registry"} 0
haproxy_server_bytes_in_total{namespace="default",pod="hello-rc-vkjqx",route="hello-route",server="10.130.0.90:8080",service="hello-svc-1"} 0
...
----

* To get metrics in a browser:
+
. Delete the following xref:../../architecture/networking/routes.adoc#env-variables[environment variables]
from the router deployment configuration file:
+
----
$ oc edit service router

- name: ROUTER_LISTEN_ADDR
  value: 0.0.0.0:1936
- name: ROUTER_METRICS_TYPE
  value: haproxy
----
+
. Launch the stats window using the following URL in a browser, where the `STATS_PORT` value is `1936` by default:
+
----
http://admin:<Password>@<router_IP>:<STATS_PORT>
----
+
You can get the stats in CSV format by adding `;csv` to the URL:
+
For example:
+
----
http://admin:<Password>@<router_IP>:1936;csv
----
+
To get the router IP, admin name, and password:
+
----
oc describe pod <router_pod>
----

* To suppress metrics collection:
+
----
$ oc adm router --service-account=router --stats-port=0
----

[[preventing-connection-failures-during-restarts]]
== Preventing Connection Failures During Restarts

If you connect to the router while the proxy is reloading, there is a small
chance that your connection will end up in the wrong network queue and be
dropped. The issue is being addressed. In the meantime, it is possible to work
around the problem by installing `iptables` rules to prevent connections during
the reload window. However, doing so means that the router needs to run with
elevated privilege so that it can manipulate `iptables` on the host. It also

means that connections that happen during the reload are temporarily ignored and
must retransmit their connection start, lengthening the time it takes to
connect, but preventing connection failure.

To prevent this, configure the router to use `iptables` by changing the service
account, and setting an environment variable on the router.

*Use a Privileged SCC*

When creating the router, allow it to use the privileged SCC. This gives the
router user the ability to create containers with root privileges on the nodes:

----
$ oc adm policy add-scc-to-user privileged -z router
----

*Patch the Router Deployment Configuration to Create a Privileged Container*

You can now create privileged containers. Next, configure the router deployment
configuration to use the privilege so that the router can set the iptables rules
it needs. This patch changes the router deployment configuration so that the
container that is created runs as privileged (and therefore gets correct
capabilities) and run as root:

----
$ oc patch dc router -p '{"spec":{"template":{"spec":{"containers":[{"name":"router","securityContext":{"privileged":true}}],"securityContext":{"runAsUser": 0}}}}}'
----

*Configure the Router to Use iptables*

Set the option on the router deployment configuration:

----
$ oc set env dc/router -c router DROP_SYN_DURING_RESTART=1
----

If you used a non-default name for the router, you must change *_dc/router_*
accordingly.

[[deploy-router-arp-cach-tuning-for-large-scale-clusters]]
== ARP Cache Tuning for Large-scale Clusters

In {product-title} clusters with large numbers of routes (greater than the
value of `net.ipv4.neigh.default.gc_thresh3`, which is `65536` by default), you
must increase the default values of sysctl variables on each node in the cluster
running the router pod to allow more entries in the ARP cache.

When the problem is occuring, the kernel messages would be similar to the following:

----
[ 1738.811139] net_ratelimit: 1045 callbacks suppressed
[ 1743.823136] net_ratelimit: 293 callbacks suppressed
----

When this issue occurs, the `oc` commands might start to fail with the following
error:

----
Unable to connect to the server: dial tcp: lookup <hostname> on <ip>:<port>: write udp <ip>:<port>-><ip>:<port>: write: invalid argument
----

To verify the actual amount of ARP entries for IPv4, run the following:

----
# ip -4 neigh show nud all | wc -l
----

If the number begins to approach the `net.ipv4.neigh.default.gc_thresh3`
threshold, increase the values. Get the current value by running:

----
# sysctl net.ipv4.neigh.default.gc_thresh1
net.ipv4.neigh.default.gc_thresh1 = 128
# sysctl net.ipv4.neigh.default.gc_thresh2
net.ipv4.neigh.default.gc_thresh2 = 512
# sysctl net.ipv4.neigh.default.gc_thresh3
net.ipv4.neigh.default.gc_thresh3 = 1024
----

The following sysctl sets the variables to the {product-title} current default values.

----
# sysctl net.ipv4.neigh.default.gc_thresh1=8192
# sysctl net.ipv4.neigh.default.gc_thresh2=32768
# sysctl net.ipv4.neigh.default.gc_thresh3=65536
----

To make these settings permanent,
link:https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html-single/Performance_Tuning_Guide/index.html#custom-profiles[see this document].

[[deploy-router-protecting-against-ddos-attacks]]
== Protecting Against DDoS Attacks

Add *timeout http-request* to the default HAProxy router image to
protect the deployment against distributed denial-of-service (DDoS) attacks (for
example, slowloris):

----
# and the haproxy stats socket is available at /var/run/haproxy.stats
global
  stats socket ./haproxy.stats level admin

defaults
  option http-server-close
  mode http
  timeout http-request 5s
  timeout connect 5s <1>
  timeout server 10s
  timeout client 30s
----
<1> *timeout http-request* is set up to 5 seconds. HAProxy gives a client 5 seconds
 *to send its whole HTTP request. Otherwise, HAProxy shuts the connection with
 *an error.

Also, when the environment variable `*ROUTER_SLOWLORIS_TIMEOUT*` is set, it
limits the amount of time a client has to send the whole HTTP request.
Otherwise, HAProxy will shut down the connection.

Setting the environment variable allows information to be captured as part
of the router's deployment configuration and does not require manual
modification of the template, whereas manually adding the HAProxy setting
requires you to rebuild the router pod and maintain your router template file.

Using annotations implements basic DDoS protections in the HAProxy template
router, including the ability to limit the:

* number of concurrent TCP connections
* rate at which a client can request TCP connections
* rate at which HTTP requests can be made

These are enabled on a per route basis because applications can have extremely
different traffic patterns.

.HAProxy Template Router Settings
[cols="2",options="header"]
|===

|Setting |Description

|`*haproxy.router.openshift.io/rate-limit-connections*`
|Enables the settings be configured (when set to *true*, for example).

|`*haproxy.router.openshift.io/rate-limit-connections.concurrent-tcp*`
|The number of concurrent TCP connections that can be made by the same IP
address on this route.

|`*haproxy.router.openshift.io/rate-limit-connections.rate-tcp*`
|The number of TCP connections that can be opened by a client IP.

|`*haproxy.router.openshift.io/rate-limit-connections.rate-http*`
|The number of HTTP requests that a client IP can make in a 3-second
period.
|===
