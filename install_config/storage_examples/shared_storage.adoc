= Sharing an NFS Persistent Volume (PV) Across Two Pods
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
The following use case describes how a user wanting to leverage shared storage for use by two seperate containers would configure the solution. This example highlights the use of NFS, but can easily be adapted to other shared storage types, such as GlusterFS. In addition, this example will show configuration of pod security as it relates to shared storage.

For an explanation of persistent volumes (PVs), persistent volume claims (PVCs), and
using NFS as persistent storage, please see the section on
link:../persistent_storage/persistent_storage_nfs[Persistent Storage Using NFS].
This topic shows an end-to-end example of using an existing NFS cluster as an OpenShift persistent store.
It is assumed that a working NFS server and exports have already been setup. 


[NOTE]
====
All `oc ...` commands are executed on the OpenShift master node.
====

== Creating the Persistent Volume
The persistent volume (PV) file is defined next before creating the PV object in
OpenShift:

.Persistent Volume Object Definition Using NFS
====

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv <1>
spec:
  capacity:
    storage: 1Gi <2>
  accessModes:
    - ReadWriteMany <3>
  persistentVolumeReclaimPolicy: Retain <4>
  nfs: <5>
    path: /opt/nfs <6>
    server: nfs.f22 <7> 
    readOnly: false
----
<1> The name of the PV which is referenced in pod definitions or displayed in
various `oc` volume commands.
<2> The amount of storage allocated to this volume.
<3> `accessModes` are used as labels to match a PV and a PVC. They currently
do not define any form of access control. 
<4> A volume reclaim policy of "retain" indicates to preserve the volume after the pods 
<5> This defines the volume type being used, in this case the *nfs* plug-in.
<6> This is the NFS mount path
<7> This is the NFS server. This can also be specified by IP address.
====

Save the PV definition to a file, for example *_nfs-pv.yaml_*,
and create the persistent volume:

====
----
# oc create -f nfs-pv.yaml 
persistentvolume "nfs-pv" created
----
====

Verify that the persistent volume was created:

====
----
# oc get pv
NAME         LABELS    CAPACITY   ACCESSMODES   STATUS      CLAIM     REASON    AGE
nfs-pv       <none>    1Gi        RWX           Available                       37s
----
====

== Creating the Persistent Volume Claim
A persistent volume claim (PVC) specifies the desired access mode and storage capacity.
Currently, based on only these two attributes, a PVC is bound to a single PV. Once a PV is
bound to a PVC that PV is essentially tied to the PVC's project and cannot be bound to by
another PVC. There is a one-to-one mapping of PVs and PVCs. However, multiple pods in the same
project can use the same PVC. This is the use case we are highlighting in this example.

.PVC Object Definition
====
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc  <1>
spec:
  accessModes:
  - ReadWriteMany      <2>
  resources:
     requests:
       storage: 1Gi    <3>
----
<1> The claim name is referenced by the pod under its `*volumes*` section.
<2> As mentioned above for PVs, the `*accessModes*` do not enforce access right,
but rather act as labels to match a PV to a PVC.
<3> This claim will look for PVs offering *1Gi* or greater capacity.
====

Save the PVC definition to a file, for example *_nfs-pvc.yaml_*,
and create the PVC:

====
----
# oc create -f nfs-pvc.yaml 
persistentvolumeclaim "nfs-pvc" created

#and verify the PVC was created and bound to the expected PV:
# oc get pvc
NAME            LABELS    STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE
nfs-pvc         <none>    Bound     nfs-pv       1Gi        RWX           24s
                                    <1>
----
<1> the claim (nfs-pvc) was bound to the "nfs-pv" PV.
====

== Ensuring NFS Volume Access
Access is necessary to a node in the NFS server. On this node examine
the nfs export mount:

====
----
#on a nfs storage node:
[root@nfs nfs]# ls -lZ /opt/nfs/
total 8
-rw-r--r--. 1 root 100003  system_u:object_r:usr_t:s0     10 Oct 12 23:27 test2b
              <1>   
                     <2>
----
<1> the owner has id 0.
<2> the group has id 100003.
====

In order to access the NFS mount, the container must match the SELinux label, and
either run with a UID of 0, or with 100003 in its supplemental groups range. It is recommended to gain
access to the volume by matching the NFS mount's groups, which will be defined in the pod
definition below.

By default, SELinux does not allow writing from a pod to a remote NFS server. To enable
writing to NFS volumes with SELinux enforcing on each node, run:

----
# setsebool -P virt_sandbox_use_nfs on
# setsebool -P virt_use_nfs on
----

[NOTE]
====
The `virt_sandbox_use_nfs` boolean is defined by the *docker-selinux* package.
If you get an error saying it is not defined, please ensure that this package is installed.
====

== Creating the Pod
A pod definition file or a template file can be used to define a pod. Below is a pod spec that
creates a single container and mounts the nfs volume for read-write access:

.Pod Object Definition
====
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nfs-pod <1>
  labels:
    name: nginx-nfs-pod
spec:
  containers:
    - name: nginx-nfs-pod
      image: fedora/nginx <2>
      ports:
        - name: web
          containerPort: 80
      volumeMounts:
        - name: nfsvol <3>
          mountPath: /usr/share/nginx/html <4>
  securityContext:
      supplementalGroups: [100003] <5>
      privileged: false
  volumes:
    - name: nfsvol
      persistentVolumeClaim:
        claimName: nfs-pvc 
----
<1> The name of this pod as displayed by `oc get pod`.
<2> The image run by this pod.
<3> The name of the volume. This name must be the same in both the `containers` and `volumes` sections.
<4> The mount path as seen in the container.
<5> The group id to be assigned to the container.
<6> The PVC that was created in the previous step.
====

Save the pod definition to a file, for example *_nfs.yaml_*,
 and create the pod:

====
----
# oc create -f nfs.yaml 
pod "nginx-nfs-pod" created

#verify pod was created
# oc get pods
NAME                READY     STATUS    RESTARTS   AGE
nginx-nfs-pod       1/1       Running   0          4s
----
====

More details are shown in the `oc describe pod` command:

====
----
[root@ose70 nfs]# oc describe pod nginx-nfs-pod
Name:				nginx-nfs-pod
Namespace:			default <1>
Image(s):			fedora/nginx
Node:				ose70.rh7/192.168.234.148 <2>
Start Time:			Mon, 21 Mar 2016 09:59:47 -0400
Labels:				name=nginx-nfs-pod
Status:				Running
Reason:				
Message:			
IP:				10.1.0.4
Replication Controllers:	<none>
Containers:
  nginx-nfs-pod:
    Container ID:	docker://a3292104d6c28d9cf49f440b2967a0fc5583540fc3b062db598557b93893bc6f
    Image:		fedora/nginx
    Image ID:		docker://403d268c640894cbd76d84a1de3995d2549a93af51c8e16e89842e4c3ed6a00a
    QoS Tier:
      cpu:		BestEffort
      memory:		BestEffort
    State:		Running
      Started:		Mon, 21 Mar 2016 09:59:49 -0400
    Ready:		True
    Restart Count:	0
    Environment Variables:
Conditions:
  Type		Status
  Ready 	True 
Volumes:
  nfsvol:
    Type:	PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:	nfs-pvc <3>
    ReadOnly:	false
  default-token-a06zb:
    Type:	Secret (a secret that should populate this volume)
    SecretName:	default-token-a06zb
Events: <4>
  FirstSeen	LastSeen	Count	From			SubobjectPath				Reason		Message
  ─────────	────────	─────	────			─────────────				──────		───────
  4m		4m		1	{scheduler }							Scheduled	Successfully assigned nginx-nfs-pod to ose70.rh7
  4m		4m		1	{kubelet ose70.rh7}	implicitly required container POD	Pulled		Container image "openshift3/ose-pod:v3.1.0.4" already present on machine
  4m		4m		1	{kubelet ose70.rh7}	implicitly required container POD	Created		Created with docker id 866a37108041
  4m		4m		1	{kubelet ose70.rh7}	implicitly required container POD	Started		Started with docker id 866a37108041
  4m		4m		1	{kubelet ose70.rh7}	spec.containers{nginx-nfs-pod}		Pulled		Container image "fedora/nginx" already present on machine
  4m		4m		1	{kubelet ose70.rh7}	spec.containers{nginx-nfs-pod}		Created		Created with docker id a3292104d6c2
  4m		4m		1	{kubelet ose70.rh7}	spec.containers{nginx-nfs-pod}		Started		Started with docker id a3292104d6c2


----
<1> The project (namespace) name.
<2> The IP address of the OpenShift node running the pod.
<3> The PVC name used by the pod.
<4> The list of events resulting in the pod being launched and the nfs volume being
mounted. The container will not start correctly if the volume cannot mount.
====

There is more internal information, including the SCC used to authorize the pod, the pod's
user and group ids, the selinux label, etc. shown in the
`oc get pod <name> -o yaml` command:

====
----
[root@ose70 nfs]# oc get pod nginx-nfs-pod -o yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    openshift.io/scc: restricted <1>
  creationTimestamp: 2016-03-21T13:59:47Z
  labels:
    name: nginx-nfs-pod
  name: nginx-nfs-pod
  namespace: default <2>
  resourceVersion: "2814411"
  selfLink: /api/v1/namespaces/default/pods/nginx-nfs-pod
  uid: 2c22d2ea-ef6d-11e5-adc7-000c2900f1e3
spec:
  containers:
  - image: fedora/nginx
    imagePullPolicy: IfNotPresent
    name: nginx-nfs-pod
    ports:
    - containerPort: 80
      name: web
      protocol: TCP
    resources: {}
    securityContext:
      privileged: false
    terminationMessagePath: /dev/termination-log
    volumeMounts:
    - mountPath: /usr/share/nginx/html
      name: nfsvol
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-a06zb
      readOnly: true
  dnsPolicy: ClusterFirst
  host: ose70.rh7
  imagePullSecrets:
  - name: default-dockercfg-xvdew
  nodeName: ose70.rh7
  restartPolicy: Always
  securityContext:
    supplementalGroups:
    - 100003i <3>
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  volumes:
  - name: nfsvol
    persistentVolumeClaim:
      claimName: nfs-pvc <4>
  - name: default-token-a06zb
    secret:
      secretName: default-token-a06zb
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: 2016-03-21T13:59:49Z
    status: "True"
    type: Ready
  containerStatuses:
  - containerID: docker://a3292104d6c28d9cf49f440b2967a0fc5583540fc3b062db598557b93893bc6f
    image: fedora/nginx
    imageID: docker://403d268c640894cbd76d84a1de3995d2549a93af51c8e16e89842e4c3ed6a00a
    lastState: {}
    name: nginx-nfs-pod
    ready: true
    restartCount: 0
    state:
      running:
        startedAt: 2016-03-21T13:59:49Z
  hostIP: 192.168.234.148
  phase: Running
  podIP: 10.1.0.4
  startTime: 2016-03-21T13:59:47Z

----
<1> The SCC used by the pod.
<2> The project (namespace) name.
<3> The supplemental group ID for the pod (all containers).
<4> The PVC name used by the pod.
====

== Creating an additional pod to reference the same PVC
This pod defintion, created in the same namespace, uses a different container. However, we can use the 
same backing storage by specifying the claim name in the volumes section below:

.Pod Object Definition
====
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: busybox-nfs-pod <1>
  labels:
    name: busybox-nfs-pod   
spec:
  containers:
  - name: busybox-nfs-pod
    image: busybox <2>       
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfsvol-2 <3>
      mountPath: /usr/share/busybox  <4>
      readOnly: false
  securityContext:
    supplementalGroups: [100003] <5>       
    privileged: false
  volumes:
  - name: nfsvol-2   
    persistentVolumeClaim:
      claimName: nfs-pvc <6>     

----
<1> The name of this pod as displayed by `oc get pod`.
<2> The image run by this pod.
<3> The name of the volume. This name must be the same in both the `containers` and `volumes` sections.
<4> The mount path as seen in the container.
<5> The group id to be assigned to the container.
<6> The PVC that was created earlier and is ALSO being used by a different container
====

Save the pod definition to a file, for example *_nfs-2.yaml_*,
 and create the pod:

====
----
# oc create -f nfs-2.yaml
pod "busybox-nfs-pod" created

#verify pod was created
# oc get pods
NAME                READY     STATUS    RESTARTS   AGE
busybox-nfs-pod     1/1       Running   0          3s
----
====

More details are shown in the `oc describe pod` command:

====
----
[root@ose70 nfs]# oc describe pod busybox-nfs-pod
Name:				busybox-nfs-pod
Namespace:			default
Image(s):			busybox
Node:				ose70.rh7/192.168.234.148
Start Time:			Mon, 21 Mar 2016 10:19:46 -0400
Labels:				name=busybox-nfs-pod
Status:				Running
Reason:				
Message:			
IP:				10.1.0.5
Replication Controllers:	<none>
Containers:
  busybox-nfs-pod:
    Container ID:	docker://346d432e5a4824ebf5a47fceb4247e0568ecc64eadcc160e9bab481aecfb0594
    Image:		busybox
    Image ID:		docker://17583c7dd0dae6244203b8029733bdb7d17fccbb2b5d93e2b24cf48b8bfd06e2
    QoS Tier:
      cpu:		BestEffort
      memory:		BestEffort
    State:		Running
      Started:		Mon, 21 Mar 2016 10:19:48 -0400
    Ready:		True
    Restart Count:	0
    Environment Variables:
Conditions:
  Type		Status
  Ready 	True 
Volumes:
  nfsvol-2:
    Type:	PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:	nfs-pvc
    ReadOnly:	false
  default-token-32d2z:
    Type:	Secret (a secret that should populate this volume)
    SecretName:	default-token-32d2z
Events:
  FirstSeen	LastSeen	Count	From			SubobjectPath				Reason		Message
  ─────────	────────	─────	────			─────────────				──────		───────
  4m		4m		1	{scheduler }							Scheduled	Successfully assigned busybox-nfs-pod to ose70.rh7
  4m		4m		1	{kubelet ose70.rh7}	implicitly required container POD	Pulled		Container image "openshift3/ose-pod:v3.1.0.4" already present on machine
  4m		4m		1	{kubelet ose70.rh7}	implicitly required container POD	Created		Created with docker id 249b7d7519b1
  4m		4m		1	{kubelet ose70.rh7}	implicitly required container POD	Started		Started with docker id 249b7d7519b1
  4m		4m		1	{kubelet ose70.rh7}	spec.containers{busybox-nfs-pod}	Pulled		Container image "busybox" already present on machine
  4m		4m		1	{kubelet ose70.rh7}	spec.containers{busybox-nfs-pod}	Created		Created with docker id 346d432e5a48
  4m		4m		1	{kubelet ose70.rh7}	spec.containers{busybox-nfs-pod}	Started		Started with docker id 346d432e5a48
----
As you can see both containers are using the same storage claim that is attached to the same NFS mount on the backend.


