[[install-config-preqeqs-enviornment-reqs]]
= Environment Requirements
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]
[[envirornment-requirements]]

== Overview

The following must be set up in your environment before {product-title} can be
installed.

[[prereq-dns]]
== DNS

A fully functional DNS environment is a requirement for {product-title} to work
correctly. Adding entries into the *_/etc/hosts_* file is not enough, because
that file is not copied into containers running on the platform.

To configure the {product-title} DNS environment:

- xref:../../install_config/install/prerequisites.adoc#dns-config-prereq[Complete DNS configuration]
- (Optionally) xref:../../install_config/install/prerequisites.adoc#wildcard-dns-prereq[Configure a wildcard for the router]

Key components of {product-title} run themselves inside of containers. By
default, these containers receive their *_/etc/resolv.conf_* DNS configuration
file from their host. {product-title} then inserts one DNS value into the pods
(above the node's nameserver values). That value is defined in the
*_/etc/origin/node/node-config.yaml_* file by the `*dnsIP*` parameter, which by
default is set to the address of the host node because the host is using
*dnsmasq*. If the `*dnsIP*` parameter is omitted from the *_node-config.yaml_*
file, then the value defaults to the kubernetes service IP, which is the first
nameserver in the pod's *_/etc/resolv.conf_* file.

As of {product-title}
ifdef::openshift-enterprise[]
3.2,
endif::[]
ifdef::openshift-origin[]
1.2,
endif::[]
*dnsmasq* is automatically configured on all masters and nodes. The pods use the
nodes as their DNS, and the nodes forward the requests. By default, *dnsmasq*
is configured on the nodes to listen on port 53, therefore the nodes cannot run
any other type of DNS application.

[NOTE]
====
Previously, in {product-title}
ifdef::openshift-enterprise[]
3.1,
endif::[]
ifdef::openshift-origin[]
1.1,
endif::[]
a DNS server could not be installed on a master node, because it ran its own
internal DNS server. Now, with master nodes using *dnsmasq*, SkyDNS is now
configured to listen on port 8053 so that *dnsmasq* can run on the masters. Note
that these DNS changes (*dnsmasq* configured on nodes and the SkyDNS port
change) only apply to new installations of {product-title} 3.2. Clusters
upgraded to {product-title}
ifdef::openshift-enterprise[]
3.2 and later
endif::[]
ifdef::openshift-origin[]
1.2 and later
endif::[]
from a previous version do not currently have these changes applied during the
upgrade process.
====

[NOTE]
====
*NetworkManager* is required on the nodes in order to populate *dnsmasq* with
the DNS IP addresses.
====

If you do not have a properly functioning DNS environment, you could experience failure with:

- Product installation via the reference Ansible-based scripts
- Deployment of the infrastructure containers (registry, routers)
- Access to the {product-title} web console, because it is not accessible via IP address alone


[[dns-config-prereq]]
=== Configuring a DNS Environment

To properly configure your DNS environment for {product-title}:

. Check the contents of *_/etc/resolv.conf_*:
+
----
$ cat /etc/resolv.conf
# Generated by NetworkManager
search ose3.example.com
nameserver 10.64.33.1
# nameserver updated by /etc/NetworkManager/dispatcher.d/99-origin-dns.sh
----
. Ensure that the DNS servers listed in *_/etc/resolv.conf_* are able to resolve to the addresses of all the masters and nodes in your {product-title} environment:
+
----
$ dig <node_hostname> @<IP_address> +short
----
+
For example:
+
----
$ dig node1.ose3.example.com @10.64.33.1 +short
10.64.33.156
$ dig master.ose3.example.com @10.64.33.1 +short
10.64.33.37
----
. If DHCP is:
+
- Disabled, then configure your network interface to be static, and add DNS nameservers to NetworkManager.
- Enabled, then the NetworkManager dispatch script automatically configures DNS based on the DHCP configuration. Optionally, you can add a value to `*dnsIP*` in the *_node-config.yaml_* file to prepend the pod's *_resolv.conf_* file. The second nameserver is then defined by the host's first nameserver. By default, this will be the IP address of the node host.
+
[NOTE]
====
For most configurations, do not set the `*openshift_dns_ip*` option during the
advanced installation of {product-title} (using Ansible), because this option
overrides the default IP address set by `*dnsIP*`.

Instead, allow the installer to configure each node to use *dnsmasq* and forward
requests to SkyDNS or the external DNS provider. If you do set the
`*openshift_dns_ip*` option, then it should be set either with a DNS IP that
queries SkyDNS first, or to the SkyDNS service or endpoint IP (the Kubernetes
service IP).
====


[[wildcard-dns-prereq]]
=== Configuring Wildcard DNS

Optionally, configure a wildcard for the router to use, so that you do not need
to update your DNS configuration when new routes are added.

A wildcard for a DNS zone must ultimately resolve to the IP address of the
{product-title} xref:../../architecture/core_concepts/routes.adoc#routers[router].

For example, create a wildcard DNS entry for *cloudapps* that has a low
time-to-live value (TTL) and points to the public IP address of the host where
the router will be deployed:

----
*.cloudapps.example.com. 300 IN  A 192.168.133.2
----

In almost all cases, when referencing VMs you must use host names, and the host
names that you use must match the output of the `hostname -f` command on each
node.

[WARNING]
====
In your *_/etc/resolv.conf_* file on each node host, ensure that the DNS server
that has the wildcard entry is not listed as a nameserver or that the wildcard
domain is not listed in the search list. Otherwise, containers managed by
{product-title} may fail to resolve host names properly.
====


[[run-dns-diagnostics]]
=== Running Diagnostics

To explore your DNS setup and run specific DNS queries, you can use the `host` and `dig` commands (part of the `bind-utils` package). For example, you can query a specific DNS server, or check if recursion is involved.

----
$ host `hostname`
ose3-master.example.com has address 172.16.25.41

$ dig ose3-node1.example.com  +short
172.16.25.45
----


[[prereq-network-access]]
== Network Access

A shared network must exist between the master and node hosts. If you plan to
configure
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#high-availability-masters[multiple
masters for high-availability] using the xref:advanced_install.adoc#install-config-install-advanced-install[advanced
installation method], you must also select an IP to be configured as your
xref:../../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master-components[virtual
IP] (VIP) during the installation process. The IP that you select must be
routable between all of your nodes, and if you configure using a FQDN it should
resolve on all nodes.

[[prereq-networkmanager]]
=== NetworkManager

NetworkManager, a program for providing detection and configuration for systems
to automatically connect to the network, is required.

[[required-ports]]
=== Required Ports

{product-title} infrastructure components communicate with each other using
ports, which are communication endpoints that are identifiable for specific
processes or services. Ensure the following ports required by {product-title}
are open between hosts, for example if you have a firewall in your environment.
Some ports are optional depending on your configuration and usage.

.Node to Node
[cols='2,1,8']
|===
| *4789*
|UDP
|Required for SDN communication between pods on separate hosts.
|===

.Nodes to Master
[cols='2,1,8']
|===
| *53* or *8053*
|TCP/UDP
|Required for DNS resolution of cluster services (SkyDNS).
ifdef::openshift-origin[]
Installations prior to 1.2 or environments upgraded to 1.2 use port 53.
endif::[]
ifdef::openshift-enterprise[]
Installations prior to 3.2 or environments upgraded to 3.2 use port 53.
endif::[]
New installations will use 8053 by default so that *dnsmasq* may be configured.

| *4789*
|UDP
|Required for SDN communication between pods on separate hosts.

| *443* or *8443*
|TCP
|Required for node hosts to communicate to the master API, for the node hosts to
post back status, to receive tasks, and so on.
|===

.Master to Node
[cols='2,1,8']
|===
| *4789*
|UDP
|Required for SDN communication between pods on separate hosts.

| *10250*
|TCP
|The master proxies to node hosts via the Kubelet for `oc` commands.
|===

[NOTE]
====
In the following table,
*(L)* indicates the marked port is also used in _loopback mode_,
enabling the master to communicate with itself.

In a single-master cluster:

- Ports marked with *(L)* must be open.
- Ports not marked with *(L)* need not be open.

In a multiple-master cluster, all the listed ports must be open.
====

.Master to Master
[cols='2,1,8']
|===
| *53 (L)* or *8053* (L)
|TCP/UDP
|Required for DNS resolution of cluster services (SkyDNS).
ifdef::openshift-origin[]
Installations prior to 1.2 or environments upgraded to 1.2 use port 53.
endif::[]
ifdef::openshift-enterprise[]
Installations prior to 3.2 or environments upgraded to 3.2 use port 53.
endif::[]
New installations will use 8053 by default so that *dnsmasq* may be configured.

| *2049* (L)
|TCP/UDP
|Required when provisioning an NFS host as part of the installer.

| *2379*
|TCP
|Used for standalone etcd (clustered) to accept changes in state.

| *2380*
|TCP
|etcd requires this port be open between masters for leader election and peering
connections when using standalone etcd (clustered).

| *4001 (L)*
|TCP
|Used for embedded etcd (non-clustered) to accept changes in state.

| *4789 (L)*
|UDP
|Required for SDN communication between pods on separate hosts.

|===

.External to Load Balancer
[cols='2,1,8']
|===
| *9000*
|TCP
|If you choose the `*native*` HA method, optional to allow access to the HAProxy statistics page.

|===


.External to Master
[cols='2,1,8']
|===
| *443* or *8443*
|TCP
|Required for node hosts to communicate to the master API, for node hosts to
post back status, to receive tasks, and so on.
|===

.IaaS Deployments
[cols='2,1,8']
|===
| *22*
|TCP
| Required for SSH by the installer or system administrator.

| *53* or *8053*
|TCP/UDP
|Required for DNS resolution of cluster services (SkyDNS).
ifdef::openshift-origin[]
Installations prior to 1.2 or environments upgraded to 1.2 use port 53.
endif::[]
ifdef::openshift-enterprise[]
Installations prior to 3.2 or environments upgraded to 3.2 use port 53.
endif::[]
New installations will use 8053 by default so that *dnsmasq* may be configured.
Only required to be internally open on master hosts.

| *80* or *443*
|TCP
| For HTTP/HTTPS use for the router. Required to be externally open on node hosts, especially on nodes running the router.

| *1936*
|TCP
| For router statistics use. Required to be open when running the template
router to access statistics, and can be open externally or internally to
connections depending on if you want the statistics to be expressed publicly.

| *4001*
|TCP
| For embedded etcd (non-clustered) use. Only required to be internally open on
the master host. *4001* is for server-client connections.

| *2379* and *2380*
|TCP
| For standalone etcd use. Only required to be internally open on the master host.
*2379* is for server-client connections. *2380* is for server-server
connections, and is only required if you have clustered etcd.

| *4789*
|UDP
| For VxLAN use ({product-title} SDN). Required only internally on node hosts.

| *8443*
|TCP
| For use by the {product-title} web console, shared with the API server.

| *10250*
|TCP
| For use by the Kubelet. Required to be externally open on nodes.
|===

*Notes*

* In the above examples, port *4789* is used for User Datagram Protocol (UDP).
* When deployments are using the SDN, the pod network is accessed via a service proxy, unless it is accessing the registry from the same node the registry is deployed on.
* {product-title} internal DNS cannot be received over SDN. Depending on the detected values of `*openshift_facts*`, or if the `*openshift_ip*` and `*openshift_public_ip*` values are overridden, it will be the computed value of `*openshift_ip*`. For non-cloud deployments, this will default to the IP address associated with the default route on the master host. For cloud deployments, it will default to the IP address associated with the first internal interface as defined by the cloud metadata.
* The master host uses port *10250* to reach the nodes and does not go over SDN. It depends on the target host of the deployment and uses the computed values of `*openshift_hostname*` and `*openshift_public_hostname*`.

[[prereq-git]]

== Git Access

You must have either Internet access and a GitHub account, or read and write
access to an internal, HTTP-based Git server.

[[prereq-persistent-storage]]

== Persistent Storage

The Kubernetes
xref:../../architecture/additional_concepts/storage.adoc#architecture-additional-concepts-storage[persistent volume]
framework allows you to provision an {product-title} cluster with persistent storage
using networked storage available in your environment. This can be done after
completing the initial {product-title} installation depending on your application
needs, giving users a way to request those resources without having any
knowledge of the underlying infrastructure.

The xref:../../install_config/index.adoc#install-config-index[Installation and Configuration Guide]
provides instructions for cluster administrators on provisioning an {product-title}
cluster with persistent storage using
xref:../../install_config/persistent_storage/persistent_storage_nfs.adoc#install-config-persistent-storage-persistent-storage-nfs[NFS],
xref:../../install_config/persistent_storage/persistent_storage_glusterfs.adoc#install-config-persistent-storage-persistent-storage-glusterfs[GlusterFS],
xref:../../install_config/persistent_storage/persistent_storage_ceph_rbd.adoc#install-config-persistent-storage-persistent-storage-ceph-rbd[Ceph
RBD],
xref:../../install_config/persistent_storage/persistent_storage_cinder.adoc#install-config-persistent-storage-persistent-storage-cinder[OpenStack
Cinder],
xref:../../install_config/persistent_storage/persistent_storage_aws.adoc#install-config-persistent-storage-persistent-storage-aws[AWS Elastic Block Store (EBS)],
xref:../../install_config/persistent_storage/persistent_storage_gce.adoc#install-config-persistent-storage-persistent-storage-gce[GCE
Persistent Disks], and
xref:../../install_config/persistent_storage/persistent_storage_iscsi.adoc#install-config-persistent-storage-persistent-storage-iscsi[iSCSI].

[[prereq-cloud-provider-considerations]]

== Cloud Provider Considerations

=== Set up the Security Group

When installing on AWS or OpenStack, ensure that you set up the appropriate
security groups. These are some ports that you should have in your security
groups, without which the installation will fail. You may need more depending on
the cluster configuration you want to install. For more information and to
adjust your security groups accordingly, see xref:required-ports[Required Ports]
for more information.


[cols="1,2"]
|===
|*All {product-title} Hosts*
a|- tcp/22 from host running the installer/Ansible

|*etcd Security Group*
a|- tcp/2379 from masters
- tcp/2380 from etcd hosts

|*Master Security Group*
a|- tcp/8443 from 0.0.0.0/0
ifdef::openshift-origin[]
- tcp/53 from all {product-title} hosts for environments installed prior to or upgraded to 1.2
- udp/53 from all {product-title} hosts for environments installed prior to or upgraded to 1.2
- tcp/8053 from all {product-title} hosts for new environments installed with 1.2
- udp/8053 from all {product-title} hosts for new environments installed with 1.2
endif::[]
ifdef::openshift-enterprise[]
- tcp/53 from all {product-title} hosts for environments installed prior to or upgraded to 3.2
- udp/53 from all {product-title} hosts for environments installed prior to or upgraded to 3.2
- tcp/8053 from all {product-title} hosts for new environments installed with 3.2
- udp/8053 from all {product-title} hosts for new environments installed with 3.2
endif::[]

|*Node Security Group*
a|- tcp/10250 from masters
- tcp/4789 from nodes

|*Infrastructure Nodes*
(ones that can host the {product-title} router)
a|- tcp/443 from 0.0.0.0/0
- tcp/80 from 0.0.0.0/0

|===

If configuring ELBs for load balancing the masters and/or routers, you also need
to configure Ingress and Egress security groups for the ELBs appropriately.

=== Override Detected IP Addresses and Host Names

Some deployments require that the user override the detected host names and IP
addresses for the hosts. To see the default values, run the `*openshift_facts*`
playbook:

====
----
# ansible-playbook playbooks/byo/openshift_facts.yml
----
====

Now, verify the detected common settings. If they are not what you expect them
to be, you can override them.

The
xref:../../install_config/install/advanced_install.adoc#configuring-ansible[Advanced
Installation] topic discusses the available Ansible variables in greater detail.

[cols="1,2",options="header"]
|===
|Variable |Usage

|`*hostname*`
a| - Should resolve to the internal IP from the instances themselves.
- `*openshift_hostname*` overrides.

|`*ip*`
a| - Should be the internal IP of the instance.
- `*openshift_ip*` will overrides.

|`*public_hostname*`
a| - Should resolve to the external IP from hosts outside of the cloud.
- Provider `*openshift_public_hostname*` overrides.

|`*public_ip*`
a| - Should be the externally accessible IP associated with the instance.
- `*openshift_public_ip*` overrides.

|`*use_openshift_sdn*`
a| - Should be true unless the cloud is GCE.
- `*openshift_use_openshift_sdn*` overrides.

|===

[WARNING]
====
If `*openshift_hostname*` is set to a value other than the metadata-provided
`*private-dns-name*` value, the native cloud integration for those providers
will no longer work.
====

In AWS, situations that require overriding the variables include:

[cols="1,2"options="header"]
|===
|Variable |Usage

|`*hostname*`
a|The user is installing in a VPC that is not configured for both `*DNS hostnames*` and `*DNS resolution*`.

|`*ip*`
a|Possibly if they have multiple network interfaces configured and they want to
use one other than the default. You must first set
`*openshift_node_set_node_ip*` to `True`. Otherwise, the SDN would attempt to
use the `*hostname*` setting or try to resolve the host name for the IP.

|`*public_hostname*`
a| - A master instance where the VPC subnet is not configured for `*Auto-assign
Public IP*`. For external access to this master, you need to have an ELB or
other load balancer configured that would provide the external access needed, or
you need to connect over a VPN connection to the internal name of the host.
- A master instance where metadata is disabled.
- This value is not actually used by the nodes.

|`*public_ip*`
a| - A master instance where the VPC subnet is not configured for `*Auto-assign Public IP*`.
- A master instance where metadata is disabled.
- This value is not actually used by the nodes.

|===

If setting `*openshift_hostname*` to something other than the metadata-provided
`*private-dns-name*` value, the native cloud integration for those providers
will no longer work.

For EC2 hosts in particular, they must be deployed in a VPC that has both
`*DNS host names*` and `*DNS resolution*` enabled, and `*openshift_hostname*`
should not be overridden.

=== Post-Installation Configuration for Cloud Providers

Following the installation process, you can configure {product-title} for
xref:../../install_config/configuring_aws.adoc#install-config-configuring-aws[AWS],
xref:../../install_config/configuring_openstack.adoc#install-config-configuring-openstack[OpenStack], or
xref:../../install_config/configuring_gce.adoc#install-config-configuring-gce[GCE].
