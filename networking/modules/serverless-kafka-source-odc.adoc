// Module included in the following assemblies:
//
// * serverless/develop/serverless-kafka-developer.adoc

:_content-type: PROCEDURE
[id="serverless-kafka-source-odc_{context}"]
= Creating a Kafka event source by using the web console

After Knative Kafka is installed on your cluster, you can create a Kafka source by using the web console. Using the {product-title} web console provides a streamlined and intuitive user interface to create a Kafka source.

.Prerequisites

* The {ServerlessOperatorName}, Knative Eventing, and the `KnativeKafka` custom resource are installed on your cluster.
* You have logged in to the web console.
* You have access to a Red Hat AMQ Streams (Kafka) cluster that produces the Kafka messages you want to import.
* You have created a project or have access to a project with the appropriate roles and permissions to create applications and other workloads in {product-title}.

.Procedure

. In the *Developer* perspective, navigate to the *+Add* page and select *Event Source*.
. In the *Event Sources* page, select *Kafka Source* in the *Type* section.
. Configure the *Kafka Source* settings:
.. Add a comma-separated list of *Bootstrap Servers*.
.. Add a comma-separated list of *Topics*.
.. Add a *Consumer Group*.
.. Select the *Service Account Name* for the service account that you created.
.. Select the *Sink* for the event source. A *Sink* can be either a *Resource*, such as a channel, broker, or service, or a *URI*.
.. Enter a *Name* for the Kafka event source.
. Click *Create*.

.Verification

You can verify that the Kafka event source was created and is connected to the sink by viewing the *Topology* page.

. In the *Developer* perspective, navigate to *Topology*.
. View the Kafka event source and sink.
+
image::verify-kafka-ODC.png[View the Kafka source and service in the Topology view]
