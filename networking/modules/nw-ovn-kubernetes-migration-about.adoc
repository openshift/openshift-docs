// Module included in the following assemblies:
//
// * networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc

[id="nw-ovn-kubernetes-migration-about_{context}"]
= Migration to the OVN-Kubernetes network plugin

Migrating to the OVN-Kubernetes network plugin is a manual process that includes some downtime during which your cluster is unreachable. Although a rollback procedure is provided, the migration is intended to be a one-way process.

A migration to the OVN-Kubernetes network plugin is supported on the following platforms:

* Bare metal hardware
* Amazon Web Services (AWS)
* Google Cloud Platform (GCP)
* IBM Cloud
* Microsoft Azure
* {rh-openstack-first}
* {rh-virtualization-first}
* VMware vSphere

[id="considerations-migrating-ovn-kubernetes-network-provider_{context}"]
== Considerations for migrating to the OVN-Kubernetes network plugin

If you have more than 150 nodes in your {product-title} cluster, then open a support case for consultation on your migration to the OVN-Kubernetes network plugin.

The subnets assigned to nodes and the IP addresses assigned to individual pods are not preserved during the migration.

While the OVN-Kubernetes network plugin implements many of the capabilities present in the OpenShift SDN network plugin, the configuration is not the same.

* If your cluster uses any of the following OpenShift SDN network plugin capabilities, you must manually configure the same capability in the OVN-Kubernetes network plugin:
+
--
* Namespace isolation
* Egress router pods
--

* If your cluster or surrounding network uses any part of the `100.64.0.0/16` address range, you must choose another unused IP range by specifying the `v4InternalSubnet` spec under the `spec.defaultNetwork.ovnKubernetesConfig` object definition. OVN-Kubernetes uses the IP range `100.64.0.0/16` internally by default.

The following sections highlight the differences in configuration between the aforementioned capabilities in OVN-Kubernetes and OpenShift SDN network plugins.

[discrete]
[id="namespace-isolation_{context}"]
=== Namespace isolation

OVN-Kubernetes supports only the network policy isolation mode.

[IMPORTANT]
====
If your cluster uses OpenShift SDN configured in either the multitenant or subnet isolation modes, you cannot migrate to the OVN-Kubernetes network plugin.
====

[discrete]
[id="egress-ip-addresses_{context}"]
=== Egress IP addresses

OpenShift SDN supports two different Egress IP modes:

* In the _automatically assigned_ approach, an egress IP address range is assigned to a node.
* In the _manually assigned_ approach, a list of one or more egress IP addresses is assigned to a node.

The migration process supports migrating Egress IP configurations that use the automatically assigned mode.

The differences in configuring an egress IP address between OVN-Kubernetes and OpenShift SDN is described in the following table:

.Differences in egress IP address configuration
[cols="1a,1a",options="header"]
|===
|OVN-Kubernetes|OpenShift SDN

|
* Create an `EgressIPs` object
* Add an annotation on a `Node` object

|
* Patch a `NetNamespace` object
* Patch a `HostSubnet` object
|===

For more information on using egress IP addresses in OVN-Kubernetes, see "Configuring an egress IP address".

[discrete]
[id="egress-network-policies_{context}"]
=== Egress network policies

The difference in configuring an egress network policy, also known as an egress firewall, between OVN-Kubernetes and OpenShift SDN is described in the following table:

.Differences in egress network policy configuration
[cols="1a,1a",options="header"]
|===
|OVN-Kubernetes|OpenShift SDN

|
* Create an `EgressFirewall` object in a namespace

|
* Create an `EgressNetworkPolicy` object in a namespace
|===

[NOTE]
====
Because the name of an `EgressFirewall` object can only be set to `default`, after the migration all migrated `EgressNetworkPolicy` objects are named `default`, regardless of what the name was under OpenShift SDN.

If you subsequently rollback to OpenShift SDN, all `EgressNetworkPolicy` objects are named `default` as the prior name is lost.

For more information on using an egress firewall in OVN-Kubernetes, see "Configuring an egress firewall for a project".
====

[discrete]
[id="egress-router-pods_{context}"]
=== Egress router pods

OVN-Kubernetes supports egress router pods in redirect mode. OVN-Kubernetes does not support egress router pods in HTTP proxy mode or DNS proxy mode.

When you deploy an egress router with the Cluster Network Operator, you cannot specify a node selector to control which node is used to host the egress router pod.

[discrete]
[id="multicast_{context}"]
=== Multicast

The difference between enabling multicast traffic on OVN-Kubernetes and OpenShift SDN is described in the following table:

.Differences in multicast configuration
[cols="1a,1a",options="header"]
|===
|OVN-Kubernetes|OpenShift SDN

|
* Add an annotation on a `Namespace` object

|
* Add an annotation on a `NetNamespace` object
|===

For more information on using multicast in OVN-Kubernetes, see "Enabling multicast for a project".

[discrete]
[id="network-policies_{context}"]
=== Network policies

OVN-Kubernetes fully supports the Kubernetes `NetworkPolicy` API in the `networking.k8s.io/v1` API group. No changes are necessary in your network policies when migrating from OpenShift SDN.

[id="how-the-migration-process-works_{context}"]
== How the migration process works

The following table summarizes the migration process by segmenting between the user-initiated steps in the process and the actions that the migration performs in response.

.Migrating to OVN-Kubernetes from OpenShift SDN
[cols="1,1a",options="header"]
|===

|User-initiated steps|Migration activity

|
Set the `migration` field of the `Network.operator.openshift.io` custom resource (CR) named `cluster` to `OVNKubernetes`. Make sure the `migration` field is `null` before setting it to a value.
|
Cluster Network Operator (CNO):: Updates the status of the `Network.config.openshift.io` CR named `cluster` accordingly.
Machine Config Operator (MCO):: Rolls out an update to the systemd configuration necessary for OVN-Kubernetes; The MCO updates a single machine per pool at a time by default, causing the total time the migration takes to increase with the size of the cluster.

|Update the `networkType` field of the `Network.config.openshift.io` CR.
|
CNO:: Performs the following actions:
+
--
* Destroys the OpenShift SDN control plane pods.
* Deploys the OVN-Kubernetes control plane pods.
* Updates the Multus objects to reflect the new network plugin.
--

|
Reboot each node in the cluster.
|
Cluster:: As nodes reboot, the cluster assigns IP addresses to pods on the OVN-Kubernetes cluster network.

|===

If a rollback to OpenShift SDN is required, the following table describes the process.

.Performing a rollback to OpenShift SDN
[cols="1,1a",options="header"]
|===

|User-initiated steps|Migration activity

|Suspend the MCO to ensure that it does not interrupt the migration.
|The MCO stops.

|
Set the `migration` field of the `Network.operator.openshift.io` custom resource (CR) named `cluster` to `OpenShiftSDN`. Make sure the `migration` field is `null` before setting it to a value.
|
CNO:: Updates the status of the `Network.config.openshift.io` CR named `cluster` accordingly.

|Update the `networkType` field.
|
CNO:: Performs the following actions:
+
--
* Destroys the OVN-Kubernetes control plane pods.
* Deploys the OpenShift SDN control plane pods.
* Updates the Multus objects to reflect the new network plugin.
--

|
Reboot each node in the cluster.
|
Cluster:: As nodes reboot, the cluster assigns IP addresses to pods on the OpenShift-SDN network.

|
Enable the MCO after all nodes in the cluster reboot.
|
MCO:: Rolls out an update to the systemd configuration necessary for OpenShift SDN; The MCO updates a single machine per pool at a time by default, so the total time the migration takes increases with the size of the cluster.

|===
