//Module included in the following assemblies:
//
// * sandboxed_containers/deploying_sandboxed_containers.adoc

:_content-type: CONCEPT
[id="sandboxed-containers-prerequisites_{context}"]
= Prerequisites

Before you install {sandboxed-containers-first}, ensure that your {product-title} cluster meets the following requirements:

* Your cluster must be installed on on-premise bare-metal infrastructure with {op-system-first} workers. You can use any installation method including user-provisioned, installer-provisioned, or assisted installer to deploy your cluster.
+
[NOTE]
====
* {sandboxed-containers-first} only supports {op-system} worker nodes. {op-system-base} nodes are not supported.
* Nested virtualization is not supported.
====

* You can install {sandboxed-containers-first} on Amazon Web Services (AWS) bare-metal instances. Bare-metal instances offered by other cloud providers are not supported.

[id="sandboxed-containers-resource-requirements_{context}"]
== Resource requirements for {sandboxed-containers-first}

{sandboxed-containers-first} lets users run workloads on their {product-title} clusters inside a sandboxed runtime (Kata). Each pod is represented by a virtual machine (VM). Each VM runs in a QEMU process and hosts a `kata-agent` process that acts as a supervisor for managing container workloads, and the processes running in those containers. Two additional processes add more overhead:

* `containerd-shim-kata-v2` is used to communicate with the pod.
* `virtiofsd` handles host file system access on behalf of the guest.

Each VM is configured with a default amount of memory. Additional memory is hot-plugged into the VM for containers that explicitly request memory.

A container running without a memory resource consumes free memory until the total memory used by the VM reaches the default allocation. The guest and its I/O buffers also consume memory.

If a container is given a specific amount of memory, then that memory is hot-plugged into the VM before the container starts.

When a memory limit is specified, the workload is terminated if it consumes more memory than the limit. If no memory limit is specified, the kernel running on the VM might run out of memory. If the kernel runs out of memory, it might terminate other processes on the VM.

.Default memory sizes

The following table lists some the default values for resource allocation.

[cols="2,2"]
|===
|Resource |Value

|Memory allocated by default to a virtual machine | 2Gi
|Guest Linux kernel memory usage at boot | ~110Mi
|Memory used by the QEMU process (excluding VM memory) | ~30Mi
|Memory used by the `virtiofsd` process (excluding VM I/O buffers) | ~10Mi
|Memory used by the `containerd-shim-kata-v2` process | ~20Mi
|File buffer cache data after running `dnf install` on Fedora | ~300Mi* ^[1]^
|===
[.small]
--

File buffers appear and are accounted for in multiple locations:

* In the guest where it appears as file buffer cache.
* In the `virtiofsd` daemon that maps allowed user-space file I/O operations.
* In the QEMU process as guest memory.

[NOTE]
====
Total memory usage is properly accounted for by the memory utilization metrics, which only count that memory once.
====
--

link:https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/[Pod overhead] describes the amount of system resources that a pod on a node uses. You can get the current pod overhead for the Kata runtime by using `oc describe runtimeclass kata` as shown below.

.Example
[source,terminal]
----
$ oc describe runtimeclass kata
----

.Example output
[source,terminal]
----
kind: RuntimeClass
apiVersion: node.k8s.io/v1
metadata:
  name: kata
overhead:
  podFixed:
    memory: "500Mi"
    cpu: "500m"
----

You can change the pod overhead by changing the `spec.overhead` field for a `RuntimeClass`. For example, if the configuration that you run for your containers consumes more than 350Mi of memory for the QEMU process and guest kernel data, you can alter the `RuntimeClass` overhead to suit your needs.

[NOTE]
====
The specified default overhead values are supported by Red Hat. Changing default overhead values is not supported and can result in technical issues.
====

When performing any kind of file system I/O in the guest, file buffers are allocated in the guest kernel. The file buffers are also mapped in the QEMU process on the host, as well as in the `virtiofsd` process.

For example, if you use 300Mi of file buffer cache in the guest, both QEMU and `virtiofsd` appear to use 300Mi additional memory. However, the same memory is being used in all three cases. In other words, the total memory usage is only 300Mi, mapped in three different places. This is correctly accounted for when reporting the memory utilization metrics.
