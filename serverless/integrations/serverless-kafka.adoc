include::modules/serverless-document-attributes.adoc[]
[id="serverless-kafka"]
= Knative Kafka
include::modules/common-attributes.adoc[]
:context: serverless-kafka

toc::[]

To use Kafka with Knative, you must install the Knative Kafka components and configure the integration between {ServerlessProductName} and a supported link:https://access.redhat.com/documentation/en-us/red_hat_amq/[Red Hat AMQ Streams] cluster.

[NOTE]
====
Knative Kafka is not currently supported for IBM Z and IBM Power Systems.
====

[id="serverless-kafka-delivery-retries"]
== Event delivery and retries

Using Kafka components in your event-driven architecture provides "at least once" guarantees for event delivery. This means that operations are retried until a return code value is received. However, while this makes your application more resilient to lost events, it might result in duplicate events being sent.

For the Kafka event source, there is a fixed number of retries for event delivery by default. For Kafka channels, retries are only performed if they are configured in the Kafka channel `Delivery` spec.

[id="install-serverless-kafka"]
== Installing Knative Kafka

The {ServerlessOperatorName} provides the Knative Kafka API that can be used to create a `KnativeKafka` custom resource:

.Example `KnativeKafka` custom resource
[source,yaml]
----
apiVersion: operator.serverless.openshift.io/v1alpha1
kind: KnativeKafka
metadata:
    name: knative-kafka
    namespace: knative-eventing
spec:
    channel:
        enabled: true <1>
        bootstrapServers: <bootstrap_servers> <2>
    source:
        enabled: true <3>
    broker:
        enabled: true <4>
        defaultConfig:
            bootstrapServers: <bootstrap_servers> <5>
            numPartitions: <num_partitions> <6>
            replicationFactor: <replication_factor> <7>
----
<1> Enables developers to use the `KafkaChannel` channel type in the cluster.
<2> A comma-separated list of bootstrap servers from your AMQ Streams cluster.
<3> Enables developers to use the `KafkaSource` event source type in the cluster.
<4> Enables developers to use the Knative Kafka broker implementation in the cluster.
<5> A comma-separated list of bootstrap servers from your Red Hat AMQ Streams cluster.
<6> Defines the number of partitions of the Kafka topics, backed by Kafka `Broker` objects. The default is `10`.
<7> Defines the replication factor of the Kafka topics, backed by Kafka `Broker` objects. The default is `3`.
+
[NOTE]
====
The `replicationFactor` value must match the number of nodes of your Red Hat AMQ Streams cluster.
====

// Install Kafka
include::modules/serverless-install-kafka-odc.adoc[leveloffset=+2]

// Configure TLS and SASL for Kafka
[id="serverless-kafka-authentication"]
== Configuring authentication for Kafka

In production, Kafka clusters are often secured using the TLS or SASL authentication methods. This section shows how to configure the Kafka channel to work against a protected Red Hat AMQ Streams (Kafka) cluster using TLS or SASL.

[NOTE]
====
If you choose to enable SASL, Red Hat recommends to also enable TLS.
====

include::modules/serverless-kafka-tls.adoc[leveloffset=+2]
include::modules/serverless-kafka-sasl.adoc[leveloffset=+2]
include::modules/serverless-kafka-sasl-public-certs.adoc[leveloffset=+2]

[id="next-steps_serverless-kafka"]
== Next steps

* Create a xref:../../serverless/channels/serverless-creating-channels.adoc#serverless-creating-channels[Kafka channel].
* Create a xref:../../serverless/event_sources/serverless-kafka-source.adoc#serverless-kafka-source[Kafka event source].
* Create a xref:../../serverless/integrations/serverless-kafka-broker.adoc#serverless-kafka-broker[Kafka broker].
