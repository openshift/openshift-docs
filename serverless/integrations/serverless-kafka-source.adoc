include::modules/serverless-document-attributes.adoc[]
[id="serverless-kafka-source"]
= Using a Kafka source
include::modules/common-attributes.adoc[]
:context: serverless-kafka-source

toc::[]

You can create a Knative Kafka event source that reads events from an Apache Kafka cluster and passes these events to a sink.

[id="prerequisites_serverless-kafka-source"]
== Prerequisites

You can use the `KafkaSource` event source with {ServerlessProductName} after you have xref:../../serverless/install/installing-knative-eventing.adoc#installing-knative-eventing[Knative Eventing] and xref:../../serverless/integrations/serverless-kafka.adoc#serverless-kafka[Knative Kafka] installed on your cluster.

// dev console
include::modules/serverless-kafka-source-odc.adoc[leveloffset=+1]
// kn commands
include::modules/serverless-kafka-source-kn.adoc[leveloffset=+1]
include::modules/specifying-sink-flag-kn.adoc[leveloffset=+2]
// YAML
include::modules/serverless-kafka-source-yaml.adoc[leveloffset=+1]

[id="additional-resources_serverless-kafka-source"]
== Additional resources

* See xref:../../serverless/event_sources/knative-event-sources.adoc#knative-event-sources[Understanding event sources].
* See xref:../../serverless/integrations/serverless-kafka.adoc#serverless-kafka[Knative Kafka].
* See the link:https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html/amq_streams_on_openshift_overview/kafka-concepts_str#kafka-concepts-key_str[Red Hat AMQ Streams] documentation for more information about Kafka concepts.
