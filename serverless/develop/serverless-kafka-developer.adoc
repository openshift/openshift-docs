:_content-type: ASSEMBLY
[id="serverless-kafka-developer"]
= Using Knative Kafka
include::_attributes/common-attributes.adoc[]
:context: serverless-kafka-developer

toc::[]

Knative Kafka provides integration options for you to use supported versions of the Apache Kafka message streaming platform with {ServerlessProductName}. Kafka provides options for event source, channel, broker, and event sink capabilities.

Knative Kafka functionality is available in an {ServerlessProductName} installation xref:../../serverless/admin_guide/serverless-kafka-admin.adoc#serverless-install-kafka-odc_serverless-kafka-admin[if a cluster administrator has installed the `KnativeKafka` custom resource].

// OCP
ifdef::openshift-enterprise[]
[NOTE]
====
Knative Kafka is not currently supported for IBM Z and IBM Power.
====
endif::[]

Knative Kafka provides additional options, such as:

* Kafka source
* Kafka channel
* Kafka broker (Technology Preview)
* Kafka sink (Technology Preview)

include::modules/serverless-kafka-event-delivery.adoc[leveloffset=+1]

See the xref:../../serverless/develop/serverless-event-delivery.adoc#serverless-event-delivery[Event delivery] documentation for more information about delivery guarantees.

[id="serverless-kafka-developer-source"]
== Kafka source

You can create a Kafka source that reads events from an Apache Kafka cluster and passes these events to a sink. You can create a Kafka source by using the {product-title} web console, the Knative (`kn`) CLI, or by creating a `KafkaSource` object directly as a YAML file and using the OpenShift (`oc`) CLI to apply it.

// dev console
include::modules/serverless-kafka-source-odc.adoc[leveloffset=+2]
// kn commands
include::modules/serverless-kafka-source-kn.adoc[leveloffset=+2]
include::modules/specifying-sink-flag-kn.adoc[leveloffset=+3]
// YAML
include::modules/serverless-kafka-source-yaml.adoc[leveloffset=+2]

[id="serverless-kafka-developer-broker"]
== Kafka broker

include::snippets/serverless-kafka-broker-intro.adoc[]

:FeatureName: Kafka broker
include::snippets/technology-preview.adoc[leveloffset=+2]

include::modules/serverless-kafka-broker.adoc[leveloffset=+2]
include::modules/serverless-kafka-broker-with-kafka-topic.adoc[leveloffset=+2]

// Kafka channels
include::modules/serverless-create-kafka-channel-yaml.adoc[leveloffset=+1]

[id="serverless-kafka-developer-sink"]
== Kafka sink

Kafka sinks are a type of xref:../../serverless/develop/serverless-event-sinks.adoc#serverless-event-sinks[event sink] that are available if a cluster administrator has enabled Kafka on your cluster. You can send events directly from an xref:../../serverless/discover/knative-event-sources.adoc#knative-event-sources[event source] to a Kafka topic by using a Kafka sink.

:FeatureName: Kafka sink
include::snippets/technology-preview.adoc[leveloffset=+2]

// Kafka sink
include::modules/serverless-kafka-sink.adoc[leveloffset=+2]

[id="additional-resources_serverless-kafka-developer"]
[role="_additional-resources"]
== Additional resources
* link:https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html/amq_streams_on_openshift_overview/kafka-concepts_str#kafka-concepts-key_str[Red Hat AMQ Streams documentation]
* link:https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html-single/using_amq_streams_on_rhel/index#assembly-kafka-encryption-and-authentication-str[Red Hat AMQ Streams TLS and SASL on Kafka documentation]
* xref:../../serverless/develop/serverless-event-delivery.adoc#serverless-event-delivery[Event delivery]
* xref:../../serverless/admin_guide/serverless-kafka-admin.adoc#serverless-kafka-admin[Knative Kafka cluster administrator documentation]
