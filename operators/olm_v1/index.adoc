:_content-type: ASSEMBLY
[id="olmv1-about"]
= About Operator Lifecycle Manager v1 (Technology Preview)
include::_attributes/common-attributes.adoc[]
:context: olmv1-about

toc::[]

{product-title} 4.14 introduces components for a next-generation iteration of Operator Lifecycle Manager (OLM) as a Technology Preview feature. Known during this phase as OLM v1, the updated framework evolves many of the concepts that have been part of the version of OLM included in {product-title} up to this point, referred to retroactively as OLM v0.

:FeatureName: OLM v1
include::snippets/technology-preview.adoc[]

[id="olmv1-about-mission"]
== Mission statement and OLM v0

The mission of Operator Lifecycle Manager (OLM) has been to manage the lifecycle of cluster extensions centrally and declaratively on Kubernetes clusters. Its purpose has always been to make installing, running, and updating functional extensions to the cluster easy, safe, and reproducible for cluster administrators and platform-as-a-service (PaaS) administrators, throughout the lifecycle of the underlying cluster.

OLM v0, which launched with {product-title} 4 and is included by default, was focused on providing unique support for these specific needs for a particular type of cluster extension, which have been coined as Operators. Operators are classified as one or more Kubernetes controllers, shipping with one or more API extensions (`CustomResourceDefinition` objects) to provide additional functionality to the cluster.

[id="olmv1-about-what-changed"]
== What requirements have changed?

After running OLM v0 in production clusters for a number of years, it became apparent that there is an appetite to deviate from this coupling of CRDs and controllers to encompass the lifecycling of extensions that are not just Operators. OLM v1 aims to address and improve upon the following requirements.

[id="olmv1-about-dependencies"]
=== Dependencies and constraints

OLM has been helping to define lifecycles for these extensions in which the extensions:

* get installed, potentially causing other extensions to be installed as well as dependencies
* get customized with the help of customizable configuration at runtime
* get upgraded to newer version/s following upgrade paths defined by extension developers
* and finally, get decommissioned and removed.

In the dependency model, extensions can rely on each other for required services that are out of scope of the primary purpose of an extension, allowing each extension to focus on a specific purpose.

OLM also prevents conflicting extensions from running on the cluster, either with conflicting dependency constraints or conflicts in ownership of services provided via APIs. Because cluster extensions must be supported with an enterprise-grade product lifecycle, there has been a growing need for allowing Operator authors to limit installation and upgrade of their extension by specifing addtional environmental constraints as dependencies, primarily to align with what was tested by the Operator author's quality assurance (QA) or quality engineering (QE) processes.

In other words, there is an evergrowing ask for OLM to allow the author to enforce these support limitations in the form of additional constraints specified by Operator authors in their packaging for OLM.

[id="olmv1-about-isolation"]
=== Tenant isolation

During their lifecycle on the cluster, OLM also manages the permissions and capabilities extensions have on the cluster as well as the permission and access tenants on the cluster have to the extensions. This is done using the Kubernetes RBAC system, in combination with tenant isolation using Kubernetes namespaces.

While the interaction surface of the extensions is solely composed of Kubernetes APIs the extensions define, there is an acute need to rethink the way tenant (consumers of extensions) isolation is achieved. The ask from OLM is to provide tenant isolation in a more intuitive way than is implemented in OLM v0.

[id="olmv1-about-packaging"]
=== Improved packaging models

OLM also defines a packaging model in which catalogs of extensions, usually containing the entire version history of each extension, are made available to clusters for cluster users to browse and select from. While these catalogs have so far been packaged and shipped as container images, there is a growing appetite to allow more ways of packaging and shipping these catalogs, besides also simplifying the building process of these catalogs, which so far have been very costly.

The effort to bring down the cost was kicked off in OLM v0 with conversion of the underlying datastore for catalog metadata to file-based catalogs (FBCs), with more effort being invested to slim down the process in v1. Using new versions of extensions delivered with this packaging system, OLM is able to apply updates to existing running extensions on the cluster in a way where the integrity of the cluster is maintained and constraints and dependencies are kept satisfied.

[id="olmv1-about-multicluster"]
=== Multi-cluster support

The scope of OLM's area of operation in v0 is the one cluster it is running on, with namespace-based handling of catalog access and extension API accessibility and discoverability. Expansion of this scope is indirectly expected through the work of the Kubernetes Control Plane (kcp) project. In its first incarnation, kcp will likely use its own synchronization mechanism to get OLM-managed extensions deployed eventually on one or more physical clusters from a shared, virtual control plane called a "workspace".

While this is an area under active development and subject to change, OLM will most likely need to become aware of kcp in a future state. In OLM v1, the scope of OLM will increase to span multiple clusters following the kcp model, though likely many aspects of this will become transparent to OLM itself through the workspace abstraction that kcp provides.

[id="olmv1-about-why-build"]
== Why build OLM v1?

In other words, what needs to change with OLM v1 is how all of the tasks mentioned previously are carried out from the user perspective, how much control users have in the process, and which persona is involved.