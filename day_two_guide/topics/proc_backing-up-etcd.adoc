////
etcd backup

Module included in the following assemblies:

* day_two_guide/host_level_tasks.adoc
* day_two_guide/environment_backup.adoc
* admin_guide/assembly_restore-etcd-quorum.adoc
////

[id='backing-up-etcd_{context}']
= Backing up etcd

When you back up etcd, you must back up both the etcd configuration files and
the etcd data.

== Backing up etcd configuration files

The etcd configuration files to be preserved are all stored in the `/etc/etcd`
directory of the instances where etcd is running. This includes the etcd
configuration file (`/etc/etcd/etcd.conf`) and the required certificates for
cluster communication. All those files are generated at installation time by the
Ansible installer.

[discrete]
=== Procedure

For each etcd member of the cluster, back up the etcd configuration.

----
$ ssh master-0
# mkdir -p /backup/etcd-config-$(date +%Y%m%d)/
# cp -R /etc/etcd/ /backup/etcd-config-$(date +%Y%m%d)/
----

[NOTE]
====
The certificates and configuration files on each etcd cluster member are unique.
====

[id='etcd-data-backup_{context}']
== Backing up etcd data

[discrete]
=== Prerequisites

[NOTE]
====
The {product-title} installer creates aliases to avoid typing all the
flags named `etcdctl2` for etcd v2 tasks and `etcdctl3` for etcd v3 tasks.

However, the `etcdctl3` alias does not provide the full endpoint list to the
`etcdctl` command, so you must specify the `--endpoints` option and list all
the endpoints.
====

Before backing up etcd:

* `etcdctl` binaries must be available or, in containerized installations, the `rhel7/etcd` container must be available.
* Ensure that the {product-title} API service is running.
* Ensure connectivity with the etcd cluster (port 2379/tcp).
* Ensure the proper certificates to connect to the etcd cluster.
* Ensure `go` is installed.

ifeval::["{context}" == "environment-backup"]
. To ensure the etcd cluster is working, check its health.
** Run the following command:
+
----
# ETCDCTL_API=3 etcdctl --cert="/etc/etcd/peer.crt" \
          --key=/etc/etcd/peer.key \
          --cacert="/etc/etcd/ca.crt" \
          --endpoints="https://*master-0.example.com*:2379,\
            https://*master-1.example.com*:2379,\
            https://*master-2.example.com*:2379"
            endpoint health
https://master-0.example.com:2379 is healthy: successfully committed proposal: took = 5.011358ms
https://master-1.example.com:2379 is healthy: successfully committed proposal: took = 1.305173ms
https://master-2.example.com:2379 is healthy: successfully committed proposal: took = 1.388772ms
----

. Check the member list.
+
----
# etcdctl3 member list
2a371dd20f21ca8d, started, master-1.example.com, https://192.168.55.12:2380, https://192.168.55.12:2379
40bef1f6c79b3163, started, master-0.example.com, https://192.168.55.8:2380, https://192.168.55.8:2379
95dc17ffcce8ee29, started, master-2.example.com, https://192.168.55.13:2380, https://192.168.55.13:2379
----
endif::[]

ifeval::["{context}" == "day-two-host-level-tasks"]
. To ensure the etcd cluster is working, check its health.
+
----
# ETCDCTL_API=3 etcdctl --cert="/etc/etcd/peer.crt" \
          --key=/etc/etcd/peer.key \
          --cacert="/etc/etcd/ca.crt" \
          --endpoints="https://*master-0.example.com*:2379,\
            https://*master-1.example.com*:2379,\
            https://*master-2.example.com*:2379"
            endpoint health
https://master-0.example.com:2379 is healthy: successfully committed proposal: took = 5.011358ms
https://master-1.example.com:2379 is healthy: successfully committed proposal: took = 1.305173ms
https://master-2.example.com:2379 is healthy: successfully committed proposal: took = 1.388772ms
----

. Check the member list.
+
----
# etcdctl3 member list
2a371dd20f21ca8d, started, master-1.example.com, https://192.168.55.12:2380, https://192.168.55.12:2379
40bef1f6c79b3163, started, master-0.example.com, https://192.168.55.8:2380, https://192.168.55.8:2379
95dc17ffcce8ee29, started, master-2.example.com, https://192.168.55.13:2380, https://192.168.55.13:2379
----
endif::[]

[discrete]
=== Procedure

[NOTE]
====
While the `etcdctl backup` command is used to perform the backup, etcd v3 has
no concept of a _backup_. Instead, you either take a _snapshot_ from a live
member with the `etcdctl snapshot save` command or copy the
`member/snap/db` file from an etcd data directory.

The `etcdctl backup` command rewrites some of the metadata contained in the
backup, specifically, the node ID and cluster ID, which means that in the
backup, the node loses its former identity. To recreate a cluster from
the backup, you create a new, single-node cluster, then add the rest of the nodes
to the cluster. The metadata is rewritten to prevent the new node from
joining an existing cluster.
====

Back up the etcd data:

[IMPORTANT]
====
Clusters upgraded from previous versions of {product-title} might
contain v2 data stores. Back up all etcd data stores.
====

. Obtain the etcd endpoint IP address from the static pod manifest:
+
----
$ export ETCD_POD_MANIFEST="/etc/origin/node/pods/etcd.yaml"
$ export ETCD_EP=$(grep https ${ETCD_POD_MANIFEST} | cut -d '/' -f3)
----

. Obtain the etcd pod name:
+
----
$ oc login -u system:admin
$ export ETCD_POD=$(oc get pods -n kube-system | grep -o -m 1 '\S*etcd\S*')
----

. Take a snapshot of the etcd data in the pod and store it locally:
+
----
$ oc project kube-system
$ oc exec ${ETCD_POD} -c etcd -- /bin/bash -c "ETCDCTL_API=3 etcdctl \
    --cert /etc/etcd/peer.crt \
    --key /etc/etcd/peer.key \
    --cacert /etc/etcd/ca.crt \
    --endpoints $ETCD_EP \
    snapshot save /var/lib/etcd/snapshot.db" <1>
----
<1> You must write the snapshot to a directory under `/var/lib/etcd/`.
