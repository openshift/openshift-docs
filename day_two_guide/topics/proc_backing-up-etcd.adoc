////
etcd backup

Module included in the following assemblies:

* day_two_guide/host_level_tasks.adoc
* day_two_guide/environment_backup.adoc
* admin_guide/assembly_restore-etcd-quorum.adoc
////

[id='backing-up-etcd_{context}']
= Backing up etcd

When you back up etcd, you must back up both the etcd configuration files and
the etcd data.

== Backing up etcd configuration files

The etcd configuration files to be preserved are all stored in the `/etc/etcd`
directory of the instances where etcd is running. This includes the etcd
configuration file (`/etc/etcd/etcd.conf`) and the required certificates for
cluster communication. All those files are generated at installation time by the
Ansible installer.

[discrete]
=== Procedure

For each etcd member of the cluster, back up the etcd configuration.

----
$ ssh master-0
# mkdir -p /backup/etcd-config-$(date +%Y%m%d)/
# cp -R /etc/etcd/ /backup/etcd-config-$(date +%Y%m%d)/
----

[NOTE]
====
The certificates and configuration files on each etcd cluster member are unique.
====

[id='etcd-data-backup_{context}']
== Backing up etcd data

[discrete]
=== Prerequisites

[NOTE]
====
The {product-title} installer creates aliases to avoid typing all the
flags named `etcdctl2` for etcd v2 tasks and `etcdctl3` for etcd v3 tasks.

However, the `etcdctl3` alias does not provide the full endpoint list to the
`etcdctl` command, so the `--endpoints` option with all the endpoints must be
provided.
====

Before backing up etcd:

* `etcdctl` binaries should be available or, in containerized installations, the `rhel7/etcd` container should be available
* Ensure connectivity with the etcd cluster (port 2379/tcp)
* Ensure the proper certificates to connect to the etcd cluster

ifeval::["{context}" == "environment-backup"]
. To ensure the etcd cluster is working, check its health.
** If you use the etcd v2 API, run the following command:
+
----
# etcdctl --cert-file=/etc/etcd/peer.crt \
          --key-file=/etc/etcd/peer.key \
          --ca-file=/etc/etcd/ca.crt \
          --peers="https://*master-0.example.com*:2379,\
          https://*master-1.example.com*:2379,\
          https://*master-2.example.com*:2379"\
          cluster-health
member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
cluster is healthy
----
+
** If you use the etcd v3 API, run the following command:
+
----
# ETCDCTL_API=3 etcdctl --cert="/etc/etcd/peer.crt" \
          --key=/etc/etcd/peer.key \
          --cacert="/etc/etcd/ca.crt" \
          --endpoints="https://*master-0.example.com*:2379,\
            https://*master-1.example.com*:2379,\
            https://*master-2.example.com*:2379"
            endpoint health
https://master-0.example.com:2379 is healthy: successfully committed proposal: took = 5.011358ms
https://master-1.example.com:2379 is healthy: successfully committed proposal: took = 1.305173ms
https://master-2.example.com:2379 is healthy: successfully committed proposal: took = 1.388772ms
----

. Check the member list.
** If you use the etcd v2 API, run the following command:
+
----
# etcdctl2 member list
2a371dd20f21ca8d: name=master-1.example.com peerURLs=https://192.168.55.12:2380 clientURLs=https://192.168.55.12:2379 isLeader=false
40bef1f6c79b3163: name=master-0.example.com peerURLs=https://192.168.55.8:2380 clientURLs=https://192.168.55.8:2379 isLeader=false
95dc17ffcce8ee29: name=master-2.example.com peerURLs=https://192.168.55.13:2380 clientURLs=https://192.168.55.13:2379 isLeader=true
----
+
** If you use the etcd v3 API, run the following command:
+
----
# etcdctl3 member list
2a371dd20f21ca8d, started, master-1.example.com, https://192.168.55.12:2380, https://192.168.55.12:2379
40bef1f6c79b3163, started, master-0.example.com, https://192.168.55.8:2380, https://192.168.55.8:2379
95dc17ffcce8ee29, started, master-2.example.com, https://192.168.55.13:2380, https://192.168.55.13:2379
----
endif::[]

ifeval::["{context}" == "day-two-host-level-tasks"]
. To ensure the etcd cluster is working, check its health.
** If you use the etcd v2 API, run the following command:
+
----
# etcdctl --cert-file=/etc/etcd/peer.crt \
          --key-file=/etc/etcd/peer.key \
          --ca-file=/etc/etcd/ca.crt \
          --peers="https://*master-0.example.com*:2379,\
          https://*master-1.example.com*:2379,\
          https://*master-2.example.com*:2379"\
          cluster-health
member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
cluster is healthy
----
+
** If you use the etcd v3 API, run the following command:
+
----
# ETCDCTL_API=3 etcdctl --cert="/etc/etcd/peer.crt" \
          --key=/etc/etcd/peer.key \
          --cacert="/etc/etcd/ca.crt" \
          --endpoints="https://*master-0.example.com*:2379,\
            https://*master-1.example.com*:2379,\
            https://*master-2.example.com*:2379"
            endpoint health
https://master-0.example.com:2379 is healthy: successfully committed proposal: took = 5.011358ms
https://master-1.example.com:2379 is healthy: successfully committed proposal: took = 1.305173ms
https://master-2.example.com:2379 is healthy: successfully committed proposal: took = 1.388772ms
----

. Check the member list.
** If you use the etcd v2 API, run the following command:
+
----
# etcdctl2 member list
2a371dd20f21ca8d: name=master-1.example.com peerURLs=https://192.168.55.12:2380 clientURLs=https://192.168.55.12:2379 isLeader=false
40bef1f6c79b3163: name=master-0.example.com peerURLs=https://192.168.55.8:2380 clientURLs=https://192.168.55.8:2379 isLeader=false
95dc17ffcce8ee29: name=master-2.example.com peerURLs=https://192.168.55.13:2380 clientURLs=https://192.168.55.13:2379 isLeader=true
----
+
** If you use the etcd v3 API, run the following command:
+
----
# etcdctl3 member list
2a371dd20f21ca8d, started, master-1.example.com, https://192.168.55.12:2380, https://192.168.55.12:2379
40bef1f6c79b3163, started, master-0.example.com, https://192.168.55.8:2380, https://192.168.55.8:2379
95dc17ffcce8ee29, started, master-2.example.com, https://192.168.55.13:2380, https://192.168.55.13:2379
----
endif::[]

[discrete]
=== Procedure

[NOTE]
====
While the `etcdctl backup` command is used to perform the backup, etcd v3 has
no concept of a _backup_. Instead, you either take a _snapshot_ from a live
member with the `etcdctl snapshot save` command or copy the
`member/snap/db` file from an etcd data directory.

The `etcdctl backup` command rewrites some of the metadata contained in the
backup, specifically, the node ID and cluster ID, which means that in the
backup, the node loses its former identity. To recreate a cluster from
the backup, you create a new, single-node cluster, then add the rest of the nodes
to the cluster. The metadata is rewritten to prevent the new node from
joining an existing cluster.
====

Back up the etcd data:
* If you use the v2 API, take the following actions:
.. Stop all etcd services:
+
----
# systemctl stop etcd.service
----
.. Create the etcd data backup and copy the etcd `db` file:
+
----
# mkdir -p /backup/etcd-$(date +%Y%m%d)
# etcdctl2 backup \
    --data-dir /var/lib/etcd \
    --backup-dir /backup/etcd-$(date +%Y%m%d)
# cp /var/lib/etcd/member/snap/db /backup/etcd-$(date +%Y%m%d)
----
* If you use the v3 API, run the following command:
+
----
# mkdir -p /backup/etcd-$(date +%Y%m%d)
# etcdctl3 snapshot save */backup/etcd-$(date +%Y%m%d)*/db
Snapshot saved at /backup/etcd-<date>/db
# systemctl stop etcd.service
# etcdctl2 backup \
    --data-dir /var/lib/etcd \
    --backup-dir /backup/etcd-$(date +%Y%m%d)
# systemctl start etcd.service
----
+
[NOTE]
====
The `etcdctl snapshot save` command requires the etcd service to be running.
====
+
--
In these commands, a `/backup/etcd-<date>/` directory is created, where `<date>`
represents the current date, which must be an external NFS share, S3 bucket, or
any external storage location.

In the case of an all-in-one cluster, the etcd data directory is located in
the `/var/lib/origin/openshift.local.etcd` directory.
--
