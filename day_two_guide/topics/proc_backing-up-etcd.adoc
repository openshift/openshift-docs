////
etcd backup

Module included in the following assemblies:

* day_two_guide/host_level_tasks.adoc
* day_two_guide/environment_backup.adoc
* admin_guide/assembly_restore-etcd-quorum.adoc
////

[id='backing-up-etcd_{context}']
= Backing up etcd

When you back up etcd, you must back up both the etcd configuration files and
the etcd data.

You can use either etcd v2 or v3 API versions to back up etcd because both
versions contain commands to back up the v2 and v3 data.

== Backing up etcd configuration files

The etcd configuration files to be preserved are all stored in the `/etc/etcd`
directory of the instances where etcd is running. This includes the etcd
configuration file (`/etc/etcd/etcd.conf`) and the required certificates for
cluster communication. All those files are generated at installation time by the
Ansible installer.

[discrete]
=== Procedure

For each etcd member of the cluster, back up the etcd configuration.

----
$ ssh master-0
# mkdir -p /backup/etcd-config-$(date +%Y%m%d)/
# cp -R /etc/etcd/ /backup/etcd-config-$(date +%Y%m%d)/
----

[NOTE]
====
The certificates and configuration files on each etcd cluster member are unique.
====

[id='etcd-data-backup_{context}']
== Backing up etcd data

[discrete]
=== Prerequisites

[NOTE]
====
The {product-title} installer creates aliases to avoid typing all the
flags named `etcdctl2` for etcd v2 tasks and `etcdctl3` for etcd v3 tasks.

However, the `etcdctl3` alias does not provide the full endpoint list to the
`etcdctl` command, so you must specify the `--endpoints` option and list all
the endpoints.
====

Before backing up etcd:

* `etcdctl` binaries must be available or, in containerized installations, the `rhel7/etcd` container must be available.
* Ensure that the {product-title} API service is running.
* Ensure connectivity with the etcd cluster (port 2379/tcp).
* Ensure the proper certificates to connect to the etcd cluster.
* Ensure `go` is installed.

ifeval::["{context}" == "environment-backup"]
. To ensure the etcd cluster is working, check its health. Because the etcd node
information is the same with API v2 and v3, you can use either to run health
checks and member-related operations.

** If you use the etcd v2 API, run the following command:
+
----
# etcdctl --cert-file=/etc/etcd/peer.crt \
          --key-file=/etc/etcd/peer.key \
          --ca-file=/etc/etcd/ca.crt \
          --peers="https://*master-0.example.com*:2379,\
          https://*master-1.example.com*:2379,\
          https://*master-2.example.com*:2379"\
          cluster-health
member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
cluster is healthy
----
+
** If you use the etcd v3 API, run the following command:
+
----
# ETCDCTL_API=3 etcdctl --cert="/etc/etcd/peer.crt" \
          --key=/etc/etcd/peer.key \
          --cacert="/etc/etcd/ca.crt" \
          --endpoints="https://*master-0.example.com*:2379,\
            https://*master-1.example.com*:2379,\
            https://*master-2.example.com*:2379"
            endpoint health
https://master-0.example.com:2379 is healthy: successfully committed proposal: took = 5.011358ms
https://master-1.example.com:2379 is healthy: successfully committed proposal: took = 1.305173ms
https://master-2.example.com:2379 is healthy: successfully committed proposal: took = 1.388772ms
----

. Check the member list.
** If you use the etcd v2 API, run the following command:
+
----
# etcdctl2 member list
2a371dd20f21ca8d: name=master-1.example.com peerURLs=https://192.168.55.12:2380 clientURLs=https://192.168.55.12:2379 isLeader=false
40bef1f6c79b3163: name=master-0.example.com peerURLs=https://192.168.55.8:2380 clientURLs=https://192.168.55.8:2379 isLeader=false
95dc17ffcce8ee29: name=master-2.example.com peerURLs=https://192.168.55.13:2380 clientURLs=https://192.168.55.13:2379 isLeader=true
----
+
** If you use the etcd v3 API, run the following command:
+
----
# etcdctl3 member list
2a371dd20f21ca8d, started, master-1.example.com, https://192.168.55.12:2380, https://192.168.55.12:2379
40bef1f6c79b3163, started, master-0.example.com, https://192.168.55.8:2380, https://192.168.55.8:2379
95dc17ffcce8ee29, started, master-2.example.com, https://192.168.55.13:2380, https://192.168.55.13:2379
----
endif::[]

ifeval::["{context}" == "day-two-host-level-tasks"]
. To ensure the etcd cluster is working, check its health.
** If you use the etcd v2 API, run the following command:
+
----
# etcdctl --cert-file=/etc/etcd/peer.crt \
          --key-file=/etc/etcd/peer.key \
          --ca-file=/etc/etcd/ca.crt \
          --peers="https://*master-0.example.com*:2379,\
          https://*master-1.example.com*:2379,\
          https://*master-2.example.com*:2379"\
          cluster-health
member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
cluster is healthy
----
+
** If you use the etcd v3 API, run the following command:
+
----
# ETCDCTL_API=3 etcdctl --cert="/etc/etcd/peer.crt" \
          --key=/etc/etcd/peer.key \
          --cacert="/etc/etcd/ca.crt" \
          --endpoints="https://*master-0.example.com*:2379,\
            https://*master-1.example.com*:2379,\
            https://*master-2.example.com*:2379"
            endpoint health
https://master-0.example.com:2379 is healthy: successfully committed proposal: took = 5.011358ms
https://master-1.example.com:2379 is healthy: successfully committed proposal: took = 1.305173ms
https://master-2.example.com:2379 is healthy: successfully committed proposal: took = 1.388772ms
----

. Check the member list.
** If you use the etcd v2 API, run the following command:
+
----
# etcdctl2 member list
2a371dd20f21ca8d: name=master-1.example.com peerURLs=https://192.168.55.12:2380 clientURLs=https://192.168.55.12:2379 isLeader=false
40bef1f6c79b3163: name=master-0.example.com peerURLs=https://192.168.55.8:2380 clientURLs=https://192.168.55.8:2379 isLeader=false
95dc17ffcce8ee29: name=master-2.example.com peerURLs=https://192.168.55.13:2380 clientURLs=https://192.168.55.13:2379 isLeader=true
----
+
** If you use the etcd v3 API, run the following command:
+
----
# etcdctl3 member list
2a371dd20f21ca8d, started, master-1.example.com, https://192.168.55.12:2380, https://192.168.55.12:2379
40bef1f6c79b3163, started, master-0.example.com, https://192.168.55.8:2380, https://192.168.55.8:2379
95dc17ffcce8ee29, started, master-2.example.com, https://192.168.55.13:2380, https://192.168.55.13:2379
----
endif::[]

[discrete]
=== Procedure

[NOTE]
====
While the `etcdctl backup` command is used to perform the backup, etcd v3 has
no concept of a _backup_. Instead, you either take a _snapshot_ from a live
member with the `etcdctl snapshot save` command or copy the
`member/snap/db` file from an etcd data directory.

The `etcdctl backup` command rewrites some of the metadata contained in the
backup, specifically, the node ID and cluster ID, which means that in the
backup, the node loses its former identity. To recreate a cluster from
the backup, you create a new, single-node cluster, then add the rest of the nodes
to the cluster. The metadata is rewritten to prevent the new node from
joining an existing cluster.
====

Back up the etcd data:

* If you run etcd on standalone hosts and use the v2 API, take the following actions:
.. Stop all etcd services by removing the etcd pod definition:
+
----
# mkdir -p /etc/origin/node/pods-stopped
# mv /etc/origin/node/pods/* /etc/origin/node/pods-stopped/
----
.. Create the etcd data backup and copy the etcd `db` file:
+
----
# mkdir -p /backup/etcd-$(date +%Y%m%d)
# etcdctl2 backup \
    --data-dir /var/lib/etcd \
    --backup-dir /backup/etcd-$(date +%Y%m%d)
# cp /var/lib/etcd/member/snap/db /backup/etcd-$(date +%Y%m%d)
----
+
--
A `/backup/etcd-<date>/` directory is created, where `<date>`
represents the current date, which must be an external NFS share, S3 bucket, or
any external storage location.

In the case of an all-in-one cluster, the etcd data directory is located in
the `/var/lib/origin/openshift.local.etcd` directory.
--
+
.. Reboot the node to restart the etcd service.
+
----
# reboot
----
* If you run etcd on standalone hosts and use the v3 API, run the following commands:
+
[IMPORTANT]
====
Clusters upgraded from previous versions of {product-title} might
contain v2 data stores. Back up all etcd data stores.
====

.. Back up etcd v3 data:
+
.. Make a snapshot of the etcd node:
+
----
# systemctl show etcd --property=ActiveState,SubState
# mkdir -p /var/lib/etcd/backup/etcd-$(date +%Y%m%d) <1>
# etcdctl3 snapshot save /var/lib/etcd/backup/etcd-$(date +%Y%m%d)/db
----
<1> You must write the snapshot to a directory under `/var/lib/etcd/`.
+
[NOTE]
====
The `etcdctl snapshot save` command requires the etcd service to be running.
====
+
.. Stop all etcd services by removing the etcd pod definition and rebooting the host:
+
----
# mkdir -p /etc/origin/node/pods-stopped
# mv /etc/origin/node/pods/* /etc/origin/node/pods-stopped/
----
+
.. Create the etcd data backup and copy the etcd `db` file:
+
----
# etcdctl2 backup \
    --data-dir /var/lib/etcd \
    --backup-dir /backup/etcd-$(date +%Y%m%d)
----
+
--
A `/backup/etcd-<date>/` directory is created, where `<date>`
represents the current date, which must be an external NFS share, S3 bucket, or
any external storage location.

In the case of an all-in-one cluster, the etcd data directory is located in
the `/var/lib/origin/openshift.local.etcd` directory.
--

* If etcd runs as a static pod, run the following commands:
+
[IMPORTANT]
====
If you use static pods, use the v3 API.
====
.. Obtain the etcd endpoint IP address from the static pod manifest:
+
----
$ export ETCD_POD_MANIFEST="/etc/origin/node/pods/etcd.yaml"
$ export ETCD_EP=$(grep https ${ETCD_POD_MANIFEST} | cut -d '/' -f3)
----

.. Obtain the etcd pod name:
+
----
$ oc login -u system:admin
$ export ETCD_POD=$(oc get pods -n kube-system | grep -o -m 1 '\S*etcd\S*')
----

.. Take a snapshot of the etcd data in the pod and store it locally:
+
----
$ oc project kube-system
$ oc exec ${ETCD_POD} -c etcd -- /bin/bash -c "ETCDCTL_API=3 etcdctl \
    --cert /etc/etcd/peer.crt \
    --key /etc/etcd/peer.key \
    --cacert /etc/etcd/ca.crt \
    --endpoints $ETCD_EP \
    snapshot save /var/lib/etcd/snapshot.db"
----
