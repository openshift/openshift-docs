////
Connectivity

Module included in the following assemblies:

* day_two_guide/environment_health_checks.adoc
////

Network connectivity has two main networking layers: the cluster network for
node interaction, and the software defined network (SDN) for pod interaction.
{product-title} supports multiple network configurations, often optimized for a
specific infrastructure provider. 

[NOTE]
====
Due to the complexity of networking, not all verification scenarios are covered
in this section.
====

= Connectivity on master hosts

*etcd and master hosts*

Master services keep their state synchronized using the etcd key-value store.
Communication between master and etcd services is important, whether those
etcd services are collocated on master hosts, or running on hosts designated
only for the etcd service. This communication happens on TCP ports `2379` and
`2380`. See the
xref:../day_two_guide/environment_health_checks.adoc#day-two-guide-host-health[Host
health] section for methods to check this communication.

*SkyDNS*

`SkyDNS` provides name resolution of local services running in {product-title}.
This service uses `TCP` and `UDP` port `8053`.

To verify the name resolution:

----
$ dig +short docker-registry.default.svc.cluster.local
172.30.150.7
----

If the answer matches the output of the following, `SkyDNS` service is working correctly:

----
$ oc get svc/docker-registry
NAME              CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
docker-registry   172.30.150.7   <none>        5000/TCP   3d
----

*API service and web console*

Both the API service and web console share the same port, usually `TCP` `8443`
or `443`, depending on the setup. This port needs to be available within the
cluster and to everyone who needs to work with the deployed environment. The
URLs under which this port is reachable may differ for internal cluster and for
external clients. 

In the following example, the `https://internal-master.example.com:443` URL is
used by the internal cluster, and the `https://master.example.com:443` URL is
used by external clients. On any node host:

----
$ curl https://internal-master.example.com:443/version
{
  "major": "1",
  "minor": "6",
  "gitVersion": "v1.6.1+5115d708d7",
  "gitCommit": "fff65cf",
  "gitTreeState": "clean",
  "buildDate": "2017-10-11T22:44:25Z",
  "goVersion": "go1.7.6",
  "compiler": "gc",
  "platform": "linux/amd64"
}
----

This must be reachable from client's network:

----
$ curl -k https://master.example.com:443/healthz
ok
----

= Connectivity on node instances

The SDN connecting pod communication on nodes uses `UDP` port `4789` by default.

To verify node host functionality, create a new application. The following
example ensures the node reaches the docker registry, which is running on an
infrastructure node:

[discrete]
=== Procedure

. Create a new project:
+
----
$ oc new-project sdn-test
----

. Deploy an httpd application:
+
----
$ oc new-app centos/httpd-24-centos7~https://github.com/openshift/httpd-ex
----
+
Wait until the build is complete:
+
----
$ oc get pods
NAME               READY     STATUS      RESTARTS   AGE
httpd-ex-1-205hz   1/1       Running     0          34s
httpd-ex-1-build   0/1       Completed   0          1m
----

. Connect to the running pod:
+
----
$ oc rsh po/httpd-ex-1-205hz
----

. Check the `healthz` path of the internal registry service:
+
----
$ curl -kv https://docker-registry.default.svc.cluster.local:5000/healthz
* About to connect() to docker-registry.default.svc.cluster.locl port 5000 (#0)
*   Trying 172.30.150.7...
* Connected to docker-registry.default.svc.cluster.local (172.30.150.7) port 5000 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* skipping SSL peer certificate verification
* SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
* Server certificate:
* 	subject: CN=172.30.150.7
* 	start date: Nov 30 17:21:51 2017 GMT
* 	expire date: Nov 30 17:21:52 2019 GMT
* 	common name: 172.30.150.7
* 	issuer: CN=openshift-signer@1512059618
> GET /healthz HTTP/1.1
> User-Agent: curl/7.29.0
> Host: docker-registry.default.svc.cluster.local:5000
> Accept: */*
>
< HTTP/1.1 200 OK
< Cache-Control: no-cache
< Date: Mon, 04 Dec 2017 16:26:49 GMT
< Content-Length: 0
< Content-Type: text/plain; charset=utf-8
<
* Connection #0 to host docker-registry.default.svc.cluster.local left intact

sh-4.2$ *exit*
----
+
The `HTTP/1.1 200 OK` response means the node is correctly connecting.

. Clean up the test project:
+
----
$ oc delete project sdn-test
project "sdn-test" deleted
----

. The node host is listening on `TCP` port `10250`. This port needs to be
reachable by all master hosts on any node, and if monitoring is deployed in the
cluster, the infrastructure nodes must have access to this port on all instances
as well. Broken communication on this port can be detected with the following
command:
+
----
$ oc get nodes
NAME                  STATUS                     AGE       VERSION
ocp-infra-node-1clj   Ready                      4d        v1.6.1+5115d708d7
ocp-infra-node-86qr   Ready                      4d        v1.6.1+5115d708d7
ocp-infra-node-g8qw   Ready                      4d        v1.6.1+5115d708d7
ocp-master-94zd       Ready,SchedulingDisabled   4d        v1.6.1+5115d708d7
ocp-master-gjkm       Ready,SchedulingDisabled   4d        v1.6.1+5115d708d7
ocp-master-wc8w       Ready,SchedulingDisabled   4d        v1.6.1+5115d708d7
ocp-node-c5dg         Ready                      4d        v1.6.1+5115d708d7
ocp-node-ghxn         Ready                      4d        v1.6.1+5115d708d7
ocp-node-w135         NotReady                   4d        v1.6.1+5115d708d7
----
+
In the output above, the node service on the `ocp-node-w135` node is
not reachable by the master services, which is represented by its `NotReady`
status.

. The last service is the router, which is responsible for routing connections
to the correct services running in the {product-title} cluster. Routers listen
on `TCP` ports `80` and `443` on infrastructure nodes for ingress traffic.
Before routers can start working, DNS must be configured:
+
----
$ dig *.apps.example.com

; <<>> DiG 9.11.1-P3-RedHat-9.11.1-8.P3.fc27 <<>> *.apps.example.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 45790
;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;*.apps.example.com.	IN	A

;; ANSWER SECTION:
*.apps.example.com. 3571	IN	CNAME	apps.example.com.
apps.example.com.	3561	IN	A	35.xx.xx.92

;; Query time: 0 msec
;; SERVER: 127.0.0.1#53(127.0.0.1)
;; WHEN: Tue Dec 05 16:03:52 CET 2017
;; MSG SIZE  rcvd: 105
----
+
The IP address, in this case `35.xx.xx.92`, should be pointing to the load
balancer distributing ingress traffic to all infrastructure nodes. To verify the
functionality of the routers, check the registry service once more, but this
time from outside the cluster:
+
----
$ curl -kv https://docker-registry-default.apps.example.com/healthz
*   Trying 35.xx.xx.92...
* TCP_NODELAY set
* Connected to docker-registry-default.apps.example.com (35.xx.xx.92) port 443 (#0)
...
< HTTP/2 200
< cache-control: no-cache
< content-type: text/plain; charset=utf-8
< content-length: 0
< date: Tue, 05 Dec 2017 15:13:27 GMT
<
* Connection #0 to host docker-registry-default.apps.example.com left intact
----