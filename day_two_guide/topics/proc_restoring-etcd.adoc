////
etcd restore

Module included in the following assemblies:

* admin_guide/assembly_restoring-cluster.adoc
* day_two_guide/host_level_tasks.adoc
* upgrading/downgrade.adoc
////

[id='restoring-etcd_{context}']
= Restoring etcd

The restore procedure for etcd configuration files replaces the appropriate
files, then restarts the service or static pod.

If an etcd host has become corrupted and the `/etc/etcd/etcd.conf` file is lost,
restore it using:

----
$ ssh master-0
# cp /backup/yesterday/master-0-files/etcd.conf /etc/etcd/etcd.conf
# restorecon -Rv /etc/etcd/etcd.conf
ifeval::["{context}" != "downgrade"]
endif::[]
----

In this example, the backup file is stored in the
`/backup/yesterday/master-0-files/etcd.conf` path where it can be used as an
external NFS share, S3 bucket, or other storage solution.

[NOTE]
====
If you run etcd as a static pod, follow only the steps in that section. If you
run etcd as a separate service on either master or standalone nodes, follow the
steps to restore v2 or v3 data as required.
====

ifeval::["{context}" != "downgrade"]
== Restoring etcd v2 & v3 data

The following process restores healthy data files and starts the etcd cluster as
a single node, then adds the rest of the nodes if an etcd cluster is required.

[discrete]
=== Procedure

. Stop all etcd services by
removing the etcd pod definition and rebooting the host:
+
----
# mkdir -p /etc/origin/node/pods-stopped
# mv /etc/origin/node/pods/* /etc/origin/node/pods-stopped/
# reboot
----

. To ensure the proper backup is restored, delete the etcd directories:
+
** To back up the current etcd data before you delete the directory, run the following command:
+
----
# mv /var/lib/etcd /var/lib/etcd.old
# mkdir /var/lib/etcd
# restorecon -Rv /var/lib/etcd/
----
+
** Or, to delete the directory and the etcd, data, run the following command:
+
----
# rm -Rf /var/lib/etcd/*
----
+
[NOTE]
====
In an all-in-one cluster, the etcd data directory is located in the
`/var/lib/origin/openshift.local.etcd` directory.
====

. Restore a healthy backup data file to each of the etcd nodes. Perform this step on all etcd hosts, including master hosts collocated with
etcd.
+
----
# cp -R /backup/etcd-xxx/* /var/lib/etcd/
# mv /var/lib/etcd/db /var/lib/etcd/member/snap/db
# chcon -R --reference /backup/etcd-xxx/* /var/lib/etcd/
----

. Run the etcd service on each host, forcing a new cluster.
+
This creates a custom file for the etcd service, which overwrites the execution
command adding the `--force-new-cluster` option:
+
----
# mkdir -p /etc/systemd/system/etcd.service.d/
# echo "[Service]" > /etc/systemd/system/etcd.service.d/temp.conf
# echo "ExecStart=" >> /etc/systemd/system/etcd.service.d/temp.conf
# sed -n '/ExecStart/s/"$/ --force-new-cluster"/p' \
    /usr/lib/systemd/system/etcd.service \
    >> /etc/systemd/system/etcd.service.d/temp.conf

# systemctl daemon-reload
# master-restart etcd
----

. Check for error messages:
+
----
# master-logs etcd etcd
----

. Repeat the previous steps for every etcd node to be added.

. Check for health status:
+
----
# etcdctl2 cluster-health
member 5ee217d17301 is healthy: got healthy result from https://192.168.55.8:2379
cluster is healthy
----

. Restart the etcd service in cluster mode:
+
----
# rm -f /etc/systemd/system/etcd.service.d/temp.conf
# systemctl daemon-reload
# master-restart etcd
----

. Check for health status and member list:
+
----
# etcdctl2 cluster-health
member 5ee217d17301 is healthy: got healthy result from https://192.168.55.8:2379
cluster is healthy

# etcdctl2 member list
5ee217d17301: name=master-0.example.com peerURLs=http://localhost:2380 clientURLs=https://192.168.55.8:2379 isLeader=true
----

. After the first instance is running, you can restore the rest of your etcd servers.

=== Fix the `peerURLS` parameter

After restoring the data and creating a new cluster, the `peerURLs` parameter
shows `localhost` instead of the IP where etcd is listening for peer
communication:

----
# etcdctl2 member list
5ee217d17301: name=master-0.example.com peerURLs=http://*localhost*:2380 clientURLs=https://192.168.55.8:2379 isLeader=true
----

==== Procedure

. Get the member ID using `etcdctl member list`:
+
----
`etcdctl member list`
----

. Get the IP where etcd listens for peer communication:
+
----
$ ss -l4n | grep 2380
----

. Update the member information with that IP:
+
----
# etcdctl2 member update 5ee217d17301 https://192.168.55.8:2380
Updated member with ID 5ee217d17301 in cluster
----

. To verify, check that the IP is in the member list:
+
----
$ etcdctl2 member list
5ee217d17301: name=master-0.example.com peerURLs=https://*192.168.55.8*:2380 clientURLs=https://192.168.55.8:2379 isLeader=true
----
endif::[]

== Restoring etcd v3 snapshot

ifeval::["{context}" != "downgrade"]
The restore procedure for v3 data is similar to the restore procedure for the v2
data.
endif::[]

Snapshot integrity may be optionally verified at restore time. If the snapshot
is taken with `etcdctl snapshot save`, it will have an integrity hash that is
checked by `etcdctl snapshot restore`. If the snapshot is copied from the data
directory, there is no integrity hash and it will only restore by using
`--skip-hash-check`.

[IMPORTANT]
====
The procedure to restore the data must be performed on a single etcd
host. You can then add the rest of the nodes to the cluster.
====

[discrete]
=== Procedure

ifeval::["{context}" == "downgrade"]
. Unmask the etcd service:
+
----
# systemctl unmask etcd
----
endif::[]

. Stop all etcd services by
removing the etcd pod definition and rebooting the host:
+
----
# mkdir -p /etc/origin/node/pods-stopped
# mv /etc/origin/node/pods/* /etc/origin/node/pods-stopped/
# reboot
----

. Clear all old data, because `etcdctl` recreates it in the node where the
restore procedure is going to be performed:
+
----
# rm -Rf /var/lib/etcd
----

. Run the `snapshot restore` command, substituting the values from the
`/etc/etcd/etcd.conf` file:
+
----
# etcdctl3 snapshot restore /backup/etcd-xxxxxx/backup.db \
  --data-dir /var/lib/etcd \
  --name master-0.example.com \
  --initial-cluster "master-0.example.com=https://192.168.55.8:2380" \
  --initial-cluster-token "etcd-cluster-1" \
  --initial-advertise-peer-urls https://192.168.55.8:2380 \
  --skip-hash-check=true

2017-10-03 08:55:32.440779 I | mvcc: restore compact to 1041269
2017-10-03 08:55:32.468244 I | etcdserver/membership: added member 40bef1f6c79b3163 [https://192.168.55.8:2380] to cluster 26841ebcf610583c
----

. Restore permissions and `selinux` context to the restored files:
+
----
# restorecon -Rv /var/lib/etcd
----

. Start the etcd service:
+
----
# systemctl start etcd
----

. Check for any error messages:
+
----
ifeval::["{context}" != "downgrade"]
# master-logs etcd etcd
endif::[]
ifeval::["{context}" == "downgrade"]
# journalctl -fu etcd.service
endif::[]
----

. Repeat the previous steps for every etcd node to be added.

== Restoring etcd on a static pod

Before restoring etcd on a static pod:

* `etcdctl` binaries must be available or, in containerized installations,
the `rhel7/etcd` container must be available.
+
You can obtain etcd by running the following commands:
+
----
$ git clone https://github.com/coreos/etcd.git
$ cd etcd
$ ./build
----


To restore etcd on a static pod:

. If the pod is running, stop the etcd pod by moving the pod manifest YAML file
to another directory:
+
----
# mkdir -p /etc/origin/node/pods-stopped
# mv /etc/origin/node/pods/etcd.yaml /etc/origin/node/pods-stopped
----

. Clear all old data:
+
----
# rm -rf /var/lib/etcd
----
+
You use the etcdctl to recreate the data in the node where you restore the pod.

. Restore the etcd snapshot to the mount path for the etcd pod:
+
----
# export ETCDCTL_API=3
# etcdctl snapshot restore /etc/etcd/backup/etcd/snapshot.db
	 --data-dir /var/lib/etcd/
	 --name ip-172-18-3-48.ec2.internal
	 --initial-cluster "ip-172-18-3-48.ec2.internal=https://172.18.3.48:2380"
	 --initial-cluster-token "etcd-cluster-1"
	 --initial-advertise-peer-urls https://172.18.3.48:2380
	 --skip-hash-check=true
----
+
Obtain the values for your cluster from the *_$/backup_files/etcd.conf_* file.

. Set required permissions and selinux context on the data directory:
+
----
# restorecon -Rv /var/lib/etcd/
----

. Restart the etcd pod by moving the pod manifest YAML file to the required
directory:
+
----
# mv /etc/origin/node/pods-stopped/etcd.yaml /etc/origin/node/pods/
----
