////
concept about etcd backup

Module included in the following assemblies:

* day_two_guide/host_level_tasks.adoc
* day_two_guide/environment_backup.adoc
////

[id='etcd-backup_{context}']
= etcd backup

etcd is the key value store for all object definitions, as well as the
persistent master state. Other components watch for changes, then bring
themselves into the desired state.

{product-title} versions prior to 3.5 use etcd version 2 (v2), while 3.5 and
later use version 3 (v3). The data model between the two versions of etcd is
different. etcd v3 can use both the v2 and v3 data models, whereas etcd v2 can
only use the v2 data model. In an etcd v3 server, the v2 and v3 data stores
exist in parallel and are independent.

For both v2 and v3 operations, you can use the `ETCDCTL_API` environment
variable to use the proper API:

----
$ etcdctl -v
etcdctl version: 3.2.5
API version: 2
$ ETCDCTL_API=3 etcdctl version
etcdctl version: 3.2.5
API version: 3.2
----

See
link:https://docs.openshift.com/container-platform/3.7/upgrading/migrating_etcd.html[Migrating etcd Data (v2 to v3) section] in the {product-title} 3.7 documentation for
information about how to migrate to v3.

The etcd backup process is composed of two different procedures:

* Configuration backup: Including the required etcd configuration and
certificates
* Data backup: Including both v2 and v3 data model.

You can perform the data backup process on any host that has connectivity to the
etcd cluster, where the proper certificates are provided, and where the
`etcdctl` tool is installed.

[NOTE]
====
The backup files must be copied to an external system, ideally outside the
{product-title} environment, and then encrypted.
====

Note that the etcd backup still has all the references to current storage volumes.
When you restore etcd, {product-title} starts launching the previous pods on
nodes and reattaching the same storage. This process is no different than the
process of when you remove a node from the cluster and add a new one back in its
place. Anything attached to that node is reattached to the pods on whatever
nodes they are rescheduled to.
