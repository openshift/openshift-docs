////
PVC backup

Module included in the following assemblies:

* day_two_guide/project_level_tasks.adoc
* day_two_guide/environment_backup.adoc
////

This topic describes how to synchronize persistent data from inside of a
container to a server and then restore the data onto a new persistent volume
claim.

[IMPORTANT]
====
Depending on the provider that is hosting the {product-title} environment, the
ability to launch third party snapshot services for backup and restore purposes
also exists. As {product-title} does not have the ability to launch these
services, this guide does not describe these steps. 
====

Consult any product documentation for the correct backup procedures of specific
applications. For example, copying the mysql data directory itself would not be
a usable backup. Instead, run the specific backup procedures of the associated
application and then synchronize any data. This includes using snapshot
solutions provided by the {product-title} hosting platform.

== Backup persistent volume claims

[discrete]
=== Procedure

. View the project and pods:
+
----
$ oc get pods
NAME           READY     STATUS      RESTARTS   AGE
demo-1-build   0/1       Completed   0          2h
demo-2-fxx6d   1/1       Running     0          1h
----

. Describe the desired pod to find the volumes currently being used by a
persistent volume:
+
----
$ oc describe pod demo-2-fxx6d
Name:			demo-2-fxx6d
Namespace:		test
Security Policy:	restricted
Node:			ip-10-20-6-20.ec2.internal/10.20.6.20
Start Time:		Tue, 05 Dec 2017 12:54:34 -0500
Labels:			app=demo
			deployment=demo-2
			deploymentconfig=demo
Status:			Running
IP:			172.16.12.5
Controllers:		ReplicationController/demo-2
Containers:
  demo:
    Container ID:	docker://201f3e55b373641eb36945d723e1e212ecab847311109b5cee1fd0109424217a
    Image:		docker-registry.default.svc:5000/test/demo@sha256:0a9f2487a0d95d51511e49d20dc9ff6f350436f935968b0c83fcb98a7a8c381a
    Image ID:		docker-pullable://docker-registry.default.svc:5000/test/demo@sha256:0a9f2487a0d95d51511e49d20dc9ff6f350436f935968b0c83fcb98a7a8c381a
    Port:		8080/TCP
    State:		Running
      Started:		Tue, 05 Dec 2017 12:54:52 -0500
    Ready:		True
    Restart Count:	0
    Volume Mounts:
      */opt/app-root/src/uploaded from persistent-volume (rw)*
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8mmrk (ro)
    Environment Variables:	<none>
...omitted...
----
+
The above shows that the persistent data is currently located in the
`/opt/app-root/src/uploaded` directory. 

. Copy the data locally:
+
----
$ oc rsync demo-2-fxx6d:/opt/app-root/src/uploaded ./demo-app
receiving incremental file list
uploaded/
uploaded/ocp_sop.txt
uploaded/lost+found/

sent 38 bytes  received 190 bytes  152.00 bytes/sec
total size is 32  speedup is 0.14
----
+
The `ocp_sop.txt` file has been pulled down to the local system to be backed up
by backup software or to another backup mechanism.
+
[NOTE]
====
The steps above can also be used in the event that a pod starts without needing
to use a `pvc`, but then decides a `pvc` is necessary. This would allow for the
data to be preserved and the restore procedures to be used to populate the new
storage.
====

== Restore persistent volume claims

This topic describes two methods for restoring data. The first involves deleting
the file, then placing the file back in the expected location. The second
example shows migrating persistent volume claims. The migration would occur in
the event that the storage needs to be moved or in a disaster scenario when the
backend storage no longer exists.

Check with the restore procedures for the specific application on any steps
required to restore data to the application.

=== Restoring files to an existing PVC

[discrete]
=== Procedure

. Delete the file:
+
----
$ oc rsh demo-2-fxx6d
sh-4.2$ ls */opt/app-root/src/uploaded/*
lost+found  ocp_sop.txt
sh-4.2$ *rm -rf /opt/app-root/src/uploaded/ocp_sop.txt*
sh-4.2$ *ls /opt/app-root/src/uploaded/*
lost+found
----

. Replace the file from the server containing the rsync backup of the files that
were in the pvc:
+
----
$ oc rsync uploaded demo-2-fxx6d:/opt/app-root/src/
----

. Validate that the file is back on the pod by using `oc rsh` to connect to the
pod and view the contents of the directory:
+
----
$ oc rsh demo-2-fxx6d
sh-4.2$ *ls /opt/app-root/src/uploaded/*
lost+found  ocp_sop.txt
----

=== Restoring data to a new PVC

The following steps assume that a new `pvc` has been created.

[discrete]
=== Procedure

. Overwrite the currently defined `claim-name`:
+
----
$ oc volume dc/demo --add --name=persistent-volume \
		--type=persistentVolumeClaim --claim-name=filestore \ --mount-path=/opt/app-root/src/uploaded --overwrite
----

. Validate that the pod is using the new PVC:
+
----
$ oc describe dc/demo
Name:		demo
Namespace:	test
Created:	3 hours ago
Labels:		app=demo
Annotations:	openshift.io/generated-by=OpenShiftNewApp
Latest Version:	3
Selector:	app=demo,deploymentconfig=demo
Replicas:	1
Triggers:	Config, Image(demo@latest, auto=true)
Strategy:	Rolling
Template:
  Labels:	app=demo
		deploymentconfig=demo
  Annotations:	openshift.io/container.demo.image.entrypoint=["container-entrypoint","/bin/sh","-c","$STI_SCRIPTS_PATH/usage"]
		openshift.io/generated-by=OpenShiftNewApp
  Containers:
   demo:
    Image:	docker-registry.default.svc:5000/test/demo@sha256:0a9f2487a0d95d51511e49d20dc9ff6f350436f935968b0c83fcb98a7a8c381a
    Port:	8080/TCP
    Volume Mounts:
      /opt/app-root/src/uploaded from persistent-volume (rw)
    Environment Variables:	<none>
  Volumes:
   persistent-volume:
    Type:	PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    *ClaimName:	filestore*
    ReadOnly:	false
...omitted...
----

. Now that the new `pvc` is being used by the deployment configuration, use `oc
rsync` to place the files onto the new `pvc`:
+
----
$ oc rsync uploaded demo-3-2b8gs:/opt/app-root/src/
sending incremental file list
uploaded/
uploaded/ocp_sop.txt
uploaded/lost+found/

sent 181 bytes  received 39 bytes  146.67 bytes/sec
total size is 32  speedup is 0.15
----

. Validate that the file is back on the pod by using `oc rsh` to connect to the
pod and view the contents of the directory.
+
----
$ oc rsh demo-3-2b8gs
sh-4.2$ ls /opt/app-root/src/uploaded/
lost+found  ocp_sop.txt
----

