=== Adding Nodes to *{rhocp}* Environment
The *{rhocp}* environment can be scaled up horizontally by adding more instances to the
environment allowing pods to be spread across more *{rhocp}* nodes. This sometimes needs to be
done to support the load within the cluster, adding a new set of labeled nodes, or
due to an increase in the number of pods for OpenShift specific resources such as logging.

The scale up playbook performs all the tasks required to add a new host to the
current *{rhocp}* environment, including generating certificates, installing
*{rhocp}* packages and configuring the host itself.

*{rhocp}* infrastructure nodes are basically nodes with different labels, so the
instructions provided for nodes and infra nodes are the same taking into account
the labels can be different.

==== Requirements
This document assumes the hosts have been created in the infrastructure and the
nodes meet the minimum https://docs.openshift.com/container-platform/latest/install/prerequisites.html#hardware[hardware requirements].

The prerequisites should be also completed before running the scale-up procedure
as the as the OpenShift installation playbooks do not include the prerequisites tasks, such as registering the hosts, install the
required packages, `ssh` key exchange for `Ansible`, firewall rules, etc. For more
information about the prerequisites, see
https://docs.openshift.com/container-platform/latest/install/host_preparation.html[host preparation] documentation.

All the tasks should be performed on the host where the installation was done, and the `Ansible` hosts file should be available.

IMPORTANT: If manual changes in the *{rhocp}* environment exist, ensure the inventory
file reflects those changes prior to the scale up procedure. This includes changes
to the *{rhocp}* configuration files, for example, modifying the *{rhocp}* node labels as they could be overwritten.

==== Procedure
The latest `Ansible` playbooks should be available on the host that is going to do
the scale up procedure (a workstation or the bastion host) by updating the `atomic-openshift-utils` package:

[subs=+quotes]
----
# *yum update atomic-openshift-utils*
----

*Inventory File*

The `Ansible` hosts file should contain the `new_<host type>` group and this group
should belong to the `OSEv3` group, where `new_<host_type>` is `new_nodes`

For example, to add a two new nodes, the `Ansible` hosts file should define the new_nodes inventory groups
and contain proper labeling relevant to the environment. Below is an
example of adding two new nodes on different zones with the role of app:

[subs=+quotes]
----
... [OUTPUT ABBREVIATED] ...
[OSEv3:children]
masters
nodes
etcd
*new_nodes*

... [OUTPUT ABBREVIATED] ...
[masters]
master1.example.com
master2.example.com
master3.example.com

[nodes]
master1.example.com openshift_node_labels="{'role': 'master', 'zone': 'A'}"
master2.example.com openshift_node_labels="{'role': 'master', 'zone': 'B'}
master3.example.com openshift_node_labels="{'role': 'master', 'zone': 'C'}
infranode1.example.com openshift_node_labels="{'role': 'infra', 'zone': 'A'}"
infranode2.example.com openshift_node_labels="{'role': 'infra', 'zone': 'B'}"
infranode3.example.com openshift_node_labels="{'role': 'infra', 'zone': 'C'}"
node01.example.com openshift_node_labels="{'role':'app','zone':'A'}"
node02.example.com openshift_node_labels="{'role':'app','zone':'B'}"
node03.example.com openshift_node_labels="{'role':'app','zone':'C'}"

[new_nodes]
node04.example.com openshift_node_labels="{'role':'app','zone':'X'}"
node05.example.com openshift_node_labels="{'role':'app','zone':'Y'}"
----

The following command access the host and runs a simple command to ensure proper
connectivity:

[subs=+quotes]
----
$ *ansible new_nodes -m command -a uptime*
node04.example.com | SUCCESS | rc=0 >>
 16:05:21 up 53 min,  1 user,  load average: 0,00, 0,01, 0,05
node05.example.com | SUCCESS | rc=0 >>
 16:05:21 up 53 min,  1 user,  load average: 0,00, 0,02, 0,05
----

Once the inventory has been updated, the scale up process can be performed
by an `Ansible` playbook included in the `atomic-openshift-utils` using the `Ansible` host file:

[subs=+quotes]
----
$ *ansible-playbook -i /path/to/host/file /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-node/scaleup.yml*
----

In the event of scaling up an infrastructure node running a {rhocp} router, the
load balancer should be modified to add the new infrastructure node in the
{rhocp} router backend pool. If the amount of router or registry nodes should be increased
the `dc` should be scaled accordingly.

==== Finishing the Scale Up
In order to properly maintain the `Ansible` host file, it needs to be edited to
reflect the current status of the *{rhocp}* cluster by moving the new nodes to the proper groups.

In this example, the `Ansible` host file used for the node scale up procedure was:
[subs=+quotes]
----
... [OUTPUT ABBREVIATED] ...
[masters]
master1.example.com
master2.example.com
master3.example.com

[nodes]
master1.example.com openshift_node_labels="{'role': 'master', 'zone': 'A'}"
master2.example.com openshift_node_labels="{'role': 'master', 'zone': 'B'}
master3.example.com openshift_node_labels="{'role': 'master', 'zone': 'C'}
infranode1.example.com openshift_node_labels="{'role': 'infra', 'zone': 'A'}"
infranode2.example.com openshift_node_labels="{'role': 'infra', 'zone': 'B'}"
infranode3.example.com openshift_node_labels="{'role': 'infra', 'zone': 'C'}"
node01.example.com openshift_node_labels="{'role':'app','zone':'A'}"
node02.example.com openshift_node_labels="{'role':'app','zone':'B'}"
node03.example.com openshift_node_labels="{'role':'app','zone':'C'}"

[new_masters]
[new_nodes]
*node04.example.com openshift_node_labels="{'role': 'app','zone':'X'}"*
*node05.example.com openshift_node_labels="{'role': 'app','zone':'Y'}"*
----

It needs to be edited as:
[subs=+quotes]
----
... [OUTPUT ABBREVIATED] ...
[masters]
master1.example.com
master2.example.com
master3.example.com

[nodes]
master1.example.com openshift_node_labels="{'role': 'master', 'zone': 'A'}"
master2.example.com openshift_node_labels="{'role': 'master', 'zone': 'B'}
master3.example.com openshift_node_labels="{'role': 'master', 'zone': 'C'}
infranode1.example.com openshift_node_labels="{'role': 'infra', 'zone': 'A'}"
infranode2.example.com openshift_node_labels="{'role': 'infra', 'zone': 'B'}"
infranode3.example.com openshift_node_labels="{'role': 'infra', 'zone': 'C'}"
node01.example.com openshift_node_labels="{'role':'app','zone':'A'}"
node02.example.com openshift_node_labels="{'role':'app','zone':'B'}"
node03.example.com openshift_node_labels="{'role':'app','zone':'C'}"
*node04.example.com openshift_node_labels="{'role': 'app','zone':'X'}"*
*node05.example.com openshift_node_labels="{'role': 'app','zone':'Y'}"*

[new_masters]
[new_nodes]
----

==== Post Scale Up Checks
Once the scale up procedure has finished, a few tasks can be performed to check if the nodes can run pods properly:

Create a new project to test:

[subs=+quotes]
----
$ *oc new-project scaleuptest*
Now using project "scaleuptest" on server "https://myocpdeployment.eastus2.cloudapp.azure.com:8443".
... [OUTPUT ABBREVIATED] ...
----

Patch the node-selector to only run pods on the new node:

[subs=+quotes]
----
$ *oc patch namespace scaleuptest -p "{\"metadata\":{\"annotations\":{\"openshift.io/node-selector\":\"kubernetes.io/hostname=node04\"}}}"*
"scaleuptest" patched
----

Deploy an example app:

[subs=+quotes]
----
$ *oc new-app openshift/hello-openshift*
--> Found Docker image 8146af6 (About an hour old) from Docker Hub for "openshift/hello-openshift"
... [OUTPUT ABBREVIATED] ...
----

Scale the number of pods to ensure they are running on the same host:

[subs=+quotes]
----
$ *oc scale dc/hello-openshift --replicas=8*
deploymentconfig "hello-openshift" scaled
----

Observe where the pods run:

[subs=+quotes]
----
$ *oc get pods -o wide*
hello-openshift-1-1ffl6   1/1       Running   0          3m        10.128.4.10   *node04*
hello-openshift-1-1kgpf   1/1       Running   0          3m        10.128.4.3    *node04*
hello-openshift-1-4lk85   1/1       Running   0          3m        10.128.4.4    *node04*
hello-openshift-1-4pfkk   1/1       Running   0          3m        10.128.4.7    *node04*
hello-openshift-1-56pqg   1/1       Running   0          3m        10.128.4.6    *node04*
hello-openshift-1-r3sjz   1/1       Running   0          3m        10.128.4.8    *node04*
hello-openshift-1-t0fmm   1/1       Running   0          3m        10.128.4.5    *node04*
hello-openshift-1-v659g   1/1       Running   0          3m        10.128.4.9    *node04*
----

Clean the environment:
[subs=+quotes]
----
$ *oc delete project scaleuptest*
----

In case the checks are mandatory before adding the host to the cluster, the labels can be set to avoid the default node-selector, run the checks then relabel the node:

[subs=+quotes]
----
... [OUTPUT ABBREVIATED] ...
[new_nodes]
*node04.example.com openshift_node_labels="{'role': 'test','test':'true'}"*
----

Perform the scale up procedure, run the required tests, then relabel the node:

[subs=+quotes]
----
$ *oc label node node04 "role=app" "zone=X" --overwrite*
node "node04" labeled
$ *oc label node node04 test-*
node "node04" labeled
----

// vim: set syntax=asciidoc: