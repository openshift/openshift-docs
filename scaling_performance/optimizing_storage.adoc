[[scaling-performance-optimizing-storage]]
= Optimizing Storage
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Optimizing Storage
== General Storage Guidance

The following is a matrix which details the persistent storage backend types {product-title} can be used with, and includes implementation examples.

=== Available Storage Options
[format="csv",width="70%",cols="3"]
[options="header]
|=======================
Storage Type,Description,Examples
"Block","This type of storage is presented to the operating system as a block device. It is suitable for applications that need full control of storage and operate at a low level on files bypassing the filesystem.  It is sometimes also referred to as Storage Area Network (SAN).  It is usually a non-shareable type of storage, which means that only one client at a time can mount an endpoint of this type.","CNS/CRS GlusterFS ^(1)^, iSCSI, Fibre Channel, Ceph RBD, OpenStack Cinder, AWS EBS ^(1)^, Dell/EMC Scale.IO, VMware vSphere Volume, GCE Persistent Disk ^(1)^, Azure Disk"
File,"This type of storage is presented to the operating system as a file system export to be mounted.  File Storage is sometimes also referred to as Network Attached Storage (NAS).  Concurrency, latency, file locking mechanisms and other capabilities will vary widely between protocols, implementations, vendors, and scales.","CNS/CRS GlusterFS ^(1)^, RHEL NFS, NetApp NFS ^(2)^, Azure File, Vendor NFS, Vendor GlusterFS ^(3)^, Azure File, AWS EFS"
Object,"This type of is storage is accessible via a REST API endpoint.  Object Storage is configurable for use in the {product-title} Registry.  Applications intending to use this storage must build their drivers into the application and/or container.","CNS/CRS GlusterFS ^(1)^, Ceph Object Storage (RADOS Gateway), OpenStack Swift, Aliyun OSS, AWS S3, Google Cloud Storage, Azure Blob Storage, Vendor S3 ^(3)^, Vendor Swift ^(3)^"
|=======================
NOTE: As of {product-title} 3.6.1, CNS GlusterFS (Container-Native Storage, a hyperconverged or cluster-hosted storage solution) and CRS GlusterFS (Container-Ready Storage, an externally hosted storage solution) provides interfaces for Block Storage, File Storage and Object Storage for the purpose of the {product-title} Registry, Logging, and Metrics.

NOTE: ^(1)^ CNS/CRS GlusterFS, Ceph RBD, OpenStack Cinder, AWS EBS, Azure Disk, GCE Persistent Disk, VMWare vSphere support Dynamic PV Provisioning natively in {product-title}.

NOTE: ^(2)^ NetApp NFS supports Dynamic PV Provisioning when using the Trident Plugin

WARNING: ^(3)^ Vendor GlusterFS, Vendor S3, and Vendor Swift supportability and configurability may vary.

=== Storage Backend Recommendations
Below is a matrix that summarizes which storage backend types are Recommended and Configurable for the given
{product-title} cluster application and useful attributes.
[format="csv",width="70%",cols="8"]
[options="header]
|=======================
Storage Type,ROX,RWX,Registry,Scaled Registry,Metrics,Logging,Apps
Block,Yes^(1)^,No,Configurable,Not Configurable,Recommended,Recommended,Recommended
File,Yes^(1)^,Yes,Configurable,Configurable,Configurable,Configurable,Recommended
Object,Yes,Yes,Recommended,Recommended,Not Configurable,Not Configurable,Not Configurable^(2)^
|=======================
NOTE: A Scaled Registry is a {product-title} Registry where 3 or more pod replicas are running.

NOTE: ^(1)^ This does not apply to Physical Disk, VM Physical Disk, VMDK, Loopback over NFS, AWS EBS, and Azure Disk.

NOTE: ^(2)^ Object Storage is not consumed via {product-title}'s PVs/PVCs.  Apps must integrate with the Object Storage REST API.

==== Specific Application & Storage Recommendations
===== Registry
In a non-scaled/HA {product-title} Registry cluster deployment, the preferred Storage Backend is Object Storage
followed by Block Storage.  The Storage Backend does not need to support ReadWriteMany Access Mode (RWX).  The Storage Backend *must
ensure Read-After-Write-Consistency*.  All Network Attached File Storage Backends (excluding CNS/CRS GlusterFS as it uses an Object Storage Interface) are expressly *not recommended* for {product-title} Registry cluster deployment with production workloads.  While hostPath volumes are configurable for a non-scaled/HA {product-title} Registry cluster deployment, they are not recommended.

WARNING: Corruption may occur when using NFS to back {production-title} Registry at production workloads.

===== Scaled Registry
In a scaled/HA {product-title} Registry cluster deployment, the preferred Storage Backend is Ojbect Storage.
The Storage Backend *must support ReadWriteMany* Access Mode (RWX) and *must ensure Read-After-Write-Consistency*.  File Storage and Block Storage are
expressly *not recommended* for a scaled/HA {product-title} Registry cluster deployment with production workloads.  All Network Attached File
Storage Backends (excluding CNS/CRS GlusterFS as it uses an Object Storage Interface) are expressly *not recommended* for {product-title} Registry cluster deployment with production workloads.

WARNING: Corruption may occur when using NFS to back {production-title} scaled/HA Registry at production workloads.

===== Metrics
In {product-title} Hosted Metrics cluster deployment, the preferred Storage Backend is Block Storage.  All Network Attached File
Storage Backends (excluding CNS/CRS GlusterFS as it uses a Block Storage Interface via iSCSI) are expressly *not recommended* for Hosted Metrics cluster deployment with production workloads.

WARNING: Corruption may occur when using NFS to back Hosted Metrics at production workloads.

===== Logging
In {product-title} Hosted Logging cluster deployment, the preferred Storage Backend is Block Storage.  All Network Attached File
Storage Backends (excluding CNS/CRS GlusterFS as it uses a Block Storage Interface via iSCSI) are expressly *not recommended* for Hosted Logging cluster deployment with production workloads.

WARNING: Corruption may occur when using NFS to back Hosted Logging at production workloads.

===== Apps
Application use cases vary from app to app.  Storage Backends which support Dynamic PV Provisioning, have low mount time latencies, and are not tied to nodes support a healthy cluster.  NFS does not gurantee Read-After-Write-Consistency and is not recommended for applications which require it.  Applications which depend on writing to the same, shared NFS export may experience issues at production workloads.

==== Other Specific Application Storage Recommendations
===== {product-title} Internal etcd
For the best etcd reliability, the lowest consistent latency Storage Backend is preferrable.

===== OpenStack Cinder
OpenStack Cinder tends to be adept in ReadOnlyMany (ROX) use cases.

===== Databases
Databases (RDBMSs, NoSQL DBs, etc.) tend to perform best with dedicated Block Storage.

== Choosing a Docker graph driver

Docker stores images and containers in a graph driver (a pluggable storage
backend), such as Device Mapper, Overlay, and Btrfs. Each have advantages and
disadvantages. For example, Overlay is faster than Device Mapper at starting and
stopping containers, but is not POSIX compliant because of the architectural
limitations of a union file system, and does not yet support SELinux.

For more information about Overlay, including supportability and usage caveats,
link:https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html-single/7.3_Release_Notes/index.html#technology_previews_file_systems[see
the Red Hat Enterprise Linux (RHEL) 7 Release Notes].

In production environments, using a LVM thin pool on top of regular block
devices (not loop devices) for container images and container root file system
storage is recommended.

[NOTE]
====
Using a Loop device back-end can affect performance issues. While you can still
continue to use it, Docker logs a warning message. For example:

----
devmapper: Usage of loopback devices is strongly discouraged for production use.
Please use `--storage-opt dm.thinpooldev` or use `man docker` to refer to
dm.thinpooldev section.
----
====

To ease Docker back-end storage configuration, use the
`docker-storage-setup` utility, which automates much of the configuration
details:

. If you had a separate disk drive dedicated to Docker storage (for example,
*_/dev/xvdb_*), add the following to the *_/etc/sysconfig/docker-storage-setup_*
file:
+
----
DEVS=/dev/xvdb
VG=docker_vg
----

. Restart the `docker-storage-setup` service:
+
----
# systemctl restart docker-storage-setup
----
+
After the restart, `docker-storage-setup` sets up a volume group named
`docker_vg` and creates a thin pool logical volume. Documentation for thin
provisioning on RHEL is available in the
link:https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html-single/Logical_Volume_Manager_Administration/index.html[LVM
Administrator Guide]. View the newly created volumes with the `lsblk` command:
+
----
# lsblk /dev/xvdb
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvdb 202:16 0 20G 0 disk
└─xvdb1 202:17 0 10G 0 part
  ├─docker_vg-docker--pool_tmeta 253:0 0 12M 0 lvm
  │ └─docker_vg-docker--pool 253:2 0 6.9G 0 lvm
  └─docker_vg-docker--pool_tdata 253:1 0 6.9G 0 lvm
  └─docker_vg-docker--pool 253:2 0 6.9G 0 lvm
----
+
[NOTE]
====
Thin-provisioned volumes are not mounted and have no file system (individual
containers do have an XFS file system), thus they will not show up in “df”
output.
====

. To verify that Docker is using a LVM thin pool, and to monitor disk space
utilization, use the `docker info` command. The `Pool Name` will correspond with
the `VG` you specified in *_/etc/sysconfig/docker-storage-setup_*:
+
----
# docker info | egrep -i 'storage|pool|space|filesystem'
Storage Driver: devicemapper
 Pool Name: docker_vg-docker--pool
 Pool Blocksize: 524.3 kB
 Backing Filesystem: xfs
 Data Space Used: 62.39 MB
 Data Space Total: 6.434 GB
 Data Space Available: 6.372 GB
 Metadata Space Used: 40.96 kB
 Metadata Space Total: 16.78 MB
 Metadata Space Available: 16.74 MB
----

By default, a thin pool is configured to use 40% of the underlying block device.
As you use the storage, LVM automatically extends the thin pool up to 100%. This
is why the `Data Space Total` value does not match the full size of the
underlying LVM device. This auto-extend technique was used to unify the storage
approach taken in both Red Hat Enterprise Linux and Red Hat Atomic Host, which
only uses a single partition.

In development, Docker in Red Hat distributions defaults to a
loopback mounted sparse file. To see if your system is using the loopback mode:

----
# docker info|grep loop0
 Data file: /dev/loop0
refarch-feedback@redhat.com 16 www.redhat.com
----

[IMPORTANT]
====
Red Hat strongly recommends using the Device Mapper storage driver in thin pool
mode for production workloads.
====

Overlay is also supported for Docker use cases as of Red Hat Enterprise Linux
7.2, and provides faster start up time and page cache sharing, which can
potentially improve density by reducing overall memory utilization.

[[benefits-of-using-the-overlay-graph-driver]]
== Benefits of Using the Overlay Graph Driver with SELinux

The default Docker storage configuration on Red Hat Enterprise Linux (RHEL)
continues to be Device Mapper. While the use of Overlay as the containers
storage back-end is being monitored, moving RHEL to Overlay as the default in
future releases is under consideration. As of RHEL 7.2, Overlay became a
supported graph driver. As of RHEL 7.4, SELinux and the Overlay2 graph driver
became a supported combination.

The main advantage of the Overlay file system is Linux page cache sharing among
containers sharing an image on the same node. This attribute of Overlay leads to
reduced input/output (I/O) during container startup (and, thus, faster container
startup time by several hundred milliseconds), as well as reduced memory usage
when similar images are running on a node. Both of these results are beneficial
in many environments, especially those with the goal of optimizing for density
and have high container churn rate (such as a build farm), or those that have
significant overlap in image content.

Page cache sharing is not possible with Device Mapper because thin-provisioned
devices are allocated on a per-container basis.
