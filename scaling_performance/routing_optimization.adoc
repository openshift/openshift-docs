[[scaling-performance-routing-optimization]]
= Routing Optimization
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

[[scaling-performance-scaling-router-haproxy]]
== Scaling {product-title} HAProxy Router

[[scaling-performance-baseline-router-haproxy]]
=== Baseline Performance

The {product-title}
xref:../install_config/router/index.adoc#install-config-router-overview[router]
is the ingress point for all external traffic destined for {product-title}
services.

When evaluating a single HAProxy router performance in terms of
HTTP requests handled per second, the performance varies depending
on many factors. In particular:

* HTTP keep-alive/close mode,

* xref:../architecture/networking/routes.adoc#route-types[route type]

* TLS session resumption client support

* number of concurrent connections per target route

* number of target routes 

* backend server page size

* underlying infrastructure (network/SDN solution, CPU, and so on)

While performance in your specific environment will vary, our lab
tests on a public cloud instance of size 4 vCPU/16GB RAM, a single
HAProxy router handling 100 routes terminated by backends serving
1kB static pages is able to handle the following.

In HTTP keep-alive mode scenarios:

[cols="2",options="header"]
|===
|*Encryption* |*HTTP(s) requests per second*
|none |22577
|edge |11642
|passthrough |33335
|re-encrypt |11521
|===

In HTTP close (no keep-alive) scenarios:

[cols="2",options="header"]
|===
|*Encryption* |*HTTP(s) requests per second*
|none |5771
|edge |1780
|passthrough |3488
|re-encrypt |1248
|===

TLS session resumption was used for encrypted routes. With HTTP
keep-alive, a single HAProxy router is capable of saturating 1 Gbit
NIC at page sizes as small as 8 kB.

When running on bare metal with modern processors, you can expect roughly
twice the performance of the public cloud instance above. This
overhead is introduced by the virtualization layer in place on public clouds and
holds mostly true for private cloud-based virtualization as well. The following
table is a guide on how many applications to use behind the router:

[cols="2,4",options="header"]
|===
|*Number of applications* |*Application type*
|5-10 |static file/web server or caching proxy
|100-1000 |applications generating dynamic content

|===

In general, HAProxy can saturate about 5-1000 applications, depending on the
technology in use. The number will typically be lower for applications serving
only static content.

xref:../architecture/networking/routes.adoc#router-sharding[Router sharding]
should be used to serve more routes towards applications and help horizontally
scale the routing tier.

[[scaling-performance-optimizing-router-haproxy]]
=== Performance Optimizations

[[scaling-performance-optimizing-router-haproxy-maxconn]]
==== Setting the Maximum Number of Connections

One of the most important tunable parameters for HAProxy scalability is the
`maxconn` parameter, which sets the maximum per-process number of concurrent
connections to a given number. Adjust this parameter by editing the
xref:../install_config/router/default_haproxy_router.adoc#concurrent-connections[`ROUTER_MAX_CONNECTIONS`]
environment variable in the {product-title} HAProxy router's deployment
configuration file.

[NOTE]
====
A connection includes the frontend and internal backend. This counts as two
connections. Be sure to set `ROUTER_MAX_CONNECTIONS` to double than the number
of connections you intend to create. 
====

[[scaling-performance-optimizing-router-haproxy-cpu-affinity]]
==== CPU and Interrupt Affinity

In {product-title}, the HAProxy router runs as a single process. The
{product-title} HAProxy router typically performs better on a system with fewer
but high frequency cores, rather than on an symmetric multiprocessing (SMP)
system with a high number of lower frequency cores.

Pinning the HAProxy process to one CPU core and the network interrupts to
another CPU core tends to increase network performance. Having processes and
interrupts on the same non-uniform memory access (NUMA) node helps avoid memory
accesses by ensuring a shared L3 cache. However, this level of control is
generally not possible on a public cloud environment. On bare metal hosts,
`irqbalance` automatically handles peripheral component interconnect (PCI)
locality and NUMA affinity for interrupt request lines (IRQs). On a cloud
environment, this level of information is generally not provided to the
operating system.

CPU pinning is performed either by `taskset` or by using HAProxy's `cpu-map`
parameter. This directive takes two arguments: the process ID and the CPU core
ID. For example, to pin HAProxy process `1` onto CPU core `0`, add the following
line to the global section of HAProxy's configuration file:

----
    cpu-map 1 0
----

To modify the HAProxy configuration file, refer to
xref:../install_config/router/customized_haproxy_router.adoc#install-config-router-customized-haproxy[Deploying
a Customized HAProxy Router].

[[scaling-performance-optimizing-router-haproxy-bufsize]]
==== Impacts of Buffer Increases

The {product-title} HAProxy router request buffer configuration limits the size
of headers in incoming requests and responses from applications. The HAProxy
parameter `tune.bufsize` can be increased to allow processing of larger headers
and to allow applications with very large cookies to work, such as those
accepted by load balancers provided by many public cloud providers. However,
this affects the total memory use, especially when large numbers of connections
are open. With very large numbers of open connections, the memory usage will be
nearly proportionate to the increase of this tunable parameter.

[[optimizations-for-haproxy-reloads]]
==== Optimizations for HAProxy Reloads

Long-lasting connections, such as WebSocket connections, combined with
long client/server HAProxy timeouts and short HAProxy
reload intervals, can cause instantiation of many HAProxy processes.
These processes must handle old connections, which were started
before the HAProxy configuration reload. A large number of these processes is
undesirable, as it will exert unnecessary load on the system and can
lead to issues, such as out of memory conditions.

Router environment variables affecting this
behavior are `ROUTER_DEFAULT_TUNNEL_TIMEOUT`, `ROUTER_DEFAULT_CLIENT_TIMEOUT`, 
`ROUTER_DEFAULT_SERVER_TIMEOUT`, and `RELOAD_INTERVAL` in particular.
