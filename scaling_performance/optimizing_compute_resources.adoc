[[scaling-performance-compute-resources]]
= Optimizing Compute Resources
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

[[scaling-performance-overcomitting]]
== Overcommitting

You can use overcommit procedures so that resources such as CPU and memory are
more accessible to the parts of your cluster that need them.

[IMPORTANT]
====
To avoid erratic cluster behavior due to scheduling collisions between the
hypervisor and Kubernetes, do not overcommit at the hypervisor level.
====

Note that when you overcommit, there is a risk that another application may not
have access to the resources it requires when it needs them, which will result
in reduced performance. However, this may be an acceptable trade-off in favor of
increased density and reduced costs. For example, development, quality assurance
(QA), or test environments may be overcommitted, whereas production might not be.

{product-title} implements resource management through the compute resource model and
quota system. See the documentation for more information about the
xref:../dev_guide/compute_resources.adoc#dev-guide-compute-resources[OpenShift resource model].

For more information and strategies for overcommitting, see the
xref:../admin_guide/overcommit.adoc#admin-guide-overcommit[Overcommitting documentation in the Cluster
Administration Guide].

[[scaling-performance-image-considerations]]
== Image Considerations

[[scaling-performance-predeployed-image]]
=== Using a Pre-deployed Image to Improve Efficiency

You can create a base {product-title} image with a number of tasks built-in to
improve efficiency, maintain configuration consistency on all node hosts, and
reduce repetitive tasks. This is known as a pre-deployed image.

For example, because every node requires the `ose-pod` image in order to run
pods, each node has to periodically connect to the container image registry in order to
pull the latest image. This can become problematic when you have 100 nodes
attempting this at the same time, and can lead to resource contention on the
image registry, waste of network bandwidth, and increased pod launch times.

To build a pre-deployed image:

* Create an instance of the type and size required.
* Ensure a dedicated storage device is available for CRI-O or Docker local image or container storage, separate from any persistent volumes for containers.
* Fully update the system, and ensure CRI-O or Docker is installed.
* Ensure the host has access to all yum repositories.
* link:https://access.redhat.com/documentation/en/red-hat-enterprise-linux-atomic-host/7/getting-started-with-containers/chapter-8-managing-storage-with-docker-formatted-containers[Set up thin-provisioned LVM storage].
* Pre-seed your commonly used images (such as the *rhel7* base image), as well as
{product-title} infrastructure container images (*ose-pod*, *ose-deployer*,
etc.) into your pre-deployed image.

Ensure that pre-deployed images are configured for any appropriate cluster
configurations, such as being able to run on
xref:../install_config/configuring_openstack.adoc#install-config-configuring-openstack[OpenStack],
or
xref:../install_config/configuring_aws.adoc#install-config-configuring-aws[AWS],
as well as any other cluster configurations.

[[scaling-performance-prepulling-images]]
=== Pre-pulling Images

To efficiently produce images, you can pre-pull any necessary container images
to all node hosts. This means the image does not have to be initially pulled,
which saves time and performance over slow connections, especially for images,
such as xref:../creating_images/s2i.adoc#creating-images-s2i[S2I], metrics, and logging, which can be large.

This is also useful for machines that cannot access the registry for security
purposes.

Alternatively, you can use a local image instead of the default of a specified registry. To do this:

. Pull from local images by setting the `imagePullPolicy` parameter of a pod configuration to `IfNotPresent` or `Never`.

. Ensure that all nodes in the cluster have the same images saved locally.

[NOTE]
====
Pulling from a local registry is suitable if you can control node configuration.
However, it will not work reliably on cloud providers that do not replace nodes
automatically, such as GCE. If you are running on Google Container Engine (GKE),
there will already be a *_.dockercfg_* file on each node with Google Container
Registry  credentials.
====

[[scaling-performance-debugging]]
== Debugging Using the RHEL Tools Container Image

Red Hat distributes a *rhel-tools* container image, packaging tools that aid in
debugging scaling or performance problems. This container image:

* Allows users to deploy minimal footprint container hosts by moving packages out of the base distribution and into this support container.
* Provides debugging capabilities for Red Hat Enterprise Linux 7 Atomic Host, which has an immutable package tree. *rhel-tools* includes utilities such as tcpdump, sosreport, git, gdb, perf, and many more common system administration utilities.

Use the *rhel-tools* container with the following:

[source,terminal]
----
# atomic run rhel7/rhel-tools
----

See the link:https://access.redhat.com/documentation/en/red-hat-enterprise-linux-atomic-host/7/getting-started-with-containers/chapter-11-using-the-atomic-tools-container-image[RHEL Tools Container documentation] for more information.

[[scaling-performance-debugging-using-ansible]]
== Debugging Using Ansible-based Health Checks

include::admin_guide/diagnostics_tool.adoc[tag=ansible-based-health-checks-intro]

See
xref:../admin_guide/diagnostics_tool.adoc#ansible-based-tooling-health-checks[Ansible-based Health Checks] in the Cluster Administration guide for information on the
available health checks and example usage.
