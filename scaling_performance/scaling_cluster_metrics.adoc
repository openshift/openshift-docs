[[scaling-performance-cluster-metrics]]
= Scaling Cluster Metrics
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:

toc::[]

== Overview

{product-title} exposes metrics that can be collected and stored in back-ends by
link:https://github.com/GoogleCloudPlatform/heapster[Heapster]. As an
{product-title} administrator, you can view containers and components metrics in
one user interface. These metrics are also used by
xref:../dev_guide/pod_autoscaling.adoc#dev-guide-pod-autoscaling[horizontal pod
autoscalers] in order to determine when and how to scale.

This topic provides information for scaling the metrics components.

[[metrics-recommendations-for-OCP-version-35]]
== Recommendations for {product-title} Version 3.5

* Run metrics pods on dedicated {product-title}
xref:../admin_guide/manage_nodes.adoc#infrastructure-nodes[infrastructure
nodes].
* Use persistent storage when configuring metrics. Set
`USE_PERSISTENT_STORAGE=true`.
* Keep the `METRICS_RESOLUTION=30` parameter in {product-title} metrics
deployments. Using a value lower than the default value of `30` for
`METRICS_RESOLUTION` is not recommended. When using the Ansible metrics
installation procedure, this is the `openshift_metrics_resolution` parameter.
* Closely monitor {product-title} nodes with host metrics pods to detect early
capacity shortages (CPU and memory) on the host system. These capacity shortages
can cause problems for metrics pods.
* In {product-title} version 3.5 testing, test cases up to 25,000 pods were
monitored in a {product-title} cluster.

[[scaling-performance-capacity-planning]]
== Capacity Planning for Cluster Metrics

In tests performed with with 210 and 990 {product-title} nodes, where 10500 pods
and 11000 pods were monitored respectively, the Cassandra database grew at the
speed shown in the table below:

.Cassandra Database storage requirements based on number of nodes/pods in the cluster
[options="header"]
|===
|Number of Nodes |Number of Pods |Cassandra Storage growth speed |Cassandra storage growth per day |Cassandra storage growth per week

|210
|10500
|500 MB per hour
|15 GB
|75 GB

|990
|11000
|1 GB per hour
|30 GB
|210 GB
|===

In the above calculation, approximately 20 percent of the expected size was added as
overhead to ensure that the storage requirements do not exceed calculated value.

If the `METRICS_DURATION` and `METRICS_RESOLUTION` values are kept at the
default (`7` days and `15` seconds respectively), it is safe to plan Cassandra
storage size requirements for week, as in the values above.

[WARNING]
====
Because {product-title} metrics uses the Cassandra database as a datastore for
metrics data, if `USE_PERSISTANT_STORAGE=true` is set during the metrics set up
process, `PV` will be on top in the network storage, with NFS as the default.
However, using network storage in combination with Cassandra is not recommended,
as per the
link:http://docs.datastax.com/en/archived/cassandra/1.2/cassandra/architecture/architecturePlanningAntiPatterns_c.html[Cassandra
documentation].
====


[[cluster-metrics-scaling-openshift-metrics-pods]]
== Scaling {product-title} Metrics Pods

One set of metrics pods (Cassandra/Hawkular/Heapster) is able to monitor at
least 25,000 pods.

[CAUTION]
====
Pay attention to system load on nodes where {product-title} metrics pods run.
Use that information to determine if it is necessary to scale out a number of
{product-title} metrics pods and spread the load across multiple {product-title}
nodes. Scaling {product-title} metrics heapster pods is not recommended.
====

[[cluster-metrics-scaling-pods-prereqs]]
=== Prerequisites

If persistent storage was used to deploy {product-title} metrics, then you must
xref:../dev_guide/persistent_volumes.adoc#dev-guide-persistent-volumes[create a
persistent volume (PV)] for the new Cassandra pod to use before you can scale
out the number of {product-title} metrics Cassandra pods. However, if Cassandra
was deployed with dynamically provisioned PVs, then this step is not necessary.

[[cluster-metrics-scaling-pods-cassandra]]
=== Scaling the Cassandra Components

Cassandra nodes use persistent storage. Therefore, scaling up or down is not
possible with replication controllers.

Scaling a Cassandra cluster requires modifying the
`openshift_metrics_cassandra_replicas` variable and re-running the
xref:../install_config/cluster_metrics.adoc#deploying-the-metrics-components[deployment].
By default, the Cassandra cluster is a single-node cluster.
ifdef::openshift-origin[]
To deploy more nodes, provision storage if `openshift_metrics_cassandra_replicas` equals `pv` and
increase the `openshift_metrics_cassandra_replicas` value.
endif::openshift-origin[]

ifdef::openshift-enterprise[]
To scale up the number of {product-title} metrics hawkular pods to two
replicas, run:

----
# oc scale -n openshift-infra --replicas=2 rc hawkular-metrics
----

Alternatively, update your inventory file and re-run the
xref:../install_config/cluster_metrics.adoc#deploying-the-metrics-components[deployment].
endif::openshift-enterprise[]

[NOTE]
====
If you add a new node to or remove an existing node from a Cassandra cluster,
the data stored in the cluster rebalances across the cluster.
====

ifdef::openshift-enterprise[]
[[cluster-metrics-horizontal-pod-autoscaling]]
== Horizontal Pod Autoscaling

{product-title} version 3.3 does not provide
xref:../dev_guide/pod_autoscaling.adoc#dev-guide-pod-autoscaling[Horizontal
Pod Autoscaling (HPA)] support for metrics pods and scaling metrics pods.
endif::[]

