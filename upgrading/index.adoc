[[install-config-upgrading-index]]
= Upgrade Methods and Strategies
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

[[upgrading-introduction]]
== Introduction to Upgrading Clusters

When new versions of {product-title} are released, you can upgrade your existing
cluster to apply the latest enhancements and bug fixes.
ifdef::openshift-origin[]
For OpenShift Origin, see the
https://github.com/openshift/origin/releases[Releases page] on GitHub to review
the latest changes.
endif::[]
ifdef::openshift-enterprise[]
This includes upgrading from previous minor versions, such as release 3.9 to
3.10, and applying asynchronous errata updates within a minor version (3.10.z
releases). See the xref:../release_notes/ocp_3_10_release_notes.adoc#release-notes-ocp-3-10-release-notes[{product-title} 3.10 Release Notes] to review the latest changes.

[NOTE]
====
Due to the xref:../release_notes/v2_vs_v3.adoc#release-notes-v2-vs-v3[core architectural changes]
between the major versions, OpenShift Enterprise 2 environments cannot be
upgraded to {product-title} 3 and require a fresh installation.
====
endif::[]

Unless noted otherwise, node and masters within a major version are forward and
backward compatible
xref:../upgrading/automated_upgrades.adoc#preparing-for-an-automated-upgrade[across
one minor version], so upgrading your cluster should go smoothly. However, you
should not run mismatched versions longer than necessary to upgrade the entire
cluster.

Before upgrading, ensure that all {product-title} services are running well.

In the event of a control plane upgrade failure, check the versions of your
masters to ensure that all versions are the same. If your masters are all the
same version, re-run the upgrade. If they differ, downgrade the masters to match
the lower versioned master, then re-run the upgrade.

[IMPORTANT]
====
// tag::37to39skipping38upgrade[]
The {product-title} 3.9 release includes a merge of features and fixes from
Kubernetes 1.8 and 1.9. As a result, the upgrade process from {product-title}
3.7 completes with the cluster fully upgraded to {product-title} 3.9, seemingly
"skipping" the 3.8 release. Technically, the {product-title} 3.7 cluster is
first upgraded to 3.8-versioned packages, and then the process immediately
continues upgrading to {product-title} 3.9 automatically. Your cluster should
only remain at 3.8-versioned packages for as long as it takes to successfully
complete the upgrade to {product-title} 3.9.
// end::37to39skipping38upgrade[]
====

The {product-title} upgrade process uses Ansible playbooks to automate the tasks
needed to upgrade your cluster. You must use the inventory file that you used
during initial installation or during the last time that the upgrade was
successful to run the upgrade playbook. Using this method allows you to choose
between either xref:install-config-upgrading-strategies[upgrade strategy]:
in-place upgrades or blue-green deployments.

[[install-config-upgrading-strategies]]
== Upgrade Strategies

There are two strategies you can take for performing the {product-title} cluster
upgrade: in-place upgrades or blue-green delpoyments.

[[install-config-upgrading-strategy-inplace]]
=== In-place Upgrades

With in-place upgrades, the cluster upgrade is performed on all hosts in a
single, running cluster: first masters and then nodes. Pods are evacuated off of
nodes and recreated on other running nodes before a node upgrade begins; this
helps reduce downtime of user applications.

Using the
xref:../install/configuring_inventory_file.adoc#configuring-ansible[inventory file]
from your latest cluster installation or upgrade, you can perform an automated


[[install-config-upgrading-strategy-bluegreen]]
=== Blue-green Deployments

The
xref:../upgrading/blue_green_deployments.adoc#upgrading-blue-green-deployments[blue-green deployment] upgrade method follows a similar flow to the in-place method:
masters and etcd servers are still upgraded first, however a parallel
environment is created for new nodes instead of upgrading them in-place.

This method allows administrators to switch traffic from the old set of nodes
(e.g., the "blue" deployment) to the new set (e.g., the "green" deployment)
after the new deployment has been verified. If a problem is detected, it is also
then easy to rollback to the old deployment quickly.
