:_mod-docs-content-type: ASSEMBLY
include::_attributes/common-attributes.adoc[]
[id="configuring-log-forwarding"]
= Configuring log forwarding
:context: configuring-log-forwarding

toc::[]

The `ClusterLogForwarder` (CLF) allows users to configure forwarding of logs to various destinations. It provides a flexible way to select log messages from different sources, send them through a pipeline that can transform or filter them, and forward them to one or more outputs.

.Key Functions of the ClusterLogForwarder
* Selects log messages using inputs
* Forwards logs to external destinations using outputs
* Filters, transforms, and drops log messages using filters
* Defines log forwarding pipelines connecting inputs, filters and outputs

include::modules/setting-up-log-collection.adoc[leveloffset=+1]

[id="modifying-log-level_{context}"]
== Modifying log level in collector

To modify the log level in the collector, you can set the `observability.openshift.io/log-level` annotation to `trace`, `debug`, `info`, `warn`, `error`, and `off`.

.Example log level annotation
[source,yaml]
----
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: collector
  annotations:
    observability.openshift.io/log-level: debug
# ...
----

[id="managing-the-operator_{context}"]
== Managing the Operator

The `ClusterLogForwarder` resource has a `managementState` field that controls whether the operator actively manages its resources or leaves them Unmanaged:

Managed:: (default) The operator will drive the logging resources to match the desired state in the CLF spec.

Unmanaged:: The operator will not take any action related to the logging components.

This allows administrators to temporarily pause log forwarding by setting `managementState` to `Unmanaged`.

[id="clf-structure_{context}"]
== Structure of the ClusterLogForwarder

The CLF has a `spec` section that contains the following key components:

Inputs:: Select log messages to be forwarded. Built-in input types `application`, `infrastructure` and `audit` forward logs from different parts of the cluster. You can also define custom inputs.

Outputs:: Define destinations to forward logs to. Each output has a unique name and type-specific configuration.

Pipelines:: Define the path logs take from inputs, through filters, to outputs. Pipelines have a unique name and consist of a list of input, output and filter names.

Filters:: Transform or drop log messages in the pipeline. Users can define filters that match certain log fields and drop or modify the messages. Filters are applied in the order specified in the pipeline.

[id="clf-inputs_{context}"]
=== Inputs

Inputs are configured in an array under `spec.inputs`. There are three built-in input types:

application:: Selects logs from all application containers, excluding those in infrastructure namespaces.

infrastructure:: Selects logs from nodes and from infrastructure components running in the following namespaces:
** `default`
** `kube`
** `openshift`   
** Containing the `kube-` or `openshift-` prefix

audit:: Selects logs from the OpenShift API server audit logs, Kubernetes API server audit logs, ovn audit logs, and node audit logs from auditd.

Users can define custom inputs of type `application` that select logs from specific namespaces or using pod labels.

[id="clf-outputs_{context}"]
=== Outputs

Outputs are configured in an array under `spec.outputs`. Each output must have a unique name and a type. Supported types are:

azureMonitor:: Forwards logs to Azure Monitor.
cloudwatch:: Forwards logs to AWS CloudWatch.
elasticsearch:: Forwards logs to an external Elasticsearch instance.
googleCloudLogging:: Forwards logs to Google Cloud Logging.
http:: Forwards logs to a generic HTTP endpoint.
kafka:: Forwards logs to a Kafka broker.
loki:: Forwards logs to a Loki logging backend.
lokistack:: Forwards logs to the logging supported combination of Loki and web proxy with {ocp-product-title} authentication integration. LokiStack's proxy uses {ocp-product-title} authentication to enforce multi-tenancy
otlp:: Forwards logs using the OpenTelemetry Protocol.
splunk:: Forwards logs to Splunk.
syslog:: Forwards logs to an external syslog server.

Each output type has its own configuration fields.

include::modules/configuring-otlp-output.adoc[leveloffset=+2]

[id="clf-pipelines_{context}"]
=== Pipelines

Pipelines are configured in an array under `spec.pipelines`. Each pipeline must have a unique name and consists of:

inputRefs:: Names of inputs whose logs should be forwarded to this pipeline.
outputRefs:: Names of outputs to send logs to.
filterRefs:: (optional) Names of filters to apply.

The order of filterRefs matters, as they are applied sequentially. Earlier filters can drop messages that will not be processed by later filters.

[id="clf-filters_{context}"]
=== Filters

Filters are configured in an array under `spec.filters`. They can match incoming log messages based on the value of structured fields and modify or drop them.

include::modules/cluster-logging-collector-log-forwarding-about.adoc[leveloffset=+1]

include::modules/logging-create-clf.adoc[leveloffset=+1]

include::modules/logging-delivery-tuning.adoc[leveloffset=+1]

include::modules/enabling-multi-line-exception-detection.adoc[leveloffset=+2]

include::modules/cluster-logging-collector-log-forward-gcp.adoc[leveloffset=+1]

[id="forwarding-logs-to-splunk_{context}"]
== Forwarding logs to Splunk

Splunk is a log aggregation service that has a well defined API to make use of its feature set. You can forward logs to Splunk from the `ClusterLogForwarder` Custom Resource (CR).

include::modules/logging-forward-splunk.adoc[leveloffset=+2]

include::modules/default-splunk-metadata-key-values.adoc[leveloffset=+2]

include::modules/logging-http-forward.adoc[leveloffset=+1]

include::modules/logging-forwarding-azure.adoc[leveloffset=+1]

include::modules/cluster-logging-collector-log-forward-project.adoc[leveloffset=+1]

include::modules/cluster-logging-collector-log-forward-logs-from-application-pods.adoc[leveloffset=+1]

include::modules/cluster-logging-collector-log-forward-syslog.adoc[leveloffset=+2]

[id="forwarding-logs-to-amazon-cloudwatch-from-sts-enabled-clusters_{context}"]
== Forwarding logs to Amazon CloudWatch from STS-enabled clusters

Amazon CloudWatch is a service that helps administrators observe and monitor resources and applications on {aws-first}. You can forward logs from {loggingproductname} to CloudWatch securely by leveraging {aws-short}'s Identity and Access Management (IAM) Roles for Service Accounts (IRSA), which uses {aws-short} {sts-first}.

The authentication with CloudWatch works as follows:

. The log collector requests temporary {aws-short} credentials from {sts-first} by presenting its service account token to the OpenID Connect (OIDC) provider in {aws-short}. 
. {aws-short} validates the token. Afterward, depending on the trust policy, {aws-short} issues short-lived, temporary credentials, including an access key ID, secret access key, and session token, for the log collector to use.

On {sts-short}-enabled clusters such as {product-rosa}, {aws-short} roles are pre-configured with the required trust policies. This allows service accounts to assume the roles. Therefore, you can create a secret for {aws-short} with {sts-short} that uses the IAM role. You can then create or update a `ClusterLogForwarder` custom resource (CR) that uses the secret to forward logs to CloudWatch output. Follow these procedures to create a secret and a `ClusterLogForwarder` CR if roles have been pre-configured:

////
* xref:../modules/cluster-logging-collector-log-forward-secret-cloudwatch.adoc#cluster-logging-collector-log-forward-secret-cloudwatch_configuring-log-forwarding[Creating a secret for CloudWatch with an existing {aws-short} role]

* xref:../modules/cluster-logging-collector-log-forward-sts-cloudwatch.adoc#cluster-logging-collector-log-forward-sts-cloudwatch_configuring-log-forwarding[Forwarding logs to Amazon CloudWatch from STS enabled clusters]
////

* Creating a secret for CloudWatch with an existing {aws-short} role

* Forwarding logs to Amazon CloudWatch from STS-enabled clusters

If you do not have an {aws-short} IAM role pre-configured with trust policies, you must first create the role with the required trust policies. Complete the following procedures to create a secret, `ClusterLogForwarder` CR, and role.

////
* xref:../modules/creating-an-aws-role.adoc#creating-an-aws-role_configuring-log-forwarding[Creating an AWS IAM role]
* xref:../modules/cluster-logging-collector-log-forward-secret-cloudwatch.adoc#cluster-logging-collector-log-forward-secret-cloudwatch_configuring-log-forwarding[Creating a secret for AWS CloudWatch with an existing AWS role]
* xref:../modules/cluster-logging-collector-log-forward-sts-cloudwatch.adoc#cluster-logging-collector-log-forward-sts-cloudwatch[Forwarding logs to Amazon CloudWatch from STS enabled clusters]
////
include::modules/creating-an-aws-role.adoc[leveloffset=+2]
include::modules/cluster-logging-collector-log-forward-secret-cloudwatch.adoc[leveloffset=+2]
include::modules/cluster-logging-collector-log-forward-sts-cloudwatch.adoc[leveloffset=+2]

include::modules/logging-content-filter-drop-records.adoc[leveloffset=+2]
include::modules/logging-audit-log-filtering.adoc[leveloffset=+2]
include::modules/input-spec-filter-labels-expressions.adoc[leveloffset=+2]
include::modules/logging-content-filter-prune-records.adoc[leveloffset=+2]
include::modules/input-spec-filter-audit-infrastructure.adoc[leveloffset=+1]
include::modules/input-spec-filter-namespace-container.adoc[leveloffset=+1]

