[[admin-guide-out-of-resource-handling]]
= Out of Resource Handling
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview

The node must preserve node stability when available compute resources are
low. This is especially important when dealing with incompressible resources
such as memory or disk. If either resource is exhausted, the node becomes
unstable.

[[out-of-resource-eviction-policy]]
== Eviction Policy

The node can proactively monitor for and prevent against total starvation of a
compute resource. In cases where this appears to occur, the node can
proactively fail one or more pods in order to reclaim the starved resource.
When the node fails a pod, it terminates all containers in the pod, and the
`*PodPhase*` is transitioned to *Failed*.

Platform administrators can configure eviction settings within the
*_node-config.yaml_* file.
xref:out-of-resource-schedulable-resources-and-eviction-policies[See an example
scenario].

[[out-of-resource-eviction-signals]]
=== Eviction Signals
The node can support the ability to trigger eviction decisions on the signals
described in the table below. The value of each signal is described in the
description column based on the node summary API.

.Supported Eviction Signals
[cols="2a,10a",options="header"]
|===

|Eviction Signal |Description

|`*memory.available*`
|`memory.available` = `node.status.capacity[memory]` - `node.stats.memory.workingSet`
|===

In future releases, the node will support the ability to trigger eviction
decisions based on disk pressure. Until then, use
xref:../admin_guide/garbage_collection.adoc#admin-guide-garbage-collection[container
and image garbage collection].

[[out-of-resource-eviction-thresholds]]
=== Eviction Thresholds

The node supports the ability to specify eviction thresholds that trigger the
node to reclaim resources.

Each threshold is of the following form:

----
<eviction_signal><operator><quantity>
----

* Valid `eviction-signal` tokens as defined in
xref:out-of-resource-eviction-signals[Eviction Signals].
* Valid `operator` tokens are `<`.
* Valid `quantity` tokens must match the quantity representation used by
Kubernetes.


[[out-of-resource-soft-eviction-thresholds]]
==== Soft Eviction Thresholds

A soft eviction threshold pairs an eviction threshold with a required
administrator-specified grace period. The node does not reclaim resources
associated with the eviction signal until that grace period is exceeded. If no
grace period is provided, the node errors on startup.

In addition, if a soft eviction threshold is met, an operator can specify a
maximum allowed pod termination grace period to use when evicting pods from the
node. If specified, the node uses the lesser value among the
`*pod.Spec.TerminationGracePeriodSeconds*` and the maximum-allowed grace period.
If not specified, the node kills pods immediately with no graceful termination.

To configure soft eviction thresholds, the following flags are supported:

* `eviction-soft` describes a set of eviction thresholds (for example,
`memory.available<1.5Gi`) that, if met over a corresponding grace period,
triggers a pod eviction.
* `eviction-soft-grace-period` describes a set of eviction grace periods (for
example, `memory.available=1m30s`) that correspond to how long a soft eviction
threshold must hold before triggering a pod eviction.
* `eviction-max-pod-grace-period` describes the maximum-allowed grace period (in
seconds) to use when terminating pods in response to a soft eviction threshold
being met.

[[out-of-resource-hard-eviction-thresholds]]
==== Hard Eviction Thresholds

A hard eviction threshold has no grace period and, if observed, the node takes
immediate action to reclaim the associated starved resource. If a hard eviction
threshold is met, the node kills the pod immediately with no graceful
termination.

To configure hard eviction thresholds, the following flag is supported:

* `eviction-hard` describes a set of eviction thresholds (for example,
`memory.available<1Gi`) that, if met, triggers a pod eviction.

[[out-of-resource-eviction-monitoring-interval]]
=== Eviction Monitoring Interval

The node evaluates eviction thresholds per its housekeeping interval. It
monitors every 10 seconds and the value can not be modified.

* `housekeeping-interval` is the interval between container housekeepings.

For more information about housekeeping, see the
link:https://github.com/google/cadvisor/blob/master/docs/runtime_options.md[cAdvisor
documentation].

[[out-of-resource-node-conditions]]
=== Node Conditions

The node can map one or more eviction signals to a corresponding node
condition.

If a hard eviction threshold is met, or a soft eviction threshold is met
independent of its associated grace period, the node reports a condition
indicating that the node is under pressure.

The following node conditions are defined that correspond to the specified
eviction signal.

.Node Conditions Related to Low Resources
[cols="2a,2a,8a",options="header"]
|===

|Node Condition |Eviction Signal |Description

|`*MemoryPressure*`
|`*memory.available*`
|Available memory on the node has satisfied an eviction threshold.
|===

The node continues to report node status updates at the frequency specified by
the `node-status-update-frequency` argument, which defaults to *10s*.

[[out-of-resource-oscillation-of-node-conditions]]
=== Oscillation of Node Conditions

If a node is oscillating above and below a soft eviction threshold, but not
exceeding its associated grace period, the corresponding node condition
constantly oscillates between *true* and *false*, which can cause poor
scheduling decisions.

To protect against this oscillation, the following flag is defined to control
how long the node must wait before transitioning out of a pressure condition.

* `eviction-pressure-transition-period` is the duration for which the node has
to wait before transitioning out of an eviction pressure condition.

The node ensures that it has not observed an eviction threshold being met for
the specified pressure condition for the period specified before toggling the
condition back to *false*.

[[out-of-resource-eviction-of-pods]]
=== Eviction of Pods

If an eviction threshold is met and the grace period is passed, the node
initiates the process of evicting pods until it observes the signal going below
its defined threshold.

The node ranks pods for eviction:

. By their xref:../admin_guide/overcommit.adoc#qos-classes[quality of service]
(`BestEffort`, `Burstable`, or `Guaranteed`), and
. Among those with the same quality of service by the consumption of the
starved compute resource relative to the pod's scheduling request.

* `BestEffort` pods that consume the most of the starved resource are failed
first.
* `Burstable` pods that consume the greatest amount of the starved resource
relative to their request for that resource are killed first. If no pod has
exceeded its request, the strategy targets the largest consumer of the starved
resource.
* `Guaranteed` pods that consume the greatest amount of the starved resource
relative to their request are killed first. If no pod has exceeded its request,
the strategy targets the largest consumer of the starved resource.

A `Guaranteed` pod is guaranteed to never be evicted because of another pod's
resource consumption.

If:

* A system daemon (for example, node, *docker*, *journald*, or others) is
consuming more resources than were reserved via *system-reserved* or
*kube-reserved* allocations, and
* The node only has `Guaranteed` pods remaining

Then:

The node must choose to evict a `Guaranteed` pod in order to preserve node
stability and  to limit the impact of the unexpected consumption to other
`Guaranteed` pods.

[[out-of-resource-scheduler]]
=== Scheduler

The node reports a condition when a compute resource is under pressure. The
scheduler views that condition as a signal to dissuade placing additional
pods on the node.

.Node Conditions and Scheduler Behavior
[cols="3a,8a",options="header"]
|===

|Node Condition |Scheduler Behavior

|`*MemoryPressure*`
|`BestEffort` pods are not scheduled to the node.
|===

[[out-of-resource-node-oom-behavior]]
== Node OOM Behavior

If the node experiences a system OOM (out of memory) event before it is able to
reclaim memory, the node depends on the
link:https://lwn.net/Articles/391222/[OOM killer] to respond.

The node sets a `*oom_score_adj*` value for each container based on the quality
of service for the pod.

.Quality of Service OOM Scores
[cols="3a,8a",options="header"]
|===

| Quality of Service |`*oom_score_adj*` Value

|`Guaranteed`
|-998

|`BestEffort`
|1000

|`Burstable`
|min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)
|===

If the node is unable to reclaim memory prior to experiencing a system OOM
event, the `*oom_killer*` calculates an `*oom_score*` based on the percentage of
memory a container is using on the node, and then add the `*oom_score_adj*` to get
an effective `*oom_score*` for the container, and then kills the container with
the highest score.

The intended behavior is that containers with the lowest quality of service that
are consuming the largest amount of memory relative to the scheduling request
are killed first in order to reclaim memory.

Unlike pod eviction, if a pod container is OOM killed, it may be restarted by
the node based on its `*RestartPolicy*`.

[[out-of-resource-best-practices]]
== Best Practices

[[out-of-resource-best-practice-disable-swap-memory]]
=== Disable Swap Memory

Failure to disable swap memory makes the node not recognize it is under
*MemoryPressure*.

To take advantage of memory based evictions, operators must
xref:../admin_guide/overcommit.adoc#disabling-swap-memory[disable swap].


[[out-of-resource-schedulable-resources-and-eviction-policies]]
=== Schedulable Resources and Eviction Policies

Consider the following scenario:

* Node memory capacity: *10Gi*.
* Operator wants to reserve 10 percent of memory capacity for system daemons
(kernel, node, etc.).
* Operator wants to evict pods at 95 percent memory utilization to reduce
thrashing and incidence of system OOM.

To facilitate this scenario, the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[node
configuration file] (the *_node-config.yaml_* file) is modified as follows:

====
----
kubeletArguments:
  eviction-hard: <1>
    - "memory.available<500Mi"
  system-reserved:
    - "1.5Gi"
----
<1> This threshold can either be `eviction-hard` or `eviction-soft`.
====

Implicit in this configuration is the understanding that *system-reserved*
should include the amount of memory covered by the eviction threshold.

To reach that capacity, either some pod is using more than its request, or the
system is using more than *1Gi*.

This configuration ensures that the scheduler does not place pods on a node that
immediately induce memory pressure and trigger eviction assuming those pods use
less than their configured request.

[[out-of-resource-best-practice-daemonset]]
=== DaemonSet

It is never desired for a node to evict a pod that was derived from
a `*DaemonSet*` since the pod will immediately be recreated and rescheduled
back to the same node.

At the moment, the node has no ability to distinguish a pod created from
`*DaemonSet*` versus any other object. If and when that information is
available, the node could proactively filter those pods from the candidate set
of pods provided to the eviction strategy.

In general, it is strongly recommended that `*DaemonSet*` not create
`BestEffort` pods to avoid being identified as a candidate pod for eviction.
Instead `*DaemonSet*` should ideally launch `Guaranteed` pods.
