[[admin-guide-cluster-autoscaler]]
= Managing the Cluster Auto Scaler
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

[[managing-autoscaler-overview]]
== Overview

This topic describes setting up the auto-scaler on your
{product-title} cluster. The auto-scaler's remit is to ensure your
pods have somewhere to run, and to also ensure there are no unneeded
nodes when demand drops.

Note: this feature is currently in tech-preview and the this guide
only applies to running the auto-scaler on AWS.

=== Auto Scale Groups

An Auto Scale Group (ASG) is a logical representation of a set of
machines. An ASG is configured with a minimum number of instances to
run, a maximum number of instances that can be run and also the
desired number of instances to run. An ASG starts by launching enough
instances to meet its desired capacity. ASGs may start with a minimum
of zero instances.

=== Launch Configurations

A Launch Configuration (LC) is a template that an ASG uses to launch
instances; you specify information such as the ID of the Amazon
Machine Image (AMI), the instance type (e.g., m4.large), a key pair,
one or more security groups, subnets, etc.

=== OpenShift "golden" Images

When the ASG provisions a new instance the image that is run must have
OpenShift already prepared so that it can be both automatically
bootstrapped and enrolled within the cluster without manual
intervention.

=== Operation

The auto-scaler periodically checks (e.g., every 30 seconds) to see
how many pods are pending node allocation. If there are nodes pending
allocation and the auto-scaler has not met its maximum capacity then
new nodes are continuously provisioned to accommodate the current
demand. And, equally, when demand drops the auto-scaler will remove
nodes, thereby saving you money.

== Installation

There are currently three ways to create a cluster that has auto
scaling capabilities.

1. The first way is to install the auto-scaler automatically using the
   AWS ansible playbooks when creating a cluster from scratch. This
   method is a complete hands-off automated install that will
   provisioning the cluster nodes and install OpenShift, that includes
   all configuration of the auto-scaler.

2. The second way is to retroactively add the auto-scaler to a cluster
   that was previously setup using method (1).

3. The third way is to manually create the ASG and LC, then manually
   deploy the auto-scaler components using the
   openshift-cluster-autoscaler ansible playbooks.

=== Method 1: Standing up a cluster from scratch using AWS playbooks

If you have the opportunity to setup a cluster from scratch then there
are ansible playbooks that will automate the deployment of both the
cluster, the deployment of the auto-scaler together with all the
associated auto scale groups (ASGs) and launch configurations (LCs),
in addition to automatically installing OpenShift

There are additional variables that need to be specified in both the
inventory file and also to be passed as extra variables when invoking
the ansible playbooks.

.OpenShift Inventory File for AWS
----
[OSEv3:children]
masters
nodes
etcd

[OSEv3:vars]
openshift_deployment_type=openshift-enterprise
ansible_ssh_user=root
openshift_clusterid=mycluster

[masters]
[etcd]
[nodes]
----

.OpenShift extra variables for AWS
[source,yaml]
----
openshift_cloudprovider_kind: aws
openshift_cloudprovider_aws_access_key: "{{ lookup('env','AWS_ACCESS_KEY_ID') }}"
openshift_cloudprovider_aws_secret_key: "{{ lookup('env','AWS_SECRET_ACCESS_KEY') }}"

openshift_aws_ami: ami-12345678	# this AMI needs to be a golden image
openshift_release: "3.11"

openshift_aws_clusterid: "{{ openshift_clusterid }}"

openshift_aws_create_iam_cert: True
openshift_aws_create_iam_role: True

openshift_aws_create_s3: False

# custom certificates are required for the ELB
openshift_aws_iam_cert_path: /path/to/wildcard.<clusterid>.example.com.crt
openshift_aws_iam_cert_key_path: /path/to/wildcard.<clusterid>.example.com.key
openshift_aws_iam_cert_chain_path: /path/to/cert.ca.crt

# We don't want to scale out beyond 1 master node
openshift_aws_master_group_max_size: 1
openshift_aws_master_group_min_size: 1
openshift_aws_master_group_desired_size: 1

# We don't want to scale out beyond 1 infra node
openshift_aws_infra_group_max_size: 1
openshift_aws_infra_group_min_size: 1
openshift_aws_infra_group_desired_size: 1

openshift_aws_compute_group_max_size: 10
openshift_aws_compute_group_min_size: 1
openshift_aws_compute_group_desired_size: 1

openshift_aws_instance_type: m4.large

openshift_aws_region: us-east-1
openshift_aws_ssh_key_name: keyname

openshift_cluster_autoscaler_install: True
openshift_deployment_type: openshift-enterprise

openshift_cluster_autoscaler_node_groups:
- name: "{{ openshift_aws_clusterid ~ ' compute group 1' }}"
  min: "{{ openshift_aws_compute_group_min_size }}"
  max: "{{ openshift_aws_compute_group_max_size }}"

# New nodes created by the ASG will be automatically approved
openshift_master_bootstrap_auto_approve: True

# Tag new nodes with a name
openshift_aws_autoname_scale_group_instances: True
----

==== Run the prerequisites playbook

This playbook will create VPCs, security groups, subnets, etc.

----
# ansible-playbook -i <inventory_file> \
ifdef::openshift-enterprise[]
    /usr/openshift-ansible/playbooks/aws/openshift-cluster/prerequisites.yml
endif::[]
ifdef::openshift-origin[]
    ~/openshift-ansible/playbooks/aws/openshift-cluster/prerequisites.yml
endif::[]
    -e @vars.yaml
----

==== Provision and Install

Once the prerequisites have run to completion we can now setup the
rest of the cluster. The `provision_install.yml` playbook will create
the master, infra and compute nodes, together with an ASG and LC. It
will also install OpenShift.

----
# ansible-playbook -i <inventory_file> \
ifdef::openshift-enterprise[]
    /usr/openshift-ansible/playbooks/aws/openshift-cluster/provision_install.yml \
endif::[]
ifdef::openshift-origin[]
    ~/openshift-ansible/playbooks/aws/openshift-cluster/provision_install.yml \
endif::[]
    -e @vars.yaml
----

Once this runs to completion the cluster will comprise of three nodes:

[source,bash]
----
$ oc get nodes
NAME                            STATUS    ROLES     AGE       VERSION
ip-172-31-49-172.ec2.internal   Ready     infra     1d        v1.11.0+d4cacc0
ip-172-31-55-89.ec2.internal    Ready     compute   8h        v1.11.0+d4cacc0
ip-172-31-63-234.ec2.internal   Ready     master    1d        v1.11.0+d4cacc0
----

At this point we have the minimum number of compute nodes as
specified, together with the cluster-autoscaler deployment already
running:

[source,bash]
----
$ oc get deployment -n openshift-autoscaler
NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
cluster-autoscaler   1         1         1            1           1d
----

=== Method 2: Retroactively adding the cluster auto-scaler using AWS playbooks

This is really just running all the steps in Method (1). If you have
previously used the AWS ansible playbooks to setup and manage the
cluster you may have done so without specifying
`openshift_cluster_autoscaler_install: True`. If so, the auto-scaler
wouldn't have been deployed.

If you want to add it to an existing AWS cluster then run all the
steps defined in Method (1).

=== Method 3: Manual deployment

To manually add the auto-scaler onto an existing cluster we need to
first create an ASG and a LC before we can run the playbooks that
deploy the auto-scaler components. The LC requires a pre-existing
"golden" image. If you already have such an image you can skip the
following section.

===== Generating a "golden" image

The following steps will create a "golden" image from an existing
compute node that is currently running in the cluster:

1. Clone an existing compute node image
2. Launch a temporary instance based on the cloned image in (1)
3. Remove the node's identity in the temporary instance
4. Create a new "golden" image, having "purified" it in (3)

.Save some state
[source,bash]
----
$ TIMESTAMP=$(date --utc +%FT%T.%3NZ | sed 's/:/-/g')
$ CLUSTER_ID=mycluster
----

.Clone an existing compute node image

[source,bash]
----
# i-0cbf60ad3a93777ef is our existing compute node.

$ aws ec2 create-image \
      --instance-id i-0cbf60ad3a93777ef \
      --name "${CLUSTER_ID}-base-image-${TIMESTAMP}"
{
    "ImageId": "ami-0e1768c1e1329ce9f"
}
----

.Launch an instance based on the newly cloned image

[source,bash]
----
$ aws ec2 describe-instances --instance-id i-0cbf60ad3a93777ef | grep GroupId
      "GroupId": "sg-7e73221a"

$ aws ec2 describe-instances --instance-id i-0cbf60ad3a93777ef | grep Subnet
      "SubnetId": "subnet-cf57c596",

# ami-0e1768c1e1329ce9f is the AMI we just created.

$ aws ec2 run-instances \
      --image-id ami-0e1768c1e1329ce9f \
      --count 1 \
      --instance-type t2.micro \
      --key-name libra \
      --security-group-ids sg-7e73221a \
      --subnet-id subnet-cf57c596 \
      --tag-specifications "ResourceType=instance,Tags=[{Key=Name,Value=$CLUSTER_ID-temp-instance-$TIMESTAMP}]"
----

.Remove the node's identity

[source,bash]
----
# Find the IP address of the instance we just ran in the EC2 console.
$ ssh ec2-54-174-243-105.compute-1.amazonaws.com

# Remove the node's identity
$ sudo bash
# rm -rf /etc/origin/node/certificates /etc/origin/openvswitch/*
----

.Create a new image, having "purified" it

[source,bash]
----
# i-012b060e213821a70 is the Instance ID of our temporary instance.

$ aws ec2 create-image \
    --instance-id i-012b060e213821a70 \
    --name "${CLUSTER_ID}-ASG-golden-image-${TIMESTAMP}"
{
    "ImageId": "ami-01785f295539e6441"
}
----

===== Manually creating the LC and ASG

To create the LC we need the AMI of our golden image. This is either
an existing pre-prepared "golden" image or the AMI
(`ami-01785f295539e6441`) we created in the previous section.

[source,bash]
----
$ aws autoscaling create-launch-configuration
      --launch-configuration-name ${CLUSTER_ID}-LC \
      --image-id ami-01785f295539e6441 \
      --instance-type m4.large \
      --key-name libra
----

[source,bash]
----
$ aws autoscaling create-auto-scaling-group \
      --auto-scaling-group-name ${CLUSTER_ID}-ASG \
      --launch-configuration-name ${CLUSTER_ID}-LC \
      --min-size 0 \
      --max-size 6 \
      --vpc-zone-identifier subnet-cf57c596 \
      --tags ResourceId=${CLUSTER_ID}-ASG,ResourceType=auto-scaling-group,Key=Name,Value=${CLUSTER_ID}-ASG-node,PropagateAtLaunch=true ResourceId=${CLUSTER_ID}-ASG,ResourceType=auto-scaling-group,Key=kubernetes.io/cluster/${CLUSTER_ID},Value=true,PropagateAtLaunch=true
----

==== Deploying the auto-scaler components

Once the cloud-provider specific components have been created (i.e.,
the ASG and LC) we can run the ansible playbook to automatically
deploy the remaining cluster auto-scaler components (e.g., Service
Account, RBAC roles, the cluster-autoscaler deployment itself, etc)
but we need to specify some additional configuration:

.OpenShift extra variables for the openshift-cluster-autoscaler playbook
----
# We need to specify the ASG by name so the deployment can reference it
openshift_cluster_autoscaler_node_groups:
- min: 0
  max: 6
  name: mycluster-ASG

# We want new nodes to be automatically approved
openshift_master_bootstrap_auto_approve: true

# We want to install the cluster-autoscaler components
openshift_cluster_autoscaler_install: True
----

Now run the openshift-cluster-autoscaler playbook:

----
# ansible-playbook -i <inventory_file> \
ifdef::openshift-enterprise[]
    /usr/share/openshift-ansible/playbooks/openshift-cluster-autoscaler/config.yml \
endif::[]
ifdef::openshift-origin[]
    ~/openshift-ansible/playbooks/openshift-cluster-autoscaler/config.yml \
endif::[]
    -e @asg-vars.yaml
----

When this completes verify that there is a cluster-autoscaler deployment:

[source,bash]
----
$ oc get deployment
NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
cluster-autoscaler   1         1         1            1           23h
----

==== Cleaning up the ASG and LC

When you no longer need auto-scaling capability you can delete the
deployment on the cluster:

[source,bash]
----
$ oc delete deployment cluster-autoscaler -n openshift-autoscaler
----

And the AWS resources can be deleted as long as there are no
outstanding nodes provisioned through the ASG:

[source,bash]
----
$ aws autoscaling delete-auto-scaling-group \
      --auto-scaling-group-name ${CLUSTER_ID}-ASG

$ aws autoscaling delete-launch-configuration \
      --launch-configuration-name ${CLUSTER_ID}-LC
----

== Testing that auto-scaling is working

.Deployment configuration to test auto-scaling works
[source,yaml]
----
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: scale-up
  labels:
    app: scale-up
spec:
  replicas: 20
  selector:
    matchLabels:
      app: scale-up
  template:
    metadata:
      labels:
        app: scale-up
    spec:
      containers:
      - name: busybox
        image: docker.io/library/busybox
        resources:
          requests:
            memory: 2Gi
        command:
        - /bin/sh
        - "-c"
        - "echo 'this should be in the logs' && sleep 86400"
      terminationGracePeriodSeconds: 0
----

This deployment calls for 20 replicas but the initial size of the
cluster won't be able to run all of the pods without first increasing
the number of compute nodes. Note, it is imperative that the
deployment runs in the same namespace that the cluster-autoscaler
deployment is running in:

[source,bash]
----
$ oc apply -n openshift-autoscaler -f scale-up.yaml
----

Some pods will now be in the pending state as they have nowhere to
run:

[source,bash]
----
$ oc get pods -n openshift-autoscaler | grep Running
cluster-autoscaler-5485644d46-ggvn5   1/1       Running   0          1d
scale-up-79684ff956-45sbg             1/1       Running   0          31s
scale-up-79684ff956-4kzjv             1/1       Running   0          31s
scale-up-79684ff956-859d2             1/1       Running   0          31s
scale-up-79684ff956-h47gv             1/1       Running   0          31s
scale-up-79684ff956-htjth             1/1       Running   0          31s
scale-up-79684ff956-m996k             1/1       Running   0          31s
scale-up-79684ff956-pvvrm             1/1       Running   0          31s
scale-up-79684ff956-qs9pp             1/1       Running   0          31s
scale-up-79684ff956-zwdpr             1/1       Running   0          31s

$ oc get pods -n openshift-autoscaler | grep Pending
scale-up-79684ff956-5jdnj             0/1       Pending   0          40s
scale-up-79684ff956-794d6             0/1       Pending   0          40s
scale-up-79684ff956-7rlm2             0/1       Pending   0          40s
scale-up-79684ff956-9m2jc             0/1       Pending   0          40s
scale-up-79684ff956-9m5fn             0/1       Pending   0          40s
scale-up-79684ff956-fr62m             0/1       Pending   0          40s
scale-up-79684ff956-q255w             0/1       Pending   0          40s
scale-up-79684ff956-qc2cn             0/1       Pending   0          40s
scale-up-79684ff956-qjn7z             0/1       Pending   0          40s
scale-up-79684ff956-tdmqt             0/1       Pending   0          40s
scale-up-79684ff956-xnjhw             0/1       Pending   0          40s
----

In the background, and automatically, the cluster auto-scaler will be
provisioning new compute nodes. It can be up to 5-7 minutes before the
node is provisioned and shows as `Ready` within the cluster.

[source,bash]
----
$ oc get nodes
NAME                            STATUS    ROLES     AGE       VERSION
ip-172-31-49-172.ec2.internal   Ready     infra     1d        v1.11.0+d4cacc0
ip-172-31-53-217.ec2.internal   Ready     compute   7m        v1.11.0+d4cacc0
ip-172-31-55-89.ec2.internal    Ready     compute   9h        v1.11.0+d4cacc0
ip-172-31-56-21.ec2.internal    Ready     compute   7m        v1.11.0+d4cacc0
ip-172-31-56-71.ec2.internal    Ready     compute   7m        v1.11.0+d4cacc0
ip-172-31-63-234.ec2.internal   Ready     master    1d        v1.11.0+d4cacc0
----

Once the new nodes have been provisioned the pending pods will be
allocated to a node and start running:

[source,bash]
----
$ oc get pods -n openshift-autoscaler
NAME                                  READY     STATUS    RESTARTS   AGE
cluster-autoscaler-5485644d46-ggvn5   1/1       Running   0          1d
scale-up-79684ff956-45sbg             1/1       Running   0          8m
scale-up-79684ff956-4kzjv             1/1       Running   0          8m
scale-up-79684ff956-5jdnj             1/1       Running   0          8m
scale-up-79684ff956-794d6             1/1       Running   0          8m
scale-up-79684ff956-7rlm2             1/1       Running   0          8m
scale-up-79684ff956-859d2             1/1       Running   0          8m
scale-up-79684ff956-9m2jc             1/1       Running   0          8m
scale-up-79684ff956-9m5fn             1/1       Running   0          8m
scale-up-79684ff956-fr62m             1/1       Running   0          8m
scale-up-79684ff956-h47gv             1/1       Running   0          8m
scale-up-79684ff956-htjth             1/1       Running   0          8m
scale-up-79684ff956-m996k             1/1       Running   0          8m
scale-up-79684ff956-pvvrm             1/1       Running   0          8m
scale-up-79684ff956-q255w             1/1       Running   0          8m
scale-up-79684ff956-qc2cn             1/1       Running   0          8m
scale-up-79684ff956-qjn7z             1/1       Running   0          8m
scale-up-79684ff956-qs9pp             1/1       Running   0          8m
scale-up-79684ff956-tdmqt             1/1       Running   0          8m
scale-up-79684ff956-xnjhw             1/1       Running   0          8m
scale-up-79684ff956-zwdpr             1/1       Running   0          8m
...
----
