= Aggregating Container Logs
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

ifdef::openshift-origin[]
As an OpenShift administrator, you may want to view the logs from all containers
in one user interface. There are two options for aggregating container logs,
depending on user requirements:

. link:#using-a-centralized-file-system[Using a centralized file system]
. link:#using-elasticsearch[Using *Elasticsearch*]

[IMPORTANT]
====
These solutions are a work in progress. As packaging improvements are made,
these instructions will be simplified.
====
endif::[]
ifdef::openshift-enterprise[]
As an OpenShift administrator, you may want to view the logs from all containers
in one user interface. The currently supported method for aggregating container
logs in OpenShift Enterprise is link:#using-a-centralized-file-system[using a
centralized file system]. Additional supported methods are planned for inclusion
in future releases.

[NOTE]
====
As packaging improvements are made, these instructions will be simplified.
====
endif::[]

[[using-a-centralized-file-system]]

== Using a Centralized File System

This method reads all container logs and forwards them to a central server for
storage on the file system.
ifdef::openshift-origin[]
This solution requires less resources and requires less management than the
link:#using-elasticsearch[*Elasticsearch* option], but the logs are not indexed
and searchable.
endif::[]

[[installing-fluentd-td-agent-on-nodes]]
=== Installing fluentd (td-agent) on Nodes

Perform the following steps on each node to install and configure *fluentd*
(*td-agent*):

. Run the following commands:
+
====
----
# export RPM=td-agent-2.2.0-0.x86_64.rpm
# curl https://packages.treasuredata.com/2/redhat/7/x86_64/$RPM \
    -o /tmp/$RPM
# yum localinstall /tmp/$RPM
# /opt/td-agent/embedded/bin/gem install fluent-plugin-kubernetes
# mkdir -p /etc/td-agent/config.d
# chown td-agent:td-agent /etc/td-agent/config.d
----
====

. To allow *td-agent* access to the containers logs, create the
*_/etc/sysconfig/td-agent_* file with the following contents:
+
====
----
DAEMON_ARGS=
TD_AGENT_ARGS="/usr/sbin/td-agent --log /var/log/td-agent/td-agent.log --use-v1-config"
----
====

. Add the following line to the *_/etc/td-agent/td-agent.conf_* file:
+
====
----
  @include config.d/*.conf
----
====

. Create the *_/etc/td-agent/config.d/kubernetes.conf_* file with the following
contents:
+
====

[source,xml]
----
    <source>
      type tail
      path /var/lib/docker/containers/*/*-json.log
      pos_file /var/log/td-agent/tmp/fluentd-docker.pos
      time_format %Y-%m-%dT%H:%M:%S
      tag docker.*
      format json
      read_from_head true
    </source>

    <match docker.var.lib.docker.containers.*.*.log>
      type kubernetes
      container_id ${tag_parts[5]}
      tag docker.${name}
    </match>

    <match kubernetes>
      type copy
      <store>
        type forward
        send_timeout 60s
        recover_wait 10s
        heartbeat_interval 1s
        phi_threshold 16
        hard_timeout 60s
        log_level trace
        require_ack_response true
        heartbeat_type tcp
        <server>
          name logging_name <1>
          host host_name <2>
          port 24224
          weight 60
        </server>

        <secondary>
          type file
          path /var/log/td-agent/forward-failed
        </secondary>
      </store>

      <store>
        type file
        path /var/log/td-agent/containers.log
        time_slice_format %Y%m%d
        time_slice_wait 10m
        time_format %Y%m%dT%H%M%S%z
        compress gzip
        utc
      </store>
    </match>
----
<1> The name for the master that will be used during logging.
<2> The IP or a DNS resolvable name used to access the master.
====

. Enable *fluentd*:

====
----
# systemctl enable td-agent
# systemctl start td-agent
----
====

[TIP]
====
Any errors are logged in the *_/var/log/td-agent/td-agent.log_* file.
====

[[optional-method-to-verify-working-nodes]]

=== Optional Method to Verify Working Nodes

You can optionally set up the master to be the aggregator to test and verify
that the nodes are working properly.

. Install *fluentd* (*td-agent*) on the master:
+
====
----
# export RPM=td-agent-2.2.0-0.x86_64.rpm
# curl https://packages.treasuredata.com/2/redhat/7/x86_64/$RPM \
    -o /tmp/$RPM
# yum localinstall $RPM
# mkdir -p /etc/td-agent/config.d
# chown td-agent:td-agent /etc/td-agent/config.d
----
====

. Ensure port *24224* is open on the master's firewall to allow the nodes
access.

. Configure *fluentd* to aggregate container logs by adding the following line
to the *_/etc/td-agent/td-agent.conf_* file:
+
====
----
  @include config.d/*.conf
----
====

. Create the *_/etc/td-agent/config.d/kubernetes.conf_* file with the following
contents:
+
====
----
    <match kubernetes.**>
        type file
        path /var/log/td-agent/containers.log
        time_slice_format %Y%m%d
        time_slice_wait 10m
        time_format %Y%m%dT%H%M%S%z
        compress gzip
        utc
    </match>
----
====

. Enable *fluentd*:
+
====
----
# systemctl enable td-agent
# systemctl start td-agent
----
====
+
[TIP]
====
Any errors are logged in the *_/var/log/td-agent/td-agent.log_* file.
====

You should now find all the containers' logs available on the master in the
*_/var/log/td-agent/containers.log_* file.

ifdef::openshift-origin[]

[[using-elasticsearch]]

== Using Elasticsearch

https://www.elastic.co/products/elasticsearch[*Elasticsearch*] is an open source
distributed document database that indexes documents and provides full-text
search capabilities. By storing container logs in *Elasticsearch*, users are
able to search all content and filter appropriately. This documentation shows
how to run https://www.elastic.co/products/kibana[*Kibana*].

This method requires more configuration and more resources than the
link:#using-a-centralized-file-system[centralized file system option], but makes
logs more useful for troubleshooting and fault finding.

Enabling aggregated logging to *Elasticsearch* involves:

. link:#creating-an-elasticsearch-cluster[Creating an *Elasticsearch* cluster]
. link:#creating-logging-pods[Creating logging pods]
. link:#creating-the-kibana-service[Creating the *Kibana* service]


[NOTE]
====
The following directions assume everything is being created in the *default*
project, but should also work for arbitrary projects with minor adjustments.
====

[[creating-an-elasticsearch-cluster]]

=== Creating an Elasticsearch Cluster

Logs are stored in an *Elasticsearch* cluster running on OpenShift. This cluster
is scalable using a
link:../architecture/core_concepts/deployments.html#replication-controllers[replication
controller], so you can link:../dev_guide/deployments.html#scaling[scale] the
*Elasticsearch* cluster up and down as required.

You will need a privileged link:service_accounts.html[service account] to launch
the current *ElasticSearch* image, as it runs as root (which should be corrected
in time). First, create a file with the following contents:

====

[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: es-deploy
----
====

Create the object using the new file:

====
----
$ oc create -f path/to/serviceaccount.yaml
----
====

Edit the *privileged* link:manage_scc.html[security context constraint] (SCC).
This must be done as a user with
link:../architecture/additional_concepts/authorization.html#roles[*cluster-admin*
credentials]:

====
----
$ oc edit scc/privileged
----
====

Add the new service account to the `*users*` list at the end of the SCC object
(*default* is the project name):

====
----
users:
...
- system:serviceaccount:default:es-deploy
----
====

Then save and exit. This service account now has access to deploy privileged
pods.

To create the *ElasticSearch* cluster, first create a file with the following
contents:

====

[source,yaml]
----
apiVersion: "v1"
kind: "List"
items:
-
  apiVersion: "v1"
  kind: "Service"
  metadata:
    labels:
      provider: "fabric8"
      component: "elasticsearch"
    name: "es-logging"
  spec:
    ports:
    -
      port: 9200
      targetPort: 9200
    selector:
      provider: "fabric8"
      component: "elasticsearch"
-
  apiVersion: "v1"
  kind: "Service"
  metadata:
    labels:
      provider: "fabric8"
      component: "elasticsearch"
    name: "es-logging-cluster"
  spec:
    portalIP: "None"
    ports:
    -
      port: 9300
      targetPort: 9300
    selector:
      provider: "fabric8"
      component: "elasticsearch"
-
  apiVersion: "v1"
  kind: "ReplicationController"
  metadata:
    labels:
      provider: "fabric8"
      component: "elasticsearch"
    name: "elasticsearch"
  spec:
    replicas: 1
    selector:
      provider: "fabric8"
      component: "elasticsearch"
    template:
      metadata:
        labels:
          provider: "fabric8"
          component: "elasticsearch"
      spec:
        serviceAccount: es-deploy
        containers:
          -
            securityContext:
              runAsUser: 0
            env:
            -
              name: "KUBERNETES_TRUST_CERT"
              value: "true"
            -
              name: "SERVICE_DNS"
              value: "es-logging-cluster"
            image: "fabric8/elasticsearch-k8s:1.5.2"
            name: "elasticsearch"
            ports:
            -
              containerPort: 9200
              name: "http"
            -
              containerPort: 9300
              name: "transport"
----
====

Create the objects using the new file:

====
----
$ oc create -f path/to/elasticsearch.yaml
----
====

This starts a single *Elasticsearch* instance. If you need to create a larger
cluster, you can scale the *Elasticsearch* replication controller using:

====
----
$ oc scale --replicas=3 rc elasticsearch
----
====

[[creating-logging-pods]]

=== Creating Logging Pods

To read the container logs, a static pod must be deployed on each node. To do
this, you must first ensure that the node is configured to read local pod
manifest configuration files. This is enabled by configuring the
`*podManifestConfig*` in the *_node-config.yaml_* file on each node, changing
the configuration path and check interval appropriately:

====

[source,yaml]
----
podManifestConfig:
  path: openshift.local.manifests
  fileCheckIntervalSeconds: 10
----
====

[NOTE]
====
If you are running OpenShift as an all-in-one with the `openshift start` command
(either directly or using a *systemd* unit), node configuration is overwritten
at each restart. You must use the following to write the master and node
configuration:

----
$ openshift start --write-config=<path-to-config-dir> <parameters>
----

Then modify your server command line to look like:

----
    openshift start --master-config=/<config-dir>/master/master-config.yaml \
                    --node-config=/<config-dir>/<node-dir>/node-config.yaml
----
====

To create the logging pod, create a file with the following contents in the
directory specified by `*podManifestConfig.path*` above (if relative as defined
above, then it is relative to the node configuration directory):

====

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: fluentd-elasticsearch
spec:
  containers:
  - name: fluentd-elasticsearch
    image: fabric8/fluentd-kubernetes:1.0
    securityContext:
      privileged: true
    resources:
      limits:
        cpu: 100m
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: varlibdockercontainers
      mountPath: /var/lib/docker/containers
      readOnly: true
    env:
    - name: "ES_HOST"
      value: "es-logging"
    - name: "ES_PORT"
      value: "9200"
  volumes:
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers
----
====

This starts a pod on the node and posts the container logs to *Elasticsearch*.

To validate it is working, you can query *Elasticsearch* and check that the data
is correctly being persisted. First, identify the *Elasticsearch* service:

----
$ oc get service -l component=elasticsearch
----

Then query *Elasticsearch*, replacing the service IP with one returned from the
above command for `es-logging`:

----
$ curl -s <service_ip>:9200/_cat/indices?v
----

You should see output similar to the following:

====
----
health status index               pri rep docs.count docs.deleted store.size pri.store.size
yellow open   logstash-2015.06.05   5   1        540            0      251kb          251kb
----
====

If the value for `docs.count` is more than 0, then log records are being
correctly sent to *Elasticsearch*.

If not, it is usually because the *fluentd* container cannot reach the
*ElasticSearch* service. There may be a bug currently that causes name
resolution to fail. Check `oc logs` for the *fluentd* pod. The log may report
something like:

====
----
    temporarily failed to flush the buffer.  [...]
        error="Can not reach Elasticsearch cluster ({:host=>\"es-logging\", :port=>9200, :scheme=>\"http\"})!
        getaddrinfo: Name does not resolve (SocketError)"
----
====

To work around this, you can modify the *fluentd* static pod definition to point
the `*ES_HOST*` variable at the IP for the *es-logging* service instead of its
name. It should be redeployed within 10 seconds.

[[creating-the-kibana-service]]

=== Creating the Kibana Service

To create the *Kibana* service, first create a file with the following contents:

====

[source,yaml]
----
apiVersion: "v1"
kind: "List"
items:
-
  apiVersion: "v1"
  kind: "Service"
  metadata:
    name: "kibana"
  spec:
    ports:
      -
        port: 80
        targetPort: "kibana-port"
    selector:
      provider: fabric8
      component: "kibana"
-
  apiVersion: "v1"
  kind: "ReplicationController"
  metadata:
    name: "kibana"
    labels:
      provider: fabric8
      component: "kibana"
  spec:
    replicas: 1
    selector:
      component: "kibana"
    template:
      metadata:
        name: "kibana"
        labels:
          provider: fabric8
          component: "kibana"
      spec:
        containers:
          -
            name: "kibana"
            image: "fabric8/kibana4:4.1.0"
            ports:
              -
                name: "kibana-port"
                containerPort: 5601
            env:
              -
                name: "ELASTICSEARCH_URL"
                value: "http://es-logging:9200"
----
====

Create the *Kibana* replication controller and service:

====
----
$ oc create -f path/to/kibana.yaml
----
====

Optionally, to create a link:../architecture/core_concepts/routes.html[route] to
reach *Kibana* externally:

====
----
$ oc expose service/kibana --hostname=fqdn.example.com
----
====

When you first access *Kibana*, you must specify a default index; the suggested
default should work. For more information on using *Kibana*, see its
link:https://www.elastic.co/guide/en/kibana/current/index.html[User Guide].

endif::[]
