= Aggregating Container Logs
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

As an OpenShift administrator, you may want to view the logs from all containers
in one user interface. There are two options for aggregating container logs,
depending on user requirements:

. link:#using-a-centralized-file-system[Using a centralized file system]
. link:#using-elasticsearch[Using *Elasticsearch*]

[IMPORTANT]
====
These solutions are a work in progress. As packaging improvements are made these
instructions will be simplified.
====

[[using-a-centralized-file-system]]

== Using a Centralized File System

This option reads all container logs and forwards them to a central server for
storage on the file system. This solution requires less resources and requires
less management than the link:#using-elasticsearch[*Elasticsearch* option], but
the logs are not indexed and searchable.

[[installing-fluentd-td-agent-on-nodes]]
=== Installing fluentd (td-agent) on Nodes

Perform the following steps on each node to install and configure *fluentd*
(*td-agent*):

. Run the following commands:
+
====
----
# export RPM=td-agent-2.2.0-0.x86_64.rpm
# curl http://packages.treasuredata.com/2/redhat/7/x86_64/$RPM \
    -o /tmp/$RPM
# yum localinstall $RPM
# /opt/td-agent/embedded/bin/gem install fluent-plugin-kubernetes
# mkdir -p /etc/td-agent/config.d
# chown td-agent:td-agent /etc/td-agent/config.d
----
====

. To allow *td-agent* access to the containers logs, create the
*_/etc/sysconfig/td-agent_* file with the following contents:
+
====
----
DAEMON_ARGS=
TD_AGENT_ARGS="/usr/sbin/td-agent --log /var/log/td-agent/td-agent.log --use-v1-config"
----
====

. Add the following line to the *_/etc/td-agent/td-agent.conf_* file:
+
====
----
  @include config.d/*.conf
----
====

. Create the *_/etc/td-agent/config.d/kubernetes.conf_* file with the following
contents:
+
====

[source,xml]
----
    <source>
      type tail
      path /var/lib/docker/containers/*/*-json.log
      pos_file /var/log/td-agent/tmp/fluentd-docker.pos
      time_format %Y-%m-%dT%H:%M:%S
      tag docker.*
      format json
      read_from_head true
    </source>

    <match docker.var.lib.docker.containers.*.*.log>
      type kubernetes
      container_id ${tag_parts[5]}
      tag docker.${name}
    </match>

    <match kubernetes>
      type copy
      <store>
        type forward
        send_timeout 60s
        recover_wait 10s
        heartbeat_interval 1s
        phi_threshold 16
        hard_timeout 60s
        log_level trace
        require_ack_response true
        heartbeat_type tcp
        <server>
          name logging_name <1>
          host host_name <2>
          port 24224
          weight 60
        </server>

        <secondary>
          type file
          path /var/log/td-agent/forward-failed
        </secondary>
      </store>

      <store>
        type file
        path /var/log/td-agent/containers.log
        time_slice_format %Y%m%d
        time_slice_wait 10m
        time_format %Y%m%dT%H%M%S%z
        compress gzip
        utc
      </store>
    </match>
----
<1> The name for the master that will be used during logging.
<2> The IP or a DNS resolvable name used to access the master.
====

. Enable *fluentd*:

====
----
# systemctl enable td-agent
# systemctl start td-agent
----
====

[TIP]
====
Any errors are logged in the *_/var/log/td-agent/td-agent.log_* file.
====

[[optional-method-to-verify-working-nodes]]

=== Optional Method to Verify Working Nodes

You can optionally set up the master to be the aggregator to test and verify
that the nodes are working properly.

. Install *fluentd* (*td-agent*) on the master:
+
====
----
# export RPM=td-agent-2.2.0-0.x86_64.rpm
# curl http://packages.treasuredata.com/2/redhat/7/x86_64/$RPM \
    -o /tmp/$RPM
# yum localinstall $RPM
# mkdir -p /etc/td-agent/config.d
# chown td-agent:td-agent /etc/td-agent/config.d
----
====

. Ensure port *24224* is open on the master's firewall to allow the nodes
access.

. Configure *fluentd* to aggregate container logs by adding the following line
to the *_/etc/td-agent/td-agent.conf_* file:
+
====
----
  @include config.d/*.conf
----
====

. Create the *_/etc/td-agent/config.d/kubernetes.conf_* file with the following
contents:
+
====
----
    <match kubernetes.**>
        type file
        path /var/log/td-agent/containers.log
        time_slice_format %Y%m%d
        time_slice_wait 10m
        time_format %Y%m%dT%H%M%S%z
        compress gzip
        utc
    </match>
----
====

. Enable *fluentd*:
+
====
----
# systemctl enable td-agent
# systemctl start td-agent
----
====
+
[TIP]
====
Any errors are logged in the *_/var/log/td-agent/td-agent.log_* file.
====

You should now find all the containers' logs available on the master in the
*_/var/log/td-agent/containers.log_* file.

[[using-elasticsearch]]

== Using Elasticsearch

https://www.elastic.co/products/elasticsearch[*Elasticsearch*] is an open source
distributed document database that indexes documents and provides full-text
search capabilities. By storing container logs in *Elasticsearch*, users are
able to search all content and filter appropriately. This documentation shows
how to run https://www.elastic.co/products/kibana[*Kibana*].

This option requires more configuration and more resources than the
link:#using-a-centralized-file-system[centralized file system option], but makes
logs more useful for troubleshooting and fault finding.

Enabling aggregated logging to *Elasticsearch* involves:

. link:#creating-an-elasticsearch-cluster[Creating an *Elasticsearch* cluster]
. link:#creating-logging-pods[Creating logging pods]
. link:#creating-the-kibana-search[Creating the *Kibana* service]

[[creating-an-elasticsearch-cluster]]

=== Creating an Elasticsearch cluster

Logs are stored in an *Elasticsearch* cluster running on OpenShift. This cluster
is scalable using a
link:../architecture/core_concepts/deployments.html#replication-controllers[replication
controller], so you can link:../dev_guide/deployments.html#scaling[scale] the
*Elasticsearch* cluster up and down as required.

The following is the manifest for the *Elasticsearch* cluster:

====

[source,yaml]
----
apiVersion: "v1"
kind: "List"
items:
-
  apiVersion: "v1"
  kind: "Service"
  metadata:
    labels:
      provider: "fabric8"
      component: "elasticsearch"
    name: "es-logging"
  spec:
    ports:
    -
      port: 9200
      targetPort: 9200
    selector:
      provider: "fabric8"
      component: "elasticsearch"
-
  apiVersion: "v1"
  kind: "Service"
  metadata:
    labels:
      provider: "fabric8"
      component: "elasticsearch"
    name: "es-logging-cluster"
  spec:
    portalIP: "None"
    ports:
    -
      port: 9300
      targetPort: 9300
    selector:
      provider: "fabric8"
      component: "elasticsearch"
-
  apiVersion: "v1"
  kind: "ReplicationController"
  metadata:
    labels:
      provider: "fabric8"
      component: "elasticsearch"
    name: "elasticsearch"
  spec:
    replicas: 1
    selector:
      provider: "fabric8"
      component: "elasticsearch"
    template:
      metadata:
        labels:
          provider: "fabric8"
          component: "elasticsearch"
      spec:
        containers:
          -
            env:
            -
              name: "KUBERNETES_TRUST_CERT"
              value: "true"
            -
              name: "SERVICE_DNS"
              value: "es-logging-cluster"
            image: "fabric8/elasticsearch-k8s:1.5.2"
            name: "elasticsearch"
            ports:
            -
              containerPort: 9200
              name: "http"
            -
              containerPort: 9300
              name: "transport"
----
====

Save this to a file and create it:

====
----
$ oc create -f path/to/elasticsearch.yaml
----
====

This starts a single *Elasticsearch* instance. If you need to create a larger
cluster, you can scale the *Elasticsearch* replication controller using:

====
----
$ oc scale --replicas=3 rc elasticsearch
----
====

[[creating-logging-pods]]

=== Creating Logging Pods

To read the container logs, a static pod must be deployed on each node. To do
this, you must first ensure that the node is configured to read local pod
manifest configuration files. This is enabled by configuring the
`*podManifestConfig*` in the *_node-config.yaml_* file on each node, changing
the configuration path and check interval appropriately:

====

[source,yaml]
----
podManifestConfig:
  path: openshift.local.manifests
  fileCheckIntervalSeconds: 10
----
====

To create the logging pod, create a file with the following contents in the
directory specified by `*podManifestConfig.path*` above:

====

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: fluentd-elasticsearch
spec:
  containers:
  - name: fluentd-elasticsearch
    image: fabric8/fluentd-kubernetes:1.0
    securityContext:
      privileged: true
    resources:
      limits:
        cpu: 100m
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: varlibdockercontainers
      mountPath: /var/lib/docker/containers
      readOnly: true
  volumes:
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers
----
====

This starts a pod on the node and posts the container logs to *Elasticsearch*.

To validate it is working, you can query *Elasticsearch* and check that the data
is correctly being persisted. First, identify one of the *Elasticsearch* pods:

----
$ oc get pods -l component=elasticsearch
----

Then query *Elasticsearch*, replacing the pod ID with one returned from the
above command:

----
$ oc exec -p <pod_id> -c elasticsearch -- curl -s localhost:9200/_cat/indices?v
----

You should see output similar to the following:

====
----
health status index               pri rep docs.count docs.deleted store.size pri.store.size
yellow open   logstash-2015.06.05   5   1        540            0      251kb          251kb
----
====

If the value for `docs.count` is more than 0, then log records are being
correctly sent to *Elasticsearch*.

[[creating-the-kibana-search]]

=== Creating the Kibana Service

To create the *Kibana* service, save the following specification to your file
system:

====

[source,yaml]
----
apiVersion: "v1"
kind: "List"
items:
-
  apiVersion: "v1"
  kind: "Service"
  metadata:
    name: "kibana"
  spec:
    ports:
      -
        port: 80
        targetPort: "kibana-port"
    selector:
      provider: fabric8
      component: "kibana"
-
  apiVersion: "v1"
  kind: "ReplicationController"
  metadata:
    name: "kibana"
    labels:
      provider: fabric8
      component: "kibana"
  spec:
    replicas: 1
    selector:
      component: "kibana"
    template:
      metadata:
        name: "kibana"
        labels:
          provider: fabric8
          component: "kibana"
      spec:
        containers:
          -
            name: "kibana"
            image: "fabric8/kibana4:4.0.2"
            ports:
              -
                name: "kibana-port"
                containerPort: 5601
            env:
              -
                name: "ELASTICSEARCH_URL"
                value: "http://es-logging:9200"
----
====

Create the *Kibana* replication controller and service:

====
----
$ oc create -f path/to/kibana.yaml
----
====
