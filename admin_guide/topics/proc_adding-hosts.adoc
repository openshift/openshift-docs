////
Adding hosts

Module included in the following assemblies:

* install_config/adding_hosts_to_existing_cluster.adoc
* admin_guide/assembly_replace-master-host.adoc
* admin_guide/manage_nodes.adoc
////

[id='adding-cluster-hosts_{context}']
= Adding hosts

You can add new hosts to your cluster by running the *_scaleup.yml_* playbook.
This playbook queries the master, generates and distributes new certificates for
the new hosts, and then runs the configuration playbooks on only the new hosts.
Before running the *_scaleup.yml_* playbook, complete all prerequisite
xref:../install/host_preparation.adoc#preparing-for-advanced-installations-origin[host
preparation] steps.

[IMPORTANT]
====
The *_scaleup.yml_* playbook configures only the new host. It does not update
*_NO_PROXY_* in master services, and it does not restart master services.
====

You must have an existing inventory file, for example *_/etc/ansible/hosts_*,
that is representative of your current cluster configuration in order to run the
*_scaleup.yml_* playbook.
ifdef::openshift-enterprise[]
If you previously used the `atomic-openshift-installer` command to run your
installation, you can check *_~/.config/openshift/hosts_* for the last inventory
file that the installer generated and use that file as your inventory file. You
can modify this file as required. You must then specify the file location with
`-i` when you run the `ansible-playbook`.
endif::[]

[IMPORTANT]
====
See the
xref:../scaling_performance/cluster_maximums.adoc#scaling-performance-cluster-maximums[cluster
maximums] section for the recommended maximum number of nodes.
====

[discrete]
== Procedure

. Ensure you have the latest playbooks by updating the *openshift-ansible*
package:
+
----
# yum update openshift-ansible
----

. Edit your *_/etc/ansible/hosts_* file and add *new_<host_type>* to the
*[OSEv3:children]* section. For example, to add a new node host, add *new_nodes*:
+
----
[OSEv3:children]
masters
nodes
new_nodes
----
+
To add new master hosts, add *new_masters*.

. Create a *[new_<host_type>]* section to specify host information for the new
hosts. Format this section like an existing section, as shown in the following
example of adding a new node:
+
----
[nodes]
master[1:3].example.com
node1.example.com openshift_node_group_name='node-config-compute'
node2.example.com openshift_node_group_name='node-config-compute'
infra-node1.example.com openshift_node_group_name='node-config-infra'
infra-node2.example.com openshift_node_group_name='node-config-infra'

[new_nodes]
node3.example.com openshift_node_group_name='node-config-infra'
----
+
See
xref:../install/configuring_inventory_file.adoc#advanced-host-variables[Configuring
Host Variables] for more options.
+
When adding new masters, add hosts to both the *[new_masters]* section and the
*[new_nodes]* section to ensure that the new master host is part of
the OpenShift SDN:
+
----
[masters]
master[1:2].example.com

[new_masters]
master3.example.com

[nodes]
master[1:2].example.com
node1.example.com openshift_node_group_name='node-config-compute'
node2.example.com openshift_node_group_name='node-config-compute'
infra-node1.example.com openshift_node_group_name='node-config-infra'
infra-node2.example.com openshift_node_group_name='node-config-infra'

[new_nodes]
master3.example.com
----
+
[IMPORTANT]
====
If you label a master host with the `node-role.kubernetes.io/infra=true` label and have no other
dedicated infrastructure nodes, you must also explicitly mark the host as
schedulable by adding `openshift_schedulable=true` to the entry. Otherwise, the
registry and router pods cannot be placed anywhere.
====

. Change to the playbook directory and run the *_openshift_node_group.yml_*
playbook. If your inventory file is located somewhere other than the default of
*_/etc/ansible/hosts_*, specify the location with the `-i` option:
+
----
$ cd /usr/share/ansible/openshift-ansible
$ ansible-playbook [-i /path/to/file] \
  playbooks/openshift-master/openshift_node_group.yml
----
+
This creates the ConfigMap for the new node groups, and ultimately, the
configuration file of the node on the host.
+
[NOTE]
====
Running the *_openshift_node_group.yaml_* playbook only updates new nodes. It cannot be run to update existing nodes in a cluster.
====

. Run the *_scaleup.yml_* playbook.
If your inventory file is located somewhere
other than the default of *_/etc/ansible/hosts_*, specify the location with the
`-i` option.
** For additional nodes:
+
----
$ ansible-playbook [-i /path/to/file] \
    playbooks/openshift-node/scaleup.yml
----
** For additional masters:
+
----
$ ansible-playbook [-i /path/to/file] \
    playbooks/openshift-master/scaleup.yml
----
+
. Set the node label to `logging-infra-fluentd=true`, if you deployed the EFK stack in your cluster:
+
----
# oc label node/new-node.example.com logging-infra-fluentd=true
----

. After the playbook runs,
xref:../install/running_install.adoc#advanced-verifying-the-installation[verify the installation].

. Move any hosts that you defined in the *[new_<host_type>]* section to their
appropriate section. By moving these hosts, subsequent playbook runs that use
this inventory file treat the nodes correctly. You can keep the
empty *[new_<host_type>]* section. For example, when adding new nodes:
+
----
[nodes]
master[1:3].example.com
node1.example.com openshift_node_group_name='node-config-compute'
node2.example.com openshift_node_group_name='node-config-compute'
node3.example.com openshift_node_group_name='node-config-compute'
infra-node1.example.com openshift_node_group_name='node-config-infra'
infra-node2.example.com openshift_node_group_name='node-config-infra'

[new_nodes]
----
