[[admin-guide-manage-nodes]]
= Managing Nodes
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
You can manage
link:../architecture/infrastructure_components/kubernetes_infrastructure.html#node[nodes]
in your instance using the link:../cli_reference/index.html[CLI].

When you perform node management operations, the CLI interacts with
link:../architecture/infrastructure_components/kubernetes_infrastructure.html#node-object-definition[node
objects] that are representations of actual node hosts. The
link:../architecture/infrastructure_components/kubernetes_infrastructure.html#master[master]
uses the information from node objects to validate nodes with
link:../architecture/infrastructure_components/kubernetes_infrastructure.html#node[health
checks].

== Listing Nodes
To list all nodes that are known to the master:

====
[options="nowrap"]
----
$ oc get nodes
NAME                 LABELS                                        STATUS
node1.example.com    kubernetes.io/hostname=node1.example.com      Ready
node2.example.com    kubernetes.io/hostname=node2.example.com      Ready
----
====

To only list information about a single node, replace `_<node>_` with the full
node name:

----
$ oc get node <node>
----

The `STATUS` column in the output of these commands can show nodes with the
following conditions:

.Node Conditions [[node-conditions]]
[cols="3a,8a",options="header"]
|===

|Condition |Description

|`Ready`
|The node is passing the health checks performed from the master by returning
`StatusOK`.

|`NotReady`
|The node is not passing the health checks performed from the master.

|`SchedulingDisabled`
|Pods cannot be link:#marking-nodes-as-unschedulable-or-schedulable[scheduled
for placement] on the node.

|===

NOTE: The `STATUS` column can also show `Unknown` for a node if the CLI cannot
find any node condition.

To get more detailed information about a specific node, including the reason for
the current condition:

----
$ oc describe node <node>
----

For example:

====
[options="nowrap"]
----
$ oc describe node node1.example.com
Name:			node1.example.com
Labels:			kubernetes.io/hostname=node1.example.com
CreationTimestamp:	Wed, 10 Jun 2015 17:22:34 +0000
Conditions:
  Type		Status	LastHeartbeatTime			LastTransitionTime			Reason					Message
  Ready 	True 	Wed, 10 Jun 2015 19:56:16 +0000 	Wed, 10 Jun 2015 17:22:34 +0000 	kubelet is posting ready status
Addresses:	127.0.0.1
Capacity:
 memory:	1017552Ki
 pods:		100
 cpu:		2
Version:
 Kernel Version:		3.17.4-301.fc21.x86_64
 OS Image:			Fedora 21 (Twenty One)
 Container Runtime Version:	docker://1.6.0
 Kubelet Version:		v0.17.1-804-g496be63
 Kube-Proxy Version:		v0.17.1-804-g496be63
ExternalID:			node1.example.com
Pods:				(2 in total)
  docker-registry-1-9yyw5
  router-1-maytv
No events.
----
====

== Adding Nodes
You can add a node to an existing OpenShift instance using the `oc create`
command and supplying a definition for a node object. After the newly-created
node passes the health checks that are performed by the master, pods can be
scheduled for the node.

.To Add a Node to an Existing OpenShift Instance:

. Create a definition for the new node.
.. If you have an existing node object created, you can export the existing
object to use as a starting point for the new node's
link:../architecture/infrastructure_components/kubernetes_infrastructure.html#node-object-definition[definition]:
+
----
$ oc export node <node_name> > <filename>
----
+
Modify the new file to meet the desired definition for your new node.

.. If you do not have an existing node object, create a new file from scratch by
following the
link:../architecture/infrastructure_components/kubernetes_infrastructure.html#node-object-definition[example
definition].

. Create the node object using the file you created:
+
----
$ oc create -f <filename>
----
+
At this point, the master begins validating the new node by performing health
checks.

. Verify that the node was added by checking the output of the following
command:
+
====

----
$ oc get nodes
NAME                 LABELS                                        STATUS
node1.example.com    kubernetes.io/hostname=node1.example.com      Ready
node2.example.com    kubernetes.io/hostname=node2.example.com      Ready
node3.example.com    kubernetes.io/hostname=node3.example.com      Ready
----
====
+
Pods are not scheduled for the node until the node passes the health checks that
are performed by the master. When the node passes these health checks, it is
then placed in the `Ready` condition.

== Deleting Nodes
When you delete a node with the CLI, although the node object is deleted
in Kubernetes, the pods that exist on the node itself are not deleted. However,
the pods cannot be accessed by OpenShift. The behavior around deleting nodes and
pods with the CLI is under active development.

To delete a node:

----
$ oc delete node <node>
----

== Updating Labels on Nodes
To add or update
link:../architecture/core_concepts/pods_and_services.html#labels[labels] on a
node:

----
$ oc label node <node> <key_1>=<value_1> ... <key_n>=<value_n>
----

To see more detailed usage:

----
$ oc label -h
----

== Listing Pods on Nodes
To list all or selected pods on one or more nodes:

[options="nowrap"]
----
$ oadm manage-node <node1> <node2> \
    --list-pods [--pod-selector=<pod_selector>] [-o json|yaml]
----

To list all or selected pods on selected nodes:

----
$ oadm manage-node --selector=<node_selector> \
    --list-pods [--pod-selector=<pod_selector>] [-o json|yaml]
----

== Marking Nodes as Unschedulable or Schedulable
[[marking-nodes-as-unschedulable-or-schedulable]]
By default, healthy nodes with a `Ready` link:#node-conditions[status] are
marked as schedulable, meaning that new pods are allowed for placement on the
node. Manually marking a node as unschedulable blocks any new pods from being
scheduled on the node. Existing pods on the node are not affected.

To mark a node or nodes as unschedulable:

----
$ oadm manage-node <node1> <node2> --schedulable=false
----

For example:

====
[options="nowrap"]
----
$ oadm manage-node node1.example.com --schedulable=false
NAME                 LABELS                                        STATUS
node1.example.com    kubernetes.io/hostname=node1.example.com      Ready,SchedulingDisabled
----
====

To mark a currently unschedulable node or nodes as schedulable:

----
$ oadm manage-node <node1> <node2> --schedulable
----

Alternatively, instead of specifying specific node names (e.g., `_<node1>_
_<node2>_`), you can use the `--selector=_<node_selector>_` option to mark
selected nodes as schedulable or unschedulable.

== Evacuating Pods on Nodes
Evacuating pods allows you to migrate all or selected pods from a given node or
nodes. Nodes must first be
link:#marking-nodes-as-unschedulable-or-schedulable[marked unschedulable] to
perform pod evacuation.

Only pods backed by a
link:../architecture/core_concepts/deployments.html#replication-controllers[replication
controller] can be evacuated; the replication controllers create new pods on
other nodes and remove the existing pods from the specified node(s). Bare pods,
meaning those not backed by a replication controller, are unaffected by default.

To list pods that will be migrated without actually performing the evacuation,
use the `--dry-run` option:

----
$ oadm manage-node <node1> <node2> \
    --evacuate --dry-run [--pod-selector=<pod_selector>]
----

To actually evacuate all or selected pods on one or more nodes:

----
$ oadm manage-node <node1> <node2> \
    --evacuate [--pod-selector=<pod_selector>]
----

You can force deletion of bare pods by using the `--force` option:

----
$ oadm manage-node <node1> <node2> \
    --evacuate --force [--pod-selector=<pod_selector>]
----

Alternatively, instead of specifying specific node names (e.g., `_<node1>_
_<node2>_`), you can use the `--selector=_<node_selector>_` option to evacuate
pods on selected nodes.
