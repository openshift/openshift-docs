= Overcommit
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview

The scheduler attempts to improve the utilization of compute resources across all nodes in the cluster.

It places pods on nodes relative to the pods compute resource requests to find a node that provides the best fit.

== Requests and Limits

For each compute resource, a container may specify a resource request and limit.  Scheduling decisions are made based on the request to ensure that a node has enough capacity available to meet the requested value.  If a container specifies limits, but omits requests, the requests are defaulted to the limits.  A container is not able to exceed the specified limit on the node.  The enforcement of limits is dependent upon the compute resource type.  If a container makes no request or limit, the container is scheduled to a node with no resource guarantees.  In practice, the container is able to consume as much of the specified resource as is available with the lowest local priority.  In low resource situations, containers that specify no resource requests are given the lowest quality of service.

== Compute Resources

The node enforced behavior for compute resources is resource type specific.

=== CPU

A container is guaranteed the amount of CPU it requests, but it may or may not get more CPU time based on local node conditions.  If a container does not specify a corresponding limit, it is able to consume as much excess CPU is available on the node.  If multiple containers are attempting to use excess CPU, CPU time is distributed based on the amount of CPU requested by each container.  For example, if one container requested 500m of CPU time, and another container requested 250m of CPU time, any extra CPU time available on the node is distributed among the containers in a 2:1 ratio.  If a container specified a limit, it will be throttled to not use more CPU than the specified limit. 

CPU requests are enforced using the CFS shares support in the Linux kernel.  CPU limits are enforced using the CFS quota support in the Linux kernel over a 100ms measuring interval by default.

=== Memory

A container is guaranteed the amount of memory it requests.  A container may use more memory than requested, but once it exceeds its requested amount, it could be killed in a low memory situation on the node.  If a container uses less memory than requested, it will not be killed unless system tasks or daemons need more memory than was accounted for in the nodes resource reservation.  If a container specifies a limit on memory, it is immediately killed if it exceeds the limited amount.

== Quality of Service Classes

A node is overcommmitted when it has a pod scheduled that makes no request, or when the sum of limits across all pods on that node exceeds available machine capacity.  In this environment, it is possible that the pods on the node will attempt to use more compute resource than is available at any given point in time.  When this occurs, the node must give priority to one pod over another.  The facility used to make this decision is referred to as a Quality of Service (QoS) Class.

For each compute resource, a container is divided into one of 3 QoS classes with decreasing order of priority:

1. Guaranteed
2. Burstable
3. BestEffort

If limits and optionally requests are set (not equal to 0) for all resources and they are equal, then the container is classified as Guaranteed.

If requests and optionally limits are set (not equal to 0) for all resources, and they are not equal, then the container is classified as Burstable.

If requests and limits are not set for any of the resources, then the container is classified as BestEffort.

Memory is an incompressible resource, so in low-memory situations, containers are killed that have the lowest priority.

1. BestEffort containers are treated with the lowest priority.  Processes in these containers are first to be killed if the system runs out of memory.
2. Guaranteed containers are considered top priority, and are guaranteed to only be killed if they exceed their limits, or if the system is under memory pressure and there are no lower priority containers that can be evicted.
3. Burstable containers under system memory pressure are more likely to be killed once they exceed their requests and no other BestEffort containers exist.

== Node configuration

In an overcommitted environment, it's important to properly configure your node to provide best system behavior.

=== Reserving resources for system processes

The scheduler ensures that there are enough resources for all pods on a node based on the pod requests.  It verifies that the sum of requests of containers on the node is no greater than the node capacity.  It includes all containers started by the node, but not containers or processes started outside the knowledge of the cluster.

It is recommended that you reserve some portion of the node capacity to allow for the system daemons that are required to run on your node for your cluster to function (sshd, docker, etc).  In particular, it is recommended
that you reserve resources for incompressible resources like memory.

If you want to explicitly reserve resources for non-Pod processes, you can create a resource-reserver pod that does nothing but reserve capacity from being scheduled to on the node by the cluster.

resource-reserver.yaml
====
----
apiVersion: v1
kind: Pod
metadata:
  name: resource-reserver
spec:
  containers:
  - name: sleep-forever
    image: gcr.io/google_containers/pause:0.8.0
    resources:
      limits:
        cpu: 100m <1>        
        memory: 150Mi <2>
----
<1> The amount of cpu you would like to reserve on a node for host level daemons unknown to the cluster.
<2> The amount of memory you would like to reserve on a node for host level daemons unknown to the cluster.
====

Place the file in the node config directory, i.e. --config=DIR location.

On start of the node, the node agent will launch the specified container and the remaining capacity for the scheduler to place cluster pods will adjust accordingly.

=== Kernel tunable flags

When the node starts, it ensures that the kernel tunable flags for memory management are set properly.

The kernel should never fail memory allocations unless it runs out of physical memory.  

To ensure this behavior, the node instructs the kernel to always overcommit memory:

----
$ sysctl -w vm.overcommit_memory=1
----

The node instructs the kernel to not panic when it runs out of memory.  

Instead the kernel OOM killer should kill processes based on priority.

----
$ sysctl -w vm.panic_on_oom=0
----

=== Disable swap memory

It is important to disable the use of swap memory on the node as it makes it difficult for the resource guarantees the scheduler makes during pod placement to hold.  For example, suppose 2 guaranteed pods have reached their memory limit.  Each container would start allocating memory on swap space.  Eventually, if there isn't enough swap space, processes in the pods might get killed.

To disable swap memory on each node:

----
$ swapoff -a
----
