[[admin-guide-overcommit]]
= Overcommitting
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview

A node is _overcommitted_ when it has a pod scheduled that makes no request, or
when the sum of limits across all pods on that node exceeds available machine
capacity.

In an overcommitted environment, it is possible that the pods on the node will
attempt to use more compute resource than is available at any given point in
time.

Administrators can use _requests_ and _limits_ to allow and manage the overcommitment
of resources on a node, which can be desirable in development environments where
performance is not a concern.

Scheduling is based on resources requested, while quota and hard limits refer to
resource limits, which can be set higher than requested resources. The difference
between request and limit determines the level of overcommit; for instance, if a
container is given a memory request of 1Gi and a memory limit of 2Gi, it is
scheduled based on the 1Gi request being available on the node, but could use
up to 2Gi; so it is 200% overcommitted.

Note also that after overrides, the container limits and requests must still
be validated by any LimitRange objects in the project. It is possible,
for example, for developers to specify a limit close to the minimum
limit, and have the request then be overridden below the minimum limit,
causing the pod to be forbidden. This unfortunate user experience should
be addressed with future work, but for now, configure this capability
and LimitRanges with caution.

When configured, overrides can be disabled per-project (for example,
to allow infrastructure components to be configured independently of
overrides) by editing the project and adding the following annotation:

----
quota.openshift.io/cluster-resource-override-enabled: "false"
----


[[requests-and-limits]]
== Understanding Requests and Limits

For each compute resource, a container can specify the amount of CPU it requires using xref:../dev_guide/compute_resources.html#dev-compute-resources[compute resource _requests_ (requests) and _limits_]:

* Requests - A request represents a minimum amount of CPU or memory that your container may consume and is used for scheduling your container and
providing a minimum service guarantee. 

* Limits - A limit constrains the amount of compute resource that may be consumed on your node.

The xref:../admin_guide/scheduler.adoc#admin-guide-scheduler[pod scheduler] determes placement of a new pod 
onto a cluster node that has enough capacity available to meet the requested value, 
finding the best fit for the pod based on configured policies. 

If a container specifies limits, but omits requests, the requests are defaulted to the limits. A
container is not able to exceed the specified limit on the node.

The enforcement of limits is dependent upon the compute resource type. 

If a container makes no request or limit, the container is scheduled to a node with
no resource guarantees. 

In practice, the container is able to consume as much of
the specified resource as is available with the lowest local priority. In low
resource situations, containers that specify no resource requests are given the
lowest quality of service.


[[overcommit-memory]]
[[compute-resources]]
[[qos-classes]]
== Compute Resources and Overcommit

The node-enforced behavior for compute resources is specific to the resource
type.

[[overcommit-cpu]]
.Node enforced behavior on resources
[options="header",cols="1,5"]
|===

|Resource |Behavior

|CPU
|A container is guaranteed the amount of CPU it requests, but it may or may not
get more CPU time based on local node conditions. If a container does not
specify a corresponding limit, it is able to consume excess CPU available on the
node. If multiple containers are attempting to use excess CPU, CPU time is
distributed based on the amount of CPU requested by each container.

CPU requests are enforced using the CFS shares support in the Linux kernel. By
default, CPU limits are enforced using the CFS quota support in the Linux kernel
over a 100ms measuring interval, though xref:enforcing-cpu-limits[this can be
disabled].


|Memory
|A container is guaranteed the amount of memory it requests. A container may use
more memory than requested, but once it exceeds its requested amount, it could
be killed in a low memory situation on the node.

If a container uses less memory than requested, it will not be killed unless
system tasks or daemons need more memory than was accounted for in the node's
resource reservation. If a container specifies a limit on memory, it is
immediately killed if it exceeds the limited amount.

|===

For example, if one container requested 500m of CPU time, and another container
requested 250m of CPU time, any extra CPU time available on the node is
distributed among the containers in a 2:1 ratio. If a container specified a
limit, it will be throttled to not use more CPU than the specified limit.


== Quality of Service Classes and Overcommit

If the pods on a node attempt to use more compute resource than is available at any given point in
time, the node must give priority to one pod over another. The decision is 
nased on the Quality of Service (QoS) Class.

For each compute resource, a container is divided into one of three QoS classes
with decreasing order of priority:

.Quality of Service Classes
[options="header",cols="1,1,5"]
|===
|Priority |Class Name |Description

|1 (highest)
|*Guaranteed*
|A container is classified as *Guaranteed* if limits and optionally requests 
are set (not equal to 0) for all resources
and they are equal.

Guaranteed containers are considered the top priority, and are guaranteed to
only be killed if they exceed their limits, or if the system is under memory
pressure and there are no lower priority containers that can be evicted.

|2
|*Burstable*
|A container is classified as *Burstable* if requests and optionally limits 
are set (not equal to 0) for all resources, and they are not equal.

Burstable containers under system memory pressure are more likely to be
killed once they exceed their requests and no other *BestEffort* containers
exist.

|3 
|*BestEffort*
|A container is classified as *BestEffort* if requests and limits are not set 
for any of the resources. 

BestEffort containers are considered the lowest priority. Processes in
these containers are first to be killed if the system runs out of memory.
|===


[[configuring-masters-for-overcommitment]]
== Master Options for Overcommitment

An {product-title} administrator can control the level of overcommit
and manage container density on nodes by configuring the master to override the ratio between 
request and limit set on a developer container. 

In conjunction with a xref:./limits.adoc#admin-guide-limits[per-project
range limits] adjusting the container
limit and request can achieve the desired level of overcommit.

ClusterResourceOverrideConfig

.Cluster Resource Override parameters
[options="header",cols="2,2"]
|===
Parameter | Description

|memoryRequestToLimitPercent
|Overrides the container memory limit by the specified amount.

|cpuRequestToLimitPercent
|Overrides the container CPU limit by the specified amount.

|limitCPUToMemoryPercent
|Overrides the CPU limit to a percentage of the memory limit. A 100 percentage scaling 1Gi of RAM to equal 1 CPU core. This is processed prior to overriding CPU request (if configured).
|===

====
----
kubernetesMasterConfig:
  admissionConfig:
    pluginConfig:
      ClusterResourceOverride:   <1>
        configuration:
          apiVersion: v1
          kind: ClusterResourceOverrideConfig
          memoryRequestToLimitPercent: 25  <2>
          cpuRequestToLimitPercent: 25     <3>
          limitCPUToMemoryPercent: 200     <4>
----
<1> This is the plug-in name; case matters and anything but an exact match for a plug-in name is ignored.
<2> (optional, 1-100) Allows the nodes to override the container memory limit by 25%.
<3> (optional, 1-100) Allows the nodes to override the CPU memory limit by 25%.
<4> (optional, positive integer) If a container memory limit has been specified or defaulted, the CPU limit is overridden to a percentage of the memory limit, with a 100 percentage scaling 1Gi of RAM to equal 1 CPU core. This is processed prior to overriding CPU request (if configured).
====

[[configuring-nodes-for-overcommitment]]
== Nodes Options for Overcommitment

In an overcommitted environment, it is important to properly configure your node
to provide best system behavior.

[[enforcing-cpu-limits]]

*Enforcing CPU Limits* - Nodes by default enforce specified CPU limits using the CPU CFS quota support in
the Linux kernel. If you do not want to enforce CPU limits on the node, you can
disable its enforcement by modifying the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[node configuration file]
(the *_node-config.yaml_* file) to include the following:

====
----
kubeletArguments:
  cpu-cfs-quota:
    - "false"
----
====

If CPU limit enforcement is disabled, it is important to understand the impact that will have on your node:

- If a container makes a request for CPU, it will continue to be enforced by CFS
shares in the Linux kernel.
- If a container makes no explicit request for CPU, but it does specify a limit,
the request will default to the specified limit, and be enforced by CFS shares
in the Linux kernel.
- If a container specifies both a request and a limit for CPU, the request will
be enforced by CFS shares in the Linux kernel, and the limit will have no
impact on the node.

[[reserving-resources-for-system-processes]]
*Reserving Resources for System Processes* - The xref:../admin_guide/scheduler.adoc#admin-guide-scheduler[scheduler] ensures that there are enough
resources for all pods on a node based on the pod requests. It verifies that the
sum of requests of containers on the node is no greater than the node capacity.
It includes all containers started by the node, but not containers or processes
started outside the knowledge of the cluster.

It is recommended that you reserve some portion of the node capacity to allow
for the system daemons that are required to run on your node for your cluster to
function (*sshd*, *docker*, etc.). In particular, it is recommended that you
reserve resources for incompressible resources such as memory.

If you want to explicitly reserve resources for non-pod processes, there are two
ways to do so:

- The preferred method is to allocate node resources by specifying resources
available for scheduling. See
xref:../admin_guide/allocating_node_resources.adoc#admin-guide-allocating-node-resources[Allocating Node Resources]
for more details.

- Alternatively, you can create a *resource-reserver* pod that does nothing but
reserve capacity from being scheduled on the node by the cluster. For example:
+
.*resource-reserver* Pod Definition
====
----
apiVersion: v1
kind: Pod
metadata:
  name: resource-reserver
spec:
  containers:
  - name: sleep-forever
    image: gcr.io/google_containers/pause:0.8.0
    resources:
      limits:
        cpu: 100m <1>
        memory: 150Mi <2>
----
<1> The amount of CPU to reserve on a node for host-level daemons unknown to the
cluster.
<2> The amount of memory to reserve on a node for host-level daemons unknown to
the cluster.
====
+
You can save your definition to a file, for example *_resource-reserver.yaml_*,
then place the file in the node configuration directory, for example
*_/etc/origin/node/_* or the `--config=<dir>` location if otherwise specified.
+
Additionally, the node server needs to be configured to read
the definition from the node configuration directory,
by naming the directory in the `kubeletArguments.config` field of the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[node configuration file]
(usually named *_node-config.yaml_*):
+
====
----
kubeletArguments:
  config:
    - "/etc/origin/node"  <1>
----
<1> If `--config=<dir>` is specified, use `<dir>` here.
====
+
With the *_resource-reserver.yaml_* file in place,
starting the node server also launches the *sleep-forever* container.
The scheduler takes into account the remaining capacity of the node,
adjusting where to place cluster pods accordingly.
+
To remove the *resource-reserver* pod, you can delete or move
the *_resource-reserver.yaml_* file from the node configuration directory.

[[kernel-tunable-flags]]
*Kernel Tunable Flags* - When the node starts, it ensures that the kernel tunable flags for memory
management are set properly. The kernel should never fail memory allocations
unless it runs out of physical memory.

To ensure this behavior, the node instructs the kernel to always overcommit
memory:

----
$ sysctl -w vm.overcommit_memory=1
----

The node also instructs the kernel not to panic when it runs out of memory.
Instead, the kernel OOM killer should kill processes based on priority:

----
$ sysctl -w vm.panic_on_oom=0
----

[NOTE]
====
The above flags should already be set on nodes, and no further action is
required.
====

[[disabling-swap-memory]]
*Disabling Swap Memory* - You can disable swap by default on your nodes in order to preserve quality of
service guarantees. Otherwise, physical resources on a node can oversubscribe,
affecting the resource guarantees the Kubernetes scheduler makes during pod
placement. 

For example, if two guaranteed pods have reached their memory limit, each
container could start using swap memory. Eventually, if there is not enough swap
space, processes in the pods can be terminated due to the system being
oversubscribed.

To disable swap:

----
$ swapoff -a
----

Failing to disable swap results in nodes not recognizing that they are
experiencing *MemoryPressure*, resulting in pods not receiving the memory they
made in their scheduling request. As a result, additional pods are placed on the
node to further increase memory pressure, ultimately increasing your risk of
experiencing a system out of memory (OOM) event.

[IMPORTANT]
====
If swap is enabled, any
xref:../admin_guide/out_of_resource_handling.adoc#admin-guide-handling-out-of-resource-errors[out
of resource handling] eviction thresholds for available memory will not work as
expected. Take advantage of out of resource handling to allow pods to be evicted
from a node when it is under memory pressure, and rescheduled on an alternative
node that has no such pressure.
====


== Configuring Overcommit in a Cluster 

To configure a cluster for overcommit:

. Configure the master:
+
.. Open the *_master-config.yaml_* file.
+
.. Edit the `*ClusterResourceOverride*` admission controller, as needed:
+
----
kubernetesMasterConfig:
  admissionConfig:
    pluginConfig:
      ClusterResourceOverride:   <1>
        configuration:
          apiVersion: v1
          kind: ClusterResourceOverrideConfig
          memoryRequestToLimitPercent: <percent_to_override>
          cpuRequestToLimitPercent: <percent_to_override>
          limitCPUToMemoryPercent: <amount_to_override>
----
+
.. Save the *_master-config.yaml_* file.
+
.. Restart OpenShift for the changes to take effect.
+
ifdef::openshift-enterprise[]
----
# systemctl restart atomic-openshift-master
----
endif::[]
ifdef::openshift-origin[]
----
# systemctl restart origin-master
----
endif::[]

. Configure the nodes:
+
.. Edit the *_node-config.yaml_* file.
+
.. Optionally, disable CPU limit enforcement:
+
----
kubeletArguments:
  cpu-cfs-quota:
    - "false"
----
+
.. Reserve resources for non-pod processes:
+
----
kubeletArguments:
  kube-reserved:
    - "cpu=200m,memory=30G"
  system-reserved:
    - "cpu=200m,memory=30G"
----
+
For more information, see xref:allocating_node_resources.html#allocating-node-settings[Allocating Node Resources].

.. Save the *_node-config.yaml_* file.

.. Execute the following command to set memory management kernel values:
+
----
$ sysctl -w vm.overcommit_memory=1
$ sysctl -w vm.panic_on_oom=0
----
+
.. Optionally, execute the following command to disable swap memory:
+
----
$ swapoff -a
----

. Set default limits for the project by creating a `LimitRange` object, similar to the following example: 
+
----
apiVersion: "v1"
kind: "LimitRange"
metadata:
  name: "core-resource-limits" 
spec:
  limits:
    - type: "Pod"
      max:
        cpu: "2" 
        memory: "1Gi" 
      min:
        cpu: "200m" 
        memory: "6Mi" 
    - type: "Container"
      max:
        cpu: "2" 
        memory: "1Gi" 
      min:
        cpu: "100m" 
        memory: "4Mi" 
      default:
        cpu: "300m" 
        memory: "200Mi" 
      defaultRequest:
        cpu: "200m" 
        memory: "100Mi" 
      maxLimitRequestRatio:
        cpu: "10" 
----
 
Or, you can xref:./managing_projects.adoc#modifying-the-template-for-new-projects[modify the project
template]) to set the default limits.

For infomation, see xref:./limits.adoc#admin-guide-limits[Setting Limit Ranges]. 

[NOTE]
====
You must set range limits on containers for overrides to work.
====



