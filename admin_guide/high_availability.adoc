= High Availability
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
This topic describes how to set up highly-available services on your OpenShift
cluster.

The Kubernetes
link:../architecture/core_concepts/deployments.html#replication-controllers[replication
controller] ensures that the deployment requirements, in particular the number
of replicas, are satisfied when the appropriate resources are available. When
run with two or more replicas, the
link:../architecture/core_concepts/routes.html#routers[router] can be resilient
to failures, providing a highly-available service. Depending on how the router
instances are discovered (via a service, DNS entry, or IP addresses), this could
impose operational requirements to handle failure cases when one or more router
instances are "unreachable".

For some IP-based traffic services, virtual IP addresses (VIPs) should always be
serviced for as long as a single instance is available. This simplifies the
operational overhead and handles failure cases gracefully.

IMPORTANT: Even though
a service is highly available, performance can still be affected.

Use cases for high-availability include:

* I want my cluster to be assigned a resource set and I want the cluster to automatically manage those resources.
* I want my cluster to be assigned a set of VIPs that the cluster manages and migrates (with zero or minimal downtime) on failure conditions, and I should not be required to perform any manual interactions to update the upstream "discovery" sources (e.g., DNS). The cluster should service all the assigned VIPs when at least a single node is available, despite the current available resources not being sufficient to reach the desired state.

You can configure a highly-available router or network setup by running multiple
instances of the pod and fronting them with a balancing tier. This can be
something as simple as DNS round robin, or as complex as multiple load-balancing
layers.
////
=== DNS Round Robin [[dns-round-robin]]

As a simple example, you can create a zone file for a DNS server, such as BIND,
that maps multiple A records for a single domain name. When clients do a lookup,
they are given one of the many records, in order, as a round robin scheme.

[NOTE]
====
The procedure below uses wildcard DNS with multiple A records to achieve the
desired round robin. The wildcard could be further distributed into shards with:

****
`*._<shard>_`
****
====

.To Configure Simple DNS Round Robin:
. Add a new zone that points to your file:
+
====

----
#### named.conf
    zone "v3.rhcloud.com" IN {
            type master;
            file "v3.rhcloud.com.zone";
    };

----
====

. Define the round robin mappings for the DNS lookup:
+
====

----
#### v3.rhcloud.com.zone
    $ORIGIN v3.rhcloud.com.

    @       IN      SOA     . v3.rhcloud.com. (
                         2009092001         ; Serial
                             604800         ; Refresh
                              86400         ; Retry
                            1206900         ; Expire
                                300 )       ; Negative Cache TTL
            IN      NS      ns1.v3.rhcloud.com.
    ns1     IN      A       127.0.0.1
    *       IN      A       10.245.2.2
            IN      A       10.245.2.3


----
====

. Test the entry. The following example test uses `dig` (available in the
*bind-utils* package) in a *Vagrant* environment to show multiple answers for
the same lookup. Performing multiple pings shows the resolution swapping between
IP addresses:
+
[options="nowrap"]
====

----

$ dig hello-openshift.shard1.v3.rhcloud.com

; <<>> DiG 9.9.4-P2-RedHat-9.9.4-16.P2.fc20 <<>> hello-openshift.shard1.v3.rhcloud.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 36389
;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 2
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;hello-openshift.shard1.v3.rhcloud.com. IN A

;; ANSWER SECTION:
hello-openshift.shard1.v3.rhcloud.com. 300 IN A	10.245.2.2
hello-openshift.shard1.v3.rhcloud.com. 300 IN A	10.245.2.3

;; AUTHORITY SECTION:
v3.rhcloud.com.		300	IN	NS	ns1.v3.rhcloud.com.

;; ADDITIONAL SECTION:
ns1.v3.rhcloud.com.	300	IN	A	127.0.0.1

;; Query time: 5 msec
;; SERVER: 10.245.2.3#53(10.245.2.3)
;; WHEN: Wed Nov 19 19:01:32 UTC 2014
;; MSG SIZE  rcvd: 132

$ ping hello-openshift.shard1.v3.rhcloud.com
PING hello-openshift.shard1.v3.rhcloud.com (10.245.2.3) 56(84) bytes of data.
...
^C
--- hello-openshift.shard1.v3.rhcloud.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.272/0.573/0.874/0.301 ms

$ ping hello-openshift.shard1.v3.rhcloud.com
[...]
----

====
////

== Configuring IP Failover

Using IP failover involves switching IP addresses to a redundant or stand-by
set of nodes on failure conditions.

The `oadm ipfailover` command helps set up the VIP failover configuration. As
an administrator, you can configure IP failover on an entire cluster, or on a
subset of nodes, as defined by the labeled selector. If you are running in
production, match the labeled selector with at least two nodes to ensure you
have failover protection and provide a `--replicas=<n>` value that matches the
number of nodes for the given labeled selector:

----
$ oadm ipfailover [<Ip_failover_config_name>] <options> --replicas=<n>
----

////
You can view what the configuration configuration that would look like
using one of the supported formats (the example below uses the JSON format):

----
$ oadm ipfailover [<Ip_failover_config_name>] <options> -o json
----

==== ipfailover command options (subset)
The list of command options described here are a subset that are relevant to this document.

            <options> = One or more of:
                --create
                --credentials=<credentials>
                -l,--selector=<selector>
                --virtual-ips=<ip-range>
                -i|--interface=<interface>
                -w|--watch-port=<port>

            <credentials> = <string> - Path to .kubeconfig file containing credentials to use to contact the master.
            <selector> = <string> - The node selector to use for running the HA sidecar pods.
            <ip-range> = string - One or more comma separated IP address or ranges.
                                  Example: 10.2.3.42,10.2.3.80-84,10.2.3.21
            <interface> = <string> - The interface to use.
                                     Default: Default interface on node or eth0
            <port> = <number> - Port to watch for resource availability.
                                Default: 80.
            <string> = a string of characters.
            <number> = a number ([0-9]*).
////

The `oadm ipfailover` command ensures that a failover pod runs on each of
the nodes matching the constraints or label used. This pod uses VRRP (Virtual
Router Redundancy Protocol) with link:http://www.keepalived.org/[Keepalived] to ensure that the service on the
watched port is available, and, if needed, Keepalived will automatically float
the VIPs if the service is not available.

=== Configuring a Highly-available Routing Service
The following steps describe how to set up a highly-available router environment
with IP failover:

. Label the nodes for the service. This step can be optional if you run the
service on any of the nodes in your Kubernetes cluster and use VIPs that can
float within those nodes. This process may already exist within a complex
cluster, in that nodes may be filtered by any constraints or requirements
specified (e.g., nodes with SSD drives, or higher CPU, memory, or disk
requirements, etc.).
+
The following example defines a label as router instances that are servicing
traffic in the US west geography *ha-router=geo-us-west*:
+
====
----
$ oc label nodes openshift-node-{5,6,7,8,9} "ha-router=geo-us-west"
----
====

. OpenShift's *ipfailover* internally uses *keepalived*, so ensure that
multicast is enabled on the nodes labeled above and that the nodes can accept
network traffic for 224.0.0.18 (the VRRP multicast IP address). Depending on
your environment's multicast configuration, you may need to add an `iptables`
rule to each of the above labeled nodes. If you do need to add the `iptables`
rules, please also ensure that the rules persist after a system restart:
+
====
----
$ for node in openshift-node-{5,6,7,8,9}; do   ssh $node <<EOF

export interface=${interface:-"eth0"}
echo "Check multicast enabled ... ";
ifconfig $interface | grep -i MULTICAST

echo "Check multicast groups ... "
netstat -g -n | grep 224.0.0 | grep $interface

echo "Optionally, add accept rule and persist it ... "
sudo /sbin/iptables -I INPUT -i $interface -d 224.0.0.18/32 -j ACCEPT

echo "Please ensure the above rule is added on system restarts."

EOF
done;
----
====

. Depending on your environment policies, you can either reuse the *router*
service account created previously or create a new *ipfailover* service account.
+
Ensure that either the *router* service account exists as described in
link:../install_config/install/deploy_router.html[Deploying a Router] or create
a new *ipfailover* service account. The example below creates a new service
account with the name *ipfailover*:
+
====
----
$ echo '
  { "kind": "ServiceAccount",
    "apiVersion": "v1",
    "metadata": { "name": "ipfailover" }
  }
  ' | oc create -f -
----
====

. You can manually edit the *privileged* SCC and add the
*ipfailover* service account, or you can script editing the *privileged* SCC if
you have `jq` installed.

.. To manually edit the *privileged* SCC, run:
+
====
----
$ oc edit scc privileged
----
====
+
Then add the *ipfailover* service account in form
*system:serviceaccount:<project>:<name>* to the `*users*` section:
+
====
----
...
users:
- system:serviceaccount:openshift-infra:build-controller
- system:serviceaccount:default:router
- system:serviceaccount:default:ipfailover
----
====

.. Alternatively, to script editing *privileged* SCC if you have `jq` installed,
run:
+
====
----
$ oc get scc privileged -o json |
    jq '.users |= .+ ["system:serviceaccount:default:ipfailover"]' |
      oc replace scc -f -
----
====

. Start the router with at least two replicas on nodes matching the labels used
in the first step. The following example runs three instances using the
*ipfailover* service account:
+
ifdef::openshift-enterprise[]
====
----
$ oadm router ha-router-us-west --replicas=3 \
    --selector="ha-router=geo-us-west" \
    --labels="ha-router=geo-us-west" \
    --credentials=/etc/origin/master/openshift-router.kubeconfig \
    --service-account=ipfailover
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm router ha-router-us-west --replicas=3 \
    --selector="ha-router=geo-us-west" \
    --labels="ha-router=geo-us-west" \
    --credentials="$KUBECONFIG" \
    --service-account=ipfailover
----
====
endif::[]
+
[NOTE]
====
The above command runs fewer router replicas than available nodes, so
that, in the chance of node failures, Kubernetes can still ensure three
available instances until the number of available nodes labeled
*ha-router=geo-us-west* is below three. Additionally, the router uses the host
network as well as ports 80 and 443, so fewer number of replicas are running to
ensure a higher Service Level Availability (SLA). If there are no constraints on
the service being setup for failover, it is possible to target the service to
run on one or more, or even all, of the labeled nodes.
====

. Finally, configure the VIPs and failover for the nodes labeled with
*ha-router=geo-us-west* in the first step. Ensure the number of replicas match
the number of nodes and that they satisfy the label setup in the first step. The
name of the *ipfailover* configuration (*ipf-ha-router-us-west* in the example
below) should be different from the name of the router configuration
(*ha-router-us-west*) as both the router and *ipfailover* create deployment
configuration with those names. Specify the VIPs addresses and the port number
that *ipfailover* should monitor on the desired instances:
+
ifdef::openshift-enterprise[]
====
----
$ oadm ipfailover ipf-ha-router-us-west \
    --replicas=5 --watch-port=80 \
    --selector="ha-router=geo-us-west" \
    --virtual-ips="10.245.2.101-105" \
    --credentials=/etc/origin/master/openshift-router.kubeconfig \
    --service-account=ipfailover --create
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm ipfailover ipf-ha-router-us-west \
    --replicas=5 --watch-port=80 \
    --selector="ha-router=geo-us-west" \
    --virtual-ips="10.245.2.101-105" \
    --credentials="$KUBECONFIG" \
    --service-account=ipfailover --create
----
====
endif::[]

=== Configuring a Highly-available Network Service [[ip-failover]]

The following steps describe how to set up a highly-available IP-based network
service with IP failover:

. Label the nodes for the service. This step can be optional if you run the
service on any of the nodes in your Kubernetes cluster and use VIPs that can
float within those nodes. This process may already exist within a complex
cluster, in that the nodes may be filtered by any constraints or requirements
specified (e.g., nodes with SSD drives, or higher CPU, memory, or disk
requirements, etc.).
+
The following example labels a highly-available cache service that is listening
on port 9736 as *ha-cache=geo*:
+
====
----
$ oc label nodes openshift-node-{6,3,7,9} "ha-cache=geo"
----
====

. OpenShift's *ipfailover* internally uses *keepalived*, so ensure that
multicast is enabled on the nodes labeled above and that the nodes can accept
network traffic for 224.0.0.18 (the VRRP multicast IP address). Depending on
your environment's multicast configuration, you may need to add an `iptables`
rule to each of the above labeled nodes. If you do need to add the `iptables`
rules, please also ensure that the rules persist after a system restart:
+
====
----
$ for node in openshift-node-{6,3,7,9}; do   ssh $node <<EOF
export interface=${interface:-"eth0"}
echo "Check multicast enabled ... ";
ifconfig $interface | grep -i MULTICAST

echo "Check multicast groups ... "
netstat -g -n | grep 224.0.0 | grep $interface

echo "Optionally, add accept rule and persist it ... "
sudo /sbin/iptables -I INPUT -i $interface -d 224.0.0.18/32 -j ACCEPT

echo "Please ensure the above rule is added on system restarts."

EOF
done;
----
====

. Create a new *ipfailover* service account:
+
====
----
$ echo '
  { "kind": "ServiceAccount",
    "apiVersion": "v1",
    "metadata": { "name": "ipfailover" }
  }
  ' | oc create -f -
----
====

. You can manually edit the *privileged* SCC and add the
*ipfailover* service account, or you can script editing the *privileged* SCC if
you have `jq` installed.

.. To manually edit the *privileged* SCC, run:
+
====
----
$ oc edit scc privileged
----
====
+
Then add the *ipfailover* service account in form
*system:serviceaccount:<project>:<name>* to the `*users*` section:
+
====
----
...
users:
- system:serviceaccount:openshift-infra:build-controller
- system:serviceaccount:default:router
- system:serviceaccount:default:ipfailover
----
====

.. Alternatively, to script editing *privileged* SCC if you have `jq` installed,
run:
+
====
----
$ oc get scc privileged -o json |
    jq '.users |= .+ ["system:serviceaccount:default:ipfailover"]' |
      oc replace scc -f -
----
====

. Run a *geo-cache* service with two or more replicas. An example configuration
for running a *geo-cache* service
https://raw.githubusercontent.com/openshift/openshift-docs/master/admin_guide/examples/geo-cache.json[is
provided here].
+
[IMPORTANT]
====
Be sure to replace the *myimages/geo-cache* Docker image referenced in the
file with your intended image. Also, change the number of replicas to the
desired amount and ensure the label matches the one used in the first step.
====
+
----
$ oc create -n <namespace> -f ./examples/geo-cache.json
----

. Finally, configure the VIPs and failover for the nodes labeled with
*ha-cache=geo* in the first step. Ensure the number of replicas match the number
of nodes and that they satisfy the label setup in first step. Specify the VIP
addresses and the port number that *ipfailover* should monitor for the desired
instances:
+
ifdef::openshift-enterprise[]
====
----
$ oadm ipfailover ipf-ha-geo-cache \
    --replicas=4 --selector="ha-cache=geo" \
    --virtual-ips=10.245.2.101-104 --watch-port=9736  \
    --credentials=/etc/origin/master/openshift-router.kubeconfig \
    --service-account=ipfailover --create
----
====
endif::[]
ifdef::openshift-origin[]
====
----
$ oadm ipfailover ipf-ha-geo-cache \
    --replicas=4 --selector="ha-cache=geo" \
    --virtual-ips=10.245.2.101-104 --watch-port=9736 \
    --credentials="$KUBECONFIG" \
    --service-account=ipfailover --create
----
====
endif::[]
////
+
As an alternative, the following example creates an IP failover configuration on
a selection of nodes labeled "my-ha-service=har-reporter" (on 4 nodes with 7
VIPs monitoring a service listening on port 4242:
+
====
----
$ oadm ipfailover harreporter --selector="my-ha-service=har-reporter" --virtual-ips="10.245.2.42,10.245.2.100-104,10.245.2.142,10.245.2.242" --watch-port=4242 --replicas=7 --service-account=ipfailover --create
----
====
////

Using the above example, you can now use the VIPs 10.245.2.101 through
10.245.2.104 to send traffic to the geo-cache service. If a particular geo-cache
instance is "unreachable", perhaps due to a node failure, Keepalived ensures
that the VIPs automatically float amongst the group of nodes labeled
"ha-cache=geo" and the service is still reachable via the virtual IP addresses.
