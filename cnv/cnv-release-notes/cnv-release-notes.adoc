[id="cnv-release-notes"]
= {RN_BookName}
include::modules/cnv-document-attributes.adoc[]
:context: cnv-release-notes
toc::[]


== About {CNVProductName}

include::modules/cnv-what-you-can-do-with-cnv.adoc[leveloffset=+2]

=== {CNVProductNameStart} support

:FeatureName: {CNVProductName}
include::modules/technology-preview.adoc[leveloffset=+2]

== New and changed features

=== Web console improvements

* The {product-title} dashboard captures high-level information about clusters.
From the {product-title} web console, access the dashboard by clicking
*Home -> Dashboards -> Overview*. Note that virtual machines are no longer listed
in the web console project overview. Virtual machines are now listed within the
*Cluster Inventory* dashboard card.

=== Other improvements

* After you install {CNVProductName}, MAC pool manager automatically starts.
If you define a secondary NIC without specifying the MAC address, the MAC pool
manager allocates a unique MAC address to the NIC.
+
[NOTE]
====
If you define a secondary NIC with a specific MAC address, it is possible
that the MAC address might conflict with another NIC in the cluster.
====


== Resolved issues

* Previously, if you used the web console to create a virtual machine template
that had the same name as an existing virtual machine, the operation failed.
This resulted in the message `Name is already used by another virtual machine`.
This issue is fixed in {CNVProductName} 2.1.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1717802[*BZ#1717802*])

* Previously, if you created a virtual machine with the Pod network connected in
`bridge` mode and used a `cloud-init` disk, the virtual machine lost its network
connectivity after being restarted. This issue is fixed in {CNVProductName} 2.1.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1708680[*BZ#1708680*])


== Known issues

// Don't remove: this BZ is probably true for all 2.x releases
* The `masquerade` binding method for virtual machines cannot be used in clusters with RHEL 7 compute nodes. 
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1741626[*BZ#1741626*])

* When adding a disk to a virtual machine via the *Disks* tab in the web console,
the added disk always has a `Filesystem` volumeMode, regardless of the volumeMode
set in the `kubevirt-storage-class-default` ConfigMap.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1753688[*BZ#1753688*])

* After migration, a virtual machine is assigned a new IP address. However, the
commands `oc get vmi` and `oc describe vmi` still generate output containing the
obsolete IP address. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1686208[*BZ#1686208*])
+
** As a workaround, view the correct IP address by running the following command:
+
----
$ oc get pod -o wide
----

* The virtual machines wizard does not load for users without administrator
privileges. This issue is caused by missing permissions that allow users to load
network attachment definitions.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1743985[*BZ#1743985*])
+
** As a workaround, provide the user with permissions to load the network attachment
definitions.
+
. Define `ClusterRole` and `ClusterRoleBinding` objects to the YAML configuration
file, using the following examples:
+
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
 name: cni-resources
rules:
- apiGroups: ["k8s.cni.cncf.io"]
 resources: ["*"]
 verbs: ["*"]
----
+
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: <role-binding-name>
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cni-resources
subjects:
- kind: User
  name: <user to grant the role to>
  namespace: <namespace of the user>
----
+
. As a `cluster-admin` user, run the following command to create the `ClusterRole`
and `ClusterRoleBinding` objects you defined:
+
----
$ oc create -f <filename>.yaml
----

* When navigating to the *Virtual Machines Console* tab, sometimes no content
is displayed. As a workaround, use the serial console.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1753606[*BZ#1753606*])

* When you attempt to list all instances of the {CNVProductName} operator from a
browser, you receive a 404 (page not found) error.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1757526[*BZ#1757526*])
+
** As a workaround, run the following command:
+
----
$ oc get pods -n openshift-cnv | grep operator
----

* Some resources are improperly retained when removing {CNVProductName}. You
must manually remove these resources in order to reinstall {CNVProductName}.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1712429[*BZ#1712429*]),
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1757705[*BZ#1757705*])
+
** As a workaround, follow this procedure:
link:https://access.redhat.com/articles/4484961[Removing leftover resources from container-native virtualization 2.1 uninstallation]

* If a virtual machine uses guaranteed CPUs, it will not be scheduled, because
the label `cpumanager=true` is not automatically set on nodes. As a
workaround, remove the `CPUManager` entry from the `kubevirt-config` ConfigMap.
Then, manually label the nodes with `cpumanager=true` before running virtual
machines with guaranteed CPUs on your cluster.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1718944[*BZ#1718944*])

* Live migration fails when nodes have different CPU models. Even in cases where
nodes have the same physical CPU model, differences introduced by microcode
updates have the same effect. This is because the default settings trigger
host CPU passthrough behavior, which is incompatible with live migration.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1760028[*BZ#1760028*])
+
** As a workaround, set the default CPU model in the `kubevirt-config` ConfigMap,
as shown in the following example:
+
[NOTE]
====
You must make this change before starting the virtual machines that support
live migration.
====
+
. Open the `kubevirt-config` ConfigMap for editing by running the following command:
+
----
$ oc edit configmap kubevirt-config -n openshift-cnv
----
+
. Edit the ConfigMap:
+
[source,yaml]
----
kind: ConfigMap
metadata:
  name: kubevirt-config
data:
  default-cpu-model: "<cpu-model>" <1>
----
<1> Replace `<cpu-model>` with the actual CPU model value. You can determine this
value by running `oc describe node <node>` for all nodes and looking at the
`cpu-model-<name>` labels. Select the CPU model that is present on all of your
nodes.

* The {CNVProductName} upgrade process occasionally fails due to an interruption
from the Operator Lifecycle Manager (OLM). This issue is caused by the limitations
associated with using a declarative API to track the state of {CNVProductName}
Operators. Enabling automatic updates during
xref:../cnv-install/installing-container-native-virtualization.adoc#cnv-subscribing-to-hco-catalog_installing-container-native-virtualization[installation]
decreases the risk of encountering this issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1759612[*BZ#1759612*])

* {CNVProductNameStart} cannot reliably identify node drains that are triggered by
running either `oc adm drain` or `kubectl drain`. Do not run these commands on
the nodes of any clusters where {CNVProductName} is deployed. The nodes might not
drain if there are virtual machines running on top of them.
The current solution is to put nodes into maintenance.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1707427[*BZ#1707427*])

* Under unclear circumstances, clicking on the edit channel button in the subscription
view in the Operator Lifecycle Manager (OLM) console can occasionally produce
no visible results, with no visible errors.
+
As a workaround, the upgrade process from {CNVProductNameStart} 2.1 to {CNVProductNameStart}
2.2 can be triggered from CLI with a single oc patch command:
+
[source,bash]
----
TARGET_NAMESPACE=openshift-cnv HCO_CHANNEL=2.2 oc patch -n "${TARGET_NAMESPACE}" $(oc get subscription -n ${TARGET_NAMESPACE} --no-headers -o name) --type='json' -p='[{"op": "replace", "path": "/spec/channel", "value":"'"${HCO_CHANNEL}"'"}, {"op": "replace", "path": "/spec/installPlanApproval", "value":"Automatic"}]'
----
+
This command will edit the existing subscription pointing to channel "2.2"
and enabling automatic updates. At the end of the upgrade process the user can
eventually revert back to Manual update policy if he wishes.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1796410[*BZ#1796410])
