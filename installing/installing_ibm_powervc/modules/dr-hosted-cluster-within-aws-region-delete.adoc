// Module included in the following assembly:
//
// * hosted_control_planes/hcp_high_availability/hcp-backup-restore-aws.adoc

:_mod-docs-content-type: PROCEDURE
[id="dr-hosted-cluster-within-aws-region-delete_{context}"]
= Deleting a hosted cluster from your source management cluster

After you back up your hosted cluster and restore it to your destination management cluster, you shut down and delete the hosted cluster on your source management cluster.

.Prerequisites

You backed up your data and restored it to your source management cluster.

[TIP]
====
Ensure that the `kubeconfig` file of the destination management cluster is placed as it is set in the `KUBECONFIG` variable or, if you use the script, in the `MGMT_KUBECONFIG` variable. Use `export KUBECONFIG=<Kubeconfig FilePath>` or, if you use the script, use `export KUBECONFIG=${MGMT_KUBECONFIG}`.
====

.Procedure

. Scale the `deployment` and `statefulset` objects by entering these commands:
+
[IMPORTANT]
====
Do not scale the stateful set if the value of its `spec.persistentVolumeClaimRetentionPolicy.whenScaled` field is set to `Delete`, because this could lead to a loss of data.

As a workaround, update the value of the `spec.persistentVolumeClaimRetentionPolicy.whenScaled` field to `Retain`. Ensure that no controllers exist that reconcile the stateful set and would return the value back to `Delete`, which could lead to a loss of data.
====
+
[source,terminal]
----
$ export KUBECONFIG=${MGMT_KUBECONFIG}
----
+
.Scale down deployment commands
[source,terminal]
----
$ oc scale deployment -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --replicas=0 --all
----
+
[source,terminal]
----
$ oc scale statefulset.apps -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --replicas=0 --all
----
+
[source,terminal]
----
$ sleep 15
----

. Delete the `NodePool` objects by entering these commands:
+
[source,terminal]
----
NODEPOOLS=$(oc get nodepools -n ${HC_CLUSTER_NS} -o=jsonpath='{.items[?(@.spec.clusterName=="'${HC_CLUSTER_NAME}'")].metadata.name}')
if [[ ! -z "${NODEPOOLS}" ]];then
    oc patch -n "${HC_CLUSTER_NS}" nodepool ${NODEPOOLS} --type=json --patch='[ { "op":"remove", "path": "/metadata/finalizers" }]'
    oc delete np -n ${HC_CLUSTER_NS} ${NODEPOOLS}
fi
----

. Delete the `machine` and `machineset` objects by entering these commands:
+
[source,terminal]
----
# Machines
for m in $(oc get machines -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name); do
    oc patch -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${m} --type=json --patch='[ { "op":"remove", "path": "/metadata/finalizers" }]' || true
    oc delete -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${m} || true
done
----
+
[source,terminal]
----
$ oc delete machineset -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --all || true
----

. Delete the cluster object by entering these commands:
+
[source,terminal]
----
$ C_NAME=$(oc get cluster -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name)
----
+
[source,terminal]
----
$ oc patch -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${C_NAME} --type=json --patch='[ { "op":"remove", "path": "/metadata/finalizers" }]'
----
+
[source,terminal]
----
$ oc delete cluster.cluster.x-k8s.io -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --all
----

. Delete the AWS machines (Kubernetes objects) by entering these commands. Do not worry about deleting the real AWS machines. The cloud instances will not be affected.
+
[source,terminal]
----
for m in $(oc get awsmachine.infrastructure.cluster.x-k8s.io -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name)
do
    oc patch -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${m} --type=json --patch='[ { "op":"remove", "path": "/metadata/finalizers" }]' || true
    oc delete -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${m} || true
done
----

. Delete the `HostedControlPlane` and `ControlPlane` HC namespace objects by entering these commands:
+
.Delete HCP and ControlPlane HC NS commands
[source,terminal]
----
$ oc patch -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} hostedcontrolplane.hypershift.openshift.io ${HC_CLUSTER_NAME} --type=json --patch='[ { "op":"remove", "path": "/metadata/finalizers" }]'
----
+
[source,terminal]
----
$ oc delete hostedcontrolplane.hypershift.openshift.io -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --all
----
+
[source,terminal]
----
$ oc delete ns ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} || true
----

. Delete the `HostedCluster` and HC namespace objects by entering these commands:
+
.Delete HC and HC Namespace commands
[source,terminal]
----
$ oc -n ${HC_CLUSTER_NS} patch hostedclusters ${HC_CLUSTER_NAME} -p '{"metadata":{"finalizers":null}}' --type merge || true
----
+
[source,terminal]
----
$ oc delete hc -n ${HC_CLUSTER_NS} ${HC_CLUSTER_NAME}  || true
----
+
[source,terminal]
----
$ oc delete ns ${HC_CLUSTER_NS} || true
----

.Verification

* To verify that everything works, enter these commands:
+
.Validations commands
[source,terminal]
----
$ export KUBECONFIG=${MGMT2_KUBECONFIG}
----
+
[source,terminal]
----
$ oc get hc -n ${HC_CLUSTER_NS}
----
+
[source,terminal]
----
$ oc get np -n ${HC_CLUSTER_NS}
----
+
[source,terminal]
----
$ oc get pod -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}
----
+
[source,terminal]
----
$ oc get machines -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}
----
+
.Commands for inside the HostedCluster
[source,terminal]
----
$ export KUBECONFIG=${HC_KUBECONFIG}
----
+
[source,terminal]
----
$ oc get clusterversion
----
+
[source,terminal]
----
$ oc get nodes
----

.Next steps

Delete the OVN pods in the hosted cluster so that you can connect to the new OVN control plane that runs in the new management cluster:

. Load the `KUBECONFIG` environment variable with the hosted cluster's kubeconfig path.

. Enter this command:
+
[source,terminal]
----
$ oc delete pod -n openshift-ovn-kubernetes --all
----
