:_mod-docs-content-type: PROCEDURE
[id="installation-manual-recovering-when-auto-recovery-is-unavail_{context}"]
= Manually recovering from a disruption event when automated recovery is unavailable

You might need to perform manual recovery steps if a disruption event prevents fencing from functioning correctly. In this case, you can run commands directly on the control plane nodes to recover the cluster. There are four main recovery scenarios, which should be attempted in the following order:

. Update fencing secrets:  Refresh the Baseboard Management Console (BMC) credentials if they are incorrect or outdated.
. Recover from a single-node failure: Restore functionality when only one control plane node is down.
. Recover from a complete node failure: Restore functionality when both control plane nodes are down.
. Replace a control plane node that cannot be recovered: Replace the node to restore cluster functionality.

.Prerequisites

* You have administrative access to the control plane nodes.
* You can connect to the nodes by using SSH.

[NOTE]
====
Do an etcd backup before proceeding to ensure that you can restore the cluster if any issues occur.
====

.Procedure

. Update the fencing secrets:

.. If the Cluster API is unavilable, update fencing secret by running the following command on one of the cluster nodes:
+
[source,terminal]
----
$ sudo pcs stonith update <node_name>_redfish username=<user_name> password=<password>
----
+
After the Cluster API recovers, or the Cluster API is already available, update fencing secret in the cluster to ensure it stays in sync, as described in the following step.

.. Edit the username and password for the existing fencing secret for the control plane node by running the following commads:
+
[source,terminal]
----
$ oc project openshift-etcd
----
+
[source,terminal]
----
$ oc edit secret <node_name>-fencing
----
+
If the cluster recovers after updating the fencing secrets, no further action is required. If the issue persists, proceed to the next step.

. Recover from a single-node failure:

.. Gather initial diagnostics by running the following command:
+
[source,terminal]
----
$ sudo pcs status --full
----
+
This command provides a detailed view of the current cluster and resource states. You can use the output to identify issues with fencing or etcd startup.

.. Run the following additional diagnostic commands, if necessary:
+
Reset the resources on your cluster and instruct Pacemaker to attempt to start them fresh by running the following command:
+
[source,terminal]
----
$ sudo pcs resource cleanup
----
+
Review all Pacemaker activity on the node by running the following command:
+
[source,terminal]
----
$ sudo journalctl -u pacemaker
----
+
Diagnose etcd resource startup issues by running the following command:
+
[source,terminal]
----
$ sudo journalctl -u pacemaker | grep podman-etcd
----

.. View the fencing configuration for the node by running the following command:
+
[source,terminal]
----
$ sudo pcs stonith config <node_name>_redfish
----
+
If fencing is required but is not functioning, ensure that the Redfish fencing endpoint is accessible and verify that the credentials are correct.

.. If etcd is not starting despite fencing being operational, restore etcd from a backup by running the following commands:
+
[source,terminal]
----
$ sudo cp -r /var/lib/etcd-backup/* /var/lib/etcd/
----
+
[source,terminal]
----
$ sudo chown -R etcd:etcd /var/lib/etcd
----
+
If the recovery is successful, no further action is required. If the issue persists, proceed to the next step.

. Recover from a complete node failure:

.. Power on both control plane nodes.
+
Pacemaker starts automatically and begins the recovery operation when it detects both nodes are online. If the recovery does not start as expected, use the diagnostic commands described in the previous step to investigate the issue.

.. Reset the resources on your cluster and instruct Pacemaker to attempt to start them fresh by running the following command:
+
[source,terminal]
----
$ sudo pcs resource cleanup
----

.. Check resource start order by running the following command:
+
[source,terminal]
----
$ sudo pcs status --full
----

.. Inspect the pacemaker service journal if kubelet fails by running the following commands:
+
[source,terminal]
----
$ sudo journalctl -u pacemaker
----
+
[source,terminal]
----
$ sudo journalctl -u kubelet
----

.. Handle out-of-sync etcd. 
+
If one node has a more up-to-date etcd, Pacemaker attempts to fence the lagging node and start it as a learner. If this process stalls, verify the Redfish fencing endpoint and credentials by running the following command:
+
[source,terminal]
----
$ sudo pcs stonith config
----
+
If the recovery is successful, no further action is required. If the issue persists, perform manual recovery as described in the next step.

. If you need to manually recover from an event when one of the nodes is not recoverable, follow the procedure in "Replacing control plane nodes in a two-node OpenShift cluster".
+
When a cluster loses a single node, it enters the degraded mode. In this state, Pacemaker automatically unblocks quorum and allows the cluster to temporarily operate on the remaining node.
+
If both nodes fail, you must restart both nodes to reestablish quorum so that Pacemaker can resume normal cluster operations.
+
If only one of the two nodes can be restarted, follow the node replacement procedure to manually reestablish quorum on the surviving node.
+
If manual recovery is still required and it fails, collect a must-gather and SOS report, and file a bug.

.Verification

For information about verifying that both control plane nodes and etcd are operating correctly, see "Verifying etcd health in a two-node OpenShift cluster with fencing".
