// Module included in the following assemblies:
//
// * machine_management/cpmso-configuration.adoc

:_mod-docs-content-type: REFERENCE
[id="cpmso-yaml-provider-spec-nutanix_{context}"]
= Sample Nutanix provider specification

When you create a control plane machine set for an existing cluster, the provider specification must match the `providerSpec` configuration in the control plane machine custom resource (CR) that the installation program creates.


[id="cpmso-yaml-provider-spec-nutanix-oc_{context}"]
== Values obtained by using the OpenShift CLI

In the following example, you can obtain some of the values for your cluster by using the OpenShift CLI.

Infrastructure ID:: The `<cluster_id>` string is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
+
[source,terminal]
----
$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster
----

.Sample Nutanix `providerSpec` values
[source,yaml]
----
apiVersion: machine.openshift.io/v1
kind: ControlPlaneMachineSet
metadata:
  name: cluster
  namespace: openshift-machine-api
spec:
# ...
  template:
# ...
      spec:
        providerSpec:
          value:
            apiVersion: machine.openshift.io/v1
            bootType: "" <1>
            categories: <2>
            - key: <category_name>
              value: <category_value>
            cluster: <3>
              type: uuid
              uuid: <cluster_uuid>
            credentialsSecret:
              name: nutanix-credentials <4>
            image: <5>
              name: <cluster_id>-rhcos
              type: name
            kind: NutanixMachineProviderConfig <6>
            memorySize: 16Gi <7>
            metadata:
              creationTimestamp: null
            project: <8>
              type: name
              name: <project_name>
            subnets: <9>
            - type: uuid
              uuid: <subnet_uuid>
            systemDiskSize: 120Gi <10>
            userDataSecret:
              name: master-user-data <11>
            vcpuSockets: 8 <12>
            vcpusPerSocket: 1 <13>
----
<1> Specifies the boot type that the control plane machines use. For more information about boot types, see link:https://portal.nutanix.com/page/documents/kbs/details?targetId=kA07V000000H3K9SAK[Understanding UEFI, Secure Boot, and TPM in the Virtualized Environment]. Valid values are `Legacy`, `SecureBoot`, or `UEFI`. The default is `Legacy`.
+
[NOTE]
====
You must use the `Legacy` boot type in {product-title} {product-version}.
====
<2> Specifies one or more Nutanix Prism categories to apply to control plane machines. This stanza requires `key` and `value` parameters for a category key-value pair that exists in Prism Central. For more information about categories, see link:https://portal.nutanix.com/page/documents/details?targetId=Prism-Central-Guide-vpc_2022_6:ssp-ssp-categories-manage-pc-c.html[Category management].
<3> Specifies a Nutanix Prism Element cluster configuration. In this example, the cluster type is `uuid`, so there is a `uuid` stanza.
+
[NOTE]
====
Clusters that use {product-title} version 4.15 or later can use failure domain configurations.

If the cluster uses a failure domain, configure this parameter in the failure domain.
If you specify this value in the provider specification when using failure domains, the Control Plane Machine Set Operator ignores it.
====
<4> Specifies the secret name for the cluster. Do not change this value.
<5> Specifies the image that was used to create the disk.
<6> Specifies the cloud provider platform type. Do not change this value.
<7> Specifies the memory allocated for the control plane machines.
<8> Specifies the Nutanix project that you use for your cluster. In this example, the project type is `name`, so there is a `name` stanza.
<9> Specify one or more Prism Element subnet objects.
In this example, the subnet type is `uuid`, so there is a `uuid` stanza.
A maximum of 32 subnets for each Prism Element failure domain in the cluster is supported.
+
[IMPORTANT]
====
The following known issues with configuring multiple subnets for an existing Nutanix cluster by using a control plane machine set exist in {product-title} version 4.18:

* Adding subnets above the existing subnet in the `subnets` stanza causes a control plane node to become stuck in the `Deleting` state.
As a workaround, only add subnets below the existing subnet in the `subnets` stanza.

* Sometimes, after adding a subnet, the updated control plane machines appear in the Nutanix console but the {product-title} cluster is unreachable.
There is no workaround for this issue.

These issues occur on clusters that use a control plane machine set to configure subnets regardless of whether subnets are specified in a failure domain or the provider specification.
For more information, see link:https://issues.redhat.com/browse/OCPBUGS-50904[*OCPBUGS-50904*].
====
+
The CIDR IP address prefix for one of the specified subnets must contain the virtual IP addresses that the {product-title} cluster uses. 
All subnet UUID values must be unique.
+
[NOTE]
====
Clusters that use {product-title} version 4.15 or later can use failure domain configurations.

If the cluster uses a failure domain, configure this parameter in the failure domain.
If you specify this value in the provider specification when using failure domains, the Control Plane Machine Set Operator ignores it.
====
<10> Specifies the VM disk size for the control plane machines.
<11> Specifies the control plane user data secret. Do not change this value.
<12> Specifies the number of vCPU sockets allocated for the control plane machines.
<13> Specifies the number of vCPUs for each control plane vCPU socket.