// Module included in the following assemblies:
//
// * hardware_accelerators/about-hardware-accelerators.adoc

:_mod-docs-content-type: CONCEPT
[id="nvidia-gpu-csps_{context}"]
ifndef::openshift-dedicated,openshift-rosa[]
= GPUs and CSPs

endif::openshift-dedicated,openshift-rosa[]
ifdef::openshift-rosa,openshift-dedicated[]
= GPUs and {product-title}

endif::openshift-rosa,openshift-dedicated[]

ifndef::openshift-dedicated,openshift-rosa[]
You can deploy {product-title} to one of the major cloud service providers (CSPs): {aws-first}, {gcp-full}, or Microsoft Azure.

Two modes of operation are available: a fully managed deployment and a self-managed deployment.

* In a fully managed deployment, everything is automated by Red{nbsp}Hat in collaboration with CSP. You can request an OpenShift instance through the CSP web console, and the cluster is automatically created and fully managed by Red{nbsp}Hat. You do not have to worry about node failures or errors in the environment. Red{nbsp}Hat is fully responsible for maintaining the uptime of the cluster. The fully managed services are available on {aws-short}, {azure-short}, and {gcp-short}. For {aws-short}, the OpenShift service is called (Red{nbsp}Hat OpenShift Service on AWS). For Azure, the service is called Azure Red{nbsp}Hat OpenShift. For {gcp-short}, the service is called OpenShift Dedicated on {gcp-short}.

* In a self-managed deployment, you are responsible for instantiating and maintaining the OpenShift cluster. Red{nbsp}Hat provides the OpenShift-install utility to support the deployment of the OpenShift cluster in this case. The self-managed services are available globally to all CSPs.
endif::openshift-dedicated,openshift-rosa[]

ifdef::openshift-dedicated,openshift-rosa[]
You can deploy {product-title} on NVIDIA GPU instance types.
endif::openshift-dedicated,openshift-rosa[]

It is important that this compute instance is a GPU-accelerated compute instance and that the GPU type matches the list of supported GPUs from NVIDIA AI Enterprise. For example, T4, V100, and A100 are part of this list.

You can choose one of the following methods to access the containerized GPUs:

* GPU passthrough to access and use GPU hardware within a virtual machine (VM).

* GPU (vGPU) time slicing when the entire GPU is not required.
