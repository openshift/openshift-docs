
// Module included in the following assemblies:
//
// * assemblies/osd-service-definition.adoc

[id="sdpolicy-platform_{context}"]
= Platform

[id="cluster-backup-policy_{context}"]
== Cluster backup policy

[IMPORTANT]
====
It is critical that customers have a backup plan for their applications and application data.
====
Application and application data backups are not a part of the {product-title} service.
All Kubernetes objects in each {product-title} cluster are backed up to facilitate a prompt recovery in the unlikely event that a cluster becomes irreparably inoperable.

The backups are stored in a secure object storage (Multi-AZ) bucket in the same account as the cluster.
Node root volumes are not backed up because Red Hat Enterprise Linux CoreOS is fully managed by the {OCP} cluster and no stateful data should be stored on the root volume of a node.

The following table shows the frequency of backups:
[cols="4",options="header"]
|===

|Component
|Snapshot Frequency
|Retention
|Notes

|Full object store backup
|Daily at 0100 UTC
|7 days
|This is a full backup of all Kubernetes objects. No persistent volumes (PVs) are backed up in this backup schedule.

|Full object store backup
|Weekly on Mondays at 0200 UTC
|30 days
|This is a full backup of all Kubernetes objects. No PVs are backed up in this backup schedule.

|Full object store backup
|Hourly at 17 minutes past the hour
|24 hours
|This is a full backup of all Kubernetes objects. No PVs are backed up in this backup schedule.

|===

[id="autoscaling_{context}"]
== Autoscaling
Node autoscaling is not available on {product-title} at this time.

[id="daemon-sets_{context}"]
== Daemon sets
Customers may create and run DaemonSets on {product-title}. In order to restrict DaemonSets to only running on worker nodes, use the following nodeSelector:

[source,yaml]
----
...
spec:
  nodeSelector:
    role: worker
...
----

[id="multi-availability-zones_{context}"]
== Multiple availability zone
In a multiple availability zone cluster, control nodes are distributed across availability zones and at least three worker nodes are required in each availability zone.

[id="node-labels_{context}"]
== Node labels
Custom node labels are created by Red Hat during node creation and cannot be changed on {product-title} clusters at this time.

[id="openshift-version_{context}"]
== OpenShift version
{product-title} is run as a service and is kept up to date with the latest {OCP} version.

[id="upgrades_{context}"]
== Upgrades
Refer to link:https://access.redhat.com/support/policy/updates/openshift/dedicated[{product-title} Life Cycle] for more information on the upgrade policy and procedures.

[id="windows-containers_{context}"]
== Windows containers
Windows containers are not available on {product-title} at this time.

[id="container-engine_{context}"]
== Container engine
{product-title} runs on OpenShift 4 and uses link:https://www.redhat.com/en/blog/red-hat-openshift-container-platform-4-now-defaults-cri-o-underlying-container-engine[CRI-O ] as the only available container engine.

[id="operating-system_{context}"]
== Operating system
{product-title} runs on OpenShift 4 and uses Red Hat Enterprise Linux CoreOS as the operating system for all control plane and worker nodes.

[id="kubernetes-operator-support_{context}"]
== Kubernetes Operator support
All Operators listed in the OperatorHub marketplace should be available for installation. Operators installed from OperatorHub, including Red Hat Operators, are not SRE managed as part of the {product-title} service. Refer to the link:https://access.redhat.com/solutions/4807821[Red Hat Customer Portal] for more information on the supportability of a given Operator.
