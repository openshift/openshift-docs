:_content-type: PROCEDURE
[id="cluster-logging-collector-log-forward-logs-from-application-pods_{context}"]
= Forwarding application logs from specific pods

As a cluster administrator, you can use Kubernetes pod labels to gather log data from specific pods and forward it to a log collector.

Suppose that you have an application composed of pods running alongside other pods in various namespaces. If those pods have labels that identify the application, you can gather and output their log data to a specific log collector.

To specify the pod labels, you use one or more `matchLabels` key-value pairs. If you specify multiple key-value pairs, the pods must match all of them to be selected.

.Procedure

. Create or edit a YAML file that defines the `ClusterLogForwarder` CR object. In the file, specify the pod labels using simple equality-based selectors under `inputs[].name.application.selector.matchLabels`, as shown in the following example.
+
.Example `ClusterLogForwarder` CR YAML file
[source,yaml]
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance <1>
  namespace: openshift-logging <2>
spec:
  pipelines:
    - inputRefs: [ myAppLogData ] <3>
      outputRefs: [ default ] <4>
      parse: json <5>
  inputs: <6>
    - name: myAppLogData
      application:
        selector:
          matchLabels: <7>
            environment: production
            app: nginx
        namespaces: <8>
        - app1
        - app2
  outputs: <9>
    - default
    ...
----
<1> The name of the `ClusterLogForwarder` CR must be `instance`.
<2> The namespace for the `ClusterLogForwarder` CR must be `openshift-logging`.
<3> Specify one or more comma-separated values from `inputs[].name`.
<4> Specify one or more comma-separated values from `outputs[]`.
<5> Optional: Specify whether to forward structured JSON log entries as JSON objects in the `structured` field. The log entry must contain valid structured JSON; otherwise, OpenShift Logging removes the `structured` field and instead sends the log entry to the default index, `app-00000x`.
<6> Define a unique `inputs[].name` for each application that has a unique set of pod labels.
<7> Specify the key-value pairs of pod labels whose log data you want to gather. You must specify both a key and value, not just a key. To be selected, the pods must match all the key-value pairs.
<8> Optional: Specify one or more namespaces.
<9> Specify one or more outputs to forward your log data to. The optional `default` output shown here sends log data to the internal Elasticsearch instance.

. Optional: To restrict the gathering of log data to specific namespaces, use `inputs[].name.application.namespaces`, as shown in the preceding example.

. Optional: You can send log data from additional applications that have different pod labels to the same pipeline.
.. For each unique combination of pod labels, create an additional `inputs[].name` section similar to the one shown.
.. Update the `selectors` to match the pod labels of this application.
.. Add the new `inputs[].name` value to `inputRefs`. For example:
+
----
- inputRefs: [ myAppLogData, myOtherAppLogData ]
----

. Create the CR object:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----

[role="_additional-resources"]
.Additional resources

* For more information on `matchLabels` in Kubernetes, see link:https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#resources-that-support-set-based-requirements[Resources that support set-based requirements].
