// Module included in the following assemblies:
//
// * networking/metallb/about-metallb.adoc

[id="nw-metallb-layer2_{context}"]
= MetalLB concepts for layer 2 mode

In layer 2 mode, the `speaker` pod on one node announces the external IP address for a service to the host network.
From a network perspective, the node appears to have multiple IP addresses assigned to a network interface.

The `speaker` pod responds to ARP requests for IPv4 services and NDP requests for IPv6.

In layer 2 mode, all traffic for a service IP address is routed through one node.
After traffic enters the node, the service proxy for the CNI network provider distributes the traffic to all the pods for the service.

Because all traffic for a service enters through a single node in layer 2 mode, in a strict sense, MetalLB does not implement a load balancer for layer 2.
Rather, MetalLB implements a failover mechanism for layer 2 so that when a `speaker` pod becomes unavailable, a `speaker` pod on a different node can announce the service IP address.

When a node becomes unavailable, failover is automatic.
The `speaker` pods on the other nodes detect that a node is unavailable and a new `speaker` pod and node take ownership of the service IP address from the failed node.

image::nw-metallb-layer2.png[Conceptual diagram for MetalLB and layer 2 mode]

The preceding graphic shows the following concepts related to MetalLB:

* An application is available through a service that has a cluster IP on the `172.130.0.0/16` subnet.
That IP address is accessible from inside the cluster.
The service also has an external IP address that MetalLB assigned to the service, `192.168.100.200`.

* Nodes 1 and 3 have a pod for the application.

* The `speaker` daemon set runs a pod on each node.
The MetalLB Operator starts these pods.

* Each `speaker` pod is a host-networked pod.
The IP address for the pod is identical to the IP address for the node on the host network.

* The `speaker` pod on node 1 uses ARP to announce the external IP address for the service, `192.168.100.200`.
The `speaker` pod that announces the external IP address must be on the same node as an endpoint for the service and the endpoint must be in the `Ready` condition.

* Client traffic is routed to the host network and connects to the `192.168.100.200` IP address.
After traffic enters the node, the service proxy sends the traffic to the application pod on the same node or another node according to the external traffic policy that you set for the service.

* If node 1 becomes unavailable, the external IP address fails over to another node.
On another node that has an instance of the application pod and service endpoint, the `speaker` pod begins to announce the external IP address, `192.168.100.200` and the new node receives the client traffic.
In the diagram, the only candidate is node 3.

