// Module included in the following assemblies:
//
// * nodes/nodes-cluster-resource-levels.adoc

:_content-type: PROCEDURE
[id="nodes-cluster-resource-levels-job_{context}"]
= Running the cluster capacity tool as a job inside a pod

Running the cluster capacity tool as a job inside of a pod has the advantage of
being able to be run multiple times without needing user intervention. Running
the cluster capacity tool as a job involves using a `ConfigMap` object.

.Prerequisites

Download and install link:https://github.com/kubernetes-incubator/cluster-capacity[the cluster capacity tool].

.Procedure

To run the cluster capacity tool:

. Create the cluster role:
+
[source,terminal]
----
$ cat << EOF| oc create -f -
----
+
.Example output
[source,terminal]
----
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cluster-capacity-role
rules:
- apiGroups: [""]
  resources: ["pods", "nodes", "persistentvolumeclaims", "persistentvolumes", "services", "replicationcontrollers"]
  verbs: ["get", "watch", "list"]
- apiGroups: ["apps"]
  resources: ["replicasets", "statefulsets"]
  verbs: ["get", "watch", "list"]
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["get", "watch", "list"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list"]
EOF
----

. Create the service account:
+
[source,terminal]
----
$ oc create sa cluster-capacity-sa
----

. Add the role to the service account:
+
[source,terminal]
----
$ oc adm policy add-cluster-role-to-user cluster-capacity-role \
    system:serviceaccount:default:cluster-capacity-sa
----

. Define and create the `Pod` spec:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: small-pod
  labels:
    app: guestbook
    tier: frontend
spec:
  containers:
  - name: php-redis
    image: gcr.io/google-samples/gb-frontend:v4
    imagePullPolicy: Always
    resources:
      limits:
        cpu: 150m
        memory: 100Mi
      requests:
        cpu: 150m
        memory: 100Mi
----

. The cluster capacity analysis is mounted in a volume using a
`ConfigMap` object named `cluster-capacity-configmap` to mount input pod spec file
`pod.yaml` into a volume `test-volume` at the path `/test-pod`.
+
If you haven't created a `ConfigMap` object, create one before creating the job:
+
----
$ oc create configmap cluster-capacity-configmap \
    --from-file=pod.yaml=pod.yaml
----

. Create the job using the below example of a job specification file:
+
[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: cluster-capacity-job
spec:
  parallelism: 1
  completions: 1
  template:
    metadata:
      name: cluster-capacity-pod
    spec:
        containers:
        - name: cluster-capacity
          image: openshift/origin-cluster-capacity
          imagePullPolicy: "Always"
          volumeMounts:
          - mountPath: /test-pod
            name: test-volume
          env:
          - name: CC_INCLUSTER <1>
            value: "true"
          command:
          - "/bin/sh"
          - "-ec"
          - |
            /bin/cluster-capacity --podspec=/test-pod/pod.yaml --verbose
        restartPolicy: "Never"
        serviceAccountName: cluster-capacity-sa
        volumes:
        - name: test-volume
          configMap:
            name: cluster-capacity-configmap
----
<1> A required environment variable letting the cluster capacity tool
 know that it is running inside a cluster as a pod.
 +
The `pod.yaml` key of the `ConfigMap` object is the same as the `Pod` spec file
name, though it is not required. By doing this, the input pod spec file can be
accessed inside the pod as `/test-pod/pod.yaml`.

. Run the cluster capacity image as a job in a pod:
+
[source,terminal]
----
$ oc create -f cluster-capacity-job.yaml
----

. Check the job logs to find the number of pods that can be scheduled in the
 cluster:
+
[source,terminal]
----
$ oc logs jobs/cluster-capacity-job
----
+
.Example output
[source,terminal]
----
small-pod pod requirements:
        - CPU: 150m
        - Memory: 100Mi

The cluster can schedule 52 instance(s) of the pod small-pod.

Termination reason: Unschedulable: No nodes are available that match all of the
following predicates:: Insufficient cpu (2).

Pod distribution among nodes:
small-pod
        - 192.168.124.214: 26 instance(s)
        - 192.168.124.120: 26 instance(s)
----
