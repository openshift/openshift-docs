:_mod-docs-content-type: ASSEMBLY
include::_attributes/common-attributes.adoc[]
include::_attributes/attributes-openshift-dedicated.adoc[]
[id="log-forwarding"]
= About log collection and forwarding
:context: log-forwarding

toc::[]

The {clo} deploys a collector based on the `ClusterLogForwarder` resource specification. There are two collector options supported by this Operator: the legacy Fluentd collector, and the Vector collector.

include::snippets/logging-fluentd-dep-snip.adoc[]

include::modules/about-log-collection.adoc[leveloffset=+1]

include::modules/logging-vector-fluentd-feature-comparison.adoc[leveloffset=+2]

include::modules/log-forwarding-collector-outputs.adoc[leveloffset=+2]

[id="log-forwarding-about-clf"]
== Log forwarding

Administrators can create `ClusterLogForwarder` resources that specify which logs are collected, how they are transformed, and where they are forwarded to.

`ClusterLogForwarder` resources can be used up to forward container, infrastructure, and audit logs to specific endpoints within or outside of a cluster. Transport Layer Security (TLS) is supported so that log forwarders can be configured to send logs securely.

Administrators can also authorize RBAC permissions that define which service accounts and users can access and forward which types of logs.

include::modules/log-forwarding-implementations.adoc[leveloffset=+1]

[id="log-forwarding-enabling-multi-clf-feature"]
== Enabling the multi log forwarder feature for a cluster

To use the multi log forwarder feature, you must create a service account and cluster role bindings for that service account. You can then reference the service account in the `ClusterLogForwarder` resource to control access permissions.

[IMPORTANT]
====
In order to support multi log forwarding in additional namespaces other than the `openshift-logging` namespace, you must xref:../../logging/cluster-logging-upgrading.adoc#logging-operator-upgrading-all-ns_cluster-logging-upgrading[update the {clo} to watch all namespaces]. This functionality is supported by default in new {clo} version 5.8 installations.
====

include::modules/log-collection-rbac-permissions.adoc[leveloffset=+2]

[role="_additional-resources"]
.Additional resources
ifdef::openshift-enterprise[]
* xref:../../authentication/using-rbac.adoc#using-rbac[Using RBAC to define and apply permissions]
* xref:../../authentication/using-service-accounts-in-applications.adoc#using-service-accounts-in-applications[Using service accounts in applications]
endif::[]
* link:https://kubernetes.io/docs/reference/access-authn-authz/rbac/[Using RBAC Authorization Kubernetes documentation]

include::modules/logging-create-clf.adoc[leveloffset=+1]

include::modules/logging-multiline-except.adoc[leveloffset=+1]

[id="log-forwarding-audit-logs"]
== Sending audit logs to the internal log store

By default, the {logging} sends container and infrastructure logs to the default internal log store defined in the `ClusterLogging` custom resource. However, it does not send audit logs to the internal store because it does not provide secure storage. If this default configuration meets your needs, you do not need to configure the Cluster Log Forwarder.

[NOTE]
====
To send audit logs to the internal Elasticsearch log store, use the Cluster Log Forwarder as described in xref:../../logging/config/cluster-logging-log-store.adoc#cluster-logging-elasticsearch-audit_cluster-logging-log-store[Forward audit logs to the log store].
====

include::modules/cluster-logging-collector-log-forwarding-about.adoc[leveloffset=+1]

include::modules/cluster-logging-forwarding-separate-indices.adoc[leveloffset=+1]

include::modules/cluster-logging-collector-log-forward-es.adoc[leveloffset=+1]

include::modules/cluster-logging-collector-log-forward-fluentd.adoc[leveloffset=+1]

include::modules/cluster-logging-collector-log-forward-syslog.adoc[leveloffset=+1]

include::modules/cluster-logging-collector-log-forward-kafka.adoc[leveloffset=+1]

include::modules/cluster-logging-collector-log-forward-cloudwatch.adoc[leveloffset=+1]

[id="cluster-logging-collector-log-forward-sts-cloudwatch_{context}"]
=== Forwarding logs to Amazon CloudWatch from STS enabled clusters

For clusters with AWS Security Token Service (STS) enabled, you can create an AWS service account manually or create a credentials request by using the
ifdef::openshift-enterprise,openshift-origin[]
xref:../../authentication/managing_cloud_provider_credentials/about-cloud-credential-operator.adoc[Cloud Credential Operator(CCO)]
endif::[]
ifdef::openshift-rosa,openshift-dedicated[]
link:https://docs.openshift.com/container-platform/latest/authentication/managing_cloud_provider_credentials/about-cloud-credential-operator.html[Cloud Credential Operator(CCO)]
endif::[]
 utility `ccoctl`.

.Prerequisites

* {logging-title-uc}: 5.5 and later

.Procedure

. Create a `CredentialsRequest` custom resource YAML by using the template below:
+
.CloudWatch credentials request template
[source,yaml]
----
apiVersion: cloudcredential.openshift.io/v1
kind: CredentialsRequest
metadata:
  name: <your_role_name>-credrequest
  namespace: openshift-cloud-credential-operator
spec:
  providerSpec:
    apiVersion: cloudcredential.openshift.io/v1
    kind: AWSProviderSpec
    statementEntries:
      - action:
          - logs:PutLogEvents
          - logs:CreateLogGroup
          - logs:PutRetentionPolicy
          - logs:CreateLogStream
          - logs:DescribeLogGroups
          - logs:DescribeLogStreams
        effect: Allow
        resource: arn:aws:logs:*:*:*
  secretRef:
    name: <your_role_name>
    namespace: openshift-logging
  serviceAccountNames:
    - logcollector
----
+
. Use the `ccoctl` command to create a role for AWS using your `CredentialsRequest` CR. With the `CredentialsRequest` object, this `ccoctl` command creates an IAM role with a trust policy that is tied to the specified OIDC identity provider, and a permissions policy that grants permissions to perform operations on CloudWatch resources. This command also creates a YAML configuration file in `/<path_to_ccoctl_output_dir>/manifests/openshift-logging-<your_role_name>-credentials.yaml`. This secret file contains the `role_arn` key/value used during authentication with the AWS IAM identity provider.
+
[source,terminal]
----
$ ccoctl aws create-iam-roles \
--name=<name> \
--region=<aws_region> \
--credentials-requests-dir=<path_to_directory_with_list_of_credentials_requests>/credrequests \
--identity-provider-arn=arn:aws:iam::<aws_account_id>:oidc-provider/<name>-oidc.s3.<aws_region>.amazonaws.com <1>
----
<1> <name> is the name used to tag your cloud resources and should match the name used during your STS cluster install
+
. Apply the secret created:
[source,terminal]
+
----
$ oc apply -f output/manifests/openshift-logging-<your_role_name>-credentials.yaml
----
+
. Create or edit a `ClusterLogForwarder` custom resource:
+
[source,yaml]
----
apiVersion: "logging.openshift.io/v1"
kind: ClusterLogForwarder
metadata:
  name: instance <1>
  namespace: openshift-logging <2>
spec:
  outputs:
   - name: cw <3>
     type: cloudwatch <4>
     cloudwatch:
       groupBy: logType <5>
       groupPrefix: <group prefix> <6>
       region: us-east-2 <7>
     secret:
        name: <your_role_name> <8>
  pipelines:
    - name: to-cloudwatch <9>
      inputRefs: <10>
        - infrastructure
        - audit
        - application
      outputRefs:
        - cw <11>
----
<1> The name of the `ClusterLogForwarder` CR must be `instance`.
<2> The namespace for the `ClusterLogForwarder` CR must be `openshift-logging`.
<3> Specify a name for the output.
<4> Specify the `cloudwatch` type.
<5> Optional: Specify how to group the logs:
+
* `logType` creates log groups for each log type
* `namespaceName` creates a log group for each application name space. Infrastructure and audit logs are unaffected, remaining grouped by `logType`.
* `namespaceUUID` creates a new log groups for each application namespace UUID. It also creates separate log groups for infrastructure and audit logs.
<6> Optional: Specify a string to replace the default `infrastructureName` prefix in the names of the log groups.
<7> Specify the AWS region.
<8> Specify the name of the secret that contains your AWS credentials.
<9> Optional: Specify a name for the pipeline.
<10> Specify which log types to forward by using the pipeline: `application,` `infrastructure`, or `audit`.
<11> Specify the name of the output to use when forwarding logs with this pipeline.

[role="_additional-resources"]
.Additional resources
* link:https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html[AWS STS API Reference]

include::modules/cluster-logging-collector-log-forward-secret-cloudwatch.adoc[leveloffset=+2]

include::modules/cluster-logging-collector-log-forward-loki.adoc[leveloffset=+1]

include::modules/loki-rate-limit-errors.adoc[leveloffset=+2]

[role="_additional-resources"]
.Additional resources

* xref:../../logging/cluster-logging-exported-fields.adoc#cluster-logging-exported-fields-kubernetes_cluster-logging-exported-fields[Log Record Fields]

* link:https://grafana.com/docs/loki/latest/configuration/[Configuring Loki server]

ifndef::openshift-rosa[]
include::modules/cluster-logging-collector-log-forward-gcp.adoc[leveloffset=+1]
endif::openshift-rosa[]

include::modules/logging-forward-splunk.adoc[leveloffset=+1]

include::modules/logging-http-forward.adoc[leveloffset=+1]

include::modules/cluster-logging-collector-log-forward-project.adoc[leveloffset=+1]

include::modules/cluster-logging-collector-log-forward-logs-from-application-pods.adoc[leveloffset=+1]

[role="_additional-resources"]
.Additional resources

ifdef::openshift-enterprise,openshift-origin[]
* xref:../../networking/ovn_kubernetes_network_provider/logging-network-policy.adoc#logging-network-policy[Logging for egress firewall and network policy rules]
endif::[]
ifdef::openshift-rosa,openshift-dedicated[]
* link:https://docs.openshift.com/container-platform/latest/networking/ovn_kubernetes_network_provider/logging-network-policy.html#logging-network-policy[Logging for egress firewall and network policy rules]
endif::[]

include::modules/cluster-logging-troubleshooting-log-forwarding.adoc[leveloffset=+1]
