:_content-type: ASSEMBLY
[id="logging-reference-5-5"]
= Logging References
include::_attributes/common-attributes.adoc[]
:context: logging-reference

toc::[]

. Create the CR object:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
. Populate the CR object with the template you need.


== ClusterLogForwarder Templates
.ClusterLogFowarder Fields
[source,yaml]
----
apiVersion: "logging.openshift.io/v1"
kind: ClusterLogForwarder
metadata:
  name: instance <1>
  namespace: openshift-logging <2>
spec:
  outputs:
   - name: elasticsearch-secure <3>
     type: "elasticsearch"
     url: https://elasticsearch.secure.com:9200
     secret:
        name: elasticsearch
   - name: elasticsearch-insecure <4>
     type: "elasticsearch"
     url: http://elasticsearch.insecure.com:9200
   - name: kafka-app <5>
     type: "kafka"
     url: tls://kafka.secure.com:9093/app-topic
  inputs: <6>
   - name: my-app-logs
     application:
        namespaces:
        - my-project
  pipelines:
   - name: audit-logs <7>
     inputRefs:
      - audit
     outputRefs:
      - elasticsearch-secure
      - default
     parse: json <8>
     labels:
       secure: "true" <9>
       datacenter: "east"
   - name: infrastructure-logs <10>
     inputRefs:
      - infrastructure
     outputRefs:
      - elasticsearch-insecure
     labels:
       datacenter: "west"
   - name: my-app <11>
     inputRefs:
      - my-app-logs
     outputRefs:
      - default
   - inputRefs: <12>
      - application
     outputRefs:
      - kafka-app
     labels:
       datacenter: "south"
----
<1> The name of the `ClusterLogForwarder` CR must be `instance`.
<2> The namespace for the `ClusterLogForwarder` CR must be `openshift-logging`.
<3> Configuration for an secure Elasticsearch output using a secret with a secure URL.
** A name to describe the output.
** The type of output: `elasticsearch`.
** The secure URL and port of the Elasticsearch instance as a valid absolute URL, including the prefix.
** The secret required by the endpoint for TLS communication. The secret must exist in the `openshift-logging` project.
<4> Configuration for an insecure Elasticsearch output:
** A name to describe the output.
** The type of output: `elasticsearch`.
** The insecure URL and port of the Elasticsearch instance as a valid absolute URL, including the prefix.
<5> Configuration for a Kafka output using a client-authenticated TLS communication over a secure URL
** A name to describe the output.
** The type of output: `kafka`.
** Specify the URL and port of the Kafka broker as a valid absolute URL, including the prefix.
<6> Configuration for an input to filter application logs from the `my-project` namespace.
<7> Configuration for a pipeline to send audit logs to the secure external Elasticsearch instance:
** A name to describe the pipeline.
** The `inputRefs` is the log type, in this example `audit`.
** The `outputRefs` is the name of the output to use, in this example `elasticsearch-secure` to forward to the secure Elasticsearch instance and `default` to forward to the internal Elasticsearch instance.
** Optional: Labels to add to the logs.
<8> Optional: Specify whether to forward structured JSON log entries as JSON objects in the `structured` field. The log entry must contain valid structured JSON; otherwise, OpenShift Logging removes the `structured` field and instead sends the log entry to the default index, `app-00000x`.
<9> Optional: String. One or more labels to add to the logs. Quote values like "true" so they are recognized as string values, not as a boolean.
<10> Configuration for a pipeline to send infrastructure logs to the insecure external Elasticsearch instance.
<11> Configuration for a pipeline to send logs from the `my-project` project to the internal Elasticsearch instance.
** A name to describe the pipeline.
** The `inputRefs` is a specific input: `my-app-logs`.
** The `outputRefs` is `default`.
** Optional: String. One or more labels to add to the logs.
<12> Configuration for a pipeline to send logs to the Kafka broker, with no pipeline name:
** The `inputRefs` is the log type, in this example `application`.
** The `outputRefs` is the name of the output to use.
** Optional: String. One or more labels to add to the logs.

=== Google Cloud Logging
[%collapsible]
[source,yaml]
....
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogForwarder"
metadata:
  name: "instance"
  namespace: "openshift-logging"
spec:
  outputs:
    - name: gcp-1
      type: googleCloudLogging
      secret:
        name: gcp-secret
      googleCloudLogging:
        projectId : "openshift-gce-devel" <1>
        logId : "app-gcp" <2>
  pipelines:
    - name: test-app
      inputRefs: <3>
        - application
      outputRefs:
        - gcp-1
....
<1> Set either a `projectId`, `folderId`, `organizationId`, or `billingAccountId` field and its corresponding value, depending on where you want to store your logs in the link:https://cloud.google.com/resource-manager/docs/cloud-platform-resource-hierarchy[GCP resource hierarchy].
<2> Set the value to add to the `logName` field of the link:https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry[Log Entry].
<3> Specify which log types to forward by using the pipeline: `application`, `infrastructure`, or `audit`.


== LokiStack CR Tempates
.LokiStack custom resource
[source,yaml]
----
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki
  namespace: openshift-logging
spec:
  size: 1x.small
  storage:
    schemas:
    - version: v12
      effectiveDate: 2022-06-01
    secret:
      name: logging-loki-s3
      type: s3
  storageClassName: gp2
  tenants:
    mode: openshift-logging
----


== Creating secrets
include::snippets/logging-create-secret-snip.adoc[]

=== Amazon Cloudwatch
If you have an existing role for AWS, you can create a secret for AWS with STS using the `oc create secret --from-literal` command.

[source,terminal]
----
oc create secret generic cw-sts-secret -n openshift-logging --from-literal=role_arn=arn:aws:iam::123456789012:role/my-role_with-permissions
----

.Example Secret
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  namespace: openshift-logging
  name: my-secret-name
stringData:
  role_arn: arn:aws:iam::123456789012:role/my-role_with-permissions
----

=== Google Cloud Platform
Create a secret using your link:https://cloud.google.com/iam/docs/creating-managing-service-account-keys[Google service account key].

[source,terminal,subs="+quotes"]
----
$ oc -n openshift-logging create secret generic gcp-secret --from-file google-application-credentials.json=_<your_service_account_key_file.json>_
----

== Output Types
