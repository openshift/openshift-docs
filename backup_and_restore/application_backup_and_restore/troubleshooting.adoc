:_mod-docs-content-type: ASSEMBLY
[id="troubleshooting"]
= Troubleshooting
include::_attributes/common-attributes.adoc[]
:context: oadp-troubleshooting
:oadp-troubleshooting:
:namespace: openshift-adp
:local-product: OADP
:must-gather-v1-3: registry.redhat.io/oadp/oadp-mustgather-rhel9:v1.3

toc::[]

You can debug Velero custom resources (CRs) by using the xref:../../backup_and_restore/application_backup_and_restore/troubleshooting.adoc#oadp-debugging-oc-cli_oadp-troubleshooting[OpenShift CLI tool] or the xref:../../backup_and_restore/application_backup_and_restore/troubleshooting.adoc#migration-debugging-velero-resources_oadp-troubleshooting[Velero CLI tool]. The Velero CLI tool provides more detailed logs and information.

You can check xref:../../backup_and_restore/application_backup_and_restore/troubleshooting.adoc#oadp-installation-issues_oadp-troubleshooting[installation issues], xref:../../backup_and_restore/application_backup_and_restore/troubleshooting.adoc#oadp-backup-restore-cr-issues_oadp-troubleshooting[backup and restore CR issues], and xref:../../backup_and_restore/application_backup_and_restore/troubleshooting.adoc#oadp-restic-issues_oadp-troubleshooting[Restic issues].

You can collect logs and CR information by using the xref:../../backup_and_restore/application_backup_and_restore/troubleshooting.adoc#migration-using-must-gather_oadp-troubleshooting[`must-gather` tool].

You can obtain the Velero CLI tool by:

* Downloading the Velero CLI tool
* Accessing the Velero binary in the Velero deployment in the cluster

include::modules/velero-obtaining-by-downloading.adoc[leveloffset=+1]
include::modules/velero-oadp-version-relationship.adoc[leveloffset=+2]
include::modules/velero-obtaining-by-accessing-binary.adoc[leveloffset=+1]

include::modules/oadp-debugging-oc-cli.adoc[leveloffset=+1]
include::modules/migration-debugging-velero-resources.adoc[leveloffset=+1]



[id="oadp-pod-crash-resource-request"]
== Pods crash or restart due to lack of memory or CPU

If a Velero or Restic pod crashes due to a lack of memory or CPU, you can set specific resource requests for either of those resources.
[role="_additional-resources"]
.Additional resources
* xref:../../backup_and_restore/application_backup_and_restore/installing/about-installing-oadp.adoc#oadp-velero-cpu-memory-requirements_about-installing-oadp[CPU and memory requirements]

include::modules/oadp-pod-crash-set-resource-request-velero.adoc[leveloffset=+2]
include::modules/oadp-pod-crash-set-resource-request-restic.adoc[leveloffset=+2]

[IMPORTANT]
====
The values for the resource request fields must follow the same format as Kubernetes resource requirements.
Also, if you do not specify `configuration.velero.podConfig.resourceAllocations` or `configuration.restic.podConfig.resourceAllocations`, the default `resources` specification for a Velero pod or a Restic pod is as follows:

[source,yaml]
----
requests:
  cpu: 500m
  memory: 128Mi
----
====

[id="podvolumerestore-fails_{context}"]
== PodVolumeRestore fails to complete when StorageClass is NFS

The restore operation fails when there is more than one volume during a NFS restore by using `Restic` or `Kopia`. `PodVolumeRestore` either fails with the following error or keeps trying to restore before finally failing.

.Error message

[source,terminal]
----
Velero: pod volume restore failed: data path restore failed: \
Failed to run kopia restore: Failed to copy snapshot data to the target: \
restore error: copy file: error creating file: \
open /host_pods/b4d...6/volumes/kubernetes.io~nfs/pvc-53...4e5/userdata/base/13493/2681: \
no such file or directory
----

.Cause

The NFS mount path is not unique for the two volumes to restore. As a result, the `velero` lock files use the same file on the NFS server during the restore, causing the `PodVolumeRestore` to fail.

.Solution

You can resolve this issue by setting up a unique `pathPattern` for each volume, while defining the `StorageClass` for `nfs-subdir-external-provisioner` in the `deploy/class.yaml` file. Use the following `nfs-subdir-external-provisioner` `StorageClass` example:


[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner 
parameters:
  pathPattern: "${.PVC.namespace}/${.PVC.annotations.nfs.io/storage-path}" # <1>
  onDelete: delete
----

<1> Specifies a template for creating a directory path by using `PVC` metadata such as labels, annotations, name, or namespace. To specify metadata, use `${.PVC.<metadata>}`. For example, to name a folder: `<pvc-namespace>-<pvc-name>`, use `${.PVC.namespace}-${.PVC.name}` as `pathPattern`.

[id="issues-with-velero-and-admission-workbooks"]
== Issues with Velero and admission webhooks

Velero has limited abilities to resolve admission webhook issues during a restore. If you have workloads with admission webhooks, you might need to use an additional Velero plugin or make changes to how you restore the workload.

Typically, workloads with admission webhooks require you to create a resource of a specific kind first. This is especially true if your workload has child resources because admission webhooks typically block child resources.

For example, creating or restoring a top-level object such as `service.serving.knative.dev` typically creates child resources automatically. If you do this first, you will not need to use Velero to create and restore these resources. This avoids the problem of child resources being blocked by an admission webhook that Velero might use.

[id="velero-restore-workarounds-for-workloads-with-admission-webhooks"]
=== Restoring workarounds for Velero backups that use admission webhooks

This section describes the additional steps required to restore resources for several types of Velero backups that use admission webhooks.

include::modules/migration-debugging-velero-admission-webhooks-knative.adoc[leveloffset=+3]
include::modules/migration-debugging-velero-admission-webhooks-ibm-appconnect.adoc[leveloffset=+3]
include::modules/oadp-features-plugins-known-issues.adoc[leveloffset=+2]
include::modules/oadp-plugins-receiving-eof-message.adoc[leveloffset=+2]

[role="_additional-resources"]
.Additional resources

* xref:../../architecture/admission-plug-ins.adoc[Admission plugins]
* xref:../../architecture/admission-plug-ins.adoc#admission-webhooks-about_admission-plug-ins[Webhook admission plugins]
* xref:../../architecture/admission-plug-ins.adoc#admission-webhook-types_admission-plug-ins[Types of webhook admission plugins]

include::modules/oadp-installation-issues.adoc[leveloffset=+1]
include::modules/oadp-operator-issues.adoc[leveloffset=+1]
include::modules/oadp-timeouts.adoc[leveloffset=+1]
include::modules/oadp-restic-timeouts.adoc[leveloffset=+2]
include::modules/oadp-velero-timeouts.adoc[leveloffset=+2]
include::modules/oadp-datamover-timeouts.adoc[leveloffset=+2]
include::modules/oadp-csi-snapshot-timeouts.adoc[leveloffset=+2]
include::modules/oadp-velero-default-timeouts.adoc[leveloffset=+2]
include::modules/oadp-item-restore-timeouts.adoc[leveloffset=+2]
include::modules/oadp-item-backup-timeouts.adoc[leveloffset=+2]
include::modules/oadp-backup-restore-cr-issues.adoc[leveloffset=+1]
include::modules/oadp-restic-issues.adoc[leveloffset=+1]

include::modules/migration-using-must-gather.adoc[leveloffset=+1]
[role="_additional-resources"]
.Additional resources
* xref:../../support/gathering-cluster-data.adoc#gathering-cluster-data[Gathering cluster data]

include::modules/oadp-monitoring.adoc[leveloffset=+1]
[role="_additional-resources"]
.Additional resources
* xref:../../observability/monitoring/monitoring-overview.adoc#about-openshift-monitoring[Monitoring stack]

include::modules/oadp-monitoring-setup.adoc[leveloffset=+2]
include::modules/oadp-creating-service-monitor.adoc[leveloffset=+2]
include::modules/oadp-creating-alerting-rule.adoc[leveloffset=+2]
[role="_additional-resources"]
.Additional resources
* xref:../../observability/monitoring/managing-alerts.adoc#managing-alerts[Managing alerts]

include::modules/oadp-list-of-metrics.adoc[leveloffset=+2]
include::modules/oadp-viewing-metrics-ui.adoc[leveloffset=+2]

:oadp-troubleshooting!:
