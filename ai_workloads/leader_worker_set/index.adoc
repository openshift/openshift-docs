:_mod-docs-content-type: ASSEMBLY
include::_attributes/common-attributes.adoc[]
[id="lws-about"]
= {lws-operator} overview

:context: lws-about

toc::[]

Using large language models (LLMs) for AI/ML inference often requires significant compute resources, and workloads typically must be sharded across multiple nodes. This can make deployments complex, creating challenges around scaling, recovery from failures, and efficient pod placement.

The {lws-operator} simplifies these multi-node deployments by treating a group of pods as a single, coordinated unit. It manages the lifecycle of each pod in the group, scales the entire group together, and performs updates and failure recovery at the group level to ensure consistency.

// About the {lws-operator}
include::modules/lws-about.adoc[leveloffset=+1]

// LeaderWorkerSet architecture
include::modules/lws-arch.adoc[leveloffset=+2]

[role="_additional-resources"]
[id="lws-about_additional-resources"]
== Additional resources

* link:https://lws.sigs.k8s.io/docs/overview/[LeaderWorkerSet documentation (Kubernetes)]
