[[getting-traffic-into-cluster]]
= Getting Traffic into the Cluster
{product-author}
{product-version]
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

ifdef::openshift-origin,openshift-enterprise,openshift-dedicated[]
== Overview
There are many ways to access the cluster. This section describes some
commonly used approaches.

The recommendation is:

- If you have HTTP/HTTPS, use the xref:using-a-router[router].
- If you have a TLS-encrypted protocol other than HTTPS (for example, TLS with the
SNI header), use the xref:using-a-router[router].
- Otherwise, use xref:using-the-loadbalancer[Load Balancer],
xref:using-externalIP[ExternalIP], or xref:using-nodeport[NodePort].

TCP or UDP offers several approaches:

- Use the non-cloud xref:using-the-loadbalancer[Load Balancer]. This limits you to
a single ingress IP (which can be a virtual IP (VIP), but still is a single machine for
initial load balancing). It simplifies the administrator's job, but uses one IP
per service.
- Manually assign xref:using-externalIP[ExternalIPs] to the service. You can
assign a set of IPs, so you can have multiple machines for the incoming load
balancing. However, this requires elevated permissions to assign, and manual
tracking of what IP:ports that are used.
- Use xref:using-nodeport[NodePorts]
to expose the service on _all_ nodes in the cluster. This is more wasteful
of scarce port resources. However, it is slightly easier to set up multiple.
Again, this requires more privileges.

The router is the most common way to access the cluster. This is limited to
HTTP/HTTPS(SNI)/TLS(SNI), which covers web applications.

ExternalIP or NodePort is useful when the HTTP protocol is not being used or
non-standard ports are in use. There is more manual setup and monitoring
involved.

The administrator must set up the external port to the cluster networking
environment so that requests can reach the cluster. For example, names can be
configured into
xref:../install_config/install/prerequisites.adoc#prereq-dns[DNS] to point to
specific nodes or other IP addresses in the cluster. The DNS wildcard feature
can be used to configure a subset of names to an IP address in the cluster. This
is convenient when using routers because it allows the users to set up routes
within the cluster without further administrator attention.

The administrator must ensure that the local firewall on each node permits the
request to reach the IP address.

The xref:../admin_guide/high_availability.adoc#admin-guide-high-availability[High
Availability] section describes how to make this highly available using
replicated services.
endif::[]

[[using-a-router]]
== Using a Router

This is the most common way to access the cluster. A
xref:../install_config/configuring_routing.adoc#install-config-configuring-routing[router]
is configured to accept external requests and proxy them based on the
configured routes. This is limited to HTTP/HTTPS(SNI)/TLS(SNI), which
covers web applications.

An administrator can create a
xref:../install_config/install/prerequisites.adoc#prereq-dns[wildcard DNS]
entry, and then set up a router. Afterward, the users can self-service the
ingress without having to contact the administrators. The router has controls to
allow the administrator to specify whether the users can self-provision host
names, or if they must fit a pattern the administrator defines. The other
solutions require the administrator to do the provisioning, or they require that
the administrator delegates a lot of privilege.

A set of routes can be created in the various projects. The overall set of
routes is available to the set of routers. Each router selects from the set of
routes. All routers see all routes unless restricted by labels on the router,
which is called router
xref:../architecture/core_concepts/routes.adoc#router-sharding[sharding].

The
xref:../admin_guide/high_availability.adoc#admin-guide-high-availability[High
Availability] section describes how to configure a router for high availability
service using multiple replicas.

[[using-the-loadbalancer]]
== Using a Load Balancer Service

link:http://kubernetes.io/docs/user-guide/services/#type-loadbalancer[Load balancers] are available on
xref:../install_config/configuring_aws.adoc#install-config-configuring-aws[AWS]
and
xref:../install_config/configuring_gce.adoc#install-config-configuring-gce[GCE]
clouds, and
xref:../admin_guide/tcp_ingress_external_ports.adoc#admin-guide-expose-external-ports[non-cloud]
options are also available.

The non-cloud
xref:../admin_guide/tcp_ingress_external_ports.adoc#admin-guide-expose-external-ports[load
balancer] allocates a unique IP from a configured pool. This limits you to a
single ingress IP, which can be a VIP, but still will be a single machine for
initial load balancing. The non-cloud load balancer simplifies the
administrator's job by providing the needed IP address, but uses one IP per
service.

[[using-externalIP]]
== Using a Service ExternalIP

The user can manually assign
link:http://kubernetes.io/docs/user-guide/services/#external-ips[ExternalIPs]
to the service. The supplied list of IP addresses are used for load balancing
incoming requests. The service port is opened on all nodes running kube-proxy.
ExternalIPs require elevated permissions to assign, and manual tracking of the
IP:ports that are in use.

An externally visible IP can be configured in one of several ways:

- Manually configure the ExternalIP with a known external IP address.
- Configure ExternalIP to a
xref:../admin_guide/high_availability.adoc#admin-guide-high-availability[VIP]
that is generated from (VRRP).
- In a cloud (xref:../install_config/configuring_aws.adoc#install-config-configuring-aws[AWS] or
xref:../install_config/configuring_gce.adoc#install-config-configuring-gce[GCE])
use `type=LoadBalancer`
- In a non-cloud environment, configure an ingressIP range (IngressIPNetworkCIDR),
`service.type=LoadBalancer` and `service.port.ingressIP`.

The administrator must ensure the external IPs are routed to the nodes and local
firewall rules on all nodes allow access to the open port.

Use the same nodes as the router, but advertise them with VRRP and use
the DNS address of the router nodes.

[[using-nodeport]]
== Using a Service NodePort

Use NodePorts to expose the service nodePort on all nodes in the cluster.
NodePorts are in the 30-60k range by default, which means a NodePort is
unlikely to match a service's intended port (for example, 8080 may be exposed
as 31020). This use of ports is wasteful of scarce port resources.
However, it is slightly easier to set up. Again, this requires more privileges.

The administrator must ensure the external IPs are routed to the nodes and local
firewall rules on all nodes allow access to the open port.

NodePorts and externalIP are independent and both can be used concurrently.

[[virtual-ip]]
== Using Virtual IPs

All of the approaches can expose
xref:../admin_guide/high_availability.adoc#admin-guide-high-availability[VIPs].
The VIPs are generated from the VRRP protocol.


[[edge-load-balancer]]
== Edge Load Balancer

An edge xref:../install_config/routing_from_edge_lb.adoc#install-config-routing-from-edge-lb[load balancer]
can be used to accept traffic from outside networks and proxy the traffic
to pods inside the cluster.

In this configuration, the internal pod network is visible to the outside.
