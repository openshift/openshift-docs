[[dev-guide-jobs]]
= Jobs
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
A job, in contrast to
xref:../architecture/core_concepts/deployments.adoc#replication-controllers[a
replication controller], runs a pod with any number of replicas to completion. A
job tracks the overall progress of a task and updates its status with information
about active, succeeded, and failed pods. Deleting a job will clean up any pod
replicas it created. Jobs are part of the Kubernetes API, which can be managed
with `oc` commands like other
xref:../cli_reference/basic_cli_operations.adoc#object-types[object types].

See the http://kubernetes.io/docs/user-guide/jobs/[Kubernetes documentation] for
more information about jobs.

[[creating-a-job]]
== Creating a Job

A job configuration consists of the following key parts:

- A pod template, which describes the application the pod will create.
- An optional `*parallelism*` parameter, which specifies how many pod replicas running in parallel should execute a job. If not specified, this defaults to
the value in the `*completions*` parameter.
- An optional `*completions*` parameter, specifying how many concurrently running pods should execute a job. If not specified, this value defaults to one.

The following is an example of a `*job*` resource:

[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  parallelism: 1    <1>
  completions: 1    <2>
  template:         <3>
    metadata:
      name: pi
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: OnFailure    <4>
----

1. Optional value for how many pod replicas a job should run in parallel; defaults to `*completions*`.
2. Optional value for how many successful pod completions are needed to mark a job completed; defaults to one.
3. Template for the pod the controller creates.
4. The restart policy of the pod. This does not apply to the job controller. See xref:creating-a-job-known-issues[] for details.

You can also create and launch a job from a single command using `oc run`. The following command creates and launches the same job as specified in the previous example:

----
$ oc run pi --image=perl --replicas=1  --restart=OnFailure \
    --command -- perl -Mbignum=bpi -wle 'print bpi(2000)' 
----

[[creating-a-job-known-issues]]
=== Known Limitations

The job specification restart policy only applies to the _pods_, and not the _job controller_. However, the job controller is hard-coded to keep retrying jobs to completion. 
 
As such, `restartPolicy: Never` or `--restart=Never` results in the same behavior as `restartPolicy: OnFailure` or `--restart=OnFailure`. That is, when a job fails it is restarted automatically until it succeeds (or is manually discarded). The policy only sets which subsystem performs the restart.
 
With the `Never` policy, the _job controller_ performs the restart. With each attempt, the job controller increments the number of failures in the job status and create new pods. This means that with each failed attempt, the number of pods increases. 
 
With the `OnFailure` policy, _kubelet_ performs the restart. Each attempt does not increment the number of failures in the job status. In addition, kubelet will retry failed jobs starting pods on the same nodes.

[[scaling-a-job]]
== Scaling a Job

A job can be scaled up or down by using the `oc scale` command with the
`--replicas` option, which, in the case of jobs, modifies the
`*spec.parallelism*` parameter. This will result in modifying the number of pod
replicas running in parallel, executing a job.

The following command uses the example job above, and sets the `*parallelism*`
parameter to three:

====
----
$ oc scale job pi --replicas=3
----
====

[NOTE]
Scaling replication controllers also uses the `oc scale` command with the
`--replicas` option, but instead changes the `*replicas*` parameter of a
replication controller configuration.

[[jobs-setting-maximum-duration]]
== Setting Maximum Duration

When defining a `*Job*`, you can define its maximum duration by setting
the `*activeDeadlineSeconds*` field. It is specified in seconds and is not
set by default. When not set, there is no maximum duration enforced.

The maximum duration is counted from the time when a first pod gets scheduled in
the system, and defines how long a job can be active. It tracks overall time of
an execution and is irrelevant to the number of completions (number of pod replicas
needed to execute a task). After reaching the specified timeout, the job is
terminated by {product-title}.

The following example shows the part of a `*Job*` specifying
`*activeDeadlineSeconds*` field for 30 minutes:

====
[source,yaml]
----
  spec:
    activeDeadlineSeconds: 1800
----
====
