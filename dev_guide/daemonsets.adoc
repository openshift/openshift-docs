[[dev-guide-daemonsets]]
= Using Daemonsets
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

A daemonset can be used to run replicas of a pod on specific or all nodes in an
{product-title} cluster.

Use daemonsets to create shared storage, run a logging pod on every node in
your cluster, or deploy a monitoring agent on every node.

For security reasons, only cluster administrators can create daemonsets.
(xref:../admin_guide/manage_rbac.adoc#admin-guide-granting-users-daemonset-permissions[Granting Users Daemonset Permissions.])

For more information on daemonsets, see the link:http://kubernetes.io/docs/admin/daemons/[Kubernetes documentation].

[IMPORTANT]
====
Daemonset scheduling is incompatible with project's default node selector.
If you fail to disable it, the daemonset gets restricted by merging with the
default node selector. This results in frequent pod recreates on the nodes that
got unselected by the merged node selector, which in turn puts unwanted load on
the cluster.

Therefore,

* Before you start using daemonsets, disable the default project-wide
xref:../admin_guide/managing_projects.adoc#using-node-selectors[node selector]
in your namespace, by setting the namespace annotation `openshift.io/node-selector` to an empty string:

----
# oc patch namespace myproject -p \
    '{"metadata": {"annotations": {"openshift.io/node-selector": ""}}}'
----

* If you are creating a new project, overwrite the default node selector using
`oc adm new-project --node-selector=""`.

====

[[dev-guide-creating-daemonsets]]
== Creating Daemonsets

When creating daemonsets, the `*nodeSelector*` field is used to indicate the
nodes on which the daemonset should deploy replicas.

. Define the daemonset yaml file:
+
====
----
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: hello-daemonset
spec:
  selector:
      matchLabels:
        name: hello-daemonset <1>
  template:
    metadata:
      labels:
        name: hello-daemonset <2>
    spec:
      nodeSelector: <3>
        type: infra
      containers:
      - image: openshift/hello-openshift
        imagePullPolicy: Always
        name: registry
        ports:
        - containerPort: 80
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
      serviceAccount: default
      terminationGracePeriodSeconds: 10
----
<1> The label selector that determines which pods belong to the daemonset.
<2> The pod template's label selector. Must match the label selector above.
<3> The node selector that determines on which nodes pod replicas should be deployed.
====

. Create the daemonset object:
+
----
oc create -f daemonset.yaml
----

. To verify that the pods were created, and that each node has a pod replica:
+
.. Find the daemonset pods:
+
====
----
$ oc get pods
hello-daemonset-cx6md   1/1       Running   0          2m
hello-daemonset-e3md9   1/1       Running   0          2m
----
====
+
.. View the pods to verify the pod has been placed onto the node:
+
====
----
$ oc describe pod/hello-daemonset-cx6md|grep Node
Node:        openshift-node01.hostname.com/10.14.20.134
$ oc describe pod/hello-daemonset-e3md9|grep Node
Node:        openshift-node02.hostname.com/10.14.20.137
----
====

[IMPORTANT]
====
* If you update a DaemonSet's pod template, the existing pod
replicas are not affected. 

* If you delete a DaemonSet and then create a new DaemonSet
with a different template but the same label selector, it recognizes any
existing pod replicas as having matching labels and thus does not update them or
create new replicas despite a mismatch in the pod template.

* If you change node labels, the DaemonSet adds pods to nodes that match the new labels and deletes pods 
from nodes that do not match the new labels.
 
To update a DaemonSet, force new pod replicas to be created by deleting the old
replicas or nodes.
====

[[dev-guide-scheduling-daemonsets]]
== Scheduled by default scheduler

A DaemonSet ensures that all eligible nodes run a copy of a Pod. Normally, the node that a Pod runs on is selected by the Kubernetes scheduler. However, previously daemonSet pods are created and scheduled by the DaemonSet controller. That introduces the following issues:

* Inconsistent Pod behavior: Normal Pods waiting to be scheduled are created and in Pending state, but DaemonSet pods are not created in Pending state. This is confusing to the user.
* Pod preemption is handled by default scheduler. When preemption is enabled, the DaemonSet controller will make scheduling decisions without considering pod priority and preemption.

ScheduleDaemonSetPods is enabled by default in {product-title} which lets you to schedule DaemonSets using the default scheduler instead of the DaemonSet controller, by adding the NodeAffinity term to the DaemonSet pods, instead of the .spec.nodeName term. The default scheduler is then used to bind the pod to the target host. If node affinity of the DaemonSet pod already exists, it is replaced. The DaemonSet controller only performs these operations when creating or modifying DaemonSet pods, and no changes are made to the spec.template of the DaemonSet.

nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchFields:
      - key: metadata.name
        operator: In
        values:
        - target-host-name

In addition, node.kubernetes.io/unschedulable:NoSchedule toleration is added automatically to DaemonSet Pods. The default scheduler ignores unschedulable Nodes when scheduling DaemonSet Pods.
