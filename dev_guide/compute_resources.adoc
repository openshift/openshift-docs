[[dev-guide-compute-resources]]
= Quotas and limit ranges
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

Using xref:dev-quotas[quotas] and xref:dev-limit-ranges[limit ranges], cluster
administrators can set constraints to limit the number of objects or amount of
compute resources that are used in your project. This helps cluster
administrators better manage and allocate resources across all projects, and
ensure that no projects are using more than is appropriate for the cluster size.

ifdef::openshift-origin,openshift-enterprise,openshift-dedicated[]
As a developer, you can also
set xref:dev-compute-resources[requests and limits on compute resources] at the
pod and container level.
endif::[]

ifdef::openshift-online[]
[IMPORTANT]
====
{product-title} Pro project owners can change quotas for their project, but not
limit ranges. {product-title} Starter users cannot modify quotas or limit ranges.
====
endif::[]

The following sections help you understand how to check on your quota and limit
range settings, what sorts of things they can constrain, and how you can request
or limit compute resources in your own pods and containers.

[[dev-quotas]]
== Quotas

A resource quota, defined by a `*ResourceQuota*` object, provides constraints
that limit aggregate resource consumption per project. It can limit the quantity
of objects that can be created in a project by type, as well as the total amount
of compute resources and storage that may be consumed by resources in that project.

[NOTE]
====
Quotas are set by cluster administrators and are scoped to a given project.
====

[[dev-viewing-quotas]]
=== Viewing quotas

You can view usage statistics related to any hard limits defined in a project's
quota by navigating in the web console to the project's *Quota* page.

You can also use the CLI to view quota details:

. Get the list of quotas defined in the project. For example, for a project
called *demoproject*:
+

----
$ oc get quota -n demoproject
NAME                AGE
besteffort          11m
compute-resources   2m
core-object-counts  29m
----


. Describe the quota you are interested in, for example, the
*core-object-counts* quota:
+

----
$ oc describe quota core-object-counts -n demoproject
Name:			core-object-counts
Namespace:		demoproject
Resource		Used	Hard
--------		----	----
configmaps		3	10
persistentvolumeclaims	0	4
replicationcontrollers	3	20
secrets			9	10
services		2	10
----

Full quota definitions can be viewed by running `oc export` on the object. The
following show some sample quota definitions:

include::admin_guide/quota.adoc[tag=admin_quota_sample_definitions]

[[dev-managed-by-quota]]
=== Resources managed by quota

.Compute Resources Managed by Quota
[cols="3a,8a",options="header"]
|===

|Resource Name |Description

|`*cpu*`
|The sum of CPU requests across all pods in a non-terminal state cannot exceed
this value. `*cpu*` and `*requests.cpu*` are the same value and can be used
interchangeably.

|`*memory*`
|The sum of memory requests across all pods in a non-terminal state cannot
exceed this value. `*memory*` and `*requests.memory*` are the same value and can
be used interchangeably.

|`*ephemeral-storage*`
|The sum of local ephemeral storage requests across all pods in a
non-terminal state cannot exceed this value. `*ephemeral-storage*` and
`*requests.ephemeral-storage*` are the same value and can be used
interchangeably. This resource is available only if you enabled the ephemeral storage
technology preview in {product-title} 3.10. This feature is disabled by
default.

|`*requests.cpu*`
|The sum of CPU requests across all pods in a non-terminal state cannot exceed
this value. `*cpu*` and `*requests.cpu*` are the same value and can be used
interchangeably.

|`*requests.memory*`
|The sum of memory requests across all pods in a non-terminal state cannot
exceed this value. `*memory*` and `*requests.memory*` are the same value and can
be used interchangeably.

|`*requests.ephemeral-storage*`
|The sum of ephemeral storage requests across all pods in a non-terminal state
cannot exceed this value. `*ephemeral-storage*` and
`*requests.ephemeral-storage*` are the same value and can be used
interchangeably. This resource is available only if you enabled the ephemeral storage
technology preview in {product-title} 3.10. This feature is disabled by
default.

|`*limits.cpu*`
|The sum of CPU limits across all pods in a non-terminal state cannot exceed
this value.

|`*limits.memory*`
|The sum of memory limits across all pods in a non-terminal state cannot exceed
this value.

|`*limits.ephemeral-storage*`
|The sum of ephemeral storage limits across all pods in a non-terminal state cannot exceed
this value. This resource is available only if you enabled the ephemeral storage
technology preview in {product-title} 3.10. This feature is disabled by
default.
|===

[[dev-quota-scopes]]
=== Quota scopes

.Storage Resources Managed by Quota
[cols="3a,8a",options="header"]
|===

|Resource Name |Description

|`*requests.storage*`
|The sum of storage requests across all persistent volume claims in any state cannot
exceed this value.

|`*persistentvolumeclaims*`
|The total number of persistent volume claims that can exist in the project.

|`*<storage-class-name>.storageclass.storage.k8s.io/requests.storage*`
|The sum of storage requests across all persistent volume claims in any state that have a matching storage class, cannot exceed this value.

|`*<storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims*`
|The total number of persistent volume claims with a matching storage class that can exist in the project.
|===

[[dev-quota-enforcement]]
=== Quota enforcement

After a resource quota for a project is first created, the project restricts the
ability to create any new resources that may violate a quota constraint until it
has calculated updated usage statistics.

After a quota is created and usage statistics are updated, the project accepts
the creation of new content. When you create or modify resources, your quota
usage is incremented immediately upon the request to create or modify the
resource.

When you delete a resource, your quota use is decremented during the next full
recalculation of quota statistics for the project.

If project modifications exceed a quota usage limit, the server denies the
action. An appropriate error message is returned explaining the quota constraint
violated, and what your currently observed usage stats are in the system.

[[dev-requests-vs-limits]]
=== Requests versus limits

When allocating
xref:../dev_guide/compute_resources.adoc#dev-compute-resources[compute
resources], each container may specify a request and a limit value each for
CPU, memory, and ephemeral storage. Quotas can restrict any of these values.

If the quota has a value specified for `*requests.cpu*` or `*requests.memory*`,
then it requires that every incoming container make an explicit request for
those resources. If the quota has a value specified for `*limits.cpu*` or
`*limits.memory*`, then it requires that every incoming container specify an
explicit limit for those resources.

See xref:dev-compute-resources[Compute Resources] for more on setting requests
and limits in pods and containers.

[[dev-limit-ranges]]
== Limit ranges

A limit range, defined by a `*LimitRange*` object, enumerates
xref:../dev_guide/compute_resources.adoc#dev-compute-resources[compute resource
constraints] in a xref:../dev_guide/projects.adoc#dev-guide-projects[project] at the pod,
container, image, image stream, and persistent volume claim level, and specifies the amount of resources
that a pod, container, image, image stream, or persistent volume claim can consume.

All resource create and modification requests are evaluated against each
`*LimitRange*` object in the project. If the resource violates any of the
enumerated constraints, then the resource is rejected. If the resource does not
set an explicit value, and if the constraint supports a default value, then the
default value is applied to the resource.

ifdef::openshift-origin,openshift-enterprise[]
[NOTE]
====
As of {product-title} 3.10, it is possible to specify limits and requests for
ephemeral storage by using the ephemeral storage technology preview. This
feature is disabled by default. To enable this feature, see
xref:../install_config/configuring_ephemeral.adoc[configuring for ephemeral
storage]. 
====
endif::openshift-origin,openshift-enterprise[]

[NOTE]
====
Limit ranges are set by cluster administrators and are scoped to a given
project.
====

[[dev-viewing-limit-ranges]]
=== Viewing limit ranges

You can view any limit ranges defined in a project by navigating in the web
console to the project's *Quota* page.

You can also use the CLI to view limit range details:

. Get the list of limit ranges defined in the project. For example, for a
project called *demoproject*:
+
----
$ oc get limits -n demoproject
NAME              AGE
resource-limits   6d
----

. Describe the limit range you are interested in, for example, the
*resource-limits* limit range:
+
----
$ oc describe limits resource-limits -n demoproject
Name:                           resource-limits
Namespace:                      demoproject
Type                            Resource                Min     Max     Default Request Default Limit   Max Limit/Request Ratio
----                            --------                ---     ---     --------------- -------------   -----------------------
Pod                             cpu                     200m    2       -               -               -
Pod                             memory                  6Mi     1Gi     -               -               -
Container                       cpu                     100m    2       200m            300m            10
Container                       memory                  4Mi     1Gi     100Mi           200Mi           -
openshift.io/Image              storage                 -       1Gi     -               -               -
openshift.io/ImageStream        openshift.io/image      -       12      -               -               -
openshift.io/ImageStream        openshift.io/image-tags -       10      -               -               -
----

Full limit range definitions can be viewed by running `oc export` on the object.
The following shows an example limit range definition:

include::admin_guide/limits.adoc[tag=admin_limits_sample_definitions]

[[dev-container-limits]]
=== Container limits

include::admin_guide/limits.adoc[tag=admin_limits_container_limits]

[[dev-pod-limits]]
=== Pod limits

include::admin_guide/limits.adoc[tag=admin_limits_pod_limits]

[[dev-compute-resources]]
== Compute resources

Each container running on a node consumes compute resources, which are
measurable quantities that can be requested, allocated, and consumed.

When you write a pod configuration file, you can optionally specify how much
CPU and memory (RAM) that each container needs to schedule pods in the cluster
and ensure satisfactory performance. If your administrator enabled the ephemeral
storage technology preview in {product-title} 3.10, you can also specify
local ephemeral storage.

CPU is measured in units called millicores. Each node in a cluster inspects the
operating system to determine the amount of CPU cores on the node, then
multiplies that value by 1000 to express its total capacity. For example, if a
node has 2 cores, the node's CPU capacity would be represented as 2000m. If you
wanted to use 1/10 of a single core, it would be represented as 100m.

Memory and ephemeral storage are measured in bytes. In addition, it may be used with SI suffixes (E, P,
T, G, M, K) or their power-of-two-equivalents (Ei, Pi, Ti, Gi, Mi, Ki).

[source,yaml]
----
apiVersion: v1
kind: Pod
spec:
  containers:
  - image: nginx
    name: nginx
    resources:
      requests:
        cpu: 100m <1>
        memory: 200Mi <2>
        ephemeral-storage: 1Gi <3>
      limits:
        cpu: 200m <4>
        memory: 400Mi <5>
        ephemeral-storage: 2Gi <6>
----
<1> The container requests 100m CPU.
<2> The container requests 200Mi memory.
<3> If your administrator enabled the ephemeral storage technology feature preview in {product-title} 3.10, the container requests 1Gi ephemeral storage.
<4> The container limits 200m CPU.
<5> The container limits 400Mi memory.
<6> If your administrator enabled the ephemeral storage technology feature preview in {product-title} 3.10, the container limits ephemeral storage to 2Gi.

[[dev-cpu-requests]]
=== CPU requests

Each container in a pod can specify the amount of CPU it requests on a node. The
scheduler uses CPU requests to find a node with an appropriate fit for a
container.

The CPU request represents a minimum amount of CPU that your container may
consume, but if there is no contention for CPU, it can use all available CPU on
the node. If there is CPU contention on the node, CPU requests provide a
relative weight across all containers on the system for how much CPU time the
container can use.

On the node, CPU requests map to Kernel CFS shares to enforce this behavior.

ifdef::openshift-online[]
[NOTE]
====
In {product-title}, CPU requests are set automatically based on the memory limit specified. If no memory limit is specified, a CPU request of `60m` is set.
====
endif::[]

[[viewing-compute-resources]]
=== Viewing compute resources

To view compute resources for a pod:

----
$ oc describe pod nginx-tfjxt
Name:       nginx-tfjxt
Namespace:      default
Image(s):     nginx
Node:       /
Labels:       run=nginx
Status:       Pending
Reason:
Message:
IP:
Replication Controllers:  nginx (1/1 replicas created)
Containers:
  nginx:
    Container ID:
    Image:    nginx
    Image ID:
    QoS Tier:
      cpu:  Burstable
      memory: Burstable
    Limits:
      cpu:  200m
      memory: 400Mi
      ephemeral-storage: 1Gi
    Requests:
      cpu:    100m
      memory:   200Mi
      ephemeral-storage: 2Gi
    State:    Waiting
    Ready:    False
    Restart Count:  0
    Environment Variables:
----

[[dev-cpu-limits]]
=== CPU limits

Each container in a pod can specify the amount of CPU it is limited to use on a
node. CPU limits control the maximum amount of CPU that your container can use
independent of contention on the node. If a container attempts to exceed the
specified limit, the system throttles the container. This behavior ensures that
the container has a consistent level of service independent of the number of
pods scheduled to the node.

ifdef::openshift-online[]
[NOTE]
====
In {product-title}, CPU limits are set automatically based on the memory limit specified. If no memory limit is specified, a CPU limit of `1` core is set.
====
endif::[]

[[dev-memory-requests]]
=== Memory requests

By default, a container can consume as much memory on the node as possible. To
improve placement of pods in the cluster, specify the amount of memory required
for a container to run. The scheduler then takes the available node memory
capacity into account before it binds your pod to a node. A container can
consume as much memory on the node as possible even when you specify a request.

ifdef::openshift-online[]
[NOTE]
====
In {product-title}, memory requests are set automatically based on the memory limit specified. If no memory limit is specified, a memory request of `307Mi` applies.
====
endif::[]

[[dev-ephemeral-storage-requests]]
=== Ephemeral storage requests

[NOTE]
====
This applies only if your administrator enabled the ephemeral storage technology
preview in {product-title} 3.10.
====

By default, a container can consume as much local ephemeral storage on
the node as is available. To improve placement of pods in the cluster, specify
the amount of required local ephemeral storage for a container to run. The
scheduler then takes the available node local storage capacity into account
before it binds your pod to a node. A container can consume as much local
ephemeral storage on the node as possible even when you specify a request.

[[dev-memory-limits]]
=== Memory limits

If you specify a memory limit, you can constrain the amount of memory the container can use. For example, if you specify a limit of 200Mi, a container is limited to using that amount of memory on the node. If the container exceeds the specified memory limit, it is terminated and potentially restarted dependent upon the container restart policy.
ifdef::openshift-online[]
[NOTE]
====
In {product-title}, the memory request, CPU request, and CPU limit are automatically determined and set appropriately based off of the specified memory limit. If no memory limit is specified, a memory limit of `512Mi` applies.
====
endif::[]

[[dev-ephemeral-storage-limits]]
=== Ephemeral storage limits

[NOTE]
====
This applies only if your administrator enabled the ephemeral storage technology
preview in {product-title} 3.10.
====

If you specify an ephemeral storage limit, you can constrain the amount of ephemeral storage the container can use. For example, if you specify a limit of 2Gi, a container is limited to using that amount of ephemeral storage on the node. If the container exceeds the specified memory limit, it is terminated and potentially restarted dependent upon the container restart policy.

[[quality-of-service-tiers]]
=== QoS tiers

When created, a compute resource is classified with a _quality of service_
(QoS). There are three tiers, and each is based on the request and limit value
specified for each resource:

[cols="3,8",options="header"]
|===
|*Quality of Service*
|*Description*

|*BestEffort*
|Provided when a request and limit are not specified.

|*Burstable*
|Provided when a request is specified that is less than an optionally specified
limit.

|*Guaranteed*
|Provided when a limit is specified that is equal to an optionally specified
request.
|===

A container may have a different quality of service for each compute resource.
For example, a container can have *Burstable* CPU and *Guaranteed* memory
qualities of service.

The quality of service has different impacts on different resources, depending
on whether the resource is compressible or not. CPU is a compressible resource,
whereas memory is an incompressible resource.

With CPU resources: ::
- A *BestEffort CPU* container can consume as much CPU as is available on
a node but runs with the lowest priority.
- A *Burstable CPU* container is guaranteed to get the minimum amount of CPU
requested, but it may or may not get additional CPU time. Excess CPU resources
are distributed based on the amount requested across all containers on the node.
- A *Guaranteed CPU* container is guaranteed to get the amount requested and no
more, even if there are additional CPU cycles available. This provides a
consistent level of performance independent of other activity on the node.

With memory resources: ::
- A *BestEffort memory* container can consume as much memory as is
available on the node, but there are no guarantees that the scheduler places
that container on a node with enough memory to meet its needs. In addition, a
*BestEffort* container has the greatest chance of being killed if there
is an out of memory event on the node.
- A *Burstable memory* container is scheduled on the node to get the amount of
memory requested, but it may consume more. If there is an out of memory event on
the node, *Burstable* containers are killed after *BestEffort* containers when
attempting to recover memory.
- A *Guaranteed memory* container gets the amount of memory requested, but no
more. In the event of an out-of-memory event, the container is only killed if there
are no more *BestEffort* or *Burstable* containers on the system.

[[specifying-compute-resources-via-cli]]
=== Specifying compute resources via CLI

To specify compute resources via the CLI:

ifdef::openshift-origin,openshift-enterprise,openshift-dedicated[]
----
$ oc run nginx --image=nginx --limits=cpu=200m,memory=400Mi,ephemeral-storage-2Gi --requests=cpu=100m,memory=200Mi,ephemeral-storage=1Gi
----

[NOTE]
====
This applies only if your administrator enabled the ephemeral storage technology
preview in {product-title} 3.10.
====
endif::[]

ifdef::openshift-online[]
----
$ oc run nginx --image=nginx --limits=memory=400Mi
----
endif::[]


ifdef::openshift-origin,openshift-enterprise[]
[[opaque-integer-resources-dev]]
=== Opaque integer resources

include::admin_guide/opaque_integer_resources.adoc[tag=oir-intro]

The cluster administrator is usually responsible for creating the resources and making them available.
For more information on creating opaque integer resources, see xref:../admin_guide/opaque_integer_resources.adoc#admin-guide-opaque-resources[Opaque Integer Resources] in the Administrator Guide.

To consume an opaque integer resource in a pod, edit the pod to
include the name of the opaque resource as a key in the `spec.containers[].resources.requests` field.

include::admin_guide/opaque_integer_resources.adoc[tag=oir-step]
endif::[]

ifdef::openshift-enterprise,openshift-dedicated,openshift-origin[]
== Project resource limits

xref:../admin_guide/limits.adoc#admin-guide-limits[Resource limits can be
set per-project] by cluster administrators. Developers do not
have the ability to create, edit, or delete these limits, but can
xref:../admin_guide/limits.adoc#viewing-limits[view them] for projects
they have access to.
endif::[]
