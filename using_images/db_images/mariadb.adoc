[[using-images-db-images-mariadb]]
= MariaDB
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

[[mariadb-overview]]
== Overview
{product-title} provides a container image for running MariaDB. This image can
provide database services based on username, password, and database name
settings provided in a configuration file.

[[mariadb-versions]]
== Versions
Currently, {product-title} provides versions
https://github.com/sclorg/mariadb-container/tree/master/10.0[10.0] and
https://github.com/sclorg/mariadb-container/tree/master/10.1[10.1] of MariaDB.

[[mariadb-images]]
== Images

These images come in two flavors, depending on your needs:

* RHEL 7
* CentOS 7

*RHEL 7 Based Image*

The RHEL 7 images are available through Red Hat's subscription registry:

----
$ docker pull registry.access.redhat.com/rhscl/mariadb-100-rhel7
$ docker pull registry.access.redhat.com/rhscl/mariadb-101-rhel7
----

*CentOS 7 Based Image*

These images are available on DockerHub. To download them:

----
$ docker pull openshift/mariadb-100-centos7
$ docker pull centos/mariadb-101-centos7
----

To use these images, you can either access them directly from these
registries or push them into your {product-title} Docker registry. Additionally,
you can create an ImageStream that points to the image,
either in your Docker registry or at the external location. Your {product-title}
resources can then reference the ImageStream. You can find
https://github.com/openshift/origin/tree/master/examples/image-streams[example]
ImageStream definitions for all the provided {product-title} images.

[[mariadb-configuration-and-usage]]
== Configuration and Usage

[[initializing-the-database]]
=== Initializing the Database

The first time you use the shared volume, the database is created along with
the database administrator user and the MariaDB root user (if you specify the
`MYSQL_ROOT_PASSWORD` environment variable). Afterwards, the MariaDB daemon
starts up. If you are re-attaching the volume to another container, then the
database, database user, and the administrator user are not created, and the
MariaDB daemon starts.

The following command creates a new database
xref:../../architecture/core_concepts/pods_and_services.adoc#pods[pod] with
MariaDB running in a container:

----
$ oc new-app \
    -e MYSQL_USER=<username> \
    -e MYSQL_PASSWORD=<password> \
    -e MYSQL_DATABASE=<database_name> \
ifdef::openshift-enterprise[]
    registry.access.redhat.com/rhscl/mariadb-101-rhel7
endif::[]
ifdef::openshift-origin[]
    docker pull centos/mariadb-101-centos7
endif::[]
----

If you want to set only the mandatory environment variables, and not store the
database in a host directory, run:

----
ifdef::openshift-enterprise[]
$ docker run -d --name mariadb_database -e MYSQL_USER=user -e MYSQL_PASSWORD=pass -e MYSQL_DATABASE=db -p 3306:3306 rhscl/mariadb-100-rhel7
endif::[]
ifdef::openshift-origin[]
$ docker run -d --name mariadb_database -e MYSQL_USER=user -e MYSQL_PASSWORD=pass -e MYSQL_DATABASE=db -p 3306:3306 mariadb-100-centos7
endif::[]
----

This will create a container named `mariadb_database` running MySQL with
database `db` and user with credentials `user:pass`. Port 3306 will be exposed
and mapped to the host. If you want your database to be persistent across
container executions, also add a `-v /host/db/path:/var/lib/mysql/data` argument.
This will be the MySQL data directory.

If the database directory is not initialized, the entrypoint script will first
run `mysql_install_db` and set up the necessary database users and passwords. After the
database is initialized, or if it was already present, `mysqld` is executed and
will run as PID 1. To stop the detached container, run:

----
docker stop mariadb_database
----

[[running-mariadb-commands-in-containers]]
=== Running MariaDB Commands in Containers

{product-title} uses https://www.softwarecollections.org/[Software Collections]
(SCLs) to install and launch MariaDB. If you want to execute a MariaDB command
inside of a running container (for debugging), you must invoke it using bash.

To do so, first identify the name of the running MariaDB pod. For example, you
can view the list of pods in your current project:

----
$ oc get pods
----

Then, open a remote shell session to the pod:

----
$ oc rsh <pod>
----

When you enter the container, the required SCL is automatically enabled.

You can now run *mysql* commands from the bash shell to start a MariaDB
interactive session and perform normal MariaDB operations. For example, to
authenticate as the database user:

====
----
bash-4.2$ mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -h $HOSTNAME $MYSQL_DATABASE
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 4
Server version: 5.5.37 MySQL Community Server (GPL)
...
mysql>
----
====

When you are finished, enter *quit* or *exit* to leave the MySQL session.

[[mariadb-environment_variables]]
=== Environment Variables

The MariaDB user name, password, and database name must be configured with the
following environment variables:

.MariaDB Environment Variables
[cols="4a,6a",options="header"]
|===

|Variable Name |Description

|`MYSQL_USER`
|User name for MySQL account to be created.

|`MYSQL_PASSWORD`
|Password for the user account.

|`MYSQL_DATABASE`
|Database name.

|`MYSQL_ROOT_PASSWORD`
|Password for the root user (optional).
|===

[WARNING]
====
You must specify the user name, password, and database name. If you do not
specify all three, the pod will fail to start and {product-title} will
continuously try to restart it.
====

MariaDB settings can be configured with the following environment variables:

.Additional MariaDB Settings
[cols="3a,6a,1a",options="header"]
|===

|Variable Name |Description |Default

|`MYSQL_LOWER_CASE_TABLE_NAMES`
|Sets how the table names are stored and compared.
|0

|`MYSQL_MAX_CONNECTIONS`
|The maximum permitted number of simultaneous client connections.
|151

|`MYSQL_MAX_ALLOWED_PACKET`
|The maximum size of one packet or any generated/intermediate string.
|200M

|`MYSQL_FT_MIN_WORD_LEN`
|The minimum length of the word to be included in a FULLTEXT index.
|4

|`MYSQL_FT_MAX_WORD_LEN`
|The maximum length of the word to be included in a FULLTEXT index.
|20

|`MYSQL_AIO`
|Controls the *innodb_use_native_aio* setting value if the native AIO is broken.
|1

|`MYSQL_TABLE_OPEN_CACHE`
|The number of open tables for all threads.
|400

|`MYSQL_KEY_BUFFER_SIZE`
|The size of the buffer used for index blocks.
|32M (or 10% of available memory)

|`MYSQL_SORT_BUFFER_SIZE`
|The size of the buffer used for sorting.
|256K

|`MYSQL_READ_BUFFER_SIZE`
|The size of the buffer used for a sequential scan.
|8M (or 5% of available memory)

|`MYSQL_INNODB_BUFFER_POOL_SIZE`
|The size of the buffer pool where InnoDB caches table and index data.
|32M (or 50% of available memory)

|`MYSQL_INNODB_LOG_FILE_SIZE`
|The size of each log file in a log group.
|8M (or 15% of available memory)

|`MYSQL_INNODB_LOG_BUFFER_SIZE`
|The size of the buffer that InnoDB uses to write to the log files on disk.
|8M (or 15% of available memory)

|`MYSQL_DEFAULTS_FILE`
|Point to an alternative configuration file.
|/etc/my.cnf

|`MYSQL_BINLOG_FORMAT`
|Set sets the binlog format, supported values are `row` and `statement`.
|statement
|===

[[mariadb-volume-mount-points]]
=== Volume Mount Points

The MariaDB image can be run with mounted volumes to enable persistent storage
for the database:

* *_/var/lib/mysql/data_* - The MySQL data directory is where
MariaDB stores database files.

[NOTE]
====
When mounting a directory from the host into the container, ensure that the
mounted directory has the appropriate permissions. Also verify that the owner
and group of the directory match the user name running inside the container.
====

[[mariadb-changing-passwords]]
=== Changing Passwords

Passwords are part of the image configuration, therefore the only supported
method to change passwords for the database user (`MYSQL_USER`) and *admin*
user is by changing the environment variables `MYSQL_PASSWORD` and
`MYSQL_ROOT_PASSWORD`, respectively.

You can view the current passwords by viewing the pod or deployment
configuration in the web console or by listing the environment variables with
the CLI:

----
$ oc set env pod <pod_name> --list
----

Changing database passwords through SQL statements or any way other than through
the environment variables aforementioned causes a mismatch between the values
stored in the variables and the actual passwords. Whenever a database container
starts, it resets the passwords to the values stored in the environment
variables.

To change these passwords, update one or both of the desired environment
variables for the related deployment configuration(s) using the `oc set env`
command. If multiple deployment configurations utilize these environment
variables, for example in the case of an application created from a template,
you must update the variables on each deployment configuration so that the
passwords are in sync everywhere. This can be done all in the same command:

----
$ oc set env dc <dc_name> [<dc_name_2> ...] \
  MYSQL_PASSWORD=<new_password> \
  MYSQL_ROOT_PASSWORD=<new_root_password>
----

[IMPORTANT]
====
Depending on your application, there may be other environment variables for
passwords in other parts of the application that should also be updated to
match. For example, there could be a more generic `DATABASE_USER` variable in
a front-end pod that should match the database user's password. Ensure that
passwords are in sync for all required environment variables per your
application, otherwise your pods may fail to redeploy when triggered.
====

Updating the environment variables triggers the redeployment of the database
server if you have a
xref:../../dev_guide/deployments/basic_deployment_operations.adoc#config-change-trigger[configuration change
trigger]. Otherwise, you must manually start a new deployment in order to apply
the password changes.

To verify that new passwords are in effect, first open a remote shell session to
the running MariaDB pod:

----
$ oc rsh <pod>
----

From the bash shell, verify the database user's new password:

----
bash-4.2$ mysql -u $MYSQL_USER -p<new_password> -h $HOSTNAME $MYSQL_DATABASE -te "SELECT * FROM (SELECT database()) db CROSS JOIN (SELECT user()) u"
----

If the password was changed correctly, you should see a table like this:

====
----
+------------+---------------------+
| database() | user()              |
+------------+---------------------+
| sampledb   | user0PG@172.17.42.1 |
+------------+---------------------+
----
====

To verify the *root* user's new password:

====
----
bash-4.2$ mysql -u root -p<new_root_password> -h $HOSTNAME $MYSQL_DATABASE -te "SELECT * FROM (SELECT database()) db CROSS JOIN (SELECT user()) u"
----
====

If the password was changed correctly, you should see a table like this:

====
----
+------------+------------------+
| database() | user()           |
+------------+------------------+
| sampledb   | root@172.17.42.1 |
+------------+------------------+
----
====

[[creating-database-service-from-template]]
== Creating a Database Service from a Template

{product-title} provides a xref:../../dev_guide/templates.adoc#dev-guide-templates[template] to make
creating a new database service easy. The template provides parameter fields to
define all the mandatory environment variables (user, password, database name,
etc) with predefined defaults including auto-generation of password values. It
will also define both a
xref:../../architecture/core_concepts/deployments.adoc#deployments-and-deployment-configurations[deployment
configuration] and a
xref:../../architecture/core_concepts/pods_and_services.adoc#services[service].

The MariaDB templates should have been registered in the default *openshift*
project by your cluster administrator during the initial cluster setup.
ifdef::openshift-enterprise,openshift-origin[]
See xref:../../install_config/imagestreams_templates.adoc#install-config-imagestreams-templates[Loading the Default Image Streams and Templates]
for more details, if required.
endif::[]

There are two templates available:

* `mariadb-ephemeral` is for development or testing purposes only because it uses
ephemeral storage for the database content. This means that if the database
pod is restarted for any reason, such as the pod being moved to another node
or the deployment configuration being updated and triggering a redeploy, all
data will be lost.
* `mariadb-persistent` uses a persistent volume store for the database data
which means the data will survive a pod restart. Using persistent volumes
requires a persistent volume pool be defined in the {product-title} deployment.
ifdef::openshift-enterprise,openshift-origin[]
Cluster administrator instructions for setting up the pool are located
xref:../../install_config/persistent_storage/persistent_storage_nfs.adoc#install-config-persistent-storage-persistent-storage-nfs[here].
endif::[]

You can find instructions for instantiating templates by following these
xref:../../dev_guide/templates.adoc#dev-guide-templates[instructions].

Once you have instantiated the service, you can copy the user name, password,
and database name environment variables into a deployment configuration for
another component that intends to access the database. That component can then
access the database through the service that was defined.

[[using-mariadb-replication]]
== Using MariaDB Replication

ifdef::openshift-origin[]
[IMPORTANT]
====
Replication support provided by the MariaDB image is experimental and should not
be used in production.
====
endif::[]

ifdef::openshift-enterprise[]
[NOTE]
====
Enabling clustering for database images is currently in Technology Preview and
not intended for production use.
====
endif::[]

Red Hat provides a proof-of-concept
xref:../../architecture/core_concepts/templates.adoc#architecture-core-concepts-templates[template] for MariaDB
replication (clustering); you can obtain the
https://github.com/sclorg/mysql-container/tree/master/5.5/examples/replica[example
template from GitHub].

To upload the example template into the current project's template
library:

====
----
$ oc create -f \
    https://raw.githubusercontent.com/openshift/mariadb/master/2.4/examples/replica/mariadb-clustered.json
----
====

[IMPORTANT]
====
The example template does not use persistent storage. When
you lose all members of the replication set, your data will be lost.
====

The following sections detail the objects defined in the example template and
describe how they work together to start a cluster of MariaDB servers
implementing master-slave replication and automated failover. This is the
recommended replication strategy for MariaDB.

[[mariadb-creating-the-deploymentconfig]]
=== Creating the Deployment Configuration

To set up MariaDB replication, a
xref:../../architecture/core_concepts/deployments.adoc#deployments-and-deployment-configurations[deployment
configuration] is defined in the example template that defines a
xref:../../architecture/core_concepts/deployments.adoc#replication-controllers[replication
controller]. The replication controller manages the members of the MariaDB
cluster.

To tell a MariaDB server to act as the master, the `command` field in the
container's definition in the deployment configuration must be set to
*run-mysqld-master*. This script acts as an alternative entrypoint for the
MariaDB image and configures the MariaDB server to run as the master in replication.

MariaDB replication requires a special user that relays data between the master
and slaves. The following environment variables are defined in the template for
this purpose:

[cols="3a,6a,1a",options="header"]
|===

|Variable Name |Description |Default

|`*MYSQL_MASTER_USER*`
|The user name of the replication user
|*master*

|`*MYSQL_MASTER_PASSWORD*`
|The password for the replication user
|*generated*
|===

.MySQL Master Deployment Configuration Object Definition in the Example Template
====

[source,yaml]
----
kind: "DeploymentConfig"
apiVersion: "v1"
metadata:
  name: "mysql-master"
spec:
  strategy:
    type: "Recreate"
  triggers:
    - type: "ConfigChange"
  replicas: 1
  selector:
    name: "mysql-master"
  template:
    metadata:
      labels:
        name: "mysql-master"
    spec:
      volumes:
        - name: "mysql-master-data"
          persistentVolumeClaim:
            claimName: "mysql-master"
      containers:
        - name: "server"
          image: "openshift/mariadb-100-centos7"
          command:
            - "run-mysqld-master"
          ports:
            - containerPort: 3306
              protocol: "TCP"
          env:
            - name: "MYSQL_MASTER_USER"
              value: "${MYSQL_MASTER_USER}"
            - name: "MYSQL_MASTER_PASSWORD"
              value: "${MYSQL_MASTER_PASSWORD}"
            - name: "MYSQL_USER"
              value: "${MYSQL_USER}"
            - name: "MYSQL_PASSWORD"
              value: "${MYSQL_PASSWORD}"
            - name: "MYSQL_DATABASE"
              value: "${MYSQL_DATABASE}"
            - name: "MYSQL_ROOT_PASSWORD"
              value: "${MYSQL_ROOT_PASSWORD}"
          volumeMounts:
            - name: "mysql-master-data"
              mountPath: "/var/lib/mysql/data"
          resources: {}
          terminationMessagePath: "/dev/termination-log"
          imagePullPolicy: "IfNotPresent"
          securityContext:
            capabilities: {}
            privileged: false
      restartPolicy: "Always"
      dnsPolicy: "ClusterFirst"
====

Since we claimed a persistent volume in this deployment configuration to have
all data persisted for the MySQL master server, you must ask your cluster
administrator to create a persistent volume that you can claim the storage from.

After the deployment configuration is created and the pod with MariaDB master
server is started, it will create the database defined by `MYSQL_DATABASE` and
configure the server to replicate this database to slaves.

The example provided defines only one replica of the MariaDB master server. This
causes {product-title} to start only one instance of the server. Multiple
instances (multi-master) is not supported and therefore you can not scale this
replication controller.

[[mariadb-creating-the-mysql-slaves]]
==== Creating the MySQL Slaves

To replicate the database created by the
xref:mariadb-creating-the-deploymentconfig[MySQL master], a
deployment configuration is defined in the template. This deployment
configuration creates a replication controller that launches the MySQL image
with the `command` field set to *run-mysqld-slave*. This alternative
entrypoints skips the initialization of the database and configures the MySQL
server to connect to the *mysql-master* service, which is also defined in
example template.

.MariaDB Slave Deployment Configuration Object Definition in the Example Template
====

[source,yaml]
----
kind: "DeploymentConfig"
apiVersion: "v1"
metadata:
  name: "mysql-slave"
spec:
  strategy:
    type: "Recreate"
  triggers:
    - type: "ConfigChange"
  replicas: 1
  selector:
    name: "mysql-slave"
  template:
    metadata:
      labels:
        name: "mysql-slave"
    spec:
      containers:
        - name: "server"
          image: "openshift/mariadb-100-centos7"
          command:
            - "run-mysqld-slave"
          ports:
            - containerPort: 3306
              protocol: "TCP"
          env:
            - name: "MYSQL_MASTER_USER"
              value: "${MYSQL_MASTER_USER}"
            - name: "MYSQL_MASTER_PASSWORD"
              value: "${MYSQL_MASTER_PASSWORD}"
            - name: "MYSQL_DATABASE"
              value: "${MYSQL_DATABASE}"
          resources: {}
          terminationMessagePath: "/dev/termination-log"
          imagePullPolicy: "IfNotPresent"
          securityContext:
            capabilities: {}
            privileged: false
      restartPolicy: "Always"
      dnsPolicy: "ClusterFirst"
----
====

This example deployment configuration starts the replication controller with the
initial number of replicas set to *1*. You can
xref:scaling-the-mariadb-slaves[scale this replication controller] in both
directions, up to the resources capacity of your account.

ifdef::openshift-origin[]
If either the master or any of the slaves goes down, {product-title} will bring them
back up. The master will reuse the persistent volume, while any restarted slaves
will replicate data from the master.
endif::openshift-origin[]

[[mariadb-creating-headless-service]]
=== Creating a Headless Service

The `initiate` argument in the container specification instructs the container to first discover all running member pods within
the MariaDB cluster. To achieve this, a _headless service_ is defined named
*mariadb* in the example template.

To have a headless service, the `*portalIP*` parameter in the service definition
is set to *None*. Then you can use a DNS query to get a list of the pod IP
addresses that represents the current endpoints for this service.

.Headless Service Object Definition in the Example Template
====

[source,yaml]
----
kind: "Service"
apiVersion: "v1"
metadata:
  name: "mysql-master"
  labels:
    name: "mysql-master"
spec:
  ports:
    - protocol: "TCP"
      port: 3306
      targetPort: 3306
      nodePort: 0
  selector:
    name: "mysql-master"
  portalIP: "None"
  type: "ClusterIP"
  sessionAffinity: "None"
status:
  loadBalancer: {}
----
====

[[scaling-the-mariadb-slaves]]
=== Scaling the MariaDB Slaves

To xref:../../dev_guide/deployments/basic_deployment_operations.adoc#scaling[increase the number of members]
in the cluster:

----
$ oc scale rc mysql-slave-1 --replicas=<number>
----

This tells xref:mariadb-creating-the-deploymentconfig[the replication controller] to
create a new MySQL slave pod. When a new slave is created, the slave entrypoint
first attempts to contact the *mysql-master* service and register itself to the
replication set. Once that is done, the MySQL master server sends the slave the
replicated database.

When scaling down, the MySQL slave is shut down and, because the slave does not
have any persistent storage defined, all data on the slave is lost. The MySQL
master server then discovers that the slave is not reachable anymore, and it
automatically removes it from the replication.

[[mariadb-troubleshooting]]
== Troubleshooting

// TODO: Put tags around the body of this section so that
// it can be snarfed by the OSE Troubleshooting Guide.

This section describes some troubles you might encounter
and presents possible resolutions.

[[mariadb-linux-native-aio-failure]]
=== Linux Native AIO Failure

.Symptom
The MySQL container fails to start and the logs show something like:

----
151113  5:06:56 InnoDB: Using Linux native AIO
151113  5:06:56  InnoDB: Warning: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
InnoDB: Warning: io_setup() attempt 1 failed.
InnoDB: Warning: io_setup() attempt 2 failed.
Waiting for MySQL to start ...
InnoDB: Warning: io_setup() attempt 3 failed.
InnoDB: Warning: io_setup() attempt 4 failed.
Waiting for MySQL to start ...
InnoDB: Warning: io_setup() attempt 5 failed.
151113  5:06:59  InnoDB: Error: io_setup() failed with EAGAIN after 5 attempts.
InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
151113  5:06:59 InnoDB: Fatal error: cannot initialize AIO sub-system
151113  5:06:59 [ERROR] Plugin 'InnoDB' init function returned error.
151113  5:06:59 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
151113  5:06:59 [ERROR] Unknown/unsupported storage engine: InnoDB
151113  5:06:59 [ERROR] Aborting
----

.Explanation
MariaDB's storage engine was unable to use the kernel's
AIO (Asynchronous I/O) facilities due to resource limits.

.Resolution

1. Turn off AIO usage entirely,
by setting environment variable `*MYSQL_AIO*` to have value `0`.
On subsequent deployments, this arranges for the
MySQL configuration variable `*innodb_use_native_aio*`
to have value `0`.

2. Increase the `aio-max-nr` kernel resource.
The following example examines the current value of `aio-max-nr` and doubles it.
+
----
$ sysctl fs.aio-max-nr
fs.aio-max-nr = 1048576
# sysctl -w fs.aio-max-nr=2097152
----
+
This is a per-node resolution and lasts until the next node reboot.

// Add more subsections here.
// TEMPLATE:
// .Symptom
// .Explanation
// .Resolution
