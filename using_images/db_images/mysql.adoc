[[using-images-db-images-mysql]]
= MySQL
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
{product-title} provides a container image for running MySQL. This image can
provide database services based on username, password, and database name
settings provided via configuration.

== Versions
Currently, {product-title} provides versions
link:https://github.com/openshift/mysql/tree/master/5.5[5.5],
link:https://github.com/openshift/mysql/tree/master/5.5[5.6], and
link:https://github.com/openshift/mysql/tree/master/5.6[5.7] of MySQL.

== Images

ifdef::openshift-online[]
RHEL 7 images are available through the Red Hat Registry:

----
$ docker pull registry.access.redhat.com/openshift3/mysql-55-rhel7
$ docker pull registry.access.redhat.com/rhscl/mysql-56-rhel7
$ docker pull registry.access.redhat.com/rhscl/mysql-57-rhel7
----

You can use these images through the `mysql` image stream.
endif::[]

ifndef::openshift-online[]
This image comes in two flavors, depending on your needs:

* RHEL 7
* CentOS 7

*RHEL 7 Based Images*

The RHEL 7 image is available through the Red Hat Registry:

----
$ docker pull registry.access.redhat.com/openshift3/mysql-55-rhel7
$ docker pull registry.access.redhat.com/rhscl/mysql-56-rhel7
$ docker pull registry.access.redhat.com/rhscl/mysql-57-rhel7
----

*CentOS 7 Based Images*

CentOS images for MySQL 5.5 and 5.6 are available on Docker Hub:

----
$ docker pull openshift/mysql-55-centos7
$ docker pull openshift/mysql-56-centos7
----

To use these images, you can either access them directly from these
registries or push them into your {product-title} Docker registry. Additionally,
you can create an ImageStream that points to the image,
either in your Docker registry or at the external location. Your {product-title}
resources can then reference the ImageStream. You can find
https://github.com/openshift/origin/tree/master/examples/image-streams[example]
ImageStream definitions for all the provided {product-title} images.
endif::[]

== Configuration and Usage

=== Initializing the Database

The first time you use the shared volume, the database is created along with
the database administrator user and the MySQL root user (if you specify the
`*MYSQL_ROOT_PASSWORD*` environment variable).  Afterwards, the MySQL daemon
starts up. If you are re-attaching the volume to another container, then the
database, database user, and the administrator user are not created, and the
MySQL daemon starts.

The following command creates a new database
xref:../../architecture/core_concepts/pods_and_services.adoc#pods[pod] with
MySQL running in a container:

----
$ oc new-app \
    -e MYSQL_USER=<username> \
    -e MYSQL_PASSWORD=<password> \
    -e MYSQL_DATABASE=<database_name> \
ifdef::openshift-enterprise,openshift-dedicated[]
    registry.access.redhat.com/openshift3/mysql-55-rhel7
endif::[]
ifdef::openshift-origin[]
    openshift/mysql-55-centos7
endif::[]
ifdef::openshift-online[]
    mysql:5.5
endif::[]
----

=== Running MySQL Commands in Containers

{product-title} uses https://www.softwarecollections.org/[Software Collections]
(SCLs) to install and launch MySQL. If you want to execute a MySQL command
inside of a running container (for debugging), you must invoke it using bash.

To do so, first identify the name of the pod. For example, you can view the list
of pods in your current project:

----
$ oc get pods
----

Then, open a remote shell session to the pod:

----
$ oc rsh <pod>
----

When you enter the container, the required SCL is automatically enabled.

You can now run the *mysql* command from the bash shell to start a MySQL
interactive session and perform normal MySQL operations. For example, to
authenticate as the database user:

====
----
bash-4.2$ mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -h $HOSTNAME $MYSQL_DATABASE
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 4
Server version: 5.5.37 MySQL Community Server (GPL)
...
mysql>
----
====

When you are finished, enter *quit* or *exit* to leave the MySQL session.

=== Environment Variables

The MySQL user name, password, and database name must be configured with the
following environment variables:

.MySQL Environment Variables
[cols="4a,6a",options="header"]
|===

|Variable Name |Description

|`*MYSQL_USER*`
|Specifies the user name for the database user that is created for use by your
application.

|`*MYSQL_PASSWORD*`
|Password for the `*MYSQL_USER*`.

|`*MYSQL_DATABASE*`
|Name of the database to which `*MYSQL_USER*` has full rights.

|`*MYSQL_ROOT_PASSWORD*`
|Optional password for the root user. If this is not set, then remote login to
the root account is not possible. Local connections from within the container
are always permitted without a password.

|`*MYSQL_SERVICE_HOST*`
|Service host variable automatically created by Kubernetes.

|`*MYSQL_SERVICE_PORT*`
|Service port variable automatically created by Kubernetes.
|===

[WARNING]
====
You must specify the user name, password, and database name. If you do not
specify all three, the pod will fail to start and {product-title} will
continuously try to restart it.
====

MySQL settings can be configured with the following environment variables:

.Additional MySQL Settings
[cols="3a,6a,1a",options="header"]
|===

|Variable Name |Description |Default

|`*MYSQL_LOWER_CASE_TABLE_NAMES*`
|Sets how the table names are stored and compared.
|0

|`*MYSQL_MAX_CONNECTIONS*`
|The maximum permitted number of simultaneous client connections.
|151

|`*MYSQL_MAX_ALLOWED_PACKET*`
|The maximum size of one packet or any generated/intermediate string.
|200M

|`*MYSQL_FT_MIN_WORD_LEN*`
|The minimum length of the word to be included in a FULLTEXT index.
|4

|`*MYSQL_FT_MAX_WORD_LEN*`
|The maximum length of the word to be included in a FULLTEXT index.
|20

|`*MYSQL_AIO*`
|Controls the *innodb_use_native_aio* setting value if the native AIO is broken.
|1

|`*MYSQL_TABLE_OPEN_CACHE*`
|The number of open tables for all threads.
|400

|`*MYSQL_KEY_BUFFER_SIZE*`
|The size of the buffer used for index blocks.
|
ifdef::openshift-online[]
10% of available memory
endif::[]
ifndef::openshift-online[]
32M (or 10% of available memory)
endif::[]

|`*MYSQL_SORT_BUFFER_SIZE*`
|The size of the buffer used for sorting.
|256K

|`*MYSQL_READ_BUFFER_SIZE*`
|The size of the buffer used for a sequential scan.
|
ifdef::openshift-online[]
5% of available memory
endif::[]
ifndef::openshift-online[]
8M (or 5% of available memory)
endif::[]

|`*MYSQL_INNODB_BUFFER_POOL_SIZE*`
|The size of the buffer pool where InnoDB caches table and index data.
|
ifdef::openshift-online[]
50% of available memory
endif::[]
ifndef::openshift-online[]
32M (or 50% of available memory)
endif::[]

|`*MYSQL_INNODB_LOG_FILE_SIZE*`
|The size of each log file in a log group.
|
ifdef::openshift-online[]
15% of available memory
endif::[]
ifndef::openshift-online[]
8M (or 15% of available memory)
endif::[]

|`*MYSQL_INNODB_LOG_BUFFER_SIZE*`
|The size of the buffer that InnoDB uses to write to the log files on disk.
|
ifdef::openshift-online[]
15% of available memory
endif::[]
ifndef::openshift-online[]
8M (or 15% of available memory)
endif::[]
|===

ifdef::openshift-online[]
Some of the memory-related parameters have percentages as default values.  These
values are calculated dynamically during a container's startup based on
xref:../../dev_guide/compute_resources.adoc#dev-memory-limits[memory limits].
endif::[]

ifndef::openshift-online[]
Some of the memory-related parameters have two default values. The fixed value
is used when a container does not have xref:../../dev_guide/compute_resources.adoc#dev-memory-limits[memory limits]
assigned. The other value is calculated dynamically during a container's startup
based on available memory.
endif::[]

=== Volume Mount Points
The MySQL image can be run with mounted volumes to enable persistent storage for
the database:

* *_/var/lib/mysql/data_* - This is the data directory where MySQL stores
database files.

[[mysql-changing-passwords]]

=== Changing Passwords

Passwords are part of the image configuration, therefore the only supported
method to change passwords for the database user (`*MYSQL_USER*`) and *root*
user is by changing the environment variables `*MYSQL_PASSWORD*` and
`*MYSQL_ROOT_PASSWORD*`, respectively.

You can view the current passwords by viewing the pod or deployment
configuration in the web console or by listing the environment variables with
the CLI:

----
$ oc set env pod <pod_name> --list
----

Whenever `*MYSQL_ROOT_PASSWORD*` is set, it enables remote access for the *root*
user with the given password, and whenever it is unset, remote access for the
*root* user is disabled. This does not affect the regular user `*MYSQL_USER*`,
who always has remote access. This also does not affect local access by the
*root* user, who can always log in without a password in *localhost*.

Changing database passwords through SQL statements or any way other than through
the environment variables aforementioned causes a mismatch between the values
stored in the variables and the actual passwords. Whenever a database container
starts, it resets the passwords to the values stored in the environment
variables.

To change these passwords, update one or both of the desired environment
variables for the related deployment configuration(s) using the `oc set env`
command. If multiple deployment configurations utilize these environment
variables, for example in the case of an application created from a template,
you must update the variables on each deployment configuration so that the
passwords are in sync everywhere. This can be done all in the same command:

----
$ oc set env dc <dc_name> [<dc_name_2> ...] \
  MYSQL_PASSWORD=<new_password> \
  MYSQL_ROOT_PASSWORD=<new_root_password>
----

[IMPORTANT]
====
Depending on your application, there may be other environment variables for
passwords in other parts of the application that should also be updated to
match. For example, there could be a more generic `*DATABASE_USER*` variable in
a front-end pod that should match the database user's password. Ensure that
passwords are in sync for all required environment variables per your
application, otherwise your pods may fail to redeploy when triggered.
====

Updating the environment variables triggers the redeployment of the database
server if you have a
xref:../../dev_guide/deployments/basic_deployment_operations.adoc#config-change-trigger[configuration change
trigger]. Otherwise, you must manually start a new deployment in order to apply
the password changes.

To verify that new passwords are in effect, first open a remote shell session to
the running MySQL pod:

----
$ oc rsh <pod>
----

From the bash shell, verify the database user's new password:

----
bash-4.2$ mysql -u $MYSQL_USER -p<new_password> -h $HOSTNAME $MYSQL_DATABASE -te "SELECT * FROM (SELECT database()) db CROSS JOIN (SELECT user()) u"
----

If the password was changed correctly, you should see a table like this:

====
----
+------------+---------------------+
| database() | user()              |
+------------+---------------------+
| sampledb   | user0PG@172.17.42.1 |
+------------+---------------------+
----
====

To verify the *root* user's new password:

====
----
bash-4.2$ mysql -u root -p<new_root_password> -h $HOSTNAME $MYSQL_DATABASE -te "SELECT * FROM (SELECT database()) db CROSS JOIN (SELECT user()) u"
----
====

If the password was changed correctly, you should see a table like this:

====
----
+------------+------------------+
| database() | user()           |
+------------+------------------+
| sampledb   | root@172.17.42.1 |
+------------+------------------+
----
====

== Creating a Database Service from a Template

{product-title} provides a xref:../../dev_guide/templates.adoc#dev-guide-templates[template] to make
creating a new database service easy. The template provides parameter fields to
define all the mandatory environment variables (user, password, database name,
etc) with predefined defaults including auto-generation of password values. It
will also define both a
xref:../../architecture/core_concepts/deployments.adoc#deployments-and-deployment-configurations[deployment
configuration] and a
xref:../../architecture/core_concepts/pods_and_services.adoc#services[service].

The MySQL
ifdef::openshift-online[]
template
endif::[]
ifndef::openshift-online[]
templates
endif::[]
should have been registered in the default *openshift*
project by your cluster administrator during the initial cluster setup.
ifdef::openshift-enterprise,openshift-origin[]
See xref:../../install_config/imagestreams_templates.adoc#install-config-imagestreams-templates[Loading the Default Image Streams and Templates]
for more details, if required.
endif::[]

ifdef::openshift-online[]
The following template is available:
endif::[]
ifndef::openshift-online[]
There are two templates available:
endif::[]

ifndef::openshift-online[]
* `mysql-ephemeral` is for development or testing purposes only because it uses
ephemeral storage for the database content. This means that if the database
pod is restarted for any reason, such as the pod being moved to another node
or the deployment configuration being updated and triggering a redeploy, all
data will be lost.
endif::[]
* `mysql-persistent` uses a persistent volume store for the database data which
means the data will survive a pod restart.
ifndef::openshift-online[]
Using persistent volumes requires a persistent volume pool be defined in the
{product-title} deployment.
endif::[]
ifdef::openshift-enterprise,openshift-origin[]
Cluster administrator instructions for setting up the pool are located
xref:../../install_config/persistent_storage/persistent_storage_nfs.adoc#install-config-persistent-storage-persistent-storage-nfs[here].
endif::[]

You can find instructions for instantiating templates by following these
xref:../../dev_guide/templates.adoc#dev-guide-templates[instructions].

Once you have instantiated the service, you can copy the user name, password,
and database name environment variables into a deployment configuration for
another component that intends to access the database. That component can then
access the database via the service that was defined.

[[using-mysql-replication]]
== Using MySQL Replication

ifdef::openshift-origin[]
[IMPORTANT]
====
Replication support provided by the MySQL image is experimental and should not
be used in production.
====
endif::[]

ifdef::openshift-enterprise,openshift-dedicated,openshift-online[]
[NOTE]
====
Enabling clustering for database images is currently in Technology Preview and
not intended for production use.
====
endif::[]

Red Hat provides a proof-of-concept
xref:../../dev_guide/templates.adoc#dev-guide-templates[template] for MySQL
master-slave replication (clustering); you can obtain the
https://github.com/openshift/mysql/tree/master/5.5/examples/replica[example
template from GitHub].

To upload the example template into the current project's template library:

====
----
$ oc create -f \
    https://raw.githubusercontent.com/openshift/mysql/master/5.5/examples/replica/mysql_replica.json
----
====

The following sections detail the objects defined in the example template and
describe how they work together to start a cluster of MySQL servers implementing
master-slave replication. This is the recommended replication strategy for
MySQL.

[[creating-the-deployment-configuration-for-mysql-master]]
=== Creating the Deployment Configuration for the MySQL Master

To set up MySQL replication, a
xref:../../architecture/core_concepts/deployments.adoc#deployments-and-deployment-configurations[deployment
configuration] is defined in the example template that defines a
xref:../../architecture/core_concepts/deployments.adoc#replication-controllers[replication
controller]. For MySQL master-slave replication, two deployment configurations
are needed. One deployment configuration defines the MySQL _master_ server and
second the MySQL _slave_ servers.

To tell a MySQL server to act as the master, the `*command*` field in the
container's definition in the deployment configuration must be set to
*run-mysqld-master*. This script acts as an alternative entrypoint for the
MySQL image and configures the MySQL server to run as the master in replication.

MySQL replication requires a special user that relays data between the master
and slaves. The following environment variables are defined in the template for
this purpose:

[cols="3a,6a,1a",options="header"]
|===

|Variable Name |Description |Default

|`*MYSQL_MASTER_USER*`
|The user name of the replication user
|*master*

|`*MYSQL_MASTER_PASSWORD*`
|The password for the replication user
|*generated*
|===

.MySQL Master Deployment Configuration Object Definition in the Example Template
====

[source,yaml]
----
kind: "DeploymentConfig"
apiVersion: "v1"
metadata:
  name: "mysql-master"
spec:
  strategy:
    type: "Recreate"
  triggers:
    - type: "ConfigChange"
  replicas: 1
  selector:
    name: "mysql-master"
  template:
    metadata:
      labels:
        name: "mysql-master"
    spec:
      volumes:
        - name: "mysql-master-data"
          persistentVolumeClaim:
            claimName: "mysql-master"
      containers:
        - name: "server"
          image: "openshift/mysql-55-centos7"
          command:
            - "run-mysqld-master"
          ports:
            - containerPort: 3306
              protocol: "TCP"
          env:
            - name: "MYSQL_MASTER_USER"
              value: "${MYSQL_MASTER_USER}"
            - name: "MYSQL_MASTER_PASSWORD"
              value: "${MYSQL_MASTER_PASSWORD}"
            - name: "MYSQL_USER"
              value: "${MYSQL_USER}"
            - name: "MYSQL_PASSWORD"
              value: "${MYSQL_PASSWORD}"
            - name: "MYSQL_DATABASE"
              value: "${MYSQL_DATABASE}"
            - name: "MYSQL_ROOT_PASSWORD"
              value: "${MYSQL_ROOT_PASSWORD}"
          volumeMounts:
            - name: "mysql-master-data"
              mountPath: "/var/lib/mysql/data"
          resources: {}
          terminationMessagePath: "/dev/termination-log"
          imagePullPolicy: "IfNotPresent"
          securityContext:
            capabilities: {}
            privileged: false
      restartPolicy: "Always"
      dnsPolicy: "ClusterFirst"
----
====

ifndef::openshift-online[]
Since we claimed a persistent volume in this deployment configuration to have
all data persisted for the MySQL master server, you must ask your cluster
administrator to create a persistent volume that you can claim the storage from.
endif::[]

After the deployment configuration is created and the pod with MySQL master
server is started, it will create the database defined by `*MYSQL_DATABASE*` and
configure the server to replicate this database to slaves.

The example provided defines only one replica of the MySQL master server. This
causes {product-title} to start only one instance of the server. Multiple
instances (multi-master) is not supported and therefore you can not scale this
replication controller.

[[creating-the-mysql-slaves]]

To replicate the database created by the
xref:creating-the-deployment-configuration-for-mysql-master[MySQL master], a
deployment configuration is defined in the template. This deployment
configuration creates a replication controller that launches the MySQL image
with the `*command*` field set to *run-mysqld-slave*. This alternative
entrypoints skips the initialization of the database and configures the MySQL
server to connect to the *mysql-master* service, which is also defined in
example template.

.MySQL Slave Deployment Configuration Object Definition in the Example Template
====

[source,yaml]
----
kind: "DeploymentConfig"
apiVersion: "v1"
metadata:
  name: "mysql-slave"
spec:
  strategy:
    type: "Recreate"
  triggers:
    - type: "ConfigChange"
  replicas: 1
  selector:
    name: "mysql-slave"
  template:
    metadata:
      labels:
        name: "mysql-slave"
    spec:
      containers:
        - name: "server"
          image: "openshift/mysql-55-centos7"
          command:
            - "run-mysqld-slave"
          ports:
            - containerPort: 3306
              protocol: "TCP"
          env:
            - name: "MYSQL_MASTER_USER"
              value: "${MYSQL_MASTER_USER}"
            - name: "MYSQL_MASTER_PASSWORD"
              value: "${MYSQL_MASTER_PASSWORD}"
            - name: "MYSQL_DATABASE"
              value: "${MYSQL_DATABASE}"
          resources: {}
          terminationMessagePath: "/dev/termination-log"
          imagePullPolicy: "IfNotPresent"
          securityContext:
            capabilities: {}
            privileged: false
      restartPolicy: "Always"
      dnsPolicy: "ClusterFirst"
----
====

This example deployment configuration starts the replication controller with the
initial number of replicas set to *1*. You can
xref:scaling-the-mysql-slaves[scale this replication controller] in both
directions, up to the resources capacity of your account.

ifdef::openshift-origin[]
If either the master or any of the slaves goes down, {product-title} will bring them
back up. The master will reuse the persistent volume, while any restarted slaves
will replicate data from the master.
endif::openshift-origin[]

[[mysql-creating-headless-service]]
=== Creating a Headless Service

The pods created by the MySQL slave replication controller must reach the MySQL
master server in order to register for replication. The example template defines
a headless service named *mysql-master* for this purpose. This service is not
used only for replication, but the clients can also send the queries to
*mysql-master:3306* as the MySQL host.

To have a headless service, the `*portalIP*` parameter in the service definition
is set to *None*. Then you can use a DNS query to get a list of the pod IP
addresses that represents the current endpoints for this service.

.Headless Service Object Definition in the Example Template
====

[source,json]
----
kind: "Service"
apiVersion: "v1"
metadata:
  name: "mysql-master"
  labels:
    name: "mysql-master"
spec:
  ports:
    - protocol: "TCP"
      port: 3306
      targetPort: 3306
      nodePort: 0
  selector:
    name: "mysql-master"
  portalIP: "None"
  type: "ClusterIP"
  sessionAffinity: "None"
status:
  loadBalancer: {}
----
====

[[scaling-the-mysql-slaves]]
=== Scaling the MySQL Slaves

To xref:../../dev_guide/deployments/basic_deployment_operations.adoc#scaling[increase the number of members]
in the cluster:

----
$ oc scale rc mysql-slave-1 --replicas=<number>
----

This tells xref:creating-the-deployment-configuration-for-mysql-master[the replication controller] to
create a new MySQL slave pod. When a new slave is created, the slave entrypoint
first attempts to contact the *mysql-master* service and register itself to the
replication set. Once that is done, the MySQL master server sends the slave the
replicated database.

When scaling down, the MySQL slave is shut down and, because the slave does not
have any persistent storage defined, all data on the slave is lost. The MySQL
master server then discovers that the slave is not reachable anymore, and it
automatically removes it from the replication.

[[troubleshooting]]
== Troubleshooting

// TODO: Put tags around the body of this section so that
// it can be snarfed by the OSE Troubleshooting Guide.

This section describes some troubles you might encounter
and presents possible resolutions.

[[linux-native-aio-failure]]
=== Linux Native AIO Failure

.Symptom
The MySQL container fails to start and the logs show something like:

----
151113  5:06:56 InnoDB: Using Linux native AIO
151113  5:06:56  InnoDB: Warning: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
InnoDB: Warning: io_setup() attempt 1 failed.
InnoDB: Warning: io_setup() attempt 2 failed.
Waiting for MySQL to start ...
InnoDB: Warning: io_setup() attempt 3 failed.
InnoDB: Warning: io_setup() attempt 4 failed.
Waiting for MySQL to start ...
InnoDB: Warning: io_setup() attempt 5 failed.
151113  5:06:59  InnoDB: Error: io_setup() failed with EAGAIN after 5 attempts.
InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
151113  5:06:59 InnoDB: Fatal error: cannot initialize AIO sub-system
151113  5:06:59 [ERROR] Plugin 'InnoDB' init function returned error.
151113  5:06:59 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
151113  5:06:59 [ERROR] Unknown/unsupported storage engine: InnoDB
151113  5:06:59 [ERROR] Aborting
----

.Explanation
MySQL's storage engine was unable to use the kernel's
AIO (Asynchronous I/O) facilities due to resource limits.

.Resolution

Turn off AIO usage entirely
by setting environment variable `*MYSQL_AIO*` to have value `0`.
On subsequent deployments, this arranges for the
MySQL configuration variable `*innodb_use_native_aio*`
to have value `0`.

ifndef::openshift-online[]
Alternatively, increase the `aio-max-nr` kernel resource.
The following example examines the current value of `aio-max-nr` and doubles it.

----
$ sysctl fs.aio-max-nr
fs.aio-max-nr = 1048576
# sysctl -w fs.aio-max-nr=2097152
----

This is a per-node resolution and lasts until the next node reboot.
endif::[]

// Add more subsections here.
// TEMPLATE:
// .Symptom
// .Explanation
// .Resolution
