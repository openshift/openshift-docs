= MongoDB
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
OpenShift provides a Docker image for running MongoDB.  This image can provide
database services based on username, password, and database name settings
provided via configuration.

== Versions
Currently, OpenShift provides version
https://github.com/openshift/mongodb/tree/master/2.4[2.4] of MongoDB.

== Images

This image comes in two flavors, depending on your needs:

* RHEL 7
* CentOS 7

*RHEL 7 Based Image*

The RHEL 7 image is available through Red Hat's subscription registry via:

----
$ docker pull registry.access.redhat.com/openshift3/mongodb-24-rhel7
----

*CentOS 7 Based Image*

This image is available on DockerHub. To download it:

----
$ docker pull openshift/mongodb-24-centos7
----

To use these images, you can either access them directly from these
registries or push them into your OpenShift docker registry. Additionally,
you can create an ImageStream that points to the image,
either in your docker registry or at the external location. Your OpenShift
resources can then reference the ImageStream. You can find
https://github.com/openshift/origin/tree/master/examples/image-streams[example]
ImageStream definitions for all the provided OpenShift images.

== Configuration and Usage

=== Initializing the Database

The first time you use the shared volume, the database is created along with the
database administrator user. Afterwards, the MongoDB daemon starts up. If you
are re-attaching the volume to another container, then the database, database
user, and the administrator user are not created, and the MongoDB daemon starts.

This example will create a new database Pod with MongoDB running in a container:

----
$ oc new-app -e MONGODB_USER=<username>,MONGODB_PASSWORD=<password>,MONGODB_DATABASE=<database_name>,MONGODB_ADMIN_PASSWORD=<admin_password> openshift/mongodb-24-centos7
----


=== Running MongoDB Commands in Containers

OpenShift uses https://www.softwarecollections.org/[Software Collections] to
install and launch MongoDB. If you want to execute a MongoDB command inside of a
running container (for debugging), you must invoke it using bash, to make sure
the MongoDB collection is enabled, for example:

----
$ oc exec -ti -p <POD> -c <CONTAINER> /bin/bash -c mongo
----

To enter a container from the host:

----
$ oc exec -it -p <POD> -c <CONTAINER> /bin/bash
----

When you enter the container, the required software collection is automatically enabled.

=== Environment Variables

The MongoDB username, password, database name, and admin password must be
configured with the following environment variables:

.MongoDB Environment Variables
[cols="4a,6a",options="header"]
|===

|Variable name |Description

|`*MONGODB_USER*`
|User name for MongoDB account to be created.

|`*MONGODB_PASSWORD*`
|Password for the user account.

|`*MONGODB_DATABASE*`
|Database name.

|`*MONGODB_ADMIN_PASSWORD*`
|Password for the admin user.
|===

[WARNING]
====
You must specify the username, password, database name, and admin password.
If you do not specify all four, the Pod will fail to start and OpenShift
will continuously try to restart it.
====

[NOTE]
====
Admin user name is set to `admin` and you have to specify his password by
setting `MONGODB_ADMIN_PASSWORD` environment variable. This process is done
upon database initialization.
====

MongoDB settings can be configured with the following environment variables.

.Additional MongoDB Settings
[cols="3a,6a,1a",options="header"]
|===

|Variable name |Description |Default

|`*MONGODB_NOPREALLOC*`
|Disable data file preallocation.
|true

|`*MONGODB_SMALLFILES*`
|Set MongoDB to use a smaller default data file size.
|true

|`*MONGODB_QUIET*`
|Runs MongoDB in a quiet mode that attempts to limit the amount of output.
|true
|===

=== Volume Mount Points

The MongoDB image can be run with mounted volumes to enable persistent storage for the database:

* *_/var/lib/mongodb_* - This is the database directory where
MongoDB stores database files.

== Creating a Database Service from a Template

OpenShift provides a link:../../dev_guide/templates.html[template] to make creating a new database service easy.  The template provides parameter fields to define all the mandatory environment variables (user, password, database name, etc) with predefined defaults including auto-generation of password values.  It will also define both a link:../../dev_guide/deployments.html[DeploymentConfig] and a link:../../architecture/core_concepts/pods_and_services.html#services[Service].

The MongoDB templates should have been registered in the `openshift` project by the administrator during the link:../../admin_guide/install/first_steps.html[first steps] setup process.  There are two templates available: 

* `mongodb-ephemeral` is for development/testing purposes only because it uses ephemeral storage for the database content.  This means that if the database Pod is restarted for any reason, such as the Pod being moved to another node or the DeploymentConfig being updated and triggered a redeploy, all data will be lost.
* `mongodb-persistent` uses a persistent volume store for the database data which means the data will survive a Pod restart.  Using persistent volumes requires a persistent volume pool be defined in the OpenShift deployment.  Instructions for setting up the pool are located link:../../admin_guide/persistent_storage_nfs.html[here].


You can find instructions for instantiating templates by following these link:../../dev_guide/templates.html#creating-resources-from-a-template[instructions].

Once you have instantiated the service, you can copy the username, password, and database name environment variables into a DeploymentConfig for another component that intends to access the database.  That component can then access the database via the Service that was defined.

== Using MongoDB replication

IMPORTANT: The replication support provided by the MongoDB image is experimental and
should not be used in production.

The full source of replication example is available here:
https://github.com/openshift/mongodb/tree/master/2.4/examples/replica

IMPORTANT: The template provided in the example above does not use persistent
storage. When you lose all members of the replication set, your data will be
lost.

This section describes how to start a cluster of MongoDB servers that implements
master-slave replication and automated failover. This is the recommended
replication strategy for MongoDB.

=== Creating the DeploymentConfig

To start with MongoDB replication, you first have to define a DeploymentConfig
that defines a replication controller which will manage the members of the
MongoDB cluster. To tell a MongoDB server that the member will be part of the
cluster, you have to provide additional environment variables for the container
defined in the replication controller pod template:

[cols="3a,6a,1a",options="header"]
|===

|Variable name |Description |Default

|`*MONGODB_REPLICA_NAME*`
|Specify the name of the replication set.
|rs0

|`*MONGODB_KEYFILE_VALUE*`
|See: http://docs.mongodb.org/manual/tutorial/generate-key-file[Generate a Key File]
|generated
|===

The DeploymentConfig defined in the example replication template looks like
this:

[source,json]
----
{
      "kind": "DeploymentConfig",
      "apiVersion": "v1",
      "metadata": {
        "name": "${MONGODB_SERVICE_NAME}",
      },
      "spec": {
        "strategy": {
          "type": "Recreate",
          "resources": {}
        },
        "triggers": [
          {
            "type":"ConfigChange"
          }
        ],
        "replicas": 3,
        "selector": {
          "name": "mongodb-replica"
        },
        "template": {
          "metadata": {
            "labels": {
              "name": "mongodb-replica"
            }
          },
          "spec": {
            "containers": [
              {
                "name":  "member",
                "image": "openshift/mongodb-24-centos7",
                "env": [
                  {
                    "name": "MONGODB_USER",
                    "value": "${MONGODB_USER}"
                  },
                  {
                    "name": "MONGODB_PASSWORD",
                    "value": "${MONGODB_PASSWORD}"
                  },
                  {
                    "name": "MONGODB_DATABASE",
                    "value": "${MONGODB_DATABASE}"
                  },
                  {
                    "name": "MONGODB_ADMIN_PASSWORD",
                    "value": "${MONGODB_ADMIN_PASSWORD}"
                  },
                  {
                    "name": "MONGODB_REPLICA_NAME",
                    "value": "${MONGODB_REPLICA_NAME}"
                  },
                  {
                    "name": "MONGODB_SERVICE_NAME",
                    "value": "${MONGODB_SERVICE_NAME}"
                  },
                  {
                    "name": "MONGODB_KEYFILE_VALUE",
                    "value": "${MONGODB_KEYFILE_VALUE}"
                  }
                ],
                "ports":[
                  {
                    "containerPort": 27017,
                    "protocol": "TCP"
                  }
                ]
              }
            ]
          }
        },
        "restartPolicy": "Never",
        "dnsPolicy": "ClusterFirst"
      }
    }
----

When the DeploymentConfig is created and the pods with MongoDB cluster members
are started, they will not be initialized, but they will start as part of the
`rs0` replication set, as the value of the `MONGODB_REPLICA_NAME` is set to
`rs0` by default.

=== Creating the service pod

To initialize members created by the DeploymentConfig, a "service pod" has to be
defined in the Template. This pod will start MongoDB with the `initiate` argument,
which will make the container entrypoint behave slighly different than having a
regular standalone MongoDB database.

The service pod defined in the example replication template looks like this:

[source,json]
----
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "mongodb-service",
    "creationTimestamp": null,
    "labels": {
      "name": "mongodb-service"
    }
  },
  "spec": {
    "restartPolicy": "Never",
    "dnsPolicy": "ClusterFirst",
    "containers": [
      {
        "name": "initiate",
        "image": "openshift/mongodb-24-centos7",
        "args": ["initiate"],
        "env": [
          {
            "name": "MONGODB_USER",
            "value": "${MONGODB_USER}"
          },
          {
            "name": "MONGODB_PASSWORD",
            "value": "${MONGODB_PASSWORD}"
          },
          {
            "name": "MONGODB_DATABASE",
            "value": "${MONGODB_DATABASE}"
          },
          {
            "name": "MONGODB_ADMIN_PASSWORD",
            "value": "${MONGODB_ADMIN_PASSWORD}"
          },
          {
            "name": "MONGODB_REPLICA_NAME",
            "value": "${MONGODB_REPLICA_NAME}"
          },
          {
            "name": "MONGODB_SERVICE_NAME",
            "value": "${MONGODB_SERVICE_NAME}"
          },
          {
            "name": "MONGODB_KEYFILE_VALUE",
            "value": "${MONGODB_KEYFILE_VALUE}"
          }
        ]
      }
    ]
  }
}
----

=== Creating headless service

The `initiate` argument in the container specification above will make the container
first discover all running member pods within the MongoDB cluster. To achieve
that, we defined a 'headless' service named 'mongodb' in the example template.

To have a 'headless' service, you have to set the `portalIP` field value in the
service definition to `None`.  Then you can use a DNS query to get a list of the
pod IP addresses that represents the current endpoints for this service.

The example replication template defines the headless service as following:

[source,json]
----
{
  "kind": "Service",
  "apiVersion": "v1",
  "metadata": {
    "name": "${MONGODB_SERVICE_NAME}",
    "labels": {
      "name": "${MONGODB_SERVICE_NAME}"
    }
  },
  "spec": {
    "ports": [
      {
        "protocol": "TCP",
        "port": 27017,
        "targetPort": 27017,
        "nodePort": 0
      }
    ],
    "selector": {
      "name": "mongodb-replica"
    },
    "portalIP": "None",
    "type": "ClusterIP",
    "sessionAffinity": "None"
  },
  "status": {
    "loadBalancer": {}
  }
}
----

=== Creating the final replication set

When the script that runs as the container entrypoint has the IP addresses of
all running MongoDB members, it creates a MongoDB replication set configuration
where it will list all member IP addresses and then initiate the replication set
by `rs.initiate(config)`.  Then the script waits until the MongoDB elect the
PRIMARY member of the cluster.

Once the PRIMARY member has been elected, the entrypoint script will start
creating MongoDB users and databases. The 'service pod' is running MongoDB
without the `--auth` argument, so it can bootstrap the PRIMARY member without
providing any authentication.

When the user accounts and databases are created and the data are replicated to
other members, it is time for the 'service pod' to give up its PRIMARY role and
shut down. It is important to set the `restartPolicy` field in the 'service
pod' to `Never` to prevent the 'service pod' from restarting when the container
exits.  Right after the 'service pod' goes down, other members will start a new
election and the new PRIMARY member is being elected from the running members.

The clients can then start using the MongoDB by sending the queries to the
`mongodb` service. As this service is a 'headless' service, they don't have to
provide the IP address. They can use `mongodb:27017` for the client connections.
The service will then send the query to one of the members in the replication
set.

=== Scaling MongoDB replication set

To increase the number of members in the cluster, use this command:

```
$ oc scale rc mongodb-1 --replicas=4
```

This will tell the replication controller to create a new MongoDB member pod.
When a new member is created, the member entrypoint will first attempt to
discover other running members in the cluster. Then it will choose one and add
itself to the list of members. Once the replication configuration is updated, the
other members will replicate the data to a new pod and start a new election.
