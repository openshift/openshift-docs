[id="virt-2-6-release-notes"]
= {RN_BookName}
include::modules/virt-document-attributes.adoc[]
include::modules/common-attributes.adoc[]
:context: virt-release-notes
toc::[]

== About Red Hat {VirtProductName}

Red Hat {VirtProductName} enables you to bring traditional virtual machines (VMs) into {product-title} where they run alongside containers, and are managed as native Kubernetes objects.

{VirtProductName} is represented by the image:Operator_Icon-OpenShift_Virtualization-5.png[{VirtProductName},40,40] logo.

You can use {VirtProductName} with either the xref:../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[OVN-Kubernetes] or the xref:../networking/openshift_sdn/about-openshift-sdn.adoc#about-openshift-sdn[OpenShiftSDN] default Container Network Interface (CNI) network provider.

Learn more about xref:../virt/about-virt.adoc#about-virt[what you can do with {VirtProductName}].

include::modules/virt-supported-cluster-version.adoc[leveloffset=+2]

[id="virt-guest-os"]
=== Supported guest operating systems
{VirtProductName} guests can use the following operating systems:

* Red Hat Enterprise Linux 6, 7, and 8.
* Microsoft Windows Server 2012 R2, 2016, and 2019.
* Microsoft Windows 10.
* CentOS 7 and 8.

Other operating system templates shipped with {VirtProductName} are not supported.

[id="virt-2-6-new"]
== New and changed features

//CNV-7544: Microsoft's Windows Server Virtualization Validation Program (SVVP) applies to RHCOS workers

//CNV-9723: VMs are now migrated when a node is drained

//CNV-9521: VMs that use UEFI can now be booted by using secure boot


[id="virt-2-6-installation-new"]
=== Installation

//CNV-7294: The cluster admin can now influence the node placement of OpenShift Virtualization pods

//CNV-9340: The cluster admin can now influence the node placement of the HPP pods


[id="virt-2-6-networking-new"]
=== Networking

//CNV-9518: OpenShift Virtualization is now supported by Calico CNI


[id="virt-2-6-storage-new"]
=== Storage

//CNV-9454: On filesystem-based PVCs, CDI now accounts for FS overhead

//CNV-9455: Improvements to KubeVirt and CDI ensure that storage is allocated on the correct node

//CNV-9456: When importing VM disk images, you can now control which multus network is used


[id="virt-2-6-web-new"]
=== Web console

//CNV-9312: The VM wizard has been completely redesigned, simplifying VM creation

//CNV-9305: Templates in UI now contain default flavor and workload profile


[id="virt-2-6-changes"]
== Notable technical changes

//CNV-9339: When deploying from the GUI, the CR for the HCO is now created during subscription

//CNV-9457: CDI configuration parameters have moved from the CDIConfig object to the CDI CR


[id="virt-2-6-known-issues"]
== Known issues

//New known issues for 2.6 (WIP)

//CNV-9288: Blank block disks and VMware imported disk images are not preallocated even if requested

//The issues below are from the 2.5 release. See items above for new known issues for 2.6.
//The below issues have not yet been pruned/updated for 2.6.

* When a node fails in user-provisioned installations of {product-title} on bare metal deployments, the virtual machine does not automatically restart on another node. Automatic restart is supported only for installer-provisioned installations that have machine health checks enabled. Learn more about xref:/../virt/install/preparing-cluster-for-virt#preparing-cluster-for-virt[configuring your cluster for {VirtProductName}].

* If your {product-title} cluster uses OVN-Kubernetes as the default Container Network Interface (CNI) provider, you cannot attach a Linux bridge or bonding to the default interface of a host because of a change in the host network topology of OVN-Kubernetes. As a workaround, you can use a secondary network interface connected to your host or switch to the OpenShift SDN default CNI provider. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1887456[*BZ#1887456*])

* If you xref:../virt/virtual_machines/importing_vms/virt-importing-vmware-vm.adoc#virt-creating-vddk-image_virt-importing-vmware-vm[add a VMware Virtual Disk Development Kit (VDDK) image] to the `openshift-cnv/v2v-vmware` config map by using the web console, a *Managed resource* error message displays. You can safely ignore this error. Save the config map by clicking *Save*. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1884538[*BZ#1884538*])

* When nodes are evicted, for example, when they are placed in maintenance mode during an {product-title} cluster upgrade, virtual machines are migrated twice instead of just once. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1888790[*BZ#1888790*])

* If you enable a MAC address pool for a namespace by applying the KubeMacPool label and using the `io` attribute for virtual machines in that namespace, the `io` attribute configuration is not retained for the VMs. As a workaround, do not use the `io` attribute for VMs. Alternatively, you can disable KubeMacPool for the namespace. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1869527[*BZ#1869527*])

* If you upgrade to {VirtProductName} {VirtVersion}, both older and newer versions of common templates are available for each combination of operating system, workload, and flavor. When you create a virtual machine by using a common template, you must use the newer version of the template. Disregard the older version to avoid issues. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1859235[*BZ#1859235*])

* Running virtual machines that cannot be live migrated might block an {product-title} cluster upgrade. This includes virtual machines that use hostpath-provisioner storage or SR-IOV network interfaces. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1858777[*BZ#1858777*])
+
As a workaround, you can reconfigure the virtual machines so that they can be powered off during a cluster upgrade. In the `spec` section of the virtual machine configuration file:
+
. Remove the `evictionStrategy: LiveMigrate` field. See xref:../virt/live_migration/virt-configuring-vmi-eviction-strategy.adoc#virt-configuring-vmi-eviction-strategy[Configuring virtual machine eviction strategy] for more information on how to configure eviction strategy.
. Set the `runStrategy` field to `Always`.

* For unknown reasons, memory consumption for the `containerDisk` volume type might gradually increase until it exceeds the memory limit. To resolve this issue, restart the VM. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1855067[*BZ#1855067*])

* Sometimes, when attempting to edit the subscription channel of the *{VirtProductName} Operator* in the web console, clicking the *Channel* button of the *Subscription Overview* results in a JavaScript error. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1796410[*BZ#1796410*])

** As a workaround, trigger the upgrade process to {VirtProductName} {VirtVersion} from the CLI by running the following `oc` patch command:
+
[source,terminal]
----
$ export TARGET_NAMESPACE=openshift-cnv CNV_CHANNEL=2.5 && oc patch -n "${TARGET_NAMESPACE}" $(oc get subscription -n ${TARGET_NAMESPACE} --no-headers -o name) --type='json' -p='[{"op": "replace", "path": "/spec/channel", "value":"'${CNV_CHANNEL}'"}, {"op": "replace", "path": "/spec/installPlanApproval", "value":"Automatic"}]'
----
+
This command points your subscription to upgrade channel `2.5` and enables automatic updates.

* Live migration fails when nodes have different CPU models. Even in cases where nodes have the same physical CPU model, differences introduced by microcode updates have the same effect. This is because the default settings trigger host CPU passthrough behavior, which is incompatible with live migration. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1760028[*BZ#1760028*])

** As a workaround, set the default CPU model in the `kubevirt-config` ConfigMap, as shown in the following example:
+
[NOTE]
====
You must make this change before starting the virtual machines that support live migration.
====
+
. Open the `kubevirt-config` ConfigMap for editing by running the following command:
+
[source,terminal]
----
$ oc edit configmap kubevirt-config -n openshift-cnv
----
+
. Edit the ConfigMap:
+
[source,yaml]
----
kind: ConfigMap
metadata:
  name: kubevirt-config
data:
  default-cpu-model: "<cpu-model>" <1>
----
<1> Replace `<cpu-model>` with the actual CPU model value. You can determine this value by running `oc describe node <node>` for all nodes and looking at the `cpu-model-<name>` labels. Select the CPU model that is present on all of your nodes.

* {VirtProductName} cannot reliably identify node drains that are triggered by running either `oc adm drain` or `kubectl drain`. Do not run these commands on the nodes of any clusters where {VirtProductName} is deployed. The nodes might not drain if there are virtual machines running on top of them.

** The current solution is to xref:../virt/node_maintenance/virt-setting-node-maintenance.adoc#virt-setting-node-maintenance[put nodes into maintenance].

* If the {VirtProductName} storage PV is not suitable for importing a RHV VM, the progress bar remains at 10% and the import does not complete. The VM Import Controller Pod log displays the following error message: `Failed to bind volumes: provisioning failed for PVC`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1857784[*BZ#1857784*])

* If you enter the wrong credentials for the RHV Manager while importing a RHV VM, the Manager might lock the admin user account because the `vm-import-operator` tries repeatedly to connect to the RHV API. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1887140[*BZ#1887140*])
+
To unlock the account, log in to the Manager and enter the following command:
+
[source,terminal]
----
$ ovirt-aaa-jdbc-tool user unlock admin
----
