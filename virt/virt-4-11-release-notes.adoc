:_content-type: ASSEMBLY
[id="virt-4-11-release-notes"]
= {VirtProductName} release notes
include::_attributes/common-attributes.adoc[]
:context: virt-4-11-release-notes

toc::[]

== About Red Hat {VirtProductName}

Red Hat {VirtProductName} enables you to bring traditional virtual machines (VMs) into {product-title} where they run alongside containers, and are managed as native Kubernetes objects.

{VirtProductName} is represented by the image:Operator_Icon-OpenShift_Virtualization-5.png[{VirtProductName},40,40] logo.

You can use {VirtProductName} with either the xref:../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[OVN-Kubernetes] or the xref:../networking/openshift_sdn/about-openshift-sdn.adoc#about-openshift-sdn[OpenShiftSDN] default Container Network Interface (CNI) network provider.

Learn more about xref:../virt/about-virt.adoc#about-virt[what you can do with {VirtProductName}].

xref:../virt/install/preparing-cluster-for-virt.adoc#preparing-cluster-for-virt[Prepare your cluster] for {VirtProductName}.

include::modules/virt-supported-cluster-version.adoc[leveloffset=+2]

[id="virt-guest-os"]
=== Supported guest operating systems
//CNV-16390 Supported guest operating systems
To view the supported guest operating systems for {VirtProductName}, refer to link:https://access.redhat.com/articles/973163#ocpvirt[Certified Guest Operating Systems in Red Hat OpenStack Platform, Red Hat Virtualization and OpenShift Virtualization].

[id="virt-4-11-inclusive-language"]
== Making open source more inclusive

Red Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases. For more details, see link:https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language[our CTO Chris Wright's message].

[id="virt-4-11-new"]
== New and changed features

//CNV-15207 SVVP for 4.11: Ensure platform passes Windows Server Virtualization Validation Program - with RHCOS workers
//NOTE: This is a recurring release note. Modify the existing note text below if recommended by QE.
* OpenShift Virtualization is certified in Microsoftâ€™s Windows Server Virtualization Validation Program (SVVP) to run Windows Server workloads.
+
The SVVP Certification applies to:
+
** Red Hat Enterprise Linux CoreOS workers. In the Microsoft SVVP Catalog, they are named __Red Hat OpenShift Container Platform 4 on RHEL CoreOS 8__.
** Intel and AMD CPUs.

//CNV-16067 CNV revalidated on a 3-node compact cluster
* You can now deploy {VirtProductName} on a xref:../installing/installing_bare_metal/installing-bare-metal.adoc#installation-three-node-cluster_installing-bare-metal[three-node cluster] with zero compute nodes.

//CNV-15886 VMs can run in session mode and as non-root
* Virtual machines run as unprivileged workloads in _session mode_ by default. This feature improves cluster security by mitigating escalation-of-privilege attacks.

//CNV-15218 RHEL 9 is a supported OS
* {op-system-base-full} 9 is now supported as a guest operating system.

//CNV-18162 Move MTV action from VM list page
* The link for installing the Migration Toolkit for Virtualization (MTV) Operator in the {product-title} web console has been moved. It is now located in the *Related operators* section of the *Getting started resources* card on the *Virtualization* -> *Overview* page.

//CNV-19224 Hypervisor logs
* You can configure the verbosity level of the `virtLauncher`, `virtHandler`, `virtController`, `virtAPI`, and `virtOperator` pod logs to debug specific components by editing the `HyperConverged` custom resource (CR).

[id="virt-4-11-quick-starts"]
=== Quick starts

* Quick start tours are available for several {VirtProductName} features. To view the tours, click the *Help* icon *?* in the menu bar on the header of the {VirtProductName} console and then select *Quick Starts*. You can filter the available tours by entering the `virtualization` keyword in the *Filter* field.

[id="virt-4-11-installation-new"]
=== Installation

//CNV-17268 Component relationship diagrams

//[id="virt-4-11-networking-new"]
//=== Networking



[id="virt-4-11-storage-new"]
=== Storage

//CNV-16785 Snapshot metrics

[id="virt-4-11-web-new"]
=== Web console

//CNV-18319 UI can switch between BIOS and UEFI
* You can set the xref:../virt/virtual_machines/virt-edit-vms.adoc#virt-editing-vm-web_virt-edit-vms[boot mode] of templates and virtual machines to *BIOS*, *UEFI*, or *UEFI (secure)* by using the web console.

//CNV-17269 Enable and disabe VM descheduler from UI

//CNV-17805 Single VM Overview page
* You can access virtual machines by navigating to *Virtualization* -> *VirtualMachines* in the side menu. Each virtual machine now has an xref:../virt/logging_events_monitoring/virt-viewing-information-about-vm-workloads.adoc#virt-about-the-vm-dashboard_virt-viewing-information-about-vm-workloads[updated *Overview* tab] that provides information about the virtual machine configuration, alerts, snapshots, network interfaces, disks, usage data, and hardware devices.

* If your Windows virtual machine has a vGPU attached, you can now xref:../virt/virtual_machines/virt-accessing-vm-consoles.adoc#virt-switching-displays_virt-accessing-vm-consoles[switch between the default display and the vGPU display] by using the web console.

//CNV-17806 Updated design of the templates list page
* You can access virtual machine templates by navigating to *Virtualization* -> *Templates* in the side menu. The updated *Virtual Machine Templates* page now provides useful information about each template, including workload profile, boot source, and CPU and memory configuration.

// NOTE: Comment out deprecated and removed features (and their IDs) if not used in a release
[id="virt-4-11-deprecated-removed"]
== Deprecated and removed features

[id="virt-4-11-deprecated"]
=== Deprecated features
// NOTE: when uncommenting deprecated features list, change the header level below to ===

Deprecated features are included in the current release and supported. However, they will be removed in a future release and are not recommended for new deployments.

* In a future release, support for the legacy HPP custom resource, and the associated storage class, will be deprecated. Beginning in {VirtProductName} {VirtVersion}, the HPP Operator uses the Kubernetes Container Storage Interface (CSI) driver to configure local storage. The Operator continues to support the existing (legacy) format of the HPP custom resource and the associated storage class. If you use the HPP Operator, plan to xref:../virt/virtual_machines/virtual_disks/virt-configuring-local-storage-for-vms.adoc#virt-configuring-local-storage-for-vms[create a storage class for the CSI driver] as part of your migration strategy.

[id="virt-4-11-removed"]
=== Removed features

Removed features are not supported in the current release.

//CNV-16317 NMState is no longer part of CNV
* {VirtProductName} {VirtVersion} removes support for link:https://nmstate.io/[nmstate], including the following objects:
+
--
** `NodeNetworkState`
** `NodeNetworkConfigurationPolicy`
** `NodeNetworkConfigurationEnactment`
--
+
To preserve and support your existing nmstate configuration, install the xref:../networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.adoc#k8s-nmstate-about-the-k8s-nmstate-operator[Kubernetes NMState Operator] before updating to {VirtProductName} {VirtVersion}. You can install it from the *OperatorHub* in the {product-title} web console, or by using the OpenShift CLI (`oc`).

[id="virt-4-11-changes"]
== Notable technical changes

//CNV-17807 New Create VM wizard workflow

//CNV-18323 VM templates can be made public i.e. available to all users

* As of version 4.11, `node-maintenance-operator` (NMO) is no longer shipped with {VirtProductName}. It is now available to deploy as a standalone operator through the Operator Lifecycle Manager (OLM) in the OperatorHub.
+
If you are using {VirtProductName} 4.10.2 or later, you can upgrade to {product-title}  4.11, but will be blocked from upgrading {VirtProductName} if the non-standalone NMO is still in use. To upgrade to {VirtProductName} 4.11, you must move all nodes out of maintenance mode, or the cluster admin must install the standalone NMO and replace the `nodemaintenances.nodemaintenance.kubevirt.io` CR with a `nodemaintenances.nodemaintenance.medik8s.io` CR.

[id="virt-4-11-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

* You can now use the Red Hat Enterprise Linux 9 Alpha template to create virtual machines.

* You can now link:https://access.redhat.com/articles/6409731[deploy {VirtProductName} on AWS bare metal nodes].

* {VirtProductName} has xref:../virt/logging_events_monitoring/virt-virtualization-alerts.adoc#virt-virtualization-alerts[critical alerts] that inform you when a problem occurs that requires immediate attention. Now, each alert has a corresponding description of the problem, a reason for why the alert is occurring, a troubleshooting process to diagnose the source of the problem, and steps for resolving the alert.

* Administrators can now declaratively xref:../virt/virtual_machines/advanced_vm_management/virt-configuring-mediated-devices.adoc#virt-configuring-mediated-devices[create and expose mediated devices] such as virtual graphics processing units (vGPUs) by editing the `HyperConverged` CR. Virtual machine owners can then assign these devices to VMs.

//CNV-13660 Inherit static IP from a NIC attached to the bridge
* You can xref:../networking/k8s_nmstate/k8s-nmstate-updating-node-network-config.adoc#capturing-nic-static-ip_k8s-nmstate-updating-node-network-config[transfer the static IP configuration of the NIC attached to the bridge] by applying a single `NodeNetworkConfigurationPolicy` manifest to the cluster.

//CNV-16639
* You can now install {VirtProductName} on IBM Cloud bare-metal servers. Bare-metal servers offered by other cloud providers are not supported.

//CNV-19603
* You can check your {VirtProductName} cluster for compliance issues by installing the xref:../security/compliance_operator/compliance-operator-understanding.adoc#understanding-compliance[Compliance Operator] and running a scan with the xref:../security/compliance_operator/compliance-operator-supported-profiles.adoc#compliance-operator-supported-profiles[`ocp4-moderate` and `ocp4-moderate-node` profiles].

//CNV-17453 Network latency check
* {VirtProductName} now includes a xref:../virt/logging_events_monitoring/virt-running-cluster-checkups.adoc#virt-running-cluster-checkups[diagnostic framework] to run predefined checkups that can be used for cluster maintenance and troubleshooting. You can run a predefined checkup to xref:../virt/logging_events_monitoring/virt-running-cluster-checkups.adoc#virt-measuring-latency-vm-secondary-network_virt-running-cluster-checkups[check network connectivity and latency] for virtual machines on a secondary network.

//CNV-19145 Support live migration policies
* You can create live migration policies with specific parameters, such as bandwidth usage, maximum number of parallel migrations, and timeout, and apply the policies to groups of virtual machines by using virtual machine and namespace labels.

[id="virt-4-11-bug-fixes"]
== Bug fixes

* Previously, on a large cluster, the {VirtProductName} MAC pool manager would take too much time to boot and {VirtProductName} might not become ready. With this update, the pool initialization and startup latency is reduced. As a result, VMs can now be successfully defined. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2035344[*BZ#2035344*])

* If a Windows VM crashes or hangs during shutdown, you can now manually issue a force shutdown request to stop the VM. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2040766[*BZ#2040766*])

* The YAML examples in the VM wizard have now been updated to contain the latest upstream changes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2055492[*BZ#2055492*])

* The *Add Network Interface* button on the VM *Network Interfaces* tab is no longer disabled for non-privileged users. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2056420[*BZ#2056420*])

* A non-privileged user can now successfully add disks to a VM without getting a RBAC rule error. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2056421[*BZ#2056421*])

* The web console now successfully displays virtual machine templates that are deployed to a custom namespace. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2054650[*BZ#2054650*])

* Previously, updating a Single Node OpenShift (SNO) cluster failed if the `spec.evictionStrategy` field was set to `LiveMigrate` for a VMI. For live migration to succeed, the cluster must have more than one compute node. With this update, the `spec.evictionStrategy` field is removed from the virtual machine template in a SNO environment. As a result, cluster update is now successful. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2073880[*BZ#2073880*])


[id="virt-4-11-known-issues"]
== Known issues

//New known issues for 4.11 go above this comment

* If a single node contains more than 50 images, pod scheduling might be imbalanced across nodes. This is because the list of images on a node is shortened to 50 by default. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1984442[*BZ#1984442*])
** As a workaround, you can disable the image limit by xref:../nodes/nodes/nodes-nodes-managing.adoc#nodes-nodes-managing[editing the `KubeletConfig` object] and setting the value of `nodeStatusMaxImages` to `-1`.

* If you deploy the xref:../virt/virtual_machines/virtual_disks/virt-configuring-local-storage-for-vms.adoc#virt-configuring-local-storage-for-vms[hostpath provisioner] on a cluster where any node has a fully qualified domain name (FQDN) that exceeds 42 characters, the provisioner fails to bind PVCs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2057157[*BZ#2057157*])
+
--
.Example error message
[source,terminal]
----
E0222 17:52:54.088950       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: unable to parse requirement: values[0][csi.storage.k8s.io/managed-by]: Invalid value: "external-provisioner-<node_FQDN>": must be no more than 63 characters <1>
----
<1> Though the error message refers to a maximum of 63 characters, this includes the `external-provisioner-` string that is prefixed to the node's FQDN.
--
** As a workaround, disable the `storageCapacity` option in the hostpath provisioner CSI driver by running the following command:
+
[source,terminal]
----
$ oc patch csidriver kubevirt.io.hostpath-provisioner --type merge --patch '{"spec": {"storageCapacity": false}}'
----

* If your {product-title} cluster uses OVN-Kubernetes as the default Container Network Interface (CNI) provider, you cannot attach a Linux bridge or bonding device to a host's default interface because of a change in the host network topology of OVN-Kubernetes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1885605[*BZ#1885605*])
** As a workaround, you can use a secondary network interface connected to your host, or switch to the OpenShift SDN default CNI provider.

* If you use Red Hat Ceph Storage or Red Hat OpenShift Data Foundation Storage, cloning more than 100 VMs at once might fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1989527[*BZ#1989527*])
** As a workaround, you can perform a host-assisted copy by setting `spec.cloneStrategy: copy` in the storage profile manifest. For example:
+
[source,yaml]
----
apiVersion: cdi.kubevirt.io/v1beta1
kind: StorageProfile
metadata:
  name: <provisioner_class>
#   ...
spec:
  claimPropertySets:
  - accessModes:
    - ReadWriteOnce
    volumeMode: Filesystem
  cloneStrategy: copy <1>
status:
  provisioner: <provisioner>
  storageClass: <provisioner_class>
----
<1> The default cloning method set to `copy`.

* In some instances, multiple virtual machines can mount the same PVC in read-write mode, which might result in data corruption. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1992753[*BZ#1992753*])
** As a workaround, avoid using a single PVC in read-write mode with multiple VMs.

* The Pod Disruption Budget (PDB) prevents pod disruptions for migratable virtual machine images. If the PDB detects pod disruption, then `openshift-monitoring` sends a `PodDisruptionBudgetAtLimit` alert every 60 minutes for virtual machine images that use the `LiveMigrate` eviction strategy. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2026733[*BZ#2026733*])
** As a workaround, xref:../monitoring/managing-alerts.adoc#silencing-alerts_managing-alerts[silence alerts].

* {VirtProductName} links a service account token in use by a pod to that specific pod. {VirtProductName} implements a service account volume by creating a disk image that contains a token. If you migrate a VM, then the service account volume becomes invalid. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2037611[*BZ#2037611*])
** As a workaround, use user accounts rather than service accounts because user account tokens are not bound to a specific pod.

* If you configure the `HyperConverged` custom resource (CR) to enable mediated devices before drivers are installed, the new device configuration does not take effect. This issue can be triggered by updates. For example, if `virt-handler` is updated before `daemonset`, which installs NVIDIA drivers, then nodes cannot provide virtual machine GPUs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2046298[*BZ#2046298*])
** As a workaround:
. Remove `mediatedDevicesConfiguration` and `permittedHostDevices` from the `HyperConverged` CR.
. Update both `mediatedDevicesConfiguration` and `permittedHostDevices` stanzas with the configuration you want to use.

* If you clone more than 100 VMs using the `csi-clone` cloning strategy, then the Ceph CSI might not purge the clones. Manually deleting the clones can also fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2055595[*BZ#2055595*])
** As a workaround, you can restart the `ceph-mgr` to purge the VM clones.
