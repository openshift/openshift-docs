:_content-type: ASSEMBLY
[id="virt-4-11-release-notes"]
= {VirtProductName} release notes
include::_attributes/common-attributes.adoc[]
:context: virt-4-11-release-notes

toc::[]

include::modules/making-open-source-more-inclusive.adoc[leveloffset=+1]


== About Red Hat {VirtProductName}

Red Hat {VirtProductName} enables you to bring traditional virtual machines (VMs) into {product-title} where they run alongside containers, and are managed as native Kubernetes objects.

{VirtProductName} is represented by the image:virt-icon.svg[{VirtProductName},40,40] logo.

You can use {VirtProductName} with either the xref:../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[OVN-Kubernetes] or the xref:../networking/openshift_sdn/about-openshift-sdn.adoc#about-openshift-sdn[OpenShiftSDN] default Container Network Interface (CNI) network provider.

Learn more about xref:../virt/about-virt.adoc#about-virt[what you can do with {VirtProductName}].

Learn more about xref:../virt/virt-architecture.adoc#virt-how-virt-works_virt-architecture[{VirtProductName} architecture and deployments].

xref:../virt/install/preparing-cluster-for-virt.adoc#preparing-cluster-for-virt[Prepare your cluster] for {VirtProductName}.

include::modules/virt-supported-cluster-version.adoc[leveloffset=+2]

[id="virt-guest-os"]
=== Supported guest operating systems
//CNV-16390 Supported guest operating systems
To view the supported guest operating systems for {VirtProductName}, refer to link:https://access.redhat.com/articles/973163#ocpvirt[Certified Guest Operating Systems in Red Hat OpenStack Platform, Red Hat Virtualization and OpenShift Virtualization].


[id="virt-4-11-new"]
== New and changed features

//CNV-15207 SVVP for 4.11: Ensure platform passes Windows Server Virtualization Validation Program - with RHCOS workers
//Commenting out the RN because it was decided not to include this note for 4.11.
////
* OpenShift Virtualization is certified in Microsoftâ€™s Windows Server Virtualization Validation Program (SVVP) to run Windows Server workloads.
+
The SVVP Certification applies to:
+
** Red Hat Enterprise Linux CoreOS workers. In the Microsoft SVVP Catalog, they are named __Red Hat OpenShift Container Platform 4 on RHEL CoreOS 8__.
** Intel and AMD CPUs.
////

//CNV-16067 CNV revalidated on a 3-node compact cluster
* You can now deploy {VirtProductName} on a xref:../installing/installing_bare_metal/installing-bare-metal.adoc#installation-three-node-cluster_installing-bare-metal[three-node cluster] with zero compute nodes.

//CNV-15886 VMs can run in session mode and as non-root
* Virtual machines run as unprivileged workloads in _session mode_ by default. This feature improves cluster security by mitigating escalation-of-privilege attacks.

//CNV-15218 RHEL 9 is a supported OS
* {op-system-base-full} 9 is now supported as a guest operating system.

//CNV-18162 Move MTV action from VM list page
* The link for installing the Migration Toolkit for Virtualization (MTV) Operator in the {product-title} web console has been moved. It is now located in the *Related operators* section of the *Getting started resources* card on the *Virtualization* -> *Overview* page.

//CNV-19224 Hypervisor logs
* You can configure the xref:../virt/logging_events_monitoring/virt-logs.adoc#virt-logs[verbosity level] of the `virtLauncher`, `virtHandler`, `virtController`, `virtAPI`, and `virtOperator` pod logs to debug specific components by editing the `HyperConverged` custom resource (CR).

[id="virt-4-11-quick-starts"]
=== Quick starts

* Quick start tours are available for several {VirtProductName} features. To view the tours, click the *Help* icon *?* in the menu bar on the header of the {VirtProductName} console and then select *Quick Starts*. You can filter the available tours by entering the `virtualization` keyword in the *Filter* field.

//[id="virt-4-11-installation-new"]
//=== Installation


//[id="virt-4-11-networking-new"]
//=== Networking

[id="virt-4-11-storage-new"]
=== Storage

//CNV-16785 Snapshot metrics
* New metrics are available that provide information about xref:../virt/logging_events_monitoring/virt-prometheus-queries.adoc#virt-storage-snapshot-data_virt-prometheus-queries[virtual machine snapshots].

//CNV-19602 Automatic boot source configuration
* You can reduce the number of logs on disconnected environments or reduce resource usage by xref:../virt/virtual_machines/advanced_vm_management/virt-automatic-bootsource-updates.adoc#virt-disable-individual-bootsource-update_virt-automatic-bootsource-updates[disabling the automatic imports and updates for a boot source].

[id="virt-4-11-web-new"]
=== Web console

//CNV-18319 UI can switch between BIOS and UEFI
* You can set the xref:../virt/virtual_machines/virt-edit-vms.adoc#virt-editing-vm-web_virt-edit-vms[boot mode] of templates and virtual machines to *BIOS*, *UEFI*, or *UEFI (secure)* by using the web console.

* You can now enable and disable the descheduler from the web console on the *Scheduling* tab of the xref:../virt/virtual_machines/virt-create-vms.adoc#virt-vm-fields-web_virt-create-vms[*VirtualMachine details*] page.

//CNV-17805 Single VM Overview page
* You can access virtual machines by navigating to *Virtualization* -> *VirtualMachines* in the side menu. Each virtual machine now has an xref:../virt/logging_events_monitoring/virt-viewing-information-about-vm-workloads.adoc#virt-about-the-vm-dashboard_virt-viewing-information-about-vm-workloads[updated *Overview* tab] that provides information about the virtual machine configuration, alerts, snapshots, network interfaces, disks, usage data, and hardware devices.

* The *Create a Virtual Machine* wizard in the web console is now xref:../virt/virtual_machines/virt-create-vms.adoc#virt-quick-creating-vm_virt-create-vms[replaced by the *Catalog* page], which lists available templates that you can use to create a virtual machine. You can use a template with an available boot source to quickly create a virtual machine or you can customize a template to create a virtual machine.

* If your Windows virtual machine has a vGPU attached, you can now xref:../virt/virtual_machines/virt-accessing-vm-consoles.adoc#virt-switching-displays_virt-accessing-vm-consoles[switch between the default display and the vGPU display] by using the web console.

//CNV-17806 Updated design of the templates list page
* You can access virtual machine templates by navigating to *Virtualization* -> *Templates* in the side menu. The updated *VirtualMachine Templates* page now provides useful information about each template, including workload profile, boot source, and CPU and memory configuration.

// CNV-20186 Create template wizard removed.
* The *Create Template* wizard has been removed from the *VirtualMachine Templates* page. You xref:../virt/vm_templates/virt-creating-vm-template.adoc#virt-creating-template_virt-creating-vm-template[create a virtual machine template] by editing a YAML file example.

// NOTE: Comment out deprecated and removed features (and their IDs) if not used in a release
[id="virt-4-11-deprecated-removed"]
== Deprecated and removed features

[id="virt-4-11-deprecated"]
=== Deprecated features
// NOTE: when uncommenting deprecated features list, change the header level below to ===

Deprecated features are included in the current release and supported. However, they will be removed in a future release and are not recommended for new deployments.

* In a future release, support for the legacy HPP custom resource, and the associated storage class, will be deprecated. Beginning in {VirtProductName} {VirtVersion}, the HPP Operator uses the Kubernetes Container Storage Interface (CSI) driver to configure local storage. The Operator continues to support the existing (legacy) format of the HPP custom resource and the associated storage class. If you use the HPP Operator, plan to xref:../virt/virtual_machines/virtual_disks/virt-configuring-local-storage-for-vms.adoc#virt-configuring-local-storage-for-vms[create a storage class for the CSI driver] as part of your migration strategy.

[id="virt-4-11-removed"]
=== Removed features

Removed features are not supported in the current release.

//CNV-16317 NMState is no longer part of CNV
* {VirtProductName} {VirtVersion} removes support for link:https://nmstate.io/[nmstate], including the following objects:
+
--
** `NodeNetworkState`
** `NodeNetworkConfigurationPolicy`
** `NodeNetworkConfigurationEnactment`
--
+
To preserve and support your existing nmstate configuration, install the xref:../networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.adoc#k8s-nmstate-about-the-k8s-nmstate-operator[Kubernetes NMState Operator] before updating to {VirtProductName} {VirtVersion}. You can install it from the *OperatorHub* in the {product-title} web console, or by using the OpenShift CLI (`oc`).

* The Node Maintenance Operator (NMO) is no longer shipped with {VirtProductName}. You can xref:../nodes/nodes/eco-node-maintenance-operator.adoc#node-maintenance-operator[install the NMO] from the *OperatorHub* in the {product-title} web console, or by using the OpenShift CLI (`oc`).
+
You must perform one of the following tasks before updating to {VirtProductName} {VirtVersion} from {VirtProductName} 4.10.2 and later releases:

** Move all nodes out of maintenance mode.
** Install the standalone NMO and replace the `nodemaintenances.nodemaintenance.kubevirt.io` custom resource (CR) with a `nodemaintenances.nodemaintenance.medik8s.io` CR.

//[id="virt-4-11-changes"]
//== Notable technical changes

//CNV-18323 VM templates can be made public i.e. available to all users
// Feature never implemented, so this is obsolete

[id="virt-4-11-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

//CNV-20963
* You can now use Microsoft Windows 11 as a guest operating system. However, {VirtProductName} {VirtVersion} does not support USB disks, which are required for a critical function of BitLocker recovery. To protect recovery keys, use other methods described in the link:https://learn.microsoft.com/en-us/windows/security/information-protection/bitlocker/bitlocker-recovery-guide-plan[BitLocker recovery guide].

* You can now link:https://access.redhat.com/articles/6409731[deploy {VirtProductName} on AWS bare metal nodes].

* {VirtProductName} has xref:../virt/logging_events_monitoring/virt-virtualization-alerts.adoc#virt-virtualization-alerts[critical alerts] that inform you when a problem occurs that requires immediate attention. Now, each alert has a corresponding description of the problem, a reason for why the alert is occurring, a troubleshooting process to diagnose the source of the problem, and steps for resolving the alert.

* Administrators can now declaratively xref:../virt/virtual_machines/advanced_vm_management/virt-configuring-mediated-devices.adoc#virt-configuring-mediated-devices[create and expose mediated devices] such as virtual graphics processing units (vGPUs) by editing the `HyperConverged` CR. Virtual machine owners can then assign these devices to VMs.

//CNV-13660 Inherit static IP from a NIC attached to the bridge
* You can xref:../networking/k8s_nmstate/k8s-nmstate-updating-node-network-config.adoc#capturing-nic-static-ip_k8s-nmstate-updating-node-network-config[transfer the static IP configuration of the NIC attached to the bridge] by applying a single `NodeNetworkConfigurationPolicy` manifest to the cluster.

//CNV-16639
* You can now install {VirtProductName} on IBM Cloud bare-metal servers. Bare-metal servers offered by other cloud providers are not supported.

//CNV-19603
* You can check your {VirtProductName} cluster for compliance issues by installing the xref:../security/compliance_operator/compliance-operator-understanding.adoc#understanding-compliance[Compliance Operator] and running a scan with the xref:../security/compliance_operator/compliance-operator-supported-profiles.adoc#compliance-operator-supported-profiles[`ocp4-moderate` and `ocp4-moderate-node` profiles].

//CNV-17453 Network latency check
* {VirtProductName} now includes a xref:../virt/logging_events_monitoring/virt-running-cluster-checkups.adoc#virt-running-cluster-checkups[diagnostic framework] to run predefined checkups that can be used for cluster maintenance and troubleshooting. You can run a predefined checkup to xref:../virt/logging_events_monitoring/virt-running-cluster-checkups.adoc#virt-measuring-latency-vm-secondary-network_virt-running-cluster-checkups[check network connectivity and latency] for virtual machines on a secondary network.

//CNV-19145 Support live migration policies
* You can create live migration policies with specific parameters, such as bandwidth usage, maximum number of parallel migrations, and timeout, and apply the policies to groups of virtual machines by using virtual machine and namespace labels.

[id="virt-4-11-bug-fixes"]
== Bug fixes

* Previously, on a large cluster, the {VirtProductName} MAC pool manager would take too much time to boot and {VirtProductName} might not become ready. With this update, the pool initialization and startup latency is reduced. As a result, VMs can now be successfully defined. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2035344[*BZ#2035344*])

* If a Windows VM crashes or hangs during shutdown, you can now manually issue a force shutdown request to stop the VM. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2040766[*BZ#2040766*])

* The YAML examples in the VM wizard have now been updated to contain the latest upstream changes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2055492[*BZ#2055492*])

* The *Add Network Interface* button on the VM *Network Interfaces* tab is no longer disabled for non-privileged users. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2056420[*BZ#2056420*])

* A non-privileged user can now successfully add disks to a VM without getting a RBAC rule error. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2056421[*BZ#2056421*])

* The web console now successfully displays virtual machine templates that are deployed to a custom namespace. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2054650[*BZ#2054650*])

* Previously, updating a Single Node OpenShift (SNO) cluster failed if the `spec.evictionStrategy` field was set to `LiveMigrate` for a VMI. For live migration to succeed, the cluster must have more than one compute node. With this update, the `spec.evictionStrategy` field is removed from the virtual machine template in a SNO environment. As a result, cluster update is now successful. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2073880[*BZ#2073880*])


[id="virt-4-11-known-issues"]
== Known issues

* Uninstalling {VirtProductName} does not remove the node labels created by {VirtProductName}. You must remove the labels manually. (link:https://issues.redhat.com/browse/CNV-22036[*CNV-22036*])

* The OVN-Kubernetes cluster network provider crashes from peak RAM and CPU usage if you create a large number of `NodePort` services. This can happen if you use `NodePort` services to expose SSH access to a large number of virtual machines (VMs). (link:https://issues.redhat.com/browse/OCPBUGS-1940[*OCPBUGS-1940*])
** As a workaround, use the OpenShift SDN cluster network provider if you want to expose SSH access to a large number of VMs via `NodePort` services.

* Updating to {VirtProductName} {VirtVersion} from version 4.10 is blocked until you install the standalone Kubernetes NMState Operator. This occurs even if your cluster configuration does not use any link:https://nmstate.io/[nmstate] resources. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2126537[*BZ#2126537*])
** As a workaround:
. Verify that there are no node network configuration policies defined on the cluster:
+
[source,terminal]
----
$ oc get nncp
----
. Choose the appropriate method to update {VirtProductName}:
.. If the list of node network configuration policies is not empty, exit this procedure and install the xref:../networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.adoc#k8s-nmstate-about-the-k8s-nmstate-operator[Kubernetes NMState Operator] to preserve and support your existing nmstate configuration.
.. If the list is empty, go to step 3.

. Annotate the `HyperConverged` custom resource (CR). The following command overwrites any existing JSON patches:
+
[source,terminal]
----
$ oc annotate --overwrite -n openshift-cnv hco kubevirt-hyperconverged 'networkaddonsconfigs.kubevirt.io/jsonpatch=[{"op": "replace","path": "/spec/nmstate", "value": null}]'
----
+
[NOTE]
====
The `HyperConverged` object reports a `TaintedConfiguration` condition while this patch is applied. This is benign.
====
. Update {VirtProductName}.
. After the update completes, remove the annotation by running the following command:
+
[source,terminal]
----
$ oc annotate -n openshift-cnv hco kubevirt-hyperconverged networkaddonsconfigs.kubevirt.io/jsonpatch-
----
. Optional: Add back any previously configured JSON patches that were overwritten.

* Some persistent volume claim (PVC) annotations created by the Containerized Data Importer (CDI) can cause the virtual machine snapshot restore operation to hang indefinitely. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2070366[*BZ#2070366*])
** As a workaround, you can remove the annotations manually:
. Obtain the xref:../virt/virtual_machines/virtual_disks/virt-managing-vm-snapshots.adoc#virtual-machine-snapshot-controller-and-custom-resource-definitions-crds[VirtualMachineSnapshotContent] custom resource (CR) name from the `status.virtualMachineSnapshotContentName` value in the `VirtualMachineSnapshot` CR.
. Edit the `VirtualMachineSnapshotContent` CR and remove all lines that contain `k8s.io/cloneRequest`.
. If you did not specify a value for `spec.dataVolumeTemplates` in the `VirtualMachine` object, delete any `DataVolume` and `PersistentVolumeClaim` objects in this namespace where both of the following conditions are true:
+
.. The object's name begins with `restore-`.
.. The object is not referenced by virtual machines.
+
This step is optional if you specified a value for `spec.dataVolumeTemplates`.
. Repeat the xref:../virt/virtual_machines/virtual_disks/virt-managing-vm-snapshots.adoc#virt-restoring-vm-from-snapshot-cli_virt-managing-vm-snapshots[restore operation] with the updated `VirtualMachineSnapshot` CR.

* Windows 11 virtual machines do not boot on clusters running in link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/security_hardening/index#con_federal-information-processing-standard-fips_assembly_installing-the-system-in-fips-mode[FIPS mode]. Windows 11 requires a TPM (trusted platform module) device by default. However, the `swtpm` (software TPM emulator) package is incompatible with FIPS. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2089301[*BZ#2089301*])

* In a Single Node OpenShift (SNO) cluster, a `VMCannotBeEvicted` alert occurs on virtual machines that are created from common templates that have the eviction strategy set to `LiveMigrate`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2092412[*BZ#2092412*])

* The QEMU guest agent on a Fedora 35 virtual machine is blocked by SELinux and does not report data. Other Fedora versions might be affected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2028762[*BZ#2028762*])
** As a workaround, disable SELinux on the virtual machine, run the QEMU guest agent commands, and then re-enable SELinux.

//New known issues for 4.11 go above this comment

* If your {product-title} cluster uses OVN-Kubernetes as the default Container Network Interface (CNI) provider, you cannot attach a Linux bridge or bonding device to a host's default interface because of a change in the host network topology of OVN-Kubernetes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1885605[*BZ#1885605*])
** As a workaround, you can use a secondary network interface connected to your host, or switch to the OpenShift SDN default CNI provider.

* If you use Red Hat Ceph Storage or Red Hat OpenShift Data Foundation Storage, cloning more than 100 VMs at once might fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1989527[*BZ#1989527*])
** As a workaround, you can perform a host-assisted copy by setting `spec.cloneStrategy: copy` in the storage profile manifest. For example:
+
[source,yaml]
----
apiVersion: cdi.kubevirt.io/v1beta1
kind: StorageProfile
metadata:
  name: <provisioner_class>
#   ...
spec:
  claimPropertySets:
  - accessModes:
    - ReadWriteOnce
    volumeMode: Filesystem
  cloneStrategy: copy <1>
status:
  provisioner: <provisioner>
  storageClass: <provisioner_class>
----
<1> The default cloning method set to `copy`.

* In some instances, multiple virtual machines can mount the same PVC in read-write mode, which might result in data corruption. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1992753[*BZ#1992753*])
** As a workaround, avoid using a single PVC in read-write mode with multiple VMs.

* The Pod Disruption Budget (PDB) prevents pod disruptions for migratable virtual machine images. If the PDB detects pod disruption, then `openshift-monitoring` sends a `PodDisruptionBudgetAtLimit` alert every 60 minutes for virtual machine images that use the `LiveMigrate` eviction strategy. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2026733[*BZ#2026733*])
** As a workaround, xref:../monitoring/managing-alerts.adoc#silencing-alerts_managing-alerts[silence alerts].

* {VirtProductName} links a service account token in use by a pod to that specific pod. {VirtProductName} implements a service account volume by creating a disk image that contains a token. If you migrate a VM, then the service account volume becomes invalid. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2037611[*BZ#2037611*])
** As a workaround, use user accounts rather than service accounts because user account tokens are not bound to a specific pod.

* If you configure the `HyperConverged` custom resource (CR) to enable mediated devices before drivers are installed, the new device configuration does not take effect. This issue can be triggered by updates. For example, if `virt-handler` is updated before `daemonset`, which installs NVIDIA drivers, then nodes cannot provide virtual machine GPUs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2046298[*BZ#2046298*])
** As a workaround:
. Remove `mediatedDevicesConfiguration` and `permittedHostDevices` from the `HyperConverged` CR.
. Update both `mediatedDevicesConfiguration` and `permittedHostDevices` stanzas with the configuration you want to use.

* If you clone more than 100 VMs using the `csi-clone` cloning strategy, then the Ceph CSI might not purge the clones. Manually deleting the clones can also fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2055595[*BZ#2055595*])
** As a workaround, you can restart the `ceph-mgr` to purge the VM clones.
