[id="virt-2-5-release-notes"]
= {RN_BookName}
include::modules/virt-document-attributes.adoc[]
include::modules/common-attributes.adoc[]
:context: virt-release-notes

toc::[]

== About Red Hat {VirtProductName}

Red Hat {VirtProductName} enables you to bring traditional virtual machines (VMs) into {product-title} where they run alongside containers, and are managed as native Kubernetes objects.

//Branding-Logo
{VirtProductName} is represented by the image:Operator_Icon-OpenShift_Virtualization-5.png[{VirtProductName},40,40] logo.

You can use {VirtProductName} with either the xref:../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[OVN-Kubernetes] or the xref:../networking/openshift_sdn/about-openshift-sdn.adoc#about-openshift-sdn[OpenShiftSDN] default Container Network Interface (CNI) network provider.

Learn more about xref:../virt/about-virt.adoc#about-virt[what you can do with {VirtProductName}].

include::modules/virt-supported-cluster-version.adoc[leveloffset=+2]

//CNV-4572-Guest-OS
[id="virt-guest-os-new"]
=== Supported guest operating systems
{VirtProductName} guests can use the following operating systems:

* Red Hat Enterprise Linux 6, 7, and 8.
* Microsoft Windows Server 2012 R2, 2016, and 2019.
* Microsoft Windows 10.

Other operating system templates shipped with {VirtProductName} are not supported.

[id="virt-2-5-new"]
== New and changed features
//Placeholder for new content.

//CNV-4904 Microsoft's SVVP certification applies to RHCOS workers. The content below from the 2.4 release notes seems to cover this already.
* {VirtProductName} is certified in Microsoft's Windows Server Virtualization Validation Program (SVVP) to run Windows Server workloads.
+
The SVVP Certification applies to:

** Red Hat Enterprise Linux CoreOS 8 workers. In the Microsoft SVVP Catalog, they are named _Red Hat {product-title} 4 on RHEL CoreOS 8_.

** Intel and AMD CPUs.

//CNV-7160 - New virtctl commands to expose raw QEMU guest agent data
* {VirtProductName} {VirtVersion} adds three new xref:../virt/virt-using-the-cli-tools.adoc#virt-virtctl-commands_virt-using-the-cli-tools[`virtctl` commands] to manage QEMU guest agent data:

** `virtctl fslist <vmi_name>` returns a full list of file systems available on the guest machine.
** `virtctl guestosinfo <vmi_name>` returns guest agent information about the operating system.
** `virtctl userlist <vmi_name>` returns a full list of logged-in users on the guest machine.

//CNV-7295 - Download virtctl binary from the CLI Tools page
* You can now download the `virtctl` client from the xref:../virt/install/virt-installing-virtctl.adoc#virt-installing-virtctl-client-web_virt-installing-virtctl[*Command Line Tools* page in the web console].

//CNV-5783 - VM migration (RHV) - SRIOV support
* You can now import a virtual machine with a Single Root I/O Virtualization (SR-IOV) network interface from Red Hat Virtualization.

[id="virt-2-5-networking-new"]
=== Networking
//Placeholder for new content.

//CNV-5263 - Supported bonding modes with nmstate
* The xref:../virt/node_network/virt-updating-node-network-config.adoc#virt-example-bond-nncp_virt-updating-node-network-config[supported bond modes] with nmstate now includes `mode=2 balance-xor` and `mode=4 802.3ad`.

[id="virt-2-5-storage-new"]
=== Storage
//Placeholder for new content.

//CNV-7170 - CDI import of container disks is faster and more efficient
* The Containerized Data Importer (CDI) can now xref:../virt/virtual_machines/virtual_disks/virt-using-container-disks-with-vms.adoc#virt-using-container-disks-with-vms[import container disk storage volumes] from the container image registry at a faster speed and allocate storage capacity more efficiently. CDI can pull a container disk image from the registry in about the same amount of time as it would take to import from an HTTP endpoint. You can import the disk into a persistent volume claim (PVC) equal in size to the disk image to use the underlying storage more efficiently.

//CNV-7168 - DataVolume and storage diagnostics
* It is now easier to xref:../virt/logging_events_monitoring/virt-diagnosing-datavolumes-using-events-and-conditions.adoc#virt-diagnosing-datavolumes-using-events-and-conditions[diagnose and troubleshoot issues] when preparing virtual machine (VM) disks that are managed by DataVolumes:
+
** For asynchronous image upload, if the virtual size of the disk image is larger than the size of the target DataVolume, an error message is returned before the connection is closed.
** You can use the `oc describe dv` command to monitor changes in the `PersistentVolumeClaim` (PVC) `Bound` conditions or transfer failures. If the value of the `Status:Phase` field is `Succeeded`, then the DataVolume is ready to be used.

//CNV-6824 - Offline virtual machine snapshots
* You can create, restore, and delete virtual machine (VM) snapshots in the CLI for VMs that are powered off (offline). {VirtProductName} supports
xref:../virt/virtual_machines/virtual_disks/virt-managing-offline-vm-snapshots.adoc#virt-managing-offline-vm-snapshots[offline VM snapshots] on:
+
** Red Hat OpenShift Container Storage
** Any other storage provider with the Container Storage Interface (CSI) driver that supports the Kubernetes Volume Snapshot API

//CNV-6825 - Smart cloning with snapshots
//CNV-4089 - Cloning a disk quickly using the storage array
// These two release note stories are combined in a single release note.
* You can now clone virtual disks efficiently and quickly using xref:../virt/virtual_machines/virtual_disks/virt-cloning-a-datavolume-using-smart-cloning#virt-cloning-a-datavolume-using-smart-cloning[smart-cloning]. Smart-cloning occurs automatically when you create a DataVolume with a `PersistentVolumeClaim` (PVC) source. Your storage provider must support the CSI Snapshots API to use smart-cloning.

[id="virt-2-5-web-new"]
=== Web console
//Placeholder for new content.

//CNV-7315 - Expose next run configuration in the UI
* If the virtual machine is running, changes made to the following fields and tabs in the web console will not take effect until you restart the virtual machine:
+
** *Boot Order* and *Flavor* in the *Details* tab
** The *Network Interfaces* tab
** The *Disks* tab
** The *Environment* tab
+
The *Pending Changes* banner at the top of the page displays a list of all changes that will be applied when the virtual machine restarts.

//CNV-7318 - Open the vnc/serial console in a new window
* You can now xref:../virt/virtual_machines/virt-accessing-vm-consoles.adoc#virt-vm-serial-console-web_virt-accessing-vm-consoles[open a virtual machine console] in a separate window.

//CNV-7169 - Auto-clone base images when creating VM in UI
* You can now xref:../virt/virtual_machines/virtual_disks/virt-creating-and-using-default-os-images#virt-creating-and-using-default-os-images[create default OS images] and automatically upload them using the {product-title} web console. A _default OS image_ is a bootable disk containing an operating system and all of the operating system's configuration settings, such as drivers. You use a default OS image to create bootable virtual machines with specific configurations.

//CNV-7314 - Expose VM image upload in UI
* You can now xref:../virt/virtual_machines/virtual_disks/virt-uploading-local-disk-images-web.adoc#virt-uploading-local-disk-images-web[upload a virtual machine image file] to a new persistent volume claim by using the web console.

//CNV-7317 - Expose qemu guest agent data in the UI
* When the QEMU guest agent runs on the virtual machine, you can use the web console to xref:../virt/virtual_machines/virt-viewing-qemu-guest-agent-web#virt-viewing-qemu-guest-agent-web[view information] about the virtual machine, users, file systems, and secondary networks.

[id="virt-2-5-changes"]
== Notable technical changes
//Placeholder for new content.

//CNV-7293 - Use OLM stable channel for installation
* When you xref:../virt/install/installing-virt-web.adoc#installing-virt-web[install] or xref:../virt/upgrading-virt.adoc#upgrading-virt[upgrade] {VirtProductName}, you select an *Update Channel*. There is a new *Update Channel* option that is named *stable*. Select the *stable* channel to ensure that you install or upgrade to the version of {VirtProductName} that is compatible with your {product-title} version.

//CNV-7102 - Block based storage support using vm-import-controller
* You can now import VMs with block-based storage into {VirtProductName}.

//CNV-7296 - HCO and HPP CRs moved to v1beta1
* The HyperConverged Operator (HCO), Containerized Data Importer (CDI), Hostpath Provisioner (HPP), and VM import custom resources have moved to API version `v1beta1`. The respective API version for these components is now:
+
`hco.kubevirt.io/v1beta1` +
`cdi.kubevirt.io/v1beta1` +
`hostpathprovisioner.kubevirt.io/v1beta1` +
`v2v.kubevirt.io/v1beta1`

//BZ-1874403
* The default `cloud-init` user password is now auto-generated for virtual machines that are created from templates.

//BZ-1885964 - Host assisted cloning is now faster
* When using host-assisted cloning, you can now clone virtual machine disks at a faster speed because of a more efficient compression algorithm.

//CNV-7548
* When a node fails in user-provisioned installations of {product-title} on bare metal deployments, the virtual machine does not automatically restart on another node. Automatic restart is supported only for installer-provisioned installations that have machine health checks enabled. Learn more about xref:../virt/install/preparing-cluster-for-virt.adoc#preparing-cluster-for-virt[configuring your cluster for {VirtProductName}].

[id="virt-2-5-known-issues"]
== Known issues

//This section is work in progress

* If your {product-title} cluster uses OVN-Kubernetes as the default Container Network Interface (CNI) provider, you cannot attach a Linux bridge or bonding to the default interface of a host because of a change in the host network topology of OVN-Kubernetes. As a workaround, you can use a secondary network interface connected to your host or switch to the OpenShift SDN default CNI provider. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1887456[*BZ#1887456*])

//This defect is on Verified status but the fix is not ideal. This known issue is likely to be removed for 2.6.
* If you xref:../virt/virtual_machines/importing_vms/virt-importing-vmware-vm.adoc#virt-creating-vddk-image_virt-importing-vmware-vm[add a VMware Virtual Disk Development Kit (VDDK) image] to the `openshift-cnv/v2v-vmware` config map by using the web console, a *Managed resource* error message displays. You can safely ignore this error. Save the config map by clicking *Save*. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1884538[*BZ#1884538*])

* When nodes are evicted, for example, when they are placed in maintenance mode during an {product-title} cluster upgrade, virtual machines are migrated twice instead of just once. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1888790[*BZ#1888790*])

* Following an upgrade, there might be more than one template per operating system workload. When creating a Microsoft Windows virtual machine from a cloned PVC using the default operating system (OS) images feature, the OS must have the correct workload value defined. Selecting an incorrect *Workload* value does not allow you to use a default OS image, even though the *(Source available)* label displays in the web console. The default OS image is attached to the newer template but the wizard might use the old template, which is not configured to support default OS images. Windows 2010 systems only support a workload value of *Desktop*, while Windows 2012, Windows 2016, and Windows 2019 only support a workload value of *Server*. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1907183[*BZ#1907183*])

//The issues below are from the 2.4 release. See items above for new known issues for 2.5.

//This defect is on Verified status
* If you enable a MAC address pool for a namespace by applying the KubeMacPool label and using the `io` attribute for virtual machines in that namespace, the `io` attribute configuration is not retained for the VMs. As a workaround, do not use the `io` attribute for VMs. Alternatively, you can disable KubeMacPool for the namespace. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1869527[*BZ#1869527*])

//Unresolved, moved to 2.6
* If you upgrade to {VirtProductName} {VirtVersion}, both older and newer versions of common templates are available for each combination of operating system, workload, and flavor. When you create a virtual machine by using a common template, you must use the newer version of the template. Disregard the older version to avoid issues. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1859235[*BZ#1859235*])

//Unresolved
* Running virtual machines that cannot be live migrated might block an {product-title} cluster upgrade. This includes virtual machines that use hostpath-provisioner storage or SR-IOV network interfaces. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1858777[*BZ#1858777*])
+
As a workaround, you can reconfigure the virtual machines so that they can be powered off during a cluster upgrade. In the `spec` section of the virtual machine configuration file:
+
. Remove the `evictionStrategy: LiveMigrate` field. See xref:../virt/live_migration/virt-configuring-vmi-eviction-strategy.adoc#virt-configuring-vmi-eviction-strategy[Configuring virtual machine eviction strategy] for more information on how to configure eviction strategy.
. Set the `runStrategy` field to `Always`.

//Unresolved, on ASSIGNED status
* For unknown reasons, memory consumption for the `containerDisk` volume type might gradually increase until it exceeds the memory limit. To resolve this issue, restart the VM. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1855067[*BZ#1855067*])

//Unresolved, on ASSIGNED status
* Sometimes, when attempting to edit the subscription channel of the *{VirtProductName} Operator* in the web console, clicking the *Channel* button of the *Subscription Overview* results in a JavaScript error. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1796410[*BZ#1796410*])

** As a workaround, trigger the upgrade process to {VirtProductName} {VirtVersion} from the CLI by running the following `oc` patch command:
+
[source,terminal]
----
$ export TARGET_NAMESPACE=openshift-cnv CNV_CHANNEL=2.5 && oc patch -n "${TARGET_NAMESPACE}" $(oc get subscription -n ${TARGET_NAMESPACE} --no-headers -o name) --type='json' -p='[{"op": "replace", "path": "/spec/channel", "value":"'${CNV_CHANNEL}'"}, {"op": "replace", "path": "/spec/installPlanApproval", "value":"Automatic"}]'
----
+
This command points your subscription to upgrade channel `2.5` and enables automatic updates.

//This defect is on POST status
* Live migration fails when nodes have different CPU models. Even in cases where nodes have the same physical CPU model, differences introduced by microcode updates have the same effect. This is because the default settings trigger host CPU passthrough behavior, which is incompatible with live migration. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1760028[*BZ#1760028*])

** As a workaround, set the default CPU model in the `kubevirt-config` ConfigMap, as shown in the following example:
+
[NOTE]
====
You must make this change before starting the virtual machines that support live migration.
====
+
. Open the `kubevirt-config` ConfigMap for editing by running the following command:
+
[source,terminal]
----
$ oc edit configmap kubevirt-config -n openshift-cnv
----
+
. Edit the ConfigMap:
+
[source,yaml]
----
kind: ConfigMap
metadata:
  name: kubevirt-config
data:
  default-cpu-model: "<cpu-model>" <1>
----
<1> Replace `<cpu-model>` with the actual CPU model value. You can determine this value by running `oc describe node <node>` for all nodes and looking at the `cpu-model-<name>` labels. Select the CPU model that is present on all of your nodes.

//Jira story for the following content https://issues.redhat.com/browse/CNV-4757.
* {VirtProductName} cannot reliably identify node drains that are triggered by running either `oc adm drain` or `kubectl drain`. Do not run these commands on the nodes of any clusters where {VirtProductName} is deployed. The nodes might not drain if there are virtual machines running on top of them.

** The current solution is to xref:../virt/node_maintenance/virt-setting-node-maintenance.adoc#virt-setting-node-maintenance[put nodes into maintenance].

//This defect is on Verified status
* If the {VirtProductName} storage PV is not suitable for importing a RHV VM, the progress bar remains at 10% and the import does not complete. The VM Import Controller Pod log displays the following error message: `Failed to bind volumes: provisioning failed for PVC`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1857784[*BZ#1857784*])

//Unresolved
* If you enter the wrong credentials for the RHV Manager while importing a RHV VM, the Manager might lock the admin user account because the `vm-import-operator` tries repeatedly to connect to the RHV API. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1887140[*BZ#1887140*])
+
To unlock the account, log in to the Manager and enter the following command:
+
[source,terminal]
----
$ ovirt-aaa-jdbc-tool user unlock admin
----
