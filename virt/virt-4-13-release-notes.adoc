:_content-type: ASSEMBLY
[id="virt-4-13-release-notes"]
= {VirtProductName} release notes
include::_attributes/common-attributes.adoc[]
:context: virt-4-13-release-notes

toc::[]

[id="virt-4-13-inclusive-language"]
== Making open source more inclusive

Red Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases. For more details, see link:https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language[our CTO Chris Wright's message].


== About Red Hat {VirtProductName}

Red Hat {VirtProductName} enables you to bring traditional virtual machines (VMs) into {product-title} where they run alongside containers, and are managed as native Kubernetes objects.

{VirtProductName} is represented by the image:virt-icon.png[{VirtProductName},40,40] icon.

You can use {VirtProductName} with either the xref:../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[OVN-Kubernetes] or the xref:../networking/openshift_sdn/about-openshift-sdn.adoc#about-openshift-sdn[OpenShiftSDN] default Container Network Interface (CNI) network provider.

Learn more about xref:../virt/about-virt.adoc#about-virt[what you can do with {VirtProductName}].

Learn more about xref:../virt/virt-architecture.adoc#virt-how-virt-works_virt-architecture[{VirtProductName} architecture and deployments].

xref:../virt/install/preparing-cluster-for-virt.adoc#preparing-cluster-for-virt[Prepare your cluster] for {VirtProductName}.

include::modules/virt-supported-cluster-version.adoc[leveloffset=+2]


[id="virt-guest-os"]
=== Supported guest operating systems
//CNV-16390 Supported guest operating systems
To view the supported guest operating systems for {VirtProductName}, refer to link:https://access.redhat.com/articles/973163#ocpvirt[Certified Guest Operating Systems in Red Hat OpenStack Platform, Red Hat Virtualization and OpenShift Virtualization].


[id="virt-4-13-new"]
== New and changed features

//CNV-21735 SVVP for 4.13: Ensure platform passes Windows Server Virtualization Validation Program - with RHCOS workers
//NOTE: This is a recurring release note. Modify the existing note text below if recommended by QE.
* {VirtProductName} is certified in Microsoft's Windows Server Virtualization Validation Program (SVVP) to run Windows Server workloads.
+
The SVVP Certification applies to:
+
** Red Hat Enterprise Linux CoreOS workers. In the Microsoft SVVP Catalog, they are named __Red Hat OpenShift Container Platform 4 on RHEL CoreOS 9__.
** Intel and AMD CPUs.

//CNV-21918 Release note: NEW VMs as restricted workloads

//CNV-21919 Release note: NEW RHEL9-based CNV builds available

//CNV-22041 Release note: NEW Retrieve a portable VM definition (yaml) when exporting a VM or VMSnapshot

//CNV-22972 Release note: Docs reorg
* The "Logging, events, and monitoring" documentation is now called xref:../virt/support/virt-support-overview.adoc#virt-support-overview[Support]. The monitoring tools documentation has been moved to xref:../virt/support/monitoring/virt-monitoring-overview.adoc#virt-monitoring-overview[Monitoring].

// CNV-24976 Release note: CHANGE Specify the supported guest operating systems



[id="virt-4-13-quick-starts"]
=== Quick starts

* Quick start tours are available for several {VirtProductName} features. To view the tours, click the *Help* icon *?* in the menu bar on the header of the {VirtProductName} console and then select *Quick Starts*. You can filter the available tours by entering the `virtualization` keyword in the *Filter* field.


[id="virt-4-13-installation-new"]
=== Installation


[id="virt-4-13-networking-new"]
=== Networking

//CNV-21838 Release note: NEW OpenShift Virtualization supports jumbo frames with OVN Kubernetes
* You can now xref:../virt/virtual_machines/vm_networking/virt-using-the-default-pod-network-with-virt.adoc#virt-jumbo-frames-vm-pod-nw_virt-using-the-default-pod-network-with-virt[send unfragmented jumbo frame packets] between two virtual machines (VMs) that are connected on the default pod network when you use the OVN-Kubernetes CNI plugin.

[id="virt-4-13-storage-new"]
=== Storage

//CNV-18462 Release note: CHANGE Retire alpha APIs for Storage
* {VirtProductName} storage resources now migrate automatically to the beta API versions. Alpha API versions are no longer supported.

[id="virt-4-13-web-new"]
=== Web console

//CNV-27552 Release note: VM configuration tab
* On the *VirtualMachine details* page, the *Scheduling*, *Environment*, *Network interfaces*, *Disks*, and *Scripts* tabs are displayed on the new xref:../virt/virt-web-console-overview.adoc#ui-virtualmachine-details-configuration[*Configuration* tab].

//CNV-23474 Release note: NEW Paste clipboard into VNC
* You can now xref:../virt/virt-web-console-overview.adoc#ui-virtualmachine-details-console[paste a string] from your clientâ€™s clipboard into the guest when using the VNC console.

//CNV-22555 Release note: CHANGE Hostname field moving from cloud-init options to VM details page

//CNV-25427 Release note: NEW Allow VM admin to expose SSH service using LB in the UI
* The *VirtualMachine details* -> *Details* tab now provides a new SSH service type *SSH over LoadBalancer* to expose the SSH service over a load balancer.

//CNV-25574 Release note: NEW UI - Ability to make a hot-plug disk part of the VM
* The option to make a hot-plug volume a persistent volume is added to the xref:../virt/virt-web-console-overview.adoc#ui-virtualmachine-details-disks[*Disks* tab].

//CNV-24128 Release notes: NEW - UI improve error condition display
* There is now a xref:../virt/virt-web-console-overview.adoc#ui-virtualmachine-details-diagnostics[*VirtualMachine details* -> *Diagnostics*] tab where you can view the status conditions of VMs and the snapshot status of volumes.

//NOTE: Comment out deprecated and removed features (and their IDs) if not used in a release
[id="virt-4-13-deprecated-removed"]
== Deprecated and removed features


[id="virt-4-13-deprecated"]
=== Deprecated features
// NOTE: when uncommenting deprecated features list, change the header level below to ===

Deprecated features are included in the current release and supported. However, they will be removed in a future release and are not recommended for new deployments.



[id="virt-4-13-removed"]
=== Removed features

Removed features are not supported in the current release.


//CNV-19043 Release note: CHANGE Remove rhel6 support
* Red Hat Enterprise Linux 6 is no longer supported on {VirtProductName}.

//CNV-23499: Carry over/repeat removed feature from version 4.12
* Support for the legacy HPP custom resource, and the associated storage class, has been removed for all new deployments. In {VirtProductName} 4.13, the HPP Operator uses the Kubernetes Container Storage Interface (CSI) driver to configure local storage. A legacy HPP custom resource is supported only if it had been installed on a previous version of {VirtProductName}.


//NOTE: Removed features for 4.13 are above this line.

//CNV-16317 NMState is no longer part of CNV
* {VirtProductName} 4.11 removed support for link:https://nmstate.io/[nmstate], including the following objects:
+
--
** `NodeNetworkState`
** `NodeNetworkConfigurationPolicy`
** `NodeNetworkConfigurationEnactment`
--
+
To preserve and support your existing nmstate configuration, install the xref:../networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.adoc#k8s-nmstate-about-the-k8s-nmstate-operator[Kubernetes NMState Operator] before updating to {VirtProductName} 4.11. For {VirtVersion} for xref:../virt/upgrading-virt.adoc#virt-about-eus-updates_upgrading-virt[Extended Update Support (EUS)] versions, install the Kubernetes NMState Operator after updating to {VirtVersion}. You can install the Operator from the *OperatorHub* in the {product-title} web console, or by using the OpenShift CLI (`oc`).

* The Node Maintenance Operator (NMO) is no longer shipped with {VirtProductName}. You can install the NMO from the *OperatorHub* in the {product-title} web console, or by using the OpenShift CLI (`oc`).
+
You must perform one of the following tasks before updating to {VirtProductName} 4.11 from {VirtProductName} 4.10.2 and later 4.10 releases. For xref:../virt/upgrading-virt.adoc#virt-about-eus-updates_upgrading-virt[Extended Update Support (EUS)] versions, you must perform the following tasks before updating to {VirtProductName} {VirtVersion} from 4.10.2 and later 4.10 releases:

** Move all nodes out of maintenance mode.
** Install the standalone NMO and replace the `nodemaintenances.nodemaintenance.kubevirt.io` custom resource (CR) with a `nodemaintenances.nodemaintenance.medik8s.io` CR.


[id="virt-4-13-changes"]
== Notable technical changes


[id="virt-4-13-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]


//NOTE: RNs related to 4.13 TP features begin here

//CNV-17910 Release note: PREVIEW CNV Observability

//CNV-18090 Release note: PREVIEW OVN Secondary Network

//CNV-18095 Release note: NEW [TechPreview] Add Virtual machines CPU metrics
* You can now use xref:../virt/support/monitoring/virt-prometheus-queries.adoc#about-querying-metrics_virt-prometheus-queries[Prometheus] to monitor the following metrics:

** `kubevirt_vmi_cpu_system_usage_seconds` returns the physical system CPU time consumed by the hypervisor.
** `kubevirt_vmi_cpu_user_usage_seconds` returns the physical user CPU time consumed by the hypervisor.
** `kubevirt_vmi_cpu_usage_seconds` returns the total CPU time used in seconds by calculating the sum of the vCPU and the hypervisor usage.

//CNV-18330 Release note: PREVIEW Availability of new checkup capable of verifying cluster's readiness to run DPDK applications in VMs
* You can now run a xref:../virt/support/monitoring/virt-running-cluster-checkups.adoc#virt-checking-cluster-dpdk-readiness_virt-running-cluster-checkups[checkup] to verify if your {product-title} cluster node can run a virtual machine with a Data Plane Development Kit (DPDK) workload with zero packet loss.

//Adding another TP RN as part of CNV-18330: Configuring VM for DPDK
* You can xref:../virt/virtual_machines/vm_networking/virt-attaching-vm-to-sriov-network.adoc#virt-configuring-vm-dpdk_virt-attaching-vm-to-sriov-network[configure your virtual machine to run DPDK workloads] to achieve lower latency and higher throughput for faster packet processing in the user space.

//CNV-18413 Release note: PREVIEW Accessing VMs on secondary networks by using cluster domain name
* You can now xref:../virt/virtual_machines/vm_networking/virt-accessing-vm-secondary-network-fqdn.adoc#virt-accessing-vm-secondary-network-fqdn[access a VM that is attached to a secondary network interface] from outside the cluster by using its fully qualified domain name (FQDN).


//CNV-19436 Release note: NEW Retrieve a temporary token to access the VNC endpoint of a VM
//NOTE: This is a TP item for virt-4.13

//CNV-20965 Release note: PREVIEW Default creation and deployment of common set of instancetypes and preferences that eventually replace common templates

//CNV-21991 Release note: PREVIEW CNV Provider for Hypershift


//NOTE: Existing 4.12 TP notes begin here


//CNV-21824
* You can now run xref:../virt/support/monitoring/virt-running-cluster-checkups.adoc#virt-running-cluster-checkups[{product-title} cluster checkups] to measure network latency between VMs.

//CNV-20526
* The Tekton Tasks Operator (TTO) now xref:../virt/virtual_machines/virt-managing-vms-openshift-pipelines.adoc#virt-managing-vms-openshift-pipelines[integrates {VirtProductName} with {pipelines-title}]. TTO includes cluster tasks and example pipelines that allow you to:
+
** Create and manage virtual machines (VMs), persistent volume claims (PVCs), and data volumes.
** Run commands in VMs.
** Manipulate disk images with `libguestfs` tools.
** Install Windows 10 into a new data volume from a Windows installation image (ISO file).
** Customize a basic Windows 10 installation and then create a new image and template.

//CNV-20149
* You can now use the xref:../virt/support/monitoring/virt-monitoring-vm-health.adoc#virt-define-guest-agent-ping-probe_virt-monitoring-vm-health[guest agent ping probe] to determine if the QEMU guest agent is running on a virtual machine.

//CNV-20963
* You can now use Microsoft Windows 11 as a guest operating system. However, {VirtProductName} {VirtVersion} does not support USB disks, which are required for a critical function of BitLocker recovery. To protect recovery keys, use other methods described in the link:https://learn.microsoft.com/en-us/windows/security/information-protection/bitlocker/bitlocker-recovery-guide-plan[BitLocker recovery guide].

//CNV-19145 Support live migration policies
* You can create live migration policies with specific parameters, such as bandwidth usage, maximum number of parallel migrations, and timeout, and apply the policies to groups of virtual machines by using virtual machine and namespace labels.


[id="virt-4-13-bug-fixes"]
== Bug fixes





[id="virt-4-13-known-issues"]
== Known issues

* You cannot run {VirtProductName} on a single-stack IPv6 cluster. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2193267[*BZ#2193267*])

* When you use two pods with different SELinux contexts, VMs with the `ocs-storagecluster-cephfs` storage class fail to migrate and the VM status changes to `Paused`. This is because both pods try to access the shared `ReadWriteMany` CephFS volume at the same time. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2092271[*BZ#2092271*])
** As a workaround, use the `ocs-storagecluster-ceph-rbd` storage class to live migrate VMs on a cluster that uses Red Hat Ceph Storage.

// Fix targeted for 4.12.1
* The `TopoLVM` provisioner name string has changed in {VirtProductName} 4.12. As a result, the automatic import of operating system images might fail with the following error message (link:https://bugzilla.redhat.com/show_bug.cgi?id=2158521[*BZ#2158521*]):
+
[source,terminal]
----
DataVolume.storage spec is missing accessMode and volumeMode, cannot get access mode from StorageProfile.
----
** As a workaround:
. Update the `claimPropertySets` array of the storage profile:
+
[source,terminal]
----
$ oc patch storageprofile <storage_profile> --type=merge -p '{"spec": {"claimPropertySets": [{"accessModes": ["ReadWriteOnce"], "volumeMode": "Block"}, \
    {"accessModes": ["ReadWriteOnce"], "volumeMode": "Filesystem"}]}}'
----
. Delete the affected data volumes in the `openshift-virtualization-os-images` namespace. They are recreated with the access mode and volume mode from the updated storage profile.

// Fix targeted for 4.13
* When restoring a VM snapshot for storage whose binding mode is `WaitForFirstConsumer`, the restored PVCs remain in the `Pending` state and the restore operation does not progress.
** As a workaround, start the restored VM, stop it, and then start it again. The VM will be scheduled, the PVCs will be in the `Bound` state, and the restore operation will complete. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2149654[*BZ#2149654*])

//New known issues for 4.13 go above this comment
* VMs created from common templates on a Single Node OpenShift (SNO) cluster display a `VMCannotBeEvicted` alert because the template's default eviction strategy is `LiveMigrate`. You can ignore this alert or remove the alert by updating the VM's eviction strategy. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2092412[*BZ#2092412*])

* Uninstalling {VirtProductName} does not remove the `feature.node.kubevirt.io` node labels created by {VirtProductName}. You must remove the labels manually. (link:https://issues.redhat.com/browse/CNV-22036[*CNV-22036*])

* Some persistent volume claim (PVC) annotations created by the Containerized Data Importer (CDI) can cause the virtual machine snapshot restore operation to hang indefinitely. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2070366[*BZ#2070366*])
** As a workaround, you can remove the annotations manually:
. Obtain the xref:../virt/virtual_machines/virtual_disks/virt-managing-vm-snapshots.adoc#vm-snapshot-controller-and-crds_virt-managing-vm-snapshots[VirtualMachineSnapshotContent] custom resource (CR) name from the `status.virtualMachineSnapshotContentName` value in the `VirtualMachineSnapshot` CR.
. Edit the `VirtualMachineSnapshotContent` CR and remove all lines that contain `k8s.io/cloneRequest`.
. If you did not specify a value for `spec.dataVolumeTemplates` in the `VirtualMachine` object, delete any `DataVolume` and `PersistentVolumeClaim` objects in this namespace where both of the following conditions are true:
+
.. The object's name begins with `restore-`.
.. The object is not referenced by virtual machines.
+
This step is optional if you specified a value for `spec.dataVolumeTemplates`.
. Repeat the xref:../virt/virtual_machines/virtual_disks/virt-managing-vm-snapshots.adoc#virt-restoring-vm-from-snapshot-cli_virt-managing-vm-snapshots[restore operation] with the updated `VirtualMachineSnapshot` CR.

* Windows 11 virtual machines do not boot on clusters running in link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/security_hardening/index#con_federal-information-processing-standard-fips_assembly_installing-the-system-in-fips-mode[FIPS mode]. Windows 11 requires a TPM (trusted platform module) device by default. However, the `swtpm` (software TPM emulator) package is incompatible with FIPS. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2089301[*BZ#2089301*])

//BZ-1885605
* If your {product-title} cluster uses OVN-Kubernetes as the default Container Network Interface (CNI) provider, you cannot attach a Linux bridge or bonding device to a host's default interface because of a change in the host network topology of OVN-Kubernetes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1885605[*BZ#1885605*])
** As a workaround, you can use a secondary network interface connected to your host, or switch to the OpenShift SDN default CNI provider.

* In some instances, multiple virtual machines can mount the same PVC in read-write mode, which might result in data corruption. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1992753[*BZ#1992753*])
** As a workaround, avoid using a single PVC in read-write mode with multiple VMs.

* The Pod Disruption Budget (PDB) prevents pod disruptions for migratable virtual machine images. If the PDB detects pod disruption, then `openshift-monitoring` sends a `PodDisruptionBudgetAtLimit` alert every 60 minutes for virtual machine images that use the `LiveMigrate` eviction strategy. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2026733[*BZ#2026733*])
** As a workaround, xref:../monitoring/managing-alerts.adoc#silencing-alerts_managing-alerts[silence alerts].

* {VirtProductName} links a service account token in use by a pod to that specific pod. {VirtProductName} implements a service account volume by creating a disk image that contains a token. If you migrate a VM, then the service account volume becomes invalid. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2037611[*BZ#2037611*])
** As a workaround, use user accounts rather than service accounts because user account tokens are not bound to a specific pod.

* If you clone more than 100 VMs using the `csi-clone` cloning strategy, then the Ceph CSI might not purge the clones. Manually deleting the clones can also fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2055595[*BZ#2055595*])
** As a workaround, you can restart the `ceph-mgr` to purge the VM clones.
