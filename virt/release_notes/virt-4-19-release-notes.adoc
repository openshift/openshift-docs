:_mod-docs-content-type: ASSEMBLY
include::_attributes/common-attributes.adoc[]
[id="virt-4-19-release-notes"]
= {VirtProductName} release notes
:context: virt-4-19-release-notes

toc::[]

[id="virt-doc-feedback_{context}"]
== Providing documentation feedback

To report an error or to improve our documentation, log in to your link:https://issues.redhat.com[Red Hat Jira account] and submit a link:https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12323181&issuetype=1&components=12333768&priority=10200&summary=%5BDoc%5D&customfield_12316142[Jira issue].

[id="virt-about-virt_{context}"]
== About Red Hat {VirtProductName}

With Red Hat {VirtProductName}, you can bring traditional virtual machines (VMs) into {product-title} and run them alongside containers. In {VirtProductName}, VMs are native Kubernetes objects that you can manage by using the {product-title} web console or the command line.

{VirtProductName} is represented by the image:virt-icon.png[{VirtProductName},40,40] icon.

You can use {VirtProductName} the xref:../../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[OVN-Kubernetes] Container Network Interface (CNI) network provider.

Learn more about xref:../../virt/about_virt/about-virt.adoc#about-virt[what you can do with {VirtProductName}].

Learn more about xref:../../virt/about_virt/virt-architecture.adoc#virt-architecture[{VirtProductName} architecture and deployments].

xref:../../virt/install/preparing-cluster-for-virt.adoc#preparing-cluster-for-virt[Prepare your cluster] for {VirtProductName}.

include::modules/virt-supported-cluster-version.adoc[leveloffset=+2]


[id="virt-guest-os_{context}"]
=== Supported guest operating systems
To view the supported guest operating systems for {VirtProductName}, see link:https://access.redhat.com/articles/973163#ocpvirt[Certified Guest Operating Systems in Red Hat OpenStack Platform, Red Hat Virtualization, OpenShift Virtualization and Red Hat Enterprise Linux with KVM].

//Ensure platform passes Windows Server Virtualization Validation Program. Otherwise, comment out the section below.
[id="virt-svvp-certification_{context}"]
=== Microsoft Windows SVVP certification

//CNV-47134 SVVP 4.18 Release Note: New
//NOTE: This is a recurring release note. Modify the existing note text below if recommended by QE.
{VirtProductName} is certified in Microsoft's Windows Server Virtualization Validation Program (SVVP) to run Windows Server workloads.

The SVVP certification applies to:

* Red Hat Enterprise Linux CoreOS workers. In the Microsoft SVVP Catalog, they are named __Red{nbsp}Hat {product-title} 4.19__.
* Intel and AMD CPUs.

[id="virt-quick-starts_{context}"]
== Quick starts

Quick start tours are available for several {VirtProductName} features. To view the tours, click the *Help* icon *?* in the menu bar on the header of the {product-title} web console and then select *Quick Starts*. You can filter the available tours by entering the keyword `virtualization` in the *Filter* field.


[id="virt-4-19-new_{context}"]
== New and changed features

This release adds new features and enhancements related to the following components and concepts:

//[id="virt-4-19-installation-update_{context}"]
//=== Installation and update
//[id="virt-4-19-installation-update"]
//=== Installation and update

[id="virt-4-19-infrastructure_{context}"]
=== Infrastructure
//[id="virt-4-19-infrastructure"]
//=== Infrastructure

//CNV-49593
* You can now prevent the inadvertent deletion of a virtual machine (VM) by xref:../../virt/managing_vms/virt-enabling-disabling-vm-delete-protection.adoc#virt-enabling-disabling-vm-delete-protection[enabling delete protection for the VM]. You can also disable delete protection that has been set for a VM.
+
As a cluster administrator, you can prevent users from enabling VM delete protection xref:../../virt/managing_vms/virt-enabling-disabling-vm-delete-protection.adoc#virt-removing-vm-delete-protection_virt-enabling-disabling-vm-delete-protection[by removing the option at the cluster level].

[id="virt-4-19-virtualization_{context}"]
=== Virtualization

//CNV-54747
* You can now choose to expand virtual machines (VMs) using instance types and preferences. For more information, see link:https://access.redhat.com/solutions/7107803[Using InstancetypeReferencePolicy to expand VirtualMachines] in the Red Hat Customer Portal.
//CNV-58007
* {op-system-base-full} 10 is added as a certified guest operating system. For a complete list of supported guest operating systems, see link:https://access.redhat.com/articles/4234591[Certified Guest Operating Systems in OpenShift Virtualization].

//CNV-54610: Release note: Mass machine type change procedure
* You can now xref:../../virt/managing_vms/virt-edit-vms.adoc#virt-updating-multiple-vms_virt-edit-vms[update the machine type of multiple virtual machines (VMs) at the same time] from the {oc-first}.

[id="virt-4-19-networking_{context}"]
=== Networking

//CNV-30151 NNCP to enable LLDP
* You can now xref:../../networking/k8s_nmstate/k8s-nmstate-updating-node-network-config.adoc#virt-example-enabling-lldp-policy_k8s-nmstate-updating-node-network-config[configure a `NodeNetworkConfigurationPolicy` manifest] to enable the Link Layer Discovery Protocol (LLDP) listener for all ethernet ports in your {product-title} cluster.
//CNV-55191 Host network configuration visualization
* You can now create a new Node Network Configuration Policy (NNCP) in the topology view of the cluster and see its graphical representation in real time. Clicking *Create* on the *Node network configuration* page opens a form for configuring the elements of the new NNCP, and the NNCP is also displayed as a diagram.

//CNV-52856 UDN localnet
* You can now use the OVN-Kubernetes localnet network topology to connect a VM to a xref:../../virt/vm_networking/virt-connecting-vm-to-secondary-udn.adoc#virt-connecting-vm-to-secondary-udn[secondary user-defined network].


[id="virt-4-19-storage_{context}"]
=== Storage

//CNV-55497 Doc: PVC source support for DataImportCron
* You can now use a PVC as the source of a custom `DataImportCron` in the `dataImportCronTemplates` section of the `HyperConverged` custom resource (CR). See xref:../../virt/storage/virt-automatic-bootsource-updates.adoc#virt-automatic-bootsource-updates[Managing automatic boot source updates] for more information.

[id="virt-4-19-web_{context}"]
=== Web console

// CNV-52871 - Doc: Multi IOthread with fast storage
* You can now xref:../../virt/managing_vms/virt-edit-vms.adoc#virt-configure-multiple-iothreads_virt-edit-vms[configure multiple IOThreads for virtual machines that use fast storage], such as SSD (solid-state drive) or NVMe (non-volatile memory express). This improves I/O performance by enabling multiple threads for disk access.

// CNV-56843:
* On the *VirtualMachines* page, you can now xref:../../virt/support/virt-support-overview.adoc#virt-web-console_virt-support-overview[see the summary of CPU, memory, and storage usage by your VMs]. To restrict this summary to the VMs in a specific project, select the project name in the tree view.

// CNV-55958:
* On the *VirtualMachines* page, you can now xref:../../virt/managing_vms/virt-list-vms.adoc#virt-listing-vms-web_virt-list-vms[navigate between your VMs by using the tree view].

//CNV-56275
* An attempt to xref:../../virt/managing_vms/virt-controlling-vm-states.adoc#virt-controlling-vm-states[stop, restart, or pause a VM or multiple VMs] now displays a confirmation dialog.

//CNV-55965
* You can now access xref:../../virt/managing_vms/virt-controlling-vm-states.adoc#virt-controlling-vm-states[the commands of the *Options* menu] {kebab} from the tree view by right-clicking the VM. If you right-click a project and select a command, the action is applied to all VMs in the project.
//CNV-59097
* You can now perform bulk actions on multiple virtual machines (VMs), including adding or removing labels, viewing the number of VMs selected for deletion, and moving VMs to a folder within the same namespace.

// CNV-56003:
* On the *VirtualMachines* page, you can now xref:../../virt/managing_vms/virt-list-vms.adoc#virt-organize-vms-web_virt-list-vms[use the tree view to organize VMs in folders] and drag and drop VMs to these folders.

// CNV-56266: Advanced search for VMs
* You can now xref:../../virt/managing_vms/virt-manage-vmis.adoc#virt-searching-vmis-web_virt-manage-vmis[search for virtual machines] by fields such as name, project, description, labels, date created, vCPU, and memory. You can also save frequently used search queries.


[id="virt-4-19-monitoring_{context}"]
=== Monitoring

//CNV-59720
* The following xref:../../virt/monitoring/virt-runbooks.adoc#virt-runbooks[alerts for the {CNVOperatorDisplayName}] are now included in the {product-title} runbooks:
** `HAControlPlaneDown`
** `HighCPUWorkload`
** `NodeNetworkInterfaceDown`

//CNV-50346
* New metrics are now available and improve the observability of virtual machines (VMs) and virtual machine instances (VMIs). You can use these metrics to monitor the following VM lifecycle events, resource usage, and migration details:
+
--
** Migration metrics
** vNIC networking information metrics
** Allocated storage size metrics for running and stopped VMs
--
+
In addition, the following VM and VMI metadata metrics are now available:
+
--
** The `pod_name` label in `kubevirt_vmi_info`
** UID in VM and VMI metrics
** The VM creation date
--
+
For a complete list of virtualization metrics, see link:https://github.com/kubevirt/monitoring/blob/main/docs/metrics.md[KubeVirt components metrics].

//[id="virt-4-19-documentation_{context}"]
//=== Documentation improvements


[id="virt-4-19-notable-technical-changes_{context}"]
=== Notable technical changes

// CNV-48243: RN - Support declarative VirtualMachine management with instance types
* VirtualMachines that use instance types and preferences no longer have their specification mutated at runtime to include derived metadata, such as `revisionName`. This metadata is now stored in the `status` field to preserve the declarative VM specification and ensure compatibility.

//CNV-57108
* In {VirtProductName} 4.19, the default permissions for live migration have changed to improve cluster security. Users must now be explicitly granted the `kubevirt.io:migrate` xref:../../virt/about_virt/virt-security-policies.adoc#default-cluster-roles-for-virt_virt-security-policies[cluster role] to create, delete, or update live migration requests. Previously, namespace administrators had these permissions by default. For more information, see xref:../../virt/live_migration/virt-about-live-migration.adoc#virt-about-live-migration-permissions_virt-about-live-migration[About live migration permissions].

[id="virt-4-19-deprecated-removed_{context}"]
== Deprecated and removed features
//NOTE: Comment out deprecated and removed features (and their IDs) if not used in a release

[id="virt-4-19-deprecated_{context}"]
=== Deprecated features
// NOTE: When uncommenting deprecated features list, change the Removed features header level below to ===

Deprecated features are included in the current release and supported. However, they will be removed in a future release and are not recommended for new deployments.

* The `OperatorConditionsUnhealthy` alert is deprecated. You can safely xref:../../observability/monitoring/managing-alerts/managing-alerts-as-an-administrator.adoc#silencing-alerts-adm_managing-alerts-as-an-administrator[silence] it.

//CNV-63003 Release note: Deprecate feature gates
* The following `HyperConverged` custom resource (CR) fields have been deprecated and copied from their original location under the `spec.featureGates` fields to a new location in the `spec` field, where they can be used if needed:

    ** `DeployVmConsoleProxy`
    ** `EnableApplicationAwareQuota`
    ** `EnableCommonBootImageImport`

+
If used in the `spec.featureGates` location, the old fields are ignored.

//[id="virt-4-19-removed_{context}"]
//=== Removed features

//Removed features are no longer supported in {VirtProductName}.

[id="virt-4-19-technology-preview_{context}"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

//CNV-28816
* With the release of {VirtProductName} 4.19.1, you can install {VirtProductName} on {oci-first-no-rt}. For more information, see link:https://access.redhat.com/articles/7118050[{VirtProductName} and Oracle Cloud Infrastructure known issues and limitations] in the Red{nbsp}Hat Knowledgebase, and link:https://github.com/oracle-quickstart/oci-openshift/blob/main/custom_manifests/oci-ccm-csi-drivers/v1.30.0-RWX-LA/openshift-virtualization.md[Installing {VirtProductName} on OCI] on GitHub.
+
[NOTE]
====
In {VirtProductName} 4.19.6 and later, installing {VirtProductName} on OCI is generally available.
====

//CNV-38944
* You can install {VirtProductName} on {gcp-first}. For more information, see link:https://access.redhat.com/articles/7120382[{VirtProductName} and {gcp-full} known storage issues and limitations] in the Red{nbsp}Hat Knowledgebase.

//CNV-52857 and CNV-56324
* You can now xref:../../virt/vm_networking/virt-setting-interface-link-state.adoc#virt-setting-interface-link-state[manage the link state] of a primary or secondary virtual machine (VM) interface by using the {product-title} web console or the CLI.

//CNV-58005
* You can install {VirtProductName} on Azure Red Hat OpenShift (ARO). For more information, see link:https://learn.microsoft.com/en-us/azure/openshift/howto-create-openshift-virtualization[{VirtProductName} for Azure Red Hat OpenShift (preview)] in the Microsoft documentation.

// CNV-58423
* The `DevKubeVirtRelieveAndMigrate` descheduler profile is xref:../../virt/managing_vms/advanced_vm_management/virt-enabling-descheduler-evictions.adoc#nodes-descheduler-profiles_virt-enabling-descheduler-evictions[now available]. This profile enhances the `LongLifecycle` profile by supporting load-aware descheduling, dynamic soft taints, and improved workload rebalancing.

//CNV-62829
* You can now deploy {VirtProductName} on IPv6 single-stack clusters. Support for IPv6 single-stack is limited to the OVN-Kubernetes localnet and Linux bridge Container Network Interface (CNI) plugins.

//[id="virt-4-19-bug-fixes_{context}"]
//== Bug fixes


[id="virt-4-19-known-issues_{context}"]
== Known issues

//[discrete]
//[id="virt-4-19-ki-monitoring_{context}"]
//=== Monitoring

[discrete]
[id="virt-4-19-ki-networking_{context}"]
=== Networking
//CNV-38746
* When you update from {product-title} 4.12 to a newer minor version, VMs that use the `cnv-bridge` Container Network Interface (CNI) fail to live migrate. (link:https://access.redhat.com/solutions/7069807[])
** As a workaround, change the `spec.config.type` field in your `NetworkAttachmentDefinition` manifest from `cnv-bridge` to `bridge` before performing the update.

//CNV-69040
* {SMProductName} 3.1.1 and Istio versions 1.25 and later are incompatible with {VirtProductName} {VirtVersion} because the annotation `traffic.sidecar.istio.io/kubevirtInterfaces` is deprecated. (link:https://issues.redhat.com/browse/OSSM-10883[*OSSM-10883*])
** As a workaround, when installing {SMProductShortName} for integration with {VirtProductName}, select version 3.0.4 and Istio 1.24.4 instead of the default versions that are displayed in the web console.


[discrete]
[id="virt-4-19-ki-nodes_{context}"]
==== Nodes
//CNV-38543 - 4.16 Still an issue
* Uninstalling {VirtProductName} does not remove the `feature.node.kubevirt.io` node labels created by {VirtProductName}. You must remove the labels manually. (link:https://issues.redhat.com/browse/CNV-38543[*CNV-38543*])

[discrete]
[id="virt-4-19-ki-storage_{context}"]
==== Storage

//CNV-61279: 4.19 - Storage Migrate VM with MTC
* Restoring a snapshot of a virtual machine (VM) migrated using Migration Toolkit for Containers (MTC) fails. The restore creates a persistent volume claim (PVC) but not a data volume (DV). The VM spec references a `DataVolumeTemplate` missing from the `volumes` list. (link:https://issues.redhat.com/browse/CNV-61279[*CNV-61279*])
** As a workaround, restart the VM after storage migration and before taking the snapshot. This creates a new controller revision that avoids the issue.

// CNV-55104: 4.18 - Unresolved
* If you perform storage class migration for a stopped VM, the VM might not be able to start because of a missing bootable device. To prevent this, do not attempt storage class migration if the VM is not running. (link:https://issues.redhat.com/browse/CNV-55104[*CNV-55104*])

[discrete]
[id="virt-4-19-ki-virtualization_{context}"]
=== Virtualization

//CNV-61066
* Live migration fails if the VM name exceeds 47 characters. (link:https://issues.redhat.com/browse/CNV-61066[*CNV-61066*])

//CNV-48348
* When the mode of live migration is *PostCopy*, hot-plugging CPU or memory resource fails. (link:https://issues.redhat.com/browse/CNV-48348[*CNV-48348*])

//CNV-33835: 4.16 - Unresolved
* {VirtProductName} links a service account token in use by a pod to that specific pod. {VirtProductName} implements a service account volume by creating a disk image that contains a token. If you migrate a VM, then the service account volume becomes invalid. (link:https://issues.redhat.com/browse/CNV-33835[*CNV-33835*])
** As a workaround, use user accounts rather than service accounts because user account tokens are not bound to a specific pod.

//CNV-36448
* When adding a virtual Trusted Platform Module (vTPM) device to a Windows VM, the BitLocker Drive Encryption system check passes even if the vTPM device is not persistent. This is because a vTPM device that is not persistent stores and recovers encryption keys using ephemeral storage for the lifetime of the virt-launcher pod. When the VM migrates or is shut down and restarts, the vTPM data is lost. (link:https://issues.redhat.com/browse/CNV-36448[CNV-36448])

//[discrete]
//[id="virt-4-19-ki-webconsole_{context}"]
//=== Web console

[discrete]
[id="virt-4-19-ki-ibm-z_{context}"]
=== {ibm-z-title} and {ibm-linuxone-title}

// cnv-61740 - s390x VM fails to boot with SATA CD-ROM
* If you create a VM from a template and select *Boot from CD*, the VM fails to boot and the error `unsupported configuration: SATA is not supported with this QEMU binary` is logged. This occurs because the CD-ROM is automatically mounted as a SATA device, which is not supported on s390x architecture. (link:https://issues.redhat.com/browse/CNV-61740[CNV-61740])
** As a workaround, navigate to the VM's *Configuration* ->  *Storage* tab, select the CD-ROM, and change the interface type from *SATA* to *SCSI*.

// cnv-61957 - GPU devices should not be shown in UI
* GPU devices appear in the *Hardware Devices* list for s390x VMs, but GPU support is not available for s390x architecture. You can disregard these list entries. (link:https://issues.redhat.com/browse/CNV-61957[CNV-61957])

// ocpbugs-51113 - ballooning call traces
* When you create a VM by using {op-system-base-full} container disk images for s390x architecture, call traces referencing `virtio_balloon` free page reporting print to the VM console. This is due to a kernel bug. (link:https://issues.redhat.com/browse/OCPBUGS-51113[OCPBUGS-51113])
** As a workaround, disable memory ballooning for the VM by adding the following parameter to the VM YAML configuration: `spec.domain.devices.autoattachMemBalloon: false`.
+
You can also disable free page reporting of memory ballooning for all new VMs. To do so, edit the `HyperConverged` CR and add the parameter `spec.virtualMachineOptions.disableFreePageReporting: true`.

// cnv-56889 - boot mode
* VMs based on s390x architecture can only use the *IPL* boot mode. However, in the {product-title} web console, the *Boot mode* list for s390x VMs incorrectly includes *BIOS*, *UEFI*, and *UEFI (secure)* boot modes. If you select one of these modes for an s390x-based VM, the operation fails. (link:https://issues.redhat.com/browse/CNV-56889[CNV-56889])

// cnv-56890 - threads
* In the {product-title} web console, it is erroneously possible to define multiple CPU threads for a VM based on s390x architecture. If you define multiple CPU threads, the VM enters a `CrashLoopBackOff` state with the `qemu-kvm: S390 does not support more than 1 threads` error. (link:https://issues.redhat.com/browse/CNV-56890[CNV-56890])

[id="virt-4.19-asynch-releases_{context}"]
== Maintenance releases

Release notes for asynchronous releases of Red Hat {VirtProductName}.

[id="virt-4.19.6_{context}"]
=== Version 4.19.6

.New and changed features

//CNV-43322 Release notes:TP to GA OCI and CNV-69919 (backporting)
* Installing {VirtProductName} on {oci-first-no-rt} is now generally available. For more information, see link:https://access.redhat.com/articles/7118050[{VirtProductName} and Oracle Cloud Infrastructure known issues and limitations] in the Red{nbsp}Hat Knowledgebase, and link:https://github.com/oracle-quickstart/oci-openshift/blob/main/docs/openshift-virtualization.md[Installing {VirtProductName} on OCI] on GitHub.

[id="virt-4.19.4_{context}"]
=== Version 4.19.4

.New and changed features

//CNV-63767 Release note: TP to GA (formerly CNV-58547 as TP UI - Bulk Storage Class Migration within a single cluster)
* By using the {product-title} web console, you can now xref:../../virt/managing_vms/virt-migrating-vms-in-single-cluster-to-different-storage-class.adoc#virt-migrating-bulk-vms-different-storage-class-web_virt-migrating-vms-in-single-cluster-to-different-storage-class[migrate VMs in bulk from one storage class to another storage class].
+
This feature requires both {VirtProductName} 4.19.4 or greater and {mtc-first} 1.8.9 or greater.

[id="virt-4.19.3_{context}"]
=== Version 4.19.3

.New and changed features

//CNV-65955 DOC: s390 Architecture TP to GA
* Support for s390x architecture is now generally available. You can use {VirtProductName} on an {product-title} cluster that has been deployed in one or more logical partitions (LPARs) on {ibm-z-name} and {ibm-linuxone-name} (s390x architecture) systems. For more information, see xref:../../virt/install/preparing-cluster-for-virt.adoc#ibm-z-linuxone-compatibility_preparing-cluster-for-virt[{ibm-z-title} and {ibm-linuxone-title} compatibility].

[id="virt-4.19.1_{context}"]
=== Version 4.19.1

.New and changed features

//CNV-57758 DOC: deploying clusters with GPFS
* With the new {IBMFusionFirst}, you can now deploy VMs on a scalable, clustered file system in Red{nbsp}Hat {VirtProductName}. {FusionSAN} offers access to consolidated, block-level data storage. It presents storage devices such as disk arrays to the operating system as if they were direct-attached storage.
+
The {FusionSAN} Operator is available in the {product-title} Operator hub.
+
See xref:../../virt/storage/install-configure-fusion-access-san.adoc#about-fusion-access-san_install-configure-fusion-access-san[About {IBMFusionFirst}] for more information.

.Known issues

//OCPNAS-56 Failed localdisk should have its status reflected
* When a file system in {FusionSAN} has two local disks and one local disk fails, both local disks move to the `Unknown` state, with no indication which of the local disks failed. (*OCPNAS-56*)
//(link:https://issues.redhat.com/browse/OCPNAS-56[OCPNAS-56])

//OCPNAS-61 Removing primary filesystem causes GPFS storage to become unusable
* When creating more than one file system for VM storage in {FusionSAN}, deleting the initial primary file system results in all of the remaining file systems becoming unusable. You cannot migrate or restart any of the VMs running on the remaining file systems, and you cannot create new VMs on the remaining file systems.
+
To determine which file system is the primary file system, run the following command:
+
[source,terminal]
----
$ oc get cso -n ibm-spectrum-scale-csi ibm-spectrum-scale-csi -o jsonpath='{.spec.clusters[*].primary.primaryFs}'
----
+
(*OCPNAS-61*)
//(link:https://issues.redhat.com/browse/OCPNAS-61[OCPNAS-61])

//OCPNAS-62 VM cannot be unpaused after disruption to GPFS storage backend
* When a disruption occurs between the worker nodes in a {FusionSAN} storage cluster and the shared LUNs they are connected to, the VMs on the storage cluster pause and cannot be unpaused even after the service was restored. The only way to recover the VM is to restart it. (*OCPNAS-62*)
//(link:https://issues.redhat.com/browse/OCPNAS-62[OCPNAS-62])

//OCPNAS-77 MTC storage live migration fails for target GPFS/RWX access mode
* Storage live migration from ODF to {FusionSAN} using MTC (v1.8.6) only works when the target access mode is specified as `RWO`. However, {FusionSAN} uses `filesystem/RWX` by default.
+
When you migrate from ODF to {FusionSAN} (RWO) you receive the following error in the VM logs:
+
[source,text]
----
message: 'cannot migrate VMI: PVC dv-fedora000-mig-hwtp is not shared, live migration
  requires that all PVCs must be shared (using ReadWriteMany access mode)'
reason: DisksNotLiveMigratable
----
+
This results in the VM being inaccessible when the worker node is not available.
+
(*OCPNAS-77*)
//(link:https://issues.redhat.com/browse/OCPNAS-77[OCPNAS-77])

//OCPNAS-81 Trying to create a file system with the same name as an existing one - getting an error and UI doesn't allow to change/lock
* When you create a new file system in {FusionSAN} with the same name as an existing file system, an error appears, and the *Create file system* button is stuck displaying a loading spinner. If you reload the page, it lists only the original file system. However, if you try to create another new file system, the LUNs you selected for the second file system no longer appear as available. (*OCPNAS-81*)
//(link:https://issues.redhat.com/browse/OCPNAS-81[OCPNAS-81])

//OCPNAS-110 Improve short output for filesystem resource
* If a {FusionSAN} file system is filled to its maximum capacity, the `mmhealth state` of the file system custom resource (CR) becomes `Degraded`. This is caused by the `no_disk_space_warn` event. After freeing disk space, you can once again use the file system, but the file system keeps the `Degraded` status. (*OCPNAS-110*)
//(link:https://issues.redhat.com/browse/OCPNAS-110[OCPNAS-110])

//OCPNAS-124 Deleting the localdisk when using multipath does not remove the partition
* When using a multipath LUN in {FusionSAN}, removing a local disk does not remove the partition. (*OCPNAS-124*)
//(link:https://issues.redhat.com/browse/OCPNAS-124[OCPNAS-124])
** As a workaround, run the following commands on one of the nodes:
+
[source,terminal]
----
$ multipath -f <device>
----
+
[source,terminal]
----
$ multipath -r
----
+
Running these commands on one of the nodes fixes all of the nodes.

//OCPNAS-126 0.0.15 UI - Used LUNs should not be available even if file system is in "Creating" state
* LUNs used to create a file system in {FusionSAN} still appear as available for use until the file system moves from the `Creating` state to the `Healthy` state. This can result in users creating an additional file system with LUNs that that are already in use. After the first file system shifts to the `Healthy` state, the LUNs disappear from the second file system. (*OCPNAS-126*)
//(link:https://issues.redhat.com/browse/OCPNAS-126[OCPNAS-126])

//OCPNAS-143 Shares with existing partitions are automatically formatted
* {FusionSAN} formats disks with existing partitions that are not {FusionSAN} related. When attempting to add a new iSCSI target with an existing partition and data, {FusionSAN} automatically formats the share without warning. (*OCPNAS-143*)
//(link:https://issues.redhat.com/browse/OCPNAS-143[OCPNAS-143])

//OCPNAS-163 UI - Deleting second file system crashes the UI
* Deleting a second file system in {FusionSAN} results in the following error:
+
[source,text]
----
Your focus-trap must have at least one container with at least one tabbable node in it at all times.
----
+
(*OCPNAS-163*)
//(link:https://issues.redhat.com/browse/OCPNAS-163[OCPNAS-163])
** As a workaround, reload the page and delete the second file system.

//OCPNAS-170 fusion access operator needs to watch the builder-dockercfg-* account and reconcile the kmm-registry-push-pull-secret secret
* If your credentials for the image registry used to install {FusionSAN} change, you must delete the `kmm-registry-push-pull-secret` pull secret in the `ibm-fusion-access` namespace.Then you must restart the `fusion-access-operator-controller-manager` pod in the `ibm-fusion-access` namespace. (*OCPNAS-170*)
//(link:https://issues.redhat.com/browse/OCPNAS-170[OCPNAS-170])

//OCPNAS-172 kmm-worker-worker-0-2-gpfs-module pod fails with 'Fatal error"
* If you change the KMM settings that trigger a rebuild while the {FusionSAN} storage cluster is running and using the kernel modules, KMM cannot unload the modules, resulting in an error. (*OCPNAS-172*)
//(link:https://issues.redhat.com/browse/OCPNAS-172[OCPNAS-172])

//OCPNAS-175 OADP backup - pvc in 'Pending' status for a few minutes.
* When backing up VMs with OADP datamover on a {FusionSAN} storage cluster, the process remains in the `Pending` state for a long time before shifting to the `Bound` state and beginning the backup. The process might even remain in `Pending` until it times out completely. (*OCPNAS-175*)
//(link:https://issues.redhat.com/browse/OCPNAS-175[OCPNAS-175])

//OCPNAS-184 Creating a filesystem may take a long time - appears as stuck
* When creating a file system, it may take over twenty minutes for the *Status* of the new file system to change from *Creating* to *Healthy*. During that time, the *Status* appears stuck in *Creating*, and the following error message appears when you click on the status:
+
[source,text]
----
Failed to create filesystem. Check the operator log for more details.
----
+
This error is not correct.
+
(*OCPNAS-184*)
//(link:https://issues.redhat.com/browse/OCPNAS-184[OCPNAS-184])
