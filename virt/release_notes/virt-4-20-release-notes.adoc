:_mod-docs-content-type: ASSEMBLY
include::_attributes/common-attributes.adoc[]
[id="virt-4-20-release-notes"]
= {VirtProductName} release notes
:context: virt-4-20-release-notes

toc::[]

[id="virt-doc-feedback_{context}"]
== Providing documentation feedback

To report an error or to improve our documentation, log in to your link:https://issues.redhat.com[Red Hat Jira account] and submit a link:https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12323181&issuetype=1&components=12333768&priority=10200&summary=%5BDoc%5D&customfield_12316142[Jira issue].

[id="virt-about-virt_{context}"]
== About Red Hat {VirtProductName}

With Red Hat {VirtProductName}, you can bring traditional virtual machines (VMs) into {product-title} and run them alongside containers. In {VirtProductName}, VMs are native Kubernetes objects that you can manage by using the {product-title} web console or the command line.

{VirtProductName} is represented by the image:virt-icon.png[{VirtProductName},40,40] icon.

You can use {VirtProductName} the xref:../../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[OVN-Kubernetes] Container Network Interface (CNI) network provider.

Learn more about xref:../../virt/about_virt/about-virt.adoc#about-virt[what you can do with {VirtProductName}].

Learn more about xref:../../virt/about_virt/virt-architecture.adoc#virt-architecture[{VirtProductName} architecture and deployments].

xref:../../virt/install/preparing-cluster-for-virt.adoc#preparing-cluster-for-virt[Prepare your cluster] for {VirtProductName}.

include::modules/virt-supported-cluster-version.adoc[leveloffset=+2]


[id="virt-guest-os_{context}"]
=== Supported guest operating systems

To view the supported guest operating systems for {VirtProductName}, see link:https://access.redhat.com/articles/973163#ocpvirt[Certified Guest Operating Systems in Red Hat OpenStack Platform, Red Hat Virtualization, OpenShift Virtualization and Red Hat Enterprise Linux with KVM].

//Ensure platform passes Windows Server Virtualization Validation Program. Otherwise, comment out the section below.
[id="virt-svvp-certification_{context}"]
=== Microsoft Windows SVVP certification

//CNV-47134 SVVP 4.18 Release Note: New
//NOTE: This is a recurring release note. Modify the existing note text below if recommended by QE.
{VirtProductName} is certified in Microsoft's Windows Server Virtualization Validation Program (SVVP) to run Windows Server workloads.

The SVVP certification applies to:

* Red Hat Enterprise Linux CoreOS workers. In the Microsoft SVVP Catalog, they are named __Red{nbsp}Hat {product-title} 4.20__.
* Intel and AMD CPUs.

[id="virt-quick-starts_{context}"]
== Quick starts

Quick start tours are available for several {VirtProductName} features. To view the tours, click the *Help* icon *?* in the menu bar on the header of the {product-title} web console and then select *Quick Starts*. You can filter the available tours by entering the keyword `virtualization` in the *Filter* field.


[id="virt-4-20-new_{context}"]
== New and changed features

This release adds new features and enhancements related to the following components and concepts:

[id="virt-4-20-installation-update_{context}"]
=== Installation and update

//CNV-62403 direct z-stream updates
* You can now directly update {VirtProductName} to a later z-stream (x.y.z) release without applying each intermediate z-stream version. 
+
[NOTE]
====
Ensure that you update to the latest z-stream release of your current minor (x.y) version before updating to the next minor version.
====

//CNV-43322 Release notes:TP to GA OCI
* Installing {VirtProductName} on {oci-first-no-rt} is now generally available. For more information, see link:https://access.redhat.com/articles/7118050[{VirtProductName} and Oracle Cloud Infrastructure known issues and limitations] in the Red{nbsp}Hat Knowledgebase, and link:https://github.com/oracle-quickstart/oci-openshift/blob/main/docs/openshift-virtualization.md[Installing {VirtProductName} on OCI] on GitHub.

//[id="virt-4-20-infrastructure_{context}"]
//=== Infrastructure

[id="virt-4-20-virtualization_{context}"]
=== Virtualization

// CNV-62399
* The descheduler profile `DevKubeVirtRelieveAndMigrate` has been renamed to `KubeVirtRelieveAndMigrate` and is now generally available. The updated profile improves VM eviction stability during live migrations by enabling background evictions and reducing oscillatory behavior. For more information, see xref:../../virt/managing_vms/advanced_vm_management/virt-enabling-descheduler-evictions.adoc#virt-configuring-descheduler-evictions_virt-enabling-descheduler-evictions[Configuring descheduler evictions for virtual machines].

//CNV-61642
* You can now use the `kube_application_aware_resourcequota` and `kube_application_aware_resourcequota_creation_timestamp` metrics to query the current usage and creation times of the Application-Aware Quota (AAQ) Operator resources. For more information, see xref:../../virt/monitoring/virt-prometheus-queries.adoc#virt-promql-AAQ-metrics_context[AAQ Operator metrics].


[id="virt-4-20-networking_{context}"]
=== Networking

//CNV-62387
* You can now hot plug and hot unplug a secondary network interface to a VM without manually triggering live migration. You do not need permission to create and list `VirtualMachineInstanceMigration` objects. For more information, see xref:../../virt/vm_networking/virt-hot-plugging-network-interfaces.adoc#virt-hot-plugging-bridge-network-interface_virt-hot-plugging-network-interfaces[Hot plugging secondary network interfaces].

//CNV-52857 and CNV-56324
* xref:../../virt/vm_networking/virt-setting-interface-link-state.adoc#virt-setting-interface-link-state[Managing the link state of a virtual machine interface] is now generally available. In previous releases this was a Technology Preview feature.

//CNV-59402
* You can now use the Border Gateway Protocol (BGP) to configure dynamic ingress and egress routing for VMs that are connected to primary user-defined networks. Importing routes from provider networks into OVN-Kubernetes eliminates the need to manually configure routes on hosts. With dynamic egress, you can export VM IP addresses to provider networks, making the VMs directly reachable from outside the cluster. For more information, see xref:../../networking/advanced_networking/route_advertisements/about-route-advertisements.adoc#nw-routeadvertisements-about_about-route-advertisements[Advertise cluster network routes with Border Gateway Protocol].

//[id="virt-4-20-storage_{context}"]
//=== Storage

[id="virt-4-20-web_{context}"]
=== Web console
//CNV-62026
* In the {product-title} web console, the *Migrations* tab of the *Virtualization* page now displays a progress bar for each migrating virtual machine.

//CNV-63497
* When performing live migration of a VM, you can now xref:../../virt/live_migration/virt-initiating-live-migration.adoc#virt-initiating-vm-migration-web_virt-initiating-live-migration[specify the particular node for the VM to migrate to.]
//CNV-61645
* The procedure for hot plugging disks now includes an optional step for selecting a bus type. You can select the `virtio-blk` or the `virtio-scsi` bus type. The `virtio-blk` type is the default. For more information, see xref:../../virt/managing_vms/virtual_disks/virt-hot-plugging-virtual-disks.adoc#virt-hot-plugging-virtual-disks[Hot plugging VM disks].

[id="virt-4-20-monitoring_{context}"]
=== Monitoring
//CNV-63662
* Added documentation for the `kubevirt_vmi_vcpu_delay_seconds_total` Prometheus metric. This metric reports the time a virtual CPU (vCPU) was queued by the host scheduler but was not running. The updated documentation helps users better understand vCPU queue delays in OpenShift Virtualization environments.

//CNV-59719
* The following xref:../../virt/monitoring/virt-runbooks.adoc#virt-runbooks[alerts for the {CNVOperatorDisplayName}] are now included in the {VirtProductName} runbooks:
+
--
** `HighNodeCPUFrequency`
** `VirtualMachineStuckInUnhealthyState`
** `VirtualMachineStuckOnNode`
** `PersistentVolumeFillingUp`
** `DeprecatedMachineType`
** `HCOGoldenImageWithNoSupportedArchitecture`
** `HCOGoldenImageWithNoArchitectureAnnotation`
** `HCOMultiArchGoldenImagesDisabled`
--
+
For a complete list of virtualization metrics, see the link:https://github.com/openshift/runbooks/tree/master/alerts/openshift-virtualization-operator[openshift/runbooks] Git repository. 

//CNV-67575
The guest agent ping probe is now generally available (GA). Previously, this feature was provided as a Technology Preview. It has been fully tested and is now supported for production use.

//[id="virt-4-20-documentation_{context}"]
//=== Documentation improvements

[id="virt-4-20-notable-technical-changes_{context}"]
=== Notable technical changes
//CNV-61645
* Before this update, only the `virtio-scsi` bus type could be used for hot plugging disks. With this update, the `virtio-blk` bus type is supported as well.

[id="virt-4-20-deprecated-removed_{context}"]
== Deprecated and removed features

[id="virt-4-20-deprecated_{context}"]
=== Deprecated features
// NOTE: When uncommenting deprecated features list, change the Removed features header level below to ===

Deprecated features are included in the current release and supported. However, they will be removed in a future release and are not recommended for new deployments.

* The `OperatorConditionsUnhealthy` alert is deprecated. You can safely xref:../../observability/monitoring/managing-alerts/managing-alerts-as-an-administrator.adoc#silencing-alerts-adm_managing-alerts-as-an-administrator[silence] it.
* All hot plugged disks are persistent by default. The use of non-persistent hot plugged disks is deprecated. They will not be supported in future releases.

[id="virt-4-20-removed_{context}"]
=== Removed features

Removed features are no longer supported in {VirtProductName}.

//CNV-62385
* With this release, support for the Data Plane Development Kit (DPDK) checkup has been removed. You can no longer run a predefined checkup to verify if your {product-title} cluster node can run a VM with a DPDK workload with zero packet loss.

[id="virt-4-20-technology-preview_{context}"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

//CNV-58005
* {VirtProductName} is now supported on link:https://learn.microsoft.com/en-us/azure/azure-boost/overview[Azure Boost].

//CNV-62357
* Golden image support for heterogeneous clusters is now available.

//CNV-65204 Cross-cluster live migration (one note of two; related note added to Known issues)
* Cross-cluster live migration is now available. By enabling cross-cluster live migration, you can move a virtual machine (VM) workload from one {product-title} cluster to another {product-title} cluster without disruption.
+
This feature currently requires installation of {VirtProductName} version 4.20 or later and a pre-release version of {mtv-first}.

//CNV-52861
* You can now use the Plug a Simple Socket Transport (passt) network binding plugin to connect a VM to a primary user-defined network (UDN). For more information, see xref:../../virt/vm_networking/virt-connecting-vm-to-primary-udn.adoc#attaching-vm-to-primary-udn_virt-connecting-vm-to-primary-udn[Attaching a virtual machine to the primary user-defined network].

[id="virt-4-20-bug-fixes_{context}"]
== Bug fixes

// CNV-61279
* Restoring a snapshot of a VM after a storage migration no longer fails because of unreferenced `dataVolumeTemplate` objects. The snapshot process now refreshes the data volume templates in the controller revision to match the `volumes` list, ensuring consistent data recovery. (link:https://issues.redhat.com/browse/CNV-61279[*CNV-61279*])

// CNV-48348
* The migration controller in the `virt-handler` pod was redesigned to separate source, target, and VM responsibilities, ensure deterministic completion, and use a unified `VirtualMachineInstance` (VMI) cache. (link:https://issues.redhat.com/browse/CNV-48348[*CNV-48348*])

// CNV-36448
* Virtual Trusted Platform Module (vTPM) persistence is now enabled by default in VM templates. BitLocker system checks in Windows VMs no longer pass with non-persistent vTPM devices. (link:https://issues.redhat.com/browse/CNV-36448[*CNV-36448*])

// CNV-61740
* On s390x systems, VMs created from a template with the *Boot from CD* option now boot correctly. CD-ROM devices are attached as SCSI instead of SATA, which is not supported on s390x architecture. (link:https://issues.redhat.com/browse/CNV-61740[*CNV-61740*])



[id="virt-4-20-known-issues_{context}"]
== Known issues

//[id="virt-4-20-ki-monitoring_{context}"]
//=== Monitoring

[id="virt-4-20-ki-networking_{context}"]
=== Networking

//CNV-38746: According to Petr, this will remain relevant as long as 4.12 is available. Anyone upgrading from 4.12 could be affected. Version 4.12 is supported until January 2026
* When you update from {product-title} 4.12 to a newer minor version, VMs that use the `cnv-bridge` Container Network Interface (CNI) fail to live migrate. (link:https://access.redhat.com/solutions/7069807[])
** As a workaround, change the `spec.config.type` field in your `NetworkAttachmentDefinition` manifest from `cnv-bridge` to `bridge` before performing the update.

//CNV-69039
* {SMProductName} 3.1.1 and Istio versions 1.25 and later are incompatible with {VirtProductName} {VirtVersion} because the annotation `traffic.sidecar.istio.io/kubevirtInterfaces` is deprecated. (link:https://issues.redhat.com/browse/OSSM-10883[*OSSM-10883*])
** As a workaround, when installing {SMProductShortName} for integration with {VirtProductName}, select version 3.0.4 and Istio 1.24.4 instead of the default versions that are displayed in the web console.


[id="virt-4-20-ki-nodes_{context}"]
==== Nodes

//CNV-38543: According to Stu, this remains an issue in 4.20
* Uninstalling {VirtProductName} does not remove the `feature.node.kubevirt.io` node labels created by {VirtProductName}. You must remove the labels manually. (link:https://issues.redhat.com/browse/CNV-38543[*CNV-38543*])

[id="virt-4-20-ki-storage_{context}"]
==== Storage

//CNV-65204 Cross-cluster live migration (one note of two; related note added to TP)
* When you create an {virtproductname} migration plan by using the {mtv-first} wizard to enable the cross-cluster live migration Technology Preview in the {product-title} web console, you are presented with a choice: *Cold migration* or *Live migration*. Choose *Live migration*. This step is missing in the cross-cluster live migration Technology Preview documentation.

[id="virt-4-20-ki-virtualization_{context}"]
=== Virtualization

//CNV-61066
* Live migration fails if the VM name exceeds 47 characters. (link:https://issues.redhat.com/browse/CNV-61066[*CNV-61066*])

//CNV-33835: Per German, this was targeted for 4.20 but moved to 4.21 (see also CNV-27131)
* {VirtProductName} links a service account token in use by a pod to that specific pod. {VirtProductName} implements a service account volume by creating a disk image that contains a token. If you migrate a VM, then the service account volume becomes invalid. (link:https://issues.redhat.com/browse/CNV-33835[*CNV-33835*])
** As a workaround, use user accounts rather than service accounts because user account tokens are not bound to a specific pod.

//[id="virt-4-20-ki-webconsole_{context}"]
//=== Web console

[id="virt-4-20-ki-ibm-z_{context}"]
=== {ibm-z-title} and {ibm-linuxone-title}

ifdef::openshift-enterprise[]
:FeatureName: Using {VirtProductName} in a cluster deployed on s390x architecture
include::snippets/technology-preview.adoc[]
:!FeatureName:
endif::[]

// ocpbugs-51113 - ballooning call traces
* When you create a VM by using {op-system-base-full} container disk images for s390x architecture, call traces referencing `virtio_balloon` free page reporting print to the VM console. This is due to a kernel bug. (link:https://issues.redhat.com/browse/OCPBUGS-51113[OCPBUGS-51113])
** As a workaround, disable memory ballooning for the VM by adding the following parameter to the VM YAML configuration: `spec.domain.devices.autoattachMemBalloon: false`.
+
You can also disable free page reporting of memory ballooning for all new VMs. To do so, edit the `HyperConverged` CR and add the parameter `spec.virtualMachineOptions.disableFreePageReporting: true`.

// CNV-56889: Matan indicated this will remain in 4.20 but is targeted for 4.21
* VMs based on s390x architecture can only use the *IPL* boot mode. However, in the {product-title} web console, the *Boot mode* list for s390x VMs incorrectly includes *BIOS*, *UEFI*, and *UEFI (secure)* boot modes. If you select one of these modes for an s390x-based VM, the operation fails. (link:https://issues.redhat.com/browse/CNV-56889[CNV-56889])
