[id="virt-2-4-release-notes"]
= {RN_BookName}
include::modules/virt-document-attributes.adoc[]
:context: virt-release-notes
toc::[]

== About {VirtProductName} {VirtVersion}

include::modules/virt-what-you-can-do-with-virt.adoc[leveloffset=+2]

[id="virt-2-4-new"]
== New and changed features
//Placeholder for new content.

//CNV-4686 - Container-native Virtualization x" in the SVVP catalog listing

//CNV-4687 - Ensure platform passes Windows Server Virtualization Program with RHCOS workers.

//CNV-5371 - Import existing virtual machines from Red Hat Virtualization to OpenShift Virtualization.

[id="virt-2-4-networking-new"]
=== Networking
//Placeholder for new content.

#CNV-5286 - SR-IOV Networking to provide a basic fast datapath option

#CNV-5285 - MAC pool support was reintroduced to OpenShift virtualization. It is disabled by default and can be enabled per namespace.

[id="virt-2-4-storage-new"]
=== Storage
//Placeholder for new content.

//CNV-5274 - Customize accessMode and volumeMode when creating VM disks.

//CNV-5422 - CNV support for Ceph-based OCS 4.4.

//CNV-5423 - CDI can now import, clone, and upload virtual machine disks into namespaces that are restricted by resource quotas. Administrators can configure the resource limits applied to CDI worker Pods.

//CNV-5424 - virtctl can now use a DataVolume when uploading virtual machine disks to the cluster which helps prevent virtual machines from being inadvertently started before an upload has completed.

//CNV-5421 - DataVolumes have been enhanced with conditions that make it easier to understand the state of VM disk import, clone, and upload.

[id="virt-2-4-web-new"]
=== Web console
//Placeholder for new content.

//CNV-5275 - Update template designs - virtual machines and virtual machine templates merged under one menu called Virtualization.

//CNV-5272 - Configure scheduling directly from CNV UI.

//CNV-5273 - Attach a config map / secret / service account to a VM from the UI.

[id="virt-2-4-changes"]
== Notable technical changes
//Placeholder for new content.

<<<<<<< HEAD
//CNV-5465-Disconnected installation
* {VirtProductName} can be installed on disconnected clusters in restricted network environments that do not have Internet connectivity. You can create local mirrors for the {VirtProductName} Operator, and install the Operator from a local catalog image that is accessible to the disconnected cluster. Learn more about xref:../operators/olm-restricted-networks.adoc#olm-understanding-operator-catalog-images_olm-restricted-networks[using Operator Lifecycle Manager on restricted networks].
=======
//CNV-5439 - Auto-renewal of SSL certificates

* TLS certificates for {VirtProductName} are created during installation for these components: KubeVirt, containerized data importer controller (CDI) and MAC pool. All certificates are renewed and rotated automatically at periodic intervals. You are not required to rotate and refresh TLS certificates manually.
>>>>>>> SSL cert info in rel notes

//CNV-5371 - Import existing virtual machines from Red Hat Virtualization to OpenShift Virtualization.

[id="virt-2-4-known-issues"]
== Known issues
//This is work in progress. This content has not been updated.

* KubeMacPool is disabled in {VirtProductName} {VirtVersion}. This means that a secondary
interface of a Pod or virtual machine obtains a randomly generated MAC address rather
than a unique one from a pool. Although rare, randomly assigned MAC addresses can conflict.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1816971[*BZ#1816971*])

* Sometimes, when attempting to edit the subscription channel of the *{VirtProductName} Operator* in the web console, clicking the
*Channel* button of the *Subscription Overview* results in a JavaScript error.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1796410[*BZ#1796410*])

** As a workaround, trigger the upgrade process to {VirtProductName} {VirtVersion}
from the CLI by running the following `oc` patch command:
+
----
$ export TARGET_NAMESPACE=openshift-cnv CNV_CHANNEL=2.3 && oc patch -n "${TARGET_NAMESPACE}" $(oc get subscription -n ${TARGET_NAMESPACE} --no-headers -o name) --type='json' -p='[{"op": "replace", "path": "/spec/channel", "value":"'${CNV_CHANNEL}'"}, {"op": "replace", "path": "/spec/installPlanApproval", "value":"Automatic"}]'
----
+
This command points your subscription to upgrade channel `2.3` and enables automatic updates.

//Need complete info for this paragraph
* If you have {VirtProductName} 2.1.0 deployed, you must first upgrade {VirtProductName}.

* In the virtual machine and virtual machine template wizards, *virtIO* is the default interface when you attach a CD-ROM. However, a *virtIO* CD-ROM does not pass virtual machine validation and cannot be created.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1817394[*BZ#1817394*])

//Don't remove: this BZ is probably true fo all 2.x releases.

** The `masquerade` binding method for virtual machines cannot be used in clusters with RHEL 7 compute nodes.

** As a workaround, select *SATA* as the CD-ROM interface when you create virtual machines and virtual machine templates.

* The Containerized Data Importer (CDI) does not always use the `scratchSpaceStorageClass` setting in the CDIConfig object for importing and uploading operations.
Instead, the CDI uses the default storage class to allocate scratch space.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1828198[*BZ#1828198*])

** As a workaround, ensure you have defined a default storage class for your cluster. The following command can be used to apply the necessary annotation:
+
----
$ oc patch storageclass <STORAGE_CLASS_NAME> -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'
----

* If you renamed the Operator deployment custom resource when you deployed
an earlier version of {VirtProductName}, you cannot upgrade directly to {VirtProductName} {VirtVersion}.
The custom resource must be named `kubevirt-hyperconverged`, which is the default
name. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1822266[*BZ#1822266*])

** As a workaround, you can either:
*** Rename the existing custom resource to `kubevirt-hyperconverged`.
*** Create a new custom resource that is named the default `kubevirt-hyperconverged`.
Then, delete the custom resource that is not named `kubevirt-hyperconverged`.

* The {product-title} 4.4 web console includes *slirp* as an option when you add a NIC to a virtual machine, but *slirp* is not a valid NIC type. Do not select *slirp* when adding a NIC to a virtual machine.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1828744[*BZ#1828744*])

// For 2.3: Add new Known Issues above this line (so that we don't mix the new with the old/possibly no longer irrelevant ones)
// The following 7 bugs are from the 2.2 release notes but from their bugs they still seem to be relevant for 2.3:
* After migration, a virtual machine is assigned a new IP address. However, the
commands `oc get vmi` and `oc describe vmi` still generate output containing the
obsolete IP address. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1686208[*BZ#1686208*])
+
** As a workaround, view the correct IP address by running the following command:
+
----
$ oc get pod -o wide
----
//The content for the following paragraph needs to be completed.
* Some resources are improperly retained when removing {VirtProductName}.

* Users without administrator privileges cannot add a network interface
to a project in an L2 network using the virtual machine wizard.
This issue is caused by missing permissions that allow users to load
network attachment definitions.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1743985[*BZ#1743985*])
+
** As a workaround, provide the user with permissions to load the network attachment
definitions.
+
. Define `ClusterRole` and `ClusterRoleBinding` objects to the YAML configuration
file, using the following examples:
+
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
 name: cni-resources
rules:
- apiGroups: ["k8s.cni.cncf.io"]
 resources: ["*"]
 verbs: ["*"]
----
+
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: <role-binding-name>
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cni-resources
subjects:
- kind: User
  name: <user to grant the role to>
  namespace: <namespace of the user>
----
+
. As a `cluster-admin` user, run the following command to create the `ClusterRole`
and `ClusterRoleBinding` objects you defined:
+
----
$ oc create -f <filename>.yaml
----

* When running `virtctl image-upload` to upload large VM disk images in `qcow2`
format, an end-of-file (EOF) error may be reported after the data is
transmitted, even though the upload is either progressing normally or completed.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1789093[*BZ#1789093*])
+
Run the following command to check the status of an upload on a given PVC:
+
----
$ oc describe pvc <pvc-name> | grep cdi.kubevirt.io/storage.pod.phase
----

* Live migration fails when nodes have different CPU models. Even in cases where
nodes have the same physical CPU model, differences introduced by microcode
updates have the same effect. This is because the default settings trigger
host CPU passthrough behavior, which is incompatible with live migration.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1760028[*BZ#1760028*])
+
** As a workaround, set the default CPU model in the `kubevirt-config` ConfigMap,
as shown in the following example:
+
[NOTE]
====
You must make this change before starting the virtual machines that support
live migration.
====
+
. Open the `kubevirt-config` ConfigMap for editing by running the following command:
+
----
$ oc edit configmap kubevirt-config -n openshift-cnv
----
+
. Edit the ConfigMap:
+
[source,yaml]
----
kind: ConfigMap
metadata:
  name: kubevirt-config
data:
  default-cpu-model: "<cpu-model>" <1>
----
<1> Replace `<cpu-model>` with the actual CPU model value. You can determine this
value by running `oc describe node <node>` for all nodes and looking at the
`cpu-model-<name>` labels. Select the CPU model that is present on all of your
nodes.

* When attempting to create and launch a virtual machine using a Haswell CPU,
the launch of the virtual machine can fail due to incorrectly labeled nodes.
This is a change in behavior from previous versions of {VirtProductName}, where virtual machines could be successfully launched on Haswell hosts.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1781497[*BZ#1781497*])
+
As a workaround, select a different CPU model, if possible.

* If you select a directory that shares space with your operating system, you can potentially exhaust the space on the partition, causing the node to be non-functional. Instead, create a separate partition
and point the hostpath provisioner to that partition so it will not interfere with your operating system.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1793132[*BZ#1793132*])

* The {VirtProductName} upgrade process occasionally fails due to an interruption
from the Operator Lifecycle Manager (OLM). This issue is caused by the limitations
associated with using a declarative API to track the state of {VirtProductName}
Operators. Enabling automatic updates during
//xref:install/installing-virt.adoc#virt-subscribing-to-the-catalog_installing-virt[installation] (This xref was causing the build to fail. Requires troubleshooting).
decreases the risk of encountering this issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1759612[*BZ#1759612*])

* {VirtProductName} cannot reliably identify node drains that are triggered by
running either `oc adm drain` or `kubectl drain`. Do not run these commands on
the nodes of any clusters where {VirtProductName} is deployed. The nodes might not
drain if there are virtual machines running on top of them.
The current solution is to put nodes into maintenance.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1707427[*BZ#1707427*])

// Temporary comment: BZ1819700 PR (#20823) likely to conflict here because of updated version change.
* If you navigate to the *Subscription* tab on the *Operators* -> *Installed Operators*
page and click the current upgrade channel to edit it, there might be no visible results.
If this occurs, there are no visible errors.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1796410[*BZ#1796410*])
+
** As a workaround, trigger the upgrade process to {VirtProductName} {VirtVersion}
from the CLI by running the following `oc` patch command:
+
----
$ export TARGET_NAMESPACE=openshift-cnv CNV_CHANNEL=2.3 && oc patch -n "${TARGET_NAMESPACE}" $(oc get subscription -n ${TARGET_NAMESPACE} --no-headers -o name) --type='json' -p='[{"op": "replace", "path": "/spec/channel", "value":"'${CNV_CHANNEL}'"}, {"op": "replace", "path": "/spec/installPlanApproval", "value":"Automatic"}]'
----
+
This command points your subscription to upgrade channel `2.3` and enables automatic updates.
+
If you have {VirtProductName} 2.1.0 deployed, you must first upgrade {VirtProductName}.
