[id="virt-4-8-release-notes"]
= {RN_BookName}
include::modules/virt-document-attributes.adoc[]
include::modules/common-attributes.adoc[]
:context: virt-release-notes

toc::[]

== About Red Hat {VirtProductName}

Red Hat {VirtProductName} enables you to bring traditional virtual machines (VMs) into {product-title} where they run alongside containers, and are managed as native Kubernetes objects.

{VirtProductName} is represented by the image:Operator_Icon-OpenShift_Virtualization-5.png[{VirtProductName},40,40] logo.

You can use {VirtProductName} with either the xref:../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[OVN-Kubernetes] or the xref:../networking/openshift_sdn/about-openshift-sdn.adoc#about-openshift-sdn[OpenShiftSDN] default Container Network Interface (CNI) network provider.

Learn more about xref:../virt/about-virt.adoc#about-virt[what you can do with {VirtProductName}].

include::modules/virt-supported-cluster-version.adoc[leveloffset=+2]

[id="virt-guest-os"]
//CNV-8167  Supported guest operating systems
=== Supported guest operating systems
{VirtProductName} guests can use the following operating systems:

* Red Hat Enterprise Linux 6, 7, and 8.
* Microsoft Windows Server 2012 R2, 2016, and 2019.
* Microsoft Windows 10.

Other operating system templates shipped with {VirtProductName} are not supported.

[id="virt-4-8-inclusive-language"]
== Making open source more inclusive

Red Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases. For more details, see link:https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language[our CTO Chris Wright's message].

[id="virt-4-8-new"]
== New and changed features
//CNV-8914 Microsoft's Windows Server Virtualization Validation Program (SVVP)
* {VirtProductName} is certified in Microsoftâ€™s Windows Server Virtualization Validation Program (SVVP) to run Windows Server workloads.
+
The SVVP Certification applies to:
+
** Red Hat Enterprise Linux CoreOS workers. In the Microsoft SVVP Catalog, they are named __Red Hat OpenShift Container Platform 4 on RHEL CoreOS__.
** Intel and AMD CPUs.
* The Containerized Data Importer (CDI) now uses the {product-title} xref:../networking/enable-cluster-wide-proxy.adoc#enable-cluster-wide-proxy[cluster-wide proxy configuration].

//CNV-9518 Release Note for calico
* {VirtProductName} now supports third-party Container Network Interface (CNI) plug-ins that are link:https://access.redhat.com/articles/5436171[certified by Red Hat] for use with {product-title}.

//CNV-12323 OpenShift Virtualization is exposing additional metrics
* {VirtProductName} now provides metrics for monitoring how infrastructure resources are consumed in the cluster. You can use the {product-title} monitoring dashboard to xref:../virt/logging_events_monitoring/virt-prometheus-queries.adoc#virt-prometheus-queries[query metrics] for the following resources:
+
** vCPU
** Network
** Storage
** Guest memory swapping

//CNV-12326 Openshift Virtualization is providing an unified API to configure cert rotation on different components.
* {VirtProductName} now provides a unified API to xref:../virt/virtual_machines/vm_networking/virt-configuring-certificate-rotation.adoc#virt-configuring-certificate-rotation[configure certificate rotation].

//CNV-12346 Windows VMs will now only be scheduled to hyper-v capable nodes
* If a Windows virtual machine is created from a template or has predefined Hyper-V capabilities, it can now only be scheduled to Hyper-V capable nodes.

//CNV-12348 Add `--proxy-only` option to `virtctl vnc` for only proxying a VNC connection to allow manual connections
* The `--proxy-only` option for the `virtctl vnc` command allows you to xref:../virt/virt-using-the-cli-tools.adoc#virt-virtctl-commands_virt-using-the-cli-tools[manually connect to a virtual machine instance] through a Virtual Network Client (VNC) connection by using any VNC viewer.

//CNV-8875
* If your {VirtProductName} Operator subscription used any update channel other than *stable*, it is now automatically subscribed to the *stable* channel. The *stable* channel subscribes you to minor version updates and ensures that your {VirtProductName} version is compatible with your {product-title} version.

[id="virt-4-8-quick-starts"]
=== Quick starts

* Quick start tours are available for several {VirtProductName} features. To view the tours, click the *Help* icon *?* in the menu bar on the header of the {VirtProductName} console and then select *Quick Starts*. You can filter the available tours by entering the `virtualization` keyword in the *Filter* field.

//[id="virt-4-8-installation-new"]
//=== Installation

[id="virt-4-8-networking-new"]
=== Networking

//CNV-9055 Kubernetes NMState now supports new IP configuration options.
* You can use the Kubernetes NMSstate Operator to xref:../virt/node_network/virt-updating-node-network-config.adoc#virt-example-nmstate-IP-management_virt-updating-node-network-config[configure and manage IP addresses] on your cluster nodes.

//CNV-11390 OpenShift Virtualization now supports live-migration of VM connected to an SR-IOV network.
* {VirtProductName} now supports xref:../virt/live_migration/virt-live-migration.adoc#virt-live-migration[live migration of virtual machines] that are attached to an SR-IOV network interface if the `sriovLiveMigration` feature gate is enabled in the `HyperConverged` custom resource (CR).

[id="virt-4-8-storage-new"]
=== Storage

* Cloning a data volume into a different namespace is now faster and more efficient when using storage that supports Container Storage Interface (CSI) snapshots. The Containerized Data Importer (CDI) uses CSI snapshots, when they are available, to improve performance when you create a virtual machine from a template.

//CNV-12269 When a VM frees space on a virtual disk the discard requests are passed to the underlying storage device which may free up real storage capacity.
* When the `fstrim` or `blkdiscard` commands are run on a virtual disk, the discard requests are passed to the underlying storage device. If the storage provider supports the Pass Discard feature, the discard requests free up storage capacity.

//CNV-12270 CDI can now automatically choose preferred accessMode and volumeMode settings when importing VM disk images.
* You can now specify data volumes by using the storage API. The storage API, unlike the PVC API, allows the system to optimize `accessModes`, `volumeMode`, and storage capacity when allocating storage.

//CNV-12272 It is now possible to clone virtual machine disks from a Filesystem PVC to a Block PVC and visa versa.
* You can now xref:../virt/virtual_machines/cloning_vms/virt-cloning-vm-disk-into-new-datavolume#virt-cloning-vm-disk-into-new-datavolume[clone virtual machine disks between different data volume modes] if they have the content type `kubevirt`. For example, you can clone a persistent volume (PV) with `volumeMode: Block` to a PV with `volumeMode: Filesystem.`

//CNV-12273 CDI now follows the OpenShift cluster-wide proxy configuration when importing virtual machine disk images.
* You can xref:../virt/virtual_machines/virtual_disks/virt-creating-and-using-boot-sources.adoc#virt-creating-a-custom-disk-image-boot-source-web_virt-creating-and-using-boot-sources[create a custom disk image as a boot source] for any template that has a defined source by running a wizard in the {VirtProductName} console.

//[id="virt-4-8-web-new"]
//=== Web console

[id="virt-4-8-deprecated-removed"]
== Deprecated and removed features

[id="virt-4-8-deprecated"]
=== Deprecated features

Deprecated features are included in the current release and supported. However, they will be removed in a future release and are not recommended for new deployments.

//CNV-11468 Single VM import from RHV/VMware deprecated
* Importing a single virtual machine from Red Hat Virtualization (RHV) or VMware is deprecated in the current release and will be removed in {VirtProductName} 4.9. This feature is replaced by the link:https://access.redhat.com/documentation/en-us/migration_toolkit_for_virtualization[Migration Toolkit for Virtualization].

// [id="virt-4-8-removed"]
// === Removed features

[id="virt-4-8-changes"]
== Notable technical changes
//CNV-9052 OpenShift Virtualization now automatically configures IPv6 IP when running on dual-stack clusters.
* {VirtProductName} now configures IPv6 addresses when running on clusters that have dual-stack networking enabled. You can xref:../virt/virtual_machines/vm_networking/virt-using-the-default-pod-network-with-virt.adoc#virt-creating-a-service-from-a-virtual-machine_virt-using-the-default-pod-network-with-virt[create a service] that uses IPv4, IPv6, or both IP address families, if dual-stack networking is enabled for the underlying {product-title} cluster.

//CNV-9054 MAC address pool is now enabled on all namespaces by default.
* KubeMacPool is now enabled by default when you install {VirtProductName}. You can xref:../virt/virtual_machines/vm_networking/virt-using-mac-address-pool-for-vms.adoc#virt-using-mac-address-pool-for-vms[disable a MAC address pool] for a namespace by adding the `mutatevirtualmachines.kubemacpool.io=ignore` label to the namespace. Re-enable KubeMacPool for the namespace by removing the label.

//CNV-12325 All product configuration options are expressed from the HCO CR
* The `HyperConverged` custom resource (CR) is now the central point of configuration for {VirtProductName}. By editing the `HyperConverged` CR, you can:
** xref:../virt/live_migration/virt-live-migration-limits.adoc#virt-configuring-live-migration-limits_virt-live-migration-limits[Configure live migration limits and timeouts]
** xref:../virt/virtual_machines/importing_vms/virt-importing-vmware-vm.adoc#virt-creating-vddk-image_virt-importing-vmware-vm[Define the location of a VMware Virtual Disk Development Kit (VDDK) image]
** xref:../virt/node_maintenance/virt-managing-node-labeling-obsolete-cpu-models.adoc#virt-configuring-obsolete-cpu-models_virt-managing-node-labeling-obsolete-cpu-models[Configure obsolete CPU models]
** xref:../virt/virtual_machines/virtual_disks/virt-using-container-disks-with-vms.adoc#virt-disabling-tls-for-registry_virt-using-container-disks-with-vms[Disable TLS for container registries]
** xref:../virt/virtual_machines/virtual_disks/virt-preparing-cdi-scratch-space.adoc#virt-defining-storageclass_virt-preparing-cdi-scratch-space[Configure a storage class for scratch space]
** xref:../virt/virtual_machines/virtual_disks/virt-configuring-cdi-for-namespace-resourcequota.adoc#virt-overriding-cpu-and-memory-defaults_virt-configuring-cdi-for-namespace-resourcequota[Configure resource requirements for storage workloads]

[id="virt-4-8-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

* You can now xref:../virt/virtual_machines/virtual_disks/virt-hot-plugging-virtual-disks.adoc#virt-hot-plugging-virtual-disks[hot-plug and hot-unplug virtual disks] when you want to add or remove them from your virtual machine without stopping the virtual machine instance.

[id="virt-4-8-known-issues"]
== Known issues

* If you delete {VirtProductName}-provided templates in version 4.8 or later, the templates are automatically recreated by the {VirtProductName} Operator. However, if you delete {VirtProductName}-provided templates created before version 4.8, those earlier templates are not automatically recreated after deletion. As a result, any edit or update to a virtual machine referencing a deleted earlier template will fail.

* If a cloning operation is initiated before the source is available to be cloned, the operation stalls indefinitely. This is because the clone authorization expires before the cloning operation starts. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1855182[*BZ#1855182*])
+
** As a workaround, delete the `DataVolume` object that is requesting the clone. When the source is available, recreate the `DataVolume` object that you deleted so that the cloning operation can complete successfully.

* If your {product-title} cluster uses OVN-Kubernetes as the default Container Network Interface (CNI) provider, you cannot attach a Linux bridge or bonding to the default interface of a host because of a change in the host network topology of OVN-Kubernetes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1885605[*BZ#1885605*])

** As a workaround, you can use a secondary network interface connected to your host, or switch to the OpenShift SDN default CNI provider.

* Running virtual machines that cannot be live migrated might block an {product-title} cluster upgrade. This includes virtual machines that use hostpath-provisioner storage or SR-IOV network interfaces. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1858777[*BZ#1858777*])

** As a workaround, you can reconfigure the virtual machines so that they can be powered off during a cluster upgrade. In the `spec` section of the virtual machine configuration file:
+
. Remove the `evictionStrategy: LiveMigrate` field. See xref:../virt/live_migration/virt-configuring-vmi-eviction-strategy.adoc#virt-configuring-vmi-eviction-strategy[Configuring virtual machine eviction strategy] for more information on how to configure eviction strategy.
. Set the `runStrategy` field to `Always`.

* Live migration fails when nodes have different CPU models. Even in cases where nodes have the same physical CPU model, differences introduced by microcode updates have the same effect. This is because the default settings trigger host CPU passthrough behavior, which is incompatible with live migration. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1760028[*BZ#1760028*])

** As a workaround, set the default CPU model by running the following command:
+
[NOTE]
====
You must make this change before starting the virtual machines that support live migration.
====
+
[source,terminal]
----
$ oc annotate --overwrite -n openshift-cnv hyperconverged kubevirt-hyperconverged kubevirt.kubevirt.io/jsonpatch='[
  {
      "op": "add",
      "path": "/spec/configuration/cpuModel",
      "value": "<cpu_model>" <1>
  }
]'
----
<1> Replace `<cpu_model>` with the actual CPU model value. You can determine this value by running `oc describe node <node>` for all nodes and looking at the `cpu-model-<name>` labels. Select the CPU model that is present on all of your nodes.

* If you enter the wrong credentials for the RHV Manager while importing a RHV VM, the Manager might lock the admin user account because the `vm-import-operator` tries repeatedly to connect to the RHV API. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1887140[*BZ#1887140*])
+
** To unlock the account, log in to the Manager and enter the following command:
+
[source,terminal]
----
$ ovirt-aaa-jdbc-tool user unlock admin
----

// fix targeted for 4.8.1
* RHV VM import fails if the VM affinity policy is `Migratable` even when live migration is enabled in {VirtProductName}. VM import succeeds if the affinity policy is `Pinned`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1977277[*BZ#1977277*])

// fix targeted for 4.8.1
* Selecting *Create* -> *With Import wizard* on the *Virtualization* page of the {VirtProductName} console displays the following warning message:
+
[source]
----
Could not load VirtualMachines
No model registered for VirtualMachines
----
+
You can ignore this message. It does not affect VM import. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1974812[*BZ#1974812*])

* If you run {VirtProductName} 2.6.5 with {product-title} 4.8, various issues occur. You can avoid these issues by upgrading {VirtProductName} to version 4.8.
+
** In the web console, if you navigate to the *Virtualization* page and select *Create* -> *With YAML* the following error message is displayed:
+
[source,text]
----
The server doesn't have a resource type "kind: VirtualMachine, apiVersion: kubevirt.io/v1"
----
+
*** As a workaround, edit the `VirtualMachine` manifest so the `apiVersion` is `kubevirt.io/v1alpha3`. For example:
+
[source,yaml]
----
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine
metadata:
  annotations:
...
----
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1979114[*BZ#1979114*])
+
** If you use the *Customize* wizard to create a VM, the following error message is displayed:
+
[source,text]
----
Error creating virtual machine
----
+
*** As a workaround, copy the manifest and xref:../virt/virtual_machines/virt-create-vms.adoc#virt-creating-vm_virt-create-vms[create the virtual machine from the CLI].
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1979116[*BZ#1979116*])
+
// fix targeted for 4.8.1
** When connecting to the VNC console by using the {VirtProductName} web console, the VNC console always fails to respond.
+
*** As a workaround, create the virtual machine from the CLI or upgrade to {VirtProductName} 4.8.
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1977037[*BZ#1977037*])
