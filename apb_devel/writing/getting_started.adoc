[[apb-devel-writing-getting-started]]
= Writing APBs: Getting Started
{product-author}
{product-version]
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

[[apb-devel-writing-gs-overview]]
== Overview

In this tutorial, you will walk through the creation of some sample Ansible
Playbook Bundles (APBs). You will create actions for them to allow provision,
deprovision, bind, and unbind. You can find more information about the design of
APBs in the xref:../index.adoc#apb-devel-intro-design[Design] topic. More in-depth
information about writing APBs is available in the
xref:reference.adoc#apb-devel-writing-reference[Reference] topic.

[NOTE]
====
For the remainder of this tutorial, substitute your own information for items
marked in brackets; for example, `<host>:<port>` might need to be replaced with
`172.17.0.1.nip.io:8443`.
====

[[apb-devel-writing-gs-dev-env]]
== Before You Begin

Before getting started creating your own APBs, you must set up your development
environment:

. Ensure you have access to an {product-title} cluster. The cluster
should be running both the service catalog and the OpenShift Ansible broker
(OAB), which is supported starting with {product-title} 3.7.

. Install the APB tools as documented in the
xref:../cli_tooling.adoc#apb-devel-cli[CLI Tooling] topic. To verify, you can
run the `apb help` command and check for a valid response.

[[apb-devel-writing-gs-creating]]
== Creating Your First APB

In this tutorial, you will create an APB for a containerized
link:https://hub.docker.com/r/ansibleplaybookbundle/hello-world/[hello world application].  You will work through a basic APB that will mirror the APB
link:https://github.com/ansibleplaybookbundle/hello-world-apb[*hello-world-apb*].

. Your first task is to initialize the APB using the `apb` CLI tool. This creates
the skeleton for your APB. The command for this is simple:
+
----
$ apb init my-test-apb
----
+
After initialization, you will see the following file structure:
+
[source,bash]
----
my-test-apb/
├── apb.yml
├── Dockerfile
├── playbooks
│   ├── deprovision.yml
│   └── provision.yml
└── roles
    ├── deprovision-my-test-apb
    │   └── tasks
    │       └── main.yml
    └── provision-my-test-apb
        └── tasks
            └── main.yml
----
+
Two files were created at the root directory: an *_apb.yml_* (the APB spec file)
and a *_Dockerfile_*. These are the minimum files required for any APB. For more
information about the APB spec file, see the
xref:reference.adoc#apb-devel-writing-ref-spec[Reference] topic. There is
also an explanation of what you can do in the
xref:reference.adoc#apb-devel-writing-ref-dockerfile[*_Dockerfile_*].
+
.*_apb.yml_*
[source,yaml]
----
version: 1.0
name: my-test-apb
description: This is a sample application generated by apb init
bindable: False
async: optional
metadata:
  displayName: my-test
plans:
  - name: default
    description: This default plan deploys my-test-apb
    free: True
    metadata: {}
    parameters: []
----
+
.*_Dockerfile_*
----
FROM ansibleplaybookbundle/apb-base

LABEL "com.redhat.apb.spec"=\

COPY playbooks /opt/apb/actions
COPY roles /opt/ansible/roles
RUN chmod -R g=u /opt/{ansible,apb}
USER apb
----

. In the *_Dockerfile_*, there are two updates to make:

.. Change the `FROM` directive to use the image from the Red Hat Container Catalog.
The first line should now read:
+
----
FROM openshift3/apb-base
----

.. Update `com.redhat.apb.spec` in the `LABEL` instruction with a base64 encoded
version of *_apb.yml_*. To do this, run `apb prepare`:
+
----
$ cd my-test-apb
$ apb prepare
----
+
This updates the *_Dockerfile_* as follows:
+
.*_Dockerfile_*
----
FROM openshift3/apb-base

LABEL "com.redhat.apb.spec"=\
"dmVyc2lvbjogMS4wCm5hbWU6IG15LXRlc3QtYXBiCmRlc2NyaXB0aW9uOiBUaGlzIGlzIGEgc2Ft\
cGxlIGFwcGxpY2F0aW9uIGdlbmVyYXRlZCBieSBhcGIgaW5pdApiaW5kYWJsZTogRmFsc2UKYXN5\
bmM6IG9wdGlvbmFsCm1ldGFkYXRhOgogIGRpc3BsYXlOYW1lOiBteS10ZXN0CnBsYW5zOgogIC0g\
bmFtZTogZGVmYXVsdAogICAgZGVzY3JpcHRpb246IFRoaXMgZGVmYXVsdCBwbGFuIGRlcGxveXMg\
bXktdGVzdC1hcGIKICAgIGZyZWU6IFRydWUKICAgIG1ldGFkYXRhOiB7fQogICAgcGFyYW1ldGVy\
czogW10="

COPY playbooks /opt/apb/actions
COPY roles /opt/ansible/roles
RUN chmod -R g=u /opt/{ansible,apb}
USER apb
----

. At this point, you have a fully formed APB that you can build. If you skipped
using `apb prepare`, the `apb build` command will still prepare the APB before
building the image:
+
----
$ apb build
----

. You can now push the new APB image to the local OpenShift Container Registry:
+
----
$ apb push -o
----

. Querying the OAB will now show your new APB listed:
+
----
$ apb list
ID                                NAME            DESCRIPTION                                         
< ------------ ID ------------->  dh-my-test-apb  This is a sample application generated by apb init  
----
+
Similarly, visiting the {product-title} web console will now display the new APB
named *my-test-apb* in the service catalog under the *All* and *Other* tabs.

[[apb-devel-writing-gs-adding-actions]]
== Adding Actions

The brand new APB created in the last section does not do much in its current state. For that, you must add some actions. The actions supported are:

- provision
- deprovision
- bind
- unbind
- test

You will add each of these actions in the following sections. But before beginning:

. Ensure that you are logged in to your {product-title} cluster via the `oc` CLI.
This will ensure the `apb` tool can interact with {product-title} and the OAB:
+
----
# oc login <cluster_host>:<port> -u <user_name> -p <password>
----

. Log in to the {product-title} web console and verify
your APB listed in the catalog:
+
.{product-title} Web Console
image::browse-catalog-my-test.png[]

. Create a project named *getting-started* where you will deploy {product-title}
resources. You can create it using the web console or CLI:
+
----
$ oc new-project getting-started
----

////
[TODO]: # (change the example yaml so that service/route/dc are all different names to explicitly show the relationships specified by selector, etc)
////

[[apb-devel-writing-gs-provision]]
==== Provision

During the `apb init` process, two parts of the provision task were stubbed out. The playbook, *_playbooks/provision.yml_*, and the associated role in *_roles/provision-my-test-apb_*:

[source,bash]
----
my-test-apb
├── apb.yml
├── Dockerfile
├── playbooks
│   └── provision.yml <1>
└── roles
    └── provision-my-test-apb
        └── tasks
            └── main.yml <2>
----
<1> Inspect this playbook.
<2> Edit this role.

The *_playbooks/provision.yml_* file is the Ansible playbook that will be run
when the provision action is called from the OAB. You can change the playbook,
but for now you can just leave the code as is.

.*_playbooks/provision.yml_*
[source,yaml]
----
- name: my-test-apb playbook to provision the application
  hosts: localhost
  gather_facts: false
  connection: local
  roles:
  - role: ansible.kubernetes-modules
    install_python_requirements: no
  - role: ansibleplaybookbundle.asb-modules
  - role: provision-my-test-apb
    playbook_debug: false
----

The playbook will execute on `localhost` and execute the role
*provision-my-test-apb*. This playbook works on its local container created by
the service broker. The *ansible.kubernetes-modules* role allow you to use the
link:https://github.com/ansible/ansible-kubernetes-modules[*kubernetes-modules*]
to create your {product-title} resources. The
link:https://github.com/fusor/ansible-asb-modules[*asb-modules*] provide
additional functionality for use with the OAB.

Currently, there are no tasks in the role. The contents of the
*_roles/provision-my-test-apb/tasks/main.yml_* only contains comments showing
common resource creation tasks.  ou can currently execute the provision task,
but since there are no tasks to perform, it would simply launch the APB
container and exit without deploying anything.

You can try this now by clicking on the *my-test* APB and deploying it to the *getting-started* project using the web console:

.Provisioning *my-test*
image::provision-my-test.png[]

When the provision is executing, a new namespace is created with the name
*dh-my-test-apb-prov-<random>*. In development mode, it will persist, but
usually this namespace would be deleted after successful completion. If the APB
fails provisioning, the namespace will persist by default.

By looking at the pod resources, you can see the log for the execution of the
APB. To view the pod's logs:

. Find the namespaces by either using the web console to view all namespaces and
sort by creation date, or using the following command:
+
----
$ oc get ns
NAME                                STATUS    AGE
ansible-service-broker              Active    1h
default                             Active    1h
dh-my-test-apb-prov-<random>        Active    4m
----

. Switch to the project:
+
----
$ oc project dh-my-test-apb-prov-<random>
Now using project "dh-my-test-apb-prov-<random>" on server "<cluster_host>:<port>".
----

. Get the pod name:
+
----
$ oc get pods
NAME             READY     STATUS      RESTARTS   AGE
<apb_pod_name>   0/1       Completed   0          3m
----

. View the logs:
+
----
$ oc logs -f <apb_pod_name>
...
+ ansible-playbook /opt/apb/actions/provision.yml --extra-vars '{"_apb_plan_id":"default","namespace":"getting-started"}'
PLAY [my-test-apb playbook to provision the application] ***********************
TASK [ansible.kubernetes-modules : Install latest openshift client] *************
skipping: [localhost]
TASK [ansibleplaybookbundle.asb-modules : debug] *******************************
skipping: [localhost]
PLAY RECAP *********************************************************************
localhost                  : ok=0    changed=0    unreachable=0    failed=0
----

[[apb-devel-writing-gs-provision-dc]]
===== Creating a Deploying Configuration

At the minimum, your APB should deploy the application pods. You can do this by
specifying a
xref:../../architecture/core_concepts/deployments.adoc#deployments-and-deployment-configurations[deployment configuration]:

. One of the first tasks that is commented out in the
*_provision-my-test-apb/tasks/main.yml_* file is the creation of the deployment
configuration. You can uncomment it or paste the following:
+
[NOTE]
====
Normally, you would replace the `image:` value with your own application image.
====
+
[source,yaml]
----
- name: create deployment config
  openshift_v1_deployment_config:
    name: my-test
    namespace: '{{ namespace }}' <1>
    labels: <2>
      app: my-test
      service: my-test
    replicas: 1 <3>
    selector: <4>
      app: my-test
      service: my-test
    spec_template_metadata_labels:
      app: my-test
      service: my-test
    containers: <5>
    - env:
      image: docker.io/ansibleplaybookbundle/hello-world:latest
      name: my-test
      ports:
      - container_port: 8080
        protocol: TCP
----
<1> Designates which
xref:../../architecture/core_concepts/projects_and_users.adoc#namespaces[namespace]
the deployment configuration should be in.
<2> Used to help organize, group, and select objects.
<3> Specifies that you only want one xref:../../architecture/core_concepts/pods_and_services.adoc#pods[pod].
<4> The `selector` section is a
xref:../../architecture/core_concepts/pods_and_services.adoc#labels[labels]
query over pods.
<5> This `containers` section specifies a
xref:../../architecture/core_concepts/containers_and_images.adoc#containers[container]
with a *hello-world* application running on port 8080 on TCP. The
xref:../../architecture/core_concepts/containers_and_images.adoc#docker-images[image]
is stored at
link:https://hub.docker.com/r/ansibleplaybookbundle/hello-world/[docker.io/ansibleplaybookbundle/hello-world].
+
For more information,
xref:reference.adoc#apb-devel-writing-ref-resources-dc[Writing APBs: Reference] has more detail, and you can see the
link:https://github.com/ansible/ansible-kubernetes-modules/blob/master/library/openshift_v1_deployment_config.py[*ansible-kubernetes-modules* documentation] for a full accounting of all fields.

. Build and push the APB:
+
----
$ apb build
$ apb push -o
----

. Provision the APB using the web console.

. After provisioning, there will be a new running pod and a new deployment
configuration. Verify by checking your {product-title} resources:
+
----
$ oc project getting-started
$ oc get all
NAME         REVISION   DESIRED   CURRENT   TRIGGERED BY
dc/my-test   1          1         1         config

NAME           DESIRED   CURRENT   READY     AGE
rc/my-test-1   1         1         1         35s

NAME                 READY     STATUS    RESTARTS   AGE
po/my-test-1-2pw4t   1/1       Running   0          33s
----
+
You will also be able to see the deployed application in the web console on the
project's *Overview* page.

The only way to use this pod in its current state is to use:

----
$ oc describe pods/<pod_name>
----

to find its IP address and access it directly. If there were multiple pods, they
would be accessed separately. To treat them like a single host, you need to
create a _service_, described in the next section.

[TIP]
====
To clean up before moving on and allow you to provision again, you can delete the
*getting-started* project and recreate it or create a new one.
====

[[apb-devel-writing-gs-provision-svc]]
===== Creating a Service

You will want to use multiple pods, load balance them, and create a
xref:../../architecture/core_concepts/pods_and_services.adoc#services[service]
so that a user can access them as a single host:

. Modify the
*_provision-my-test-apb/tasks/main.yml_* file and add the following:
+
[source,yaml]
----
- name: create my-test service
  k8s_v1_service:
    name: my-test
    namespace: '{{ namespace }}'
    labels:
      app: my-test
      service: my-test
    selector:
      app: my-test
      service: my-test
    ports:
      - name: web
        port: 80
        target_port: 8080
----
+
The `selector` section will allow the *my-test* service to include the correct
pods. The `ports` will take the target port from the pods (8080) and expose them
as a single port for the service (80). Notice the application was running on
8080 but has now been made available on the default HTTP port of 80.
+
The `name` field of the port allows you to specify this port in the future with
other resources. More information is available in the
link:https://github.com/ansible/ansible-kubernetes-modules/blob/master/library/k8s_v1_service.py[*k8s_v1_service* module].

. Build and push the APB:
+
----
$ apb build
$ apb push -o
----

. Provision the APB using the web console.

After provisioning, you will see a new service in the web console or CLI. In
the web console, you can click on the new service under *Networking* in the
application on the *Overview* page or under *Applications -> Services*. The
service's IP address will be shown which you can use to access the load balanced
application.

To view the service information from the command line, you can do the following:

----
$ oc project getting-started
$ oc get services
$ oc describe services/my-test
----

The `describe` command will show the IP address to access the service. However,
using an IP address for users to access your application is not generally what
you want. Instead, you should create a _route_, described in the next section.

[TIP]
====
To clean up before moving on and allow you to provision again, you can delete the
*getting-started* project and recreate it or create a new one.
====

[[apb-devel-writing-gs-provision-route]]
===== Creating a Route

You can expose external access to your application through a reliable named
xref:../../architecture/networking/routes.adoc#architecture-core-concepts-routes[route]:

. Modify the *_provision-my-test-apb/tasks/main.yml_* file and adding the
following:
+
[source,yaml]
----
- name: create my-test route
  openshift_v1_route:
    name: my-test
    namespace: '{{ namespace }}'
    labels:
      app: my-test
      service: my-test
    to_name: my-test
    spec_port_target_port: web
----
+
The `to_name` is the name of the target service. The `spec_port_target_port`
refers to the name of the target service's port. More information is available
in the
link:https://github.com/ansible/ansible-kubernetes-modules/blob/master/library/openshift_v1_route.py[*openshift_v1_route* module].

. Build and push the APB:
+
----
$ apb build
$ apb push -o
----

. Provision the APB using the web console. 

After provisioning, you will see the new route created. On the web console's *Overview* page for the *getting-started* project, you will now see an active and clickable route link listed on the application. Clicking on the route or visiting the URL will bring up the *hello-world* application.

You can also view the route information from the CLI:

----
$ oc project getting-started

$ oc get routes
NAME      HOST/PORT                                   PATH      SERVICES   PORT      TERMINATION   WILDCARD
my-test   my-test-getting-started.172.17.0.1.nip.io             my-test    web                     None

$ oc describe routes/my-test
Name:			my-test
Namespace:		getting-started
...
----

At this point, your *my-test* application is fully functional, load balanced,
scalable, and accessible. You can compare your finished APB to the *hello-world*
APB in the
link:https://github.com/ansibleplaybookbundle/hello-world-apb[*hello-world-apb*]
example repository.

[[apb-devel-writing-gs-deprovision]]
==== Deprovision

For the deprovision task, you must destroy all provisioned resources, usually in
reverse order from how they were created.

To add the deprovision action, you need a *_deprovision.yml_* file under
*_playbooks/_* directory and related tasks in the
*_roles/deprovision-my-test-apb/tasks/main.yml_*. Both these files should already be created for you:

[source,bash]
----
my-test-apb/
├── apb.yml
├── Dockerfile
├── playbooks
│   └── provision.yml <1>
└── roles
    └── deprovision-my-test-apb
        └── tasks
            └── main.yml <2>
----
<1> Inspect this file.
<2> Edit this file.

The content of the *_deprovision.yml_* file looks the same as the provision
task, except it is calling a different role:

.*_playbooks/deprovision.yml_*
[source,yaml]
----
- name: my-test-apb playbook to deprovision the application
  hosts: localhost
  gather_facts: false
  connection: local
  roles:
  - role: ansible.kubernetes-modules
    install_python_requirements: no
  - role: ansibleplaybookbundle.asb-modules
  - role: deprovision-my-test-apb
    playbook_debug: false
----

Edit that role in the file *_roles/deprovision-my-test-apb/tasks/main.yml_*. By
uncommenting the tasks, the resulting file without comments should look like the
following:

[source,yaml]
----
- openshift_v1_route:
    name: my-test
    namespace: '{{ namespace }}'
    state: absent

- k8s_v1_service:
    name: my-test
    namespace: '{{ namespace }}'
    state: absent

- openshift_v1_deployment_config:
    name: my-test
    namespace: '{{ namespace }}'
    state: absent
----

In the *_provision.yml_* file created earlier, you created a deployment
configuration, service, then route. For the deprovision action, you should
delete the resources in reverse order. You can do so by identifying the resource
by `namespace` and `name`, and then marking it as `state: absent`.

To run the deprovision template, click on the menu on the list of *Deployed
Services* and select *Delete*.

[[apb-devel-writing-gs-bind]]
==== Bind

From the previous sections, you learned how to deploy a standalone application.
However, in most cases applications will need to communicate with other
applications, and often with a data source. In the following sections, you will
create a PostgreSQL database that the *hello-world* application deployed from
*my-test-apb* can use.

////
[[apb-devel-writing-gs-bind-async]]
===== Asynchronous Binding (Experimental)

For a look at executing the bind action playbooks using asynchronous bind and bind parameters, look [here](./getting_started_async_bind.md) to try out the experimental feature.  This will be enabled by default when Kubernetes supports [asynchronous bind](https://github.com/kubernetes-incubator/service-catalog/issues/1209)
////

[[apb-devel-writing-gs-bind-prep]]
===== Preparation

For a good starting point, create the necessary files for provision and
deprovisioning PostgreSQL.

[NOTE]
====
A more in-depth example can be found at the
link:https://github.com/ansibleplaybookbundle/rhscl-postgresql-apb[PostgreSQL example APB].
====

. Initialize the APB using the `--bindable` option:
+
----
$ apb init my-pg-apb --bindable
----
+
This creates the normal APB file structure with a few differences:
+
[source,bash]
----
my-pg-apb/
├── apb.yml <1>
├── Dockerfile
├── playbooks
│   ├── bind.yml <2>
│   ├── deprovision.yml
│   ├── provision.yml
│   └── unbind.yml <3>
└── roles
    ├── bind-my-pg-apb
    │   └── tasks
    │       └── main.yml <4>
    ├── deprovision-my-pg-apb
    │   └── tasks
    │       └── main.yml
    ├── provision-my-pg-apb
    │   └── tasks
    │       └── main.yml <5>
    └── unbind-my-pg-apb
        └── tasks
            └── main.yml <6>
----
<1> `bindable` flag set to `true`
<2> New file
<3> New file
<4> New empty file
<5> Encoded binding credentials
<6> New empty file
+
In addition to the normal files, new playbooks *_bind.yml_*, *_unbind.yml_*, and
their associated roles have been stubbed out. The *_bind.yml_* and
*_unbind.yml_* files are both empty and, because you are using the default
binding behavior, will remain empty.

. Edit the *_apb.yml_* file. Notice the setting `bindable: true`. In addition to
those changes, you must add some parameters to the *_apb.yml_* for configuring
PostgreSQL. They will be available fields in the web console when provisioning
your new APB:
+
[source,yaml]
----
version: 1.0
name: my-pg-apb
description: This is a sample application generated by apb init
bindable: True
async: optional
metadata:
  displayName: my-pg
plans:
  - name: default
    description: This default plan deploys my-pg-apb
    free: True
    metadata: {}
    # edit the parameters and add the ones below.
    parameters:
      - name: postgresql_database
        title: PostgreSQL Database Name
        type: string
        default: admin
      - name: postgresql_user
        title: PostgreSQL User
        type: string
        default: admin
      - name: postgresql_password
        title: PostgreSQL Password
        type: string
        default: admin
----
+
The *_playbooks/provision.yml_* will look like the following:
+
[source,yaml]
----
- name: my-pg-apb playbook to provision the application
  hosts: localhost
  gather_facts: false
  connection: local
  roles:
  - role: ansible.kubernetes-modules
    install_python_requirements: no
  - role: ansibleplaybookbundle.asb-modules
  - role: provision-my-pg-apb
    playbook_debug: false
----
+
The *_playbooks/deprovision.yml_* will look like the following:
+
[source,yaml]
----
- name: my-pg-apb playbook to deprovision the application
  hosts: localhost
  gather_facts: false
  connection: local
  roles:
  - role: ansible.kubernetes-modules
    install_python_requirements: no
  - role: deprovision-my-pg-apb
    playbook_debug: false
----

. Edit the *_roles/provision-my-pg-apb/tasks/main.yml_* file. This file mirrors
your *hello-world* application in many respects, but adds a
xref:../../architecture/additional_concepts/storage.adoc#persistent-volume-claims[persistent
volume (PV)] to save data between restarts and various configuration options for
the deployment configuration.
+
In addition, a new task has been added at the the very bottom after the
provision tasks. To save the credentials created during the provision process,
you must encode them for retrieval by the OAB. The new task, using the module
`asb_encode_binding`, will do so for you.
+
You can safely delete everything in that file and replace it with the following:
+
[source,yaml]
----
# New persistent volume claim
- name: create volumes
  k8s_v1_persistent_volume_claim:
    name: my-pg
    namespace: '{{ namespace }}'
    state: present
    access_modes:
      - ReadWriteOnce
    resources_requests:
      storage: 1Gi

- name: create deployment config
  openshift_v1_deployment_config:
    name: my-pg
    namespace: '{{ namespace }}'
    labels:
      app: my-pg
      service: my-pg
    replicas: 1
    selector:
      app: my-pg
      service: my-pg
    spec_template_metadata_labels:
      app: my-pg
      service: my-pg
    containers:
    - env:
      - name: POSTGRESQL_PASSWORD
        value: '{{ postgresql_password }}'
      - name: POSTGRESQL_USER
        value: '{{ postgresql_user }}'
      - name: POSTGRESQL_DATABASE
        value: '{{ postgresql_database }}'
      image: docker.io/centos/postgresql-94-centos7
      name: my-pg
      ports:
      - container_port: 5432
        protocol: TCP
      termination_message_path: /dev/termination-log
      volume_mounts:
      - mount_path: /var/lib/pgsql/data
        name: my-pg
      working_dir: /
    volumes:
    - name: my-pg
      persistent_volume_claim:
        claim_name: my-pg
      test: false
      triggers:
      - type: ConfigChange

- name: create service
  k8s_v1_service:
    name: my-pg
    namespace: '{{ namespace }}'
    state: present
    labels:
      app: my-pg
      service: my-pg
    selector:
      app: my-pg
      service: my-pg
    ports:
    - name: port-5432
      port: 5432
      protocol: TCP
      target_port: 5432

# New encoding task makes credentials available to future bind operations
- name: encode bind credentials
  asb_encode_binding:
    fields:
      DB_TYPE: postgres
      DB_HOST: my-pg
      DB_PORT: "5432"
      DB_USER: "{{ postgresql_user }}"
      DB_PASSWORD: "{{ postgresql_password }}"
      DB_NAME: "{{ postgresql_database }}"
----
+
The `encode bind credentials` task will make available several fields as
environment variables: `DB_TYPE`, `DB_HOST`, `DB_PORT`, `DB_USER`,
`DB_PASSWORD`, and `DB_NAME`. This is the default behavior when the *_bind.yml_*
file is left empty. Any application (such as *hello-world*) can use these
environment variables to connect to the configured database after performing a
bind operation.

. Edit the *_roles/deprovision-my-pg-apb/tasks/main.yml_* and uncomment the
following lines so that the created resources will be deleted during
deprovisioning:
+
[source,yaml]
----
- k8s_v1_service:
    name: my-pg
    namespace: '{{ namespace }}'
    state: absent

- openshift_v1_deployment_config:
    name: my-pg
    namespace: '{{ namespace }}'
    state: absent
    
- k8s_v1_persistent_volume_claim:
    name: my-pg
    namespace: '{{ namespace }}'
    state: absent
----

. Finally, build and push your APB:
+
----
$ apb build
$ apb push -o
----

At this point, the APB can create a fully functional PostgreSQL database to your
cluster. You can test it out in the next section.

[[apb-devel-writing-gs-bind-executing]]
===== Executing From the UI

To test your application, you can bind a *hello-world* application to the
provisioned PostgreSQL database. You can use the application previously created
in the xref:apb-devel-writing-gs-provision[Provision] section of this tutorial,
or you can use the
link:https://github.com/ansibleplaybookbundle/hello-world-apb[*hello-world-apb*]:

. First, provision *my-test-apb*.

. Then, provision *my-pg-apb* and select the option to *Create a secret*:
+
image::provision-my-pg.png[]
+
image::provision-my-pg-params.png[]
+
image::provision-my-pg-binding.png[]
+
image::provision-my-pg-results.png[]

. Now, if you have not already done so, navigate to the project. You can see both
your *hello-world* application and your PostgreSQL database. If you did not
select to create a binding at provision time, you can also do so here with the
*Create binding* link.

. After you the binding has been created, you must add the secret created by the
binding into the application. First, navigate to the secrets on the *Resources -> Secrets* page:
+
image::my-pg-nav-secrets.png[]
+
image::my-pg-secrets.png[]

. Add the secret as environment variables:
+
image::my-pg-add-secret.png[]
+
image::my-pg-add-secret-app.png[]

. After this addition, you can return to the *Overview* page. The *my-test*
application may still be redeploying from the configuration change. If so, wait
until you can click on the route to view the application:
+
image::my-pg-overview.png[]
+
After clicking the route, you will see the *hello-world* application has
detected and connected to the *my-pg* database:
+
image::my-pg-hello-world.png[]

[[apb-devel-writing-gs-test]]
==== Test

Test actions are intended to check that an APB passes a basic sanity check
before publishing to the service catalog. They are not meant to test a live
service. {product-title} provides the ability to test a live service using
xref:../../dev_guide/application_health.adoc#dev-guide-application-health[liveness
and readiness probes], which you can add when provisioning. 

The actual implementation of your test is left to you as the APB author. The
following sections provide guidance and best practices.

[[apb-devel-writing-gs-test-action]]
===== Writing a Test Action

To create a test action for your APB:

- Include a *_playbooks/test.yml_* file.
- Include defaults for the test in the *_playbooks/vars/_* directory.

[source,bash]
----
my-apb/
├── ...
├── playbooks/
    ├── test.yml  
    └── vars/
        └── test_defaults.yml
----

To orchestrate the testing of an APB, you should use the
link:http://docs.ansible.com/ansible/latest/include_vars_module.html[*include_vars*]
and
link:http://docs.ansible.com/ansible/latest/include_role_module.html[*include_role*]
modules in your *_test.yml_* file:

.*_test.yml_*
[source,yaml]
----
- name: test media wiki abp 
  hosts: localhost
  gather_facts: false
  connection: local

  roles:
  - role: ansible.kubernetes-modules <1>
    install_python_requirements: no

  post_tasks:
  - name: Load default variables for testing <2>
    include_vars: test_defaults.yaml
  - name: create project for namespace
    openshift_v1_project:
      name: '{{ namespace }}'
  - name: Run the provision role. <3>
    include_role:
      name: provision-mediawiki123-apb
  - name: Run the verify role. <4>
    include_role:
      name: verify-mediawiki123-apb
----
<1> Load the Ansible Kubernetes modules.
<2> Include the default values needed for provision from the test role.
<3> Include the provision role to run.
<4> Include the verify role to run. See
xref:apb-devel-writing-gs-test-verify[Writing a Verify Role].

[[apb-devel-writing-gs-test-verify]]
===== Writing a Verify Role

A _verify role_ allows you to determine if the provision has failed or
succeeded. The *verify_<name>* role should be in the *_roles/_* directory. This should be a
normal
link:http://docs.ansible.com/ansible/latest/playbooks_reuse_roles.html[Ansible role].

[source,bash]
----
my-apb/
├── ...
└── roles/
    ├── ...
    └── verify_<name>
        ├── defaults
             └── defaults.yml
        └── tasks  
            └── main.yml
----

An example task in the *_main.yml_* file could look like:

[source,yaml]
----
 - name: url check for media wiki
   uri:
     url: "http://{{ route.route.spec.host }}"
     return_content: yes
   register: webpage
   failed_when: webpage.status != 200
----

[[apb-devel-writing-gs-test-saving]]
===== Saving Test Results

The *asb_save_test_result* module can also be used in the verify role, allowing
the APB to save test results so that the `apb test` command can return them. The
APB pod will stay alive for the tool to retrieve the test results.

For example, adding *asb_save_test_result* usage to the previous *_main.yml_*
example:

[source,yaml]
----
 - name: url check for media wiki
   uri:
     url: "http://{{ route.route.spec.host }}"
     return_content: yes
   register: webpage
   
  - name: Save failure for the web page
    asb_save_test_result:
      fail: true
      msg: "Could not reach route and retrieve a 200 status code. Recieved status - {{ webpage.status }}"
    when: webpage.status != 200
  
  - fail:
      msg: "Could not reach route and retrieve a 200 status code. Recieved status - {{ webpage.status }}"
    when: webpage.status != 200
  
  - name: Save test pass
    asb_save_test_result:
      fail: false
    when: webpage.status == 200
----

[[apb-devel-writing-gs-test-running]]
===== Running a Test Action

After you have defined your test action, you can use the CLI tooling to run the
test:

----
$ apb test
----

The test action will:

- build the image, 
- start up a pod as if it was being run by the service broker, and 
- retrieve the test results if any were saved.

The status of pod after execution has finished will determine the status of the
test. If the pod is in an error state, then something failed and the command
reports that the test was unsuccessful.
