// Module included in the following assemblies:
//
// * microshift_ai/microshift-rhoai.adoc

:_mod-docs-content-type: PROCEDURE
[id="microshift-rhoai-install_{context}"]
= Installing the {rhoai-full} RPM

To use AI models in {microshift-short} deployments, use this procedure to install the {rhoai-full} ({rhoai}) RPM with a new {microshift-short} installation. You can also install the RPM on an existing {microshift-short} instance if you restart the system.

:FeatureName: {rhoai-full}
include::snippets/technology-preview.adoc[leveloffset=+1]

//[IMPORTANT]
//====
//Installing the `microshift-ai-model-serving` RPM before running the `$ systemctl start microshift` command for the first time can cause {microshift-short} to fail to start. However, {microshift-short} automatically restarts successfully in this case.
//====

.Prerequisites

* The system requirements for installing {microshift-short} have been met.
* You have root user access to your machine.
* The {oc-first} is installed.
* You configured your LVM VG with the capacity needed for the PVs of your workload.
* You have the RAM and disk space required for your AI model.
* You configured the required accelerators, hardware, operating system, and {microshift-short} to provide the resources your model needs.
* Your AI model is ready to use.

[NOTE]
====
The `microshift-ai-model-serving` RPM contains manifests that deploy `kserve`, with the raw deployment mode enabled, and `ServingRuntimes` objects in the `redhat-ods-applications` namespace.
====

.Procedure

. Install the {microshift-short} AI-model-serving RPM package by running the following command:
+
[source,terminal]
----
$ sudo dnf install microshift-ai-model-serving
----

. As a root user, restart the {microshift-short} service by entering the following command:
+
[source,terminal]
----
$ sudo systemctl restart microshift
----

. Optional: Install the release information package by running the following command:
+
[source,terminal]
----
$ sudo dnf install microshift-ai-model-serving-release-info <1>
----
<1> The release information package contains a JSON file with image references useful for offline procedures or deploying copy of a `ServingRuntime` to your namespace during a bootc image build.

.Verification

* Verify that the `kserve` pod is running in the `redhat-ods-applications` namespace by entering the following command:
+
[source,terminal]
----
$ oc get pods -n redhat-ods-applications
----
+
.Example output
+
[source,text]
----
NAME                                        READY   STATUS    RESTARTS   AGE
kserve-controller-manager-7fc9fc688-kttmm   1/1     Running   0          1h
----

.Next steps

* Create a namespace for your AI model.
* Package your model into an OCI image.
* Configure a model-serving runtime.
* Verify that your model is ready for inferencing.
* Make requests against the model server.
