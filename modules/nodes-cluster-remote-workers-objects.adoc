// Module included in the following assemblies:
//
// * logging/nodes-cluster-remote-workers.adoc

[id="nodes-cluster-remote-workers-objects_{context}"]
= About Kubernetes objects and remote worker nodes



If you are using remote worker nodes, you should consider which objects to use to run your applications. 

Daemonsets::
Daemonsets are the best approach to managing pods on the edge because they do not typically need rescheduling behavior. The Openshift API will only get status updates if the Node is in communication with the API. If the Node disconnects from the cluster, then the Daemonset Pod within the API will not change state, and will continue in the last state that was reported. For example, if a daemonset pod is in the running state when a node stops communicating then the pod keeps running and is assumed to be running within the API. 

A workload that wants to target all edge workers would likely want to use a daemonset. A daemonset will run a workload on all the matching workers automatically, and an Openshift/Kubernetes Service endpoint can be used to load balance to those daemonset pods. Daemonsets support label and toleration matching so a subset of nodes can be selected to place workloads.

Daemonsets will not re-run after a reboot of the node if they can’t reach the API server. 

Another limitation is for workloads requiring continuous operation. Daemonsets can’t guarantee updates without disruption. Meaning, the old pod is removed first before the new (updated) pod can start and has a non-zero window of disruption.

Static Pods::
If Pods need to be restarted when a node reboots while network separated from the supervisor cluster, Static Pods can be used. Drawback of static pods  is that they can't use secrets and ConfigMaps. 
See https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/

Other Types of Kubernetes Objects::
Replicasets, Deployments, ReplicationControllers are object types that can be used. Using these types will allow the scheduler to reschedule these pods onto other nodes after the 5 minute disconnect of the node. Some workloads can benefit from this, like REST APIs where an administrator might want to guarantee N number of pods are running and accessible.

Having said that, in the case of the Edge Remote Worker node, it may in fact not be desirable for the workload/pod to be scheduled to a different node in the cluster (because of the nature of the Edge workload doing a specific function at that specific edge location).

Persistent Volumes::
Workloads that require persistent volumes will not be able to migrate to other zones in case of node/network failure.

Tuneables
Only node-status-update-frequency is configurable via a KubeletConfig [docs].
node-monitor-grace-period and pod-eviction-timeout that are part of Controller Manager are not configurable at this time.
<TBD zone config>

