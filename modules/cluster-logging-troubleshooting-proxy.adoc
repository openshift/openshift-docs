// Module included in the following assemblies:
//
// * logging/cluster-logging-troublehsooting.adoc

[id="cluster-logging-troubleshooting-proxy_{context}"]
= Troubleshooting a Kubernetes 503 error when viewing the Kibana console

If you receive a proxy error when viewing the Kibana console, it could be caused
by one of two issues:

* Kibana might not be recognizing pods. If Elasticsearch is slow in starting
up, Kibana may timeout trying to reach it. Check whether the relevant service
has any endpoints:
+
[source,terminal]
----
$ oc describe service kibana
----
+
.Example output
[source,terminal]
----
Name:                   kibana
[...]
Endpoints:              <none>
----
+
If any Kibana pods are live, endpoints are listed. If they are not, check
the state of the Kibana pods and deployment. You might have to scale the
deployment down and back up again.

* The route for accessing the Kibana service is masked. This can happen if you perform a test deployment in one
project, then deploy in a different project without completely removing the
first deployment. When multiple routes are sent to the same destination, the
default router will only route to the first created. Check the problematic route
to see if it is defined in multiple places:
+
[source,terminal]
----
$ oc get route  --all-namespaces --selector logging-infra=support
----
