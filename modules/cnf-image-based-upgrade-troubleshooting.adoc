// Module included in the following assemblies:
// * edge_computing/image-based-upgrade/cnf-image-based-upgrade-base.adoc
// * edge_computing/image-based-upgrade/ztp-image-based-upgrade.adoc

:_mod-docs-content-type: PROCEDURE
[id="cnf-image-based-upgrade-troubleshooting_{context}"]
= Troubleshooting image-based upgrades with {lcao}

Perform troubleshooting steps on the managed clusters that are affected by an issue.

[IMPORTANT]
====
If you are using the `ImageBasedGroupUpgrade` CR to upgrade your clusters, ensure that the `lcm.openshift.io/ibgu-<stage>-completed or `lcm.openshift.io/ibgu-<stage>-failed` cluster labels are updated properly after performing troubleshooting or recovery steps on the managed clusters.
This ensures that the {cgu-operator} continues to manage the image-based upgrade for the cluster.
====

[id="cnf-image-based-upgrade-troubleshooting-must-gather_{context}"]
== Collecting logs

You can use the `oc adm must-gather` CLI to collect information for debugging and troubleshooting.

.Procedure

* Collect data about the Operators by running the following command:
+
[source,terminal]
----
$  oc adm must-gather \
  --dest-dir=must-gather/tmp \
  --image=$(oc -n openshift-lifecycle-agent get deployment.apps/lifecycle-agent-controller-manager -o jsonpath='{.spec.template.spec.containers[?(@.name == "manager")].image}') \
  --image=quay.io/konveyor/oadp-must-gather:latest \// # <1>
  --image=quay.io/openshift/origin-must-gather:latest # <2>
----
<1> Optional: Add this option if you need to gather more information from the {oadp-short} Operator.
<2> Optional: Add this option if you need to gather more information from the SR-IOV Operator.

[id="cnf-image-based-upgrade-troubleshooting-manual-cleanup_{context}"]
== AbortFailed or FinalizeFailed error

Issue::
+
--
During the finalize stage or when you stop the process at the `Prep` stage, {lcao} cleans up the following resources:

* Stateroot that is no longer required
* Precaching resources
* {oadp-short} CRs
* `ImageBasedUpgrade` CR

If the {lcao} fails to perform the above steps, it transitions to the `AbortFailed` or `FinalizeFailed` states.
The condition message and log show which steps failed.

.Example error message
[source,yaml]
----
message: failed to delete all the backup CRs. Perform cleanup manually then add 'lca.openshift.io/manual-cleanup-done' annotation to ibu CR to transition back to Idle
      observedGeneration: 5
      reason: AbortFailed
      status: "False"
      type: Idle
----
--

Resolution::
+
--
. Inspect the logs to determine why the failure occurred.

. To prompt {lcao} to retry the cleanup, add the `lca.openshift.io/manual-cleanup-done` annotation to the `ImageBasedUpgrade` CR.

+
After observing this annotation, {lcao} retries the cleanup and, if it is successful, the `ImageBasedUpgrade` stage transitions to `Idle`.

+
If the cleanup fails again, you can manually clean up the resources.
--

[id="cnf-image-based-upgrade-troubleshooting-stateroot_{context}"]
=== Cleaning up stateroot manually

Issue::

Stopping at the `Prep` stage, {lcao} cleans up the new stateroot. When finalizing after a successful upgrade or a rollback, {lcao} cleans up the old stateroot.
If this step fails, it is recommended that you inspect the logs to determine why the failure occurred.

Resolution::
+
--
. Check if there are any existing deployments in the stateroot by running the following command:
+
[source,terminal]
----
$ ostree admin status
----

. If there are any, clean up the existing deployment by running the following command:
+
[source,terminal]
----
$ ostree admin undeploy <index_of_deployment>
----

. After cleaning up all the deployments of the stateroot, wipe the stateroot directory by running the following commands:

+
[WARNING]
====
Ensure that the booted deployment is not in this stateroot.
====

+
[source,terminal]
----
$ stateroot="<stateroot_to_delete>"
----

+
[source,terminal]
----
$ unshare -m /bin/sh -c "mount -o remount,rw /sysroot && rm -rf /sysroot/ostree/deploy/${stateroot}"
----
--

[id="cnf-image-based-upgrade-troubleshooting-oadp-resources_{context}"]
=== Cleaning up {oadp-short} resources manually

Issue::

Automatic cleanup of {oadp-short} resources can fail due to connection issues between {lcao} and the S3 backend. By restoring the connection and adding the `lca.openshift.io/manual-cleanup-done` annotation, the {lcao} can successfully cleanup backup resources.

Resolution::
--
. Check the backend connectivity by running the following command:
+
[source,terminal]
----
$ oc get backupstoragelocations.velero.io -n openshift-adp
----

+
.Example output
[source,terminal]
----
NAME                          PHASE       LAST VALIDATED   AGE   DEFAULT
dataprotectionapplication-1   Available   33s              8d    true
----

. Remove all backup resources and then add the `lca.openshift.io/manual-cleanup-done` annotation to the `ImageBasedUpgrade` CR.
--

[id="cnf-image-based-upgrade-troubleshooting-lvms_{context}"]
== {lvms} volume contents not restored

When {lvms} is used to provide dynamic persistent volume storage, {lvms} might not restore the persistent volume contents if it is configured incorrectly.

[id="cnf-image-based-upgrade-troubleshooting-lvms-backup_{context}"]
=== Missing {lvms}-related fields in Backup CR

Issue::
Your `Backup` CRs might be missing fields that are needed to restore your persistent volumes.
You can check for events in your application pod to determine if you have this issue by running the following:
+
--
[source,terminal]
----
$ oc describe pod <your_app_name>
----

.Example output showing missing {lvms}-related fields in Backup CR
[source,terminal]
----
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  58s (x2 over 66s)  default-scheduler  0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..
  Normal   Scheduled         56s                default-scheduler  Successfully assigned default/db-1234 to sno1.example.lab
  Warning  FailedMount       24s (x7 over 55s)  kubelet            MountVolume.SetUp failed for volume "pvc-1234" : rpc error: code = Unknown desc = VolumeID is not found
----
--

Resolution::
You must include `logicalvolumes.topolvm.io` in the application `Backup` CR.
Without this resource, the application restores its persistent volume claims and persistent volume manifests correctly, however, the `logicalvolume` associated with this persistent volume is not restored properly after pivot.
+
.Example Backup CR
[source,yaml]
----
apiVersion: velero.io/v1
kind: Backup
metadata:
  labels:
    velero.io/storage-location: default
  name: small-app
  namespace: openshift-adp
spec:
  includedNamespaces:
  - test
  includedNamespaceScopedResources:
  - secrets
  - persistentvolumeclaims
  - deployments
  - statefulsets
  includedClusterScopedResources: <1>
  - persistentVolumes
  - volumesnapshotcontents
  - logicalvolumes.topolvm.io
----
<1> To restore the persistent volumes for your application, you must configure this section as shown.

[id="cnf-image-based-upgrade-troubleshooting-lvms-restore_{context}"]
=== Missing {lvms}-related fields in Restore CR

Issue::
The expected resources for the applications are restored but the persistent volume contents are not preserved after upgrading.

. List the persistent volumes for you applications by running the following command before pivot:
+
--
[source,terminal]
----
$ oc get pv,pvc,logicalvolumes.topolvm.io -A
----

.Example output before pivot
[source,terminal]
----
NAME                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE
persistentvolume/pvc-1234   1Gi        RWO            Retain           Bound    default/pvc-db   lvms-vg1                4h45m

NAMESPACE   NAME                           STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
default     persistentvolumeclaim/pvc-db   Bound    pvc-1234   1Gi        RWO            lvms-vg1       4h45m

NAMESPACE   NAME                                AGE
            logicalvolume.topolvm.io/pvc-1234   4h45m
----
--

. List the persistent volumes for you applications by running the following command after pivot:
+
--
[source,terminal]
----
$ oc get pv,pvc,logicalvolumes.topolvm.io -A
----

.Example output after pivot
[source,terminal]
----
NAME                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE
persistentvolume/pvc-1234   1Gi        RWO            Delete           Bound    default/pvc-db   lvms-vg1                19s

NAMESPACE   NAME                           STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
default     persistentvolumeclaim/pvc-db   Bound    pvc-1234   1Gi        RWO            lvms-vg1       19s

NAMESPACE   NAME                                AGE
            logicalvolume.topolvm.io/pvc-1234   18s
----
--

Resolution::
The reason for this issue is that the `logicalvolume` status is not preserved in the `Restore` CR.
This status is important because it is required for Velero to reference the volumes that must be preserved after pivoting.
You must include the following fields in the application `Restore` CR:
+
.Example Restore CR
[source,yaml]
----
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: sample-vote-app
  namespace: openshift-adp
  labels:
    velero.io/storage-location: default
  annotations:
    lca.openshift.io/apply-wave: "3"
spec:
  backupName:
    sample-vote-app
  restorePVs: true <1>
  restoreStatus: <2>
    includedResources:
      - logicalvolumes
----
<1> To preserve the persistent volumes for your application, you must set `restorePVs` to `true`.
<2> To preserve the persistent volumes for your application, you must configure this section as shown.

[id="cnf-image-based-upgrade-troubleshooting-debugging-oadp-crs_{context}"]
== Debugging failed Backup and Restore CRs

Issue::
The backup or restoration of artifacts failed.

Resolution::
You can debug `Backup` and `Restore` CRs and retrieve logs with the Velero CLI tool.
The Velero CLI tool provides more detailed information than the OpenShift CLI tool.

. Describe the `Backup` CR that contains errors by running the following command:
+
[source,terminal]
----
$ oc exec -n openshift-adp velero-7c87d58c7b-sw6fc -c velero -- ./velero describe backup -n openshift-adp backup-acm-klusterlet --details
----

. Describe the `Restore` CR that contains errors by running the following command:
+
[source,terminal]
----
$ oc exec -n openshift-adp velero-7c87d58c7b-sw6fc -c velero -- ./velero describe restore -n openshift-adp restore-acm-klusterlet --details
----

. Download the backed up resources to a local directory by running the following command:
+
[source,terminal]
----
$ oc exec -n openshift-adp velero-7c87d58c7b-sw6fc -c velero -- ./velero backup download -n openshift-adp backup-acm-klusterlet -o ~/backup-acm-klusterlet.tar.gz
----