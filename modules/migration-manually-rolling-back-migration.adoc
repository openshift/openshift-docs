// Module included in the following assemblies:
// * migration/migrating_3_4/troubleshooting-3-4.adoc
// * migration/migrating_4_1_4/troubleshooting-4-1-4.adoc
// * migration/migrating_4_2_4/troubleshooting-4-2-4.adoc
[id='migration-manually-rolling-back-migration_{context}']
= Manually rolling back a migration

If your application was stopped during a failed migration, you must roll it back manually in order to prevent data corruption in the PV.

This procedure is not required if the application was not stopped during migration, because the original application is still running on the source cluster.

.Procedure

. On the target cluster, switch to the migrated project:
+
[source,terminal]
----
$ oc project <project>
----

. Get the deployed resources:
+
[source,terminal]
----
$ oc get all
----

. Delete the deployed resources to ensure that the application is not running on the target cluster and accessing data on the PVC:
+
[source,terminal]
----
$ oc delete <resource_type>
----

. To stop a daemon set without deleting it, update the `nodeSelector` in the YAML file:
+
[source,yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: hello-daemonset
spec:
  selector:
      matchLabels:
        name: hello-daemonset
  template:
    metadata:
      labels:
        name: hello-daemonset
    spec:
      nodeSelector:
        role: worker <1>
----
<1> Specify a `nodeSelector` value that does not exist on any node.

. Update each PV's reclaim policy so that unnecessary data is removed. During migration, the reclaim policy for bound PVs is `Retain`, to ensure that data is not lost when an application is removed from the source cluster. You can remove these PVs during rollback.
+
[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0001
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain <1>
  ...
status:
  ...
----
<1> Specify `Recycle` or `Delete`.

. On the source cluster, switch to your migrated project:
+
[source,terminal]
----
$ oc project <project_name>
----

. Obtain the project's deployed resources:
+
[source,terminal]
----
$ oc get all
----

. Start one or more replicas of each deployed resource:
+
[source,terminal]
----
$ oc scale --replicas=1 <resource_type>/<resource_name>
----

. Update the `nodeSelector` of the `DaemonSet` resource to its original value, if you changed it during the procedure.
