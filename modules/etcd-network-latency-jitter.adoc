// Module included in the following assemblies:
//
// * etcd/etcd-performance.adoc

:_mod-docs-content-type: PROCEDURE
[id="etcd-network-latency-jitter_{context}"]
= Measuring network jitter between control plane nodes

//lahinson: The following line is in the original KCS article, but no "MTU discovery and validation section" is found, so I commented-out this line.

//Use the tools that are described in the MTU discovery and validation section to obtain the average and maximum network latency.

The value of the heartbeat interval should be around the maximum of the average round-trip time (RTT) between members, normally around 1.5 times the round-trip time. With the {product-title} default heartbeat interval of 100 ms, the recommended RTT between control plane nodes is less than approximately 33 ms with a maximum of less than 66 ms (66 ms multiplied by 1.5 equals 99 ms). For more information, see "Setting tuning parameters for etcd". Any network latency that is higher might cause service-affecting events and cluster instability.

The network latency is influenced by many factors, including but not limited to the following factors:

* The technology of the transport networks, such as copper, fiber, wireless, or satellite
* The number and quality of the network devices in the transport network

A good evaluation reference is the comparison of the network latency in the organization with the commercial latencies that are published by telecommunications providers, such as monthly IP latency statistics.

Consider network latency with network jitter for more accurate calculations. _Network jitter_ is the variance in network latency or, more specifically, the variation in the delay of received packets. On ideal network conditions, the jitter is as close to zero as possible. Network jitter affects the network latency calculations for etcd because the actual network latency over time will be the RTT plus or minus jitter. For example, a network with a maximum latency of 80 ms and jitter of 30 ms will experience latencies of 110 ms, which means etcd is missing heartbeats, causing request timeouts and temporary leader loss. During a leader loss and reelection, the Kubernetes API cannot process any request that causes a service-affecting event and instability of the cluster.

It's important to measure the network jitter among all control plane nodes. To do so, you can use the `iPerf3` tool in UDP mode.

.Prerequisite

* You built your own iPerf image. For more information, see the following Red{nbsp}Hat Knowledgebase articles 

** link:https://access.redhat.com/articles/5233541[Testing Network Bandwidth in OpenShift using iPerf Container]
** link:https://access.redhat.com/solutions/6129701[How to run iPerf network performance test in OpenShift 4]

.Procedure

. Connect to one of the control plane nodes and run the iPerf container as iPerf server in host network mode. When you are running in server mode, the tool accepts TCP and UDP tests. Enter the following command, being careful to replace `<iperf_image>` with your iPerf image:
+
[source,terminal]
----
# podman run -ti --rm --net host <iperf_image> iperf3 -s
----

. Connect to another control plane node and run the iPerf in UDP client mode by entering the following command:
+
[source,terminal]
----
# podman run -ti --rm --net host <iperf_image> iperf3 -u -c <node_iperf_server> -t 300
----
+
The default test runs for 10 seconds, and at the end, the client output shows the average jitter from the client perspective. 

. Run the debug node mode by entering the following command:
+
[source,terminal]
----
# oc debug node/m1
----
+
.Example output
[source,terminal]
----
Starting pod/m1-debug ...
To use host binaries, run `chroot /host`
Pod IP: 198.18.111.13
If you don't see a command prompt, try pressing enter.
----

. Enter the following commands:
+
[source,terminal]
----
sh-4.4# chroot /host
----
+
[source,terminal]
----
sh-4.4# podman run -ti --rm --net host <iperf_image> iperf3 -u -c m0
----
+
.Example output
[source,terminal]
----
Connecting to host m0, port 5201
[  5] local 198.18.111.13 port 60878 connected to 198.18.111.12 port 5201
[ ID] Interval           Transfer     Bitrate         Total Datagrams
[  5]   0.00-1.00   sec   129 KBytes  1.05 Mbits/sec  91
[  5]   1.00-2.00   sec   127 KBytes  1.04 Mbits/sec  90
[  5]   2.00-3.00   sec   129 KBytes  1.05 Mbits/sec  91
[  5]   3.00-4.00   sec   129 KBytes  1.05 Mbits/sec  91
[  5]   4.00-5.00   sec   127 KBytes  1.04 Mbits/sec  90
[  5]   5.00-6.00   sec   129 KBytes  1.05 Mbits/sec  91
[  5]   6.00-7.00   sec   127 KBytes  1.04 Mbits/sec  90
[  5]   7.00-8.00   sec   129 KBytes  1.05 Mbits/sec  91
[  5]   8.00-9.00   sec   127 KBytes  1.04 Mbits/sec  90
[  5]   9.00-10.00  sec   129 KBytes  1.05 Mbits/sec  91
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Jitter    Lost/Total Datagrams
[  5]   0.00-10.00  sec  1.25 MBytes  1.05 Mbits/sec  0.000 ms  0/906 (0%)  sender
[  5]   0.00-10.04  sec  1.25 MBytes  1.05 Mbits/sec  1.074 ms  0/906 (0%)  receiver

iperf Done.
----

. On the iPerf server, the output shows the jitter on every second interval. The average is shown at the end. For the purpose of this test, you want to identify the maximum jitter that is experienced during the test, ignoring the output of the first second as it might contain an invalid measurement. Enter the following command:
+
[source,terminal]
----
# oc debug node/m0
----
+
.Example output
[source,terminal]
----
Starting pod/m0-debug ...
To use host binaries, run `chroot /host`
Pod IP: 198.18.111.12
If you don't see a command prompt, try pressing enter.
----

. Enter the following commands:
+
[source,terminal]
----
sh-4.4# chroot /host
----
+
[source,terminal]
----
sh-4.4# podman run -ti --rm --net host <iperf_image> iperf3 -s
----
+
.Example output
[source,terminal]
----
-----------------------------------------------------------
Server listening on 5201
-----------------------------------------------------------
Accepted connection from 198.18.111.13, port 44136
[  5] local 198.18.111.12 port 5201 connected to 198.18.111.13 port 60878
[ ID] Interval           Transfer     Bitrate         Jitter    Lost/Total Datagrams
[  5]   0.00-1.00   sec   124 KBytes  1.02 Mbits/sec  4.763 ms  0/88 (0%)
[  5]   1.00-2.00   sec   127 KBytes  1.04 Mbits/sec  4.735 ms  0/90 (0%)
[  5]   2.00-3.00   sec   129 KBytes  1.05 Mbits/sec  0.568 ms  0/91 (0%)
[  5]   3.00-4.00   sec   127 KBytes  1.04 Mbits/sec  2.443 ms  0/90 (0%)
[  5]   4.00-5.00   sec   129 KBytes  1.05 Mbits/sec  1.372 ms  0/91 (0%)
[  5]   5.00-6.00   sec   127 KBytes  1.04 Mbits/sec  2.769 ms  0/90 (0%)
[  5]   6.00-7.00   sec   129 KBytes  1.05 Mbits/sec  2.393 ms  0/91 (0%)
[  5]   7.00-8.00   sec   127 KBytes  1.04 Mbits/sec  0.883 ms  0/90 (0%)
[  5]   8.00-9.00   sec   129 KBytes  1.05 Mbits/sec  0.594 ms  0/91 (0%)
[  5]   9.00-10.00  sec   127 KBytes  1.04 Mbits/sec  0.953 ms  0/90 (0%)
[  5]  10.00-10.04  sec  5.66 KBytes  1.30 Mbits/sec  1.074 ms  0/4 (0%)
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Jitter    Lost/Total Datagrams
[  5]   0.00-10.04  sec  1.25 MBytes  1.05 Mbits/sec  1.074 ms  0/906 (0%)  receiver
-----------------------------------------------------------
Server listening on 5201
-----------------------------------------------------------
----

. Add the calculated jitter as a penalty to the network latency. For example, if the network latency is 80 ms and the jitter is 30 ms, consider an effective network latency of 110 ms for the purposes of the control plane. In this example, that value goes above the 100 ms threshold, and the system will miss heartbeats.

. When you calculate the network latency for etcd, use the effective network latency, which is the sum of the following equation:
+
RTT + jitter
+
You might be able to use the average jitter value to calculate the penalty, but the cluster can sporadically miss heartbeats if the etcd heartbeat timer is lower than the sum of the following equation:
+
RTT + max(jitter)
+
Instead, consider using the 99th percentile or max jitter value for a more resilient deployment:
+
Effective Network Latency = RTT + max(jitter)