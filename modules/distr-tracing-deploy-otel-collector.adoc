////
This module included in the following assemblies:
- distr_tracing_install/distr-tracing-deploying.adoc
////

[id="distr-tracing-deploy-otel-collector_{context}"]
= Deploying distributed tracing data collection

The custom resource definition (CRD) defines the configuration used when you deploy an instance of {OTELName}.

.Prerequisites

* The {OTELName} Operator has been installed.
//* You have reviewed the instructions for how to customize the deployment.
* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure

. Log in to the OpenShift web console as a user with the `cluster-admin` role.

. Create a new project, for example `tracing-system`.
+
[NOTE]
====
If you are installing distributed tracing as part of Service Mesh, the {DTShortName} resources must be installed in the same namespace as the `ServiceMeshControlPlane` resource, for example `istio-system`.
====
+
.. Navigate to *Home* -> *Projects*.

.. Click *Create Project*.

.. Enter `tracing-system` in the *Name* field.

.. Click *Create*.

. Navigate to *Operators* -> *Installed Operators*.

. If necessary, select `tracing-system` from the *Project* menu. You might have to wait a few moments for the Operators to be copied to the new project.

. Click the *{OTELName} Operator*. On the *Details* tab, under *Provided APIs*, the Operator provides a single link.

. Under *OpenTelemetryCollector*, click *Create Instance*.

. On the *Create OpenTelemetry Collector* page, to install using the defaults, click *Create* to create the {OTELShortName} instance.

. On the *OpenTelemetryCollectors* page, click the name of the {OTELShortName} instance, for example, `opentelemetrycollector-sample`.

. On the *Details* page, click the *Resources* tab. Wait until the pod has a status of "Running" before continuing.

[id="distr-tracing-deploy-otel-collector-cli_{context}"]
= Deploying {OTELShortName} from the CLI

Follow this procedure to create an instance of {OTELShortName} from the command line.

.Prerequisites

* The {OTELName} Operator has been installed and verified.
+
//* You have reviewed the instructions for how to customize the deployment.
+
* You have access to the OpenShift CLI (`oc`) that matches your {product-title} version.
* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure

. Log in to the {product-title} CLI as a user with the `cluster-admin` role.
+
[source,terminal]
----
$ oc login https://<HOSTNAME>:8443
----

. Create a new project named `tracing-system`.
+
[source,terminal]
----
$ oc new-project tracing-system
----

. Create a custom resource file named `jopentelemetrycollector-sample.yaml` that contains the following text:
+
.Example opentelemetrycollector.yaml
[source,yaml]
----
  apiVersion: opentelemetry.io/v1alpha1
  kind: OpenTelemetryCollector
  metadata:
    name: opentelemetrycollector-sample
    namespace: openshift-operators
  spec:
    image: >-
      registry.redhat.io/rhosdt/opentelemetry-collector-rhel8@sha256:61934ea5793c55900d09893e8f8b1f2dbd2e712faba8e97684e744691b29f25e
    config: |
      receivers:
        jaeger:
          protocols:
            grpc:
      exporters:
        logging:
      service:
        pipelines:
          traces:
            receivers: [jaeger]
            exporters: [logging]
----

. Run the following command to deploy {JaegerShortName}:
+
[source,terminal]
----
$ oc create -n tracing-system -f opentelemetrycollector.yaml
----

. Run the following command to watch the progress of the pods during the installation process:
+
[source,terminal]
----
$ oc get pods -n tracing-system -w
----
+
After the installation process has completed, you should see output similar to the following example:
+
[source,terminal]
----
NAME                                         READY   STATUS    RESTARTS   AGE
opentelemetrycollector-cdff7897b-qhfdx       2/2     Running   0          24s
----
