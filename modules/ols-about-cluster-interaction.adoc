// Module included in the following assemblies:
// * lightspeed-docs-main/configure/ols-configuring-openshift-lightspeed.adoc

:_mod-docs-content-type: CONCEPT
[id="about-cluster-interaction_{context}"]
= About cluster interaction

The {ols-long} Service uses a large language model (LLM) to generate responses to questions. You can enable the cluster interaction feature to enhance the knowledge available to the LLM with information about your {ocp-product-title} cluster. Providing information about the Kubernetes objects that the cluster contains enables the LLM to generate highly specific responses for your environment.

The Model Context Protocol (MCP) is an open protocol that standardizes how applications provide context to an LLM. Using the protocol, an MCP server offers a standardized way for an LLM to increase context by requesting and receiving real-time information from external resources.

When you enable cluster interaction, the {ols-long} Operator installs an MCP server. The MCP server provides the {ols-long} Service with access to the {ocp-short-name} API. Through this access, the Service performs read operations to gather more context for the LLM, enabling the service to answer questions about the Kubernetes objects that reside in your {ocp-short-name} cluster.

[NOTE]
====
The ability of {ols-long} to choose and use a tool effectively is very sensitive to the large language (LLM) model. In general, a larger model with more parameters performs better, and the best performance comes from an extremely large frontier model that represents the latest AI capabilities. When using a small model, you might notice poor performance in tool selection or other aspects of cluster interaction.
====

To activate the cluster interaction feature in the {ols-long} Service, tool calling must be enabled in the LLM provider.

[NOTE]
====
Enabling tool calling can dramatically increase token usage. When you use public model providers, increased token usage can increase billing costs.
====