////
This module included in the following assemblies:
- distr_tracing_otel/distr-tracing-otel-installing.adoc
////

:_content-type: PROCEDURE
[id="distr-tracing-install-otel-operator_{context}"]
= Installing the {OTELShortName} by using the OpenShift web console

You can install the {OTELShortName} by using the *Administrator* view of the OpenShift web console.

.Prerequisites

* You are logged in to the OpenShift web console as a cluster administrator with the `cluster-admin` role. For {product-dedicated}, you must be logged in using an account with the `dedicated-admin` role.

* An active OpenShift CLI (`oc`) session by a cluster administrator with the `cluster-admin` role.
+
[TIP]
====
* Ensure that your OpenShift CLI (`oc`) version is up to date and matches your {product-title} version.

* Run `oc login`:
+
[source,terminal]
----
$ oc login --username=<your_username> 
----
====

.Procedure

. Install the {OTELOperator}:

.. Go to *Operators* -> *OperatorHub* and search for `{OTELOperator}`.

.. Select the *{OTELOperator}* that is *provided by Red Hat* -> *Install* -> *Install* -> *View Operator*.
+
[IMPORTANT]
====
This installs the Operator with the default presets:

* *Update channel* -> *stable*
* *Installation mode* -> *All namespaces on the cluster*
* *Installed Namespace* -> *openshift-operators*
* *Update approval* -> *Automatic*
====

.. In the *Details* tab of the installed Operator page, under *ClusterServiceVersion details*, verify that the installation *Status* is *Succeeded*.

. Create an OpenShift project of your choice for the *OpenTelemetry Collector* instance that you will create in the next step: go to *Home* -> *Projects* -> *Create Project*.

. Create an *OpenTelemetry Collector* instance.

.. Go to *Operators* -> *Installed Operators*.
+
[TIP]
====
You can use the *Project:* dropdown list to select the OpenShift project that you created for the *OpenTelemetry Collector* instance in the previous step.
====

.. Select *OpenTelemetry Collector* -> *Create OpenTelemetryCollector* -> *YAML view*.

.. In the *YAML view*, customize the `OpenTelemetryCollector` custom resource (CR) with the OTLP, Jaeger, Zipkin receiver, and logging exporter.
+
[source,yaml]
----
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel
  namespace: <openshift_project_of_opentelemetry_collector_instance>
spec:
  mode: deployment
  config: |
    receivers:
      otlp:
        protocols:
          grpc:
          http:
      jaeger:
        protocols:
          grpc:
          thrift_binary:
          thrift_compact:
          thrift_http:
      zipkin:
    processors:
      batch:
      memory_limiter:
        check_interval: 1s
        limit_percentage: 50
        spike_limit_percentage: 30
    exporters:
      logging:
    service:
      pipelines:
        traces:
          receivers: [otlp,jaeger,zipkin]
          processors: [memory_limiter,batch]
          exporters: [logging]
----

.. Select *Create*.

.Verification

. Verify that the `status.phase` of  the OpenTelemetry Collector pod is `Running` and the `conditions` are `type: Ready`.
+
[source,console]
----
$ oc get pod -l app.kubernetes.io/managed-by=opentelemetry-operator,app.kubernetes.io/instance=<namespace>.<instance_name> -o yaml
----

. Get the OpenTelemetry Collector service:
+
[source,console]
----
$ oc get service -l app.kubernetes.io/managed-by=opentelemetry-operator,app.kubernetes.io/instance=<namespace>.<instance_name>
----
