// Module included in the following assemblies:
// * lightspeed-docs-main/install/ols-installing-openshift-lightspeed.adoc

:_mod-docs-content-type: CONCEPT
[id="ols-large-language-model-overview_{context}"]

= Large Language Model (LLM) overview

[role="_abstract"]
Learn how the {ols-long} Service uses large language models (LLMs) to provide intelligent, context-aware answers to your questions about {ocp-product-title}.

You can configure {rhelai} or {rhoai} as the LLM provider for the {ols-long} Service. Either LLM provider can use a server or inference service that processes inference queries. 

Alternatively, you can connect the {ols-long} Service to a publicly available LLM provider, such as {watsonx}, {openai}, or {azure-openai}.

[NOTE]
====
Installing the {ols-long} Operator does not install an LLM provider. You must have an LLM provider available for use before you install the {ols-long} Operator.
====

[id="rhelai-with-ols_{context}"]
== {rhelai} with {ols-long}

You can use {rhelai} to host an LLM. 

For more information, see link:https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.5/html/generating_a_custom_llm_using_rhel_ai/index[Generating a custom LLM using RHEL AI].

[id="rhoai-with-ols_{context}"]
== {rhoai} with {ols-long}

You can use {rhoai} to host an LLM. 

For more information, see link:https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/latest/html/configuring_your_model-serving_platform/configuring-your-model-serving-platform_rhoai-admin#about-model-serving_rhoai-admin[About model serving].

[id="watsnx-with-ols_{context}"]
== {watsonx} with {ols-long}

To configure {watsonx} as the LLM provider, you need an IBM Cloud project with access to {watsonx}. You also need your {watsonx} API key.

For more information, see the official {watsonx} link:https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/welcome-main.html?context=wx&audience=wdp[product documentation].

[id="openai-with-ols_{context}"]
== {openai} with {ols-long}

To configure {openai} as the LLM provider with {ols-long}, you need either the {openai} API key or the {openai} project name during the configuration process.

The {openai} Service has a feature for projects and service accounts. You can use a service account in a dedicated project so that you can precisely track {ols-long} usage.

For more information, see the official {openai} link:https://platform.openai.com/docs/overview[product documentation].

[id="azure-openai-with-ols_{context}"]
== {azure-openai} with {ols-long}

To configure {azure-openai} as the LLM provider, you need a {azure-openai} Service instance. You must have at least one model deployment in {azure-studio} for that instance.

For more information, see the official {azure-openai} link:https://learn.microsoft.com/en-us/azure/ai-services/openai/[product documentation].
