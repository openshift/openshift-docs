// Module included in the following assemblies:
//
// * configuring/configuring-log-forwarding.adoc

:_mod-docs-content-type: PROCEDURE
[id="logging-forward-splunk-http-event-collector_{context}"]
= Forwarding logs to Splunk HTTP Event Collector

You can forward logs to the Splunk HTTP Event Collector (HEC).


.Prerequisites
* {clo} has been installed
* You have obtained a Base64 encoded Splunk HEC token.

.Procedure

. Create a secret using your Base64 encoded Splunk HEC token.
+
[source,terminal]
----
$ oc -n openshift-logging create secret generic vector-splunk-secret --from-literal hecToken=<HEC_Token>
----

. Create or edit the `ClusterLogForwarder` Custom Resource (CR) using the template below:
+
[source,yaml]
----
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: <log_forwarder_name>
  namespace: openshift-logging
spec:
  serviceAccount:
    name: <service_account_name> #<1>
  outputs:
    - name: splunk-receiver #<2>
      type: splunk #<3>
      splunk:
        url: '<http://your.splunk.hec.url:8088>' #<5>
        authentication:
          token:
            secretName: splunk-secret
            key: hecToken # <4>
        index: '{.log_type||"undefined"}' #<6>
        source: '{.log_source||"undefined"}' #<7>
        indexedFields: ['.log_type', '.log_source'] #<8>
        payloadKey: '.kubernetes' #<9>
        tuning:
            compression: gzip #<10>
  pipelines:
    - name: my-logs
      inputRefs: #<11>
        - application
        - infrastructure
      outputRefs:
        - splunk-receiver #<12>
----
<1> The name of your service account.
<2> Specify a name for the output.
<3> Specify the output type as `splunk`.
<4> Specify the name of the secret that contains your HEC token.
<5> Specify the URL, including port, of your Splunk HEC.
<6> Specify the name of the index to send events to. If you do not specify an index, the default index of the splunk server configuration is used. This is an optional field.
<7> Specify the source of events to be sent to this sink. You can configure dynamic per-event values. This field is optional. If you do not specify a value, the value of the field will be determined by the `log_type` and `log_source` values. For example, see link:https://docs.redhat.com/en/documentation/red_hat_openshift_logging/6.3/html-single/configuring_logging/index#default-splunk-metadata-key-values_configuring-log-forwarding[Default Splunk metadata key values]. 
<8> Specify the fields to be added to the Splunk index. This field is optional. The values are stored directly in the index alongside the raw event data, allowing for faster search performance on those fields.
However, `indexed_fields` fields increase storage use. Use them only for high-value fields that provide significant search benefits, for example, large datasets with frequent queries on specific fields.
You can use complex and nested fields as indexed fields. These are automatically transformed to meet Splunk's requirements.
<9> Specify the record field to be used as the payload.
By default, the `payloadKey` field is not set, which means the complete log record is forwarded as the payload.
Use the `payloadKey` field carefully. Selecting a single field as the payload may cause other important information in the log to be dropped, potentially leading to inconsistent or incomplete log events.
<10> Specify the compression configuration, which can be either `gzip` or `none`. The default value is `none`. This field is optional.
<11> Specify the input names. 
<12> Specify the name of the output to use when forwarding logs with this pipeline.

[role="_additional-resources"]
.Additional resources
* link:https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector[Set up and use HTTP Event Collector in Splunk Web in Splunk documentation]
