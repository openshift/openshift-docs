// This module is included in the following assemblies:
// * about/op-release-notes.adoc

:_mod-docs-content-type: REFERENCE
[id="op-release-notes-1-15_{context}"]
= Release notes for {pipelines-title} General Availability 1.15

With this update, {pipelines-title} General Availability (GA) 1.15 is available on {OCP} 4.14 and later versions.

[id="new-features-1-15_{context}"]
== New features

In addition to fixes and stability improvements, the following sections highlight what is new in {pipelines-title} 1.15:

[id="pipelines-new-features-1-15_{context}"]
=== Pipelines

* With this update, when incorporating a step from another custom resource (CR) using a `stepRef:` section, you can use parameters in the same way that you use parameters in `taskRef:` and `pipelineRef:` sections.
+
.Example usage
[source,yaml]
----
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: test-task
spec:
  steps:
  - name: fetch-repository
    stepRef:
      resolver: git
      params:
      - name: url
        value: https://github.com/tektoncd/catalog.git
      - name: revision
        value: main
      - name: pathInRepo
        value: stepaction/git-clone/0.1/git-clone
    params:
    - name: url
      value: $(params.repo-url)
    - name: revision
      value: $(params.tag-name)
    - name: output-path
      value: $(workspaces.output.path)
----

* Before this update, when using a resolver to incorporate a task or pipeline from a remote source, if one of the parameters expected an array you had to specify the type of the parameter explicitly. With this update, when using a resolver to incorporate a task or pipeline from a remote source, you do not have to set the type of any parameters.

* With this update, when specifying the use of a workspace in a pipeline run or task run, you can use parameters and other variables in the specification in the `secret`, `configMap`, and `projected.sources` sections.
+
.Example usage
[source,yaml]
----
apiVersion: tekton.dev/v1
kind: Task
metadata:
  generateName: something-
spec:
  params:
  - name: myWorkspaceSecret
  steps:
  - image: registry.redhat.io/ubi/ubi8-minimal:latest
    script: |
      echo “Hello World”
  workspaces:
  - name: myworkspace
    secret:
      secretName: $(params.myWorkspaceSecret)
----

* By default, when {pipelines-shortname} fails to pull the container image that is required for the execution of a task, the task fails. With this release, you can configure an image pull backoff timeout. If you configure this timeout, when {pipelines-shortname} fails to pull the container image that is required for the execution of a task, it continues to attempt to pull the image for the specified time period. The task fails if {pipelines-shortname} is unable to pull the image within the specified period.
+
.Example specification
[source,yaml]
----
apiVersion: operator.tekton.dev/v1alpha1
kind: TektonConfig
metadata:
  name: config
spec:
  pipeline:
    options:
      configMaps:
          config-defaults:
              data:
                   default-imagepullbackoff-timeout: "5m"
----

* With this release, the YAML manifest of a completed pipeline run or task run includes a `displayName` field in the `childReferences` section. This field contains the display name of the pipeline run or task run, which can differ from the full name of the pipeline run or task run.

* With this update, the YAML manifest of every step in a completed `TaskRun` CR includes a new `terminationReason` field. This field contains the reason the step execution ended. {pipelines-shortname} uses the following values for the `terminationReason` field:
** `Completed`: The step completed successfully and any commands invoked in the step ended with exit code 0.
** `Continued`: There was an error during execution of the step, for example a command returned a non-zero exit code, but the step execution continued because the `onError` value was set to `continue`. See the log output for the details of the error.
** `Error`: There was an error during execution of the step, for example a command returned a non-zero exit code, and this error caused the step to fail. See the log output for the details of the error.
** `TimeoutExceeded`: The execution of the step timed out. See the log output for the details of the timeout.
** `Skipped`: The step was skipped because a previous step failed.
** `TaskRunCancelled`: The task run was cancelled.

* With this update, you can use the `pipeline.disable-inline-spec` spec in the `TektonConfig` CR to disable specifying pipelines and tasks inside `PipelineRun` CRs, specifying tasks inside `Pipeline` CRs, or specifying tasks inside `TaskRun` CRs. If you use this option, you must refer to pipelines by using the `pipelineRef:` specification and refer to tasks by using the `taskRef:` specification.

* With this update, some metrics for Prometheus monitoring of {pipelines-shortname} were renamed to ensure compliance with the Prometheus naming convention. Gauge and Counter metric names no longer end with `count`.

[id="Operator-new-features-1-15_{context}"]
=== Operator

* With this update, several tasks are added to the `openshift-pipelines` namespace in the `resolverTasks` add-on. You can incorporate these tasks in your pipelines using the cluster resolver. Most of these tasks were previously available as cluster tasks (`ClusterTask` resources). You can access the following tasks by using the cluster resolver:
** `buildah`
** `git-cli`
** `git-clone`
** `kn`
** `kn-apply`
** `maven`
** `openshift-client`
** `s2i-dotnet`
** `s2i-go`
** `s2i-java`
** `s2i-nodejs`
** `s2i-perl`
** `s2i-php`
** `s2i-python`
** `s2i-ruby`
** `skopeo-copy`
** `tkn`

* With this update, you can set the `pruner.startingDeadlineSeconds` spec in the `TektonConfig` CR. If the pruner job that removes old resources associated with pipeline runs and task runs is not started at the scheduled time for any reason, this setting configures the maximum time, in seconds, in which the job can still be started. If the job is not started within the specified time, {pipelines-shortname} considers this job failed and starts the pruner at the next scheduled time.

* With this update, you can use the `targetNamespaceMetadata` spec in the `TektonConfig` CR to set labels and annotations for the `openshift-pipelines` namespace in which the Operator installs {pipelines-shortname}.

* With this update, error messages for the {pipelines-shortname} Operator include additional context information such as namespace.

[id="triggers-new-features-1-15_{context}"]
=== Triggers

* With this update, you can use the `TriggerTemplate` CR to specify templates for any types of resources. When the trigger is invoked, {pipelines-shortname} creates the resources that you define in the `TriggerTemplate` CR for the trigger. In the following example, a `ConfigMap` resource is created when the trigger is invoked:
+
.Example `TriggerTemplate` CR
[source,yaml]
----
apiVersion: triggers.tekton.dev/v1beta1
kind: TriggerTemplate
metadata:
  name: create-configmap-template
spec:
  params:
    - name: action
  resourcetemplates:
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        generateName: sample-
      data:
        field: "Action is : $(tt.params.action)"
----

* With this update, you can define the `ServiceType` in an `EventListener` CR as `NodePort` and define the port number for the event listener, as shown in the following example:
+
.Example `EventListener` CR defining a port number
[source,yaml]
----
apiVersion: triggers.tekton.dev/v1beta1
kind: EventListener
metadata:
  name: simple-eventlistener
spec:
  serviceAccountName:  simple-tekton-robot
  triggers:
    - name: simple-trigger
      bindings:
      - ref: simple-binding
      template:
        ref: simple-template
  resources:
    kubernetesResource:
      serviceType: NodePort
      servicePort: 38080
----

* With this update, if you use a `serviceType` value of `LoadBalancer` in an `EventListener` CR, you can optionally specify a load balancer class in the `serviceLoadBalancerClass` field. If your cluster provides multiple load balancer controllers, you can use the load balancer class to select one of these controllers. For more information about setting a load balancer class, see the link:https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-class[Kubernetes documentation].
+
.Example specifying a LoadBalancerClass setting
[source,yaml]
----
apiVersion: triggers.tekton.dev/v1beta1
kind: EventListener
metadata:
  name: listener-loadbalancerclass
spec:
  serviceAccountName: tekton-triggers-example-sa
  triggers:
    - name: example-trig
      bindings:
        - ref: pipeline-binding
        - ref: message-binding
      template:
        ref: pipeline-template
  resources:
    kubernetesResource:
      serviceType: LoadBalancer
      serviceLoadBalancerClass: private
----

[id="manual-approval-new-features-1-15_{context}"]
=== Manual approval

With this update, {pipelines-shortname} includes the new Manual Approval Gate functionality.

Manual Approval Gate is a custom resource definition (CRD) controller. You can use this controller to add manual approval points in the pipeline so that the pipeline stops at that point and waits for a manual approval before continuing execution. To use this feature, specify an `ApprovalTask` in the pipeline, in a similar way to specifying a `Task`. Users can provide the approval by using the web console or by using the `opc` command line utility.

The Manual Approval Gate controller includes the following features:

* You must set the following parameters in the `ApprovalTask` specification:
** `approvers`: The users who can approve or reject the `approvalTask` to unblock the pipeline
** `numberOfApprovalsRequired`: The number of approvals required to unblock the pipeline
** `description`: (Optional) Description of the `approvalTask` that {pipelines-shortname} displays to the users
* The manual approval gate supports approval from multiple users:
** The approval requires the configured minimum number of approvals from the configured users. Until this number is reached, the approval task does not finish and its `approvalState` value remains `pending`.
** If any one approver rejects the approval, the `ApprovalTask` controller changes the `approvalState` of the task to `rejected` and the pipeline run fails.
** If one user approves the task but the configured number of approvals is still not reached, the same user can change to rejecting the task and the pipeline run fails.
* Users can provide approval using the `opc approvaltask` CLI and the OpenShift web console. Approval in the OpenShift web console requires installation of the {pipelines-shortname} web console plugin. This plugin requires {OCP} version 4.15 or later.
* Users can add messages while approving or rejecting the `approvalTask`.
* You can add a timeout setting to the `approvalTask` specification. If the required number of approvals is not provided during this time period, the pipeline run fails.

:FeatureName: The manual approval gate
include::snippets/technology-preview.adoc[]


[id="cli-new-features-1-15_{context}"]
=== CLI

* With this update, the `tkn` command line utility supports the `-E` or `--exit-with-pipelinerun-error` option for the `pipeline showlog` command. With this option, the command line utility returns an error code of `0` if the pipeline run completed successfully, `1` if the pipeline run ended with an error, and `2` if the status of the pipeline run is unknown.

* With this update, the `tkn` command line utility supports the `--label` option for the `bundle push` command. With this option, you can provide the value of a label in the `<label-name>=<value>` format; the utility adds the label to the OCI image that it creates. You can use this option several times to provide several labels for the same image.

[id="pac-new-features-1-15_{context}"]
=== {pac}

* With this update, when using {pac}, you can set a `pipelinesascode.tekton.dev/on-comment` annotation on a pipeline run to start the pipeline run when a developer adds a matching comment to a pull request. This setting is supported only for pull requests and only for GitHub and GitLab repository providers.

:FeatureName: Matching a comment event to a pipeline run
include::snippets/technology-preview.adoc[]

* With this update, when using {pac}, you can enter the `/test <pipeline_run_name>` comment on a pull request to start any {pac} pipeline run on the repository, whether or not it was triggered by an event for this pipeline run. This feature is Technological preview only.

* With this update, when providing a `/test` or `/retest` command for {pac} in a Git request comment, you can now set any standard or custom parameters for the pipeline run.
+
.Example commands in a Git request comment
----
/test pipelinerun1 revision=main param1="value1" param2="value \"value2\" with quotes"
----
+
This command runs the `pipelinerun1` pipeline run on the `main` branch instead of the pull request branch.
+
----
/test checker target_branch=backport-branch
----
+
This command runs the `checker` pipeline run on a backport (cherry-pick) of the pull request to the `backport-branch` branch.

* With this update, when using {pac}, you can create a global `Repository` CR with the name `pipelines-as-code` in the namespace in which {pipelines-shortname} is installed, normally `openshift-pipelines`. In this CR, you can set the configuration options that apply to all `Repository` CRs. You can override any of these default options by setting different values in the `Repository` CR for a particular repository.

:FeatureName: The global `Repository` CR
include::snippets/technology-preview.adoc[]

* With this update, {pac} processes both the `OWNERS` file and the `OWNERS_ALIASES` file when determining which users can trigger pipeline runs. However, if the `OWNERS` file includes a `filters` section, {pac} matches approvers and reviewers only against the `.*` filter.

* With this update, when {pac} generates a random secret name for storing the GitHub temporary token, it uses two additional random characters. This change decreases the probability of a collision in secret names.

* With this update, when a pipeline run defined by using {pac} causes a YAML validation error, {pipelines-shortname} reports the error and the pipeline run name in the event log of the user namespace where the pipeline run executes, as well as in the {pipelines-shortname} controller log. The error report is also displayed in the Git repository provider, for example, in the GitHub CheckRun user interface. With this change, a user who does not have access to the controller namespace can access the error messages.

[id="tekton-results-new-features-1-15_{context}"]
=== {tekton-results}

* {tekton-results} uses an `UpdateLog` operation to store logging information in the database. With this update, you can use the TektonResult CR to specify a timeout value for this operation. If the operation does not complete within the specified time period, {tekton-results} ends the operation.
+
.Example specification
[source,yaml]
----
apiVersion: operator.tekton.dev/v1
kind: TektonResult
metadata:
  name: result
spec:
     options:
        deployments:
           tekton-results-watcher:
              spec:
                  template:
                     spec:
                        containers:
                        - name: watcher
                          args:
                          - "--updateLogTimeout=60s"
----

* With this update, when configuring {tekton-results}, you can optionally specify the following database configuration settings in the `options.configMaps.tekton-results-api-config.data.config` section of the `TektonResult` CR:
+
** `DB_MAX_IDLE_CONNECTIONS`: The maximum number of idle connections to the database server that can remain open
** `DB_MAX_OPEN_CONNECTIONS`: The maximum total number of connections to the database server that can remain open
** `GRPC_WORKER_POOL`: The size of the GRPC worker pool
** `K8S_QPS`: The Kubernetes client QPS setting
** `K8S_BURST`: The Kubernetes client burst QPS setting
+
If you want to use this setting, when configuring {tekton-results} you must also use alternate specs for several other configuration parameters, as listed in the following table. Both the regular and the alternate parameter specs are in the `TektonResult` CR.
+
.Alternate configuration parameters for {tekton-results}
[options="header"]
|===

| Regular parameter spec | Alternate parameter spec

| `logs_api` | `options.configMaps.tekton-results-api-config.data.config.LOGS_API`
| `log_level` | `options.configMaps.tekton-results-api-config.data.config.LOG_LEVEL`
| `db_port` | `options.configMaps.tekton-results-api-config.data.config.DB_PORT`
| `db_host` | `options.configMaps.tekton-results-api-config.data.config.DB_HOST`
| `logs_path` | `options.configMaps.tekton-results-api-config.data.config.LOGS_PATH`
| `logs_type` | `options.configMaps.tekton-results-api-config.data.config.LOGS_TYPE`
| `logs_buffer_size` | `options.configMaps.tekton-results-api-config.data.config.LOGS_BUFFER_SIZE`
| `auth_disable` | `options.configMaps.tekton-results-api-config.data.config.AUTH_DISABLE`
| `db_enable_auto_migration` | `options.configMaps.tekton-results-api-config.data.config.DB_ENABLE_AUTO_MIGRATION`
| `server_port` | `options.configMaps.tekton-results-api-config.data.config.SERVER_PORT`
| `prometheus_port` | `options.configMaps.tekton-results-api-config.data.config.PROMETHEUS_PORT`
| `gcs_bucket_name` | `options.configMaps.tekton-results-api-config.data.config.GCS_BUCKET_NAME`

|===
+
For the configuration parameters not listed in this table, use the regular specs as described in the documentation.
+
[IMPORTANT]
====
Use the alternate parameter specs only if you need to use the additional settings in the `options.configMaps.tekton-results-api-config.data.config` section of the `TektonResult` CR.
====

* With this update, you can use the {tekton-results} API to retrieve the Go profiling data for {tekton-results}.

* Before this update, {tekton-results} checked the user authentication when displaying every fragment of log data. With this update, {tekton-results} checks the user authentication only once per log data request. This change improves the response time for the {tekton-results} log API, which is used for displaying logs using the command line utility.

[id="breaking-changes-1-15_{context}"]
== Breaking changes

* Before this update, {pac} set the `git-provider`, `sender`, and `branch` labels in a pipeline run. With this update, {pac} no longer sets these labels, Instead, it sets the `pipelinesascode.tekton.dev/git-provider`, `pipelinesascode.tekton.dev/sender`, and `pipelinesascode.tekton.dev/branch` annotations.

* With this update, you can no longer use the `jaeger` exporter for OpenTelemetry tracing. You can use the `oltptraceexporter` for tracing.

[id="known-issues-1-15_{context}"]
== Known issues

* The new `skopeo-copy` task, which is available from the `openshift-pipelines` namespace by using the cluster resolver, does not work when the `VERBOSE` parameter is set to `false`, which is the default setting. As a workaround, when you use this task, set the `VERBOSE` parameter to `true`. The issue does not apply to the `skopeo-copy` `ClusterTask`.

* The new `skopeo-copy` task, which is available from the `openshift-pipelines` namespace by using the cluster resolver, fails when you use it to push or pull an image to or from an {OCP} internal image repository, such as `image-registry.openshift-image-registry.svc:5000`. As a workaround, set the `DEST_TLS_VERIFY` or `SRC_TLS_VERIFY` parameter to `false`. Alternatively, use an external image repository that has a valid SSL certificate. The issue does not apply to the `skopeo-copy` `ClusterTask`.

* The new `s2i-pass:[*]` tasks, which are available from the `openshift-pipelines` namespace by using the cluster resolver, fail if you clone a Git tepository to a subdirectory of the `source` workspace and then set the `CONTEXT` parameter of the task. As a workaround, when you use these tasks, do not set the `CONTEXT` parameter. The issue does not apply to the `s2i-pass:[*]` `ClusterTasks`.

* The new `git-clone` task, which is available from the `openshift-pipelines` namespace by using the cluster resolver, does not set the `COMMIT` result value. The issue does not apply to the `git-clone` `ClusterTask`.

* The `jib-maven` `ClusterTask` does not work if you are using {OCP} version 4.16.

* When using {pac}, if you set the `concurrency_limit` spec in the global `Repository` CR named `pipelines-as-code` in the `openshift-pipelines` namespace, which provides default settings for all `Repository` CRs, the {pac} watcher crashes. As a workaround, do not set this spec in this CR. Instead, set the `concurrency_limit` spec in the other `Repository` CRs that you create.

* When using {pac}, if you set the `settings.pipelinerun_provenance` spec in the global `Repository` CR named `pipelines-as-code` in the `openshift-pipelines` namespace, which provides default settings for all `Repository` CRs, the {pac} controller crashes. As a workaround, do not set this spec in this CR. Instead, set the `settings.pipelinerun_provenance` spec in the other `Repository` CRs that you create.

[id="fixed-issues-1-15_{context}"]
== Fixed issues

* Before this update, many `info` messages about `ClusterTask` resources being repeatedly reconciled were present in the {pipelines-shortname} Operator log. With this update, the excessive reconciliation no longer happens and the excessive messages do not appear.
+
If the reconciliation messages still appear, remove the earlier version of the `ClusterTask` `installerset` resource. However, if you remove the `installerset` resource, you cannot reference `ClusterTasks` with this specified version in your pipelines.
+
Enter the following command to list `installerset` resources:
+
[source,terminal]
----
$ oc get tektoninstallersets
----
+
The names for versioned `ClusterTask` `installerset` resources are `addon-versioned-clustertasks-<version>-<unique_id>`, for example, `addon-versioned-clustertasks-1.12-fblb8`.
+
Enter the following command to remove an `installerset` resource:
+
[source,terminal]
----
$ oc delete tektoninstallerset <installerset_name>
----

* Before this update, if a task run or pipeline run referenced a service account and this service account referenced a secret that did not exist, the task run or pipeline run failed. With this update, the task run or pipeline run logs a warning and continues.

* Before this update, when you referenced a `StepAction` CR inside a step of a task, {pipelines-shortname} passed all parameters of the step to the `StepAction` CR. With this update, {pipelines-shortname} passes only the parameters defined in the `StepAction` CR to the step action.

* Before this update, if you defined a parameter of a task within a pipeline twice, {pipelines-shortname} logged the wrong path to the definition in the error message. With this update, the error message contains the correct path.

* Before this update, if you specified a task under the `finally:` clause of a pipeline, used an expression in the `when:` clause of this task, and referenced the status of another task in this expression (for example, `'$(tasks.a-task.status)' == 'Succeeded'`), this expression was not evaluated correctly. With this update, the expression is evaluated correctly.

* Before this update, if you specified a negative number of retries when specifying a task run, {pipelines-shortname} did not detect the error. With this update, {pipelines-shortname} detects and reports this error.

* Before this update, when you use a `pipelineRef:` section inside a task of a pipeline to reference another pipeline or when you use a `pipelineSpec:` section inside a task of a pipeline to specify another pipeline, the {pipelines-shortname} controller could crash. With this update, the crash does not happen and the correct error message is logged. Use of `pipelineRef:` and `pipelineSpec:` sections inside a pipeline is not supported.

* Before this update, when you configured a task to use a workspace using the `workspace.<workspace_name>.volume` keyword and then the task failed and was retried, creation of the pod for the task failed. With this update, the pod is created successfully.

* Before this update, {pipelines-shortname} sometimes modified recorded annotations on a completed pipeline run or task run after its completion. For example, the  `pipeline.tekton.dev/release` annotation records the version information of the pipeline, and if the pipeline version was updated after the execution of the pipeline run, this annotation could be changed to reflect the new version instead of the version that was run. With this update, the annotations reflect the status of the pipeline run when it was completed and {pipelines-shortname} does not modify the annotations later.

* Before this update, if a YAML manifest that a pipeline run uses (for example, the manifest of a task or pipeline) had syntax errors, the logged error message was unspecific or no error message was logged. With this update, the logged error message includes the syntax errors.

* Before this update, when you used the `buildah` cluster task with a secret with the `.dockerconfigjson` file provided using a workspace, the task failed during the `cp` command because the `/root/.docker` directory did not exist. With this update, the task completes successfully.

* Before this update, if a pipeline run timed out and a `TaskRun` or `CustomRun` resource that this pipeline run included was deleted, the pipeline run execution was blocked and never completed. With this update, the execution correctly ends, logging a canceled state.

* Before this update, when using a resolver to incorporate a task from a remote source, the resolver automatically added the `kind` value of `Task` to the resulting specification. With this update, the resolver does not add a `kind` value to the specification.

* Before this update, when you set configuration options using an `options:` section in the `TektonConfig` CR, these options were sometimes not applied correctly. With this update, the options are applied correctly.

* Before this update, if you set the `enable-api-fields` field and certain other fields in the `TektonConfig` CR, the settings were lost after any update of {pipelines-shortname}. With this update, the settings are preserved during updates.

* Before this update, if you configured the horizontal pod autoscaler (HPA) using the options section in the `TektonConfig` CR, any existing HPA was updated correctly but a new HPA was not created when required. With this update, HPA configuration using the options section works correctly.

* Before this update, you could erroneously change the `targetNamespace` field in the `TektonConfig` CR, creating an unsupported configuration. With this update, you can no longer change this field. Changing the target namespace name from `openshift-pipelines` is not supported.

* Before this update, if the `pipelines-scc-rolebinding` rolebinding was missing or deleted in any namespace, the {pipelines-shortname} Operator controller failed to create default resources in new namespaces correctly. With this update, the controller functions correctly.

* Before this update, when you specified a `namespaceSelector` value when defining a `triggerGroup` in an `EventListener` CR, the event listener was unable to access triggers in the specified namespace if it was not the same as the namespace of the event listener. With this update, the event listener can access triggers in the specified namespace.

* Before this update, when a request was sent to an `EventListener` route URL with a `Content-Type` header, this header was not passed to the interceptor. With this update, the header is passed to the interceptor.

* With this update, several potential causes for {tekton-results} becoming unresponsive, crashing, or consuming a large amount of memory were removed.

* Before this update, in the *Pipeline details* page of the web console, if a `when` expression using CEL was configured for a task, information was not displayed correctly. With this update, the information is displayed correctly.

* Before this update, in the *Pipeline details* page of the web console, the menu was not visible when you enabled dark mode in the web console. With this update, the menu is visible.

* Before this update, in the *Pipelines* page of the web console, information about running statistics of pipelines did not include the information saved in {tekton-results}. With this update, the page includes all available running statistics information for every pipeline.

* Before this update, when you viewed a list of tasks for a namespace in the web console, a task from another namespace was sometimes displayed in the list. With this update, the web console correctly lists tasks for each namespace.

* Before this update, when you viewed the list of task runs in the web console, the status for each task run was not displayed. With this update, the list of task runs in the web console includes the status for each task run.

* Before this update, if you disabled cluster tasks in your {pipelines-shortname} deployment, the Pipeline Builder in the web console did not work. With this update, if you disable cluster tasks, the Pipeline Builder in the web console works correctly.

* Before this update, the {pipelines-shortname} console plugin pod did not move to the node specified using the `nodeSelector`, `tolerations`, and `priorityClassName` settings. With this update, the {pipelines-shortname} plugin pod moves to the correct node.

* Before this update, the {pac} watcher sometimes crashed when processing a pipeline run for which a concurrency limit was not set. With this update, these pipeline runs are processed correctly.

* Before this update, in {pac}, a concurrency limit setting of `0` was not interpreted as disabling the concurrency limit. With this update, a concurrency limit setting of `0` disables the concurrency limit.

* Before this update, when you defined annotations and labels for a task in {pac}, the annotations and labels were not set on the pod that is running the task. With this update, {pac} correctly sets the configured annotations and labels on the pod that is running the task.

* Before this update, {pac} sometimes caused a load on the Kubernetes service by re-reading an internal configuration `ConfigMap` resource frequently. With this update, {pac} no longer causes this load, because it reloads the `ConfigMap` resource only after the `ConfigMap` resource is modified.

* Before this update, when using {pac}, when you deleted a comment on a pull request such as `/test` or `/retest`, {pac} executed the command in the comment again. With this update, {pac} executes a command only when you add the comment.

* Before this update, when using {pac}, if some pipeline runs for a pull request failed and then re-ran successfully after a `/test` or `/retest` command without pushing a new commit, the user interface of the Git provider, such as GitHub, displayed the previous failure result along with the new result. With this update, the up-to-date status is displayed.

* Before this update, when you used the `tkn pr logs -f` command to view the logs for a running pipeline, the command line utility stopped responding, even if the pipeline run completed successfully. With this update, the `tkn pr logs -f` command properly displays the log information and exits.
