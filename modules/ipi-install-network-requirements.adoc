// Module included in the following assemblies:
//
// * installing/installing_bare_metal_ipi/ipi-install-prerequisites.adoc

[id='network-requirements_{context}']
= Network requirements

Installer-provisioned installation of {product-title} involves several network requirements. First, installer-provisioned installation involves an optional non-routable `provisioning` network for provisioning the operating system on each bare metal node. Second, installer-provisioned installation involves a routable `baremetal` network.

.Configuring NICs

{product-title} deploys with two networks:

- `provisioning`: The `provisioning` network is an optional non-routable network used for provisioning the underlying operating system on each node that is a part of the {product-title} cluster. The network interface for the `provisioning` network on each cluster node must have the BIOS or UEFI configured to PXE boot.
+
In {product-title} 4.3, when deploying using the `provisioning` network, the first NIC on each node, such as `eth0` or `eno1`, must interface with the `provisioning` network.
+
In {product-title} 4.4 and later releases, you can specify the provisioning network NIC with the `provisioningNetworkInterface` configuration setting.

- `baremetal`: The `baremetal` network is a routable network.
+
In {product-title} 4.3, when deploying using the `provisioning` network, the second NIC on each node, such as `eth1` or `eno2`, must interface with the `baremetal` network.
+
In {product-title} 4.4 and later releases, you can use any NIC order to interface with the `baremetal` network, provided it is the same NIC order across worker and control plane nodes and not the NIC specified in the `provisioningNetworkInterface` configuration setting for the `provisioning` network.

[NOTE]
====
Use a compatible approach such that cluster nodes use the same NIC ordering on all cluster nodes. NICs must have heterogeneous hardware with the same NIC naming convention such as `eth0` or `eno1`.
====

[IMPORTANT]
====
When using a VLAN, each NIC must be on a separate VLAN corresponding to the appropriate network.
====

.Configuring the DNS server

Clients access the {product-title} cluster nodes over the `baremetal` network. A network administrator must configure a subdomain or subzone where the canonical name extension is the cluster name.

----
<cluster-name>.<domain-name>
----

For example:

----
test-cluster.example.com
----

ifeval::[{product-version}>4.7]
{product-title} 4.8 and later releases include functionality that uses cluster membership information to generate A/AAAA records. This resolves the node names to their IP addresses. Once the nodes are registered with the API, the cluster can disperse node information without using CoreDNS-mDNS. This eliminates the network traffic associated with multicast DNS.
endif::[]

ifdef::upstream[]
For assistance in configuring the DNS server, check xref:ipi-install-upstream-appendix[Appendix] section for:

- xref:creating-dns-records-on-a-dns-server-option1_{context}[Creating DNS Records with Bind (Option 1)]
- xref:creating-dns-records-using-dnsmasq-option2_{context}[Creating DNS Records with dnsmasq (Option 2)]

endif::[]

.Dynamic Host Configuration Protocol (DHCP) requirements

By default, installer-provisioned installation deploys `ironic-dnsmasq` with DHCP enabled for the `provisioning` network. No other DHCP servers should be running on the `provisioning` network when the `provisioningNetwork` configuration setting is set to `managed`, which is the default value. If you have a DHCP server running on the `provisioning` network, you must set the `provisioningNetwork` configuration setting to `unmanaged` in the `install-config.yaml` file.

Network administrators must reserve IP addresses for each node in the {product-title} cluster for the `baremetal` network on an external DHCP server.

.Reserving IP addresses for nodes with the DHCP server

For the `baremetal` network, a network administrator must reserve a number of IP addresses, including:

ifeval::[{product-version} > 4.5]
. Two virtual IP addresses.
endif::[]
ifeval::[{product-version} <= 4.5]
. Three virtual IP addresses
endif::[]
+
- One IP address for the API endpoint
- One IP address for the wildcard ingress endpoint
ifeval::[{product-version} <= 4.5]
- One IP address for the name server
endif::[]

. One IP address for the provisioner node.
. One IP address for each control plane (master) node.
. One IP address for each worker node, if applicable.

ifeval::[{product-version} > 4.6]
[IMPORTANT]
.Reserving IP addresses so they become static IP addresses
====
Some administrators prefer to use static IP addresses so that each node's IP address remains constant in the absence of a DHCP server. To use static IP addresses in the {product-title} cluster, reserve the IP addresses with an infinite lease. During deployment, the installer will reconfigure the NICs from DHCP assigned addresses to static IP addresses. NICs with DHCP leases that are not infinite will remain configured to use DHCP.
====
endif::[]

ifeval::[{product-version} > 4.7]
[IMPORTANT]
.Networking between external load balancers and control plane nodes
====
External load balancing services and the control plane nodes must run on the same L2 network, and on the same VLAN when using VLANs to route traffic between the load balancing services and the control plane nodes.
====
endif::[]

The following table provides an exemplary embodiment of fully qualified domain names. The API and Nameserver addresses begin with canonical name extensions. The host names of the control plane and worker nodes are exemplary, so you can use any host naming convention you prefer.

[width="100%", cols="3,5e,2e", frame="topbot",options="header"]
|=====
| Usage | Host Name | IP
| API | api.<cluster-name>.<domain> | <ip>
| Ingress LB (apps) |  *.apps.<cluster-name>.<domain>  | <ip>
ifeval::[{product-version} <= 4.5]
| Nameserver | ns1.<cluster-name>.<domain> | <ip>
endif::[]
| Provisioner node | provisioner.<cluster-name>.<domain> | <ip>
| Master-0 | openshift-master-0.<cluster-name>.<domain> | <ip>
| Master-1 | openshift-master-1.<cluster-name>-.<domain> | <ip>
| Master-2 | openshift-master-2.<cluster-name>.<domain> | <ip>
| Worker-0 | openshift-worker-0.<cluster-name>.<domain> | <ip>
| Worker-1 | openshift-worker-1.<cluster-name>.<domain> | <ip>
| Worker-n | openshift-worker-n.<cluster-name>.<domain> | <ip>
|=====

ifdef::upstream[]
For assistance in configuring the DHCP server, check xref:ipi-install-upstream-appendix[Appendix] section for:

- xref:creating-dhcp-reservations-option1_{context}[Creating DHCP reservations with dhcpd (Option 1)]
- xref:creating-dhcp-reservations-using-dnsmasq-option2_{context}[Creating DHCP reservations with dnsmasq (Option 2)]
endif::[]

.Network Time Protocol (NTP)

Each {product-title} node in the cluster must have access to an NTP server. {product-title} nodes use NTP to synchronize their clocks. For example, cluster nodes use SSL certificates that require validation, which might fail if the date and time between the nodes are not in sync.

[IMPORTANT]
====
Define a consistent clock date and time format in each cluster node's BIOS settings, or installation might fail.
====

You may reconfigure the control plane nodes to act as NTP servers on disconnected clusters, and reconfigure worker nodes to retrieve time from the control plane nodes.

ifeval::[{product-version} == 4.6]
.Additional requirements with no provisioning network

All installer-provisioned installations require a `baremetal` network. The `baremetal` network is a routable network used for external network access to the outside world. In addition to the IP address supplied to the {product-title} cluster node, installations without a `provisioning` network require the following:

- Setting an available IP address from the `baremetal` network to the `bootstrapProvisioningIP` configuration setting within the `install-config.yaml` configuration file.

- Setting an available IP address from the `baremetal` network to the `provisioningHostIP` configuration setting within the `install-config.yaml` configuration file.

- Deploying the {product-title} cluster using RedFish Virtual Media/iDRAC Virtual Media.

[NOTE]
====
Configuring additional IP addresses for `bootstrapProvisioningIP` and `provisioningHostIP` is not required when using a `provisioning` network.
====
endif::[]

ifeval::[{product-version} > 4.6]
.State-driven network configuration requirements (Technology Preview)

{product-title} supports additional post-installation state-driven network configuration on the secondary network interfaces of cluster nodes using `kubernetes-nmstate`. For example, system administrators might configure a secondary network interface on cluster nodes after installation for a storage network.

[NOTE]
====
Configuration must occur before scheduling pods.
====

State-driven network configuration requires installing `kubernetes-nmstate`, and also requires Network Manager running on the cluster nodes. See *OpenShift Virtualization > Kubernetes NMState (Tech Preview)* for additional details.
endif::[]
