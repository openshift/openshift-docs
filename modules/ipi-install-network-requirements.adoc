// Module included in the following assemblies:
//
// * installing/installing_bare_metal_ipi/ipi-install-prerequisites.adoc

[id='network-requirements_{context}']
= Network requirements

Installer-provisioned installation of {product-title} involves several network requirements. First, installer-provisioned installation involves an optional non-routable `provisioning` network for provisioning the operating system on each bare metal node. Second, installer-provisioned installation involves a routable `baremetal` network.

.Configuring NICs

{product-title} deploys with two networks:

- `provisioning`: The `provisioning` network is an *optional* non-routable network used for provisioning the underlying operating system on each node that is a part of the {product-title} cluster. When deploying using the `provisioning` network, the first NIC on each node, such as `eth0` or `eno1`,
*must* interface with the `provisioning` network.

- `baremetal`: The `baremetal` network is a routable network. When deploying using the `provisioning` network, the second NIC on each node, such as `eth1` or `eno2`, *must* interface with the `baremetal` network. When deploying without a `provisioning` network, you can use any NIC on each node to interface with the `baremetal` network.

[IMPORTANT]
====
Each NIC should be on a separate VLAN corresponding to the appropriate network.
====

.Configuring the DNS server

Clients access the {product-title} cluster nodes over the `baremetal` network.
A network administrator must configure a subdomain or subzone where the canonical name extension is the cluster name.

----
<cluster_name>.<domain-name>
----

For example:

----
test-cluster.example.com
----

ifeval::[{product-version}>4.7]
{product-title} 4.8 and later releases include functionality that uses cluster membership information to generate A/AAAA records. This resolves the node names to their IP addresses. Once the nodes are registered with the API, the cluster can disperse node information without using CoreDNS-mDNS. This eliminates the network traffic associated with multicast DNS.
endif::[]

ifdef::upstream[]
For assistance in configuring the DNS server, check xref:ipi-install-upstream-appendix[Appendix] section for:

- xref:creating-dns-records-on-a-dns-server-option1_{context}[Creating DNS Records with Bind (Option 1)]
- xref:creating-dns-records-using-dnsmasq-option2_{context}[Creating DNS Records with dnsmasq (Option 2)]

endif::[]

.Dynamic Host Configuration Protocol (DHCP) requirements

By default, installer-provisioned installation deploys `ironic-dnsmasq` with DHCP enabled for the `provisioning` network. No other DHCP servers should be running on the `provisioning` network when the `provisioningNetwork` configuration setting is set to `managed`, which is the default value. If you have a DHCP server running on the `provisioning` network, you must set the `provisioningNetwork` configuration setting to `unmanaged` in the `install-config.yaml` file.

Network administrators must reserve IP addresses for each node in the {product-title} cluster for the `baremetal` network on an external DHCP server.

.Reserving IP addresses for nodes with the DHCP server

For the `baremetal` network, a network administrator must reserve a number of IP addresses, including:

ifeval::[{product-version} > 4.5]
. Two virtual IP addresses.
endif::[]
ifeval::[{product-version} <= 4.5]
. Three virtual IP addresses
endif::[]
+
- One IP address for the API endpoint
- One IP address for the wildcard ingress endpoint
ifeval::[{product-version} <= 4.5]
- One IP address for the name server
endif::[]

. One IP address for the provisioner node.
. One IP address for each control plane (master) node.
. One IP address for each worker node, if applicable.

ifeval::[{product-version} > 4.6]
[IMPORTANT]
.Reserving IP addresses so they become static IP addresses
====
Some administrators prefer to use static IP addresses so that each node's IP address remains constant in the absence of a DHCP server. To use static IP addresses in the {product-title} cluster, reserve the IP addresses with an infinite lease. During deployment, the installer will reconfigure the NICs from DHCP assigned addresses to static IP addresses. NICs with DHCP leases that are not infinite will remain configured to use DHCP.
====

[IMPORTANT]
.Ensuring that your DHCP server can provide infinite leases
====
Your DHCP server must provide a DHCP expiration time of 4294967295 seconds to properly set an infinite lease as specified by link:https://datatracker.ietf.org/doc/html/rfc2131[rfc2131]. If a lesser value is returned for the DHCP infinite lease time, the node reports an error and a static IP is not set for the node. In RHEL 8, `dhcpd` does not provides infinite leases. If you want to use the provisioner node to serve dynamic IP addresses with infinite lease times, use `dnsmasq` rather than `dhcpd`.
====
endif::[]

The following table provides an exemplary embodiment of fully qualified domain names. The API and Nameserver addresses begin with canonical name extensions. The hostnames of the control plane and worker nodes are exemplary, so you can use any host naming convention you prefer.

[width="100%",cols="3,5,2",frame="topbot",options="header"]
|=====
| Usage | Host Name | IP
| API | api.<cluster_name>.<domain> | <ip>
| Ingress LB (apps) |  *.apps.<cluster_name>.<domain>  | <ip>
ifeval::[{product-version} <= 4.5]
| Nameserver | ns1.<cluster_name>.<domain> | <ip>
endif::[]
| Provisioner node | provisioner.<cluster_name>.<domain> | <ip>
| Master-0 | openshift-master-0.<cluster_name>.<domain> | <ip>
| Master-1 | openshift-master-1.<cluster_name>.<domain> | <ip>
| Master-2 | openshift-master-2.<cluster_name>.<domain> | <ip>
| Worker-0 | openshift-worker-0.<cluster_name>.<domain> | <ip>
| Worker-1 | openshift-worker-1.<cluster_name>.<domain> | <ip>
| Worker-n | openshift-worker-n.<cluster_name>.<domain> | <ip>
|=====

ifdef::upstream[]
For assistance in configuring the DHCP server, check xref:ipi-install-upstream-appendix[Appendix] section for:

- xref:creating-dhcp-reservations-option1_{context}[Creating DHCP reservations with dhcpd (Option 1)]
- xref:creating-dhcp-reservations-using-dnsmasq-option2_{context}[Creating DHCP reservations with dnsmasq (Option 2)]
endif::[]

.Network Time Protocol (NTP)

Each {product-title} node in the cluster must have access to an NTP server. {product-title} nodes use NTP to synchronize their clocks. For example, cluster nodes use SSL certificates that require validation, which might fail if the date and time between the nodes are not in sync.

[IMPORTANT]
====
Define a consistent clock date and time format in each cluster node's BIOS settings, or installation might fail.
====

You may reconfigure the control plane nodes to act as NTP servers on disconnected clusters, and reconfigure worker nodes to retrieve time from the control plane nodes.

ifeval::[{product-version} == 4.6]
.Additional requirements with no provisioning network

All installer-provisioned installations require a `baremetal` network. The `baremetal` network is a routable network used for external network access to the outside world. In addition to the IP address supplied to the {product-title} cluster node, installations without a `provisioning` network require the following:

- Setting an available IP address from the `baremetal` network to the `bootstrapProvisioningIP` configuration setting within the `install-config.yaml` configuration file.

- Setting an available IP address from the `baremetal` network to the `provisioningHostIP` configuration setting within the `install-config.yaml` configuration file.

- Deploying the {product-title} cluster using RedFish Virtual Media/iDRAC Virtual Media.

[NOTE]
====
Configuring additional IP addresses for `bootstrapProvisioningIP` and `provisioningHostIP` is not required when using a `provisioning` network.
====
endif::[]

ifeval::[{product-version} > 4.6]
.State-driven network configuration requirements (Technology Preview)

{product-title} supports additional post-installation state-driven network configuration on the secondary network interfaces of cluster nodes using `kubernetes-nmstate`. For example, system administrators might configure a secondary network interface on cluster nodes after installation for a storage network.

[NOTE]
====
Configuration must occur before scheduling pods.
====

State-driven network configuration requires installing `kubernetes-nmstate`, and also requires Network Manager running on the cluster nodes. See *OpenShift Virtualization > Kubernetes NMState (Tech Preview)* for additional details.
endif::[]

.Port access for the out-of-band management IP address

The out-of-band management IP address is on a separate network from the node. To ensure that the out-of-band management can communicate with the `baremetal` node during installation, the out-of-band management IP address address must be granted access to the TCP 6180 port.
