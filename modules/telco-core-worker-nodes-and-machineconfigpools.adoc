// Module included in the following assemblies:
//
// * scalability_and_performance/telco_core_ref_design_specs/telco-core-rds.adoc

:_mod-docs-content-type: REFERENCE
[id="telco-core-worker-nodes-and-machineconfigpools_{context}"]
= Worker Nodes and MachineConfigPools

`MachineConfigPools` (MCPs) custom resources (CR) enable the subdivision of worker nodes in telco core clusters into different node groups based on customer planning parameters.
Careful deployment planning using MCPs is crucial to minimize deployment and upgrade time and, more importantly, to minimize interruption of telco-grade services during cluster upgrades.

*Description*

Telco core clusters can use `MachineConfigPools` (MCPs) to split worker nodes into additional separate roles, for example, due to different hardware profiles.
This allows custom tuning for each role and also plays a critical function in speeding up a telco core cluster deployment or upgrade.
Multiple MCPs can be used to properly plan cluster upgrades across one or multiple maintenance windows.
This is crucial because telco-grade services might otherwise be affected if careful planning is not considered.

During cluster upgrades, you can pause MCPs while you upgrade the control plane. See "Performing a canary rollout update" for more information. This ensures that worker nodes are not rebooted and running workloads remain unaffected until the MCP is unpaused.

Using careful MCP planning, you can control the timing and order of which set of nodes are upgraded at any time. For more information on how to use MCPs to plan telco upgrades, see "Applying MachineConfigPool labels to nodes before the update".

Before beginning the initial deployment, review the following engineering considerations:

**PerformanceProfile and Tuned profile association:**

When using PerformanceProfiles, remember that each Machine Config Pool (MCP) must be linked to exactly one PerformanceProfile or Tuned profile definition.
Consequently, even if the desired configuration is identical for multiple MCPs, each MCP still requires its own dedicated PerformanceProfile definition.

**Planning your MCP labeling strategy:**

Plan your MCP labeling with an appropriate strategy to split your worker nodes depending on parameters
such as:

* The worker node type: identifying a group of nodes with equivalent hardware profile, for example workers for control plane Network Functions (NFs) and workers for user data plane NFs.
* The number of worker nodes per worker node type.
* The minimum number of MCPs required for an equivalent hardware profile is 1, but could be larger for larger clusters.
For example, you may design for more MCPs per hardware profile to support a more granular upgrade where a smaller percentage of the cluster capacity is affected with each step.
* The update strategy for nodes within an MCP is by upgrade requirements and the chosen `maxUnavailable` value:
** Number of maintenance windows allowed
** Duration of a maintenance window
** Total number of worker nodes
** Desired `maxUnavailable` (number of nodes updated concurrently) for the MCP.
*  CNF requirements for worker nodes, in terms of:
** Minimum availability per Pod required during an upgrade, configured with a pod disruption budget (PDB). PDBs are crucial to maintain telco service level Agreements (SLAs) during upgrades. For more information about PDB, see "Understanding how to use pod disruption budgets to specify the number of pods that must be up".
** Minimum true high availability required per Pod, such that each replica runs on separate hardware.
** Pod affinity and anti-affinity link: For more information about how to use pod affinity and anti-affinity, see "Placing pods relative to other pods using affinity and anti-affinity rules".
* Duration and number of upgrade maintenance windows during which telco-grade services might be affected.

