// Module included in the following assemblies:
//
// * installing/installing_aws/installing-aws-government-region.adoc
// * installing/installing_aws/installing-aws-private.adoc
// * installing/installing_aws/installing-aws-vpc.adoc

[id="installation-custom-aws-vpc_{context}"]
= About using a custom VPC

In {product-title} {product-version}, you can deploy a cluster into existing subnets in an existing Amazon Virtual Private Cloud (VPC) in Amazon Web Services (AWS). By deploying {product-title} into an existing AWS VPC, you might be able to avoid limit constraints in new accounts or more easily abide by the operational constraints that your company's guidelines set. If you cannot obtain the infrastructure creation permissions that are required to create the VPC yourself, use this installation option.

Because the installation program cannot know what other components are also in your existing subnets, it cannot choose subnet CIDRs and so forth on your behalf. You must configure networking for the subnets that you install your cluster to yourself.

[id="installation-custom-aws-vpc-requirements_{context}"]
== Requirements for using your VPC

The installation program no longer creates the following components:

* Internet gateways
* NAT gateways
* Subnets
* Route tables
* VPCs
* VPC DHCP options
* VPC endpoints

If you use a custom VPC, you must correctly configure it and its subnets for the installation program and the cluster to use. The installation program cannot subdivide network ranges for the cluster to use, set route tables for the subnets, or set VPC options like DHCP, so you must do so before you install the cluster.

Your VPC must meet the following characteristics:

* The VPC's CIDR block must contain the `Networking.MachineCIDR` range, which is the IP address pool for cluster machines.
* The VPC must not use the `kubernetes.io/cluster/.*: owned` tag.
* You must enable the `enableDnsSupport` and `enableDnsHostnames` attributes in your VPC so that the cluster can use the Route53 zones that are attached to the VPC to resolve clusterâ€™s internal DNS records. See link:https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#vpc-dns-support[DNS Support in Your VPC] in the AWS documentation.

If you use a cluster with public access, you must create a public and a private subnet for each availability zone that your cluster uses. The installation program modifies your subnets to add the `kubernetes.io/cluster/.*: shared` tag, so your subnets must have at least one free tag slot available for it. Review the current link:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions[Tag Restrictions] in the AWS documentation to ensure that the installation program can add a tag to each subnet that you specify.

If you are working in a disconnected environment, you are unable to reach the
public IP addresses for EC2 and ELB endpoints. To resolve this, you must create
a VPC endpoint and attach it to the subnet that the clusters are using. The
endpoints should be named as follows:

* `ec2.<region>.amazonaws.com`
* `elasticloadbalancing.<region>.amazonaws.com`
* `s3.<region>.amazonaws.com`

.Required VPC components

You must provide a suitable VPC and subnets that allow communication to your
machines.

[cols="2a,7a,3a,3a",options="header"]
|===

|Component
|AWS type
2+|Description

|VPC
|* `AWS::EC2::VPC`
* `AWS::EC2::VPCEndpoint`
2+|You must provide a public VPC for the cluster to use. The VPC uses an
endpoint that references the route tables for each subnet to improve communication with the registry that is hosted in S3.

|Public subnets
|* `AWS::EC2::Subnet`
* `AWS::EC2::SubnetNetworkAclAssociation`
2+|Your VPC must have public subnets for between 1 and 3 availability zones
and associate them with appropriate Ingress rules.

|Internet gateway
|
* `AWS::EC2::InternetGateway`
* `AWS::EC2::VPCGatewayAttachment`
* `AWS::EC2::RouteTable`
* `AWS::EC2::Route`
* `AWS::EC2::SubnetRouteTableAssociation`
* `AWS::EC2::NatGateway`
* `AWS::EC2::EIP`
2+|You must have a public internet gateway, with public routes, attached to the
VPC. In the provided templates, each public subnet has a NAT gateway with an EIP address. These NAT gateways allow cluster resources, like private subnet instances, to reach the internet and are not required for some restricted network or proxy scenarios.

.7+|Network access control
.7+| * `AWS::EC2::NetworkAcl`
* `AWS::EC2::NetworkAclEntry`
2+|You must allow the VPC to access the following ports:
h|Port
h|Reason

|`80`
|Inbound HTTP traffic

|`443`
|Inbound HTTPS traffic

|`22`
|Inbound SSH traffic

|`1024` - `65535`
|Inbound ephemeral traffic

|`0` - `65535`
|Outbound ephemeral traffic


|Private subnets
|* `AWS::EC2::Subnet`
* `AWS::EC2::RouteTable`
* `AWS::EC2::SubnetRouteTableAssociation`
2+|Your VPC can have private subnets. The provided CloudFormation templates
can create private subnets for between 1 and 3 availability zones.
If you use private subnets, you must provide appropriate routes and tables
for them.

|===

[id="installation-custom-aws-vpc-validation_{context}"]
== VPC validation

To ensure that the subnets that you provide are suitable, the installation program confirms the following data:

* All the subnets that you specify exist.
* You provide private subnets.
* The subnet CIDRs belong to the machine CIDR that you specified.
* You provide subnets for each availability zone. Each availability zone contains no more than one public and one private subnet. If you use a private cluster, provide only a private subnet for each availability zone. Otherwise, provide exactly one public and private subnet for each availability zone.
* You provide a public subnet for each private subnet availability zone. Machines are not provisioned in availability zones that you do not provide private subnets for.

If you destroy a cluster that uses an existing VPC, the VPC is not deleted. When you remove the {product-title} cluster from a VPC, the `kubernetes.io/cluster/.*: shared` tag is removed from the subnets that it used.

[id="installation-about-custom-aws-permissions_{context}"]
== Division of permissions

Starting with {product-title} 4.3, you do not need all of the permissions that are required for an installation program-provisioned infrastructure cluster to deploy a cluster. This change mimics the division of permissions that you might have at your company: some individuals can create different resource in your clouds than others. For example, you might be able to create application-specific items, like instances, buckets, and load balancers, but not networking-related components such as VPCs, subnets, or ingress rules.

The AWS credentials that you use when you create your cluster do not need the networking permissions that are required to make VPCs and core networking components within the VPC, such as subnets, routing tables, internet gateways, NAT, and VPN. You still need permission to make the application resources that the machines within the cluster require, such as ELBs, security groups, S3 buckets, and nodes.

[id="installation-custom-aws-vpc-isolation_{context}"]
== Isolation between clusters

If you deploy {product-title} to an existing network, the isolation of cluster services is reduced in the following ways:

* You can install multiple {product-title} clusters in the same VPC.
* ICMP ingress is allowed from the entire network.
* TCP 22 ingress (SSH) is allowed to the entire network.
//You can restrict ingress to the control plane and compute security groups by either adding the security groups to an SSH bastion instance or altering rules to allow the bastion.
* Control plane TCP 6443 ingress (Kubernetes API) is allowed to the entire network.
* Control plane TCP 22623 ingress (MCS) is allowed to the entire network.
//This should be restricted to the control plane and compute security groups, instead of the current by-VPC-CIDR logic to avoid leaking sensitive Ignition configs to non-cluster entities sharing the VPC.
