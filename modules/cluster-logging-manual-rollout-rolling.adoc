// Module included in the following assemblies:
//
// * logging/cluster-logging-manual-rollout.adoc

[id="cluster-logging-manual-rollout-rolling_{context}"]
= Performing an Elasticsearch rolling cluster restart

Perform a rolling restart when you change the `elasticsearch` configmap
or any of the `elasticsearch-*` deployment configurations.

Also, a rolling restart is recommended if the nodes on which an Elasticsearch pod
runs requires a reboot.

.Prerequisite

* Cluster logging and Elasticsearch must be installed.

.Procedure

To perform a rolling cluster restart:

. Change to the `openshift-logging` project:
+
[source,terminal]
----
$ oc project openshift-logging
----

. Use the following command to extract the CA certificate from Elasticsearch and write to the *_admin-ca_* file:
+
[source,terminal]
----
$ oc extract secret/elasticsearch --to=. --keys=admin-ca
----
+
[source,terminal]
----
admin-ca
----

. Perform a shard synced flush to ensure there are no pending operations waiting to be written to disk prior to shutting down:
+
[source,terminal]
----
$ oc exec <any_es_pod_in_the_cluster> -c elasticsearch -- curl -s --cacert /etc/elasticsearch/secret/admin-ca --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key -XPOST 'https://localhost:9200/_flush/synced'
----
+
For example:
+
[source,terminal]
----
oc exec -c elasticsearch-cdm-5ceex6ts-1-dcd6c4c7c-jpw6 -- curl -s --cacert /etc/elasticsearch/secret/admin-ca --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key -XPOST 'https://localhost:9200/_flush/synced'
----

. Prevent shard balancing when purposely bringing down nodes using the {product-title}
link:https://github.com/openshift/origin-aggregated-logging/tree/master/elasticsearch#es_util[*es_util*] tool:
+
[source,terminal]
----
$ oc exec <any_es_pod_in_the_cluster> -c elasticsearch -- es_util --query=_cluster/settings -XPUT 'https://localhost:9200/_cluster/settings' -d '{ "transient": { "cluster.routing.allocation.enable" : "none" } }'
----
+
For example:
+
[source,terminal]
----
$ oc exec elasticsearch-cdm-5ceex6ts-1-dcd6c4c7c-jpw6 -c elasticsearch -- es_util --query=_cluster/settings?pretty=true -XPUT 'https://localhost:9200/_cluster/settings' -d '{ "transient": { "cluster.routing.allocation.enable" : "none" } }'
----
+
.Example output
[source,terminal]
----
{
  "acknowledged" : true,
  "persistent" : { },
  "transient" : {
    "cluster" : {
      "routing" : {
        "allocation" : {
          "enable" : "none"
        }
      }
    }
  }
----

. Once complete, for each deployment you have for an ES cluster:

.. By default, the {product-title} Elasticsearch cluster blocks rollouts to their nodes. Use the following command to allow rollouts
and allow the pod to pick up the changes:
+
[source,terminal]
----
$ oc rollout resume deployment/<deployment-name>
----
+
For example:
+
[source,terminal]
----
$ oc rollout resume deployment/elasticsearch-cdm-0-1
----
+
[source,terminal]
----
deployment.extensions/elasticsearch-cdm-0-1 resumed
----
+
A new pod is deployed. Once the pod has a ready container, you can
move on to the next deployment.
+
[source,terminal]
----
$ oc get pods | grep elasticsearch-*
----
+
.Example outputs
[source,terminal]
----
NAME                                            READY   STATUS    RESTARTS   AGE
elasticsearch-cdm-5ceex6ts-1-dcd6c4c7c-jpw6k    2/2     Running   0          22h
elasticsearch-cdm-5ceex6ts-2-f799564cb-l9mj7    2/2     Running   0          22h
elasticsearch-cdm-5ceex6ts-3-585968dc68-k7kjr   2/2     Running   0          22h
----

.. Once complete, reset the pod to disallow rollouts:
+
[source,terminal]
----
$ oc rollout pause deployment/<deployment-name>
----
+
For example:
+
[source,terminal]
----
$ oc rollout pause deployment/elasticsearch-cdm-0-1
----
+
[source,terminal]
----
deployment.extensions/elasticsearch-cdm-0-1 paused
----
+
.. Check that the Elasticsearch cluster is in `green` state:
+
[source,terminal]
----
$ oc exec <any_es_pod_in_the_cluster> -c elasticsearch -- es_util --query=_cluster/health?pretty=true
----
+
[NOTE]
====
If you performed a rollout on the Elasticsearch pod you used in the previous commands, the pod no longer exists and you need a new pod name here.
====
+
For example:
+
[source,terminal]
----
$ oc exec elasticsearch-cdm-5ceex6ts-1-dcd6c4c7c-jpw6 -c elasticsearch -- es_util --query=_cluster/health?pretty=true
----
+
.Example output
[source,json]
----
{
  "cluster_name" : "elasticsearch",
  "status" : "green", <1>
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 8,
  "active_shards" : 16,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 1,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
----
<1> Make sure this parameter is `green` before proceeding.

. If you changed the Elasticsearch configuration map, repeat these steps for each Elasticsearch pod.

. Once all the deployments for the cluster have been rolled out, re-enable shard balancing:
+
[source,terminal]
----
$ oc exec <any_es_pod_in_the_cluster> -c elasticsearch -- es_util --query=_cluster/settings -XPUT 'https://localhost:9200/_cluster/settings' -d '{ "transient": { "cluster.routing.allocation.enable" : "none" } }'
----
+
For example:
+
[source,terminal]
----
$ oc exec elasticsearch-cdm-5ceex6ts-1-dcd6c4c7c-jpw6 -c elasticsearch -- es_util --query=_cluster/settings?pretty=true -XPUT 'https://localhost:9200/_cluster/settings' -d '{ "transient": { "cluster.routing.allocation.enable" : "all" } }'
----
+
Example output
[source,json]
----
{
  "acknowledged" : true,
  "persistent" : { },
  "transient" : {
    "cluster" : {
      "routing" : {
        "allocation" : {
          "enable" : "all"
        }
      }
    }
  }
}
----
