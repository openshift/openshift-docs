// Module included in the following assemblies:
//
// * observability/otel/otel-configuration-of-otel-collector.adoc

:_mod-docs-content-type: REFERENCE
[id="otel-collector-components_{context}"]
= OpenTelemetry Collector components

[id="receivers_{context}"]
== Receivers

Receivers get data into the Collector.

[id="otlp-receiver_{context}"]
=== OTLP Receiver

The OTLP Receiver ingests traces, metrics, and logs by using the OpenTelemetry Protocol (OTLP).
The OTLP Receiver ingests traces and metrics using the OpenTelemetry protocol (OTLP).

.OpenTelemetry Collector custom resource with an enabled OTLP Receiver
[source,yaml]
----
  config: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317 # <1>
            tls: # <2>
              ca_file: ca.pem
              cert_file: cert.pem
              key_file: key.pem
              client_ca_file: client.pem # <3>
              reload_interval: 1h # <4>
          http:
            endpoint: 0.0.0.0:4318 # <5>
            tls: # <6>

    service:
      pipelines:
        traces:
          receivers: [otlp]
        metrics:
          receivers: [otlp]
----
<1> The OTLP gRPC endpoint. If omitted, the default `+0.0.0.0:4317+` is used.
<2> The server-side TLS configuration. Defines paths to TLS certificates. If omitted, the TLS is disabled.
<3> The path to the TLS certificate at which the server verifies a client certificate. This sets the value of `ClientCAs` and `ClientAuth` to `RequireAndVerifyClientCert` in the `TLSConfig`. For more information, see the link:https://godoc.org/crypto/tls#Config[`Config` of the Golang TLS package].
<4> Specifies the time interval at which the certificate is reloaded. If the value is not set, the certificate is never reloaded. The `reload_interval` field accepts a string containing valid units of time such as `ns`, `us` (or `Âµs`), `ms`, `s`, `m`, `h`.
<5> The OTLP HTTP endpoint. The default value is `+0.0.0.0:4318+`.
<6> The server-side TLS configuration. For more information, see the `grpc` protocol configuration section.

[id="jaeger-receiver_{context}"]
=== Jaeger Receiver

The Jaeger Receiver ingests traces in the Jaeger formats.

.OpenTelemetry Collector custom resource with an enabled Jaeger Receiver
[source,yaml]
----
  config: |
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250 # <1>
          thrift_http:
            endpoint: 0.0.0.0:14268 # <2>
          thrift_compact:
            endpoint: 0.0.0.0:6831 # <3>
          thrift_binary:
            endpoint: 0.0.0.0:6832 # <4>
          tls: # <5>

    service:
      pipelines:
        traces:
          receivers: [jaeger]
----
<1> The Jaeger gRPC endpoint. If omitted, the default `+0.0.0.0:14250+` is used.
<2> The Jaeger Thrift HTTP endpoint. If omitted, the default `+0.0.0.0:14268+` is used.
<3> The Jaeger Thrift Compact endpoint. If omitted, the default `+0.0.0.0:6831+` is used.
<4> The Jaeger Thrift Binary endpoint. If omitted, the default `+0.0.0.0:6832+` is used.
<5> The  server-side TLS configuration. See the OTLP Receiver configuration section for more details.

[id="hostmetrics-receiver_{context}"]
=== Host Metrics Receiver

:FeatureName: The Host Metrics Receiver
include::snippets/technology-preview.adoc[]

The Host Metrics Receiver ingests metrics in the OTLP format.

.OpenTelemetry Collector custom resource with an enabled Host Metrics Receiver
[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-hostfs-daemonset
  namespace: <namespace>
---
apiVersion: security.openshift.io/v1
kind: SecurityContextConstraints
allowHostDirVolumePlugin: true
allowHostIPC: false
allowHostNetwork: false
allowHostPID: true
allowHostPorts: false
allowPrivilegeEscalation: true
allowPrivilegedContainer: true
allowedCapabilities: null
defaultAddCapabilities:
- SYS_ADMIN
fsGroup:
  type: RunAsAny
groups: []
metadata:
  name: otel-hostmetrics
readOnlyRootFilesystem: true
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
supplementalGroups:
  type: RunAsAny
users:
- system:serviceaccount:<namespace>:otel-hostfs-daemonset
volumes:
- configMap
- emptyDir
- hostPath
- projected
---
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel
  namespace: <namespace>
spec:
  serviceAccount: otel-hostfs-daemonset
  mode: daemonset
  volumeMounts:
    - mountPath: /hostfs
      name: host
      readOnly: true
  volumes:
    - hostPath:
        path: /
      name: host
  config: |
    receivers:
      hostmetrics:
        collection_interval: 10s # <1>
        initial_delay: 1s # <2>
        root_path: / # <3>
        scrapers: # <4>
          cpu:
          memory:
          disk:
    service:
      pipelines:
        metrics:
          receivers: [hostmetrics]
----
<1> Sets the time interval for host metrics collection. If omitted, the default value is `+1m+`.
<2> Sets the initial time delay for host metrics collection. If omitted, the default value is `+1s+`.
<3> Configures the `root_path` so that the Host Metrics Receiver knows where the root filesystem is. If running multiple instances of the Host Metrics Receiver, set the same `root_path` value for each instance.
<4> Lists the enabled host metrics scrapers. Available scrapers are `cpu`, `disk`, `load`, `filesystem`, `memory`, `network`, `paging`, `processes`, and `process`.

[id="k8sobjectsreceiver-receiver_{context}"]
=== Kubernetes Objects Receiver

:FeatureName: The Kubernetes Objects Receiver
include::snippets/technology-preview.adoc[]

The Kubernetes Objects Receiver pulls or watches objects to be collected from the Kubernetes API server.
This receiver watches primarily Kubernetes events, but it can collect any type of Kubernetes objects.
This receiver gathers telemetry for the cluster as a whole, so only one instance of this receiver suffices for collecting all the data.

.OpenTelemetry Collector custom resource with an enabled Kubernetes Objects Receiver
[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-k8sobj
  namespace: <namespace>
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-k8sobj
  namespace: <namespace>
rules:
- apiGroups:
  - ""
  resources:
  - events
  - pods
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - "events.k8s.io"
  resources:
  - events
  verbs:
  - watch
  - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-k8sobj
subjects:
  - kind: ServiceAccount
    name: otel-k8sobj
    namespace: <namespace>
roleRef:
  kind: ClusterRole
  name: otel-k8sobj
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel-k8s-obj
  namespace: <namespace>
spec:
  serviceAccount: otel-k8sobj
  image: ghcr.io/os-observability/redhat-opentelemetry-collector/redhat-opentelemetry-collector:main
  mode: deployment
  config: |
    receivers:
      k8sobjects:
        auth_type: serviceAccount
        objects:
          - name: pods # <1>
            mode: pull # <2>
            interval: 30s # <3>
            label_selector: # <4>
            field_selector: # <5>
            namespaces: [<namespace>,...] # <6>
          - name: events
            mode: watch
    exporters:
      debug:
    service:
      pipelines:
        logs:
          receivers: [k8sobjects]
          exporters: [debug]
----
<1> The Resource name that this receiver observes: for example, `pods`, `deployments`, or `events`.
<2> The observation mode that this receiver uses: `pull` or `watch`.
<3> Only applicable to the pull mode. The request interval for pulling an object. If omitted, the default value is `+1h+`.
<4> The label selector to define targets.
<5> The field selector to filter targets.
<6> The list of namespaces to collect events from. If omitted, the default value is `+all+`.

[id="kubeletstats-receiver_{context}"]
=== Kubelet Stats Receiver

:FeatureName: The Kubelet Stats Receiver
include::snippets/technology-preview.adoc[]

The Kubelet Stats Receiver extracts metrics related to nodes, pods, containers, and volumes from the kubelet's API server. These metrics are then channeled through the metrics-processing pipeline for additional analysis.

.OpenTelemetry Collector custom resource with an enabled Kubelet Stats Receiver
[source,yaml]
----
# ...
config: |
  receivers:
    kubeletstats:
      collection_interval: 20s
      auth_type: "serviceAccount"
      endpoint: "https://${env:K8S_NODE_NAME}:10250"
      insecure_skip_verify: true
  service:
    pipelines:
      metrics:
        receivers: [kubeletstats]
env:
  - name: K8S_NODE_NAME # <1>
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName
# ...
----
<1> Sets the `K8S_NODE_NAME` to authenticate to the API.

The Kubelet Stats Receiver requires additional permissions for the service account used for running the OpenTelemetry Collector.

.Permissions required by the service account
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
rules:
  - apiGroups: ['']
    resources: ['nodes/stats']
    verbs: ['get', 'watch', 'list']
  - apiGroups: [""]
    resources: ["nodes/proxy"] # <1>
    verbs: ["get"]
----
<1> The permissions required when using the `extra_metadata_labels` or `request_utilization` or `limit_utilization` metrics.

[id="prometheus-receiver_{context}"]
=== Prometheus Receiver

The Prometheus Receiver is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Prometheus Receiver scrapes the metrics endpoints.

.OpenTelemetry Collector custom resource with an enabled Prometheus Receiver
[source,yaml]
----
  config: |
    receivers:
        prometheus:
          config:
            scrape_configs: # <1>
              - job_name: 'my-app'  # <2>
                scrape_interval: 5s # <3>
                static_configs:
                  - targets: ['my-app.example.svc.cluster.local:8888'] # <4>
    service:
      pipelines:
        metrics:
          receivers: [prometheus]
----
<1> Scrapes configurations using the Prometheus format.
<2> The Prometheus job name.
<3> The lnterval for scraping the metrics data. Accepts time units. The default value is `1m`.
<4> The targets at which the metrics are exposed. This example scrapes the metrics from a `my-app` application in the `example` project.

[id="zipkin-receiver_{context}"]
=== Zipkin Receiver

The Zipkin Receiver ingests traces in the Zipkin v1 and v2 formats.

.OpenTelemetry Collector custom resource with the enabled Zipkin Receiver
[source,yaml]
----
  config: |
    receivers:
      zipkin:
        endpoint: 0.0.0.0:9411 # <1>
        tls: # <2>

    service:
      pipelines:
        traces:
          receivers: [zipkin]
----
<1> The Zipkin HTTP endpoint. If omitted, the default `+0.0.0.0:9411+` is used.
<2> The server-side TLS configuration. See the OTLP Receiver configuration section for more details.

[id="kafka-receiver_{context}"]
=== Kafka Receiver

The Kafka Receiver is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Kafka Receiver receives traces, metrics, and logs from Kafka in the OTLP format.

.OpenTelemetry Collector custom resource with the enabled Kafka Receiver
[source,yaml]
----
  config: |
    receivers:
      kafka:
        brokers: ["localhost:9092"] # <1>
        protocol_version: 2.0.0 # <2>
        topic: otlp_spans # <3>
        auth:
          plain_text: # <4>
            username: example
            password: example
          tls: # <5>
            ca_file: ca.pem
            cert_file: cert.pem
            key_file: key.pem
            insecure: false # <6>
            server_name_override: kafka.example.corp # <7>
    service:
      pipelines:
        traces:
          receivers: [kafka]
----
<1> The list of Kafka brokers. The default is `+localhost:9092+`.
<2> The Kafka protocol version. For example, `+2.0.0+`. This is a required field.
<3> The name of the Kafka topic to read from. The default is `+otlp_spans+`.
<4> The plaintext authentication configuration. If omitted, plaintext authentication is disabled.
<5> The client-side TLS configuration. Defines paths to the TLS certificates. If omitted, TLS authentication is disabled.
<6> Disables verifying the server's certificate chain and host name. The default is `+false+`.
<7> ServerName indicates the name of the server requested by the client to support virtual hosting.

[id="k8scluster-receiver_{context}"]
=== Kubernetes Cluster Receiver

:FeatureName: The Kubernetes Cluster Receiver
include::snippets/technology-preview.adoc[]

The Kubernetes Cluster Receiver gathers cluster metrics and entity events from the Kubernetes API server. It uses the Kubernetes API to receive information about updates. Authentication for this receiver is only supported through service accounts.

.OpenTelemetry Collector custom resource with the enabled Kubernetes Cluster Receiver
[source,yaml]
----
# ...
  receivers:
    k8s_cluster:
      distribution: openshift
      collection_interval: 10s
  exporters:
    debug:
  service:
    pipelines:
      metrics:
        receivers: [k8s_cluster]
        exporters: [debug]
      logs/entity_events:
        receivers: [k8s_cluster]
        exporters: [debug]
# ...
----

This receiver requires a configured service account, RBAC rules for the cluster role, and the cluster role binding that binds the RBAC with the service account.

.`ServiceAccount` object
[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: otelcontribcol
  name: otelcontribcol
----

.RBAC rules for the `ClusterRole` object
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otelcontribcol
  labels:
    app: otelcontribcol
rules:
- apiGroups:
  - quota.openshift.io
  resources:
  - clusterresourcequotas
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  - namespaces
  - namespaces/status
  - nodes
  - nodes/spec
  - pods
  - pods/status
  - replicationcontrollers
  - replicationcontrollers/status
  - resourcequotas
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  - replicasets
  - statefulsets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - deployments
  - replicasets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  - cronjobs
  verbs:
  - get
  - list
  - watch
- apiGroups:
    - autoscaling
  resources:
    - horizontalpodautoscalers
  verbs:
    - get
    - list
    - watch
----

.`ClusterRoleBinding` object
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otelcontribcol
  labels:
    app: otelcontribcol
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otelcontribcol
subjects:
- kind: ServiceAccount
  name: otelcontribcol
  namespace: default
----

[id="opencensus-receiver_{context}"]
=== OpenCensus Receiver

The OpenCensus Receiver provides backwards compatibility with the OpenCensus project for easier migration of instrumented codebases. It receives metrics and traces in the OpenCensus format via gRPC or HTTP and Json.

.OpenTelemetry Collector custom resource with the enabled OpenCensus Receiver
[source,yaml]
----
  config: |
    receivers:
      opencensus:
        endpoint: 0.0.0.0:9411 # <1>
        tls: # <2>
        cors_allowed_origins: # <3>
          - https://*.<example>.com
    service:
      pipelines:
        traces:
          receivers: [opencensus]
          ...
----
<1> The OpenCensus endpoint. If omitted, the default is `+0.0.0.0:55678+`.
<2> The server-side TLS configuration. See the OTLP Receiver configuration section for more details.
<3> You can also use the HTTP JSON endpoint to optionally configure CORS, which is enabled by specifying a list of allowed CORS origins in this field.
Wildcards with `+*+` are accepted under the `cors_allowed_origins`.
To match any origin, enter only `+*+`.

[id="filelog-receiver_{context}"]
=== Filelog Receiver

:FeatureName: The Filelog Receiver
include::snippets/technology-preview.adoc[]

The Filelog Receiver tails and parses logs from files.

.OpenTelemetry Collector custom resource with the enabled Filelog Receiver that tails a text file
[source,yaml]
----
receivers:
  filelog:
    include: [ /simple.log ] # <1>
    operators: # <2>
      - type: regex_parser
        regex: '^(?P<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (?P<sev>[A-Z]*) (?P<msg>.*)$'
        timestamp:
          parse_from: attributes.time
          layout: '%Y-%m-%d %H:%M:%S'
        severity:
          parse_from: attributes.sev
----
<1> A list of file glob patterns that match the file paths to be read.
<2> An array of Operators. Each Operator performs a simple task such as parsing a timestamp or JSON. To process logs into a desired format, chain the Operators together.

[id="journald-receiver_{context}"]
=== Journald Receiver

:FeatureName: The Journald Receiver
include::snippets/technology-preview.adoc[]

The Journald Receiver parses *journald* events from the *systemd* journal and sends them as logs.

.OpenTelemetry Collector custom resource with the enabled Journald Receiver
[source,yaml]
----
kubectl apply -f - <<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: otel-journald
  labels:
    security.openshift.io/scc.podSecurityLabelSync: "false"
    pod-security.kubernetes.io/enforce: "privileged"
    pod-security.kubernetes.io/audit: "privileged"
    pod-security.kubernetes.io/warn: "privileged"
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: privileged-sa
  namespace: otel-journald
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-journald-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:openshift:scc:privileged
subjects:
- kind: ServiceAccount
  name: privileged-sa
  namespace: otel-journald
---
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel-journald-logs
  namespace: otel-journald
spec:
  mode: daemonset
  serviceAccount: privileged-sa
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - CHOWN
      - DAC_OVERRIDE
      - FOWNER
      - FSETID
      - KILL
      - NET_BIND_SERVICE
      - SETGID
      - SETPCAP
      - SETUID
    readOnlyRootFilesystem: true
    seLinuxOptions:
      type: spc_t
    seccompProfile:
      type: RuntimeDefault
  config: |
    receivers:
      journald:
        files: /var/log/journal/*/*
        priority: info # <1>
        units: # <2>
          - kubelet
          - crio
          - init.scope
          - dnsmasq
        all: true # <3>
        retry_on_failure:
          enabled: true # <4>
          initial_interval: 1s # <5>
          max_interval: 30s # <6>
          max_elapsed_time: 5m # <7>
    processors:
    exporters:
      debug:
        verbosity: detailed
    service:
      pipelines:
        logs:
          receivers: [journald]
          exporters: [debug]
  volumeMounts:
  - name: journal-logs
    mountPath: /var/log/journal/
    readOnly: true
  volumes:
  - name: journal-logs
    hostPath:
      path: /var/log/journal
  tolerations:
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule
EOF
----
<1> Filters output by message priorities or priority ranges. The default value is `info`.
<2> Lists the units to read entries from. If empty, entries are read from all units.
<3> Includes very long logs and logs with unprintable characters. The default value is `false`.
<4> If set to `true`, the receiver pauses reading a file and attempts to resend the current batch of logs when encountering an error from downstream components. The default value is `false`.
<5> The time interval to wait after the first failure before retrying. The default value is `1s`. The units are `ms`, `s`, `m`, `h`.
<6> The upper bound for the retry backoff interval. When this value is reached, the time interval between consecutive retry attempts remains constant at this value. The default value is `30s`. The supported units are `ms`, `s`, `m`, `h`.
<7> The maximum time interval, including retry attempts, for attempting to send a logs batch to a downstream consumer. When this value is reached, the data are discarded. If the set value is `0`, retrying never stops. The default value is `5m`. The supported units are `ms`, `s`, `m`, `h`.

[id="kubernetesevents-receiver_{context}"]
=== Kubernetes Events Receiver

:FeatureName: The Kubernetes Events Receiver
include::snippets/technology-preview.adoc[]

The Kubernetes Events Receiver collects events from the Kubernetes API server. The collected events are converted into logs.

.OpenShift Container Platform permissions required for the Kubernetes Events Receiver
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
  labels:
    app: otel-collector
rules:
- apiGroups:
  - ""
  resources:
  - events
  - namespaces
  - namespaces/status
  - nodes
  - nodes/spec
  - pods
  - pods/status
  - replicationcontrollers
  - replicationcontrollers/status
  - resourcequotas
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  - replicasets
  - statefulsets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - deployments
  - replicasets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  - cronjobs
  verbs:
  - get
  - list
  - watch
- apiGroups:
    - autoscaling
  resources:
    - horizontalpodautoscalers
  verbs:
    - get
    - list
    - watch
----

.OpenTelemetry Collector custom resource with the enabled Kubernetes Event Receiver
[source,yaml]
----
  serviceAccount: otel-collector # <1>
  config: |
    receivers:
      k8s_events:
        namespaces: [project1, project2] # <2>
    service:
      pipelines:
        logs:
          receivers: [k8s_events]
----
<1> The service account of the Collector that has the required ClusterRole `otel-collector` RBAC.
<2> The list of namespaces to collect events from. The default value is empty, which means that all namespaces are collected.

[id="processors_{context}"]
== Processors

Processors run through the data between it is received and exported.

[id="batch-processor_{context}"]
=== Batch Processor

The Batch Processor batches traces and metrics to reduce the number of outgoing connections needed to transfer the telemetry information.

.Example of the OpenTelemetry Collector custom resource when using the Batch Processor
[source,yaml]
----
  config: |
    processor:
      batch:
        timeout: 5s
        send_batch_max_size: 10000
    service:
      pipelines:
        traces:
          processors: [batch]
        metrics:
          processors: [batch]
----

.Parameters used by the Batch Processor
[options="header"]
[cols="l, a, a"]
|===
|Parameter |Description |Default

|timeout
|Sends the batch after a specific time duration and irrespective of the batch size.
|`200ms`

|send_batch_size
|Sends the batch of telemetry data after the specified number of spans or metrics.
|`8192`

|send_batch_max_size
|The maximum allowable size of the batch. Must be equal or greater than the `send_batch_size`.
|`0`

|metadata_keys
|When activated, a batcher instance is created for each unique set of values found in the `client.Metadata`.
|`[]`

|metadata_cardinality_limit
|When the `metadata_keys` are populated, this configuration restricts the number of distinct metadata key-value combinations processed throughout the duration of the process.
|`1000`
|===

[id="memorylimiter-processor_{context}"]
=== Memory Limiter Processor

The Memory Limiter Processor periodically checks the Collector's memory usage and pauses data processing when the soft memory limit is reached. This processor supports traces, metrics, and logs. The preceding component, which is typically a receiver, is expected to retry sending the same data and may apply a backpressure to the incoming data. When memory usage exceeds the hard limit, the Memory Limiter Processor forces garbage collection to run.

.Example of the OpenTelemetry Collector custom resource when using the Memory Limiter Processor
[source,yaml]
----
  config: |
    processor:
      memory_limiter:
        check_interval: 1s
        limit_mib: 4000
        spike_limit_mib: 800
    service:
      pipelines:
        traces:
          processors: [batch]
        metrics:
          processors: [batch]
----

.Parameters used by the Memory Limiter Processor
[options="header"]
[cols="l, a, a"]
|===
|Parameter |Description |Default

|check_interval
|Time between memory usage measurements. The optimal value is `1s`. For spiky traffic patterns, you can decrease the `check_interval` or increase the `spike_limit_mib`.
|`0s`

|limit_mib
|The hard limit, which is the maximum amount of memory in MiB allocated on the heap. Typically, the total memory usage of the OpenTelemetry Collector is about 50 MiB greater than this value.
|`0`

|spike_limit_mib
|Spike limit, which is the maximum expected spike of memory usage in MiB. The optimal value is approximately 20% of `limit_mib`. To calculate the soft limit, subtract the `spike_limit_mib` from the `limit_mib`.
|20% of `limit_mib`

|limit_percentage
|Same as the `limit_mib` but expressed as a percentage of the total available memory. The `limit_mib` setting takes precedence over this setting.
|`0`

|spike_limit_percentage
|Same as the `spike_limit_mib` but expressed as a percentage of the total available memory. Intended to be used with the `limit_percentage` setting.
|`0`

|===

[id="resource-detection-processor_{context}"]
=== Resource Detection Processor

The Resource Detection Processor is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Resource Detection Processor identifies host resource details in alignment with OpenTelemetry's resource semantic standards. Using the detected information, this processor can add or replace the resource values in telemetry data. This processor supports traces and metrics. You can use this processor with multiple detectors such as the Docket metadata detector or the `OTEL_RESOURCE_ATTRIBUTES` environment variable detector.

.{product-title} permissions required for the Resource Detection Processor
[source,yaml]
----
kind: ClusterRole
metadata:
  name: otel-collector
rules:
- apiGroups: ["config.openshift.io"]
  resources: ["infrastructures", "infrastructures/status"]
  verbs: ["get", "watch", "list"]
----

.OpenTelemetry Collector using the Resource Detection Processor
[source,yaml]
----
  config: |
    processor:
      resourcedetection:
        detectors: [openshift]
        override: true
    service:
      pipelines:
        traces:
          processors: [resourcedetection]
        metrics:
          processors: [resourcedetection]
----

.OpenTelemetry Collector using the Resource Detection Processor with an environment variable detector
[source,yaml]
----
  config: |
    processors:
      resourcedetection/env:
        detectors: [env] # <1>
        timeout: 2s
        override: false
----
<1> Specifies which detector to use. In this example, the environment detector is specified.

[id="attributes-processor_{context}"]
=== Attributes Processor

The Attributes Processor is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Attributes Processor can modify attributes of a span, log, or metric. You can configure this processor to filter and match input data and include or exclude such data for specific actions.

This processor operates on a list of actions, executing them in the order specified in the configuration. The following actions are supported:

Insert:: Inserts a new attribute into the input data when the specified key does not already exist.

Update:: Updates an attribute in the input data if the key already exists.

Upsert:: Combines the insert and update actions: Inserts a new attribute if the key does not exist yet. Updates the attribute if the key already exists.

Delete:: Removes an attribute from the input data.

Hash:: Hashes an existing attribute value as SHA1.

Extract:: Extracts values by using a regular expression rule from the input key to the target keys defined in the rule. If a target key already exists, it is overridden similarly to the Span Processor's `to_attributes` setting with the existing attribute as the source.

Convert:: Converts an existing attribute to a specified type.

.OpenTelemetry Collector using the Attributes Processor
[source,yaml]
----
  config: |
    processors:
      attributes/example:
        actions:
          - key: db.table
            action: delete
          - key: redacted_span
            value: true
            action: upsert
          - key: copy_key
            from_attribute: key_original
            action: update
          - key: account_id
            value: 2245
            action: insert
          - key: account_password
            action: delete
          - key: account_email
            action: hash
          - key: http.status_code
            action: convert
            converted_type: int
----

[id="resource-processor_{context}"]
=== Resource Processor

The Resource Processor is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Resource Processor applies changes to the resource attributes. This processor supports traces, metrics, and logs.

.OpenTelemetry Collector using the Resource Detection Processor
[source,yaml]
----
  config: |
    processor:
      attributes:
      - key: cloud.availability_zone
        value: "zone-1"
        action: upsert
      - key: k8s.cluster.name
        from_attribute: k8s-cluster
        action: insert
      - key: redundant-attribute
        action: delete
----

Attributes represent the actions that are applied to the resource attributes, such as delete the attribute, insert the attribute, or upsert the attribute.

[id="span-processor_{context}"]
=== Span Processor

The Span Processor is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Span Processor modifies the span name based on its attributes or extracts the span attributes from the span name. This processor can also change the span status and include or exclude spans. This processor supports traces.

Span renaming requires specifying attributes for the new name by using the `from_attributes` configuration.

.OpenTelemetry Collector using the Span Processor for renaming a span
[source,yaml]
----
  config: |
    processor:
      span:
        name:
          from_attributes: [<key1>, <key2>, ...] # <1>
          separator: <value> # <2>
----
<1> Defines the keys to form the new span name.
<2> An optional separator.

You can use this processor to extract attributes from the span name.

.OpenTelemetry Collector using the Span Processor for extracting attributes from a span name
[source,yaml]
----
  config: |
    processor:
      span/to_attributes:
        name:
          to_attributes:
            rules:
              - ^\/api\/v1\/document\/(?P<documentId>.*)\/update$ # <1>
----
<1> This rule defines how the extraction is to be executed. You can define more rules: for example, in this case, if the regular expression matches the name, a `documentID` attibute is created. In this example, if the input span name is `/api/v1/document/12345678/update`, this results in the `/api/v1/document/{documentId}/update` output span name, and a new `"documentId"="12345678"` attribute is added to the span.

You can have the span status modified.

.OpenTelemetry Collector using the Span Processor for status change
[source,yaml]
----
  config: |
    processor:
      span/set_status:
        status:
          code: Error
          description: "<error_description>"
----

[id="kubernetes-attributes-processor_{context}"]
=== Kubernetes Attributes Processor

The Kubernetes Attributes Processor is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Kubernetes Attributes Processor enables automatic configuration of spans, metrics, and log resource attributes by using the Kubernetes metadata.
This processor supports traces, metrics, and logs.
This processor automatically identifies the Kubernetes resources, extracts the metadata from them, and incorporates this extracted metadata as resource attributes into relevant spans, metrics, and logs. It utilizes the Kubernetes API to discover all pods operating within a cluster, maintaining records of their IP addresses, pod UIDs, and other relevant metadata. 

.Minimum {product-title} permissions required for the Kubernetes Attributes Processor
[source,yaml]
----
kind: ClusterRole
metadata:
  name: otel-collector
rules:
  - apiGroups: ['']
    resources: ['pods', 'namespaces']
    verbs: ['get', 'watch', 'list']
----

.OpenTelemetry Collector using the Kubernetes Attributes Processor
[source,yaml]
----
  config: |
    processors:
         k8sattributes:
             filter:
                 node_from_env_var: KUBE_NODE_NAME
----

[id="filter-processor_{context}"]
=== Filter Processor

The Filter Processor is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Filter Processor leverages the OpenTelemetry Transformation Language to establish criteria for discarding telemetry data. If any of these conditions are satisfied, the telemetry data are discarded. You can combine the conditions by using the logical OR operator. This processor supports traces, metrics, and logs.

.OpenTelemetry Collector custom resource with an enabled OTLP Exporter
[source,yaml]
----
config: |
  processors:
    filter/ottl:
      error_mode: ignore # <1>
      traces:
        span:
          - 'attributes["container.name"] == "app_container_1"' # <2>
          - 'resource.attributes["host.name"] == "localhost"' # <3>
----
<1> Defines the error mode. When set to `ignore`, ignores errors returned by conditions. When set to `propagate`, returns the error up the pipeline. An error causes the payload to be dropped from the Collector.
<2> Filters the spans that have the `container.name == app_container_1` attribute.
<3> Filters the spans that have the `host.name == localhost` resource attribute.

[id="routing-processor_{context}"]
=== Routing Processor

The Routing Processor is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Routing Processor routes logs, metrics, or traces to specific exporters. This processor can read a header from an incoming gRPC or plain HTTP request or read a resource attribute, and then direct the trace information to relevant exporters according to the read value.

.OpenTelemetry Collector custom resource with an enabled OTLP Exporter
[source,yaml]
----
config: |
  processors:
    routing:
      from_attribute: X-Tenant # <1>
      default_exporters: # <2>
      - jaeger
      table: # <3>
      - value: acme
        exporters: [jaeger/acme]
  exporters:
    jaeger:
      endpoint: localhost:14250
    jaeger/acme:
      endpoint: localhost:24250
----
<1> The HTTP header name for the lookup value when performing the route.
<2> The default exporter when the attribute value is not present in the table in the next section.
<3> The table that defines which values are to be routed to which exporters.

You can optionally create an `attribute_source` configuration, which defines where to look for the attribute in `from_attribute`. The allowed value is `context` to search the context, which includes the HTTP headers, or `resource` to search the resource attributes.

[id="cumulativetodelta-processor_{context}"]
=== Cumulative to Delta Processor

This processor converts monotonic, cumulative-sum, and histogram metrics to monotonic delta metrics.

You can filter metrics by using the `include:` or `exclude:` fields and specifying the `strict` or `regexp` metric name matching.

This processor does not convert non-monotonic sums and exponential histograms.

:FeatureName: The Cumulative to Delta Processor
include::snippets/technology-preview.adoc[]

.Example of an OpenTelemetry Collector custom resource with an enabled Cumulative to Delta Processor
[source,yaml]
----
config: |
  processors:
    cumulativetodelta:
      include: # <1>
        match_type: strict # <2>
        metrics: # <3>
        - <metric_1_name>
        - <metric_2_name>
      exclude: # <4>
        match_type: regexp
        metrics:
        - "<regular_expression_for_metric_names>"
----
<1> Optional: Configures which metrics to include. When omitted, all metrics, except for those listed in the `exclude` field, are converted to delta metrics.
<2> Defines a value provided in the `metrics` field as a `strict` exact match or `regexp` regular expression.
<3> Lists the metric names, which are exact matches or matches for regular expressions, of the metrics to be converted to delta metrics. If a metric matches both the `include` and `exclude` filters, the `exclude` filter takes precedence.
<4> Optional: Configures which metrics to exclude. When omitted, no metrics are excluded from conversion to delta metrics.

[id="exporters_{context}"]
== Exporters

Exporters send data to one or more back ends or destinations.

[id="otlp-exporter_{context}"]
=== OTLP Exporter

The OTLP gRPC Exporter exports traces and metrics by using the OpenTelemetry protocol (OTLP).

.OpenTelemetry Collector custom resource with an enabled OTLP Exporter
[source,yaml]
----
  config: |
    exporters:
      otlp:
        endpoint: tempo-ingester:4317 # <1>
        tls: # <2>
          ca_file: ca.pem
          cert_file: cert.pem
          key_file: key.pem
          insecure: false # <3>
          insecure_skip_verify: false # # <4>
          reload_interval: 1h # <5>
          server_name_override: <name> # <6>
        headers: # <7>
          X-Scope-OrgID: "dev"
    service:
      pipelines:
        traces:
          exporters: [otlp]
        metrics:
          exporters: [otlp]
----
<1> The OTLP gRPC endpoint. If the `+https://+` scheme is used, then client transport security is enabled and overrides the `insecure` setting in the `tls`.
<2> The client-side TLS configuration. Defines paths to TLS certificates.
<3> Disables client transport security when set to `true`. The default value is `false` by default.
<4> Skips verifying the certificate when set to `true`. The default value is `false`.
<5> Specifies the time interval at which the certificate is reloaded. If the value is not set, the certificate is never reloaded. The `reload_interval` accepts a string containing valid units of time such as `ns`, `us` (or `Âµs`), `ms`, `s`, `m`, `h`.
<6> Overrides the virtual host name of authority such as the authority header field in requests. You can use this for testing.
<7> Headers are sent for every request performed during an established connection.

[id="otlp-http-exporter_{context}"]
=== OTLP HTTP Exporter

The OTLP HTTP Exporter exports traces and metrics by using the OpenTelemetry protocol (OTLP).

.OpenTelemetry Collector custom resource with an enabled OTLP Exporter
[source,yaml]
----
  config: |
    exporters:
      otlphttp:
        endpoint: http://tempo-ingester:4318 # <1>
        tls: # <2>
        headers: # <3>
          X-Scope-OrgID: "dev"
        disable_keep_alives: false <4>

    service:
      pipelines:
        traces:
          exporters: [otlphttp]
        metrics:
          exporters: [otlphttp]
----
<1> The OTLP HTTP endpoint. If the `+https://+` scheme is used, then client transport security is enabled and overrides the `insecure` setting in the `tls`.
<2> The client side TLS configuration. Defines paths to TLS certificates.
<3> Headers are sent in every HTTP request.
<4> If true, disables HTTP keep-alives. It will only use the connection to the server for a single HTTP request.

[id="debug-exporter_{context}"]
=== Debug Exporter

The Debug Exporter prints traces and metrics to the standard output.

.OpenTelemetry Collector custom resource with an enabled Debug Exporter
[source,yaml]
----
  config: |
    exporters:
      debug:
        verbosity: detailed # <1>
    service:
      pipelines:
        traces:
          exporters: [logging]
        metrics:
          exporters: [logging]
----
<1> Verbosity of the debug export: `detailed` or `normal` or `basic`. When set to `detailed`, pipeline data is verbosely logged. Defaults to `normal`.

[id="load-balancing-exporter_{context}"]
=== Load Balancing Exporter

The Load Balancing Exporter consistently exports spans, metrics, and logs according to the `routing_key` configuration.

:FeatureName: The Load Balancing Exporter
include::snippets/technology-preview.adoc[leveloffset=+1]

.OpenTelemetry Collector custom resource with an enabled Load Balancing Exporter
[source,yaml]
----
# ...
  config: |
    exporters:
      loadbalancing:
        routing_key: "service" # <1>
        protocol:
          otlp: # <2>
            timeout: 1s
        resolver: # <3>
          static: # <4>
            hostnames:
            - backend-1:4317
            - backend-2:4317
          dns: # <5>
            hostname: otelcol-headless.observability.svc.cluster.local
          k8s: # <6>
            service: lb-svc.kube-public
            ports:
              - 15317
              - 16317
# ...
----
<1> The `routing_key: service` exports spans for the same service name to the same Collector instance to provide accurate aggregation. The `routing_key: traceID` exports spans based on their `traceID`. The implicit default is `traceID` based routing.
<2> The OTLP is the only supported load-balancing protocol. All options of the OTLP exporter are supported.
<3> You can configure only one resolver.
<4> The static resolver distributes the load across the listed endpoints.
<5> You can use the DNS resolver only with a Kubernetes headless service.
<6> The Kubernetes resolver is recommended.

[id="prometheus-exporter_{context}"]
=== Prometheus Exporter

The Prometheus Exporter is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Prometheus Exporter exports metrics in the Prometheus or OpenMetrics formats.

.OpenTelemetry Collector custom resource with an enabled Prometheus Exporter
[source,yaml]
----
  ports:
  - name: promexporter # <1>
    port: 8889
    protocol: TCP
  config: |
    exporters:
      prometheus:
        endpoint: 0.0.0.0:8889 # <2>
        tls: # <3>
          ca_file: ca.pem
          cert_file: cert.pem
          key_file: key.pem
        namespace: prefix # <4>
        const_labels: # <5>
          label1: value1
        enable_open_metrics: true # <6>
        resource_to_telemetry_conversion: # <7>
          enabled: true
        metric_expiration: 180m # <8>
        add_metric_suffixes: false # <9>
    service:
      pipelines:
        metrics:
          exporters: [prometheus]
----
<1> Exposes the Prometheus port from the Collector pod and service. You can enable scraping of metrics by Prometheus by using the port name in `ServiceMonitor` or `PodMonitor` custom resource.
<2> The network endpoint where the metrics are exposed.
<3> The server-side TLS configuration. Defines paths to TLS certificates.
<4> If set, exports metrics under the provided value. No default.
<5> Key-value pair labels that are applied for every exported metric. No default.
<6> If `true`, metrics are exported using the OpenMetrics format. Exemplars are only exported in the OpenMetrics format and only for histogram and monotonic sum metrics such as `counter`. Disabled by default.
<7> If `enabled` is `true`, all the resource attributes are converted to metric labels by default. Disabled by default.
<8> Defines how long metrics are exposed without updates. The default is `5m`.
<9> Adds the metrics types and units suffixes. Must be disabled if the monitor tab in Jaeger console is enabled. The default is `true`.

[id="kafka-exporter_{context}"]
=== Kafka Exporter

The Kafka Exporter is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Kafka Exporter exports logs, metrics, and traces to Kafka. This exporter uses a synchronous producer that blocks and does not batch messages. You must use it with batch and queued retry processors for higher throughput and resiliency.

.OpenTelemetry Collector custom resource with an enabled Kafka Exporter
[source,yaml]
----
  config: |
    exporters:
      kafka:
        brokers: ["localhost:9092"] # <1>
        protocol_version: 2.0.0 # <2>
        topic: otlp_spans # <3>
        auth:
          plain_text: # <4>
            username: example
            password: example
          tls: # <5>
            ca_file: ca.pem
            cert_file: cert.pem
            key_file: key.pem
            insecure: false # <6>
            server_name_override: kafka.example.corp # <7>
    service:
      pipelines:
        traces:
          exporters: [kafka]
----
<1> The list of Kafka brokers. The default is `+localhost:9092+`.
<2> The Kafka protocol version. For example, `+2.0.0+`. This is a required field.
<3> The name of the Kafka topic to read from. The following are the defaults: `+otlp_spans+` for traces, `+otlp_metrics+` for metrics, `+otlp_logs+` for logs.
<4> The plaintext authentication configuration. If omitted, plaintext authentication is disabled.
<5> The client-side TLS configuration. Defines paths to the TLS certificates. If omitted, TLS authentication is disabled.
<6> Disables verifying the server's certificate chain and host name. The default is `+false+`.
<7> ServerName indicates the name of the server requested by the client to support virtual hosting.

[id="connectors_{context}"]
== Connectors

Connectors connect two pipelines.

[id="forward-connector_{context}"]
=== Forward Connector

:FeatureName: The Forward Connector
include::snippets/technology-preview.adoc[]

The Forward Connector merges two pipelines of the same type.

.OpenTelemetry Collector custom resource with an enabled Forward Connector
[source,yaml]
----
receivers:
  otlp:
    protocols:
      grpc:
  jaeger:
    protocols:
      grpc:
processors:
  batch:
exporters:
  otlp:
    endpoint: tempo-simplest-distributor:4317
    tls:
      insecure: true
connectors:
  forward:
service:
  pipelines:
    traces/regiona:
      receivers: [otlp]
      processors: []
      exporters: [forward]
    traces/regionb:
      receivers: [jaeger]
      processors: []
      exporters: [forward]
    traces:
      receivers: [forward]
      processors: [batch]
      exporters: [otlp]
----

[id="spanmetrics-connector_{context}"]
=== Spanmetrics Connector

The Spanmetrics Connector is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Spanmetrics Connector aggregates Request, Error, and Duration (R.E.D) OpenTelemetry metrics from span data.

.OpenTelemetry Collector custom resource with an enabled Spanmetrics Connector
[source,yaml]
----
  config: |
    connectors:
      spanmetrics:
        metrics_flush_interval: 15s # <1>
    service:
      pipelines:
        traces:
          exporters: [spanmetrics]
        metrics:
          receivers: [spanmetrics]
----
<1> Defines the flush interval of the generated metrics. Defaults to `15s`.

[id="extensions_{context}"]
== Extensions

Extensions add capabilities to the Collector.

[id="bearertokenauth-extension_{context}"]
=== BearerTokenAuth Extension

The BearerTokenAuth Extension is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The BearerTokenAuth Extension is an authenticator for receivers and exporters that are based on the HTTP and the gRPC protocol.
You can use the OpenTelemetry Collector custom resource to configure client authentication and server authentication for the BearerTokenAuth Extension on the receiver and exporter side.
This extension supports traces, metrics, and logs.

.OpenTelemetry Collector custom resource with client and server authentication configured for the BearerTokenAuth Extension
[source,yaml]
----
  config: |
    extensions:
      bearertokenauth:
        scheme: "Bearer" # <1>
        token: "<token>" # <2>
        filename: "<token_file>" # <3>

    receivers:
      otlp:
        protocols:
          http:
            auth:
              authenticator: bearertokenauth # <4>
    exporters:
      otlp:
        auth:
          authenticator: bearertokenauth # <5>

    service:
      extensions: [bearertokenauth]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> You can configure the BearerTokenAuth Extension to send a custom `scheme`. The default is `Bearer`.
<2> You can add the BearerTokenAuth Extension token as metadata to identify a message.
<3> Path to a file that contains an authorization token that is transmitted with every message.
<4> You can assign the authenticator configuration to an OTLP Receiver.
<5> You can assign the authenticator configuration to an OTLP Exporter.

[id="oauth2client-extension_{context}"]
=== OAuth2Client Extension

The OAuth2Client Extension is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The OAuth2Client Extension is an authenticator for exporters that are based on the HTTP and the gRPC protocol.
Client authentication for the OAuth2Client Extension is configured in a separate section in the OpenTelemetry Collector custom resource.
This extension supports traces, metrics, and logs.

.OpenTelemetry Collector custom resource with client authentication configured for the OAuth2Client Extension
[source,yaml]
----
  config: |
    extensions:
      oauth2client:
        client_id: <client_id> # <1>
        client_secret: <client_secret> # <2>
        endpoint_params: # <3>
          audience: <audience>
        token_url: https://example.com/oauth2/default/v1/token # <4>
        scopes: ["api.metrics"] # <5>
        # tls settings for the token client
        tls: # <6>
          insecure: true # <7>
          ca_file: /var/lib/mycert.pem # <8>
          cert_file: <cert_file> # <9>
          key_file: <key_file> # <10>
        timeout: 2s # <11>

    receivers:
      otlp:
        protocols:
          http: {}

    exporters:
      otlp:
        auth:
          authenticator: oauth2client # <12>

    service:
      extensions: [oauth2client]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> Client identifier, which is provided by the identity provider.
<2> Confidential key used to authenticate the client to the identity provider.
<3> Further metadata, in the key-value pair format, which is transferred during authentication. For example, `audience` specifies the intended audience for the access token, indicating the recipient of the token.
<4> The URL of the OAuth2 token endpoint, where the Collector requests access tokens.
<5> The scopes define the specific permissions or access levels requested by the client.
<6> The Transport Layer Security (TLS) settings for the token client, which is used to establish a secure connection when requesting tokens.
<7> When set to `true`, configures the Collector to use an insecure or non-verified TLS connection to call the configured token endpoint.
<8> The path to a Certificate Authority (CA) file that is used to verify the server's certificate during the TLS handshake.
<9> The path to the client certificate file that the client must use to authenticate itself to the OAuth2 server if required.
<10> The path to the client's private key file that is used with the client certificate if needed for authentication.
<11> Sets a timeout for the token client's request.
<12> You can assign the authenticator configuration to an OTLP exporter.

[id="filestorage-extension_{context}"]
=== File Storage Extension

:FeatureName: The File Storage Extension
include::snippets/technology-preview.adoc[]

The File Storage Extension supports traces, metrics, and logs. This extension can persist the state to the local file system. This extension persists the sending queue for the OTLP exporters that are based on the HTTP and the gRPC protocols. This extension requires the read and write access to a directory. This extension can use a default directory, but the default directory must already exist.

.OpenTelemetry Collector custom resource with a configured File Storage Extension that persists an OTLP sending queue
[source,yaml]
----
  config: |
    extensions:
      file_storage/all_settings:
        directory: /var/lib/otelcol/mydir # <1>
        timeout: 1s # <2>
        compaction:
          on_start: true # <3>
          directory: /tmp/ # <4>
          max_transaction_size: 65_536 # <5>
        fsync: false # <6>

    exporters:
      otlp:
        sending_queue:
          storage: file_storage/all_settings

    service:
      extensions: [file_storage/all_settings]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> Specifies the directory in which the telemetry data is stored.
<2> Specifies the timeout time interval for opening the stored files.
<3> Starts compaction when the Collector starts. If omitted, the default is `+false+`.
<4> Specifies the directory in which the compactor stores the telemetry data.
<5> Defines the maximum size of the compaction transaction. To ignore the transaction size, set to zero. If omitted, the default is `+65536+` bytes.
<6> When set, forces the database to perform an `fsync` call after each write operation. This helps to ensure database integrity if there is an interruption to the database process, but at the cost of performance.

[id="oidcauth-extension_{context}"]
=== OIDC Auth Extension

:FeatureName: The OIDC Auth Extension
include::snippets/technology-preview.adoc[]

The OIDC Auth Extension authenticates incoming requests to receivers by using the OpenID Connect (OIDC) protocol.
It validates the ID token in the authorization header against the issuer and updates the authentication context of the incoming request.

.OpenTelemetry Collector custom resource with the configured OIDC Auth Extension
[source,yaml]
----
  config: |
    extensions:
      oidc:
        attribute: authorization # <1>
        issuer_url: https://example.com/auth/realms/opentelemetry # <2>
        issuer_ca_path: /var/run/tls/issuer.pem # <3>
        audience: otel-collector # <4>
        username_claim: email # <5>
    receivers:
      otlp:
        protocols:
          grpc:
            auth:
              authenticator: oidc
    exporters:
      otlp:
        endpoint: <endpoint>
    service:
      extensions: [oidc]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> The name of the header that contains the ID token. The default name is `authorization`.
<2> The base URL of the OIDC provider.
<3> Optional: The path to the issuer's CA certificate.
<4> The audience for the token.
<5> The name of the claim that contains the username. The default name is `sub`.

[id="jaegerremotesampling-extension_{context}"]
=== Jaeger Remote Sampling Extension

The Jaeger Remote Sampling Extension is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Jaeger Remote Sampling Extension enables serving sampling strategies after Jaeger's remote sampling API. You can configure this extension to proxy requests to a backing remote sampling server such as a Jaeger collector down the pipeline or to a static JSON file from the local file system.

.OpenTelemetry Collector custom resource with a configured Jaeger Remote Sampling Extension
[source,yaml]
----
  config: |
    extensions:
      jaegerremotesampling:
        source:
          reload_interval: 30s # <1>
          remote:
            endpoint: jaeger-collector:14250 # <2>
          file: /etc/otelcol/sampling_strategies.json # <3>

    receivers:
      otlp:
        protocols:
          http: {}

    exporters:
      otlp:

    service:
      extensions: [jaegerremotesampling]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> The time interval at which the sampling configuration is updated.
<2> The endpoint for reaching the Jaeger remote sampling strategy provider.
<3> The path to a local file that contains a sampling strategy configuration in the JSON format.

.Example of a Jaeger Remote Sampling strategy file
[source,json]
----
{
  "service_strategies": [
    {
      "service": "foo",
      "type": "probabilistic",
      "param": 0.8,
      "operation_strategies": [
        {
          "operation": "op1",
          "type": "probabilistic",
          "param": 0.2
        },
        {
          "operation": "op2",
          "type": "probabilistic",
          "param": 0.4
        }
      ]
    },
    {
      "service": "bar",
      "type": "ratelimiting",
      "param": 5
    }
  ],
  "default_strategy": {
    "type": "probabilistic",
    "param": 0.5,
    "operation_strategies": [
      {
        "operation": "/health",
        "type": "probabilistic",
        "param": 0.0
      },
      {
        "operation": "/metrics",
        "type": "probabilistic",
        "param": 0.0
      }
    ]
  }
}
----


[id="pprof-extension_{context}"]
=== Performance Profiler Extension

The Performance Profiler Extension is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Performance Profiler Extension enables the Go `net/http/pprof` endpoint. Developers use this extension to collect performance profiles and investigate issues with the service.

.OpenTelemetry Collector custom resource with the configured Performance Profiler Extension
[source,yaml]
----
  config: |
    extensions:
      pprof:
        endpoint: localhost:1777 # <1>
        block_profile_fraction: 0 # <2>
        mutex_profile_fraction: 0 # <3>
        save_to_file: test.pprof # <4>

    receivers:
      otlp:
        protocols:
          http: {}

    exporters:
      otlp:

    service:
      extensions: [pprof]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> The endpoint at which this extension listens. Use `localhost:` to make it available only locally or `":"` to make it available on all network interfaces. The default value is `localhost:1777`.
<2> Sets a fraction of blocking events to be profiled. To disable profiling, set this to `0` or a negative integer. See the link:https://golang.org/pkg/runtime/#SetBlockProfileRate[documentation] for the `runtime` package. The default value is `0`.
<3> Set a fraction of mutex contention events to be profiled. To disable profiling, set this to `0` or a negative integer. See the link:https://golang.org/pkg/runtime/#SetMutexProfileFraction[documentation] for the `runtime` package. The default value is `0`.
<4> The name of the file in which the CPU profile is to be saved. Profiling starts when the Collector starts. Profiling is saved to the file when the Collector is terminated.

[id="healthcheck-extension_{context}"]
=== Health Check Extension

The Health Check Extension is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Health Check Extension provides an HTTP URL for checking the status of the OpenTelemetry Collector. You can use this extension as a liveness and readiness probe on OpenShift.

.OpenTelemetry Collector custom resource with the configured Health Check Extension
[source,yaml]
----
  config: |
    extensions:
      health_check:
        endpoint: "0.0.0.0:13133" # <1>
        tls: # <2>
          ca_file: "/path/to/ca.crt"
          cert_file: "/path/to/cert.crt"
          key_file: "/path/to/key.key"
        path: "/health/status" # <3>
        check_collector_pipeline: # <4>
          enabled: true # <5>
          interval: "5m" # <6>
          exporter_failure_threshold: 5 # <7>

    receivers:
      otlp:
        protocols:
          http: {}

    exporters:
      otlp:

    service:
      extensions: [health_check]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> The target IP address for publishing the health check status. The default is `0.0.0.0:13133`.
<2> The TLS server-side configuration. Defines paths to TLS certificates. If omitted, the TLS is disabled.
<3> The path for the health check server. The default is `/`.
<4> Settings for the Collector pipeline health check.
<5> Enables the Collector pipeline health check. The default is `false`.
<6> The time interval for checking the number of failures. The default is `5m`.
<7> The threshold of a number of failures until which a container is still marked as healthy. The default is `5`.

[id="memory-ballast-extension_{context}"]
=== Memory Ballast Extension

The Memory Ballast Extension is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The Memory Ballast Extension enables applications to configure memory ballast for the process.

.OpenTelemetry Collector custom resource with the configured Memory Ballast Extension
[source,yaml]
----
  config: |
    extensions:
      memory_ballast:
        size_mib: 64 # <1>
        size_in_percentage: 20 # <2>

    receivers:
      otlp:
        protocols:
          http: {}

    exporters:
      otlp:

    service:
      extensions: [memory_ballast]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> Sets the memory ballast size in MiB. Takes priority over the `size_in_percentage` if both are specified.
<2> Sets the memory ballast as a percentage, `1`-`100`, of the total memory. Supports containerized and physical host environments.


[id="zpages-extension_{context}"]
=== zPages Extension

The zPages Extension is currently a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature only.

The zPages Extension provides an HTTP endpoint for extensions that serve zPages. At the endpoint, this extension serves live data for debugging instrumented components. All core exporters and receivers provide some zPages instrumentation.

zPages are useful for in-process diagnostics without having to depend on a back end to examine traces or metrics.

.OpenTelemetry Collector custom resource with the configured zPages Extension
[source,yaml]
----
  config: |
    extensions:
      zpages:
        endpoint: "localhost:55679" # <1>

    receivers:
      otlp:
        protocols:
          http: {}
    exporters:
      otlp:

    service:
      extensions: [zpages]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----

<1> Specifies the HTTP endpoint that serves zPages. Use `localhost:` to make it available only locally, or `":"` to make it available on all network interfaces. The default is `localhost:55679`.
