// Module included in the following assemblies:
//
// * observability/monitoring/troubleshooting-monitoring-issues.adoc

:_mod-docs-content-type: PROCEDURE
[id="troubleshooting-remote-write-configuration_{context}"]
= Troubleshooting remote write configuration

// Set attributes to distinguish between cluster monitoring example (core platform monitoring - CPM) and user workload monitoring (UWM) examples.
// tag::CPM[]
:namespace-name: openshift-monitoring
:pod: prometheus-k8s-0
// end::CPM[]
// tag::UWM[]
:namespace-name: openshift-user-workload-monitoring
:pod: prometheus-user-workload-0
// end::UWM[]

If your remote write configuration does not work properly, you can verify the running configuration from the Prometheus API and inspect metrics to discover other possible issues.

.Prerequisites

ifndef::openshift-dedicated,openshift-rosa-hcp,openshift-rosa[]
* You have access to the cluster as a user with the `cluster-admin` cluster role.
endif::openshift-dedicated,openshift-rosa-hcp,openshift-rosa[]
ifdef::openshift-dedicated,openshift-rosa-hcp,openshift-rosa[]
* You have access to the cluster as a user with the `dedicated-admin` role.
endif::openshift-dedicated,openshift-rosa-hcp,openshift-rosa[]
* You have configured remote write storage.
* You have installed the {oc-first}.

.Procedure

. Verify the running remote write configuration from the Prometheus API:
// tag::CPM[]

** To verify remote write configuration for core platform monitoring:
+
[source,terminal]
----
$ oc exec prometheus-k8s-0 -c prometheus -n openshift-monitoring -- curl -s 'http://localhost:9090/api/v1/status/config' 
----
** To verify remote write configuration for user workload monitoring:
// end::CPM[]
+
[source,terminal]
----
$ oc exec prometheus-user-workload-0 -c prometheus -n openshift-user-workload-monitoring -- curl -s 'http://localhost:9090/api/v1/status/config' 
----
+
.The formatted example output
[source,terminal]
----
...
remote_write:
- url: https://remote-write-endpoint.example.com
  remote_timeout: 30s
  write_relabel_configs:
  - separator: ;
    target_label: __tmp_openshift_cluster_id__
    replacement: 0b02e767-c309-41e9-8727-03bb50f0fc89
    action: replace
  - separator: ;
    regex: __tmp_openshift_cluster_id__
    replacement: $1
    action: labeldrop
  protobuf_message: prometheus.WriteRequest
  authorization:
    type: Bearer
    credentials: <secret>
  follow_redirects: true
  enable_http2: true
  queue_config:
    capacity: 10000
    max_shards: 50
    min_shards: 1
    max_samples_per_send: 2000
    batch_send_deadline: 5s
    min_backoff: 30ms
    max_backoff: 5s
  metadata_config:
    send: true
    send_interval: 1m
    max_samples_per_send: 2000
...
----
+
[NOTE]
====
The formatted example output uses a filtering tool, such as `jq`, to provide the formatted indented JSON. See the link:https://stedolan.github.io/jq/manual/[jq Manual] (jq documentation) for more information about using `jq`.
====

. Check Prometheus pod logs to see if there are any errors:
+
[source,terminal,subs="attributes+"]
----
$ oc -n {namespace-name} logs -c prometheus <prometheus_pod>
----
+
[NOTE]
====
For multi-node clusters, ensure that you check all Prometheus pods and their logs.
====
+
.Example command
[source,terminal,subs="attributes+"]
----
$ oc -n {namespace-name} logs -c prometheus {pod}
----
+
.Example output
[source,terminal]
----
...
ts=2025-04-17T14:28:35.584Z caller=dedupe.go:112 component=remote level=warn remote_name=abc123 url=https://remote-write.endpoint msg="Failed to send batch, retrying" err="Post \"https://remote-write.endpoint\": dial tcp: lookup remote-write.endpoint on 74.40.100.145: no such host"
...
----

. From the *Administrator* perspective of the {product-title} web console, go to *Observe* -> *Metrics* and query the relevant remote write metrics:
+
[NOTE]
====
For more information about the metrics used in the following steps, see: "Table of remote write metrics".
====

.. Query the `prometheus_remote_storage_samples_retried_total` metric. If you see a steady high rate for this metric, reduce throughput of remote write to reduce load on the endpoint.

.. Query the `prometheus_remote_storage_highest_timestamp_in_seconds` and `prometheus_remote_storage_queue_highest_sent_timestamp_seconds` metrics to see how far behind remote write is for an endpoint.

.. Query the `prometheus_wal_watcher_current_segment` and `prometheus_tsdb_wal_segment_current` metrics to see if the values are the same. If you see a significant gap between the two values, it could mean that remote write is falling behind.

.. Query the `prometheus_remote_storage_shards` and `prometheus_remote_storage_shards_max` metrics to see if you are running the maximum number of shards.

.. Query the `prometheus_remote_storage_shards_desired` metric to see if its value is greater than the `prometheus_remote_storage_shards_max` metric value.

. Based on the values of the metrics you queued, tune the remote write configuration. For more information, see "Example remote write queue configuration" in _Configuring metrics_.


