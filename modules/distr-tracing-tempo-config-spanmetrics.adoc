// Module included in the following assemblies:
//
// * observability/distr_tracing/distr_tracing_tempo/distr-tracing-tempo-configuring.adoc

:_mod-docs-content-type: REFERENCE
[id="distr-tracing-tempo-config-spanmetrics_{context}"]
= Configuration of the monitor tab in Jaeger UI

Trace data contains rich information, and the data is normalized across instrumented languages and frameworks.
Therefore, request rate, error, and duration (RED) metrics can be extracted from traces.
The metrics can be visualized in Jaeger console in the *Monitor* tab.

The metrics are derived from spans in the OpenTelemetry Collector that are scraped from the Collector by the Prometheus deployed in the user-workload monitoring stack.
The Jaeger UI queries these metrics from the Prometheus endpoint and visualizes them.

[id="distr-tracing-tempo-config-spanmetrics_opentelemetry-collector-configuration_{context}"]
== OpenTelemetry Collector configuration

The OpenTelemetry Collector requires configuration of the `spanmetrics` connector that derives metrics from traces and exports the metrics in the Prometheus format.

.OpenTelemetry Collector custom resource for span RED
[source,yaml]
----
kind: OpenTelemetryCollector
apiVersion: opentelemetry.io/v1alpha1
metadata:
  name: otel
spec:
  mode: deployment
  observability:
    metrics:
      enableMetrics: true # <1>
  config: |
    connectors:
      spanmetrics: # <2>
        metrics_flush_interval: 15s

    receivers:
      otlp: # <3>
        protocols:
          grpc:
          http:

    exporters:
      prometheus: # <4>
        endpoint: 0.0.0.0:8889
        add_metric_suffixes: false
        resource_to_telemetry_conversion:
          enabled: true # by default resource attributes are dropped

      otlp:
        endpoint: "tempo-simplest-distributor:4317"
        tls:
          insecure: true

    service:
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp, spanmetrics] # <5>
        metrics:
          receivers: [spanmetrics] # <6>
          exporters: [prometheus]
----
<1> Creates the `ServiceMonitor` custom resource to enable scraping of the Prometheus exporter.
<2> The Spanmetrics connector receives traces and exports metrics.
<3> The OTLP receiver to receive spans in the OpenTelemetry protocol.
<4> The Prometheus exporter is used to export metrics in the Prometheus format.
<5> The Spanmetrics connector is configured as exporter in traces pipeline.
<6> The Spanmetrics connector is configured as receiver in metrics pipeline.

[id="distr-tracing-tempo-config-spanmetrics_tempo-configuration_{context}"]
== Tempo configuration

The `TempoStack` custom resource must specify the following: the *Monitor* tab is enabled, and the Prometheus endpoint is set to the Thanos querier service to query the data from the user-defined monitoring stack.

.TempoStack custom resource with the enabled Monitor tab
[source,yaml]
----
kind:  TempoStack
apiVersion: tempo.grafana.com/v1alpha1
metadata:
  name: simplest
spec:
  template:
    queryFrontend:
      jaegerQuery:
        enabled: true
        monitorTab:
          enabled: true # <1>
          prometheusEndpoint: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091 # <2>
        ingress:
          type: route
----
<1> Enables the monitoring tab in the Jaeger console.
<2> The service name for Thanos Querier from user-workload monitoring.

[id="distr-tracing-tempo-config-spanmetrics_span-red-metrics-and-alerting-rules_{context}"]
== Span RED metrics and alerting rules

The metrics generated by the `spanmetrics` connector are usable with alerting rules. For example, for alerts about a slow service or to define service level objectives (SLOs), the connector creates a `duration_bucket` histogram and the `calls` counter metric. These metrics have labels that identify the service, API name, operation type, and other attributes.

.Labels of the metrics created in the `spanmetrics` connector
[options="header"]
[cols="l, a, a"]
|===
|Label |Description |Values

|service_name
|Service name set by the `otel_service_name` environment variable.
|`frontend`

|span_name
| Name of the operation.
|
* `/`
* `/customer`

|span_kind
|Identifies the server, client, messaging, or internal operation.
|
* `SPAN_KIND_SERVER`
* `SPAN_KIND_CLIENT`
* `SPAN_KIND_PRODUCER`
* `SPAN_KIND_CONSUMER`
* `SPAN_KIND_INTERNAL`

|===

.Example `PrometheusRule` CR that defines an alerting rule for SLO when not serving 95% of requests within 2000ms on the front-end service
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: span-red
spec:
  groups:
  - name: server-side-latency
    rules:
    - alert: SpanREDFrontendAPIRequestLatency
      expr: histogram_quantile(0.95, sum(rate(duration_bucket{service_name="frontend", span_kind="SPAN_KIND_SERVER"}[5m])) by (le, service_name, span_name)) > 2000 # <1>
      labels:
        severity: Warning
      annotations:
        summary: "High request latency on {{$labels.service_name}} and {{$labels.span_name}}"
        description: "{{$labels.instance}} has 95th request latency above 2s (current value: {{$value}}s)"
----
<1> The expression for checking if 95% of the front-end server response time values are below 2000 ms. The time range (`[5m]`) must be at least four times the scrape interval and long enough to accommodate a change in the metric.
