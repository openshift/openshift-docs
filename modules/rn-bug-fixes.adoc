
[id="ocp-release-bug-fixes_{context}"]
= Bug fixes
//Bug fix work for TELCODOCS-750
//Bare Metal Hardware Provisioning / OS Image Provider
//Bare Metal Hardware Provisioning / baremetal-operator
//Bare Metal Hardware Provisioning / cluster-baremetal-operator
//Bare Metal Hardware Provisioning / ironic"
//CNF Platform Validation
//Cloud Native Events / Cloud Event Proxy
//Cloud Native Events / Cloud Native Events
//Cloud Native Events / Hardware Event Proxy
//Cloud Native Events
//Driver Toolkit
//Installer / Assisted installer
//Installer / OpenShift on Bare Metal IPI
//Networking / ptp
//Node Feature Discovery Operator
//Performance Addon Operator
//Telco Edge / HW Event Operator
//Telco Edge / RAN
//Telco Edge / Core

//Telco Edge / TALO
//Telco Edge / ZTP


//[id="ocp-release-note-api-auth-bug-fixes_{context}"]
//== API Server and Authentication

[id="ocp-release-note-bare-metal-hardware-bug-fixes_{context}"]
== Bare Metal Hardware Provisioning

* Before this update, when installing a dual-stack cluster on bare metal by using installer-provisioned infrastructure, the installation failed because the Virtual Media URL was IPv4 instead of IPv6. As IPv4 was unreachable, the bootstrap failed on the virtual machine (VM) and cluster nodes were not created. With this release, when you install a dual-stack cluster on bare metal for installer-provisioned infrastructure, the dual-stack cluster uses the Virtual Media URL IPv6 and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-60240[OCPBUGS-60240])

* Before this update, when installing a cluster with the bare metal as a service (BMaaS) API, an ambiguous validation error was reported. When you set an image URL without a checksum, BMaaS failed to validate the deployment image source information. With this release, when you do not provide a required checksum for an image, a clear message is reported. (link:https://issues.redhat.com/browse/OCPBUGS-57472[OCPBUGS-57472])

* Before this update, when installing a cluster using bare metal, if cleaning was not disabled, the hardware tried to delete any Software RAID configuration before it ran the `coreos-installer` tool. With this release, the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-56029[OCPBUGS-56029])

* Before this update, by using a Redfish system ID, such as `redfish://host/redfish/v1/` instead of `redfish://host/redfish/v1/Self`, in a Baseboard Management Console (BMC) URL, a registration error about an invalid JSON was reported. This issue was caused by a bug in the Bare Metal Operator (BMO). With this release, BMO now handles URLs without a Redfish system ID as a valid address without causing a JSON parsing issue. This fix improves the software handling of a missing Redfish system ID in BMC URLs. (link:https://issues.redhat.com/browse/OCPBUGS-55717[OCPBUGS-55717])

* Before this update, virtual media boot attempts sometimes failed because some models of SuperMicro such as `ars-111gl-nhr` used a different virtual media device string than other SuperMicro machines. With this release, an extra conditional check is added to sushy library code to check for the specific model affected and to adjust its behavior. As a result, Supermicro `ars-111gl-nhr` can boot from virtual media. (link:https://issues.redhat.com/browse/OCPBUGS-55434[OCPBUGS-55434])

* Before this update, RAM Disk logs did not include clear file separators, which occasionally caused the content to overlap on a single line. As a consequence, users could not parse RAM Disk logs. With this release, RAM Disk logs include clear file headers to indicate the boundary between the content of each file. As a result, the readability of RAM Disk logs for users is improved. (link:https://issues.redhat.com/browse/OCPBUGS-55381[OCPBUGS-55381])

* Before this update, during Ironic Python Agent (IPA) deployments, the RAM disk logs in the `metal3-ramdisk-logs` container did not include `NetworkManager` logs. The absence of `NetworkManager` logs hindered effective debugging, which affected network issue resolution. With this release, the existing RAM disk logs in the `metal3-ramdisk-logs` container of a metal3 pod include the entire journal from the host rather than just the `dmesg` and IPA logs. As result, IPA logs provide comprehensive `NetworkManager` data for improved debugging. (link:https://issues.redhat.com/browse/OCPBUGS-55350[OCPBUGS-55350])

* Before this update, when the provisioning network was disabled in the cluster configuration, you could create a bare-metal host with a driver that required a network boot, for example Intelligent Platform Management Interface (IPMI) or Redfish without virtual media. As a result, boot failures occurred during inspection or provisioning because the correct DHCP options could not be identified. With this release, when you create a bare-metal host in this scenario the host fails to register and the reported error references the disabled provisioning network. To create the host, you must enable the provisioning network or use a virtual-media-based driver, for example, Redfish virtual media. (link:https://issues.redhat.com/browse/OCPBUGS-54965[OCPBUGS-54965])

[id="ocp-release-note-cloud-compute-bug-fixes_{context}"]
== Cloud Compute

* Before this update, {aws-short} compute machine sets could include a null value for the `userDataSecret` parameter.
Using a null value sometimes caused machines to get stuck in the `Provisioning` state. With this release, the `userDataSecret` parameter requires a value.
(link:https://issues.redhat.com/browse/OCPBUGS-55135[OCPBUGS-55135])

* Before this update, {product-title} clusters on {aws-short} that were created with version 4.13 or earlier could not update to version 4.19.
Clusters that were created with version 4.14 and later have an {aws-short} `cloud-conf` ConfigMap by default, and this ConfigMap is required starting in {product-title} 4.19.
With this release, the Cloud Controller Manager Operator creates a default `cloud-conf` ConfigMap when none is present on the cluster.
This change enables clusters that were created with version 4.13 or earlier to update to version 4.19.
(link:https://issues.redhat.com/browse/OCPBUGS-59251[OCPBUGS-59251])

* Before this update, a `failed to find machine for node ...` appeared in the logs when the `InternalDNS` address for a machine was not set as expected.
As a consequence, the user might interpret this error as the machine not existing.
With this release, the log message reads `failed to find machine with InternalDNS matching ...`.
As a result, the user has a clearer indication of why the match is failing.
(link:https://issues.redhat.com/browse/OCPBUGS-19856[OCPBUGS-19856])

* Before this update, a bug fix altered the availability set configuration by changing the fault domain count to use the maximum available value instead of being fixed at 2.
This inadvertently caused scaling issues for compute machine sets that were created prior to the bug fix, because the controller attempted to modify immutable availability sets.
With this release, availability sets are no longer modified after creation, allowing affected compute machine sets to scale properly.
(link:https://issues.redhat.com/browse/OCPBUGS-56380[OCPBUGS-56380])

* Before this update, compute machine sets migrating from the Cluster API to the Machine API got stuck in the `Migrating` state.
As a consequence, the compute machine set could not finish transitioning to use a different authoritative API or perform further reconciliation of the `MachineSet` object status.
With this release, the migration controllers watch for changes in Cluster API resources and react to authoritative API transitions.
As a result, compute machine sets successfully transition from the Cluster API to the Machine API.
(link:https://issues.redhat.com/browse/OCPBUGS-56487[OCPBUGS-56487])

* Before this update, for the `maxUnhealthy` field in the `MachineHealthCheck` custom resource definition (CRD), it did not document the default value.
With this release, the CRD documents the default value.
(link:https://issues.redhat.com/browse/OCPBUGS-61314[OCPBUGS-61314])

* Before this update, it was possible to specify the use of the `CapacityReservationsOnly` capacity reservation behavior and Spot Instances in the same machine template.
As a consequence, machines with these two incompatible settings were created.
With this release, validation of machine templates ensures that these two incompatible settings are not used in the same machine template.
As a result, machines with these two incompatible settings cannot be created. (link:https://issues.redhat.com/browse/OCPBUGS-60943[OCPBUGS-60943])

* Before this update, on clusters that support migrating Machine API resources to Cluster API resources, deleting a nonauthoritative machine did not delete the corresponding authoritative machine.
As a consequence, orphaned machines that should have been cleaned up remained on the cluster and could cause a resource leak.
With this release, deleting a nonauthoritative machine triggers propagation of the deletion to the corresponding authoritative machine.
As a result, deletion requests on nonauthoritative machine correctly cascade, preventing orphaned authoritative machines and ensuring consistency in machine cleanup.
(link:https://issues.redhat.com/browse/OCPBUGS-55985[OCPBUGS-55985])

* Before this update, on clusters that support migrating Machine API resources to Cluster API resources, the {cluster-capi-operator} could create an authoritative Cluster API compute machine set in the `Paused` state.
As a consequence, the newly created Cluster API compute machine set could not reconcile or scale machines even though it was using the authoritative API.
With this release, the Operator now ensures that Cluster API compute machine sets are created in an unpaused state when the Cluster API is authoritative.
As a result, newly created Cluster API compute machine sets are reconciled immediately and scaling and machine lifecycle operations proceed as intended when the Cluster API is authoritative.
(link:https://issues.redhat.com/browse/OCPBUGS-56604[OCPBUGS-56604])

* Before this update, scaling large numbers of nodes was slow because scaling requires reconciling each machine several times and each machine was reconciled individually.
With this release, up to ten machines can be reconciled concurrently.
This change improves the processing speed for machines during scaling.
(link:https://issues.redhat.com/browse/OCPBUGS-59376[OCPBUGS-59376])

* Before this update, the {cluster-capi-operator} status controller used an unsorted list of related objects, leading to status updates when there were no functional changes.
As a consequence, users would see significant noise in the {cluster-capi-operator} object and in logs due to continuous and unnecessary status updates.
With this release, the status controller logic sorts the list of related objects before comparing them for changes.
As a result, a status update only occurs when there is a change to the Operator's state.
(link:https://issues.redhat.com/browse/OCPBUGS-56805[OCPBUGS-56805], link:https://issues.redhat.com/browse/OCPBUGS-58880[OCPBUGS-58880])

* Before this update, the `config-sync-controller` component of the Cloud Controller Manager Operator did not display logs.
The issue is resolved in this release.
(link:https://issues.redhat.com/browse/OCPBUGS-56508[OCPBUGS-56508])

* Before this update, the Control Plane Machine Set configuration used availability zones from compute machine sets.
This is not a valid configuration.
As a consequence, the Control Plane Machine Set could not be generated when the control plane machines were in a single zone while compute machine sets spanned multiple zones.
With this release, the Control Plane Machine Set derives an availability zone configuration from existing control plane machines.
As a result, the Control Plane Machine Set generates a valid zone configuration that accurately reflects the current control plane machines.
(link:https://issues.redhat.com/browse/OCPBUGS-52448[OCPBUGS-52448])

* Before this update, the controller that annotates a Machine API compute machine set did not check whether the Machine API was authoritative before adding scale-from-zero annotations.
As a consequence, the controller repeatedly added these annotations and caused a loop of continuous changes to the `MachineSet` object.
With this release, the controller checks the value of the `authoritativeAPI` field before adding scale-from-zero annotations.
As a result, the controller avoids the looping behavior by only adding these annotations to a Machine API compute machine set when the Machine API is authoritative.
(link:https://issues.redhat.com/browse/OCPBUGS-57581[OCPBUGS-57581])

* Before this update, the Machine API Operator attempted to reconcile `Machine` resources on platforms other than {aws-short} where the `.status.authoritativeAPI` field was not populated.
As a consequence, compute machines remained in the `Provisioning` state indefinitely and never became operational.
With this release, the Machine API Operator now populates the empty `.status.authoritativeAPI` field with the corresponding value in the machine specification.
A guard is also added to the controllers to handle cases where this field might still be empty.
As a result, `Machine` and `MachineSet` resources are reconciled properly and compute machines no longer remain in the `Provisioning` state indefinitely.
(link:https://issues.redhat.com/browse/OCPBUGS-56849[OCPBUGS-56849])

* Before this update, the Machine API Provider Azure used an old version of the Azure SDK, which used an old API version that did not support referencing a Capacity Reservation group.
As a consequence, creating a Machine API machine that referenced a Capacity Reservation group in another subscription resulted in an Azure API error.
With this release, the Machine API Provider Azure uses a version of the Azure SDK that supports this configuration.
As a result, creating a Machine API machine that references a Capacity Reservation group in another subscription works as expected.
(link:https://issues.redhat.com/browse/OCPBUGS-55372[OCPBUGS-55372])

* Before this update, the two-way synchronization controller on clusters that support migrating Machine API resources to Cluster API resources did not correctly compare the machine specification when converting an authoritative Cluster API machine template to a Machine API machine set.
As a consequence, changes to the Cluster API machine template specification were not synchronized to the Machine API machine set.
With this release, changes to the comparison logic resolve the issue.
As a result, the Machine API machine set synchronizes correctly after the Cluster API machine set references the new Cluster API machine template.
(link:https://issues.redhat.com/browse/OCPBUGS-56010[OCPBUGS-56010])

* Before this update, the two-way synchronization controller on clusters that support migrating Machine API resources to Cluster API resources did not delete the machine template when its corresponding Machine API machine set was deleted.
As a consequence, unneeded Cluster API machine templates persisted in the cluster and cluttered the `openshift-cluster-api` namespace.
With this release, the two-way synchronization controller correctly handles deletion synchronization for the machine template.
As a result, deleting a Machine API authoritative machine set deletes the corresponding Cluster API machine template.
(link:https://issues.redhat.com/browse/OCPBUGS-57195[OCPBUGS-57195])

* Before this update, the two-way synchronization controller on clusters that support migrating Machine API resources to Cluster API resources prematurely reported a successful migration.
As a consequence, if any errors occurred when updating the status of related objects, the operation was not retried.
With this release, the controller ensures that all related object statuses are written before reporting a successful status.
As a result, the controller handles errors during migration better.
(link:https://issues.redhat.com/browse/OCPBUGS-57040[OCPBUGS-57040])

[id="ocp-release-note-cloud-credential-operator-bug-fixes_{context}"]
== Cloud Credential Operator

* Before this update, the `ccoctl` command unnecessarily required the `baseDomainResourceGroupName` parameter when creating the OpenID Connect (OIDC) issuer and managed identities for a private cluster by using {entra-first}. As a consequence, an error displayed when `ccoctl` tried to create private clusters. With this release, the `baseDomainResourceGroupName` parameter is removed as a requirement. As a result, the process for creating a private cluster on {azure-full} is logical and consistent with expectations. (link:https://issues.redhat.com/browse/OCPBUGS-34993[OCPBUGS-34993])

[id="ocp-release-note-cluster-autoscaler-bug-fixes_{context}"]
== Cluster Autoscaler

* Before this update, the cluster autoscaler attempted to include machine objects that were in a deleting state. As a consequence, the cluster autoscaler count of machines was inaccurate. This issue caused the cluster autoscaler to add additional taints that were not needed. With this release, the autoscaler accurately counts the machines. (link:https://issues.redhat.com/browse/OCPBUGS-60035[OCPBUGS-60035])

* Before this update, when you created a cluster autoscaler object with the Cluster Autoscaler Operator enabled in the cluster, two `cluster-autoscaler-default` pods in the `openshift-machine-api` were sometimes created at the same time and one of the pods was immediately killed. With this release, only one pod is created. (link:https://issues.redhat.com/browse/OCPBUGS-57041[OCPBUGS-57041])

//[id="ocp-release-note-cluster-override-admin-operator-bug-fixes_{context}"]
//== Cluster Resource Override Admission Operator

[id="ocp-release-note-cluster-version-operator-bug-fixes_{context}"]
== Cluster Version Operator

* Before this update, the status of the `ClusterVersion` condition could incorrectly show `ImplicitlyEnabled` instead of `ImplicitlyEnabledCapabilities`. With this release, the `ClusterVersion` condition type is fixed and changed from `ImplicitlyEnabled` to `ImplicitlyEnabledCapabilities`. (link:https://issues.redhat.com/browse/OCPBUGS-56114[OCPBUGS-56114])

[id="ocp-release-note-config-operator-bug-fixes_{context}"]
== config-operator

* Before this update, the cluster incorrectly switched to the `CustomNoUpgrade` state without the correct `featureGate` configuration. As a consequence, empty `featureGates` and subsequent controller panics occurred. With this release, the `featureGate` configuration for the `CustomNoUpgrade` cluster state matches the default which prevents empty `featureGates` and subsequent controller panics. (link:https://issues.redhat.com/browse/OCPBUGS-57187[OCPBUGS-57187])

[id="ocp-release-note-dev-console-bug-fixes_{context}"]
== Dev Console

* Before this update, some entries on the *Quick Starts* page displayed duplicate link buttons. With this update, the duplicates are removed, and the link buttons are correctly displayed. (link:https://issues.redhat.com/browse/OCPBUGS-60373[OCPBUGS-60373])

* Before this update, the onboarding modal that displayed when you first logged in was missing visuals and images, which made the modal messaging unclear. With this release, the missing elements are added to the modal. As a result, the onboarding experience provides complete visuals consistent with the overall console design. (link:https://issues.redhat.com/browse/OCPBUGS-57392[OCPBUGS-57392])

* Before this update, importing multiple files in the YAML editor copied the existing content and appended the new file, which created duplicates. With this release, the import behavior is fixed. As a result, the YAML editor displays only the new file content without duplication. (link:https://issues.redhat.com/browse/OCPBUGS-45297[OCPBUGS-45297])
* Before this update, the status of the `ClusterVersion` condition could incorrectly show `ImplicitlyEnabled` instead of `ImplicitlyEnabledCapabilities`. With this release, the `ClusterVersion` condition type is fixed and changed from `ImplicitlyEnabled` to `ImplicitlyEnabledCapabilities`. (link:https://issues.redhat.com/browse/OCPBUGS-56114[OCPBUGS-56114])

[id="ocp-release-note-etcd-bug-fixes_{context}"]
== etcd

* Before this update, the timeout on one etcd member caused context deadlines to exceed. As a consequence, all members were declared unhealthy, even though some were reachable. With this release, if one member times out, other members are no longer incorrectly marked as unhealthy. (link:https://issues.redhat.com/browse/OCPBUGS-60941[OCPBUGS-60941])

* Before this update, when you deployed {sno} with many IPs on the primary interface, the IP in the etcd certificate mismatched with the IP in the config map that the API server used to connect to etcd. As a consequence, the API server pod failed during {sno} deployment, which caused cluster initialization issues. With this release, the single IP in the etcd config map matches the IP in the certificate for {sno} deployments. As a result, the API server connects to etcd by using the correct IP included in the etcd certificate, which prevents pod failure during cluster initialization. (link:https://issues.redhat.com/browse/OCPBUGS-55404[OCPBUGS-55404])

* Before this update, during temporary downtime of the API server, the Cluster etcd Operator reported incorrect information, such as messages that the `openshift-etcd` namespace was non-existent. With this update, the Cluster etcd Operator status message correctly indicates API server unavailability instead of suggesting the absence of the `openshift-etcd` namespace. As a result, the Cluster etcd Operator status accurately reflects the presence of the `openshift-etcd` namespace, enhancing system reliability. (link:https://issues.redhat.com/browse/OCPBUGS-44570[OCPBUGS-44570])

[id="ocp-release-note-extensions-olmv1-bug-fixes_{context}"]
== Extensions ({olmv1})

* Before this update, the preflight custom resource definition (CRD) safety check in {olmv1} blocked updates if it detected changes in the description fields of a CRD. With this update, the preflight CRD safety check does not block updates when there are changes to documentation fields. (link:https://issues.redhat.com/browse/OCPBUGS-55051[OCPBUGS-55051])

* Before this update, the catalogd and Operator Controller components did not display the correct version and commit information in the {oc-first}. With this update, the correct commit and version information is displayed. (link:https://issues.redhat.com/browse/OCPBUGS-23055[OCPBUGS-23055])

//[id="ocp-release-note-image-streams-bug-fixes_{context}"]
//== ImageStreams

[id="ocp-release-note-installer-bug-fixes_{context}"]
== Installer

* Before this update, when you installed a Konflux-built cluster on {ibm-power-server-name}, the installation could fail due to errors in semantic versioning (SemVer) parsing. With this release, the parsing issue has been resolved so that the installation can continue successfully. (link:https://issues.redhat.com/browse/OCPBUGS-61120[OCPBUGS-61120])

* Before this update, when you installed a cluster on {azure-short} Stack Hub with a user-provisioned infrastructure, the API and API-int load balancers could fail to be created. As a consequence, the installation failed. With this release, the user-provisioned infrastructure templates is updated so that the load balancers are created. As a result, installation is successful. (link:https://issues.redhat.com/browse/OCPBUGS-60545[OCPBUGS-60545])

* Before this update, when you installed a cluster on {gcp-short}, the installation program read and processed the `install-config.yaml` file even when an unrecoverable error was reported about not finding a matching public DNS zone. This error was due to an invalid `baseDomain` parameter. As a consequence, cluster administrators recreated the `install-config.yaml` file unnecessarily. With this release, when the installation program reports this error the installation progam does not read and process the `install-config.yaml` file. (link:https://issues.redhat.com/browse/OCPBUGS-59430[OCPBUGS-59430])

* Before this update, {ibm-cloud-title} was omitted from the list of platforms that supported {sno} installation in the validation code. As a consequence, users could not install a single-node configuration on {ibm-cloud-title} because of a validation error. With this release, {ibm-cloud-title} support for single-node installations is enabled. As a result, users can complete single-node installations on {ibm-cloud-title}. (link:https://issues.redhat.com/browse/OCPBUGS-59220[OCPBUGS-59220])

* Before this update, installing {sno} on `platform: None` with user-provisioned infrastructure was not supported, which led to installation failures. With this release, {sno} installation on `platform: None` is supported. (link:https://issues.redhat.com/browse/OCPBUGS-58216[OCPBUGS-58216])

* Before this update, when you installed {product-title} on {aws-first}, the Machine Config Operator (MCO) for disabling boot image management failed to check edge compute machine pools. When determining whether to disable boot image management, the installation progream only checked the first compute machine pool entry in the `install-config.yaml`. As a consequence, when you specified multiple compute pools but only the second had a custom Amazon Machine Image (AMI), the installation program did not disable MCO boot image management and the MCO could overwrite the custom AMI. With this release, the installation program checks all edge compute machine pools for custom images. As a result, boot image management is disabled when a custom image is specified in any machine pool. (link:https://issues.redhat.com/browse/OCPBUGS-57803[OCPBUGS-57803])

* Before this update, the Agent-based Installer set the permissions for the etcd directory `/var/lib/etcd/member` as `0755` when using an {sno} deployment instead of `0700`, which is correctly set on a multi-node deployment. With this release, the etcd directory `/var/lib/etcd/member` permissions are set to `0700` for {sno} deployments. (link:https://issues.redhat.com/browse/OCPBUGS-57021[OCPBUGS-57201])

* Before this update, when you used the Agent-based Installer, pressing the TAB key immediately after escaping the Network Manager Text User Interface (TUI) sometimes failed to register, which caused the cursor to remain on `Configure Network` instead of moving to `Quit`. As a consequence, you were not able to quit the agent console application that verifies whether the current host can retrieve release images. With this release, the TAB key is always registered. (link:https://issues.redhat.com/browse/OCPBUGS-56934[OCPBUGS-56934])

* Before this update, when you used the Agent-based Installer, exiting the NetworkManager TUI would sometimes result in a blank screen, rather than displaying an error or proceeding with the installation. With this update, the blank screen is not displayed. (link:https://issues.redhat.com/browse/OCPBUGS-56880[OCPBUGS-56880])

* Before this update, installing a cluster on {vmw-full} failed when the API VIP and the ingress VIP used one load balancer IP address. With this release, the API VIP and the ingress VIP are now distinct in `machineNetworks` and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-56601[OCPBUGS-56601])

* Before this update, when you use the Agent-based Installer, setting the `additionalTrustBundlePolicy` field would have no effect. As a consequence, other overrides such the `fips` parameter were ignored. With this update, the `additionalTrustBundlePolicy` parameter is correctly imported and other overrides are not ignored. (link:https://issues.redhat.com/browse/OCPBUGS-56596[OCPBUGS-56596])

* Before this update, the lack of detailed logging in the cluster destroy logic for {vmw-full} meant it was unclear why virtual machines (VMs) were not properly removed. Additionally, missing power state information could cause the destroy operation to enter an infinite loop. With this update, logging for the destroy operation is enhanced to indicate when specific cleanup actions begin, include vCenter names, and display a warning if the operation fails to find VMs. As a result, the destroy process provides detailed, actionable logs. (link:https://issues.redhat.com/browse/OCPBUGS-56262[OCPBUGS-56262])

* Before this update, when you used the Agent-based Installer to install a cluster in a disconnected environment, exiting the NetworkManager Text User Interface (TUI) returned you to the agent console application that checks whether release images can be pulled from a registry. With this update, you are not returned to the agent console application when you exit the NetworkManager TUI. (link:https://issues.redhat.com/browse/OCPBUGS-56223[OCPBUGS-56223])

* Before this update, the Agent-based Installer did not validate the values used to enable disk encryption, which potentially prevented disk encryption from being enabled. With this release, validation for correct disk encryption values is performed during image creation. (link:https://issues.redhat.com/browse/OCPBUGS-54885[OCPBUGS-54885])

* Before this update, the resources containing the configuration for vSphere connection could get broken due to a mismatch between the UI and API. With this release, the UI uses the updated API definition. (link:https://issues.redhat.com/browse/OCPBUGS-54434[OCPBUGS-54434])

* Before this update, when you used the Agent-based Installer, some validation checks for the `hostPrefix` parameter were not performed when generating the ISO image. As a consequence, invalid `hostPrefix` values were detected only when users failed to boot using the ISO. With this update, these validation checks are performed during ISO generation and causes an immediate failure. (link:https://issues.redhat.com/browse/OCPBUGS-53473[OCPBUGS-53473])

* Before this update, some systemd services in the Agent-based Installer continued to run after being stopped, which caused confusing log messages during cluster installation. With this update, these services are correctly stopped. (link:https://issues.redhat.com/browse/OCPBUGS-53107[OCPBUGS-53107])

* Before this update, if the proxy configuration for an {azure-first} cluster was deleted while installing a cluster, the program reported an unreadable error and the proxy connection timed out. With this release, when the proxy configuration for the cluster is deleted while installing a cluster, the program reports a readable error message and the issue is resolved. (link:https://issues.redhat.com/browse/OCPBUGS-45805[OCPBUGS-45805])

* Before this update, after an installation was completed, the `kubeconfig` file generated by the Agent-based Installer did not contain the ingress router certificate authority (CA). With this release, the `kubeconfig` file contains the ingress router CA upon the completion of a cluster installation. (link:https://issues.redhat.com/browse/OCPBUGS-45256[OCPBUGS-45256])

* Before this update, the Agent-based Installer announced a complete cluster installation without first checking whether Operators were in a stable state. Consequently, messages about a completed installation might have appeared even if there were still issues with any of the Operators. With this release, the Agent-based Installer waits until Operators are in a stable state before declaring the cluster installation to be complete. (link:https://issues.redhat.com/browse/OCPBUGS-18658[OCPBUGS-18658])

* Before this update, the installation program did not prevent you from attempting to install {sno} on bare metal on the installer-provisioned infrastructure. As a consequence, the installation failed because it was not supported. With this release, {product-title} prevents {sno} cluster installations on unsupported platforms. (link:https://issues.redhat.com/browse/OCPBUGS-6508[OCPBUGS-6508])

[id="ocp-release-note-kube-controller-manager-bug-fixes_{context}"]
== Kube Controller Manager

* Before this update, the `cluster-policy-controller` was crashing when an invalid volume type was provided. With this release, the code no longer panics. As a result, the `cluster-policy-controller` logs an error to inform about invalidity of a volume type. (link:https://issues.redhat.com/browse/OCPBUGS-62053[OCPBUGS-62053])

* Before this update, the `cluster-policy-controller` container was exposing the `10357` port for all networks (the bind address was set to 0.0.0.0). The port was exposed outside the node's host network because the KCM pod manifest set 'hostNetwork` to `true`. This port is used solely for the container's probe. With this enhancement, the bind address was updated to listen on the localhost only. As result, the node security is improved because the port is not exposed outside the node network. (link:https://issues.redhat.com/browse/OCPBUGS-53290[OCPBUGS-53290])
