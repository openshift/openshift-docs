// Module included in the following assemblies:
//
// * backup_and_restore/graceful-cluster-restart.adoc

:_mod-docs-content-type: PROCEDURE
[id="graceful-restart_{context}"]
= Restarting the cluster

You can restart your cluster after it has been shut down gracefully.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.
* This procedure assumes that you gracefully shut down the cluster.

.Procedure

. Power on any cluster dependencies, such as external storage or an LDAP server.

. Start all cluster machines.
+
Use the appropriate method for your cloud environment to start the machines, for example, from your cloud provider's web console.
+
Wait approximately 10 minutes before continuing to check the status of control plane nodes.

. Verify that all control plane nodes are ready.
+
[source,terminal]
----
$ oc get nodes -l node-role.kubernetes.io/master
----
+
The control plane nodes are ready if the status is `Ready`, as shown in the following output:
+
[source,terminal]
----
NAME                           STATUS   ROLES    AGE   VERSION
ip-10-0-168-251.ec2.internal   Ready    master   75m   v1.27.3
ip-10-0-170-223.ec2.internal   Ready    master   75m   v1.27.3
ip-10-0-211-16.ec2.internal    Ready    master   75m   v1.27.3
----

. If the control plane nodes are _not_ ready, then check whether there are any pending certificate signing requests (CSRs) that must be approved.

.. Get the list of current CSRs:
+
[source,terminal]
----
$ oc get csr
----

.. Review the details of a CSR to verify that it is valid:
+
[source,terminal]
----
$ oc describe csr <csr_name> <1>
----
<1> `<csr_name>` is the name of a CSR from the list of current CSRs.

.. Approve each valid CSR:
+
[source,terminal]
----
$ oc adm certificate approve <csr_name>
----

. After the control plane nodes are ready, verify that all worker nodes are ready.
+
[source,terminal]
----
$ oc get nodes -l node-role.kubernetes.io/worker
----
+
The worker nodes are ready if the status is `Ready`, as shown in the following output:
+
[source,terminal]
----
NAME                           STATUS   ROLES    AGE   VERSION
ip-10-0-179-95.ec2.internal    Ready    worker   64m   v1.27.3
ip-10-0-182-134.ec2.internal   Ready    worker   64m   v1.27.3
ip-10-0-250-100.ec2.internal   Ready    worker   64m   v1.27.3
----

. If the worker nodes are _not_ ready, then check whether there are any pending certificate signing requests (CSRs) that must be approved.

.. Get the list of current CSRs:
+
[source,terminal]
----
$ oc get csr
----

.. Review the details of a CSR to verify that it is valid:
+
[source,terminal]
----
$ oc describe csr <csr_name> <1>
----
<1> `<csr_name>` is the name of a CSR from the list of current CSRs.

.. Approve each valid CSR:
+
[source,terminal]
----
$ oc adm certificate approve <csr_name>
----

. Verify that the cluster started properly.

.. Check that there are no degraded cluster Operators.
+
[source,terminal]
----
$ oc get clusteroperators
----
+
Check that there are no cluster Operators with the `DEGRADED` condition set to `True`.
+
[source,terminal,subs="attributes+"]
----
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                             {product-version}.0    True        False         False      59m
cloud-credential                           {product-version}.0    True        False         False      85m
cluster-autoscaler                         {product-version}.0    True        False         False      73m
config-operator                            {product-version}.0    True        False         False      73m
console                                    {product-version}.0    True        False         False      62m
csi-snapshot-controller                    {product-version}.0    True        False         False      66m
dns                                        {product-version}.0    True        False         False      76m
etcd                                       {product-version}.0    True        False         False      76m
...
----

.. Check that all nodes are in the `Ready` state:
+
[source,terminal]
----
$ oc get nodes
----
+
Check that the status for all nodes is `Ready`.
+
[source,terminal]
----
NAME                           STATUS   ROLES    AGE   VERSION
ip-10-0-168-251.ec2.internal   Ready    master   82m   v1.27.3
ip-10-0-170-223.ec2.internal   Ready    master   82m   v1.27.3
ip-10-0-179-95.ec2.internal    Ready    worker   70m   v1.27.3
ip-10-0-182-134.ec2.internal   Ready    worker   70m   v1.27.3
ip-10-0-211-16.ec2.internal    Ready    master   82m   v1.27.3
ip-10-0-250-100.ec2.internal   Ready    worker   69m   v1.27.3
----

If the cluster did not start properly, you might need to restore your cluster using an etcd backup.
