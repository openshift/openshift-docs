// Module included in the following assemblies:
//
// * observability/monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="setting-up-pod-topology-spread-constraints-for-thanos-ruler_{context}"]
= Setting up pod topology spread constraints for Thanos Ruler

For user-defined monitoring, you can set up pod topology spread constraints for Thanos Ruler to fine tune how pod replicas are scheduled to nodes across zones.
Doing so helps ensure that Thanos Ruler pods are highly available and run more efficiently, because workloads are spread across nodes in different data centers or hierarchical infrastructure zones.

You configure pod topology spread constraints for Thanos Ruler in the `user-workload-monitoring-config` config map.

.Prerequisites

ifndef::openshift-dedicated,openshift-rosa[]
* A cluster administrator has enabled monitoring for user-defined projects.
* You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
* You have created the `user-workload-monitoring-config` `ConfigMap` object.
endif::openshift-dedicated,openshift-rosa[]
ifdef::openshift-dedicated,openshift-rosa[]
* You have access to the cluster as a user with the `dedicated-admin` role.
* The `user-workload-monitoring-config` `ConfigMap` object exists. This object is created by default when the cluster is created.
endif::openshift-dedicated,openshift-rosa[]
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `user-workload-monitoring-config` config map in the `openshift-user-workload-monitoring` namespace:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

. Add values for the following settings under `data/config.yaml/thanosRuler` to configure pod topology spread constraints:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      topologySpreadConstraints:
      - maxSkew: 1 <1>
        topologyKey: monitoring <2>
        whenUnsatisfiable: ScheduleAnyway <3>
        labelSelector:
          matchLabels: <4>
            app.kubernetes.io/name: thanos-ruler
----
<1> Specify a numeric value for `maxSkew`, which defines the degree to which pods are allowed to be unevenly distributed. This field is required, and the value must be greater than zero. The value specified has a different effect depending on what value you specify for `whenUnsatisfiable`.
<2> Specify a key of node labels for `topologyKey`. This field is required. Nodes that have a label with this key and identical values are considered to be in the same topology. The scheduler will try to put a balanced number of pods into each domain.
<3> Specify a value for `whenUnsatisfiable`. This field is required. Available options are `DoNotSchedule` and `ScheduleAnyway`. Specify `DoNotSchedule` if you want the `maxSkew` value to define the maximum difference allowed between the number of matching pods in the target topology and the global minimum.  Specify `ScheduleAnyway` if you want the scheduler to still schedule the pod but to give higher priority to nodes that might reduce the skew.
<4> Specify a value for `matchLabels`. This value is used to identify the set of matching pods to which to apply the constraints.

. Save the file to apply the changes automatically.
+
[WARNING]
====
When you save changes to the `user-workload-monitoring-config` config map, the pods and other resources in the `openshift-user-workload-monitoring` project might be redeployed.
The running monitoring processes in that project might also restart.
====
