// Module included in the following assemblies:
//
// * nodes/nodes-nodes-rebooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="nodes-nodes-rebooting-gracefully_{context}"]
= Rebooting a node gracefully

Before rebooting a node, it is recommended to backup etcd data to avoid any data loss on the node.

[NOTE]
====
For {sno} clusters that require users to perform the `oc login` command rather than having the certificates in `kubeconfig` file to manage the cluster, the `oc adm` commands might not be available after cordoning and draining the node. This is because the `openshift-oauth-apiserver` pod is not running due to the cordon. You can use SSH to access the nodes as indicated in the following procedure.

In a {sno} cluster, pods cannot be rescheduled when cordoning and draining. However, doing so gives the pods, especially your workload pods, time to properly stop and release associated resources.
====

.Procedure

To perform a graceful restart of a node:

. Mark the node as unschedulable:
+
[source,terminal]
----
$ oc adm cordon <node1>
----

. Drain the node to remove all the running pods:
+
[source,terminal]
----
$ oc adm drain <node1> --ignore-daemonsets --delete-emptydir-data --force
----
+
You might receive errors that pods associated with custom pod disruption budgets (PDB) cannot be evicted.
+
.Example error
[source,terminal]
----
error when evicting pods/"rails-postgresql-example-1-72v2w" -n "rails" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
----
+
In this case, run the drain command again, adding the `disable-eviction` flag, which bypasses the PDB checks:
+
[source,terminal]
----
$ oc adm drain <node1> --ignore-daemonsets --delete-emptydir-data --force --disable-eviction
----

. Access the node in debug mode:
+
[source,terminal]
----
$ oc debug node/<node1>
----

. Change your root directory to `/host`:
+
[source,terminal]
----
$ chroot /host
----

. Restart the node:
+
[source,terminal]
----
$ systemctl reboot
----
+
In a moment, the node enters the `NotReady` state.
+
[NOTE]
====
With some {sno} clusters, the `oc` commands might not be available after you cordon and drain the node because the `openshift-oauth-apiserver` pod is not running. You can use SSH to connect to the node and perform the reboot.

[source,terminal]
----
$ ssh core@<master-node>.<cluster_name>.<base_domain>
----

[source,terminal]
----
$ sudo systemctl reboot
----
====

. After the reboot is complete, mark the node as schedulable by running the following command:
+
[source,terminal]
----
$ oc adm uncordon <node1>
----
+
[NOTE]
====
With some {sno} clusters, the `oc` commands might not be available after you cordon and drain the node because the `openshift-oauth-apiserver` pod is not running. You can use SSH to connect to the node and uncordon it.

[source,terminal]
----
$ ssh core@<target_node>
----

[source,terminal]
----
$ sudo oc adm uncordon <node> --kubeconfig /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig
----
====

. Verify that the node is ready:
+
[source,terminal]
----
$ oc get node <node1>
----
+
.Example output
[source,terminal]
----
NAME    STATUS  ROLES    AGE     VERSION
<node1> Ready   worker   6d22h   v1.18.3+b0068a8
----

