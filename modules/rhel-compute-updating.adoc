// Module included in the following assemblies:
//
// * updating/updating-cluster-rhel-compute.adoc

[id="rhel-compute-updating_{context}"]
= Updating RHEL compute machines in your cluster

After you update your cluster, you must update the Red Hat Enterprise Linux (RHEL)
compute machines in your cluster.

.Prerequisites

* You updated your cluster.
+
[IMPORTANT]
====
Because the RHEL machines require assets that are generated by the cluster to
complete the update process, you must update the cluster before you update
the RHEL compute machines in it.
====

* You have access to the machine that you used to add the RHEL compute machines
cluster. You must have access to the `hosts` Ansible inventory file that defines
your RHEL machines and the `upgrade` playbook.

.Procedure

. Stop and disable firewalld on the host:
+
[source,terminal]
----
# systemctl disable --now firewalld.service
----
+
[NOTE]
====
You must not enable firewalld later. If you do, you cannot access {product-title} logs on the worker.
====

. Enable the repositories that are required for {product-title} {product-version}:
.. On the machine that you run the Ansible playbooks, update the required repositories:
+
[source,terminal]
----
# subscription-manager repos --disable=rhel-7-server-ose-4.5-rpms \
                             --enable=rhel-7-server-ansible-2.9-rpms \
                             --enable=rhel-7-server-ose-4.6-rpms
----

.. On the machine that you run the Ansible playbooks, update the required packages, including `openshift-ansible`:
+
[source,terminal]
----
# yum update openshift-ansible openshift-clients
----

.. On each RHEL compute node, update the required repositories:
+
[source,terminal]
----
# subscription-manager repos --disable=rhel-7-server-ose-4.5-rpms \
                             --enable=rhel-7-server-ose-4.6-rpms  \
                             --enable=rhel-7-fast-datapath-rpms   \
                             --enable=rhel-7-server-optional-rpms
----

. Update a RHEL worker machine:
.. Review the current node status to determine which RHEL worker to update:
+
[source,terminal]
----
# oc get node
----
+
.Example output
[source,terminal]
----
NAME                        STATUS                        ROLES    AGE    VERSION
mycluster-control-plane-0   Ready                         master   145m   v1.19.0
mycluster-control-plane-1   Ready                         master   145m   v1.19.0
mycluster-control-plane-2   Ready                         master   145m   v1.19.0
mycluster-rhel7-0           NotReady,SchedulingDisabled   worker   98m    v1.14.6+97c81d00e
mycluster-rhel7-1           Ready                         worker   98m    v1.14.6+97c81d00e
mycluster-rhel7-2           Ready                         worker   98m    v1.14.6+97c81d00e
mycluster-rhel7-3           Ready                         worker   98m    v1.14.6+97c81d00e
----
+
Note which machine has the `NotReady,SchedulingDisabled` status.

.. Review your Ansible inventory file at `/<path>/inventory/hosts`
and update its contents so that only the machine with the `NotReady,SchedulingDisabled` status is listed in the `[workers]` section, as shown in the following example:
+
----
[all:vars]
ansible_user=root
#ansible_become=True

openshift_kubeconfig_path="~/.kube/config"

[workers]
mycluster-rhel7-0.example.com
----

.. Change to the `openshift-ansible` directory:
+
[source,terminal]
----
$ cd /usr/share/ansible/openshift-ansible
----

.. Run the `upgrade` playbook:
+
[source,terminal]
----
$ ansible-playbook -i /<path>/inventory/hosts playbooks/upgrade.yml <1>
----
<1> For `<path>`, specify the path to the Ansible inventory file
that you created.

. Follow the process in the previous step to update each RHEL worker machine in your cluster.

. After you update all of the workers, confirm that all of your cluster nodes have updated to the new version:
+
[source,terminal]
----
# oc get node
----
+
.Example output
[source,terminal]
----
NAME                        STATUS                        ROLES    AGE    VERSION
mycluster-control-plane-0   Ready                         master   145m   v1.19.0
mycluster-control-plane-1   Ready                         master   145m   v1.19.0
mycluster-control-plane-2   Ready                         master   145m   v1.19.0
mycluster-rhel7-0           NotReady,SchedulingDisabled   worker   98m    v1.19.0
mycluster-rhel7-1           Ready                         worker   98m    v1.19.0
mycluster-rhel7-2           Ready                         worker   98m    v1.19.0
mycluster-rhel7-3           Ready                         worker   98m    v1.19.0
----
