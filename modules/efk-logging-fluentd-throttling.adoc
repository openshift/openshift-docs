// Module included in the following assemblies:
//
// * logging/efk-logging-fluentd.adoc

[id="efk-logging-fluentd-throttling-{context}"]
= Throttling Fluentd logs

For projects that are especially verbose, an administrator can throttle down the
rate at which the logs are read in by Fluentd before being processed. By throttling, 
you deliberately slow down the rate at which you are reading logs, 
so Kibana might take longer to display records.

[WARNING]
====
Throttling can contribute to log aggregation falling behind for the configured
projects; log entries can be lost if a pod is deleted before Fluentd catches up.
====

[NOTE]
====
Throttling does not work when using the systemd journal as the log
source. The throttling implementation depends on being able to throttle the
reading of the individual log files for each project. When reading from the
journal, there is only a single log source, no log files, so no file-based
throttling is available. There is not a method of restricting the log
entries that are read into the Fluentd process.
====

To tell Fluentd which projects it should be restricting, edit the throttle
configuration in its ConfigMap after deployment:

----
$ oc edit configmap/fluentd
----

The format of the *_throttle-config.yaml_* key is a YAML file that contains
project names and the desired rate at which logs are read in on each
node. The default is 1000 lines at a time per node. For example:

----
throttle-config.yaml: |
  - opensift-logging:
      read_lines_limit: 10
  - .operations:
      read_lines_limit: 100   
----
////
When you make changes to any part of cluster logging, specifically Elasticsearch
or Fluentd, you should first scale Elasticsearch down to zero and scale Fluentd
so it does not match any other nodes. Then, make the changes and scale
Elasticsearch and Fluentd back. 

.Prerequisites

Set cluster logging to the unmanaged state.

.Procedure

. Scale Elasticsearch to zero:
+
----
$ oc scale --replicas=0 deploy/<ELASTICSEARCH_DC>
----

. Change `nodeSelector` in the daemonset configuration to match zero:

.. Get the fluentd node selector:
+
----
$ oc get ds fluentd -o yaml |grep -A 1 Selector
----

.. Use the `oc patch` command to modify the daemonset nodeSelector:
+
----
$ oc patch ds fluentd -p '{"spec":{"template":{"spec":{"nodeSelector":{"nonexistlabel":"true"}}}}}'
----

.. Get the Fluentd node selector to see the change:
+
----
$ oc get ds fluentd -o yaml |grep -A 1 Selector
     nodeSelector:
       "nonexistlabel: "true"
----

. Scale Elastcsearch back up from zero:
+
----
$ oc scale --replicas=# dc/<ELASTICSEARCH_DC>
----

. Change nodeSelector in the daemonset configuration back to
logging-infra-fluentd: "true".

. Use the `oc patch` command to modify the daemonset nodeSelector:
+
----
$ oc patch ds fluentd -p '{"spec":{"template":{"spec":{"nodeSelector":{"logging-infra-fluentd":"true"}}}}}'
----
////

