// Module included in the following assemblies:
//
// * migration/migrating_3_4/migrating-applications-with-cam-3-4.adoc
// * migration/migrating_4_1_4/migrating-applications-with-cam-4-1-4.adoc
// * migration/migrating_4_2_4/migrating-applications-with-cam-4-2-4.adoc

[id='migration-creating-migration-plan-cam_{context}']
= Creating a migration plan in the {mtc-short} web console

You can create a migration plan in the  for the {mtc-full} ({mtc-short}) web console.

.Prerequisites

* You must add source and target clusters and a replication repository to the {mtc-short} web console.
* Clusters:
** The clusters must have network access to each other.
** The clusters must have network access to the replication repository.
** The clusters must be able to communicate using OpenShift routes on port 443.
** The clusters must have no `Critical` conditions.
** The clusters must be in a `Ready` state.

* The migration plan name must not exceed 253 lower-case alphanumeric characters (`a-z, 0-9`) and must not contain spaces or underscores (`_`).
* PV `Move` copy method: The clusters must have network access to the remote volume.
* PV `Snapshot` copy method:
** The clusters must have the same cloud provider (AWS, GCP, or Azure).
** The clusters must be located in the same geographic region.
** The storage class must be the same on the source and target clusters.

* Direct image migration:
** The source cluster must have its internal registry exposed to external traffic.
** The exposed registry route of the source cluster must be added to the cluster configuration by using either the {mtc-short} web console or the `exposedRegistryPath` parameter in the `MigCluster` CR manifest.

* Direct volume migration:
** The PVs to be migrated must be valid.
** The PVs must be in a `bound` state.
** The PVs must be copied by using the file system copy method.
** The source cluster must not have an HTTP proxy configured.

.Procedure

. In the {mtc-short} web console, click *Migration plans*.
. Click *Add migration plan*.
. Enter the *Plan name* and click *Next*.
. Select a *Source cluster*.
. Select a *Target cluster*.
. Select a *Replication repository*.
. Select the projects to be migrated and click *Next*.
. Select a *Source cluster*, a *Target cluster*, and a *Repository*, and click *Next*.
. On the *Namespaces* page, select the projects to be migrated and click *Next*.
. On the *Persistent volumes* page, click a *Migration type* for each PV:

* The *Copy* option copies the data from the PV of a source cluster to the replication repository and then restores it on a newly created PV, with similar characteristics, in the target cluster.
+
If you specified a route to an image registry when you added the source cluster to the web console, you can migrate images directly from the source cluster to the target cluster.
* The *Move* option unmounts a remote volume, for example, NFS, from the source cluster, creates a PV resource on the target cluster pointing to the remote volume, and then mounts the remote volume on the target cluster. Applications running on the target cluster use the same remote volume that the source cluster was using. The remote volume must be accessible to the source and target clusters.
. Click *Next*.
. On the *Copy options* page, select a *Copy method* for each PV:

* *Snapshot copy* backs up and restores data using the cloud provider's snapshot functionality. It is significantly faster than *Filesystem copy*.
* *Filesystem copy* backs up the files on the source cluster and restores them on the target cluster.

. You can select *Verify copy* to verify data migrated with *Filesystem copy*. Data is verified by generating a checksum for each source file and checking the checksum after restoration. Data verification significantly reduces performance.

. Select a *Target storage class*.
+
You can change the storage class of data migrated with *Filesystem copy*.
. Click *Next*.
. On the *Migration options* page, the *Direct image migration* option is selected if you specified an exposed image registry route for the source cluster. The *Direct PV migration* option is selected if you are migrating data with  *Filesystem copy*.
+
The direct migration options copy images and files directly from the source cluster to the target cluster. This option is much faster than copying images and files from the source cluster to the replication repository and then from the replication repository to the target cluster.
. Click *Next*.
. Optional: On the *Hooks* page, click *Add Hook* to add a hook to the migration plan.
+
A hook runs custom code. You can add up to four hooks to a single migration plan. Each hook runs during a different migration step.

.. Enter the name of the hook to display in the web console.
.. If the hook is an Ansible playbook, select *Ansible playbook* and click *Browse* to upload the playbook or paste the contents of the playbook in the field.
.. Optional: Specify an Ansible runtime image if you are not using the default hook image.
.. If the hook is not an Ansible playbook, select *Custom container image* and specify the image name and path.
+
A custom container image can include Ansible playbooks.

.. Select *Source cluster* or *Target cluster*.
.. Enter the *Service account name* and the *Service account namespace*.
.. Select the migration step for the hook:

* *preBackup*: Before the application workload is backed up on the source cluster
* *postBackup*: After the application workload is backed up on the source cluster
* *preRestore*: Before the application workload is restored on the target cluster
* *postRestore*: After the application workload is restored on the target cluster

.. Click *Add*.

. Click *Finish*.
+
The migration plan is displayed in the *Migration plans* list.
