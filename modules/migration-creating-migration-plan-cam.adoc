// Module included in the following assemblies:
//
// * migration/migrating_3_4/migrating-applications-with-cam-3-4.adoc
// * migration/migrating_4_1_4/migrating-applications-with-cam-4-1-4.adoc
// * migration/migrating_4_2_4/migrating-applications-with-cam-4-2-4.adoc
[id='migration-creating-migration-plan-cam_{context}']
= Creating a migration plan in the {mtc-short} web console

You can create a migration plan in the {mtc-short} web console.

.Prerequisites

* The {mtc-short} web console must contain the following:
** Source cluster
** Target cluster
** Replication repository

* The source and target clusters must have network access to each other and to the replication repository.

* If you use snapshots to copy data, the source and target clusters must run on the same cloud provider (AWS, GCP, or Azure) and be located in the same region.

.Procedure

. Log in to the {mtc-short} web console.
. In the *Plans* section, click *Add plan*.
. Enter the *Plan name* and click *Next*.
+
The *Plan name* can contain up to 253 lower-case alphanumeric characters (`a-z, 0-9`). It must not contain spaces or underscores (`_`).
. Select a *Source cluster*.
. Select a *Target cluster*.
. Select a *Replication repository*.
. Select the projects to be migrated and click *Next*.
. Select *Copy* or *Move* for the PVs:

* *Copy* copies the data in a source cluster's PV to the replication repository and then restores it on a newly created PV, with similar characteristics, in the target cluster.
+
Optional: You can verify data copied with the file system method by selecting *Verify copy*. This option generates a checksum for each source file and checks it after restoration. The operation significantly reduces performance.

* *Move* unmounts a remote volume (for example, NFS) from the source cluster, creates a PV resource on the target cluster pointing to the remote volume, and then mounts the remote volume on the target cluster. Applications running on the target cluster use the same remote volume that the source cluster was using. The remote volume must be accessible to the source and target clusters.

. Click *Next*.

. Select a *Copy method* for the PVs:

* *Snapshot* backs up and restores the disk using the cloud provider's snapshot functionality. It is significantly faster than *Filesystem*.
+
[NOTE]
====
The storage and clusters must be in the same region and the storage class must be compatible.
====

* *Filesystem* copies the data files from the source disk to a newly created target disk.

. Select a *Storage class* for the PVs.
+
If you selected the *Filesystem* copy method, you can change the storage class during migration, for example, from Red Hat Gluster Storage or NFS storage to Red Hat Ceph Storage.

. Click *Next*.

. If you want to add a migration hook, click *Add Hook* and perform the following steps:

.. Specify the name of the hook.
.. Select *Ansible playbook* to use your own playbook or *Custom container image* for a hook written in another language.
.. Click *Browse* to upload the playbook.
.. Optional: If you are not using the default Ansible runtime image, specify your custom Ansible image.
.. Specify the cluster on which you want the hook to run.
.. Specify the service account name.
.. Specify the namespace.
.. Select the migration step at which you want the hook to run:

* PreBackup: Before backup tasks are started on the source cluster
* PostBackup: After backup tasks are complete on the source cluster
* PreRestore: Before restore tasks are started on the target cluster
* PostRestore: After restore tasks are complete on the target cluster

. Click *Add*.
+
You can add up to four hooks to a migration plan, assigning each hook to a different migration step.

. Click *Finish*.
. Click *Close*.
+
The migration plan appears in the *Plans* section.
