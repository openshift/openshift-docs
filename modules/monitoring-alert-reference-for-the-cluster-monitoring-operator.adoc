:_mod-docs-content-type: REFERENCE
[id="alert-reference-for-the-cluster-monitoring-operator_{context}"]
= Alert reference for the {cmo-full}

[role="_abstract"]
Learn about alerting rules that are included in your cluster by default.

[IMPORTANT]
====
* The following list of alerting rules is not exhaustive and can vary based on different configurations and installed Operators.

* In the following alert descriptions, `$value` and `$labels.*` represent value and label data that are substituted with actual values when the alert fires.
====

[options="header"]
|===
|Name |Severity |Duration |Description

| `AlertmanagerClusterDown` | warning | 300s | $value of Alertmanager instances within the $labels.job cluster have been up for less than half of the last 5m.
| `AlertmanagerClusterFailedToSendAlerts` | warning | 300s | The minimum notification failure rate to $labels.integration sent from any instance in the $labels.job cluster is $value.
| `AlertmanagerConfigInconsistent` | warning | 1200s | Alertmanager instances within the $labels.job cluster have different configurations.
| `AlertmanagerFailedReload` | critical | 600s | Configuration has failed to load for $labels.namespace/$labels.pod.
| `AlertmanagerFailedToSendAlerts` | warning | 300s | Alertmanager $labels.namespace/$labels.pod failed to send $value of notifications to $labels.integration.
| `AlertmanagerMembersInconsistent` | warning | 900s | Alertmanager $labels.namespace/$labels.pod has only found $value members of the $labels.job cluster.
| `AlertmanagerReceiversNotConfigured` | warning | 600s | Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager.
| `APIRemovedInNextEUSReleaseInUse` | info | 3600s | Deprecated API that will be removed in the next EUS version is being used. Removing the workload that is using the $labels.group.$labels.version/$labels.resource API might be necessary for a successful upgrade to the next EUS cluster version with Kubernetes $labels.removed_release. Refer to `oc get apirequestcounts $labels.resource.$labels.version.$labels.group -o yaml` to identify the workload.
| `APIRemovedInNextReleaseInUse` | info | 3600s | Deprecated API that will be removed in the next version is being used. Removing the workload that is using the $labels.group.$labels.version/$labels.resource API might be necessary for a successful upgrade to the next cluster version with Kubernetes $labels.removed_release. Refer to `oc get apirequestcounts $labels.resource.$labels.version.$labels.group -o yaml` to identify the workload.
| `AuditLogError` | warning | 60s | An API Server had an error writing to an audit log.
| `CannotEvaluateConditionalUpdates` | warning | 0s | Failure to evaluate conditional update matches means that Cluster Version Operator cannot decide whether an update path is recommended or not.
| `CannotRetrieveUpdates` | warning | 0s | Failure to retrieve updates means that cluster administrators will need to monitor for available updates on their own or risk falling behind on security or other bugfixes. If the failure is expected, you can clear spec.channel in the ClusterVersion object to tell the cluster-version operator to not retrieve updates. Failure reason $value . For more information refer to `oc get clusterversion/version -o=jsonpath="{.status.conditions[?(.type=='RetrievedUpdates')]}{'\n'}"` or /settings/cluster/.
| `CloudCredentialOperatorDeprovisioningFailed` | warning | 300s | While processing a CredentialsRequest marked for deletion, the Cloud Credential Operator encountered an issue. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .status.condition showing a condition type of CredentialsDeprovisionFailure set to True for more details on the issue.
| `CloudCredentialOperatorInsufficientCloudCreds` | warning | 300s | The Cloud Credential Operator has determined that there are insufficient permissions to process one or more CredentialsRequest CRs. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .status.condition showing a condition type of InsufficientCloudCreds set to True for more details.
| `CloudCredentialOperatorProvisioningFailed` | warning | 300s | While processing a CredentialsRequest, the Cloud Credential Operator encountered an issue. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .stats.condition showing a condition type of CredentialsProvisionFailure set to True for more details on the issue.
| `CloudCredentialOperatorStaleCredentials` | warning | 300s | The Cloud Credential Operator (CCO) has detected one or more stale CredentialsRequest CRs that need to be manually deleted. When the CCO is in Manual credentials mode, it will not automatically clean up stale CredentialsRequest CRs (that may no longer be necessary in the present version of OpenShift because it could involve needing to clean up manually created cloud resources. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .status.condition showing a condition type of StaleCredentials set to True. Determine the appropriate steps to clean up/deprovision any previously provisioned cloud resources. Finally, delete the CredentialsRequest with an 'oc delete'.
| `CloudCredentialOperatorTargetNamespaceMissing` | warning | 300s | At least one CredentialsRequest custom resource has specified in its .spec.secretRef.namespace field a namespace which does not presently exist. This means the Cloud Credential Operator in the openshift-cloud-credential-operator namespace cannot process the CredentialsRequest resource. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .status.condition showing a condition type of MissingTargetNamespace set to True.
| `ClusterMonitoringOperatorDeprecatedConfig` | info | 3600s | The configuration field $labels.field in $labels.configmap was deprecated in $labels.deprecation_version and has no effect.
| `ClusterMonitoringOperatorReconciliationErrors` | warning | 3600s | Errors are occurring during reconciliation cycles. Inspect the cluster-monitoring-operator log for potential root causes.
| `ClusterNotUpgradeable` | info | 3600s | In most cases, you will still be able to apply patch releases. Reason $value. For more information refer to 'oc adm upgrade' or /settings/cluster/.
| `ClusterOperatorDegraded` | warning | 1800s | The $labels.name operator is degraded because $labels.reason, and the components it manages may have reduced quality of service. Cluster upgrades may not complete. For more information refer to '$labels.nameoc adm upgradeoc get -o yaml clusteroperator $labels.name' or /settings/cluster/.
| `ClusterOperatorDown` | critical | 600s | The $labels.name operator may be down or disabled because $labels.reason, and the components it manages may be unavailable or degraded. Cluster upgrades may not complete. For more information refer to '$labels.nameoc adm upgradeoc get -o yaml clusteroperator $labels.name' or /settings/cluster/.
| `ClusterOperatorFlapping` | warning | 600s | The $labels.name operator behavior might cause upgrades to be unstable. For more information refer to '$labels.nameoc adm upgradeoc get -o yaml clusteroperator $labels.name' or /settings/cluster/.
| `ClusterReleaseNotAccepted` | warning | 3600s | The desired cluster release has not been accepted because $labels.reason, and the cluster will continue to reconcile an earlier release instead of moving towards that desired release. For more information refer to 'oc adm upgrade' or /settings/cluster/.
| `ClusterVersionOperatorDown` | critical | 600s | The operator may be down or disabled. The cluster will not be kept up to date and upgrades will not be possible. Inspect the openshift-cluster-version namespace for events or changes to the cluster-version-operator deployment or pods to diagnose and repair. For more information refer to /k8s/cluster/projects/openshift-cluster-version.
| `ConfigReloaderSidecarErrors` | warning | 600s | Errors encountered while the $labels.pod config-reloader sidecar attempts to sync config in $labels.namespace namespace. As a result, configuration for service running in $labels.pod may be stale and cannot be updated anymore.
| `CoreDNSErrorsHigh` | warning | 300s | CoreDNS is returning SERVFAIL for $value of requests.
| `CoreDNSHealthCheckSlow` | warning | 300s | CoreDNS Health Checks are slowing down (instance $labels.instance)
| `CoreDNSPanicking` | warning | 300s | $value CoreDNS panics observed on $labels.instance
| `CsvAbnormalFailedOver2Min` | warning | 120s | Failed to install Operator $labels.name version $labels.version. Reason-$labels.reason
| `CsvAbnormalOver30Min` | warning | 1800s | Failed to install Operator $labels.name version $labels.version. Phase-$labels.phase Reason-$labels.reason
| `etcdDatabaseHighFragmentationRatio` | warning | 600s | etcd cluster "$labels.job": database size in use on instance $labels.instance is $value of the actual allocated disk space, please run defragmentation (e.g. etcdctl defrag) to retrieve the unused fragmented disk space.
| `etcdDatabaseQuotaLowSpace` | info | 600s | etcd cluster "$labels.job": database size is 65% of the defined quota on etcd instance $labels.instance, please defrag or increase the quota as the writes to etcd will be disabled when it is full.
| `etcdDatabaseQuotaLowSpace` | warning | 600s | etcd cluster "$labels.job": database size is 75% of the defined quota on etcd instance $labels.instance, please defrag or increase the quota as the writes to etcd will be disabled when it is full.
| `etcdDatabaseQuotaLowSpace` | critical | 600s | etcd cluster "$labels.job": database size is 85% of the defined quota on etcd instance $labels.instance, please defrag or increase the quota as the writes to etcd will be disabled when it is full.
| `etcdExcessiveDatabaseGrowth` | warning | 600s | etcd cluster "$labels.job": Predicting running out of disk space in the next four hours, based on write observations within the past four hours on etcd instance $labels.instance, please check as it might be disruptive.
| `etcdGRPCRequestsSlow` | critical | 1800s | etcd cluster "$labels.job": 99th percentile of gRPC requests is $value on etcd instance $labels.instance for $labels.grpc_method method.
| `etcdHighCommitDurations` | warning | 600s | etcd cluster "$labels.job": 99th percentile commit durations $value on etcd instance $labels.instance.
| `etcdHighFsyncDurations` | warning | 600s | etcd cluster "$labels.job": 99th percentile fsync durations are $value on etcd instance $labels.instance.
| `etcdHighFsyncDurations` | critical | 600s | etcd cluster "$labels.job": 99th percentile fsync durations are $value on etcd instance $labels.instance.
| `etcdHighNumberOfFailedGRPCRequests` | warning | 600s | etcd cluster "$labels.job": $value% of requests for $labels.grpc_method failed on etcd instance $labels.instance.
| `etcdHighNumberOfFailedGRPCRequests` | critical | 600s | etcd cluster "$labels.job": $value% of requests for $labels.grpc_method failed on etcd instance $labels.instance.
| `etcdHighNumberOfFailedProposals` | warning | 900s | etcd cluster "$labels.job": $value proposal failures within the last 30 minutes on etcd instance $labels.instance.
| `etcdHighNumberOfLeaderChanges` | warning | 300s | etcd cluster "$labels.job": $value average leader changes within the last 10 minutes. Frequent elections may be a sign of insufficient resources, high network latency, or disruptions by other components and should be investigated.
| `etcdInsufficientMembers` | critical | 180s | etcd is reporting fewer instances are available than are needed ($value). When etcd does not have a majority of instances available the Kubernetes and OpenShift APIs will reject read and write requests and operations that preserve the health of workloads cannot be performed. This can occur when multiple control plane nodes are powered off or are unable to connect to each other via the network. Check that all control plane nodes are powered on and that network connections between each machine are functional.
| `etcdMemberCommunicationSlow` | warning | 600s | etcd cluster "$labels.job": member communication with $labels.To is taking $value on etcd instance $labels.instance.
| `etcdMembersDown` | critical | 1200s | etcd cluster "$labels.job": members are down ($value).
| `etcdNoLeader` | critical | 60s | etcd cluster "$labels.job": member $labels.instance has no leader.
| `etcdSignerCAExpirationCritical` | critical | 3600s | etcd is reporting the signer ca "$labels.name" to have less than year (($value days) of validity left.
| `etcdSignerCAExpirationWarning` | warning | 3600s | etcd is reporting the signer ca "$labels.name" to have less than two years (($value days) of validity left.
| `ExtremelyHighIndividualControlPlaneCPU` | warning | 300s | Extreme CPU pressure can cause slow serialization and poor performance from the kube-apiserver and etcd. When this happens, there is a risk of clients seeing non-responsive API requests which are issued again causing even more CPU pressure. It can also cause failing liveness probes due to slow etcd responsiveness on the backend. If one kube-apiserver fails under this condition, chances are you will experience a cascade as the remaining kube-apiservers are also under-provisioned. To fix this, increase the CPU and memory on your control plane nodes.
| `ExtremelyHighIndividualControlPlaneCPU` | critical | 3600s | Extreme CPU pressure can cause slow serialization and poor performance from the kube-apiserver and etcd. When this happens, there is a risk of clients seeing non-responsive API requests which are issued again causing even more CPU pressure. It can also cause failing liveness probes due to slow etcd responsiveness on the backend. If one kube-apiserver fails under this condition, chances are you will experience a cascade as the remaining kube-apiservers are also under-provisioned. To fix this, increase the CPU and memory on your control plane nodes.
| `ExtremelyHighIndividualControlPlaneMemory` | critical | 2700s | The memory utilization per instance within control plane nodes influence the stability, and responsiveness of the cluster. This can lead to cluster instability and slow responses from kube-apiserver or failing requests especially on etcd. Moreover, OOM kill is expected which negatively influences the pod scheduling. If this happens on container level, the descheduler will not be able to detect it, as it works on the pod level. To fix this, increase memory of the affected node of control plane nodes.
| `GarbageCollectorSyncFailed` | warning | 3600s | Garbage Collector had a problem with syncing and monitoring the available resources. Please see KubeControllerManager logs for more details.
| `HAProxyDown` | critical | 300s | This alert fires when metrics report that HAProxy is down.
| `HAProxyReloadFail` | warning | 300s | This alert fires when HAProxy fails to reload its configuration, which will result in the router not picking up recently created or modified routes.
| `HighOverallControlPlaneCPU` | warning | 600s | Given three control plane nodes, the overall CPU utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the remaining two must handle the load of the cluster in order to be HA. If the cluster is using more than 2/3 of all capacity, if one control plane node fails, the remaining two are likely to fail when they take the load. To fix this, increase the CPU and memory on your control plane nodes.
| `HighOverallControlPlaneMemory` | warning | 3600s | Given three control plane nodes, the overall memory utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the kube-apiserver and etcd may be slow to respond. To fix this, increase memory of the control plane nodes.
| `ImageRegistryStorageFull` | warning | 600s | The image registry storage disk is full. A full disk affects direct pushes to the image registry, and pull-through proxy caching. In the case of pull-through proxy caching, disk space is particularly important because without it the image registry won't be actually caching anything. Please verify your backing storage solution and make sure the volume mounted on the image-registry pods have enough free disk space to avoid potential outages.
| `ImageRegistryStorageReadOnly` | warning | 600s | The image registry storage is read-only. Read-only storage affects direct pushes to the image registry, and pull-through proxy caching. In the case of pull-through proxy caching, read-only storage is particularly important because without it the image registry won't be actually caching anything. Please verify your backing storage solution and make sure the volume mounted on the image-registry pods is writable to avoid potential outages.
| `IngressControllerDegraded` | warning | 300s | This alert fires when the IngressController status is degraded.
| `IngressControllerUnavailable` | warning | 300s | This alert fires when the IngressController is not available.
| `IngressWithoutClassName` | warning | 86400s | This alert fires when there is an Ingress with an unset IngressClassName for longer than one day.
| `InsightsDisabled` | info | 300s | Insights operator is disabled. In order to enable Insights and benefit from recommendations specific to your cluster, please follow steps listed in the documentation: https://docs.openshift.com/container-platform/latest/support/remote_health_monitoring/enabling-remote-health-reporting.html
| `InsightsRecommendationActive` | info | 300s | Insights recommendation "$labels.description" with total risk "$labels.total_risk" was detected on the cluster. More information is available at $labels.info_link.
| `InstallPlanStepAppliedWithWarnings` | warning | 0s | The API server returned a warning during installation or upgrade of an operator. An Event with reason "AppliedWithWarnings" has been created with complete details, including a reference to the InstallPlan step that generated the warning.
| `KubeAggregatedAPIDown` | warning | 900s | Kubernetes aggregated API $labels.name/$labels.name has been only $value% available over the last 10m.
| `KubeAggregatedAPIErrors` | warning | 600s | Kubernetes aggregated API $labels.instance/$labels.name has reported $labels.reason errors.
| `KubeAPIDown` | critical | 900s | KubeAPI has disappeared from Prometheus target discovery.
| `KubeAPIErrorBudgetBurn` | critical | 120s | The API server is burning too much error budget. This alert fires when too many requests are failing with high latency. Use the 'API Performance' monitoring dashboards to narrow down the request states and latency. The 'etcd' monitoring dashboards also provides metrics to help determine etcd stability and performance.
| `KubeAPIErrorBudgetBurn` | critical | 900s | The API server is burning too much error budget. This alert fires when too many requests are failing with high latency. Use the 'API Performance' monitoring dashboards to narrow down the request states and latency. The 'etcd' monitoring dashboards also provides metrics to help determine etcd stability and performance.
| `KubeAPIErrorBudgetBurn` | warning | 3600s | The API server is burning too much error budget. This alert fires when too many requests are failing with high latency. Use the 'API Performance' monitoring dashboards to narrow down the request states and latency. The 'etcd' monitoring dashboards also provides metrics to help determine etcd stability and performance.
| `KubeAPIErrorBudgetBurn` | warning | 10800s | The API server is burning too much error budget. This alert fires when too many requests are failing with high latency. Use the 'API Performance' monitoring dashboards to narrow down the request states and latency. The 'etcd' monitoring dashboards also provides metrics to help determine etcd stability and performance.
| `KubeAPITerminatedRequests` | warning | 300s | The kubernetes apiserver has terminated $value of its incoming requests.
| `KubeClientErrors` | warning | 900s | Kubernetes API server client '$labels.job/$labels.instance' is experiencing $value errors.
| `KubeContainerWaiting` | warning | 3600s | pod/$labels.pod in namespace $labels.namespace on container $labels.container has been in waiting state for longer than 1 hour. (reason: "$labels.reason").
| `KubeControllerManagerDown` | critical | 900s | KubeControllerManager has disappeared from Prometheus target discovery.
| `KubeCPUOvercommit` | warning | 600s | Cluster has overcommitted CPU resource requests for Pods by $value CPU shares and cannot tolerate node failure.
| `KubeDaemonSetMisScheduled` | warning | 900s | $value Pods of DaemonSet $labels.namespace/$labels.daemonset are running where they are not supposed to run.
| `KubeDaemonSetNotScheduled` | warning | 600s | $value Pods of DaemonSet $labels.namespace/$labels.daemonset are not scheduled.
| `KubeDaemonSetRolloutStuck` | warning | 1800s | DaemonSet $labels.namespace/$labels.daemonset has not finished or progressed for at least 30 minutes.
| `KubeDeploymentGenerationMismatch` | warning | 900s | Deployment generation for $labels.namespace/$labels.deployment does not match, this indicates that the Deployment has failed but has not been rolled back.
| `KubeDeploymentReplicasMismatch` | warning | 900s | Deployment $labels.namespace/$labels.deployment has not matched the expected number of replicas for longer than 15 minutes. This indicates that cluster infrastructure is unable to start or restart the necessary components. This most often occurs when one or more nodes are down or partioned from the cluster, or a fault occurs on the node that prevents the workload from starting. In rare cases this may indicate a new version of a cluster component cannot start due to a bug or configuration error. Assess the pods for this deployment to verify they are running on healthy nodes and then contact support.
| `KubeDeploymentRolloutStuck` | warning | 900s | Rollout of deployment $labels.namespace/$labels.deployment is not progressing for longer than 15 minutes.
| `KubeHpaMaxedOut` | warning | 900s | HPA $labels.namespace/$labels.horizontalpodautoscaler has been running at max replicas for longer than 15 minutes.
| `KubeHpaReplicasMismatch` | warning | 900s | HPA $labels.namespace/$labels.horizontalpodautoscaler has not matched the desired number of replicas for longer than 15 minutes.
| `KubeJobFailed` | warning | 900s | Job $labels.namespace/$labels.job_name failed to complete. Removing failed job after investigation should clear this alert.
| `KubeJobNotCompleted` | warning | 0s | Job $labels.namespace/$labels.job_name is taking more than to complete.
| `KubeletClientCertificateRenewalErrors` | warning | 900s | Kubelet on node $labels.node has failed to renew its client certificate ($value errors in the last 5 minutes).
| `KubeletDown` | critical | 900s | Kubelet has disappeared from Prometheus target discovery.
| `KubeletHealthState` | warning | 0s | Kubelet health failure threshold reached
| `KubeletPlegDurationHigh` | warning | 300s | The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of $value seconds on node $labels.node.
| `KubeletPodStartUpLatencyHigh` | warning | 900s | Kubelet Pod startup 99th percentile latency is $value seconds on node $labels.node.
| `KubeletServerCertificateRenewalErrors` | warning | 900s | Kubelet on node $labels.node has failed to renew its server certificate ($value errors in the last 5 minutes).
| `KubeletTooManyPods` | info | 900s | Kubelet '$labels.node' is running at $value of its Pod capacity.
| `KubeMemoryOvercommit` | warning | 600s | Cluster has overcommitted memory resource requests for Pods by $value bytes and cannot tolerate node failure.
| `KubeNodeEviction` | info | 0s | Node $labels.node is evicting Pods due to $labels.eviction_signal. Eviction occurs when eviction thresholds are crossed, typically caused by Pods exceeding RAM/ephemeral-storage limits.
| `KubeNodeNotReady` | warning | 900s | $labels.node has been unready for more than 15 minutes.
| `KubeNodePressure` | info | 600s | $labels.node has active Condition $labels.condition. This is caused by resource usage exceeding eviction thresholds.
| `KubeNodeReadinessFlapping` | warning | 900s | The readiness status of node $labels.node has changed $value times in the last 15 minutes.
| `KubeNodeUnreachable` | warning | 900s | $labels.node is unreachable and some workloads may be rescheduled.
| `KubePdbNotEnoughHealthyPods` | warning | 900s | PDB $labels.namespace/$labels.poddisruptionbudget expects $value more healthy pods. The desired number of healthy pods has not been met for at least 15m.
| `KubePersistentVolumeErrors` | warning | 300s | The persistent volume $labels.persistentvolume $labels.cluster on Cluster has status $labels.phase.
| `KubePersistentVolumeFillingUp` | critical | 60s | The PersistentVolume claimed by $labels.persistentvolumeclaim in Namespace $labels.namespace $labels.cluster on Cluster is only $value free.
| `KubePersistentVolumeFillingUp` | warning | 3600s | Based on recent sampling, the PersistentVolume claimed by $labels.persistentvolumeclaim in Namespace $labels.namespace $labels.cluster on Cluster is expected to fill up within four days. Currently $value is available.
| `KubePersistentVolumeInodesFillingUp` | critical | 60s | The PersistentVolume claimed by $labels.persistentvolumeclaim in Namespace $labels.namespace $labels.cluster on Cluster only has $value free inodes.
| `KubePersistentVolumeInodesFillingUp` | warning | 3600s | Based on recent sampling, the PersistentVolume claimed by $labels.persistentvolumeclaim in Namespace $labels.namespace $labels.cluster on Cluster is expected to run out of inodes within four days. Currently $value of its inodes are free.
| `KubePodCrashLooping` | warning | 900s | Pod $labels.namespace/$labels.pod ($labels.container) is in waiting state (reason: "CrashLoopBackOff").
| `KubePodNotReady` | warning | 900s | Pod $labels.namespace/$labels.pod has been in a non-ready state for longer than 15 minutes.
| `KubePodNotScheduled` | warning | 1800s | Pod $labels.namespace/$labels.pod cannot be scheduled for more than 30 minutes. Check the details of the pod with the following command: oc describe -n $labels.namespace pod $labels.pod
| `KubeQuotaAlmostFull` | info | 900s | Namespace $labels.namespace is using $value of its $labels.resource quota.
| `KubeQuotaExceeded` | warning | 900s | Namespace $labels.namespace is using $value of its $labels.resource quota.
| `KubeQuotaFullyUsed` | info | 900s | Namespace $labels.namespace is using $value of its $labels.resource quota.
| `KubeSchedulerDown` | critical | 900s | KubeScheduler has disappeared from Prometheus target discovery.
| `KubeStatefulSetGenerationMismatch` | warning | 900s | StatefulSet generation for $labels.namespace/$labels.statefulset does not match, this indicates that the StatefulSet has failed but has not been rolled back.
| `KubeStatefulSetReplicasMismatch` | warning | 900s | StatefulSet $labels.namespace/$labels.statefulset has not matched the expected number of replicas for longer than 15 minutes.
| `KubeStatefulSetUpdateNotRolledOut` | warning | 900s | StatefulSet $labels.namespace/$labels.statefulset update has not been rolled out.
| `KubeStateMetricsListErrors` | warning | 900s | kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
| `KubeStateMetricsWatchErrors` | warning | 900s | kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
| `MachineAPIOperatorMetricsCollectionFailing` | warning | 300s | For more details: oc logs <machine-api-operator-pod-name> -n openshift-machine-api
| `MachineApproverMaxPendingCSRsReached` | warning | 300s | The number of pending CertificateSigningRequests has exceeded the maximum threshold (current number of machine + 100). Check the pending CSRs to determine which machines need approval, also check that the nodelink controller is running in the openshift-machine-api namespace.
| `MachineHealthCheckUnterminatedShortCircuit` | warning | 1800s | The number of unhealthy machines has exceeded the `maxUnhealthy` limit for the check, you should check the status of machines in the cluster.
| `MachineNotYetDeleted` | warning | 21600s | The machine is not properly deleting, this may be due to a configuration issue with the infrastructure provider, or because workloads on the node have PodDisruptionBudgets or long termination periods which are preventing deletion.
| `MachineWithNoRunningPhase` | warning | 3600s | The machine has been without a Running or Deleting phase for more than 60 minutes. The machine may not have been provisioned properly from the infrastructure provider, or it might have issues with CertificateSigningRequests being approved.
| `MachineWithoutValidNode` | warning | 3600s | If the machine never became a node, you should diagnose the machine related failures. If the node was deleted from the API, you may delete the machine if appropriate.
| `MCCDrainError` | warning | 0s | Drain failed on $labels.exported_node , updates may be blocked. For more details check MachineConfigController pod logs: oc logs -f -n $labels.namespace machine-config-controller-xxxxx -c machine-config-controller
| `MCCPoolAlert` | warning | 0s | Node $labels.exported_node has triggered a pool alert due to a label change. For more details check MachineConfigController pod logs: oc logs -f -n $labels.namespace machine-config-controller-xxxxx -c machine-config-controller
| `MCDPivotError` | warning | 120s | Error detected in pivot logs on $labels.node , upgrade may be blocked. For more details: oc logs -f -n $labels.namespace $labels.pod -c machine-config-daemon
| `MCDRebootError` | critical | 300s | Reboot failed on $labels.node , update may be blocked. For more details: oc logs -f -n $labels.namespace $labels.pod -c machine-config-daemon
| `MissingMachineConfig` | warning | 0s | Could not find config $labels.mc in-cluster, this likely indicates the MachineConfigs in-cluster has changed during the install process. If you are seeing this when installing the cluster, please compare the in-cluster rendered machineconfigs to /etc/mcs-machine-config-content.json
| `MultipleDefaultStorageClasses` | warning | 600s | Cluster storage operator monitors all storage classes configured in the cluster and checks there is not more than one default StorageClass configured.
| `NodeBondingDegraded` | warning | 300s | Bonding interface $labels.master on $labels.instance is in degraded state due to one or more slave failures.
| `NodeClockNotSynchronising` | critical | 600s | Clock at $labels.instance is not synchronising. Ensure NTP is configured on this host.
| `NodeClockSkewDetected` | warning | 600s | Clock at $labels.instance is out of sync by more than 0.05s. Ensure NTP is configured correctly on this host.
| `NodeFileDescriptorLimit` | warning | 900s | File descriptors limit at $labels.instance is currently at $value%.
| `NodeFileDescriptorLimit` | critical | 900s | File descriptors limit at $labels.instance is currently at $value%.
| `NodeFilesystemAlmostOutOfFiles` | warning | 3600s | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available inodes left.
| `NodeFilesystemAlmostOutOfFiles` | critical | 3600s | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available inodes left.
| `NodeFilesystemAlmostOutOfSpace` | warning | 1800s | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available space left.
| `NodeFilesystemAlmostOutOfSpace` | critical | 1800s | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available space left.
| `NodeFilesystemFilesFillingUp` | warning | 3600s | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available inodes left and is filling up.
| `NodeFilesystemFilesFillingUp` | critical | 3600s | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available inodes left and is filling up fast.
| `NodeFilesystemSpaceFillingUp` | warning | 3600s | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available space left and is filling up.
| `NodeFilesystemSpaceFillingUp` | critical | 3600s | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available space left and is filling up fast.
| `NodeHighNumberConntrackEntriesUsed` | warning | 0s | $value of conntrack entries are used.
| `NodeMemoryMajorPagesFaults` | warning | 900s | Memory major pages are occurring at very high rate at $labels.instance, 500 major page faults per second for the last 15 minutes, is currently at $value. Please check that there is enough memory available at this instance.
| `NodeNetworkInterfaceFlapping` | warning | 120s | Network interface "$labels.device" changing its up status often on node-exporter $labels.namespace/$labels.pod
| `NodeNetworkReceiveErrs` | warning | 3600s | $labels.instance interface $labels.device has encountered $value receive errors in the last two minutes.
| `NodeNetworkTransmitErrs` | warning | 3600s | $labels.instance interface $labels.device has encountered $value transmit errors in the last two minutes.
| `NodeRAIDDegraded` | critical | 900s | RAID array '$labels.device' at $labels.instance is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
| `NodeRAIDDiskFailure` | warning | 0s | At least one device in RAID array at $labels.instance failed. Array '$labels.device' needs attention and possibly a disk swap.
| `NodeSystemdServiceFailed` | warning | 900s | Systemd service $labels.name has entered failed state at $labels.instance
| `NodeSystemSaturation` | warning | 900s | System load per core at $labels.instance has been above 2 for the last 15 minutes, is currently at $value. This might indicate this instance resources saturation and can cause it becoming unresponsive.
| `NodeTextFileCollectorScrapeError` | warning | 0s | Node Exporter text file collector on $labels.instance failed to scrape.
| `NodeWithoutOVNKubeNodePodRunning` | warning | 1200s | Networking is degraded on nodes that do not have a functioning ovnkube-node pod. Existing workloads on the node may continue to have connectivity but any changes to the networking control plane will not be implemented.
| `NoOvnClusterManagerLeader` | critical | 300s | Networking control plane is degraded. Networking configuration updates applied to the cluster will not be implemented while there is no OVN Kubernetes cluster manager leader. Existing workloads should continue to have connectivity. OVN-Kubernetes control plane is not functional.
| `NorthboundStale` | warning | 600s | OVN-Kubernetes controller and/or OVN northbound database may cause a degraded networking control plane for the affected node. Existing workloads should continue to have connectivity but new workloads may be impacted.
| `NoRunningOvnControlPlane` | critical | 300s | Networking control plane is degraded. Networking configuration updates applied to the cluster will not be implemented while there are no OVN Kubernetes control plane pods.
| `NTODegraded` | warning | 7200s | The Node Tuning Operator is degraded. Review the "node-tuning" ClusterOperator object for further details.
| `NTOInvalidTunedExist` | warning | 1800s | Invalid custom Tuned resource exists. View your custom Tuned resources and operator logs for further details.
| `NTOPodLabelsUsed` | warning | 1800s | The Node Tuning Operator is using deprecated functionality. Using pod label matching has been discouraged since OCP 4.4 and this functionality will be removed in future versions. Please revise and adjust your configuration (Tuned custom resources).
| `NTOPodsNotReady` | warning | 1800s | Pod $labels.pod is not ready. Review the "Event" objects in "openshift-cluster-node-tuning-operator" namespace for further details.
| `OperatorHubSourceError` | warning | 600s | Operators shipped via the $labels.name source are not available for installation until the issue is fixed. Operators already installed from this source will not receive updates until issue is fixed. Inspect the status of the pod owned by $labels.name source in the openshift-marketplace namespace (oc -n openshift-marketplace get pods -l olm.catalogSource=$labels.name) to diagnose and repair.
| `OVNKubernetesControllerDisconnectedSouthboundDatabase` | warning | 600s | Networking is degraded on nodes when OVN controller is not connected to OVN southbound database connection. No networking control plane updates will be applied to the node.
| `OVNKubernetesNodeOVSOverflowKernel` | warning | 900s | Netlink messages dropped by OVS kernel module due to netlink socket buffer overflow. This will result in packet loss.
| `OVNKubernetesNodeOVSOverflowUserspace` | warning | 900s | Netlink messages dropped by OVS vSwitch daemon due to netlink socket buffer overflow. This will result in packet loss.
| `OVNKubernetesNodePodAddError` | warning | 900s | OVN Kubernetes experiences pod creation errors at an elevated rate. The pods will be retried.
| `OVNKubernetesNodePodDeleteError` | warning | 900s | OVN Kubernetes experiences pod deletion errors at an elevated rate. The pods will be retried.
| `OVNKubernetesNorthboundDatabaseCPUUsageHigh` | info | 900s | High OVN northbound CPU usage indicates high load on the networking control plane for the affected node.
| `OVNKubernetesNorthdInactive` | warning | 600s | An inactive OVN northd instance may cause a degraded networking control plane for the affected node. Existing workloads should continue to have connectivity but new workloads may be impacted.
| `OVNKubernetesResourceRetryFailure` | warning | 0s | OVN Kubernetes failed to apply networking control plane configuration after several attempts. This might be because the configuration provided by the user is invalid or because of an internal error. As a consequence, the cluster might have a degraded status.
| `OVNKubernetesSouthboundDatabaseCPUUsageHigh` | info | 900s | High OVN southbound CPU usage indicates high load on the networking control plane for the affected node.
| `PodDisruptionBudgetAtLimit` | warning | 3600s | The pod disruption budget is at the minimum disruptions allowed level. The number of current healthy pods is equal to the desired healthy pods.
| `PodDisruptionBudgetLimit` | critical | 900s | The pod disruption budget is below the minimum disruptions allowed level and is not satisfied. The number of current healthy pods is less than the desired healthy pods.
| `PodSecurityViolation` | info | 0s | A workload (pod, deployment, daemonset, ...) was created somewhere in the cluster but it did not match the PodSecurity "$labels.policy_level" profile defined by its namespace either via the cluster-wide configuration (which triggers on a "restricted" profile violations) or by the namespace local Pod Security labels. Refer to Kubernetes documentation on Pod Security Admission to learn more about these violations.
| `PodSecurityViolation` | info | 0s | A workload (pod, deployment, daemonset, ...) was created in namespace "$labels.ocp_namespace" but it did not match the PodSecurity "$labels.policy_level" profile defined by its namespace either via the cluster-wide configuration (which triggers on a "restricted" profile violations) or by the namespace local Pod Security labels. Refer to Kubernetes documentation on Pod Security Admission to learn more about these violations.
| `PodStartupStorageOperationsFailing` | info | 300s | Failing storage operation "$labels.operation_name" of volume plugin $labels.volume_plugin was preventing Pods$labels.node on node $labels.node from starting for past 5 minutes. $labels.operation_name Please investigate Pods that are "ContainerCreating" on the node: "oc get pod --field-selector=spec.nodeName=$labels.node --all-namespaces \| grep ContainerCreating". $labels.operation_name Please investigate Pods that are "ContainerCreating" across all nodes: "oc get pod --all-namespaces \| grep ContainerCreating". Check volume attachment status: "oc get volumeattachment" and controller manager logs. Please investigate Pods that are "ContainerCreating"$labels.node on node $labels.node. Events of the Pods should contain exact error message: "oc describe pod -n <pod namespace> <pod name>".
| `PrometheusBadConfig` | warning | 600s | Prometheus $labels.namespace/$labels.pod has failed to reload its configuration.
| `PrometheusDuplicateTimestamps` | warning | 3600s | Prometheus $labels.namespace/$labels.pod is dropping $value samples/s with different values but duplicated timestamp.
| `PrometheusErrorSendingAlertsToSomeAlertmanagers` | warning | 900s | $value% of alerts sent by Prometheus $labels.namespace/$labels.pod to Alertmanager $labels.alertmanager were affected by errors.
| `PrometheusHighQueryLoad` | warning | 900s | Prometheus $labels.namespace/$labels.pod query API has less than 20% available capacity in its query engine for the last 15 minutes.
| `PrometheusKubernetesListWatchFailures` | warning | 900s | Kubernetes service discovery of Prometheus $labels.namespace/$labels.pod is experiencing $value failures with LIST/WATCH requests to the Kubernetes API in the last 5 minutes.
| `PrometheusLabelLimitHit` | warning | 900s | Prometheus $labels.namespace/$labels.pod has dropped $value targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.
| `PrometheusMissingRuleEvaluations` | warning | 900s | Prometheus $labels.namespace/$labels.pod has missed $value rule group evaluations in the last 5m.
| `PrometheusNotConnectedToAlertmanagers` | warning | 600s | Prometheus $labels.namespace/$labels.pod is not connected to any Alertmanagers.
| `PrometheusNotificationQueueRunningFull` | warning | 900s | Alert notification queue of Prometheus $labels.namespace/$labels.pod is running full.
| `PrometheusNotIngestingSamples` | warning | 600s | Prometheus $labels.namespace/$labels.pod is not ingesting samples.
| `PrometheusOperatorListErrors` | warning | 900s | Errors while performing List operations in controller $labels.controller in $labels.namespace namespace.
| `PrometheusOperatorNodeLookupErrors` | warning | 600s | Errors while reconciling Prometheus in $labels.namespace Namespace.
| `PrometheusOperatorNotReady` | warning | 300s | Prometheus operator in $labels.namespace namespace isn't ready to reconcile $labels.controller resources.
| `PrometheusOperatorReconcileErrors` | warning | 600s | $value of reconciling operations failed for $labels.controller controller in $labels.namespace namespace.
| `PrometheusOperatorRejectedResources` | warning | 300s | Prometheus operator in $labels.namespace namespace rejected $value $labels.controller/$labels.resource resources.
| `PrometheusOperatorStatusUpdateErrors` | warning | 600s | $value of status update operations failed for $labels.controller controller in $labels.namespace namespace.
| `PrometheusOperatorSyncFailed` | warning | 600s | Controller $labels.controller in $labels.namespace namespace fails to reconcile $value objects.
| `PrometheusOperatorWatchErrors` | warning | 900s | Errors while performing watch operations in controller $labels.controller in $labels.namespace namespace.
| `PrometheusOutOfOrderTimestamps` | warning | 3600s | Prometheus $labels.namespace/$labels.pod is dropping $value samples/s with timestamps arriving out of order.
| `PrometheusPossibleNarrowSelectors` | warning | 900s | Queries or/and relabel configs on Prometheus/Thanos $labels.namespace/$labels.pod could be too restrictive.
| `PrometheusRemoteStorageFailures` | warning | 900s | Prometheus $labels.namespace/$labels.pod failed to send $value% of the samples to $labels.remote_name:$labels.url
| `PrometheusRemoteWriteBehind` | info | 900s | Prometheus $labels.namespace/$labels.pod remote write is $value behind for $labels.remote_name:$labels.url.
| `PrometheusRemoteWriteDesiredShards` | warning | 900s | Prometheus $labels.namespace/$labels.pod remote write desired shards calculation wants to run $value shards for queue $labels.remote_name:$labels.url, which is more than the max of $value.
| `PrometheusRuleFailures` | warning | 900s | Prometheus $labels.namespace/$labels.pod has failed to evaluate $value rules in the last 5m.
| `PrometheusScrapeBodySizeLimitHit` | warning | 900s | Prometheus $labels.namespace/$labels.pod has failed $value scrapes in the last 5m because some targets exceeded the configured body_size_limit.
| `PrometheusScrapeSampleLimitHit` | warning | 900s | Prometheus $labels.namespace/$labels.pod has failed $value scrapes in the last 5m because some targets exceeded the configured sample_limit.
| `PrometheusSDRefreshFailure` | warning | 1200s | Prometheus $labels.namespace/$labels.pod has failed to refresh SD with mechanism $labels.mechanism.
| `PrometheusTargetLimitHit` | warning | 900s | Prometheus $labels.namespace/$labels.pod has dropped $value targets because the number of targets exceeded the configured target_limit.
| `PrometheusTargetSyncFailure` | critical | 300s | $value targets in Prometheus $labels.namespace/$labels.pod have failed to sync because invalid configuration was supplied.
| `PrometheusTSDBCompactionsFailing` | warning | 14400s | Prometheus $labels.namespace/$labels.pod has detected $value compaction failures over the last 3h.
| `PrometheusTSDBReloadsFailing` | warning | 14400s | Prometheus $labels.namespace/$labels.pod has detected $value reload failures over the last 3h.
| `SamplesDegraded` | warning | 7200s | Samples could not be deployed and the operator is degraded. Review the "openshift-samples" ClusterOperator object for further details.
| `SamplesImagestreamImportFailing` | warning | 7200s | Samples operator is detecting problems with imagestream image imports. You can look at the "openshift-samples" ClusterOperator object for details. Most likely there are issues with the external image registry hosting the images that needs to be investigated. Or you can consider marking samples operator Removed if you do not care about having sample imagestreams available. The list of ImageStreams for which samples operator is retrying imports:
| `SamplesInvalidConfig` | warning | 7200s | Samples operator has been given an invalid configuration.
| `SamplesMissingSecret` | warning | 7200s | Samples operator cannot find the samples pull secret in the openshift namespace.
| `SamplesMissingTBRCredential` | warning | 7200s | The samples operator cannot find credentials for 'registry.redhat.io'. Many of the sample ImageStreams will fail to import unless the 'samplesRegistry' in the operator configuration is changed.
| `SamplesRetriesMissingOnImagestreamImportFailing` | warning | 7200s | Samples operator is detecting problems with imagestream image imports, and the periodic retries of those imports are not occurring. Contact support. You can look at the "openshift-samples" ClusterOperator object for details. Most likely there are issues with the external image registry hosting the images that need to be investigated. The list of ImageStreams that have failing imports are: However, the list of ImageStreams for which samples operator is retrying imports is: retrying imports:
| `SamplesTBRInaccessibleOnBoot` | info | 172800s | One of two situations has occurred. Either samples operator could not access 'registry.redhat.io' during its initial installation and it bootstrapped as removed. If this is expected, and stems from installing in a restricted network environment, please note that if you plan on mirroring images associated with sample imagestreams into a registry available in your restricted network environment, and subsequently moving samples operator back to 'Managed' state, a list of the images associated with each image stream tag from the samples catalog is provided in the 'imagestreamtag-to-image' config map in the 'openshift-cluster-samples-operator' namespace to assist the mirroring process. Or, the use of allowed registries or blocked registries with global imagestream configuration will not allow samples operator to create imagestreams using the default image registry 'registry.redhat.io'.
| `SchedulerLegacyPolicySet` | warning | 3600s | The scheduler is currently configured to use a legacy scheduler policy API. Use of the policy API is deprecated and removed in 4.10.
| `SimpleContentAccessNotAvailable` | info | 300s | Simple content access (SCA) is not enabled. Once enabled, Insights Operator can automatically import the SCA certificates from Red Hat OpenShift Cluster Manager making it easier to use the content provided by your Red Hat subscriptions when creating container images. See https://docs.openshift.com/container-platform/latest/cicd/builds/running-entitled-builds.html for more information.
| `SouthboundStale` | warning | 600s | OVN-Kubernetes controller and/or OVN northbound database may cause a degraded networking control plane for the affected node. Existing workloads should continue to have connectivity but new workloads may be impacted.
| `SystemMemoryExceedsReservation` | warning | 900s | System memory usage of $value on $labels.node exceeds 95% of the reservation. Reserved memory ensures system processes can function even when the node is fully allocated and protects against workload out of memory events impacting the proper functioning of the node. The default reservation is expected to be sufficient for most configurations and should be increased (https://docs.openshift.com/container-platform/latest/nodes/nodes/nodes-nodes-managing.html) when running nodes with high numbers of pods (either due to rate of change or at steady state).
| `TargetDown` | warning | 900s | $value% of the $labels.job/$labels.service targets in $labels.namespace namespace have been unreachable for more than 15 minutes. This may be a symptom of network connectivity issues, down nodes, or failures within these components. Assess the health of the infrastructure and nodes running these targets and then contact support.
| `TechPreviewNoUpgrade` | warning | 600s | Cluster has enabled Technology Preview features that cannot be undone and will prevent upgrades. The TechPreviewNoUpgrade feature set is not recommended on production clusters.
| `TelemeterClientFailures` | warning | 3600s | The telemeter client in namespace $labels.namespace fails $value of the requests to the telemeter service. Check the logs of the telemeter-client pod with the following command: oc logs -n openshift-monitoring deployment.apps/telemeter-client -c telemeter-client If the telemeter client fails to authenticate with the telemeter service, make sure that the global pull secret is up to date, see https://docs.openshift.com/container-platform/latest/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets for more details.
| `ThanosQueryGrpcClientErrorRate` | warning | 3600s | Thanos Query $labels.job in $labels.namespace is failing to send $value% of requests.
| `ThanosQueryGrpcServerErrorRate` | warning | 3600s | Thanos Query $labels.job in $labels.namespace is failing to handle $value% of requests.
| `ThanosQueryHighDNSFailures` | warning | 3600s | Thanos Query $labels.job in $labels.namespace have $value% of failing DNS queries for store endpoints.
| `ThanosQueryHttpRequestQueryErrorRateHigh` | warning | 3600s | Thanos Query $labels.job in $labels.namespace is failing to handle $value% of "query" requests.
| `ThanosQueryHttpRequestQueryRangeErrorRateHigh` | warning | 3600s | Thanos Query $labels.job in $labels.namespace is failing to handle $value% of "query_range" requests.
| `ThanosQueryOverload` | warning | 3600s | Thanos Query $labels.job in $labels.namespace has been overloaded for more than 15 minutes. This may be a symptom of excessive simultaneous complex requests, low performance of the Prometheus API, or failures within these components. Assess the health of the Thanos query instances, the connected Prometheus instances, look for potential senders of these requests and then contact support.
| `ThanosSidecarBucketOperationsFailed` | warning | 3600s | Thanos Sidecar $labels.instance in $labels.namespace bucket operations are failing
| `ThanosSidecarNoConnectionToStartedPrometheus` | warning | 3600s | Thanos Sidecar $labels.instance in $labels.namespace is unhealthy.
| `UnmanagedRoutes` | warning | 3600s | This alert fires when there is a Route owned by an unmanaged Ingress.
| `UpdateAvailable` | info | 0s | For more information refer to 'oc adm upgrade' or /settings/cluster/.
| `V4SubnetAllocationThresholdExceeded` | warning | 600s | More than 80% of IPv4 subnets are used. Insufficient IPv4 subnets could degrade provisioning of workloads.
| `V6SubnetAllocationThresholdExceeded` | warning | 600s | More than 80% of IPv6 subnets are used. Insufficient IPv6 subnets could degrade provisioning of workloads.
| `Watchdog` | none | 0s | This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the "DeadMansSnitch" integration in PagerDuty.
|===
