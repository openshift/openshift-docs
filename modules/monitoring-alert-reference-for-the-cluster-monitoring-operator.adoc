:_mod-docs-content-type: REFERENCE
[id="alert-reference-for-the-cluster-monitoring-operator_{context}"]
= Alert reference for the {cmo-full}

[role="_abstract"]
Learn about alerting rules that are managed by the {cmo-first} and are included in your cluster by default.

[IMPORTANT]
====
* The following list of alerting rules is not exhaustive and can vary based on different configurations and installed Operators.

* In the following alert descriptions, `$value` and `$labels.*` represent value and label data that are substituted with actual values when the alert fires.
====

[options="header"]
|===
|Name |Severity |Duration |Description

| `AlertmanagerClusterDown` | warning | 5m | $value of Alertmanager instances within the $labels.job cluster have been up for less than half of the last 5m.
| `AlertmanagerClusterFailedToSendAlerts` | warning | 5m | The minimum notification failure rate to $labels.integration sent from any instance in the $labels.job cluster is $value.
| `AlertmanagerConfigInconsistent` | warning | 20m | Alertmanager instances within the $labels.job cluster have different configurations.
| `AlertmanagerFailedReload` | critical | 10m | Configuration has failed to load for $labels.namespace/$labels.pod.
| `AlertmanagerFailedToSendAlerts` | warning | 5m | Alertmanager $labels.namespace/$labels.pod failed to send $value of notifications to $labels.integration.
| `AlertmanagerMembersInconsistent` | warning | 15m | Alertmanager $labels.namespace/$labels.pod has only found $value members of the $labels.job cluster.
| `AlertmanagerReceiversNotConfigured` | warning | 10m | Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager.
| `ClusterMonitoringOperatorDeprecatedConfig` | info | 1h | The configuration field $labels.field in $labels.configmap was deprecated in $labels.deprecation_version and has no effect.
| `ClusterMonitoringOperatorReconciliationErrors` | warning | 1h | Errors are occurring during reconciliation cycles. Inspect the cluster-monitoring-operator log for potential root causes.
| `ConfigReloaderSidecarErrors` | warning | 10m | Errors encountered while the $labels.pod config-reloader sidecar attempts to sync config in $labels.namespace namespace. As a result, configuration for service running in $labels.pod may be stale and cannot be updated anymore.
| `KubeAggregatedAPIDown` | warning | 15m | Kubernetes aggregated API $labels.name/$labels.name has been only $value% available over the last 10m.
| `KubeAggregatedAPIErrors` | warning | 10m | Kubernetes aggregated API $labels.instance/$labels.name has reported $labels.reason errors.
| `KubeAPIDown` | critical | 15m | KubeAPI has disappeared from Prometheus target discovery.
| `KubeAPITerminatedRequests` | warning | 5m | The kubernetes apiserver has terminated $value of its incoming requests.
| `KubeClientErrors` | warning | 15m | Kubernetes API server client '$labels.job/$labels.instance' is experiencing $value errors.
| `KubeContainerWaiting` | warning | 1h | pod/$labels.pod in namespace $labels.namespace on container $labels.container has been in waiting state for longer than 1 hour. (reason: "$labels.reason").
| `KubeCPUOvercommit` | warning | 10m | Cluster has overcommitted CPU resource requests for Pods by $value CPU shares and cannot tolerate node failure.
| `KubeDaemonSetMisScheduled` | warning | 15m | $value Pods of DaemonSet $labels.namespace/$labels.daemonset are running where they are not supposed to run.
| `KubeDaemonSetNotScheduled` | warning | 10m | $value Pods of DaemonSet $labels.namespace/$labels.daemonset are not scheduled.
| `KubeDaemonSetRolloutStuck` | warning | 30m | DaemonSet $labels.namespace/$labels.daemonset has not finished or progressed for at least 30 minutes.
| `KubeDeploymentGenerationMismatch` | warning | 15m | Deployment generation for $labels.namespace/$labels.deployment does not match, this indicates that the Deployment has failed but has not been rolled back.
| `KubeDeploymentReplicasMismatch` | warning | 15m | Deployment $labels.namespace/$labels.deployment has not matched the expected number of replicas for longer than 15 minutes. This indicates that cluster infrastructure is unable to start or restart the necessary components. This most often occurs when one or more nodes are down or partioned from the cluster, or a fault occurs on the node that prevents the workload from starting. In rare cases this may indicate a new version of a cluster component cannot start due to a bug or configuration error. Assess the pods for this deployment to verify they are running on healthy nodes and then contact support.
| `KubeDeploymentRolloutStuck` | warning | 15m | Rollout of deployment $labels.namespace/$labels.deployment is not progressing for longer than 15 minutes.
| `KubeHpaMaxedOut` | warning | 15m | HPA $labels.namespace/$labels.horizontalpodautoscaler has been running at max replicas for longer than 15 minutes.
| `KubeHpaReplicasMismatch` | warning | 15m | HPA $labels.namespace/$labels.horizontalpodautoscaler has not matched the desired number of replicas for longer than 15 minutes.
| `KubeJobFailed` | warning | 15m | Job $labels.namespace/$labels.job_name failed to complete. Removing failed job after investigation should clear this alert.
| `KubeJobNotCompleted` | warning | 0s | Job $labels.namespace/$labels.job_name is taking more than to complete.
| `KubeletClientCertificateRenewalErrors` | warning | 15m | Kubelet on node $labels.node has failed to renew its client certificate ($value errors in the last 5 minutes).
| `KubeletDown` | critical | 15m | Kubelet has disappeared from Prometheus target discovery.
| `KubeletPlegDurationHigh` | warning | 5m | The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of $value seconds on node $labels.node.
| `KubeletPodStartUpLatencyHigh` | warning | 15m | Kubelet Pod startup 99th percentile latency is $value seconds on node $labels.node.
| `KubeletServerCertificateRenewalErrors` | warning | 15m | Kubelet on node $labels.node has failed to renew its server certificate ($value errors in the last 5 minutes).
| `KubeletTooManyPods` | info | 15m | Kubelet '$labels.node' is running at $value of its Pod capacity.
| `KubeMemoryOvercommit` | warning | 10m | Cluster has overcommitted memory resource requests for Pods by $value bytes and cannot tolerate node failure.
| `KubeNodeEviction` | info | 0s | Node $labels.node is evicting Pods due to $labels.eviction_signal. Eviction occurs when eviction thresholds are crossed, typically caused by Pods exceeding RAM/ephemeral-storage limits.
| `KubeNodeNotReady` | warning | 15m | $labels.node has been unready for more than 15 minutes.
| `KubeNodePressure` | info | 10m | $labels.node has active Condition $labels.condition. This is caused by resource usage exceeding eviction thresholds.
| `KubeNodeReadinessFlapping` | warning | 15m | The readiness status of node $labels.node has changed $value times in the last 15 minutes.
| `KubeNodeUnreachable` | warning | 15m | $labels.node is unreachable and some workloads may be rescheduled.
| `KubePdbNotEnoughHealthyPods` | warning | 15m | PDB $labels.namespace/$labels.poddisruptionbudget expects $value more healthy pods. The desired number of healthy pods has not been met for at least 15m.
| `KubePodCrashLooping` | warning | 15m | Pod $labels.namespace/$labels.pod ($labels.container) is in waiting state (reason: "CrashLoopBackOff").
| `KubePodNotReady` | warning | 15m | Pod $labels.namespace/$labels.pod has been in a non-ready state for longer than 15 minutes.
| `KubePodNotScheduled` | warning | 30m | Pod $labels.namespace/$labels.pod cannot be scheduled for more than 30 minutes. Check the details of the pod with the following command: oc describe -n $labels.namespace pod $labels.pod
| `KubeQuotaAlmostFull` | info | 15m | Namespace $labels.namespace is using $value of its $labels.resource quota.
| `KubeQuotaExceeded` | warning | 15m | Namespace $labels.namespace is using $value of its $labels.resource quota.
| `KubeQuotaFullyUsed` | info | 15m | Namespace $labels.namespace is using $value of its $labels.resource quota.
| `KubeStatefulSetGenerationMismatch` | warning | 15m | StatefulSet generation for $labels.namespace/$labels.statefulset does not match, this indicates that the StatefulSet has failed but has not been rolled back.
| `KubeStatefulSetReplicasMismatch` | warning | 15m | StatefulSet $labels.namespace/$labels.statefulset has not matched the expected number of replicas for longer than 15 minutes.
| `KubeStatefulSetUpdateNotRolledOut` | warning | 15m | StatefulSet $labels.namespace/$labels.statefulset update has not been rolled out.
| `KubeStateMetricsListErrors` | warning | 15m | kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
| `KubeStateMetricsWatchErrors` | warning | 15m | kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
| `NodeBondingDegraded` | warning | 5m | Bonding interface $labels.master on $labels.instance is in degraded state due to one or more slave failures.
| `NodeClockNotSynchronising` | critical | 10m | Clock at $labels.instance is not synchronising. Ensure NTP is configured on this host.
| `NodeClockSkewDetected` | warning | 10m | Clock at $labels.instance is out of sync by more than 0.05s. Ensure NTP is configured correctly on this host.
| `NodeFileDescriptorLimit` | warning | 15m | File descriptors limit at $labels.instance is currently at $value%.
| `NodeFileDescriptorLimit` | critical | 15m | File descriptors limit at $labels.instance is currently at $value%.
| `NodeFilesystemAlmostOutOfFiles` | warning | 1h | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available inodes left.
| `NodeFilesystemAlmostOutOfFiles` | critical | 1h | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available inodes left.
| `NodeFilesystemAlmostOutOfSpace` | warning | 30m | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available space left.
| `NodeFilesystemAlmostOutOfSpace` | critical | 30m | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available space left.
| `NodeFilesystemFilesFillingUp` | warning | 1h | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available inodes left and is filling up.
| `NodeFilesystemFilesFillingUp` | critical | 1h | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available inodes left and is filling up fast.
| `NodeFilesystemSpaceFillingUp` | warning | 1h | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available space left and is filling up.
| `NodeFilesystemSpaceFillingUp` | critical | 1h | Filesystem on $labels.device, mounted on $labels.mountpoint, at $labels.instance has only $value% available space left and is filling up fast.
| `NodeHighNumberConntrackEntriesUsed` | warning | 0s | $value of conntrack entries are used.
| `NodeMemoryMajorPagesFaults` | warning | 15m | Memory major pages are occurring at very high rate at $labels.instance, 500 major page faults per second for the last 15 minutes, is currently at $value. Please check that there is enough memory available at this instance.
| `NodeNetworkInterfaceFlapping` | warning | 2m | Network interface "$labels.device" changing its up status often on node-exporter $labels.namespace/$labels.pod
| `NodeNetworkReceiveErrs` | warning | 1h | $labels.instance interface $labels.device has encountered $value receive errors in the last two minutes.
| `NodeNetworkTransmitErrs` | warning | 1h | $labels.instance interface $labels.device has encountered $value transmit errors in the last two minutes.
| `NodeRAIDDegraded` | critical | 15m | RAID array '$labels.device' at $labels.instance is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
| `NodeRAIDDiskFailure` | warning | 0s | At least one device in RAID array at $labels.instance failed. Array '$labels.device' needs attention and possibly a disk swap.
| `NodeSystemdServiceFailed` | warning | 15m | Systemd service $labels.name has entered failed state at $labels.instance
| `NodeSystemSaturation` | warning | 15m | System load per core at $labels.instance has been above 2 for the last 15 minutes, is currently at $value. This might indicate this instance resources saturation and can cause it becoming unresponsive.
| `NodeTextFileCollectorScrapeError` | warning | 0s | Node Exporter text file collector on $labels.instance failed to scrape.
| `PrometheusBadConfig` | warning | 10m | Prometheus $labels.namespace/$labels.pod has failed to reload its configuration.
| `PrometheusDuplicateTimestamps` | warning | 1h | Prometheus $labels.namespace/$labels.pod is dropping $value samples/s with different values but duplicated timestamp.
| `PrometheusErrorSendingAlertsToSomeAlertmanagers` | warning | 15m | $value% of alerts sent by Prometheus $labels.namespace/$labels.pod to Alertmanager $labels.alertmanager were affected by errors.
| `PrometheusHighQueryLoad` | warning | 15m | Prometheus $labels.namespace/$labels.pod query API has less than 20% available capacity in its query engine for the last 15 minutes.
| `PrometheusKubernetesListWatchFailures` | warning | 15m | Kubernetes service discovery of Prometheus $labels.namespace/$labels.pod is experiencing $value failures with LIST/WATCH requests to the Kubernetes API in the last 5 minutes.
| `PrometheusLabelLimitHit` | warning | 15m | Prometheus $labels.namespace/$labels.pod has dropped $value targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.
| `PrometheusMissingRuleEvaluations` | warning | 15m | Prometheus $labels.namespace/$labels.pod has missed $value rule group evaluations in the last 5m.
| `PrometheusNotConnectedToAlertmanagers` | warning | 10m | Prometheus $labels.namespace/$labels.pod is not connected to any Alertmanagers.
| `PrometheusNotificationQueueRunningFull` | warning | 15m | Alert notification queue of Prometheus $labels.namespace/$labels.pod is running full.
| `PrometheusNotIngestingSamples` | warning | 10m | Prometheus $labels.namespace/$labels.pod is not ingesting samples.
| `PrometheusOperatorListErrors` | warning | 15m | Errors while performing List operations in controller $labels.controller in $labels.namespace namespace.
| `PrometheusOperatorNodeLookupErrors` | warning | 10m | Errors while reconciling Prometheus in $labels.namespace Namespace.
| `PrometheusOperatorNotReady` | warning | 5m | Prometheus operator in $labels.namespace namespace isn't ready to reconcile $labels.controller resources.
| `PrometheusOperatorReconcileErrors` | warning | 10m | $value of reconciling operations failed for $labels.controller controller in $labels.namespace namespace.
| `PrometheusOperatorRejectedResources` | warning | 5m | Prometheus operator in $labels.namespace namespace rejected $value $labels.controller/$labels.resource resources.
| `PrometheusOperatorStatusUpdateErrors` | warning | 10m | $value of status update operations failed for $labels.controller controller in $labels.namespace namespace.
| `PrometheusOperatorSyncFailed` | warning | 10m | Controller $labels.controller in $labels.namespace namespace fails to reconcile $value objects.
| `PrometheusOperatorWatchErrors` | warning | 15m | Errors while performing watch operations in controller $labels.controller in $labels.namespace namespace.
| `PrometheusOutOfOrderTimestamps` | warning | 1h | Prometheus $labels.namespace/$labels.pod is dropping $value samples/s with timestamps arriving out of order.
| `PrometheusPossibleNarrowSelectors` | warning | 15m | Queries or/and relabel configs on Prometheus/Thanos $labels.namespace/$labels.pod could be too restrictive.
| `PrometheusRemoteStorageFailures` | warning | 15m | Prometheus $labels.namespace/$labels.pod failed to send $value% of the samples to $labels.remote_name:$labels.url
| `PrometheusRemoteWriteBehind` | info | 15m | Prometheus $labels.namespace/$labels.pod remote write is $value behind for $labels.remote_name:$labels.url.
| `PrometheusRemoteWriteDesiredShards` | warning | 15m | Prometheus $labels.namespace/$labels.pod remote write desired shards calculation wants to run $value shards for queue $labels.remote_name:$labels.url, which is more than the max of $value.
| `PrometheusRuleFailures` | warning | 15m | Prometheus $labels.namespace/$labels.pod has failed to evaluate $value rules in the last 5m.
| `PrometheusScrapeBodySizeLimitHit` | warning | 15m | Prometheus $labels.namespace/$labels.pod has failed $value scrapes in the last 5m because some targets exceeded the configured body_size_limit.
| `PrometheusScrapeSampleLimitHit` | warning | 15m | Prometheus $labels.namespace/$labels.pod has failed $value scrapes in the last 5m because some targets exceeded the configured sample_limit.
| `PrometheusSDRefreshFailure` | warning | 20m | Prometheus $labels.namespace/$labels.pod has failed to refresh SD with mechanism $labels.mechanism.
| `PrometheusTargetLimitHit` | warning | 15m | Prometheus $labels.namespace/$labels.pod has dropped $value targets because the number of targets exceeded the configured target_limit.
| `PrometheusTargetSyncFailure` | critical | 5m | $value targets in Prometheus $labels.namespace/$labels.pod have failed to sync because invalid configuration was supplied.
| `PrometheusTSDBCompactionsFailing` | warning | 4h | Prometheus $labels.namespace/$labels.pod has detected $value compaction failures over the last 3h.
| `PrometheusTSDBReloadsFailing` | warning | 4h | Prometheus $labels.namespace/$labels.pod has detected $value reload failures over the last 3h.
| `TargetDown` | warning | 15m | $value% of the $labels.job/$labels.service targets in $labels.namespace namespace have been unreachable for more than 15 minutes. This may be a symptom of network connectivity issues, down nodes, or failures within these components. Assess the health of the infrastructure and nodes running these targets and then contact support.
| `TelemeterClientFailures` | warning | 1h | The telemeter client in namespace $labels.namespace fails $value of the requests to the telemeter service. Check the logs of the telemeter-client pod with the following command: oc logs -n openshift-monitoring deployment.apps/telemeter-client -c telemeter-client If the telemeter client fails to authenticate with the telemeter service, make sure that the global pull secret is up to date, see https://docs.openshift.com/container-platform/latest/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets for more details.
| `ThanosQueryGrpcClientErrorRate` | warning | 1h | Thanos Query $labels.job in $labels.namespace is failing to send $value% of requests.
| `ThanosQueryGrpcServerErrorRate` | warning | 1h | Thanos Query $labels.job in $labels.namespace is failing to handle $value% of requests.
| `ThanosQueryHighDNSFailures` | warning | 1h | Thanos Query $labels.job in $labels.namespace have $value% of failing DNS queries for store endpoints.
| `ThanosQueryHttpRequestQueryErrorRateHigh` | warning | 1h | Thanos Query $labels.job in $labels.namespace is failing to handle $value% of "query" requests.
| `ThanosQueryHttpRequestQueryRangeErrorRateHigh` | warning | 1h | Thanos Query $labels.job in $labels.namespace is failing to handle $value% of "query_range" requests.
| `ThanosQueryOverload` | warning | 1h | Thanos Query $labels.job in $labels.namespace has been overloaded for more than 15 minutes. This may be a symptom of excessive simultaneous complex requests, low performance of the Prometheus API, or failures within these components. Assess the health of the Thanos query instances, the connected Prometheus instances, look for potential senders of these requests and then contact support.
| `ThanosSidecarBucketOperationsFailed` | warning | 1h | Thanos Sidecar $labels.instance in $labels.namespace bucket operations are failing
| `ThanosSidecarNoConnectionToStartedPrometheus` | warning | 1h | Thanos Sidecar $labels.instance in $labels.namespace is unhealthy.
| `Watchdog` | none | 0s | This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the "DeadMansSnitch" integration in PagerDuty.
|===

[id="user-workload-monitoring-alerts_{context}"]
== User workload monitoring alerts

The following alerting rules are included in your cluster if you enable user workload monitoring.

[options="header"]
|===
|Name |Severity |Duration |Description

| `ThanosNoRuleEvaluations` | warning | 5m | Thanos Rule $labels.instance in $labels.namespace did not perform any rule evaluations in the past 10 minutes.
| `ThanosRuleAlertmanagerHighDNSFailures` | warning | 15m | Thanos Rule $labels.instance in $labels.namespace has $value% of failing DNS queries for Alertmanager endpoints.
| `ThanosRuleConfigReloadFailure` | info | 5m | Thanos Rule $labels.job in $labels.namespace has not been able to reload its configuration.
| `ThanosRuleGrpcErrorRate` | warning | 5m | Thanos Rule $labels.job in $labels.namespace is failing to handle $value% of requests.
| `ThanosRuleHighRuleEvaluationFailures` | warning | 5m | Thanos Rule $labels.instance in $labels.namespace is failing to evaluate rules.
| `ThanosRuleHighRuleEvaluationWarnings` | info | 15m | Thanos Rule $labels.instance in $labels.namespace has high number of evaluation warnings.
| `ThanosRuleNoEvaluationFor10Intervals` | info | 5m | Thanos Rule $labels.job in $labels.namespace has rule groups that did not evaluate for at least 10x of their expected interval.
| `ThanosRuleQueryHighDNSFailures` | warning | 15m | Thanos Rule $labels.job in $labels.namespace has $value% of failing DNS queries for query endpoints.
| `ThanosRuleQueueIsDroppingAlerts` | critical | 5m | Thanos Rule $labels.instance in $labels.namespace is failing to queue alerts.
| `ThanosRuleRuleEvaluationLatencyHigh` | warning | 5m | Thanos Rule $labels.instance in $labels.namespace has higher evaluation latency than interval for $labels.rule_group.
| `ThanosRuleSenderIsFailingAlerts` | warning | 5m | Thanos Rule $labels.instance in $labels.namespace is failing to send alerts to alertmanager.
|===