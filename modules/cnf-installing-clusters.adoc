// Module included in the following assemblies:
// Epic CNF-9657 (TELCODOCS-1540) (4.16)
// * edge_computing/cnf-understanding-siteconfig-operator.adoc

:_mod-docs-content-type: CONCEPT
[id="cnf-installing-clusters_{context}"]
= Installing {sno} clusters with the {sco}

Install your clusters with the {sco} by using the default cluster templates.
This example procedure uses the cluster templates for Image Based Install Operator.

.Prerequisites

* You have logged in to the hub cluster as a user with cluster-admin privileges.
* If using {ztp}, you have configured your {ztp} environment.
* You have the default cluster templates.
* You have installed and configured the underlying Operator of your choice. For more information, see "Image-based installations for {sno}" or "Installing an on-premise cluster using the {ai-full}" sections.

.Procedure

. Create your cluster and node templates for the Image-based Install Operator by running the following commands:
+
--
[source,terminal]
----
$ oc apply -f ibi-cluster-templates-v1.yaml
----

[source,terminal]
----
$ oc apply -f ibi-node-templates-v1.yaml
----
--

. Create a YAML file, for example named `clusterinstance-namespace.yaml`, for the target namespace:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace 
metadata:
  name: example-sno
----

.. Create the resource by running the following command on the hub cluster:
+
[source,terminal]
----
$ oc apply -f clusterinstance-namespace.yaml
----

+
[IMPORTANT]
====
The target namespace must be used when creating the pull secret, the BMC secret, extra manifest `ConfigMap` objects, and the `ClusterInstance` CR.
====

. Create a YAML file for pulling images, for example named `pull-secret.yaml`:
+
--
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: pull-secret
  namespace: example-sno <1>
data:
  .dockerconfigjson: <encoded_docker_configuration> <2>
type: kubernetes.io/dockerconfigjson
----
<1> The `namespace` value must match the target namespace.
<2> Specify the base64-encoded configuration file as the value. 
--

.. Create the resource by running the following command on the hub cluster:
+
[source,terminal]
----
$ oc apply -f pull-secret.yaml
----

. Create a YAML file for the BMC secret, for example named `example-bmc-secret.yaml`:
+
--
[source,yaml]
----
apiVersion: v1
data:
  password: <password>
  username: <username>
kind: Secret
metadata:
  name: example-bmh-secret
  namespace: "example-sno" <1>
type: Opaque
----
<1> The `namespace` value must match the target namespace.
--

.. Create the resource by running the following command on the hub cluster:
+
[source,terminal]
----
$ oc apply -f example-bmc-secret.yaml
----

. Create a YAML file for an extra manifest `ConfigMap` object, for example named `enable-crun.yaml`:
+
--
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: enable-crun
  namespace: example-sno <1>
data:
  enable-crun-master.yaml: |
    apiVersion: machineconfiguration.openshift.io/v1
    kind: ContainerRuntimeConfig
    metadata:
      name: enable-crun-master
    spec:
      machineConfigPoolSelector:
        matchLabels:
          pools.operator.machineconfiguration.openshift.io/master: ""
      containerRuntimeConfig:
        defaultRuntime: crun
  enable-crun-worker.yaml: |
    apiVersion: machineconfiguration.openshift.io/v1
    kind: ContainerRuntimeConfig
    metadata:
      name: enable-crun-worker
    spec:
      machineConfigPoolSelector:
        matchLabels:
          pools.operator.machineconfiguration.openshift.io/worker: ""
      containerRuntimeConfig:
        defaultRuntime: crun
----
<1> The `namespace` value must match the target namespace.
--

.. Create the resource by running the following command on the hub cluster:
+
[source,terminal]
----
$ oc apply -f enable-crun.yaml
----

. In the `example-sno` namespace, create the `ClusterInstance` CR, named for example `clusterinstance-ibi.yaml`, and reference your templates and supporting manifests:
+
--
[source,yaml]
----
apiVersion: siteconfig.open-cluster-management.io/v1alpha1
kind: ClusterInstance
metadata:
  name: "example-clusterinstance"
  namespace: "example-sno" <1>
spec:
  holdInstallation: false
  extraManifestsRefs: <2>
    - name: extra-machine-configs
    - name: enable-crun
  pullSecretRef:
    name: "pull-secret" <3>
  [...]
  templateRefs: <4>
    - name: ibi-cluster-templates-v1
      namespace: siteconfig-operator
  [...]
  nodes:
      [...]
      bmcCredentialsName: <5>
        name: "example-bmh-secret"
      [...]
      templateRefs: <6>
        - name: ibi-node-templates-v1
          namespace: siteconfig-operator
      [...]
----
<1> The `namespace` in the `ClusterInstance` CR must match the target namespace that you defined.
<2> Reference the `name` of one or more extra manifests `ConfigMap` objects.
<3> Reference the `name` of your pull secret.
<4> Reference the `name` of the cluster-level templates under the `spec.templateRefs` field. The `namespace` must match the namespace where the Operator is installed.
<5> Reference the `name` of the BMC secret.
<6> Reference the `name` of the node-level templates under the `spec.nodes.templateRefs` field. The `namespace` must match the namespace where the Operator is installed.
--

.. Create the resource by running the following command:
+
[source,terminal]
----
$ oc apply -f clusterinstance-ibi.yaml
----


+
After creating the CR, the {sco} starts reconciling the `ClusterInstance` CR, then validates and renders the installation manifests.
The {sco} continues to monitor for changes in the `ClusterDeployment` CRs to update the cluster installation progress of the corresponding `ClusterInstance` CR.


.Verification

. Monitor the process by running the following command:
+
--
[source,terminal]
----
$ oc get clusterinstance <cluster_name> -n <target_namespace> -o yaml
----

.Example output from status.conditions section for successful manifest generation
[source,terminal]
----
message: Applied site config manifests
reason: Completed
status: "True"
type: RenderedTemplatesApplied
----

For more information about status conditions, see "ClusterInstance CR conditions".
--

. Check the manifests that {sco} rendered by running the following command:
+
[source,terminal]
----
$ oc get clusterinstance <cluster_name> -n <target_namespace> -o jsonpath='{.status.manifestsRendered}'
----

+
include::snippets/cnf-cluster-instance-cr-manifestrendered.adoc[]