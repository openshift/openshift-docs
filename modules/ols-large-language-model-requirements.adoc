// This module is used in the following assemblies:

// * about/ols-about-openshift-lightspeed.adoc

:_mod-docs-content-type: CONCEPT
[id="ols-large-language-model-requirements"]
= Large language model (LLM) requirements
:context: ols-large-language-model-requirements

A large language model (LLM) is a type of machine learning model that interprets and generates human-like language. When an LLM is used with a virtual assistant, the LLM can accurately interpret questions and provide helpful answers in a conversational manner.

The {ols-long} service must have access to an LLM provider. The service does not provide an LLM for you, so you must configure the LLM prior to installing the {ols-long} Operator. 

The {ols-long} service can rely on the following Software as a Service (SaaS) LLM providers: 

* OpenAI

* {azure-openai}

* {watsonx}

If you want to self-host a model, you can use {rhoai} or {rhelai} as your model provider.

[id="ibm-watsonx_{context}"]
== {watsonx}

To use {watsonx} with {ols-official}, you need an account with link:https://www.ibm.com/products/watsonx-ai[IBM Cloud watsonx]. For more information, see the link:https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/welcome-main.html?context=wx[Documentation for IBM watsonx as a Service].

[id="open-ai_{context}"]
== Open AI

To use {openai} with {ols-official}, you need access to the {openai} link:https://openai.com/api/[API platform]. For more information, see the link:https://platform.openai.com/docs/overview[OpenAI developer platform] documentation.

[id="azure-open-ai_{context}"]
== {azure-openai}

To use {azure-official} with {ols-official}, you need access to link:https://azure.microsoft.com/en-us/[{azure-openai}]. For more information, see the link:https://learn.microsoft.com/en-us/azure/ai-services/openai/[Azure OpenAI documentation].

[id="rhelai_{context}"]
== {rhelai} 

{rhelai} is OpenAI API-compatible, and is configured in a similar manner as the OpenAI provider. 

You can configure {rhelai} as the LLM provider. 

Because the {rhel} is in a different environment than the {ols-long} deployment, the model deployment must allow access using a secure connection. For more information, see link:https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.2/html-single/building_your_rhel_ai_environment/index#creating_secure_endpoint[Optional: Allowing access to a model from a secure endpoint].

{ols-long} version 1.0 and later supports vLLM Server version 0.8.4 and later. When self-hosting an LLM with {rhelai}, you can use vLLM Server as the inference engine.

[id="rhoai_{context}"]
== {rhoai}

{rhoai} is OpenAI API-compatible, and is configured largely the same as the OpenAI provider. 

You must deploy an LLM on the {rhoai} single-model serving platform that uses the Virtual Large Language Model (vLLM) runtime. If the model deployment resides in a different {ocp-short-name} environment than the {ols-long} deployment, include a route to expose the model deployment outside the cluster. For more information, see link:https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2-latest/html/serving_models/serving-large-models_serving-large-models#about-the-single-model-serving-platform_serving-large-models[About the single-model serving platform].

{ols-long} version 1.0 and later supports vLLM Server version 0.8.4 and later. When self-hosting an LLM with {rhoai}, you can use vLLM Server as the inference engine.