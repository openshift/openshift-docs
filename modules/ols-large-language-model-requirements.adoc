// This module is used in the following assemblies:

// * about/ols-about-openshift-lightspeed.adoc

:_mod-docs-content-type: CONCEPT
[id="ols-large-language-model-requirements"]
= Large Language Model (LLM) requirements
:context: ols-large-language-model-requirements

A large language model (LLM) is a type of machine learning model that can interpret and generate human-like language. When an LLM is used with a virtual assistant the LLM can interpret questions accurately and provide helpful answers in a conversational manner.

The {ols-long} service must have access to an LLM provider. The service does not provide an LLM for you, so the LLM must be configured prior to installing the {ols-long} Operator. 

The {ols-long} service can rely on the following Software as a Service (SaaS) LLM providers: 

* OpenAI

* {azure-openai}

* {watsonx}

If you want to self-host a model, you can use {rhoai} or {rhelai} as your model provider.

[id="ibm-watsonx_{context}"]
== {watsonx}

To use {watsonx} with {ols-official}, you need an account with link:https://www.ibm.com/products/watsonx-ai[IBM Cloud watsonx]. For more information, see the link:https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/welcome-main.html?context=wx[Documentation for IBM watsonx as a Service].

[id="open-ai_{context}"]
== Open AI

To use {openai} with {ols-official}, you need access to the {openai} link:https://openai.com/api/[API platform]. For more information, see the link:https://platform.openai.com/docs/overview[OpenAI developer platform] documentation.

[id="azure-open-ai_{context}"]
== {azure-openai}

To use {azure-official} with {ols-official}, you need access to link:https://azure.microsoft.com/en-us/[{azure-openai}]. For more information, see the link:https://learn.microsoft.com/en-us/azure/ai-services/openai/[Azure OpenAI documentation].

[id="rhelai_{context}"]
== {rhelai} 

{rhelai} is OpenAI API-compatible, and is configured in a similar manner as the OpenAI provider. 

You can configure {rhelai} as the (Large Language Model) LLM provider. 

Because the {rhel} is in a different environment than the {ols-long} deployment, the model deployment must allow access using a secure connection. For more information, see link:https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.2/html-single/building_your_rhel_ai_environment/index#creating_secure_endpoint[Optional: Allowing access to a model from a secure endpoint].


[id="rhoai_{context}"]
== {rhoai}

{rhoai} is OpenAI API-compatible, and is configured largely the same as the OpenAI provider. 

You need a Large Language Model (LLM) deployed on the single model-serving platform of {rhoai} using the Virtual Large Language Model (vLLM) runtime. If the model deployment is in a different {ocp-short-name} environment than the {ols-long} deployment, the model deployment must include a route to expose it outside the cluster. For more information, see link:https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2-latest/html/serving_models/serving-large-models_serving-large-models#about-the-single-model-serving-platform_serving-large-models[About the single-model serving platform].
