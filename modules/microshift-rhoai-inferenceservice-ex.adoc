// Module included in the following assemblies:
//
// * microshift_ai/microshift-rhoai.adoc

:_mod-docs-content-type: PROCEDURE
[id="microshift-rhoai-inferenceservice-ex_{context}"]
= Creating an InferenceService custom resource

Create an `InferenceService` custom resource (CR) to instruct KServe how to create a deployment for serving your AI model. KServe uses the `ServingRuntime` based on the `modelFormat` value specified in the `InferenceService` CR.

.Prerequisites

* You configured the `ServingRuntimes` CR.
* You have root user access to your machine.
* The {oc-first} is installed.

.Procedure

. Create the `InferenceService` CR.
+
.Example `InferenceService` object with an `openvino_ir` model format
[source,yaml]
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: ovms-resnet50
spec:
  predictor:
    model:
      protocolVersion: v2
      modelFormat:
        name: openvino_ir
      storageUri: "oci://localhost/ovms-resnet50:test"
      args:
      - --layout=NHWC:NCHW # <1>
----
<1> An additional argument to make {ovms} ({ov}) accept the request input data in a different layout than the model was originally exported with. Extra arguments are passed through to the {ov} container.

. Save the `InferenceService` example to a file, then create it on the cluster by running the following command:
+
[source,terminal,subs="+quotes"]
----
$ oc create -n _<ai_demo>_ -f ./FILE.yaml <1>
----
<1> Replace _<ai_demo>_ with your namespace name.
+
.Example output
[source,terminal]
----
inferenceservice.serving.kserve.io/ovms-resnet50 created
----
+
[NOTE]
====
A deployment and a pod are expected to appear in the specified namespace. Depending on the size of the image specified in the `ServingRuntime` CR and the size of the ModelCar OCI image, it might take several minutes for the pod to be ready.
====

.Next step

* Verify that the model-serving runtime is ready.
