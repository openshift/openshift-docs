// Module included in the following assemblies:
//
// * nodes/nodes-scheduler-default.adoc

[id="nodes-scheduler-default-priorities_{context}"]
= Understanding the scheduler priorities

Priorities are rules that rank nodes according to preferences.

A custom set of priorities can be specified to configure the scheduler.
There are several priorities provided by default in {product-title}.
Other priorities can be customized by providing certain
parameters. Multiple priorities can be combined and different weights
can be given to each to impact the prioritization.

[id="static-priority-functions_{context}"]
== Static Priorities

Static priorities do not take any configuration parameters from
the user, except weight. A weight is required to be specified and cannot be 0 or negative.

These are specified in the scheduler policy config map in the `openshift-config` project.

[id="default-priorities_{context}"]
=== Default Priorities

The default scheduler policy includes the following priorities. Each of
the priority function has a weight of `1` except `NodePreferAvoidPodsPriority`,
which has a weight of `10000`.

The `NodeAffinityPriority` priority prioritizes nodes according to node affinity scheduling preferences

[source,yaml]
----
{"name" : "NodeAffinityPriority", "weight" : 1}
----

The `TaintTolerationPriority` priority prioritizes nodes that have a fewer number of _intolerable_ taints on them for a pod. An intolerable taint is one which has key `PreferNoSchedule`.

[source,yaml]
----
{"name" : "TaintTolerationPriority", "weight" : 1}
----

The `ImageLocalityPriority` priority prioritizes nodes that already have requested pod container's images.

[source,yaml]
----
{"name" : "ImageLocalityPriority", "weight" : 1}
----

The `SelectorSpreadPriority` priority looks for services, replication controllers (RC),
replication sets (RS), and stateful sets that match the pod,
then finds existing pods that match those selectors.
The scheduler favors nodes that have fewer existing matching pods. Then, it schedules the pod on a node with the smallest number of
pods that match those selectors as the pod being scheduled.

[source,yaml]
----
{"name" : "SelectorSpreadPriority", "weight" : 1}
----

The `InterPodAffinityPriority` priority computes a sum by iterating through the elements of `weightedPodAffinityTerm` and adding
_weight_ to the sum if the corresponding PodAffinityTerm is satisfied for that node. The node(s) with the highest sum are the most preferred.

[source,yaml]
----
{"name" : "InterPodAffinityPriority", "weight" : 1}
----

The `LeastRequestedPriority` priority favors nodes with fewer requested resources. It
calculates the percentage of memory and CPU requested by pods scheduled on the
node, and prioritizes nodes that have the highest available/remaining capacity.

[source,yaml]
----
{"name" : "LeastRequestedPriority", "weight" : 1}
----

The `BalancedResourceAllocation` priority favors nodes with balanced resource usage rate.
It calculates the difference between the consumed CPU and memory as a fraction
of capacity, and prioritizes the nodes based on how close the two metrics are to
each other. This should always be used together with `LeastRequestedPriority`.

[source,yaml]
----
{"name" : "BalancedResourceAllocation", "weight" : 1}
----

The `NodePreferAvoidPodsPriority` priority ignores pods that are owned by a controller other than a replication controller.

[source,yaml]
----
{"name" : "NodePreferAvoidPodsPriority", "weight" : 10000}
----

[id="other-priorities_{context}"]
=== Other Static Priorities

{product-title} also supports the following priorities:

The `EqualPriority` priority gives an equal weight of `1` to all nodes, if no priority
configurations are provided. We recommend using this priority only for testing environments.

[source,yaml]
----
{"name" : "EqualPriority", "weight" : 1}
----

//https://github.com/kubernetes/kubernetes/issues/41712
The `MostRequestedPriority` priority prioritizes nodes with most requested resources. It calculates the percentage of memory and CPU
requested by pods scheduled on the node, and prioritizes based on the maximum of the average of the fraction of requested to capacity.

[source,yaml]
----
{"name" : "MostRequestedPriority", "weight" : 1}
----

The `ServiceSpreadingPriority` priority spreads pods by minimizing the number of pods
belonging to the same service onto the same machine.

[source,yaml]
----
{"name" : "ServiceSpreadingPriority", "weight" : 1}
----

[id="configurable-priority-functions_{context}"]
== Configurable Priorities

You can configure these priorities in the scheduler policy config map,
in the `openshift-config` namespace, to add labels to affect how the priorities work.

The type of the priority
function is identified by the argument that they take. Since these are
configurable, multiple priorities of the same type (but different
configuration parameters) can be combined as long as their user-defined names
are different.

For information on using these priorities, see Modifying Scheduler Policy.

The `ServiceAntiAffinity` priority takes a label and ensures a good spread of the pods
belonging to the same service across the group of nodes based on the label
values. It gives the same score to all nodes that have the same value for the
specified label. It gives a higher score to nodes within a group with the least
concentration of pods.

[source,json]
----
{
"kind": "Policy",
"apiVersion": "v1",

"priorities":[
    {
        "name":"<name>", <1>
        "weight" : 1 <2>
        "argument":{
            "serviceAntiAffinity":{
                "label": "<label>" <3>
                }
           }
       }
   ]
}
----
<1> Specify a name for the priority.
<2> Specify a weight. Enter a non-zero positive value.
<3> Specify a label to match.

For example:

[source,json]
----
{
"kind": "Policy",
"apiVersion": "v1",
"priorities": [
    {
        "name":"RackSpread",
        "weight" : 1,
        "argument": {
            "serviceAntiAffinity": {
                "label": "rack"
                }
           }
       }
   ]
}
----

[NOTE]
====
In some situations using the `ServiceAntiAffinity` parameter based on custom labels does not spread pod as expected.
See link:https://access.redhat.com/solutions/3432401[this Red Hat Solution].
====

The `labelPreference` parameter gives priority based on the specified label.
If the label is present on a node, that node is given priority.
If no label is specified, priority is given to nodes that do not have a label.
If multiple priorities with the `labelPreference` parameter are set,
all of the priorities must have the same weight.

[source,json]
----
{
"kind": "Policy",
"apiVersion": "v1",
"priorities":[
    {
        "name":"<name>", <1>
        "weight" : 1 <2>
        "argument":{
            "labelPreference":{
                "label": "<label>", <3>
                "presence": true <4>
                }
            }
        }
    ]
}
----
<1> Specify a name for the priority.
<2> Specify a weight. Enter a non-zero positive value.
<3> Specify a label to match.
<4> Specify whether the label is required, either `true` or `false`.
