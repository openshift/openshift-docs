// Module included in the following assemblies:
//
// * hosted_control_planes/hcp-deploy/hcp-deploy-bm.adoc
// * hosted_control_planes/hcp-deploy/hcp-deploy-ibm-z.adoc
// * hosted_control_planes/hcp-deploy/hcp-deploy-ibm-power.adoc

:_mod-docs-content-type: PROCEDURE
[id="hcp-bm-hc_{context}"]
= Creating a hosted cluster by using the CLI

On bare-metal infrastructure, you can create or import a hosted cluster. After you enable the Assisted Installer as an add-on to {mce-short} and you create a hosted cluster with the Agent platform, the HyperShift Operator installs the Agent Cluster API provider in the hosted control plane namespace. The Agent Cluster API provider connects a management cluster that hosts the control plane and a hosted cluster that consists of only the compute nodes. 

.Prerequisites

- Each hosted cluster must have a cluster-wide unique name. A hosted cluster name cannot be the same as any existing managed cluster. Otherwise, the {mce-short} cannot manage the hosted cluster.

- Do not use the word `clusters` as a hosted cluster name.

- You cannot create a hosted cluster in the namespace of a {mce-short} managed cluster.

- For best security and management practices, create a hosted cluster separate from other hosted clusters. 

- Verify that you have a default storage class configured for your cluster. Otherwise, you might see pending persistent volume claims (PVCs).

.Procedure

. Create a namespace by entering the following command:
+
[source,terminal]
----
$ oc create ns <hosted_cluster_namespace>
----
+
Replace `<hosted_cluster_namespace>` with an identifier for your hosted cluster namespace. The HyperShift Operator creates the namespace. During the hosted cluster creation process on bare-metal infrastructure, a generated Cluster API provider role requires that the namespace already exists.

. Create the configuration file for your hosted cluster by entering the following command:
+
[source,terminal]
----
$ hcp create cluster agent \
  --name=<hosted_cluster_name> \// <1>
  --pull-secret=<path_to_pull_secret> \// <2>
  --agent-namespace=<hosted_control_plane_namespace> \// <3>
  --base-domain=<base_domain> \// <4>
  --api-server-address=api.<hosted_cluster_name>.<base_domain> \// <5>
  --etcd-storage-class=<etcd_storage_class> \// <6>
  --ssh-key=<path_to_ssh_key> \// <7>
  --namespace=<hosted_cluster_namespace> \// <8>
  --control-plane-availability-policy=HighlyAvailable \// <9>
  --release-image=quay.io/openshift-release-dev/ocp-release:<ocp_release_image>-multi \// <10>
  --node-pool-replicas=<node_pool_replica_count> \// <11>
  --render \
  --render-sensitive \
  --ssh-key <home_directory>/<path_to_ssh_key>/<ssh_key> > hosted-cluster-config.yaml <12>
----
+
<1> Specify the name of your hosted cluster, such as `example`.
<2> Specify the path to your pull secret, such as `/user/name/pullsecret`.
<3> Specify your hosted control plane namespace, such as `clusters-example`. Ensure that agents are available in this namespace by using the `oc get agent -n <hosted_control_plane_namespace>` command.
<4> Specify your base domain, such as `krnl.es`.
<5> The `--api-server-address` flag defines the IP address that gets used for the Kubernetes API communication in the hosted cluster. If you do not set the `--api-server-address` flag, you must log in to connect to the management cluster.
<6> Specify the etcd storage class name, such as `lvm-storageclass`.
<7> Specify the path to your SSH public key. The default file path is `~/.ssh/id_rsa.pub`.
<8> Specify your hosted cluster namespace.
<9> Specify the availability policy for the hosted control plane components. Supported options are `SingleReplica` and `HighlyAvailable`. The default value is `HighlyAvailable`.
<10> Specify the supported {product-title} version that you want to use, such as `4.19.0-multi`. If you are using a disconnected environment, replace `<ocp_release_image>` with the digest image. To extract the {product-title} release image digest, see _Extracting the {product-title} release image digest_.
<11> Specify the node pool replica count, such as `3`. You must specify the replica count as `0` or greater to create the same number of replicas. Otherwise, you do not create node pools.
<12> After the `--ssh-key` flag, specify the path to the SSH key, such as `user/.ssh/id_rsa`.

. Apply the changes to the hosted cluster configuration file by entering the following command:
+
[source,terminal]
----
$ oc apply -f hosted_cluster_config.yaml
----

. Check for the creation of the hosted cluster, node pools, and pods by entering the following commands:
+
[source,terminal]
----
$ oc get hostedcluster \
  <hosted_cluster_namespace> -n \
  <hosted_cluster_namespace> -o \
  jsonpath='{.status.conditions[?(@.status=="False")]}' | jq .
----
+
[source,terminal]
----
$ oc get nodepool \
  <hosted_cluster_namespace> -n \
  <hosted_cluster_namespace> -o \
  jsonpath='{.status.conditions[?(@.status=="False")]}' | jq .
----
+
[source,terminal]
----
$ oc get pods -n <hosted_cluster_namespace>
----

. Confirm that the hosted cluster is ready. The status of `Available: True` indicates the readiness of the cluster and the node pool status shows `AllMachinesReady: True`. These statuses indicate the healthiness of all cluster Operators.

.Verification

. Check the cluster Operators by entering the following command:
+
[source,terminal]
----
$ oc get clusteroperators
----
+
Ensure that all Operators show `AVAILABLE: True`, `PROGRESSING: False`, and `DEGRADED: False`.

. Check the nodes by entering the following command:
+
[source,terminal]
----
$ oc get nodes
----
+
Ensure that each node has the `READY` status.

. Test access to the console by entering the following URL in a web browser:
+
[source,text]
----
https://console-openshift-console.apps.<hosted_cluster_namespace>.<base_domain>
----