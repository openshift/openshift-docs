// Module included in the following assemblies:
//
// * logging/efk-logging-deploy.adoc

[id="efk-logging-deploying-subscription-{context}"]
= Installing the Cluster Logging and Elasticsearch Operators

You can use the {product-title} console to install cluster logging, by deploying,
the Cluster Logging and Elasticsearch Operators.  The Cluster Logging Operator
creates and manages the components of the logging stack.  The Elasticsearch Operator
creates and manages the Elasticsearch cluster used by cluster logging.

[NOTE]
====
The {product-title} cluster logging solution requires that you install both the
Cluster Logging Operator and Elasticsearch Operator. There is no use case
in {product-title} for installing the operators individually.
====

.Prerequisites

. Ensure that you have the necessary persistent storage for Elasticsearch. Note that each Elasticsearch node
requires its own storage volume.

. Create a project for cluster logging. You must create the project with the CLI:

.. Create a YAML file with the following:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-logging
  annotations:
    openshift.io/node-selector: "" <1>
  labels:
    openshift.io/cluster-logging: "true"
    openshift.io/cluster-monitoring: "true"
----
<1> Optionally specify an empty node selector in order for the logging pods to spread
evenly across your cluster. The logging pods will be spread evenly throughout the cluster.
If you want the logging pods to run on specific nodes, you can specify a node selector value here.

.. Run the following command to create the project:
+
----
$ oc create -f <file-name>.yaml
----

.Procedure

. Install the Elasticsearch Operator:

.. In the {product-title} console, click *Catalog* -> *OperatorHub*.

.. Choose  *Elasticsearch Operator* from the list of available Operators, and click *Install*.

.. On the *Create Operator Subscription* page, select *All namespaces on the cluster* under *Installation Mode*.
Then, click *Subscribe*.
+
This makes the Operator available to all users and projects that use this {product-title} cluster.

. Install the Cluster Logging Operator:

.. Choose  *Cluster Logging* from the list of available Operators, and click *Install*.

.. On the *Create Operator Subscription* page, under *A specific namespace on the cluster* select *openshift-logging*.
Then, click *Subscribe*.

. Verify the operator installations:

.. Switch to the *Catalog* → *Installed Operators* page.

.. Ensure that *Cluster Logging* and *Elasticsearch Operator* are listed on
the *InstallSucceeded* tab with a *Status* of *InstallSucceeded*. Change the project to *all projects* if necessary.
+
If either operator does not appear as installed, to troubleshoot further:

* On the *Copied* tab of the *Installed Operators* page, if an operator show a *Status* of
*Copied*, this indicates the installation is in process and is expected behavior.
* Switch to the *Catalog* → *Operator Management* page and inspect
the *Operator Subscriptions* and *Install Plans* tabs for any failure or errors
under *Status*.
* Switch to the *Workloads* → *Pods* page and check the logs in any Pods in the
`openshift-logging` and `openshift-operators` projects that are reporting issues.

. Create a cluster logging instance:

.. Switch to the the *Administration* -> *CRD* page.

.. On the *Custom Resource Definitions* page, click *ClusterLogging*.

.. On the *Cluster Loggings* page, click *Create Cluster Logging*.
+
You might need to refresh the page to load the data.

.. In the YAML, replace the code with the following:
+
[source,yaml]
----
apiVersion: "logging.openshift.io/v1alpha1"
kind: "ClusterLogging"
metadata:
  name: "instance"
  namespace: "openshift-logging"
spec:
  managementState: "Managed"
  logStore:
    type: "elasticsearch"
    elasticsearch:
      nodeCount: 3
      storage: {}
      redundancyPolicy: "SingleRedundancy"
  visualization:
    type: "kibana"
    kibana:
      replicas: 1
  curation:
    type: "curator"
    curator:
      schedule: "30 3 * * *"
  collection:
    logs:
      type: "fluentd"
      fluentd: {}
----
+
Optionally, edit the parameters as described in the previous section.


.. Click *Create*. This creates the Cluster Logging Custom Resource, which you
can edit to make changes to your cluster logging cluster.

. Verify the install:

.. Switch to the *Workloads* -> *Pods* page.

.. Select the *openshift-logging* project.
+
You should see pods for cluster logging, Elasticsearch, and Fluentd, as shown in
the following CLI output:
+
----
oc get pods -n openshift-logging

NAME                                             READY   STATUS              RESTARTS   AGE
cluster-logging-operator-788cd-8wkq7             1/1     Running             0          6m12s
elasticsearch-clientdatamaster-0-1-7c78b-qwpz7   0/2     Running             0          35s
elasticsearch-clientdatamaster-0-2-6f57b-75pt7   0/2     Running             0          34s
elasticsearch-clientdatamaster-0-3-584d-vrcvx    0/2     Running             0          34s
fluentd-5jq                                      1/1     Running             0          30s
fluentd-9fq                                      0/1     Running             0          30s
fluentd-fxh                                      1/1     Running             0          30s
fluentd-lk6                                      0/1     Running             0          30s
fluentd-lx7                                      1/1     Running             0          30s
fluentd-z9w                                      0/1     Running             0          30s
kibana-7fb49-fdvck                               0/2     Running             0          35s
----
