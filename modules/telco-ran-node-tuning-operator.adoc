// Module included in the following assemblies:
//
// * scalability_and_performance/telco_ran_du_ref_design_specs/telco-ran-du-rds.adoc

:_mod-docs-content-type: REFERENCE
[id="telco-ran-node-tuning-operator_{context}"]
= CPU partitioning and performance tuning

New in this release::
* There is now optional support for `acpi_idle` CPUIdle driver.
* Updates to `TunedPerformancePatch` to enable the triggering a kernel panic for system recovery and diagnostic purposes when x86_64 architecture nodes become unresponsive. The `TunedPerformancePatch` configures the `kernel.panic_on_unrecovered_nmi` sysctl parameter to enable triggering a kernel panic through BMC Non-Maskable Interrupt (NMI) on x86_64 architectures.



Description::
The RAN DU use model includes cluster performance tuning using `PerformanceProfile` CRs for low-latency performance, and a `TunedPerformancePatch` CR that adds additional RAN-specific tuning.
A reference `PerformanceProfile` is provided for both x86_64 and aarch64 CPU architectures.
The single `TunedPerformancePatch` object provided automatically detects the CPU architecture and performs the required additional tuning.
The RAN DU use case requires the cluster to be tuned for low-latency performance.
The Node Tuning Operator reconciles the `PerformanceProfile` and `TunedPerformancePatch` CRs.

For more information about node tuning with the `PerformanceProfile` CR, see "Tuning nodes for low latency with the performance profile".

Limits and requirements::
You must configure the following settings in the telco RAN DU profile `PerformanceProfile` CR:
+
--
* Set a reserved `cpuset` of 4 or more, equating to 4 hyper-threads (2 cores) on x86_64, or 4 cores on aarch64 for any of the following CPUs:
** Intel 3rd Generation Xeon (IceLake) 2.20 GHz, or newer, CPUs with host firmware tuned for maximum performance
** AMD EPYC Zen 4 CPUs (Genoa, Bergamo)
** ARM CPUs (Neoverse)
+
[NOTE]
====
It is recommended to evaluate features, such as per-pod power management, to determine any potential impact on performance.
====

* x86_64:
** Set the reserved `cpuset` to include both hyper-thread siblings for each included core.
   Unreserved cores are available as allocatable CPU for scheduling workloads.
** Ensure that hyper-thread siblings are not split across reserved and isolated cores.
** Ensure that reserved and isolated CPUs include all the threads for all cores in the CPU.
** Include Core 0 for each NUMA node in the reserved CPU set.
** Set the hugepage size to 1G.
* aarch64:
** Use the first 4 cores for the reserved CPU set (or more).
** Set the hugepage size to 512M.
* Only pin {product-title} pods that are by default configured as part of the management workload partition to reserved cores.
* When recommended by the hardware vendor, set the maximum CPU frequency for reserved and isolated CPUs using the `hardwareTuning` section.
--

Engineering considerations::

* RealTime (RT) kernel
** Under x86_64, to reach the full performance metrics, you must use the RT kernel, which is the default in the `x86_64/PerformanceProfile.yaml` configuration.
*** If required, you can select the non-RT kernel with corresponding impact to performance. 
** Under aarch64, only the 64k-pagesize non-RT kernel is recommended for RAN DU use cases, which is the default in the `aarch64/PerformanceProfile.yaml` configuration.
* The number of hugepages you configure depends on application workload requirements.
Variation in this parameter is expected and allowed.
* Variation is expected in the configuration of reserved and isolated CPU sets based on selected hardware and additional components in use on the system.
The variation must still meet the specified limits.
* Hardware without IRQ affinity support affects isolated CPUs.
To ensure that pods with guaranteed whole CPU QoS have full use of allocated CPUs, all hardware in the server must support IRQ affinity.
* To enable workload partitioning, set `cpuPartitioningMode` to `AllNodes` during deployment, and then use the `PerformanceProfile` CR to allocate enough CPUs to support the operating system, interrupts, and {product-title} pods.
* Tailor `systemReserved` memory for each cluster based on its size and application workload. The minimum recommended value is 11Gi.
* Under x86_64, the `PerformanceProfile` may be customized with the following optional arguments in the `additionalKernelargs` list:
** The `vcio_pci` arguments support devices such as the FEC accelerator. You can omit them if they are not required for your workload.
** To enable the `acpi_idle`` CPUIdle driver, for example, for Intel FlexRAN, add `intel_idle.max_cstate=0`
* Under aarch64, the `PerformanceProfile` must be adjusted depending on the needs of the platform:
** For Grace Hopper systems, the following kernel commandline arguments are required:
*** `acpi_power_meter.force_cap_on=y`
*** `module_blacklist=nouveau`
*** `pci=realloc=off`
*** `pci=pcie_bus_safe`
** For other ARM platforms, you may need to enable `iommu.passthrough=1` or `pci=realloc`
* Extending and augmenting `TunedPerformancePatch.yaml`:
** `TunedPerformancePatch.yaml` introduces a default top-level tuned profile named `ran-du-performance` and an architecture-aware RAN tuning profile named `ran-du-performance-architecture-common`, and additional archichitecture-specific child policies that are automatically selected by the common policy.
** By default, the `ran-du-performance` profile is set to `priority` level `18`, and it includes both the PerformanceProfile-created profile `openshift-node-performance-openshift-node-performance-profile` and `ran-du-performance-architecture-common`
** If you have customized the name of the `PerformanceProfile` object, you must create a new tuned object that includes the name change of the tuned profile created by the `PerformanceProfile` CR, as well as the `ran-du-performance-architecture-common` RAN tuning profile. This must have a `priority` less than 18.
For example, if the PerformanceProfile object is named `change-this-name`:
+
[source,yaml]
----
apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: custom-performance-profile-override
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
    - name: custom-performance-profile-x
      data: |
        [main]
        summary=Override of the default ran-du performance tuning to adjust for our renamed PerformanceProfile
        include=openshift-node-performance-change-this-name,ran-du-performance-architecture-common
  recommend:
    - machineConfigLabels:
        machineconfiguration.openshift.io/role: "master"
      priority: 15
      profile: custom-performance-profile-x
----
+
** To further override, the optional `TunedPowerCustom.yaml` config file exemplifies how to extend the provided `TunedPerformancePatch.yaml` without needing to overlay or edit it directly.
Creating an additional tuned profile which includes the top-level tuned profile named `ran-du-performance` and has a lower `priority` number in the `recommend` section allows adding additional settings easily.
** For additional information on the Node Tuning Operator, see "Using the Node Tuning Operator".
