////
This module included in the following assemblies:
-distr_tracing_otel/distr-tracing-otel-configuring.adoc
////
:_content-type: REFERENCE
[id="distr-tracing-config-otel-collector_{context}"]
= OpenTelemetry Collector configuration options

The OpenTelemetry Collector consists of three components that access telemetry data:

Receivers:: A receiver, which can be push or pull based, is how data gets into the Collector. Generally, a receiver accepts data in a specified format, translates it into the internal format, and passes it to processors and exporters defined in the applicable pipelines. By default, no receivers are configured. One or more receivers must be configured. Receivers may support one or more data sources.

Processors:: Optional. Processors run through the data between it is received and exported. By default, no processors are enabled. Processors must be enabled for every data source. Not all processors support all data sources. Depending on the data source, multiple processors might be enabled. Note that the order of processors matters.

Exporters:: An exporter, which can be push or pull based, is how you send data to one or more back ends or destinations. By default, no exporters are configured. One or more exporters must be configured. Exporters can support one or more data sources. Exporters might be used with their default settings, but many exporters require configuration to specify at least the destination and security settings.

Connectors:: A connector connects two pipelines: It consumes data as an exporter at the end of one pipeline and emits data as a receiver at the start of another pipeline. It can consume and emit data of the same or different data type. It can generate and emit data to summarize the consumed data, or it can merely replicate or route data.

You can define multiple instances of components in a custom resource YAML file. When configured, these components must be enabled through pipelines defined in the `spec.config.service` section of the YAML file. As a best practice, only enable the components that you need.

.Example of the OpenTelemetry Collector custom resource file
[source,yaml]
----
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: cluster-collector
  namespace: tracing-system
spec:
  mode: deployment
  observability:
    metrics:
      enableMetrics: true
  config: |
    receivers:
      otlp:
        protocols:
          grpc:
          http:
    processors:
    exporters:
      otlp:
        endpoint: jaeger-production-collector-headless.tracing-system.svc:4317
        tls:
          ca_file: "/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt"
      prometheus:
        endpoint: 0.0.0.0:8889
        resource_to_telemetry_conversion:
          enabled: true # by default resource attributes are dropped
    service: <1>
      pipelines:
        traces:
          receivers: [otlp]
          processors: []
          exporters: [jaeger]
        metrics:
          receivers: [otlp]
          processors: []
          exporters: [prometheus]
----
<1> If a component is configured but not defined in the `service` section, the component is not enabled.

.Parameters used by the Operator to define the OpenTelemetry Collector
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default
|receivers:
|A receiver is how data gets into the Collector. By default, no receivers are configured. There must be at least one enabled receiver for a configuration to be considered valid. Receivers are enabled by being added to a pipeline.
|`otlp`, `jaeger`, `zipkin`
|None

|processors:
|Processors run through the data between it is received and exported. By default, no processors are enabled.
|
|None

|exporters:
|An exporter sends data to one or more back ends or destinations. By default, no exporters are configured. There must be at least one enabled exporter for a configuration to be considered valid. Exporters are enabled by being added to a pipeline. Exporters might be used with their default settings, but many require configuration to specify at least the destination and security settings.
|`otlp`, `otlphttp`, `logging`, `prometheus`
|None

|service:
  pipelines:
|Components are enabled by adding them to a pipeline under `services.pipeline`.
|
|

|service:
  pipelines:
    traces:
      receivers:
|You enable receivers for tracing by adding them under `service.pipelines.traces`.
|
|None

|service:
  pipelines:
    traces:
      processors:
|You enable processors for tracing by adding them under `service.pipelines.traces`.
|
|None

|service:
  pipelines:
    traces:
      exporters:
|You enable exporters for tracing by adding them under `service.pipelines.traces`.
|
|None

|service:
  pipelines:
    metrics:
      receivers:
|You enable receivers for metrics by adding them under `service.pipelines.metrics`.
|
|None

|service:
  pipelines:
    metrics:
      processors:
|You enable processors for metircs by adding them under `service.pipelines.metrics`.
|
|None

|service:
  pipelines:
    metrics:
      exporters:
|You enable exporters for metrics by adding them under `service.pipelines.metrics`.
|
|None
|===

[id="otel-collector-components_{context}"]
== OpenTelemetry Collector components

[id="receivers_{context}"]
=== Receivers

[id="otlp-receiver_{context}"]
==== OTLP Receiver

The OTLP receiver ingests data using the OpenTelemetry protocol (OTLP).

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces, metrics

.OpenTelemetry Collector custom resource with an enabled OTLP receiver
[source,yaml]
----
  config: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317 <1>
            tls: <2>
              ca_file: ca.pem
              cert_file: cert.pem
              key_file: key.pem
              client_ca_file: client.pem <3>
              reload_interval: 1h <4>
          http:
            endpoint: 0.0.0.0:4318 <5>
            tls: <6>

    service:
      pipelines:
        traces:
          receivers: [otlp]
        metrics:
          receivers: [otlp]
----
<1> The OTLP gRPC endpoint. If omitted, the default `+0.0.0.0:4317+` is used.
<2> The server-side TLS configuration. Defines paths to TLS certificates. If omitted, TLS is disabled.
<3> The path to the TLS certificate at which the server verifies a client certificate. This sets the value of `ClientCAs` and `ClientAuth` to `RequireAndVerifyClientCert` in the `TLSConfig`. For more information, see the link:https://godoc.org/crypto/tls#Config[`Config` of the Golang TLS package].
<4> Specifies the time interval at which the certificate is reloaded. If the value is not set, the certificate is never reloaded.  `reload_interval` accepts a string containing valid units of time such as `ns`, `us` (or `µs`), `ms`, `s`, `m`, `h`.
<5> The OTLP HTTP endpoint. The default value is `+0.0.0.0:4318+`.
<6> The server-side TLS configuration. For more information, see `grpc` protocol configuration section.

[id="jaeger-receiver_{context}"]
==== Jaeger Receiver

The Jaeger receiver ingests data in Jaeger formats.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces

.OpenTelemetry Collector custom resource with an enabled Jaeger receiver
[source,yaml]
----
  config: |
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250 <1>
          thrift_http:
            endpoint: 0.0.0.0:14268 <2>
          thrift_compact:
            endpoint: 0.0.0.0:6831 <3>
          thrift_binary:
            endpoint: 0.0.0.0:6832 <4>
          tls: <5>

    service:
      pipelines:
        traces:
          receivers: [jaeger]
----
<1> The Jaeger gRPC endpoint. If omitted, the default `+0.0.0.0:14250+` is used.
<2> The Jaeger Thrift HTTP endpoint. If omitted, the default `+0.0.0.0:14268+` is used.
<3> The Jaeger Thrift Compact endpoint. If omitted, the default `+0.0.0.0:6831+` is used.
<4> The Jaeger Thrift Binary endpoint. If omitted, the default `+0.0.0.0:6832+` is used.
<5> The TLS server side configuration. See the OTLP receiver configuration section for more details.

[id="zipkin-receiver_{context}"]
==== Zipkin Receiver

The Zipkin receiver ingests data in the Zipkin v1 and v2 formats.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces

.OpenTelemetry Collector custom resource with enabled Zipkin receiver
[source,yaml]
----
  config: |
    receivers:
      zipkin:
        endpoint: 0.0.0.0:9411 <1>
        tls: <2>

    service:
      pipelines:
        traces:
          receivers: [zipkin]
----
<1> The Zipkin HTTP endpoint. If omitted, the default `+0.0.0.0:9411+` is used.
<2> The TLS server side configuration. See the OTLP receiver configuration section for more details.

[id="processors_{context}"]
=== Processors


[id="batch-processor_{context}"]
==== Batch processor

The batch processor batches the data to reduce the number of outgoing connections needed to transfer the telemetry information.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces, metrics

.Example of the OpenTelemetry Collector custom resource when using the batch processor
[source,yaml]
----
  config: |
    processor:
      batch:
        timeout: 5s
        send_batch_max_size: 10000
    service:
      pipelines:
        traces:
          processors: [batch]
        metrics:
          processors: [batch]
----

.Parameters used by the batch processor
[cols="3",options="header"]
|===
|Parameter |Description |Default

| `timeout`
| Sends the batch after a specific time duration, irrespective of its size.
| 200ms

| `send_batch_size`
| Sends the batch of telemetry data after the specified number of spans or metrics.
| 8192

| `send_batch_max_size`
| The maximum allowable size of the batch. Must be equal or greater than `send_batch_size`.
| 0

| `metadata_keys`
| When activated, a batcher instance is created for each unique set of values found in the `client.Metadata`.
| []

| `metadata_cardinality_limit`
| When the `metadata_keys` are populated, this configuration restricts the number of distinct metadata key-value combinations processed throughout the duration of the process.
| 1000
|===

[id="resource-detection-processor_{context}"]
==== Resource Detection processor

The Resource Detection processor is designed to identify host resource details in alignment with OpenTelemetry's resource semantic standards. Using this detected information, it can add or replace the resource values in telemetry data.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces, metrics

.{product-title} permissions required for the Resource Detection processor
[source,yaml]
----
kind: ClusterRole
metadata:
  name: otel-collector
rules:
- apiGroups: ["config.openshift.io"]
  resources: ["infrastructures", "infrastructures/status"]
  verbs: ["get", "watch", "list"]
----

.OpenTelemetry Collector using the Resource Detection processor
[source,yaml]
----
  config: |
    processor:
      resourcedetection:
        detectors: [openshift]
        override: true
    service:
      pipelines:
        traces:
          processors: [resourcedetection]
        metrics:
          processors: [resourcedetection]
----

[id="exporters_{context}"]
=== Exporters

[id="otlp-exporter_{context}"]
==== OTLP exporter

The OTLP gRPC exporter exports data using the OpenTelemetry protocol (OTLP).

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces, metrics

.OpenTelemetry Collector custom resource with an enabled OTLP exporter
[source,yaml]
----
  config: |
    exporters:
      otlp:
        endpoint: tempo-ingester:4317 <1>
        tls: <2>
          ca_file: ca.pem
          cert_file: cert.pem
          key_file: key.pem
          insecure: false <3>
          insecure_skip_verify: false <4>
          reload_interval: 1h <5>
          server_name_override: <name> <6>
        headers: <7>
          X-Scope-OrgID: "dev"
    service:
      pipelines:
        traces:
          exporters: [otlp]
        metrics:
          exporters: [otlp]
----
<1> The OTLP gRPC endpoint. If the `+https://+` scheme is used, then client transport security is enabled and overrides the `insecure` setting in the `tls`.
<2> The client side TLS configuration. Defines paths to TLS certificates.
<3> Disables client transport security when set to `true`. The default value is `false` by default.
<4> Skips verifying the certificate when set to `true`. The default value is `false`.
<5> Specifies the time interval at which the certificate is reloaded. If the value is not set, the certificate is never reloaded. `reload_interval` accepts a string containing valid units of time such as `ns`, `us` (or `µs`), `ms`, `s`, `m`, `h`.
<6> Overrides the virtual host name of authority such as the authority header field in requests. You can use this for testing.
<7> Headers are sent for every request performed during an established connection.

[id="otlp-http-exporter_{context}"]
==== OTLP HTTP exporter

The OTLP HTTP exporter exports data using the OpenTelemetry protocol (OTLP).

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces, metrics

.OpenTelemetry Collector custom resource with an enabled OTLP exporter
[source,yaml]
----
  config: |
    exporters:
      otlphttp:
        endpoint: http://tempo-ingester:4318 <1>
        tls: <2>
        headers: <3>
          X-Scope-OrgID: "dev"

    service:
      pipelines:
        traces:
          exporters: [otlphttp]
        metrics:
          expoters: [otlphttp]
----
<1> The OTLP HTTP endpoint. If the `+https://+` scheme is used, then client transport security is enabled and overrides the `insecure` setting in the `tls`.
<2> The client side TLS configuration. Defines paths to TLS certificates.
<3> Headers are sent in every HTTP request.

[id="jaeger-exporter_{context}"]
==== Jaeger exporter

The Jaeger exporter exports data using the Jaeger proto format through gRPC.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces

.OpenTelemetry Collector custom resource with enabled Jaeger exporter
[source,yaml]
----
  config: |
    exporters:
      jaeger:
        endpoint: jaeger-all-in-one:14250 <1>
        tls: <2>
    service:
      pipelines:
        traces:
          exporters: [jaeger]
----
<1> The Jaeger gRPC endpoint.
<2> The client side TLS configuration. Defines paths to TLS certificates.


[id="logging-exporter_{context}"]
==== Logging exporter

The Logging exporter prints data to the standard output.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces, metrics

.OpenTelemetry Collector custom resource with an enabled Logging exporter
[source,yaml]
----
  config: |
    exporters:
      logging:
        verbosity: detailed <1>
    service:
      pipelines:
        traces:
          exporters: [logging]
        metrics:
          exporters: [logging]
----
<1> Verbosity of the logging export: `detailed` or `normal` or `basic`. When set to `detailed`, pipeline data is verbosely logged. Defaults to `normal`.

[id="prometheus-exporter_{context}"]
==== Prometheus exporter

The Prometheus exporter exports data using the Prometheus or OpenMetrics formats.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: metrics

.OpenTelemetry Collector custom resource with an enabled Prometheus exporter
[source,yaml]
----
  ports:
  - name: promexporter <1>
    port: 8889
    protocol: TCP
  config: |
    exporters:
      prometheus:
        endpoint: 0.0.0.0:8889 <2>
        tls: <3>
          ca_file: ca.pem
          cert_file: cert.pem
          key_file: key.pem
        namespace: prefix <4>
        const_labels: <5>
          label1: value1
        enable_open_metrics: true <6>
        resource_to_telemetry_conversion: <7>
          enabled: true
        metric_expiration: 180m <8>
    service:
      pipelines:
        metrics:
          exporters: [prometheus]
----
<1> Exposes the Prometheus port from the collector pod and service. You can enable scraping of metrics by Prometheus by using the port name in `ServiceMonitor` or `PodMonitor` custom resource.
<2> The network endpoint where the metrics are exposed.
<3> The server-side TLS configuration. Defines paths to TLS certificates.
<4> If set, exports metrics under the provided value. No default.
<5> Key-value pair labels that are applied for every exported metric. No default.
<6> If `true`, metrics are exported using the OpenMetrics format. Exemplars are only exported in the OpenMetrics format and only for histogram and monotonic sum metrics such as `counter`. Disabled by default.
<7> If `enabled` is `true`, all the resource attributes are converted to metric labels by default. Disabled by default.
<8> Defines how long metrics are exposed without updates. The default is `5m`.

<<<<<<< HEAD
=======
[id="extensions_{context}"]
=== Extensions

[id="basicauth-extension_{context}"]
==== BasicAuth extension

You can use the BasicAuth extension as an authenticator for receivers and exporters that are based on the HTTP and the gRPC protocol.
Client authentication and server authentication for the BasicAuth extension are configured in separate sections in the OpenTelemetry Collector custom resource.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces, metrics, logs

.OpenTelemetry Collector custom resource with client and server authentication configured for the BasicAuth extension
[source,yaml]
----
  config: |
    extensions:
      basicauth/server:
        htpasswd:
          file: .htpasswd <1>
        inline: |
          ${env:BASIC_AUTH_USERNAME}:${env:BASIC_AUTH_PASSWORD} <2>

      basicauth/client:
        client_auth:
          username: username <3>
          password: password <4>

    receivers:
      otlp:
        protocols:
          http:
            auth:
              authenticator: basicauth/server <5>
    exporters:
      otlp:
        auth:
          authenticator: basicauth/client <6>

    service:
      extensions: [basicauth/server, basicauth/client]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> The BasicAuth extension can be configured as a server authenticator that reads credentials from a `.htpasswd` file.
<2> The BasicAuth extension can be configured as a client authenticator to read the credentials from an inline string that consists of environment variables.
<3> The client username is configured as a client authenticator for the BasicAuth extension.
<4> The client password is configured as a client authenticator for the BasicAuth extension.
<5> The authenticator configuration can be assigned to an OTLP receiver.
<6> The authenticator configuration can be assigned to an OTLP exporter.

[id="connectors_{context}"]
=== Connectors

[id="spanmetrics-connector_{context}"]
==== Spanmetrics connector

Aggregates Request, Error, and Duration (R.E.D) OpenTelemetry metrics from span data.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces

.OpenTelemetry Collector custom resource with an enabled spanmetrics connector
[source,yaml]
----
  config: |
    connectors:
      spanmetrics:
        metrics_flush_interval: 15s <1>
    service:
      pipelines:
        traces:
          exporters: [spanmetrics]
        metrics:
          receivers: [spanmetrics]
----
<1>: Defines the flush interval of the generated metrics. Defaults to `15s`.

[id="bearertokenauth-extension_{context}"]
==== BearerTokenAuth extension

You can use the BearerTokenAuth extension as an authenticator for receivers and exporters that are based on the HTTP and the gRPC protocol.
You can use the OpenTelemetry Collector custom resource to configure client authentication and server authentication for the BearerTokenAuth extension on the receiver and exporter side.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces, metrics, logs

.OpenTelemetry Collector custom resource with client and server authentication configured for the BearerTokenAuth extension
[source,yaml]
----
  config: |
    extensions:
      bearertokenauth:
        scheme: "Bearer" <1>
        token: "<token>" <2>
        filename: "<token_file>" <3>

    receivers:
      otlp:
        protocols:
          http:
            auth:
              authenticator: bearertokenauth <4>
    exporters:
      otlp:
        auth:
          authenticator: bearertokenauth <5>

    service:
      extensions: [bearertokenauth]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> You can configure the BearerTokenAuth extension to send a custom `scheme`. The default is `Bearer`.
<2> You can add the BearerTokenAuth extension token as metadata to identify a message.
<3> Path to a file that contains an authorization token that is transmitted with every message.
<4> You can assign the authenticator configuration to an OTLP receiver.
<5> You can assign the authenticator configuration to an OTLP exporter.


[id="oauth2client-extension_{context}"]
==== OAuth2Client extension

You can use the OAuth2Client extension as an authenticator for exporters that are based on the HTTP and the gRPC protocol.
Client authentication for the OAuth2Client extension is configured in a separate section in the OpenTelemetry Collector custom resource.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]
* Supported signals: traces, metrics, logs

.OpenTelemetry Collector custom resource with client authentication configured for the OAuth2Client extension
[source,yaml]
----
  config: |
    extensions:
      oauth2client:
        client_id: <client_id> <1>
        client_secret: <client_secret> <2>
        endpoint_params: <3>
          audience: <audience>
        token_url: https://example.com/oauth2/default/v1/token <4>
        scopes: ["api.metrics"] <5>
        # tls settings for the token client
        tls: <6>
          insecure: true <7>
          ca_file: /var/lib/mycert.pem <8>
          cert_file: <cert_file> <9>
          key_file: <key_file> <10>
        timeout: 2s <11>

    receivers:
      otlp:
        protocols:
          http:

    exporters:
      otlp:
        auth:
          authenticator: oauth2client <12>

    service:
      extensions: [oauth2client]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> Client identifier, which is provided by the identity provider.
<2> Confidential key used to authenticate the client to the identity provider.
<3> Further metadata which is transferred during authentication. Consisting of key and value. For example audience specifies the intended audience for the access token. It indicates the recipient of the token.
<4> URL of the OAuth2 token endpoint, where the Collector will request access tokens.
<5> The scopes define the specific permissions or access levels requested by the client.
<6> The Transport Layer Security (TLS) settings for the token client, which is used to establish a secure connection when requesting tokens.
<7> When set to `true`, configures the Collector to use an insecure or non-verified TLS connection to call the configured token endpoint.
<8> The path to a Certificate Authority (CA) file that is used to verify the server's certificate during the TLS handshake.
<9> The path to the client certificate file that the client must use to authenticate itself to the OAuth2 server if required.
<10> The path to the client's private key file that is used with the client certificate if needed for authentication.
<11> Sets a timeout for the token client's request.
<12> You can assign the authenticator configuration to an OTLP exporter.


[id="jaegerremotesampling-extension_{context}"]
==== Jaeger Remote Sampling extension

This extension allows serving sampling strategies after Jaeger's remote sampling API. You can configure this extension to proxy requests to a backing remote sampling server such as a Jaeger collector down the pipeline or to a static JSON file from the local file system.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]

.OpenTelemetry Collector custom resource with a configured Jaeger Remote Sampling extension
[source,yaml]
----
  config: |
    extensions:
      jaegerremotesampling:
        source:
          reload_interval: 30s <1>
          remote:
            endpoint: jaeger-collector:14250 <2>
          file: /etc/otelcol/sampling_strategies.json <3>

    receivers:
      otlp:
        protocols:
          http:

    exporters:
      otlp:

    service:
      extensions: [jaegerremotesampling]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> The time interval at which the sampling configuration is updated.
<2> The endpoint for reaching the Jaeger remote sampling strategy provider.
<3> The path to a local file that contains a sampling strategy configuration in the JSON format.

.Example of a Jaeger Remote Sampling strategy file
[source,json]
----
{
  "service_strategies": [
    {
      "service": "foo",
      "type": "probabilistic",
      "param": 0.8,
      "operation_strategies": [
        {
          "operation": "op1",
          "type": "probabilistic",
          "param": 0.2
        },
        {
          "operation": "op2",
          "type": "probabilistic",
          "param": 0.4
        }
      ]
    },
    {
      "service": "bar",
      "type": "ratelimiting",
      "param": 5
    }
  ],
  "default_strategy": {
    "type": "probabilistic",
    "param": 0.5,
    "operation_strategies": [
      {
        "operation": "/health",
        "type": "probabilistic",
        "param": 0.0
      },
      {
        "operation": "/metrics",
        "type": "probabilistic",
        "param": 0.0
      }
    ]
  }
}
----



[id="pprof-extension_{context}"]
==== Performance Profiler extension

The Performance Profiler extension enables the Go `net/http/pprof` endpoint. This is typically used by developers to collect performance profiles and investigate issues with the service.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]

.OpenTelemetry Collector custom resource with the configured Performance Profiler extension
[source,yaml]
----
  config: |
    extensions:
      pprof:
        endpoint: localhost:1777 <1>
        block_profile_fraction: 0 <2>
        mutex_profile_fraction: 0 <3>
        save_to_file: test.pprof <4>

    receivers:
      otlp:
        protocols:
          http:

    exporters:
      otlp:

    service:
      extensions: [pprof]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> The endpoint at which this extension listens. Use `localhost:` to make it available only locally or `":"` to make it available on all network interfaces. The default value is `localhost:1777`.
<2> Sets a fraction of blocking events to be profiled. To disable profiling, set this to `0` or a negative integer. See the link:https://golang.org/pkg/runtime/#SetBlockProfileRate[documentation] for the `runtime` package. The default value is `0`.
<3> Set a fraction of mutex contention events to be profiled. To disable profiling, set this to `0` or a negative integer. See the link:https://golang.org/pkg/runtime/#SetMutexProfileFraction[documentation] for the `runtime` package. The default value is `0`.
<4> The name of the file in which the CPU profile is to be saved. Profiling starts when the Collector starts. Profiling is saved to the file when the Collector is terminated.

[id="healthcheck-extension_{context}"]
==== Health Check extension

The Health Check extension provides an HTTP URL for checking the status of the OpenTelemetry Collector. You can use this extension as a liveness and readiness probe on OpenShift.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]

.OpenTelemetry Collector custom resource with the configured Health Check extension
[source,yaml]
----
  config: |
    extensions:
      health_check:
        endpoint: "0.0.0.0:13133" <1>
        tls: <2>
          ca_file: "/path/to/ca.crt"
          cert_file: "/path/to/cert.crt"
          key_file: "/path/to/key.key"
        path: "/health/status" <3>
        check_collector_pipeline: <4>
          enabled: true <5>
          interval: "5m" <6>
          exporter_failure_threshold: 5 <7>

    receivers:
      otlp:
        protocols:
          http:

    exporters:
      otlp:

    service:
      extensions: [health_check]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> The target IP address for publishing the health check status. The default is `0.0.0.0:13133`.
<2> The TLS server-side configuration. Defines paths to TLS certificates. If omitted, the TLS is disabled.
<3> The path for the health check server. The default is `/`.
<4> Settings for the Collector pipeline health check.
<5> Enables the Collector pipeline health check. The default is `false`.
<6> The time interval for checking the number of failures. The default is `5m`.
<7> The threshold of a number of failures until which a container is still marked as healthy. The default is `5`.


[id="memory-ballast-extension_{context}"]
==== Memory Ballast extension

The Memory Ballast extension enables applications to configure memory ballast for the process.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]

.OpenTelemetry Collector custom resource with the configured Memory Ballast extension
[source,yaml]
----
  config: |
    extensions:
      memory_ballast:
        size_mib: 64 <1>
        size_in_percentage: 20 <2>

    receivers:
      otlp:
        protocols:
          http:

    exporters:
      otlp:

    service:
      extensions: [memory_ballast]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----
<1> Sets the memory ballast size in MiB. Takes priority over the `size_in_percentage` if both are specified.
<2> Sets the memory ballast as a percentage, `1`-`100`, of the total memory. Supports containerized and physical host environments.


[id="zpages-extension_{context}"]
==== zPages extension

The zPages extension provides an HTTP endpoint for extensions that serve zPages. At the endpoint, this extension serves live data for debugging instrumented components. All core exporters and receivers provide some zPages instrumentation.

zPages are useful for in-process diagnostics without having to depend on a back end to examine traces or metrics.

* Support level: link:https://access.redhat.com/support/offerings/techpreview[Technology Preview]

.OpenTelemetry Collector custom resource with the configured zPages extension
[source,yaml]
----
  config: |
    extensions:
      zpages:
        endpoint: "localhost:55679" <1>

    receivers:
      otlp:
        protocols:
          http:
    exporters:
      otlp:

    service:
      extensions: [zpages]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp]
----

<1> Specifies the HTTP endpoint that serves zPages. Use `localhost:` to make it available only locally, or `":"` to make it available on all network interfaces. The default is `localhost:55679`.
