// Module included in the following assemblies:
//
// * backup_and_restore/replacing-unhealthy-etcd-member.adoc

[id="restore-replace-crashlooping-etcd-member_{context}"]
= Replacing an unhealthy etcd member whose etcd Pod is crashlooping

This procedure details the steps to replace an etcd member that is unhealthy because the etcd Pod is crashlooping.

.Prerequisites

* You have identified the unhealthy etcd member.
* You have verified that the etcd Pod is crashlooping.
* You have access to the cluster as a user with the `cluster-admin` role.
* You have taken an etcd backup.
+
[IMPORTANT]
====
It is important to take an etcd backup before performing this procedure so that your cluster can be restored if you encounter any issues.
====

.Procedure

. Stop the crashlooping etcd Pod.

.. Debug the node that is crashlooping.
+
In a terminal that has access to the cluster as a `cluster-admin` user, run the following command:
+
----
$ oc debug node/ip-10-0-131-183.ec2.internal <1>
----
<1> Replace this with the name of the unhealthy node.

.. Change your root directory to the host:
+
----
sh-4.2# chroot /host
----

.. Move the existing etcd Pod file out of the kubelet manifest directory:
+
----
sh-4.4# mv /etc/kubernetes/manifests/etcd-pod.yaml /var/lib/etcd-backup/
----

.. Move the etcd data directory to a different location:
+
----
sh-4.4# mv /var/lib/etcd/ /tmp
----
+
You can now exit the node shell.

. Remove the unhealthy member.

.. Choose a Pod that is _not_ on the affected node.
+
In a terminal that has access to the cluster as a `cluster-admin` user, run the following command:
+
----
$ oc get pods -n openshift-etcd | grep etcd
etcd-ip-10-0-131-183.ec2.internal                2/3     Error       7          6h9m
etcd-ip-10-0-164-97.ec2.internal                 3/3     Running     0          6h6m
etcd-ip-10-0-154-204.ec2.internal                3/3     Running     0          6h6m
----

.. Connect to the running etcd container, passing in the name of a Pod that is not on the affected node.
+
In a terminal that has access to the cluster as a `cluster-admin` user, run the following command:
+
----
$ oc rsh -n openshift-etcd etcd-ip-10-0-154-204.ec2.internal
----

.. View the member list:
+
----
sh-4.2# etcdctl member list -w table

+------------------+---------+------------------------------+---------------------------+---------------------------+
|        ID        | STATUS  |             NAME             |        PEER ADDRS         |       CLIENT ADDRS        |
+------------------+---------+------------------------------+---------------------------+---------------------------+
| 62bcf33650a7170a | started | ip-10-0-131-183.ec2.internal | https://10.0.131.183:2380 | https://10.0.131.183:2379 |
| b78e2856655bc2eb | started |  ip-10-0-164-97.ec2.internal |  https://10.0.164.97:2380 |  https://10.0.164.97:2379 |
| d022e10b498760d5 | started | ip-10-0-154-204.ec2.internal | https://10.0.154.204:2380 | https://10.0.154.204:2379 |
+------------------+---------+------------------------------+---------------------------+---------------------------+
----

.. Remove the unhealthy etcd member by providing the ID to the `etcdctl member remove` command:
+
----
sh-4.2# etcdctl member remove 62bcf33650a7170a
Member 62bcf33650a7170a removed from cluster ead669ce1fbfb346
----

.. View the member list again and verify that the member was removed:
+
----
sh-4.2# etcdctl member list -w table

+------------------+---------+------------------------------+---------------------------+---------------------------+
|        ID        | STATUS  |             NAME             |        PEER ADDRS         |       CLIENT ADDRS        |
+------------------+---------+------------------------------+---------------------------+---------------------------+
| b78e2856655bc2eb | started |  ip-10-0-164-97.ec2.internal |  https://10.0.164.97:2380 |  https://10.0.164.97:2379 |
| d022e10b498760d5 | started | ip-10-0-154-204.ec2.internal | https://10.0.154.204:2380 | https://10.0.154.204:2379 |
+------------------+---------+------------------------------+---------------------------+---------------------------+
----
+
You can now exit the node shell.

. Force etcd redeployment.
+
In a terminal that has access to the cluster as a `cluster-admin` user, run the following command:
+
----
$ oc patch etcd cluster -p='{"spec": {"forceRedeploymentReason": "single-master-recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge <1>
----
<1> The `forceRedeploymentReason` value must be unique, which is why a timestamp is appended.
+
When the etcd cluster Operator performs a redeployment, it ensures that all master nodes have a functioning etcd Pod.

. Verify that the new member is available and healthy.

.. Connect to the running etcd container again.
+
In a terminal that has access to the cluster as a cluster-admin user, run the following command:
+
----
$ oc rsh -n openshift-etcd etcd-ip-10-0-154-204.ec2.internal
----

.. Verify that all members are healthy:
+
----
sh-4.2# etcdctl endpoint health --cluster
https://10.0.131.183:2379 is healthy: successfully committed proposal: took = 16.671434ms
https://10.0.154.204:2379 is healthy: successfully committed proposal: took = 16.698331ms
https://10.0.164.97:2379 is healthy: successfully committed proposal: took = 16.621645ms
----
