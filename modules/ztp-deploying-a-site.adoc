// Module included in the following assemblies:
//
// *scalability_and_performance/ztp-support-for-deployment-of-multi-node-clusters.adoc

:_content-type: PROCEDURE
[id="ztp-deploying-a-site_{context}"]
= Deploying a site

Use the following procedure to prepare the hub cluster for site deployment and initiate zero touch provisioning (ZTP) by pushing custom resources (CRs) to your Git repository.

.Procedure

. Create the required secrets for the site. These resources must be in a namespace with a name matching the cluster name. In `out/argocd/example/siteconfig/example-sno.yaml`, the cluster name and namespace is `example-sno`.
+
Create the namespace for the cluster using the following commands:
+
[source,terminal]
----
$ export CLUSTERNS=example-sno
----
+
[source,terminal]
----
$ oc create namespace $CLUSTERNS
----

. Create a pull secret for the cluster. The pull secret must contain all the credentials necessary for installing {product-title} and all required Operators. In all of the example `SiteConfig` CRs, the pull secret is named `assisted-deployment-pull-secret`, as shown below:
+
[source,terminal]
----
$ oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: assisted-deployment-pull-secret
  namespace: $CLUSTERNS
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: $(base64 <pull-secret.json)
EOF
----

. Create a BMC authentication secret for each host you are deploying:
+
[source,yaml]
----
$ oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: $(read -p 'Hostname: ' tmp; printf $tmp)-bmc-secret
  namespace: $CLUSTERNS
type: Opaque
data:
  username: $(read -p 'Username: ' tmp; printf $tmp | base64)
  password: $(read -s -p 'Password: ' tmp; printf $tmp | base64)
EOF
----
+
[NOTE]
====
The secrets are referenced from the `SiteConfig` custom resource (CR) by name. The namespace
must match the `SiteConfig` namespace.
====

. Create a `SiteConfig` CR for your cluster in your local clone of the Git repository:
.. Choose the appropriate example for your CR from the  `out/argocd/example/siteconfig/` folder.
The folder includes example files for single node, three-node, and standard clusters:
+
* `example-sno.yaml`
* `example-3node.yaml`
* `example-standard.yaml`

.. Change the cluster and host details in the example file to match the type of cluster you want. The following file is a composite of the three files that explains the configuration of each cluster type:
+
[source,yaml]
----
# example-node1-bmh-secret & assisted-deployment-pull-secret need to be created under same namespace example-sno
---
apiVersion: ran.openshift.io/v1
kind: SiteConfig
metadata:
  name: "example-sno"
  namespace: "example-sno"
spec:
  baseDomain: "example.com"
  pullSecretRef:
    name: "assisted-deployment-pull-secret"
  clusterImageSetNameRef: "openshift-4.10" <1>
  sshPublicKey: "ssh-rsa AAAA..."
  clusters:
  - clusterName: "example-sno"
    networkType: "OVNKubernetes"
    clusterLabels: <2>
      # These example cluster labels correspond to the bindingRules in the PolicyGenTemplate examples in ../policygentemplates:
      # ../policygentemplates/common-ranGen.yaml will apply to all clusters with 'common: true'
      common: true
      # ../policygentemplates/group-du-sno-ranGen.yaml will apply to all clusters with 'group-du-sno: ""'
      group-du-sno: ""
      # ../policygentemplates/example-sno-site.yaml will apply to all clusters with 'sites: "example-sno"'
      # Normally this should match or contain the cluster name so it only applies to a single cluster
      sites : "example-sno"
    clusterNetwork:
      - cidr: 1001:1::/48
        hostPrefix: 64
    machineNetwork: <3>
      - cidr: 1111:2222:3333:4444::/64
      # For 3-node and standard clusters with static IPs, the API and Ingress IPs must be configured here
    apiVIP: 1111:2222:3333:4444::1:1 <4>
    ingressVIP: 1111:2222:3333:4444::1:2 <5>

    serviceNetwork:
      - 1001:2::/112
    additionalNTPSources:
      - 1111:2222:3333:4444::2
    nodes:
      - hostName: "example-node1.example.com" <6>
        role: "master"
        bmcAddress: "idrac-virtualmedia+https://[1111:2222:3333:4444::bbbb:1]/redfish/v1/Systems/System.Embedded.1" <7>
        bmcCredentialsName:
          name: "example-node1-bmh-secret" <8>
        bootMACAddress: "AA:BB:CC:DD:EE:11"
        bootMode: "UEFI"
        rootDeviceHints:
          hctl: '0:1:0'
        cpuset: "0-1,52-53"
        nodeNetwork: <9>
          interfaces:
            - name: eno1
              macAddress: "AA:BB:CC:DD:EE:11"
          config:
            interfaces:
              - name: eno1
                type: ethernet
                state: up
                macAddress: "AA:BB:CC:DD:EE:11"
                ipv4:
                  enabled: false
                ipv6:
                  enabled: true
                  address:
                  - ip: 1111:2222:3333:4444::1:1
                    prefix-length: 64
            dns-resolver:
              config:
                search:
                - example.com
                server:
                - 1111:2222:3333:4444::2
            routes:
              config:
              - destination: ::/0
                next-hop-interface: eno1
                next-hop-address: 1111:2222:3333:4444::1
                table-id: 254
----
<1> Applies to all cluster types. The value must match an image set available on the hub cluster. To see the list of supported versions on your hub, run `oc get clusterimagesets`.
<2> Applies to all cluster types. These values must correspond to the `PolicyGenTemplate` labels that you define in a later step.
<3> Applies to single node clusters. The value defines the cluster network sections for a single node deployment.
<4> Applies to three-node and standard clusters. The value defines the cluster network sections.
<5> Applies to three-node and standard clusters. The value defines the cluster network sections.
<6> Applies to all cluster types. For single node deployments, define one host. For three-node deployments, define three hosts. For standard deployments, define three hosts with `role: master` and two or more hosts defined with `role: worker`.
<7> Applies to all cluster types. Specifies the BMC address.
<8> Applies to all cluster types. Specifies the BMC credentials.
<9> Applies to all cluster types. Specifies the network settings for the node.

.. You can inspect the default set of extra-manifest `MachineConfig` CRs in `out/argocd/extra-manifest`. It is automatically applied to the cluster when it is installed.
+
Optional: To provision additional install-time manifests on the provisioned cluster, create a directory in your Git repository, for example, `sno-extra-manifest/`, and add your custom manifest CRs to this directory. If your `SiteConfig.yaml` refers to this directory in the `extraManifestPath` field, any CRs in this referenced directory are appended to the default set of extra manifests.

. Add the `SiteConfig` CR to the `kustomization.yaml` file in the `generators` section, similar to the example shown in `out/argocd/example/siteconfig/kustomization.yaml`.

. Commit your `SiteConfig` CR and associated `kustomization.yaml` in your Git repository.

. Push your changes to the Git repository. The ArgoCD pipeline detects the changes and begins the site deployment. You can push the changes to the `SiteConfig` CR and the `PolicyGenTemplate` CR simultaneously.
+
The `SiteConfig` CR creates the following CRs on the hub cluster:
+
* `Namespace` - Unique per site
* `AgentClusterInstall`
* `BareMetalHost` - One per node
* `ClusterDeployment`
* `InfraEnv`
* `NMStateConfig` - One per node
* `ExtraManifestsConfigMap` - Extra manifests. The additional manifests include workload partitioning, chronyd, mountpoint hiding, sctp enablement, and more.
* `ManagedCluster`
* `KlusterletAddonConfig`
