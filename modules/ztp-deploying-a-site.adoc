// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-deploying-far-edge-sites.adoc

:_mod-docs-content-type: PROCEDURE
[id="ztp-deploying-a-site_{context}"]
= Deploying a managed cluster with SiteConfig and {ztp}

Use the following procedure to create a `SiteConfig` custom resource (CR) and related files and initiate the {ztp-first} cluster deployment.

.Prerequisites

* You have installed the OpenShift CLI (`oc`).

* You have logged in to the hub cluster as a user with `cluster-admin` privileges.

* You configured the hub cluster for generating the required installation and policy CRs.

* You created a Git repository where you manage your custom site configuration data. The repository must be accessible from the hub cluster and you must configure it as a source repository for the ArgoCD application. See "Preparing the {ztp} site configuration repository" for more information.
+
[NOTE]
====
When you create the source repository, ensure that you patch the ArgoCD application with the `argocd/deployment/argocd-openshift-gitops-patch.json` patch-file that you extract from the `ztp-site-generate` container. See "Configuring the hub cluster with ArgoCD".
====

* To be ready for provisioning managed clusters, you require the following for each bare-metal host:
+
Network connectivity:: Your network requires DNS. Managed cluster hosts should be reachable from the hub cluster. Ensure that Layer 3 connectivity exists between the hub cluster and the managed cluster host.
+
Baseboard Management Controller (BMC) details:: {ztp} uses BMC username and password details to connect to the BMC during cluster installation. The {ztp} plugin manages the `ManagedCluster` CRs on the hub cluster based on the `SiteConfig` CR in your site Git repo. You create individual `BMCSecret` CRs for each host manually.

.Procedure

. Create the required managed cluster secrets on the hub cluster. These resources must be in a namespace with a name matching the cluster name. For example, in `out/argocd/example/siteconfig/example-sno.yaml`, the cluster name and namespace is `example-sno`.

.. Export the cluster namespace by running the following command:
+
[source,terminal]
----
$ export CLUSTERNS=example-sno
----

.. Create the namespace:
+
[source,terminal]
----
$ oc create namespace $CLUSTERNS
----

. Create pull secret and BMC `Secret` CRs for the managed cluster. The pull secret must contain all the credentials necessary for installing {product-title} and all required Operators. See "Creating the managed bare-metal host secrets" for more information.
+
[NOTE]
====
The secrets are referenced from the `SiteConfig` custom resource (CR) by name. The namespace must match the `SiteConfig` namespace.
====

. Create a `SiteConfig` CR for your cluster in your local clone of the Git repository:

.. Choose the appropriate example for your CR from the  `out/argocd/example/siteconfig/` folder.
The folder includes example files for single node, three-node, and standard clusters:
+
*** `example-sno.yaml`
*** `example-3node.yaml`
*** `example-standard.yaml`

.. Change the cluster and host details in the example file to match the type of cluster you want. For example:
+
.Example {sno} SiteConfig CR
[source,yaml]
----
include::snippets/ztp_example-sno.yaml[]
----
+
[NOTE]
====
For more information about BMC addressing, see the "Additional resources" section. The `installConfigOverrides` and  `ignitionConfigOverride` fields are expanded in the example for ease of readability.   
====

.. You can inspect the default set of extra-manifest `MachineConfig` CRs in `out/argocd/extra-manifest`. It is automatically applied to the cluster when it is installed.

.. Optional: To provision additional install-time manifests on the provisioned cluster, create a directory in your Git repository, for example, `sno-extra-manifest/`, and add your custom manifest CRs to this directory. If your `SiteConfig.yaml` refers to this directory in the `extraManifestPath` field, any CRs in this referenced directory are appended to the default set of extra manifests.
+
.Enabling the crun OCI container runtime
[IMPORTANT]
====
For optimal cluster performance, enable crun for master and worker nodes in {sno}, {sno} with additional worker nodes, {3no}, and standard clusters.

Enable crun in a `ContainerRuntimeConfig` CR as an additional Day 0 install-time manifest to avoid the cluster having to reboot.

The `enable-crun-master.yaml` and `enable-crun-worker.yaml` CR files are in the `out/source-crs/optional-extra-manifest/` folder that you can extract from the `ztp-site-generate` container.
For more information, see "Customizing extra installation manifests in the {ztp} pipeline".
====

. Add the `SiteConfig` CR to the `kustomization.yaml` file in the `generators` section, similar to the example shown in `out/argocd/example/siteconfig/kustomization.yaml`.

. Commit the `SiteConfig` CR and associated `kustomization.yaml` changes in your Git repository and push the changes.
+
The ArgoCD pipeline detects the changes and begins the managed cluster deployment.

.Verification

* Verify that the custom roles and labels are applied after the node is deployed:
+
[source,terminal]
----
$ oc describe node example-node.example.com
----

.Example output
[source,terminal]
----
Name:   example-node.example.com
Roles:  control-plane,example-label,master,worker
Labels: beta.kubernetes.io/arch=amd64
        beta.kubernetes.io/os=linux
        custom-label/parameter1=true
        kubernetes.io/arch=amd64
        kubernetes.io/hostname=cnfdf03.telco5gran.eng.rdu2.redhat.com
        kubernetes.io/os=linux
        node-role.kubernetes.io/control-plane=
        node-role.kubernetes.io/example-label= <1>
        node-role.kubernetes.io/master=
        node-role.kubernetes.io/worker=
        node.openshift.io/os_id=rhcos
----
<1> The custom label is applied to the node.