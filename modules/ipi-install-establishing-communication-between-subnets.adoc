// This module is included in the following assemblies:
//
// installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc

:_mod-docs-content-type: PROCEDURE
[id="ipi-install-establishing-communication-between-subnets_{context}"]
= Establishing communication between subnets

In a typical {product-title} cluster setup, all nodes, including the control plane and compute nodes, reside in the same network. However, for edge computing scenarios, it can be beneficial to locate compute nodes closer to the edge. This often involves using different network segments or subnets for the remote nodes than the subnet used by the control plane and local compute nodes. Such a setup can reduce latency for the edge and allow for enhanced scalability.

Before installing {product-title}, you must configure the network properly to ensure that the edge subnets containing the remote nodes can reach the subnet containing the control plane nodes and receive traffic from the control plane too.

You can run control plane nodes in the same subnet or multiple subnets by configuring a user-managed load balancer in place of the default load balancer. With a multiple subnet environment, you can reduce the risk of your {product-title} cluster from failing because of a hardware failure or a network outage. For more information, see "Services for a user-managed load balancer" and "Configuring a user-managed load balancer".

Running control plane nodes in a multiple subnet environment requires completion of the following key tasks:

* Configuring a user-managed load balancer instead of the default load balancer by specifying `UserManaged` in the `loadBalancer.type` parameter of the `install-config.yaml` file.
* Configuring a user-managed load balancer address in the `ingressVIPs` and `apiVIPs` parameters of the `install-config.yaml` file.
* Adding the multiple subnet Classless Inter-Domain Routing (CIDR) and the user-managed load balancer IP addresses to the `networking.machineNetworks` parameter in the `install-config.yaml` file.

[NOTE]
====
Deploying a cluster with multiple subnets requires using virtual media, such as `redfish-virtualmedia` and `idrac-virtualmedia`.
====

This procedure details the network configuration required to allow the remote compute nodes in the second subnet to communicate effectively with the control plane nodes in the first subnet and to allow the control plane nodes in the first subnet to communicate effectively with the remote compute nodes in the second subnet.

In this procedure, the cluster spans two subnets:

- The first subnet (`10.0.0.0`) contains the control plane and local compute nodes.
- The second subnet (`192.168.0.0`) contains the edge compute nodes.

.Procedure

. Configure the first subnet to communicate with the second subnet:

.. Log in as `root` to a control plane node by running the following command:
+
[source,terminal]
----
$ sudo su -
----

.. Get the name of the network interface by running the following command:
+
[source,terminal]
----
# nmcli dev status
----

.. Add a route to the second subnet (`192.168.0.0`) via the gateway by running the following command:
+
[source,terminal]
----
# nmcli connection modify <interface_name> +ipv4.routes "192.168.0.0/24 via <gateway>"
----
+
Replace `<interface_name>` with the interface name. Replace `<gateway>` with the IP address of the actual gateway.
+
.Example
+
[source,terminal]
----
# nmcli connection modify eth0 +ipv4.routes "192.168.0.0/24 via 192.168.0.1"
----

.. Apply the changes by running the following command:
+
[source,terminal]
----
# nmcli connection up <interface_name>
----
+
Replace `<interface_name>` with the interface name.

.. Verify the routing table to ensure the route has been added successfully:
+
[source,terminal]
----
# ip route
----

.. Repeat the previous steps for each control plane node in the first subnet.
+
[NOTE]
====
Adjust the commands to match your actual interface names and gateway.
====

. Configure the second subnet to communicate with the first subnet:

.. Log in as `root` to a remote compute node by running the following command:
+
[source,terminal]
----
$ sudo su -
----

.. Get the name of the network interface by running the following command:
+
[source,terminal]
----
# nmcli dev status
----

.. Add a route to the first subnet (`10.0.0.0`) via the gateway by running the following command:
+
[source,terminal]
----
# nmcli connection modify <interface_name> +ipv4.routes "10.0.0.0/24 via <gateway>"
----
+
Replace `<interface_name>` with the interface name. Replace `<gateway>` with the IP address of the actual gateway.
+
.Example
+
[source,terminal]
----
# nmcli connection modify eth0 +ipv4.routes "10.0.0.0/24 via 10.0.0.1"
----

.. Apply the changes by running the following command:
+
[source,terminal]
----
# nmcli connection up <interface_name>
----
+
Replace `<interface_name>` with the interface name.

.. Verify the routing table to ensure the route has been added successfully by running the following command:
+
[source,terminal]
----
# ip route
----

.. Repeat the previous steps for each compute node in the second subnet.
+
[NOTE]
====
Adjust the commands to match your actual interface names and gateway.
====

. After you have configured the networks, test the connectivity to ensure the remote nodes can reach the control plane nodes and the control plane nodes can reach the remote nodes.

.. From the control plane nodes in the first subnet, ping a remote node in the second subnet by running the following command:
+
[source,terminal]
----
$ ping <remote_node_ip_address>
----
+
If the ping is successful, it means the control plane nodes in the first subnet can reach the remote nodes in the second subnet. If you do not receive a response, review the network configurations and repeat the procedure for the node.

.. From the remote nodes in the second subnet, ping a control plane node in the first subnet by running the following command:
+
[source,terminal]
----
$ ping <control_plane_node_ip_address>
----
+
If the ping is successful, it means the remote compute nodes in the second subnet can reach the control plane in the first subnet. If you do not receive a response, review the network configurations and repeat the procedure for the node.
