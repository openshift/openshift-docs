// This module is included in the following assemblies:
//
// installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc

:_mod-docs-content-type: PROCEDURE
[id="ipi-install-establishing-communication-between-subnets_{context}"]
= Establishing communication between subnets

In a typical {product-title} cluster setup, all nodes, including the control plane and worker nodes, reside in the same network. However, for edge computing scenarios, it can be beneficial to locate worker nodes closer to the edge. This often involves using different network segments or subnets for the remote worker nodes than the subnet used by the control plane and local worker nodes. Such a setup can reduce latency for the edge and allow for enhanced scalability. 

Before installing {product-title}, you must configure the network properly to ensure that the edge subnets containing the remote nodes can reach the subnet containing the control plane nodes and receive traffic from the control plane too.

[IMPORTANT]
====
During cluster installation, assign permanent IP addresses to nodes in the network configuration of the `install-config.yaml` configuration file. If you do not do this, nodes might get assigned a temporary IP address that can impact how traffic reaches the nodes. For example, if a node has a temporary IP address assigned to it and you configured a bonded interface for a node, the bonded interface might receive a different IP address.
====

This procedure details the network configuration required to allow the remote worker nodes in the second subnet to communicate effectively with the control plane nodes in the first subnet and to allow the control plane nodes in the first subnet to communicate effectively with the remote worker nodes in the second subnet.

In this procedure, the cluster spans two subnets:

- The first subnet (`10.0.0.0`) contains the control plane and local worker nodes.
- The second subnet (`192.168.0.0`) contains the edge worker nodes.

[IMPORTANT]
====
All control plane nodes must run in the same subnet. When using more than one subnet, you can also configure the Ingress VIP to run on the control plane nodes by using a manifest. See "Configuring network components to run on the control plane" for details.

Deploying a cluster with multiple subnets requires using virtual media.
====

.Procedure

. Configure the first subnet to communicate with the second subnet:

.. Log in as `root` to a control plane node by running the following command:
+
[source,terminal]
----
$ sudo su -
----

.. Get the name of the network interface by running the following command:
+
[source,terminal]
----
# nmcli dev status
----

.. Add a route to the second subnet (`192.168.0.0`) via the gateway by running the following command:
+
[source,terminal]
----
# nmcli connection modify <interface_name> +ipv4.routes "192.168.0.0/24 via <gateway>"
----
+
Replace `<interface_name>` with the interface name. Replace `<gateway>` with the IP address of the actual gateway.
+
.Example
+
[source,terminal]
----
# nmcli connection modify eth0 +ipv4.routes "192.168.0.0/24 via 192.168.0.1"
----

.. Apply the changes by running the following command:
+
[source,terminal]
----
# nmcli connection up <interface_name>
----
+
Replace `<interface_name>` with the interface name.

.. Verify the routing table to ensure the route has been added successfully:
+
[source,terminal]
----
# ip route
----

.. Repeat the previous steps for each control plane node in the first subnet.
+
[NOTE]
====
Adjust the commands to match your actual interface names and gateway.
====

. Configure the second subnet to communicate with the first subnet:

.. Log in as `root` to a remote worker node by running the following command:
+
[source,terminal]
----
$ sudo su -
----

.. Get the name of the network interface by running the following command:
+
[source,terminal]
----
# nmcli dev status
----

.. Add a route to the first subnet (`10.0.0.0`) via the gateway by running the following command:
+
[source,terminal]
----
# nmcli connection modify <interface_name> +ipv4.routes "10.0.0.0/24 via <gateway>"
----
+
Replace `<interface_name>` with the interface name. Replace `<gateway>` with the IP address of the actual gateway.
+
.Example
+
[source,terminal]
----
# nmcli connection modify eth0 +ipv4.routes "10.0.0.0/24 via 10.0.0.1"
----

.. Apply the changes by running the following command:
+
[source,terminal]
----
# nmcli connection up <interface_name>
----
+
Replace `<interface_name>` with the interface name.

.. Verify the routing table to ensure the route has been added successfully by running the following command:
+
[source,terminal]
----
# ip route
----

.. Repeat the previous steps for each worker node in the second subnet.
+
[NOTE]
====
Adjust the commands to match your actual interface names and gateway.
====

. Once you have configured the networks, test the connectivity to ensure the remote worker nodes can reach the control plane nodes and the control plane nodes can reach the remote worker nodes.

.. From the control plane nodes in the first subnet, ping a remote worker node in the second subnet by running the following command:
+
[source,terminal]
----
$ ping <remote_worker_node_ip_address>
----
+
If the ping is successful, it means the control plane nodes in the first subnet can reach the remote worker nodes in the second subnet. If you do not receive a response, review the network configurations and repeat the procedure for the node.

.. From the remote worker nodes in the second subnet, ping a control plane node in the first subnet by running the following command:
+
[source,terminal]
----
$ ping <control_plane_node_ip_address>
----
+
If the ping is successful, it means the remote worker nodes in the second subnet can reach the control plane in the first subnet. If you do not receive a response, review the network configurations and repeat the procedure for the node.
