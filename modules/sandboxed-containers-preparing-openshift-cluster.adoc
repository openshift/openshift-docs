//Module included in the following assemblies:
//
// * sandboxed_containers/deploying_sandboxed_containers.adoc

[id="sandboxed-containers-preparing-openshift-cluster_{context}"]
= Preparing your cluster for {sandboxed-containers-first}

Before you install {sandboxed-containers-first}, ensure that your {product-title} cluster meets the following requirements:

* Your cluster must be installed on bare metal infrastructure with {op-system-first} workers. Your cluster must use installer-provisioned infrastructure.
// To Do: Add link to bare metal instructions: ../../installing/installing_bare_metal/installing-bare-metal.adoc#installing-bare-metal[bare metal]
// To Do: Add link to installer provisioned infrastructure: ../../installing/installing_bare_metal_ipi/ipi-install-overview.adoc#ipi-install-overview[Installer provisioned infrastructure]
+
[IMPORTANT]
====
* {sandboxed-containers-first} only supports {op-system} worker nodes. RHEL 7 or RHEL  8 nodes are not supported.
* Nested virtualization is not supported.
====

[id="sandboxed-containers-additional-resource-requirements_{context}"]
== Additional resource requirements for {sandboxed-containers-first}

{sandboxed-containers-first} is a product that brings the ability to run workloads inside a sandboxed runtime, such as Kata Containers, to your {product-title} clusters. Each pod is represented by a virtual machine (VM). Each VM runs in a `qemu` process and hosts a `kata-agent` process that acts as a supervisor for managing container workloads and processes that are running in those containers. There are two additional processes that add more overhead:

* `containerd-shim-kata-v2` is used to communicate with the pod.
* `virtiofsd` handles host file system access on behalf of the guest.

Each VM is configured with a default amount of memory. Additional memory is hot-plugged into the VM for containers that explicitly request memory.

* If a container runs without a given memory resource, it is able to consume free memory. It will do so until the total memory used by the VM reaches the default allocation. The guest and its I/O buffers also consume memory.
* If a container is given a specific amount of memory, then that memory is hot-plugged into the VM before the container starts.
* If a memory limit is specified, then the workload is terminated if it consumes more memory than the limit. If no memory limit is specified, the kernel  that is running on the virtual machine might run out of memory. If the kernel runs out of memory it might terminate other processes on the virtual machine.

.Default memory sizes

The following table lists some the default values for resource allocation.

[cols="2,2"]
|===
|Resource |Value

|Memory allocated by default to a virtual machine | 2Gi
|Guest Linux kernel memory usage at boot | ~110Mi
|Memory used by the QEMU process (excluding VM memory) | ~30Mi
|Memory used by the `virtiofsd` process (excluding VM I/O buffers) | ~10Mi
|Memory used by the `containerd-shim-kata-v2` process | ~20Mi
|File buffer cache data after running `dnf install` on Fedora | ~300Mi* ^[1]^
|===
[.small]
--

1. File buffers appear and are accounted for in multiple locations:
* In the guest where it appears as file buffer cache.
* In the `virtiofsd` daemon that maps allowed user-space file I/O operations.
* In the QEMU process as guest memory.

[NOTE]
====
Total memory usage is properly accounted for by the memory utilization metrics, which only count that memory once.
====
--

link:https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/[Pod overhead] describes the amount of system resources that a pod on a node uses. You can get the current pod overhead for the `kata` runtime class by using `oc describe runtimeclass kata` as shown below.

.Example
[source,terminal]
----
$ oc describe runtimeclass kata
----

.Example output
[source,terminal]
----
Name:         kata
[...]
Metadata:
[...]
Overhead:
  Pod Fixed:
    Cpu:     250m
    Memory:  350Mi
[...]
----

You can change the pod overhead by changing the `spec.overhead` field for a `RuntimeClass`. For instance, if the configuration that you run for your containers consumes more than 350Mi of memory for the QEMU process and guest kernel data, you can alter the `RuntimeClass` overhead to suit your needs.

[NOTE]
====
The specified default overhead values are supported by Red Hat. Changing default overhead values is not supported and can result in technical issues.
====

.Example
[source,yaml]
----
kind: RuntimeClass
apiVersion: node.k8s.io/v1
metadata:
  name: kata
overhead:
  podFixed:
    memory: "500Mi"
    cpu: "500m"
----

* The default allocation for virtual machines is 2Gi.
* The Linux kernel uses approximately 100Mi of memory at boot time.
* The QEMU process uses approximately 30Mi of memory.
* The `virtiofsd` process uses approximately 10Mi of memory.
* The `shim-v2` process uses approximately 20Mi of memory.

When performing any kind of file system I/O in the guest, file buffers are allocated in the guest kernel. The file buffers are also mapped in the QEMU process on the host, as well as on the `virtiofsd` process. For example, if you use 300Mi of file buffer cache in the guest, both QEMU and `virtiofsd` appear to use 300Mi additional memory. However, the same memory is being used in all three cases. In other words, the total memory usage is only 300Mi, mapped in three different places. This is correctly accounted for when reporting the memory utilization metrics.
