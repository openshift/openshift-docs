:_mod-docs-content-type: PROCEDURE
[id="installing-loki-operator-web-console_{context}"]
= Installing {loki-op} by using the web console

Install {loki-op} on your {ocp-product-title} cluster to manage the log store `Loki` from the OperatorHub by using the {ocp-product-title} web console. You can deploy and configure the `Loki` log store by reconciling the resource LokiStack with the {loki-op}.

.Prerequisites

* You have administrator permissions.
* You have access to the {ocp-product-title} web console.
* You have access to a supported object store (AWS S3, Google Cloud Storage, Azure, Swift, Minio, {rh-storage}).

.Procedure

. In the {ocp-product-title} web console *Administrator* perspective, go to *Operators* -> *OperatorHub*.

. Type {loki-op} in the *Filter by keyword* field. Click *{loki-op}* in the list of available Operators, and then click *Install*.
+
[IMPORTANT]
====
The Community {loki-op} is not supported by Red{nbsp}Hat.
====

. Select *stable-x.y* as the *Update channel*.
+
The {loki-op} must be deployed to the global Operator group namespace `openshift-operators-redhat`, so the *Installation mode* and *Installed Namespace* are already selected. If this namespace does not already exist, it will be created for you.

. Select *Enable Operator-recommended cluster monitoring on this namespace.*
+
This option sets the `openshift.io/cluster-monitoring: "true"` label in the `Namespace` object. You must select this option to ensure that cluster monitoring scrapes the `openshift-operators-redhat` namespace.

. For *Update approval* select *Automatic*, then click *Install*.
+
If the approval strategy in the subscription is set to *Automatic*, the update process initiates as soon as a new Operator version is available in the selected channel. If the approval strategy is set to *Manual*, you must manually approve pending updates.
+
[NOTE]
====
An Operator might display a `Failed` status before the installation completes. If the Operator install completes with an `InstallSucceeded` message, refresh the page.
====

. While the Operator installs, create the namespace to which the log store will be deployed.

.. Click *+* in the top right of the screen to access the *Import YAML* page. 

.. Add the YAML definition for the `openshift-logging` namespace:
+
.Example `namespace` object
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-logging # <1>
  labels:
    openshift.io/cluster-monitoring: "true" # <2>
----
<1> The `openshift-logging` namespace is dedicated for all {logging} workloads.
<2> A string value that specifies the label, as shown, to ensure that cluster monitoring scrapes the `openshift-logging` namespace.

.. Click *Create*.

. Create a secret with the credentials to access the object storage.

.. Click *+* in the top right of the screen to access the *Import YAML* page. 

.. Add the YAML definition for the secret. For example, create a secret to access Amazon Web Services (AWS) s3:
+
.Example `Secret` object
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: logging-loki-s3 <1>
  namespace: openshift-logging <2>
stringData: <3>
  access_key_id: <access_key_id>
  access_key_secret: <secret_access_key>
  bucketnames: <s3_bucket_name>
  endpoint: https://s3.eu-central-1.amazonaws.com
  region: eu-central-1
----
<1> Note down the name used for the secret `logging-loki-s3` to use it later when creating the `LokiStack` resource.
<2> Set the namespace to `openshift-logging` as that will be the namespace used to deploy `LokiStack`.
<3> For the contents of the secret see the Loki object storage section.
+
--
include::snippets/logging-retention-period-snip.adoc[leveloffset=+1]
--

.. Click *Create*.

. Navigate to the *Installed Operators* page. Select the {loki-op} under the *Provided APIs* find the *LokiStack* resource and click *Create Instance*.

. Select *YAML view*, and then use the following template to create a `LokiStack` CR:
+
--
.Example `LokiStack` CR
[source,yaml]
----
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki # <1>
  namespace: openshift-logging # <2>
spec:
  managementState: Managed
  limits:
    global: # <3>
      retention: # <4>
        days: 20 # Set the value as per requirement
  size: 1x.small # <5>
  storage:
    schemas:
    - version: v13
      effectiveDate: "<yyyy>-<mm>-<dd>" # <6>
    secret:
      name: logging-loki-s3 # <7>
      type: s3 # <8>
  storageClassName: <storage_class_name> # <9>
  tenants:
    mode: openshift-logging # <10>
----
<1> Use the name `logging-loki`.
<2> You must specify `openshift-logging` as the namespace.
<3> Define global limits that apply to the LokiStack instance. For information about setting stream-based retention, see link:https://docs.redhat.com/en/documentation/red_hat_openshift_logging/6.3/html/configuring_logging/configuring-lokistack-storage#logging-loki-retention_configuring-the-log-store[Enabling stream-based retention with Loki]. This field does not impact the retention period for stored logs in object storage.
<4> Retention is enabled in the cluster when this block is added to the CR.
<5> Specify the deployment size. Supported size options for production instances of Loki are `1x.extra-small`, `1x.small`, or `1x.medium`. Additionally, `1x.pico` is supported starting with {logging} 6.1.
<6> For new installations, set this date to yesterdays date. This ensures the schema applies to all future logs.
<7> Specify the name of your log store secret.
<8> Specify the corresponding storage type.
<9> Specify the name of a storage class for temporary storage. For best performance, specify a storage class that allocates block storage. You can list the available storage classes for your cluster by using the `oc get storageclasses` command.
<10> The `openshift-logging` mode is the default tenancy mode where a tenant is created for log types, such as audit, infrastructure, and application. This enables access control for individual users and user groups to different log streams.
--

. Click *Create*.

.Verification

. In the *LokiStack* tab veriy that you see your `LokiStack` instance.
. In the *Status* column, verify that you see the message `Condition: Ready` with a green checkmark.
