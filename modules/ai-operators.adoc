// Module included in the following assemblies:
//
// * ai_workloads/index.adoc

:_mod-docs-content-type: CONCEPT
[id="ai-operators_{context}"]
= Operators for running AI workloads

You can use Operators to run artificial intelligence (AI) and machine learning (ML) workloads on {product-title}. With Operators, you can build a customized environment that meets your specific AI/ML requirements while continuing to use {product-title} as the core platform for your applications.

{product-title} provides several Operators that can help you run AI workloads:

{lws-operator}::
You can use the {lws-operator} to enable large-scale AI inference workloads to run reliably across nodes with synchronization between leader and worker processes. Without proper coordination, large training runs might fail or stall.
+
For more information, see "{lws-operator} overview".

{kueue-prod-name}::
You can use {kueue-prod-name} to provide structured queues and prioritization so that workloads are handled fairly and efficiently. Without proper prioritization, important jobs might be delayed while less critical jobs occupy resources.
+
For more information, see link:https://docs.redhat.com/en/documentation/red_hat_build_of_kueue/latest/html/overview/about-kueue[Introduction to Red Hat build of Kueue] in the {kueue-prod-name} documentation.

// TODO: Anything else to list yet?

////
Keep for future use (JobSet and DRA) - From Gaurav (PM):
AI in OpenShift – Focus Areas

What We’re Building
- Smarter Resource Allocation (DRA) – enhancing how accelerators and devices are requested, bound, and shared to maximize efficiency and utilization.
- Coordinated Distributed Jobs (LWS) – enabling large-scale AI training workloads to run reliably across many nodes with proper synchronization between lead and worker processes.
- Intelligent Queuing and Scheduling (Kueue) – providing structured queues and prioritization so workloads are handled fairly, respecting policies while improving throughput.
- Batch and Group Workload Management (Job Set) – allowing sets of jobs to be submitted, scheduled, and managed together, making it easier to run multi-step AI pipelines.

The Problems We’re Solving
- Resource waste and inefficiency (DRA) – current systems often over- or under-allocate accelerators, increasing cost.
- Complexity of distributed AI training (LWS) – without coordination, large training runs can fail or stall.
- Unfair or unpredictable scheduling (Kueue) – important jobs may be delayed while less critical ones consume resources.
- Lack of support for pipelines (Job Set) – multi-job workflows are hard to manage and monitor as a single unit.
////
