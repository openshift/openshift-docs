
[id="ocp-release-known-issues_{context}"]
= Known issues

* There is a known issue with Gateway API and {aws-first}, {gcp-first}, and {azure-first} private clusters. The load balancer that is provisioned for a gateway is always configured to be external, which can cause errors or unexpected behavior:
+
--
** In an {aws-short} private cluster, the load balancer becomes stuck in the `pending` state and reports the error: `Error syncing load balancer: failed to ensure load balancer: could not find any suitable subnets for creating the ELB`.

** In {gcp-short} and {azure-short} private clusters, the load balancer is provisioned with an external IP address, when it should not have an external IP address.
--
+
There is no supported workaround for this issue. (link:https://issues.redhat.com/browse/OCPBUGS-57440[OCPBUGS-57440])

* When running a pod in an isolated user namespace, the UID/GID inside a pod container no longer matches the UID/GID on the host. For file system ownership to work correctly, the Linux kernel uses ID-mapped mounts, which translate user IDs between the container and the host at the virtual file system (VFS) layer.
+
However, not all file systems currently support ID-mapped mounts, such as Network File Systems (NFS) and other network or distributed file systems. Because such file systems do not support ID-mapped mounts, pods running within user namespaces can fail to access mounted NFS volumes. This behavior is not specific to {product-title}. It applies to all Kubernetes distributions from Kubernetes v1.33 and later.
+
When upgrading to {product-title} 4.20, clusters are unaffected until you opt in to user namespaces. After enabling user namespaces, any pod that is using an NFS-backed persistent volume from a vendor that does not support ID-mapped mounts might experience access or permission issues when running in a user namespace. For more information about enabling user namespaces, see xref:../nodes/pods/nodes-pods-user-namespaces.adoc#nodes-pods-user-namespaces-configuring_nodes-pods-user-namespaces[Configuring Linux user namespace support].
+
[NOTE]
====
Existing {product-title} 4.19 clusters are unaffected until you explicitly enable user namespaces, which is a Technology Preview feature in {product-title} 4.19.
====

* When installing a cluster on {azure-short}, if you set any of the `compute.platform.azure.identity.type`, `controlplane.platform.azure.identity.type`, or `platform.azure.defaultMachinePlatform.identity.type` field values to `None`, your cluster is unable to pull images from the Azure Container Registry.
You can avoid this issue by providing a user-assigned identity or by leaving the identity field blank.
In both cases, the installation program generates a user-assigned identity. (link:https://issues.redhat.com/browse/OCPBUGS-56008[OCPBUGS-56008])

* There is a known issue in the unified software catalog view of the console. When you select *Ecosystem* -> *Software Catalog*, you must enter an existing project name or create a new project to view the software catalog. The project selection field does not effect how catalog content is installed on the cluster. As a workaround, enter any existing project name to view the software catalog. (link:https://issues.redhat.com/browse/OCPBUGS-61870[OCPBUGS-61870])

* Starting with OCP 4.20, there is a decrease in the default maximum open files soft limit for containers. As a consequence, end users may experience application failures. To work around this problem, increase the container runtimes (CRI-O) ulimit configuration. (link:https://issues.redhat.com/browse/OCPBUGS-62095[OCPBUGS-62095])

* Deleting and recreating test workloads with a BlueField-3 NIC causes clock jumps due to inconsistent PTP synchronization. This disrupts time synchronization in test workloads. The time synchronization stabilizes when the workloads are stable. (link:https://issues.redhat.com/browse/RHEL-93579[RHEL-93579])

* Event logs for GNR-D interfaces are ambiguous due to identical three-letter prefixes ("eno"). As a consequence, affected interfaces are not clearly identified during state changes. To work around this problem, change interfaces used by ptp-operator to follow the "path" naming convention, ensuring per clock events are identified correctly based on interface names and clearly indicate which clock is affected by state changes. For more information, see link:https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/configuring_and_managing_networking/consistent-network-interface-device-naming_configuring-and-managing-networking#network-interface-naming-policies_consistent-network-interface-device-naming[Network interface naming policies]. (link:https://issues.redhat.com/browse/OCPBUGS-62817[OCPBUGS-62817])

[id="ocp-installer-known-issues_{context}"]

* When you install a cluster on {aws-short}, if you do not configure {aws-short} credentials before running any `openshift-install create` command, the installation program fails. (link:https://issues.redhat.com/browse/OCPBUGS-56658[OCPBUGS-56658])

[id="ocp-telco-core-release-known-issues_{context}"]

* On systems using specific AMD EPYC processors, some low-level system interrupts, for example `AMD-Vi`, might contain CPUs in the CPU mask that overlaps with CPU-pinned workloads. This behavior is because of the hardware design. These specific error-reporting interrupts are generally inactive and there is currently no known performance impact.(link:https://issues.redhat.com/browse/OCPBUGS-57787[OCPBUGS-57787])

* Currently, pods that use a `guaranteed` QoS class and request whole CPUs might not restart automatically after a node reboot or kubelet restart. The issue might occur in nodes configured with a static CPU Manager policy and using the `full-pcpus-only` specification, and when most or all CPUs on the node are already allocated by such workloads. As a workaround, manually delete and re-create the affected pods. (link:https://issues.redhat.com/browse/OCPBUGS-43280[*OCPBUGS-43280*])

* The Performance Profile Creator tool fails to analyze a `must-gather` archive if the archive contains a custom namespace directory that ends with the suffix `nodes`. The failure occurs because of the tool's search logic, which incorrectly reports an error for multiple matches. As a workaround, rename the custom namespace directory so that it does not end with the `nodes` suffix, and run the tool again. (link:https://issues.redhat.com/browse/OCPBUGS-60218[*OCPBUGS-60218*])

* Currently, on clusters with SR-IOV network virtual functions configured, a race condition might occur between system services responsible for network device renaming and the TuneD service managed by the Node Tuning Operator. As a consequence, the TuneD profile might become degraded after the node restarts, leading to performance degradation. As a workaround, restart the TuneD pod to restore the profile state. (link:https://issues.redhat.com/browse/OCPBUGS-41934[*OCPBUGS-41934*])

[id="ocp-telco-ran-release-known-issues_{context}"]

* The SuperMicro ARS-111GL-NHR server is unable to access virtual media during boot when the virtual media image is served through an IPv6 address. As a consequence, you cannot use virtual media on the SuperMicro ARS-111GL-NHR server model with an IPv6 network configuration. (link:https://issues.redhat.com/browse/OCPBUGS-60070[*OCPBUGS-60070*])

* A known latency issue currently affects systems running on 4th Gen Intel Xeon processors. (link:https://issues.redhat.com/browse/OCPBUGS-46528[OCPBUGS-46528])

* When attempting simultaneous BIOS and BMC firmware update on Dell R740, the BMC update might fail, leaving the server powered down and unresponsive. This issue occurs when the update process does not complete successfully, causing the system to remain in a non-operational state. (link:https://issues.redhat.com/browse/OCPBUGS-62009[*OCPBUGS-62009*])

* Updating the BMC firmware might fail if you configure the server with an incorrect network share location or invalid credentials, causing the server to remain powered off and unable to recover. (link:https://issues.redhat.com/browse/OCPBUGS-62010[*OCPBUGS-62010*])

[id="ocp-storage-core-release-known-issues_{context}"]
