// Module included in the following assemblies:
//
// * networking/multiple_networks/configuring-sr-iov.adoc

[id="example-vf-use-in-dpdk-mode-mellanox_{context}"]
= Example use of a virtual function in DPDK mode with Mellanox NICs

.Procedure

. Create the following SriovNetworkNodePolicy CR, and then save the YAML in the `mlx-dpdk-node-policy.yaml` file.
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: mlx-dpdk-node-policy
  namespace: openshift-sriov-network-operator
spec:
  resourceName: mlxnics
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  priority: <priority>
  numVfs: <num>
  nicSelector:
    vendor: "15b3"
    deviceID: "1015" <1>
    pfName: ["<pf_name>", ...]
    rootDevices: ["<pci_bus_id>", "..."]
  deviceType: netdevice <2>
  isRdma: true <3>
----

<1> Specify the device hex code of the SR-IOV network device. The only allowed values for Mellanox cards are `1015`, `1017`.
<2> Specify the driver type for the the virtual functions to `netdevice`. Mellanox SR-IOV VF can work in DPDK mode without using the `vfio-pci` device type. VF device appears as a kernel network interface inside a container.
<3> Enable RDMA mode. This is required by Mellanox cards to work in DPDK mode.

[NOTE]
=====
Please refer to `Configuring SR-IOV network devices` section for detailed explanation on each option in `SriovNetworkNodePolicy`.

When applying the configuration specified in a SriovNetworkNodePolicy CR, the SR-IOV Operator may drain the nodes, and in some cases, reboot nodes.
It may take several minutes for a configuration change to apply.
Ensure that there are enough available nodes in your cluster to handle the evicted workload beforehand.

After the configuration update is applied, all the Pods in the `openshift-sriov-network-operator` namespace will change to a `Running` status.
=====

. Create the SriovNetworkNodePolicy CR by running the following command:
+
----
$ oc create -f mlx-dpdk-node-policy.yaml
----

. Create the following SriovNetwork CR, and then save the YAML in the `mlx-dpdk-network.yaml` file.
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: mlx-dpdk-network
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: <target_namespace>
  ipam: |- <1>
    ...
  vlan: <vlan>
  resourceName: mlxnics
----

<1> Specify a configuration object for the ipam CNI plug-in as a YAML block scalar. The plug-in manages IP address assignment for the attachment definition.

[NOTE]
=====
Please refer to `Configuring SR-IOV additional network` section for detailed explanation on each option in `SriovNetwork`.
=====

. Create the SriovNetworkNodePolicy CR by running the following command:
+
----
$ oc create -f mlx-dpdk-network.yaml
----

. Create the following Pod spec, and then save the YAML in the `mlx-dpdk-pod.yaml` file.
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: dpdk-app
  namespace: <target_namespace> <1>
  annotations:
    k8s.v1.cni.cncf.io/networks: mlx-dpdk-network
spec:
  containers:
  - name: testpmd
    image: <DPDK_image> <2>
    securityContext:
     capabilities:
        add: ["IPC_LOCK"] <3>
    volumeMounts:
    - mountPath: /dev/hugepages <4>
      name: hugepage
    resources:
      limits:
        openshift.io/mlxnics: "1" <5>
        memory: "1Gi"
        cpu: "4" <6>
        hugepages-1Gi: "4Gi" <7>
      requests:
        openshift.io/mlxnics: "1"
        memory: "1Gi"
        cpu: "4"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
----

<1> Specify the same `target_namespace` where SriovNetwork CR `mlx-dpdk-network` is created. If you would like to create the Pod in a different namespace, change `target_namespace` in both Pod spec and SriovNetowrk CR.
<2> Specify the the DPDK image which includes your application and the DPDK library used by application.
<3> Specify the `IPC_LOCK` capability which is required by the application to allocate hugepage memory inside the container.
<4> Mount the hugepage volume to the DPDK Pod under `/dev/hugepages`. The hugepage volume is backed by the emptyDir volume type with the medium being `Hugepages`.
<5> (optional) Specify the number of DPDK devices allocated to the DPDK Pod. This resource request and limit, if not explicitly specified, will be automatically added by SR-IOV network resource injector. The SR-IOV network resource injector is an admission controller component managed by SR-IOV Operator. It is enabled by default and can be disabled by setting the `Injector` option to `false` in the default `SriovOperatorConfig` CR.
<6> Specify the number of CPUs. The DPDK Pod usually requires exclusive CPUs be allocated from kubelet. This is achieved by setting CPU Manager policy to `static` and creating a Pod with `Guaranteed` QoS.
<7> Specify hugepage size `hugepages-1Gi` or `hugepages-2Mi` and the quantity of hugepages that will be allocated to DPDK Pod. Configure `2Mi` and `1Gi` hugepages separately. Configuring `1Gi` hugepage requires adding kernel arguments to Nodes.

. Create the DPDK Pod by running the following command:
+
----
$ oc create -f mlx-dpdk-pod.yaml
----
