// Module included in the following assemblies:
//
// * networking/multiple_networks/configuring-sr-iov.adoc

[id="example-vf-use-in-dpdk-mode-mellanox_{context}"]
= Example use of a virtual function in DPDK mode with Mellanox NICs

.Procedure

. Create the following SriovNetworkNodePolicy CR, and then save the YAML in the `mlx-dpdk-node-policy.yaml` file.
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: mlx-dpdk-node-policy
  namespace: openshift-sriov-network-operator
spec:
  resourceName: mlxnics
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  priority: <priority>
  numVfs: <num>
  nicSelector:
    vendor: "15b3"
    deviceID: "1015" <1>
    pfNames: ["<pf_name>", ...]
    rootDevices: ["<pci_bus_id>", "..."]
  deviceType: netdevice <2>
  isRdma: true <3>
----
<1> Specify the device hex code of the SR-IOV network device. The only allowed values for Mellanox cards are `1015`, `1017`.
<2> Specify the driver type for the virtual functions to `netdevice`. Mellanox SR-IOV VF can work in DPDK mode without using the `vfio-pci` device type. VF device appears as a kernel network interface inside a container.
<3> Enable RDMA mode. This is required by Mellanox cards to work in DPDK mode.
+
[NOTE]
=====
Please refer to `Configuring SR-IOV network devices` section for detailed explanation on each option in `SriovNetworkNodePolicy`.
+
When applying the configuration specified in a SriovNetworkNodePolicy CR, the SR-IOV Operator may drain the nodes, and in some cases, reboot nodes.
It may take several minutes for a configuration change to apply.
Ensure that there are enough available nodes in your cluster to handle the evicted workload beforehand.
+
After the configuration update is applied, all the Pods in the `openshift-sriov-network-operator` namespace will change to a `Running` status.
=====

. Create the SriovNetworkNodePolicy CR by running the following command:
+
----
$ oc create -f mlx-dpdk-node-policy.yaml
----

. Create the following SriovNetwork CR, and then save the YAML in the `mlx-dpdk-network.yaml` file.
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: mlx-dpdk-network
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: <target_namespace>
  ipam: |- <1>
    ...
  vlan: <vlan>
  resourceName: mlxnics
----
<1> Specify a configuration object for the ipam CNI plug-in as a YAML block scalar. The plug-in manages IP address assignment for the attachment definition.
+
[NOTE]
=====
Please refer to `Configuring SR-IOV additional network` section for detailed explanation on each option in `SriovNetwork`.
=====

. Create the SriovNetworkNodePolicy CR by running the following command:
+
----
$ oc create -f mlx-dpdk-network.yaml
----

. Create the following Pod spec, and then save the YAML in the `mlx-dpdk-pod.yaml` file.
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: dpdk-app
  namespace: <target_namespace> <1>
  annotations:
    k8s.v1.cni.cncf.io/networks: mlx-dpdk-network
spec:
  containers:
  - name: testpmd
    image: <DPDK_image> <2>
    securityContext:
     capabilities:
        add: ["IPC_LOCK"] <3>
    volumeMounts:
    - mountPath: /dev/hugepages <4>
      name: hugepage
    resources:
      limits:
        openshift.io/mlxnics: "1" <5>
        memory: "1Gi"
        cpu: "4" <6>
        hugepages-1Gi: "4Gi" <7>
      requests:
        openshift.io/mlxnics: "1"
        memory: "1Gi"
        cpu: "4"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
----
<1> Specify the same `target_namespace` where SriovNetwork CR `mlx-dpdk-network` is created. If you would like to create the Pod in a different namespace, change `target_namespace` in both Pod spec and SriovNetowrk CR.
<2> Specify the DPDK image which includes your application and the DPDK library used by application.
<3> Specify the `IPC_LOCK` capability which is required by the application to allocate hugepage memory inside the container.
<4> Mount the hugepage volume to the DPDK Pod under `/dev/hugepages`. The hugepage volume is backed by the emptyDir volume type with the medium being `Hugepages`.
<5> (optional) Specify the number of DPDK devices allocated to the DPDK Pod. This resource request and limit, if not explicitly specified, will be automatically added by SR-IOV network resource injector. The SR-IOV network resource injector is an admission controller component managed by SR-IOV Operator. It is enabled by default and can be disabled by setting the `Injector` option to `false` in the default `SriovOperatorConfig` CR.
<6> Specify the number of CPUs. The DPDK Pod usually requires exclusive CPUs be allocated from kubelet. This is achieved by setting CPU Manager policy to `static` and creating a Pod with `Guaranteed` QoS.
<7> Specify hugepage size `hugepages-1Gi` or `hugepages-2Mi` and the quantity of hugepages that will be allocated to DPDK Pod. Configure `2Mi` and `1Gi` hugepages separately. Configuring `1Gi` hugepage requires adding kernel arguments to Nodes.

. Create the DPDK Pod by running the following command:
+
----
$ oc create -f mlx-dpdk-pod.yaml
----
