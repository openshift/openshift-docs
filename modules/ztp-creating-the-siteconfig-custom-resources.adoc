// Module included in the following assemblies:
//
// *scalability_and_performance/ztp-deploying-disconnected.adoc

:_content-type: PROCEDURE
[id="ztp-creating-the-siteconfig-custom-resources_{context}"]
= Creating the SiteConfig custom resources

ArgoCD is the engine for the GitOps site deployment. After completing a site plan that contains the required custom resources for the site installation, a policy generator creates the manifests and applies them to the hub cluster.

.Prerequisites

* Log in as a user with `cluster-admin` privileges on the hub cluster.
* Create a Git repository where you manage your custom site configuration data. The repo must be accessible from the hub cluster and be defined as a source repository for ArgoCD.

.Procedure

. Create one or more `SiteConfig` custom resources that contains the site-plan data for the clusters. For example, the following `SiteConfig` YAML configures a single node OpenShift cluster:
+
[source,yaml]
----
apiVersion: ran.openshift.io/v1
kind: SiteConfig
metadata:
  name: "example-sno"
  namespace: "example-sno"
spec:
  baseDomain: "example.com"
  pullSecretRef:
    name: "assisted-deployment-pull-secret"
  clusterImageSetNameRef: "openshift-4.10"
  sshPublicKey: "<ssh_public_key>" <1>
  clusters:
  - clusterName: "example-sno"
    networkType: "OVNKubernetes"
    extraManifestPath: siteconfig/extra-manifest <2>
    clusterLabels: <3>
      common: true
      sites : "example-sno"
    clusterNetwork:
      - cidr: 1001:1::/48
        hostPrefix: 64
    machineNetwork:
      - cidr: 1111:2222:3333:4444::/64
    serviceNetwork:
      - 1001:2::/112
    additionalNTPSources:
      - 1111:2222:3333:4444::2
    nodes:
      - hostName: "example-node1.example.com"
        role: "master"
        bmcAddress: "idrac-virtualmedia+https://[1111:2222:3333:4444::bbbb:1]/redfish/v1/Systems/System.Embedded.1"
        bmcCredentialsName:
          name: "example-node1-bmh-secret"
        bootMACAddress: "AA:BB:CC:DD:EE:11"
        bootMode: "UEFI"
        rootDeviceHints:
          hctl: '0:1:0'
        cpuset: "0-1,52-53"
        nodeNetwork:
          interfaces:
            - name: eno1
              macAddress: "AA:BB:CC:DD:EE:11"
          config:
            interfaces:
              - name: eno1
                type: ethernet
                state: up
                macAddress: "AA:BB:CC:DD:EE:11"
                ipv4:
                  enabled: false
                ipv6:
                  enabled: true
                  address:
                  # For SNO sites with static IP addresses, the node-specific, API and Ingress IPs should all be configured on the interface
                  - ip: 1111:2222:3333:4444::aaaa:1
                    prefix-length: 64
                  - ip: 1111:2222:3333:4444::1:1
                    prefix-length: 64
                  - ip: 1111:2222:3333:4444::1:2
                    prefix-length: 64
            dns-resolver:
              config:
                search:
                - example.com
                server:
                - 1111:2222:3333:4444::2
            routes:
              config:
              - destination: ::/0
                next-hop-interface: eno1
                next-hop-address: 1111:2222:3333:4444::1
                table-id: 254
----
<1> SSH public key used to access the cluster
<2> Optional: Specify additional manifests that get applied on the provisioned cluster.
<3> Cluster labels correspond to the `bindingRules` field in the `PolicyGenTemplate` examples in `./policygentemplates`. For example, `../policygentemplates/common-ranGen.yaml` applies to all clusters with `common: true` set.

. Set values for `clusterLabels` that correspond to the `PolicyGenTemplate` labels for that deployment type.

. Add hostname, BMC address, BMC secret name, and network configuration details.

. Enter a value for `clusterImageSetNameRef`, for example, `openshift-{prod-version}`.
+
[NOTE]
====
`clusterImageSetNameRef` must match an imageset available on the hub cluster. Run `oc get clusterimagesets` for the list of supported versions on your hub cluster.
====

. Make the required changes to the the cluster networking sections in the `SiteConfig` CR.
+
[NOTE]
====
For single node deployments, add values for a `MachineNetwork` and not the `apiVIP` and `ingressVIP` fields. For three-node and standard deployments, define the `apiVIP` and `ingressVIP` values and not the `MachineNetwork` fields.
====

. Optional. Create a set of extra manifest CRs that are used by the ZTP pipeline to customize the cluster installs. Reference the extra manifests using the `extraManifestPath` in the `SiteConfig` CR.

. Save the `SiteConfig` CRs and push them to the ZTP Git site config repo.

ArgoCD detects that the application is out of sync. After an automatic or manual sync, ArgoCD synchronizes the `PolicyGenTemplate` resources to the hub cluster and launches the associated resource hooks. These hooks are responsible for generating the policy wrapped configuration CRs that apply to the spoke cluster. The resource hooks convert the site definitions to installation CRs and applies them to the hub cluster:

* `Namespace` - Unique per site
* `AgentClusterInstall`
* `BareMetalHost`
* `ClusterDeployment`
* `InfraEnv`
* `NMStateConfig`
* `ExtraManifestsConfigMap` - Extra manifests. The additional manifests include workload partitioning, chronyd, mountpoint hiding, sctp enablement, and more.
* `ManagedCluster`
* `KlusterletAddonConfig`

{rh-rhacm-first} (ACM) deploys the hub cluster.
