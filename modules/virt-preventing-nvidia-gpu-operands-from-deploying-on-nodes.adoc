// Module included in the following assembly:
//
// * virt/virtual_machines/advanced_vm_management/virt-configuring-pci-passthrough.adoc
//

:_mod-docs-content-type: PROCEDURE
[id="virt-preventing-nvidia-operands-from-deploying-on-nodes_{context}"]
= Preventing NVIDIA GPU operands from deploying on nodes

If you use the link:https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html[NVIDIA GPU Operator] in your cluster, you can apply the `nvidia.com/gpu.deploy.operands=false` label to nodes that you do not want to configure for GPU or vGPU operands. This label prevents the creation of the pods that configure GPU or vGPU operands and terminates the pods if they already exist.

.Prerequisites

* The OpenShift CLI (`oc`) is installed.

.Procedure

* Label the node by running the following command:
+
[source,terminal]
----
$ oc label node <node_name> nvidia.com/gpu.deploy.operands=false <1>
----
<1> Replace `<node_name>` with the name of a node where you do not want to install the NVIDIA GPU operands.

.Verification

. Verify that the label was added to the node by running the following command:
+
[source,terminal]
----
$ oc describe node <node_name>
----

. Optional: If GPU operands were previously deployed on the node, verify their removal.

.. Check the status of the pods in the `nvidia-gpu-operator` namespace by running the following command:
+
[source,terminal]
----
$ oc get pods -n nvidia-gpu-operator
----
+
.Example output

[source,terminal]
----
NAME                             READY   STATUS        RESTARTS   AGE
gpu-operator-59469b8c5c-hw9wj    1/1     Running       0          8d
nvidia-sandbox-validator-7hx98   1/1     Running       0          8d
nvidia-sandbox-validator-hdb7p   1/1     Running       0          8d
nvidia-sandbox-validator-kxwj7   1/1     Terminating   0          9d
nvidia-vfio-manager-7w9fs        1/1     Running       0          8d
nvidia-vfio-manager-866pz        1/1     Running       0          8d
nvidia-vfio-manager-zqtck        1/1     Terminating   0          9d
----

.. Monitor the pod status until the pods with `Terminating` status are removed:
+
[source,terminal]
----
$ oc get pods -n nvidia-gpu-operator
----
+
.Example output

[source,terminal]
----
NAME                             READY   STATUS    RESTARTS   AGE
gpu-operator-59469b8c5c-hw9wj    1/1     Running   0          8d
nvidia-sandbox-validator-7hx98   1/1     Running   0          8d
nvidia-sandbox-validator-hdb7p   1/1     Running   0          8d
nvidia-vfio-manager-7w9fs        1/1     Running   0          8d
nvidia-vfio-manager-866pz        1/1     Running   0          8d
----
