// Module included in the following assembly:
//
// * virt/virtual_machines/advanced_vm_management/virt-configuring-pci-passthrough.adoc
//

:_mod-docs-content-type: PROCEDURE
[id="virt-preventing-nvidia-operands-from-deploying-on-nodes_{context}"]
= Preventing NVIDIA GPU operands from deploying on nodes

[role="_abstract"]
If you use the link:https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html[NVIDIA GPU Operator] in your cluster, you can apply the `nvidia.com/gpu.deploy.operands=false` label to nodes that you do not want to configure for GPU or vGPU operands. This prevents the creation of the pods that configure GPU or vGPU operands and terminates existing pods.

.Prerequisites

* The {oc-first} is installed.

.Procedure
// Cannot label nodes in ROSA/OSD, but can edit machine pools
* Label the node by running the following command:
+
ifndef::openshift-rosa,openshift-dedicated[]
[source,terminal]
----
$ oc label node <node_name> nvidia.com/gpu.deploy.operands=false
----
+
Replace `<node_name>` with the name of a node where you do not want to install the NVIDIA GPU operands.
endif::openshift-rosa,openshift-dedicated[]
+
ifdef::openshift-rosa,openshift-dedicated[]
[source,terminal]
----
$ rosa edit machinepool --cluster=<cluster_name> <machinepool_ID> nvidia.com/gpu.deploy.operands=false
----
endif::openshift-rosa,openshift-dedicated[]

.Verification

. Verify that the label was added to the node by running the following command:
+
[source,terminal]
----
$ oc describe node <node_name>
----

. Optional: If GPU operands were previously deployed on the node, verify their removal.

.. Check the status of the pods in the `nvidia-gpu-operator` namespace by running the following command:
+
[source,terminal]
----
$ oc get pods -n nvidia-gpu-operator
----
+
Example output:
+
[source,terminal]
----
NAME                             READY   STATUS        RESTARTS   AGE
gpu-operator-59469b8c5c-hw9wj    1/1     Running       0          8d
nvidia-sandbox-validator-7hx98   1/1     Running       0          8d
nvidia-sandbox-validator-hdb7p   1/1     Running       0          8d
nvidia-sandbox-validator-kxwj7   1/1     Terminating   0          9d
nvidia-vfio-manager-7w9fs        1/1     Running       0          8d
nvidia-vfio-manager-866pz        1/1     Running       0          8d
nvidia-vfio-manager-zqtck        1/1     Terminating   0          9d
----

.. Monitor the pod status until the pods with `Terminating` status are removed:
+
[source,terminal]
----
$ oc get pods -n nvidia-gpu-operator
----
+
Example output:
+
[source,terminal]
----
NAME                             READY   STATUS    RESTARTS   AGE
gpu-operator-59469b8c5c-hw9wj    1/1     Running   0          8d
nvidia-sandbox-validator-7hx98   1/1     Running   0          8d
nvidia-sandbox-validator-hdb7p   1/1     Running   0          8d
nvidia-vfio-manager-7w9fs        1/1     Running   0          8d
nvidia-vfio-manager-866pz        1/1     Running   0          8d
----
