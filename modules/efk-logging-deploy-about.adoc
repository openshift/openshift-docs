// Module included in the following assemblies:
//
// * logging/efk-logging-deploying-about.adoc

[id="efk-logging-deploy-about-{context}"]
= About deploying cluster logging

{product-title} cluster logging is designed to be used with the default configuration that should support most {product-title} environments. 

The installation instructions that follow include a template Cluster Logging Custom Resource, which you can use to configure your cluster logging
deployment.

If you want to use the default cluster logging install, you can use the template directly. 

If you want to customize your deployment, make changes to that template as needed. The following describes the configurations you can make when installing your cluster logging instance or modify after installtion. See the Configuring sections for more information on working with each component, including modifications you can make outside of the Cluster Logging Custom Resource.

[IMPORTANT]
====
If you change the default Cluster Logging Custom Resource, you must set your Cluster Logging Custom Resource to `Unmanaged`. In an unmanaged deployment the Cluster Logging Operator does not respond to changes in the CR after installation and does not receive updates until the cluster logging is placed back into a managed state.
====

[id="efk-logging-deploy-about-config"]
== Configuring and Tuning Cluster Logging

You can configure your cluster logging environment by modifying the Cluster Logging Custom Resource deployed
in the `openshift-logging` project.  

You can modify any of the following components upon install or after install

Management state::
Cluster logging is managed by the Cluster Logging Operator. You can place the Cluster Logging Operator into an _unmanaged_ state allowing an administrator to assume full control of individual
component configurations and upgrades by changing the `managementState` from `Managed` to `Unmanaged`.

----
  spec:
    managementState: "Managed"
----

[NOTE]
==== 
An unmanaged deployment will not receive updates until the `ClusterLogging` custom resource is placed back into a managed state.
====

Memory and CPU::
You can adjust both the CPU and memory limits for each component by modifying the `resources`
block with valid memory and CPU values:

----
spec:
  logStore:
    elasticsearch:
      resources:
        limits:
          cpu:
          memory:
        requests:
          cpu: 1
          memory: 16Gi
      type: "elasticsearch"
  collection:
    logs:
      fluentd:
        resources:
          limits:
            cpu:
            memory:
          requests:
            cpu:
            memory:
        type: "fluentd"
  visualization:
    kibana:
      resources:
        limits:
          cpu:
          memory:
        requests:
          cpu:
          memory:
     type: kibana
  curation:
    curator:
      resources:
        limits:
          memory: 200Mi
        requests:
          cpu: 200m
          memory: 200Mi
      type: "curator"
----

Elasticsearch storage::
You can configure a persistent storage class and size for the Elasticsearch cluster using the `storageClass` `name` and `size` parameters. The Cluster Logging Operator creates a `PersistentVolumeClaim` for each data node in the Elasticsearch cluster based on these parameters.  

----
  spec:
    logStore:
      type: "elasticsearch"
      elasticsearch:
        nodeCount: 3
        storage:
          storageClass: 
            name: "gp2"
            size: "200G"
----

This example specifies each data node in the cluster will be bound to a `PersistentVolumeClaim` that 
requests "200G" of "gp2" storage.  Additionally, each primary shard will be backed by a single replica.

[NOTE]
====
Omitting the `storage` block results in a deployment that includes ephemeral storage only.

----
  spec:
    logStore:
      type: "elasticsearch"
      elasticsearch:
        nodeCount: 3
        storage: {}
----
====

Elasticsearch replication policy::
You can set the policy that defines how Elasticsearch shards are replicated across data nodes in the cluster:

* `FullRedundancy`. The shards for each index are fully replicated to every data node.
* `MultipleRedundancy`. The shards for each index are spread over half of the data nodes.
* `SingleRedundancy`. A single copy of each shard. Logs are always available and recoverable as long as at least two data nodes exist.
* `ZeroRedundancy`. No copies of any shards.  Logs may be unavailable (or lost) in the event a node is down or fails.

Log collectors::
You can select which log collector is deployed as a Daemonset to each node in the {product-title} cluster, either: 
 
* Fluentd - The default log collector based on Fluentd. 
* Rsyslog - Alternate log collector supported as **Tech Preview** only.

----
  spec:
    collection:
      logs:
        fluentd:
          resources:
            limits:
              cpu:
              memory:
            requests:
              cpu:
              memory:
        type: "fluentd"
----

Curator schedule::
You specify the schedule for Curator in the [cron format](https://en.wikipedia.org/wiki/Cron).

----
  spec:
    curation:
    type: "curator"
    resources:
    curator:
      schedule: "30 3 * * *"
----

[id="efk-logging-deploy-about-sample"]
== Sample modified Cluster Logging Custom Resource

The following is an example of a Cluster Logging Custom Resource modified using the options previously described.

.Sample modified Cluster Logging Custom Resource
----
apiVersion: "logging.openshift.io/v1alpha1"
kind: "ClusterLogging"
metadata:
  name: "customresourcefluentd"
spec:
  managementState: "Managed"
  logStore:
    type: "elasticsearch"
    elasticsearch:
      nodeCount: 2
      resources:
        limits:
          memory: 2Gi
        requests:
          cpu: 200m
          memory: 2Gi
      storage: {}
      redundancyPolicy: "SingleRedundancy"
  visualization:
    type: "kibana"
    kibana:
      resources:
        limits:
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 1Gi
      replicas: 1
  curation:
    type: "curator"
    curator:
      resources:
        limits:
          memory: 200Mi
        requests:
          cpu: 200m
          memory: 200Mi
      schedule: "*/5 * * * *"
  collection:
    logs:
      type: "fluentd"
      fluentd:
        resources:
          limits:
            memory: 1Gi
          requests:
            cpu: 200m
            memory: 1Gi
----
