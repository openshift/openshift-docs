// Module included in the following assemblies:
//
// * logging/cluster-logging-external.adoc

[id="cluster-logging-collector-log-forward-about_{context}"]
= Understanding the Log Forwarding API

Forwarding cluster logs using the Log Forwarding API requires a combination of _outputs_ and _pipelines_ to send logs to specific endpoints inside and outside of your {product-title} cluster.

[NOTE]
====
If you want to use only the default internal {product-title} Elasticsearch logstore, do not configure any outputs and pipelines.
====

An _output_ is the destination for log data and a pipeline defines simple routing for one source to one or more outputs.

An output can be either:

* `elasticsearch` to forward logs to an external Elasticsearch v5.x, Elasticsearch v6.x, or Elasticsearch v7.x cluster, specified by server name or FQDN, and/or the internal {product-title} Elasticsearch logstore.
* `forward` to forward logs to an external log aggregation solution. This option uses the Fluentd *forward* protocols.

A _pipeline_ associates the type of data to an output. A type of data you can forward is one of the following:

* `logs.app` - Container logs generated by user applications running in the cluster, except infrastructure container applications.
* `logs.infra` - Logs generated by both infrastructure components running in the cluster and {product-title} nodes, such as journal logs. Infrastructure components are pods that run in the `openshift*`, `kube*`, or `default` projects.
* `logs.audit` - Logs generated by the node audit system (auditd), which are stored in the  */var/log/audit/audit.log* file, and the audit logs from the Kubernetes apiserver and the OpenShift apiserver.

To use the Log Forwarding API, you create a custom `logforwarding` configuration file with outputs and pipelines to send logs to destinations you specify.

Note the following:

* The internal {product-title} Elasticsearch logstore does not provide secure storage for audit logs. We recommend you ensure that the system to which you forward audit logs is compliant with your organizational and governmental regulations and is properly secured. {product-title} cluster logging does not comply with those regulations.

* An output supports TLS communication using a secret. Secrets must have keys of: *tls.crt*, *tls.key*, and *ca-bundle.crt* which point to the respective certificates for which they represent. Secrets must have the key *shared_key* for use when using forward in a secure manner.

* You are responsible for creating and maintaining any additional configurations that external destinations might require, such as keys and secrets, service accounts, port opening, or global proxy configuration.

The following example creates three outputs:

* the internal {product-title} Elasticsearch logstore,
* an unsecured externally-managed Elasticsearch logstore,
* a secured external log aggregator using the *forward* protocol.

Three pipelines send:

* the application logs to the internal {product-title} Elasticsearch logstore,
* the infrastructure logs to an external Elasticsearch logstore,
* the audit logs to the secured device over the *forward* protocol.

.Sample log forwarding outputs and pipelines
[source,yaml]
----
apiVersion: "logging.openshift.io/v1alpha1"
kind: "LogForwarding"
metadata:
  name: instance <1>
  namespace: openshift-logging
spec:
  disableDefaultForwarding: true <2>
  outputs: <3>
   - name: elasticsearch <4>
     type: "elasticsearch"  <5>
     endpoint: elasticsearch.openshift-logging.svc:9200 <6>
     secret: <7>
        name: fluentd
   - name: elasticsearch-insecure
     type: "elasticsearch"
     endpoint: elasticsearch-insecure.messaging.svc.cluster.local
     insecure: true <8>
   - name: secureforward-offcluster
     type: "forward"
     endpoint: https://secureforward.offcluster.com:24224
     secret:
        name: secureforward
  pipelines: <9>
   - name: container-logs <10>
     inputSource: logs.app <11>
     outputRefs: <12>
     - elasticsearch
     - secureforward-offcluster
   - name: infra-logs
     inputSource: logs.infra
     outputRefs:
     - elasticsearch-insecure
   - name: audit-logs
     inputSource: logs.audit
     outputRefs:
     - secureforward-offcluster
----
<1> The name of the log forwarding CR must be `instance`.
<2> Parameter to enable log forwarding. Set to `true` to enable log forwarding.
<3> Configuration for the outputs.
<4> A name to describe the output.
<5> The type of output, either `elasticsearch` or `forward`.
<6> The log forwarding endpoint, either the server name or FQDN. For the internal {product-title} Elasticsearch logstore, specify `elasticsearch.openshift-logging.svc:9200`.
<7> Optional name of the secret required by the endpoint for TLS communication. The secret must exist in the `openshift-logging` project.
<8> Optional setting if the endpoint does not use a secret, resulting in insecure communication.
<9> Configuration for the pipelines.
<10> A name to describe the pipeline.
<11> The source type, `logs.app`, `logs.infra`, or `logs.audit`.
<12> The name of one or more outputs configured in the CR.


[discrete]
[id="cluster-logging-external-fluentd"]
== Fluentd log handling when the external log aggregator is unavailable

If your external logging aggregator becomes unavailable and cannot receive logs, Fluentd continues to collect logs and stores them in a buffer. When the log aggregator becomes available, log forwarding resumes, including the buffered logs. If the buffer fills completely, Fluentd stops collecting logs. {product-title} rotates the logs and deletes them. You cannot adjust the buffer size or add a persistent volume claim (PVC) to the Fluentd daemon set or pods.
