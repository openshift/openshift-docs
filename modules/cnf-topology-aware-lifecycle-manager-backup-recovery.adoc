// Module included in the following assemblies:
// Epic CNF-3901 (CNF-2133) (4.11), Story TELCODOCS-339
// * scalability_and_performance/cnf-talm-for-cluster-upgrades.adoc

:_mod-docs-content-type: PROCEDURE
[id="talo-backup-recovery_{context}"]
= Recovering a cluster after a failed upgrade

If an upgrade of a cluster fails, you can manually log in to the cluster and use the backup to return the cluster to its preupgrade state. There are two stages:

Rollback:: If the attempted upgrade included a change to the platform OS deployment, you must roll back to the previous version before running the recovery script.

[IMPORTANT]
====
A rollback is only applicable to upgrades from TALM and single-node OpenShift. This process does not apply to rollbacks from any other upgrade type.
====

Recovery:: The recovery shuts down containers and uses files from the backup partition to relaunch containers and restore clusters.

.Prerequisites

* Install the {cgu-operator-first}.
* Provision one or more managed clusters.
* Install {rh-rhacm-first}.
* Log in as a user with `cluster-admin` privileges.
* Run an upgrade that is configured for backup.

.Procedure

. Delete the previously created `ClusterGroupUpgrade` custom resource (CR) by running the following command:
+
[source,terminal]
----
$ oc delete cgu/du-upgrade-4918 -n ztp-group-du-sno
----

. Log in to the cluster that you want to recover.

. Check the status of the platform OS deployment by running the following command:
+
[source,terminal]
----
$ ostree admin status
----
.Example outputs
+
[source,terminal]
----
[root@lab-test-spoke2-node-0 core]# ostree admin status
* rhcos c038a8f08458bbed83a77ece033ad3c55597e3f64edad66ea12fda18cbdceaf9.0
    Version: 49.84.202202230006-0
    Pinned: yes <1>
    origin refspec: c038a8f08458bbed83a77ece033ad3c55597e3f64edad66ea12fda18cbdceaf9
----
<1> The current deployment is pinned. A platform OS deployment rollback is not necessary.
+
[source,terminal]
----
[root@lab-test-spoke2-node-0 core]# ostree admin status
* rhcos f750ff26f2d5550930ccbe17af61af47daafc8018cd9944f2a3a6269af26b0fa.0
    Version: 410.84.202204050541-0
    origin refspec: f750ff26f2d5550930ccbe17af61af47daafc8018cd9944f2a3a6269af26b0fa
rhcos ad8f159f9dc4ea7e773fd9604c9a16be0fe9b266ae800ac8470f63abc39b52ca.0 (rollback) <1>
    Version: 410.84.202203290245-0
    Pinned: yes <2>
    origin refspec: ad8f159f9dc4ea7e773fd9604c9a16be0fe9b266ae800ac8470f63abc39b52ca
----
<1> This platform OS deployment is marked for rollback.
<2> The previous deployment is pinned and can be rolled back.

. To trigger a rollback of the platform OS deployment, run the following command:
+
[source,terminal]
----
$ rpm-ostree rollback -r
----

. The first phase of the recovery shuts down containers and restores files from the backup partition to the targeted directories. To begin the recovery, run the following command:
+
[source,terminal]
----
$ /var/recovery/upgrade-recovery.sh
----
+

. When prompted, reboot the cluster by running the following command:
+
[source,terminal]
----
$ systemctl reboot
----
. After the reboot, restart the recovery by running the following command:
+
[source,terminal]
----
$ /var/recovery/upgrade-recovery.sh  --resume
----

[NOTE]
====
If the recovery utility fails, you can retry with the `--restart` option:
[source,terminal]
----
$ /var/recovery/upgrade-recovery.sh --restart
----
====

.Verification
* To check the status of the recovery run the following command:
+
[source,terminal]
----
$ oc get clusterversion,nodes,clusteroperator
----
+
.Example output
[source,terminal]
----
NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version   4.9.23    True        False         86d     Cluster version is 4.9.23 <1>


NAME                          STATUS   ROLES           AGE   VERSION
node/lab-test-spoke1-node-0   Ready    master,worker   86d   v1.22.3+b93fd35 <2>

NAME                                                                           VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
clusteroperator.config.openshift.io/authentication                             4.9.23    True        False         False      2d7h    <3>
clusteroperator.config.openshift.io/baremetal                                  4.9.23    True        False         False      86d


..............
----
<1> The cluster version is available and has the correct version.
<2> The node status is `Ready`.
<3> The `ClusterOperator` object's availability is `True`.
