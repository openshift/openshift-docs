// This module is used in the following assemblies:

// * configure/ols-configuring-openshift-lightspeed.adoc

:_mod-docs-content-type: PROCEDURE
[id="ols-creating-lightspeed-custom-resource-file-using-web-console_{context}"]
= Creating the Lightspeed custom resource file using the web console

The Custom Resource (CR) file contains information that the Operator uses to deploy {ols-long}. The specific content of the CR file is unique for each LLM provider. Choose the configuration file that matches your LLM provider.

.Prerequisites

* You are logged in to the {ocp-product-title} web console as a user with the `cluster-admin` role. Alternatively, you are logged in to a user account that has permission to create a cluster-scoped CR file.

* You have installed the {ols-long} Operator.

.Procedure 

. Click *Add* in the upper-right corner of the {ocp-short-name} web console.

. Paste the YAML content for the LLM provider you use into the text area of the web console:
+
.OpenAI CR file
[source,yaml,subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
      - name: myOpenai
        type: openai
        credentialsSecretRef:
          name: credentials
        url: https://api.openai.com/v1
        models:
          - name: gpt-3.5-turbo
  ols:
    defaultModel: gpt-3.5-turbo
    defaultProvider: myOpenai
----
+
.{rhelai} CR file
[source,yaml,subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
    - credentialsSecretRef:
        name: openai-api-keys
      models:
      - name: models/granite-7b-redhat-lab
      name: rhelai
      type: rhelai_vllm
      url: <URL> <1>
  ols:
    defaultProvider: rhelai
    defaultModel: models/granite-7b-redhat-lab
----
<1> The URL endpoint must end with `v1` to be valid. For example, `\https://http://3.23.103.8:8000/v1`. 
+
.{rhoai} CR file
[source,yaml,subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
    - credentialsSecretRef:
        name: openai-api-keys
      models:
      - name: granite-8b-code-instruct-128k
      name: red_hat_openshift_ai
      type: rhoai_vllm
      url: <url> <1>
  ols:
    defaultProvider: red_hat_openshift_ai
    defaultModel: granite-8b-code-instruct-128k
----
<1> The URL endpoint must end with `v1` to be valid. For example, `\https://granite-8b-code-instruct.my-domain.com:443/v1`. 
+
.{azure-openai} CR file
[source,yaml,subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
      - credentialsSecretRef:
          name: credentials
        deploymentName: <azure_ai_deployment_name>
        models:
          - name: gpt-35-turbo-16k
        name: myAzure
        type: azure_openai
        url: <azure_ai_deployment_url>
  ols:
    defaultModel: gpt-35-turbo-16k
    defaultProvider: myAzure
----
+
.{watsonx} CR file
[source,yaml,subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
      - name: myWatsonx
        type: watsonx
        credentialsSecretRef:
          name: credentials
        url: <ibm_watsonx_deployment_name>
        projectId: <ibm_watsonx_project_id>
        models:
          - name: ibm/granite-13b-chat-v2
  ols:
    defaultModel: ibm/granite-13b-chat-v2
    defaultProvider: myWatsonx
----

. Click *Create*.