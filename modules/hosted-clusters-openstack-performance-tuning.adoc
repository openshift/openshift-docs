// Module included in the following assemblies:
//
// * hosted_control_planes/hcp-manage/hcp-manage-openstack

:_mod-docs-content-type: PROCEDURE
[id="hosted-clusters-openstack-performance-tuning_{context}"]
= Tuning performance for hosted cluster nodes

Create a performance profile and deploy a tuned `NodePool` resource to run high-performance workloads on {rh-openstack-first} {hcp}.

.Prerequisites

* You have {rh-openstack} flavor that has the necessary resources to run your workload, including dedicated CPU, memory, and host aggregate information.
* You have an {rh-openstack} network that is attached to SR-IOV or DPDK-capable NICs. The network must be available to the project used by hosted clusters.

.Procedure

. Create a performance profile that meets your requirements in a file called `perfprofile.yaml`. For example:
+
.Example performance profile in a config map
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: perfprof-1
  namespace: clusters
data:
  tuning: |
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: cnf-performanceprofile
      namespace: "${HYPERSHIFT_NAMESPACE}"
    data:
      tuning: |
        apiVersion: performance.openshift.io/v2
        kind: PerformanceProfile
        metadata:
          name: cnf-performanceprofile
        spec:
          additionalKernelArgs:
            - nmi_watchdog=0
            - audit=0
            - mce=off
            - processor.max_cstate=1
            - idle=poll
            - intel_idle.max_cstate=0
            - amd_iommu=on
          cpu:
            isolated: "${CPU_ISOLATED}"
            reserved: "${CPU_RESERVED}"
          hugepages:
            defaultHugepagesSize: "1G"
            pages:
              - count: ${HUGEPAGES}
                node: 0
                size: 1G
          nodeSelector:
            node-role.kubernetes.io/worker: ''
          realTimeKernel:
            enabled: false
          globallyDisableIrqLoadBalancing: true
----
+
IMPORTANT: If you do not already have environment variables set for the HyperShift Operator namespace, isolated and reserved CPUs, and huge pages count, create them before applying the performance profile.

. Apply the performance profile configuration by running the following command:
+
[source,terminal]
----
$ oc apply -f perfprof.yaml
----

. If you do not already have a `CLUSTER_NAME` environment variable set for the name of your cluster, define it.

. Set a node pool name environment variable by running the following command:
+
[source,terminal]
----
$ export NODEPOOL_NAME=$CLUSTER_NAME-cnf
----

. Set a flavor environment variable by running the following command:
+
[source,terminal]
----
$ export FLAVOR="m1.xlarge.nfv"
----

. Create a node pool that uses the performance profile by running the following command:
+
[source,terminal]
----
$ hcp create nodepool openstack \
  --cluster-name $CLUSTER_NAME \
  --name $NODEPOOL_NAME \
  --node-count 0 \
  --openstack-node-flavor $FLAVOR
----

. Patch the node pool to reference the `PerformanceProfile` resource by running the following command:
+
[source,terminal]
----
$ oc patch nodepool -n ${HYPERSHIFT_NAMESPACE} ${CLUSTER_NAME} \
  -p '{"spec":{"tuningConfig":[{"name":"cnf-performanceprofile"}]}}' --type=merge
----

. Scale the node pool by running the following command:
+
[source,terminal]
----
$ oc scale nodepool/$CLUSTER_NAME --namespace ${HYPERSHIFT_NAMESPACE} --replicas=1
----

. Wait for the nodes to be ready:

.. Wait for the nodes to be ready by running the following command:
+
[source,terminal]
----
$ oc wait --for=condition=UpdatingConfig=True nodepool \
  -n ${HYPERSHIFT_NAMESPACE} ${CLUSTER_NAME} \
  --timeout=5m
----

.. Wait for the configuration update to finish by running the following command:
+
[source,terminal]
----
$ oc wait --for=condition=UpdatingConfig=False nodepool \
  -n ${HYPERSHIFT_NAMESPACE} ${CLUSTER_NAME} \
  --timeout=30m
----

.. Wait until all nodes are healthy by running the following command:
+
[source,terminal]
----
$ oc wait --for=condition=AllNodesHealthy nodepool \
  -n ${HYPERSHIFT_NAMESPACE} ${CLUSTER_NAME} \
  --timeout=5m
----

NOTE: You can make an SSH connection into the nodes or use the `oc debug` command to verify performance configurations.