// Module included in the following assemblies:
//
// * scalability_and_performance/recommended-host-practices.adoc

:_content-type: PROCEDURE
[id="recommended-etcd-practices_{context}"]
= Recommended etcd practices

Because etcd writes data to disk and persists proposals on disk, its performance depends on disk performance.
Etcd is not a particularly I/O intensive component, however it requires a low latency block device for an optimal performance and stability. Since etcd’s consensus protocol depends on persistently storing metadata to a log (WAL), etcd is very sensitive to disk write latency. Slow disks and disk activity from other processes can cause long fsync latencies.

Those latencies can cause etcd to miss heartbeats, not commit new proposals to the disk on time, and ultimately experience request timeouts and temporary leader loss. High write latencies also lead to a OpenShift API slowness, therefore the performance of the whole cluster would be affected. Because of these reasons it’s not recommended to colocate other workloads in the control-plane nodes.

In terms of latency it’s recommended to run etcd on top of a block device able to write at least 50 IOPS of 8000 bytes length sequentially, that is to say with a latency of 20ms (etcd uses fdatasync to synchronize each write it performs in the WAL) and sequential 500 IOPS of 8000 bytes for heavy loaded clusters (2ms).  A benchmarking tool such as FIO can be used to measure these numbers.
To achieve such numbers it is recommended to run etcd on machines that are backed by SSD or NVMe disks with low latency and high throughput. Consider single-level cell (SLC) solid-state drives (SSDs), which provide 1 bit per memory cell, are durable and reliable, and are ideal for write-intensive workloads.

The following hard disk features provide optimal etcd performance:

* Low latency to support fast read operation.
* High-bandwidth writes for faster compactions and defragmentation.
* High-bandwidth reads for faster recovery from failures.
* Solid state drives as a minimum selection, however NVMe drives are preferred.
* Server-grade hardware from various manufacturers for increased reliability.
* RAID 0 technology for increased performance.
* Dedicated etcd drives. Do not place log files or other heavy workloads on etcd drives. 

Avoid NAS or SAN setups, and spinning drives. Always benchmark using utilities such as `fio`. Continuously monitor the cluster performance as it increases.

IMPORTANT: Avoid using the Network File System (NFS) protocol or other network based filesystems.

Some key metrics to monitor on a deployed {product-title} cluster are p99 of etcd disk write ahead log duration and the number of etcd leader changes. Use Prometheus to track these metrics.


To validate the hardware for etcd before or after you create the {product-title} cluster, you can use an I/O benchmarking tool called fio.

.Prerequisites

* Container runtimes such as Podman or Docker are installed on the machine that you're testing.
* Data is written to the `/var/lib/etcd` path.

.Procedure
* Run fio and analyze the results:
+
--
** If you use Podman, run this command:
[source,terminal]
+
----
$ sudo podman run --volume /var/lib/etcd:/var/lib/etcd:Z quay.io/openshift-scale/etcd-perf
----

** If you use Docker, run this command:
[source,terminal]
+
----
$ sudo docker run --volume /var/lib/etcd:/var/lib/etcd:Z quay.io/openshift-scale/etcd-perf
----
--

The output reports whether the disk is fast enough to host etcd by comparing the 99th percentile of the fsync metric captured from the run to see if it is less than 20 ms. These are some of the most important etcd metrics that could be potentially affected by I/O performance:

- `etcd_disk_wal_fsync_duration_seconds_bucket` metric reports the etcd's WAL fsync duration.
- `etcd_disk_backend_commit_duration_seconds_bucket`  metric reports the etcd backend commit latency duration.
- `etcd_server_leader_changes_seen_total` metric reports the leader changes.

Because etcd replicates the requests among all the members, its performance strongly depends on network input/output (I/O) latency. High network latencies result in etcd heartbeats taking longer than the election timeout, which results in leader elections that are disruptive to the cluster. A key metric to monitor on a deployed {product-title} cluster is the 99th percentile of etcd network peer latency on each etcd cluster member. Use Prometheus to track the metric.

The `histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[2m]))` metric reports the round trip time for etcd to finish replicating the client requests between the members. Ensure that it is less than 50 ms.
