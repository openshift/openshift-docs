// This module is used in the following assemblies:
// configure/ols-configuring-openshift-lightspeed.adoc

:_mod-docs-content-type: PROCEDURE
[id="ols-creating-lightspeed-custom-resource-file-using-cli_{context}"]
= Creating the Lightspeed custom resource file using the CLI

The Custom Resource (CR) file contains information that the Operator uses to deploy {ols-long}. The specific content of the CR file is unique for each LLM provider. Choose the configuration file that matches your LLM provider.

.Prerequisites

* You have access to the {ocp-short-name} CLI (oc) and are logged in as a user with the `cluster-admin` role. Alternatively, you are logged in to a user account that has permission to create a cluster-scoped custom resource file.

* You have installed the {ols-long} Operator.

.Procedure

. Create an `OLSConfig` file that contains the YAML content for the LLM provider you use:
+
.OpenAI custom resource file
+
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
      - name: myOpenai
        type: openai
        credentialsSecretRef:
          name: credentials
        url: "https://api.openai.com/v1"
        models:
          - name: gpt-3.5-turbo
  ols:
    defaultModel: gpt-3.5-turbo
    defaultProvider: myOpenai
----
+
[NOTE]
====
For OpenShift AI vLLM use the same configuration as OpenAI but update the URL to point to your Virtual Large Language Model (vLLM) endpoint. If OpenShift Lightspeed operates in the same cluster as the vLLM model serving instance, you can point to the internal OpenShift service hostname instead of exposing vLLM with a route.
====
+
.{azure-openai} custom resource file
+
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
      - credentialsSecretRef:
          name: credentials
        deploymentName: <azure_ai_deployment_name>
        models:
          - name: gpt-35-turbo-16k
        name: myAzure
        type: azure_openai
        url: <azure_ai_deployment_url>
  ols:
    defaultModel: gpt-35-turbo-16k
    defaultProvider: myAzure
    logLevel: DEBUG
----
+
.{watsonx} custom resource file
+
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
      - name: myWatsonx
        type: watsonx
        credentialsSecretRef:
          name: credentials
        url: <ibm_watsonx_deployment_name>
        projectId: <ibm_watsonx_project_id>
        models:
          - name: ibm/granite-13b-chat-v2
  ols:
    defaultModel: ibm/granite-13b-chat-v2
    defaultProvider: myWatsonx
    logLevel: DEBUG
----

. Run the following command:
+
[source,terminal]
----
$ oc create -f /path/to/config-cr.yaml
----
+
The Operator deploys {ols-long} using the information in YAML configuration file.
