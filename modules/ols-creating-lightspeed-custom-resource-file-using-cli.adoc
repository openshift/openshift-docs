// This module is used in the following assemblies:

// * configure/ols-configuring-openshift-lightspeed.adoc

:_mod-docs-content-type: PROCEDURE
[id="ols-creating-lightspeed-custom-resource-file-using-cli_{context}"]
= Creating the Lightspeed custom resource file using the CLI

The Custom Resource (CR) file contains information that the Operator uses to deploy {ols-long}. The specific content of the CR file is unique for each large language model (LLM) provider. To create the CR file, choose the configuration file for the LLM provider that you are using.

.Prerequisites

* You have access to the {oc-first} and are logged in as a user with the `cluster-admin` role. Alternatively, you are logged in to a user account that has permission to create a cluster-scoped CR file.

* You have configured the LLM provider.

* You have installed the {ols-long} Operator.

.Procedure

. Create an `OLSConfig` file that contains the YAML content for the LLM provider you use.
+
.OpenAI CR file
[source,yaml,subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
      - name: myOpenai
        type: openai
        credentialsSecretRef:
          name: credentials
        url: https://api.openai.com/v1
        models:
          - name: <model_name>
  ols:
    defaultModel: <model_name>
    defaultProvider: myOpenai
----
+
.{rhelai} CR file
[source,yaml,subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
    - credentialsSecretRef:
        name: openai-api-keys
      models:
      - name: models/<model_name>
      name: rhelai
      type: rhelai_vllm
      url: <URL> <1>
  ols:
    defaultProvider: rhelai
    defaultModel: models/<model_name>
    additionalCAConfigMapRef:
      name: openshift-service-ca.crt
----
<1> The URL endpoint must end with `v1` to be valid. For example, `\https://http://3.23.103.8:8000/v1`. 
+
.{rhoai} CR file
[source,yaml,subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
    - credentialsSecretRef:
        name: openai-api-keys
      models:
      - name: <model_name>
      name: red_hat_openshift_ai
      type: rhoai_vllm 
      url: <url> <1>
  ols:
    defaultProvider: red_hat_openshift_ai
    defaultModel: <model_name>
----
<1> The URL endpoint must end with `v1` to be valid. For example, `\https://<model_name>.<domain_name>.com:443/v1`. 
+
.{azure-openai} CR file
[source,yaml,subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
      - credentialsSecretRef:
          name: credentials
        deploymentName: <azure_ai_deployment_name>
        models:
          - name: <model_name>
        name: myAzure
        type: azure_openai
        url: <azure_ai_deployment_url>
  ols:
    defaultModel: <model_name>
    defaultProvider: myAzure
----
+
.{watsonx} CR file
[source,yaml,subs="attributes,verbatim"]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  llm:
    providers:
      - name: myWatsonx
        type: watsonx
        credentialsSecretRef:
          name: credentials
        url: <ibm_watsonx_deployment_name>
        projectID: <ibm_watsonx_project_id>
        models:
          - name: ibm/<model_name>
  ols:
    defaultModel: ibm/<model_name>
    defaultProvider: myWatsonx
----

. Run the following command:
+
[source,terminal]
----
$ oc create -f /path/to/config-cr.yaml
----
+
The Operator deploys {ols-long} using the information in YAML configuration file.
