// Module included in the following assemblies:
//
// * nodes/nodes-pods-autoscaling-custom.adoc

:_content-type: CONCEPT
[id="nodes-pods-autoscaling-custom-trigger_{context}"]
= Understanding the custom metrics autoscaler triggers

Triggers, also known as scalers, provide the metrics that the Custom Metrics Autoscaler Operator uses to scale your pods. 

[NOTE]
====
The custom metrics autoscaler currently supports only the  Prometheus, CPU, memory, and Apache Kafka triggers.  
====

//You can specify a single trigger or multiple triggers. When using multiple triggers, the scaling is based on the greatest value from all the triggers. This section contains examples of the triggers supported for use with {product-title}. 

You use a `ScaledObject` or `ScaledJob` custom resource to configure triggers for specific objects, as described in the sections that follow. 

[id="nodes-pods-autoscaling-custom-trigger-prom_{context}"]
== Understanding the Prometheus trigger

You can scale pods based on Prometheus metrics, which can use the installed {product-title} monitoring or an external Prometheus server as the metrics source. See _Additional resources_ for information on the configurations required to use the {product-title} monitoring as a source for metrics. 

[NOTE]
====
If Prometheus is taking metrics from the application that the custom metrics autoscaler is scaling, do not set the minimum replicas to `0` in the custom resource. If there are no application pods, the custom metrics autoscaler does not have any metrics to scale on.
====

.Example scaled object with a Prometheus target
[source,yaml,options="nowrap"]
----
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: prom-scaledobject
  namespace: my-namespace
spec:
 ...
  triggers:
  - type: prometheus <1>
    metadata:
      serverAddress: https://thanos-querier.openshift-monitoring.svc.cluster.local:9092 <2>
      namespace: kedatest <3>
      metricName: http_requests_total <4>
      threshold: '5' <5>
      query: sum(rate(http_requests_total{job="test-app"}[1m])) <6>
      authModes: "basic" <7>
      cortexOrgID: my-org <8>
      ignoreNullValues: false <9>
      unsafeSsl: "false" <10>
----
<1> Specifies Prometheus as the scaler/trigger type.
<2> Specifies the address of the Prometheus server. This example uses  {product-title} monitoring.
<3> Optional: Specifies the namespace of the object you want to scale. This parameter is mandatory if {product-title} monitoring as a source for the metrics.
<4> Specifies the name to identify the metric in the `external.metrics.k8s.io` API. If you are using more than one trigger, all metric names must be unique.
<5> Specifies the value to start scaling for.
<6> Specifies the Prometheus query to use.
<7> Specifies the authentication method to use. Prometheus scalers support bearer authentication (`bearer`), basic authentication (`basic`), or TLS authentication (`tls`). You configure the specific authentication parameters in a trigger authentication, as discussed in a following section. As needed, you can also use a secret.
<8> Optional: Passes the `X-Scope-OrgID` header to multi-tenant link:https://cortexmetrics.io/[Cortex] or link:https://grafana.com/oss/mimir/[Mimir] storage for Prometheus. This parameter is required only with multi-tenant Prometheus storage, to indicate which data Prometheus should return. 
<9> Optional: Specifies how the trigger should proceed if the Prometheus target is lost.
     * If `true`, the trigger continues to operate if the Prometheus target is lost. This is the default.
     * If `false`, the trigger returns an error if the Prometheus target is lost.
<10> Optional: Specifies whether the certificate check should be skipped. For example, you might skip the check if you use self-signed certificates at the Prometheus endpoint.
     * If `true`, the certificate check is performed.
     * If `false`, the certificate check is not performed. This is the default.

[id="nodes-pods-autoscaling-custom-trigger-cpu_{context}"]
== Understanding the CPU trigger

You can scale pods based on CPU metrics. This trigger uses cluster metrics as the source for metrics.

The custom metrics autoscaler scales the pods associated with an object to maintain the CPU usage that you specify. The autoscaler increases or decreases the number of replicas between the minimum and maximum numbers to maintain the specified CPU utilization across all pods. The memory trigger considers the memory utilization of the entire pod. If the pod has multiple containers, the memory utilization is the sum of all of the containers.

[NOTE]
====
* This trigger cannot be used with the `ScaledJob` custom resource.
* When using a memory trigger to scale an object, the object does not scale to `0`, even if you are using multiple triggers.
====

.Example scaled object with a CPU target
[source,yaml,options="nowrap"]
----
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: cpu-scaledobject
  namespace: my-namespace
spec:

 ...

  triggers:
  - type: cpu <1>
    metricType: Utilization <2>
    metadata:
      value: "60" <3>
      containerName: "api" <4>

----
<1> Specifies CPU as the scaler/trigger type.
<2> Specifies the type of metric to use, either `Utilization` or `AverageValue`.
<3> Specifies the value to trigger scaling actions upon:
* When using `Utilization`, the target value is the average of the resource metrics across all relevant pods, represented as a percentage of the requested value of the resource for the pods.
* When using `AverageValue`, the target value is the average of the metrics across all relevant pods.
<4> Optional. Specifies an individual container to scale, based on the memory utilization of only that container, rather than the entire pod. Here, only the container named `api` is to be scaled.

[id="nodes-pods-autoscaling-custom-trigger-memory_{context}"]
== Understanding the memory trigger

You can scale pods based on memory metrics. This trigger uses cluster metrics as the source for metrics.

The custom metrics autoscaler scales the pods associated with an object to maintain the average memory usage that you specify. The autoscaler increases and decreases the number of replicas between the minimum and maximum numbers to maintain the specified memory utilization across all pods. The memory trigger considers the memory utilization of entire pod. If the pod has multiple containers, the memory utilization is the sum of all of the containers.

[NOTE]
====
* This trigger cannot be used with the `ScaledJob` custom resource.
* When using a memory trigger to scale an object, the object does not scale to `0`, even if you are using multiple triggers.
====

.Example scaled object with a memory target
[source,yaml,options="nowrap"]
----
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: memory-scaledobject
  namespace: my-namespace
spec:

 ...

  triggers:
  - type: memory <1>
    metricType: Utilization <2>
    metadata:
      value: "60" <3>
      containerName: "api" <4>
----
<1> Specifies memory as the scaler/trigger type.
<2> Specifies the type of metric to use, either `Utilization` or `AverageValue`.
<3> Specifies the value to trigger scaling actions for:
* When using `Utilization`, the target value is the average of the resource metrics across all relevant pods, represented as a percentage of the requested value of the resource for the pods.
* When using `AverageValue`, the target value is the average of the metrics across all relevant pods.
<4> Optional. Specifies an individual container to scale, based on the memory utilization of only that container, rather than the entire pod. Here, only the container named `api` is to be scaled.

[id="nodes-pods-autoscaling-custom-trigger-kafka_{context}"]
== Understanding the Kafka trigger

You can scale pods based on an Apache Kafka topic or other services that support the Kafka protocol. The custom metrics autoscaler does not scale higher than the number of Kafka partitions, unless you set the `allowIdleConsumers` parameter to `true` in the scaled object or scaled job.

[NOTE]
====
If the number of consumer groups exceeds the number of partitions in a topic, the extra consumer groups sit idle.

To avoid this, by default the number of replicas does not exceed:

* The number of partitions on a topic, if a topic is specified.
* The number of partitions of all topics in the consumer group, if no topic is specified.
* The `maxReplicaCount` specified in scaled object or scaled job CR.

You can use the `allowIdleConsumers` parameter to disable these default behaviors.
====

.Example scaled object with a Kafka target
[source,yaml,options="nowrap"]
----
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: kafka-scaledobject
  namespace: my-namespace
spec:
 ...
  triggers:
  - type: kafka <1>
    metadata:
      topic: my-topic <2>
      bootstrapServers: my-cluster-kafka-bootstrap.openshift-operators.svc:9092 <3>
      consumerGroup: my-group <4>
      lagThreshold: '10' <5>
      activationLagThreshold <6>
      offsetResetPolicy: 'latest' <7>
      allowIdleConsumers: true <8>
      scaleToZeroOnInvalidOffset: false <9>
      excludePersistentLag: false <10>
      version: 1.0.0 <11>
      partitionLimitation: '1,2,10-20,31' <12>
----
<1> Specifies Kafka as the scaler/trigger type.
<2> Specifies the name of the Kafka topic on which Kafka is processing the offset lag.
<3> Specifies a comma-separated list of Kafka brokers to connect to.
<4> Specifies the name of the Kafka consumer group used for checking the offset on the topic and processing the related lag.
<5> Optional: Specifies the average target value to trigger scaling actions. The default is `5`.
<6> Optional: Specifies the target value for the activation phase.
<7> Optional: Specifies the Kafka offset reset policy for the Kafka consumer. The available values are: `latest` and `earliest`. The default is `latest`.
<8> Optional: Specifies whether the number of Kafka replicas can exceed the number of partitions on a topic.
     * If `true`, the number of Kafka replicas can exceed the number of partitions on a topic. This allows for idle Kafka consumers.
     * If `false`, the number of Kafka replicas cannot exceed the number of partitions on a topic. This is the default.
<9> Specifies how the trigger behaves when a Kafka partition does not have a valid offset.
     * If `true`, the consumers are scaled to zero for that partition.
     * If `false`, the scaler keeps a single consumer for that partition. This is the default.
<10> Optional: Specifies whether the trigger includes or excludes partition lag for partitions whose current offset is the same as the current offset of the previous polling cycle.
      * If `true`, the scaler excludes partition lag in these partitions.
      * If `false`, the trigger includes all consumer lag in all partitions. This is the default.
<11> Optional: Specifies the version of your Kafka brokers. The default is `1.0.0`.
<12> Optional: Specifies a comma-separated list of partition IDs to scope the scaling on. If set, only the listed IDs are considered when calculating lag. The default is to consider all partitions.

