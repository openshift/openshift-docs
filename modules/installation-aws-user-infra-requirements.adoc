// Module included in the following assemblies:
//
// * installing/installing_aws/upi/installing-aws-user-infra.adoc
// * installing/installing_aws/upi/installing-restricted-networks-aws.adoc

[id="installation-aws-user-infra-requirements_{context}"]
= Required AWS infrastructure components

To install {product-title} on user-provisioned infrastructure in Amazon Web Services (AWS), you must manually create both the machines and their supporting infrastructure.

For more information about the integration testing for different platforms, see the link:https://access.redhat.com/articles/4128421[OpenShift Container Platform 4.x Tested Integrations] page.

By using the provided CloudFormation templates, you can create stacks of AWS resources that represent the following components:

* An AWS Virtual Private Cloud (VPC)
* Networking and load balancing components
* Security groups and roles
* An {product-title} bootstrap node
* {product-title} control plane nodes
* An {product-title} compute node

Alternatively, you can manually create the components or you can reuse existing infrastructure that meets the cluster requirements. Review the CloudFormation templates for more details about how the components interrelate.

[id="installation-aws-user-infra-other-infrastructure_{context}"]
== Other infrastructure components

* A VPC
* DNS entries
* Load balancers (classic or network) and listeners
* A public and a private Route 53 zone
* Security groups
* IAM roles
* S3 buckets

If you are working in a disconnected environment, you are unable to reach the public IP addresses for EC2, ELB, and S3 endpoints. Depending on the level to which you want to restrict internet traffic during the installation, the following configuration options are available:

[discrete]
[id="create-vpc-endpoints_{context}"]
=== Option 1: Create VPC endpoints

Create a VPC endpoint and attach it to the subnets that the clusters are using. Name the endpoints as follows:

* `ec2.<aws_region>.amazonaws.com`
* `elasticloadbalancing.<aws_region>.amazonaws.com`
* `s3.<aws_region>.amazonaws.com`

With this option, network traffic remains private between your VPC and the required AWS services.

[discrete]
[id="create-proxy-without-vpc-endpoints_{context}"]
=== Option 2: Create a proxy without VPC endpoints
As part of the installation process, you can configure an HTTP or HTTPS proxy. With this option, internet traffic goes through the proxy to reach the required AWS services.

[discrete]
[id="create-proxy-with-vpc-endpoints_{context}"]
=== Option 3: Create a proxy with VPC endpoints
As part of the installation process, you can configure an HTTP or HTTPS proxy with VPC endpoints. Create a VPC endpoint and attach it to the subnets that the clusters are using. Name the endpoints as follows:

* `ec2.<aws_region>.amazonaws.com`
* `elasticloadbalancing.<aws_region>.amazonaws.com`
* `s3.<aws_region>.amazonaws.com`

When configuring the proxy in the `install-config.yaml` file, add these endpoints to the `noProxy` field. With this option, the proxy prevents the cluster from accessing the internet directly. However, network traffic remains private between your VPC and the required AWS services.

.Required VPC components

You must provide a suitable VPC and subnets that allow communication to your
machines.

[cols="2a,7a,3a,3a",options="header"]
|===

|Component
|AWS type
2+|Description

|VPC
|* `AWS::EC2::VPC`
* `AWS::EC2::VPCEndpoint`
2+|You must provide a public VPC for the cluster to use. The VPC uses an
endpoint that references the route tables for each subnet to improve communication with the registry that is hosted in S3.

|Public subnets
|* `AWS::EC2::Subnet`
* `AWS::EC2::SubnetNetworkAclAssociation`
2+|Your VPC must have public subnets for between 1 and 3 availability zones
and associate them with appropriate Ingress rules.

|Internet gateway
|
* `AWS::EC2::InternetGateway`
* `AWS::EC2::VPCGatewayAttachment`
* `AWS::EC2::RouteTable`
* `AWS::EC2::Route`
* `AWS::EC2::SubnetRouteTableAssociation`
* `AWS::EC2::NatGateway`
* `AWS::EC2::EIP`
2+|You must have a public internet gateway, with public routes, attached to the
VPC. In the provided templates, each public subnet has a NAT gateway with an EIP address. These NAT gateways allow cluster resources, like private subnet instances, to reach the internet and are not required for some restricted network or proxy scenarios.

.7+|Network access control
.7+| * `AWS::EC2::NetworkAcl`
* `AWS::EC2::NetworkAclEntry`
2+|You must allow the VPC to access the following ports:
h|Port
h|Reason

|`80`
|Inbound HTTP traffic

|`443`
|Inbound HTTPS traffic

|`22`
|Inbound SSH traffic

|`1024` - `65535`
|Inbound ephemeral traffic

|`0` - `65535`
|Outbound ephemeral traffic


|Private subnets
|* `AWS::EC2::Subnet`
* `AWS::EC2::RouteTable`
* `AWS::EC2::SubnetRouteTableAssociation`
2+|Your VPC can have private subnets. The provided CloudFormation templates
can create private subnets for between 1 and 3 availability zones.
If you use private subnets, you must provide appropriate routes and tables
for them.

|===


.Required DNS and load balancing components

Your DNS and load balancer configuration needs to use a public hosted zone and
can use a private hosted zone similar to the one that the installation program
uses if it provisions the cluster's infrastructure. You must
create a DNS entry that resolves to your load balancer. An entry for
`api.<cluster_name>.<domain>` must point to the external load balancer, and an
entry for `api-int.<cluster_name>.<domain>` must point to the internal load
balancer.

The cluster also requires load balancers and listeners for port 6443, which are
required for the Kubernetes API and its extensions, and port 22623, which are
required for the Ignition config files for new machines. The targets will be the
control plane nodes. Port 6443 must be accessible to both clients external to the
cluster and nodes within the cluster. Port 22623 must be accessible to nodes
within the cluster.


[cols="2a,2a,8a",options="header"]
|===

|Component
|AWS type
|Description

|DNS
|`AWS::Route53::HostedZone`
|The hosted zone for your internal DNS.

|Public load balancer
|`AWS::ElasticLoadBalancingV2::LoadBalancer`
|The load balancer for your public subnets.

|External API server record
|`AWS::Route53::RecordSetGroup`
|Alias records for the external API server.

|External listener
|`AWS::ElasticLoadBalancingV2::Listener`
|A listener on port 6443 for the external load balancer.

|External target group
|`AWS::ElasticLoadBalancingV2::TargetGroup`
|The target group for the external load balancer.

|Private load balancer
|`AWS::ElasticLoadBalancingV2::LoadBalancer`
|The load balancer for your private subnets.

|Internal API server record
|`AWS::Route53::RecordSetGroup`
|Alias records for the internal API server.

|Internal listener
|`AWS::ElasticLoadBalancingV2::Listener`
|A listener on port 22623 for the internal load balancer.

|Internal target group
|`AWS::ElasticLoadBalancingV2::TargetGroup`
|The target group for the internal load balancer.

|Internal listener
|`AWS::ElasticLoadBalancingV2::Listener`
|A listener on port 6443 for the internal load balancer.

|Internal target group
|`AWS::ElasticLoadBalancingV2::TargetGroup`
|The target group for the internal load balancer.

|===

.Security groups

The control plane and worker machines require access to the following ports:

[cols="2a,2a,2a,2a",options="header"]
|===

|Group
|Type
|IP Protocol
|Port range


.4+|`MasterSecurityGroup`
.4+|`AWS::EC2::SecurityGroup`
|`icmp`
|`0`

|`tcp`
|`22`

|`tcp`
|`6443`

|`tcp`
|`22623`

.2+|`WorkerSecurityGroup`
.2+|`AWS::EC2::SecurityGroup`
|`icmp`
|`0`

|`tcp`
|`22`


.2+|`BootstrapSecurityGroup`
.2+|`AWS::EC2::SecurityGroup`

|`tcp`
|`22`

|`tcp`
|`19531`

|===

.Control plane Ingress

The control plane machines require the following Ingress groups. Each Ingress group is
a `AWS::EC2::SecurityGroupIngress` resource.

[cols="2a,5a,2a,2a",options="header"]
|===

|Ingress group
|Description
|IP protocol
|Port range


|`MasterIngressEtcd`
|etcd
|`tcp`
|`2379`- `2380`

|`MasterIngressVxlan`
|Vxlan packets
|`udp`
|`4789`

|`MasterIngressWorkerVxlan`
|Vxlan packets
|`udp`
|`4789`

|`MasterIngressInternal`
|Internal cluster communication and Kubernetes proxy metrics
|`tcp`
|`9000` - `9999`

|`MasterIngressWorkerInternal`
|Internal cluster communication
|`tcp`
|`9000` - `9999`

|`MasterIngressKube`
|Kubernetes kubelet, scheduler and controller manager
|`tcp`
|`10250` - `10259`

|`MasterIngressWorkerKube`
|Kubernetes kubelet, scheduler and controller manager
|`tcp`
|`10250` - `10259`

|`MasterIngressIngressServices`
|Kubernetes Ingress services
|`tcp`
|`30000` - `32767`

|`MasterIngressWorkerIngressServices`
|Kubernetes Ingress services
|`tcp`
|`30000` - `32767`

|`MasterIngressGeneve`
|Geneve packets
|`udp`
|`6081`

|`MasterIngressWorkerGeneve`
|Geneve packets
|`udp`
|`6081`

|`MasterIngressIpsecIke`
|IPsec IKE packets
|`udp`
|`500`

|`MasterIngressWorkerIpsecIke`
|IPsec IKE packets
|`udp`
|`500`

|`MasterIngressIpsecNat`
|IPsec NAT-T packets
|`udp`
|`4500`

|`MasterIngressWorkerIpsecNat`
|IPsec NAT-T packets
|`udp`
|`4500`

|`MasterIngressIpsecEsp`
|IPsec ESP packets
|`50`
|`All`

|`MasterIngressWorkerIpsecEsp`
|IPsec ESP packets
|`50`
|`All`

|`MasterIngressInternalUDP`
|Internal cluster communication
|`udp`
|`9000` - `9999`

|`MasterIngressWorkerInternalUDP`
|Internal cluster communication
|`udp`
|`9000` - `9999`

|`MasterIngressIngressServicesUDP`
|Kubernetes Ingress services
|`udp`
|`30000` - `32767`

|`MasterIngressWorkerIngressServicesUDP`
|Kubernetes Ingress services
|`udp`
|`30000` - `32767`

|===


.Worker Ingress

The worker machines require the following Ingress groups. Each Ingress group is
a `AWS::EC2::SecurityGroupIngress` resource.

[cols="2a,5a,2a,2a",options="header"]
|===

|Ingress group
|Description
|IP protocol
|Port range


|`WorkerIngressVxlan`
|Vxlan packets
|`udp`
|`4789`

|`WorkerIngressWorkerVxlan`
|Vxlan packets
|`udp`
|`4789`

|`WorkerIngressInternal`
|Internal cluster communication
|`tcp`
|`9000` - `9999`

|`WorkerIngressWorkerInternal`
|Internal cluster communication
|`tcp`
|`9000` - `9999`

|`WorkerIngressKube`
|Kubernetes kubelet, scheduler, and controller manager
|`tcp`
|`10250`

|`WorkerIngressWorkerKube`
|Kubernetes kubelet, scheduler, and controller manager
|`tcp`
|`10250`

|`WorkerIngressIngressServices`
|Kubernetes Ingress services
|`tcp`
|`30000` - `32767`

|`WorkerIngressWorkerIngressServices`
|Kubernetes Ingress services
|`tcp`
|`30000` - `32767`

|`WorkerIngressGeneve`
|Geneve packets
|`udp`
|`6081`

|`WorkerIngressMasterGeneve`
|Geneve packets
|`udp`
|`6081`

|`WorkerIngressIpsecIke`
|IPsec IKE packets
|`udp`
|`500`

|`WorkerIngressMasterIpsecIke`
|IPsec IKE packets
|`udp`
|`500`

|`WorkerIngressIpsecNat`
|IPsec NAT-T packets
|`udp`
|`4500`

|`WorkerIngressMasterIpsecNat`
|IPsec NAT-T packets
|`udp`
|`4500`

|`WorkerIngressIpsecEsp`
|IPsec ESP packets
|`50`
|`All`

|`WorkerIngressMasterIpsecEsp`
|IPsec ESP packets
|`50`
|`All`

|`WorkerIngressInternalUDP`
|Internal cluster communication
|`udp`
|`9000` - `9999`

|`WorkerIngressMasterInternalUDP`
|Internal cluster communication
|`udp`
|`9000` - `9999`

|`WorkerIngressIngressServicesUDP`
|Kubernetes Ingress services
|`udp`
|`30000` - `32767`

|`WorkerIngressMasterIngressServicesUDP`
|Kubernetes Ingress services
|`udp`
|`30000` - `32767`

|===


.Roles and instance profiles

You must grant the machines permissions in AWS. The provided CloudFormation
templates grant the machines `Allow` permissions for the following `AWS::IAM::Role` objects
and provide a `AWS::IAM::InstanceProfile` for each set of roles. If you do
not use the templates, you can grant the machines the following broad permissions
or the following individual permissions.

[cols="2a,2a,2a,2a",options="header"]
|===

|Role
|Effect
|Action
|Resource

.4+|Master
|`Allow`
|`ec2:*`
|`*`

|`Allow`
|`elasticloadbalancing:*`
|`*`

|`Allow`
|`iam:PassRole`
|`*`

|`Allow`
|`s3:GetObject`
|`*`

|Worker
|`Allow`
|`ec2:Describe*`
|`*`


.3+|Bootstrap
|`Allow`
|`ec2:Describe*`
|`*`

|`Allow`
|`ec2:AttachVolume`
|`*`

|`Allow`
|`ec2:DetachVolume`
|`*`

|`Allow`
|`s3:GetObject`
|`*`

|===

[id="installation-aws-user-infra-cluster-machines_{context}"]
== Cluster machines

You need `AWS::EC2::Instance` objects for the following machines:

* A bootstrap machine. This machine is required during installation, but you can remove it after your cluster deploys.
* Three control plane machines. The control plane machines are not governed by a control plane machine set.
* Compute machines. You must create at least two compute machines, which are also known as worker machines, during installation. These machines are not governed by a compute machine set.

////
You can also create and control them by using a MachineSet after your
control plane initializes and you can access the cluster API by using the `oc`
command line interface.
////
