// Module is included in the following assemblies:
//
// * securing_openshift_gitops/managing-secrets-securely-using-sscsid-with-gitops.adoc

:_mod-docs-content-type: PROCEDURE
[id="gitops-storing-aws-secret-manager-resources-in-gitops-repository_{context}"]
= Storing AWS Secrets Manager resources in {gitops-shortname} repository

You can store AWS Secrets Manager configurations in the {gitops-shortname} repository for declarative and version-controlled secret management.

[IMPORTANT]
====
Using the SSCSI Driver Operator with AWS Secrets Manager is not supported in a hosted control plane cluster.
==== 

.Prerequisites

* You have access to the cluster with `cluster-admin` privileges.
* You have access to the {OCP} web console.
* You have extracted and prepared the `ccoctl` binary.
* You have installed the `jq` CLI tool.
* Your cluster is installed on AWS and uses AWS Security Token Service (STS).
* You have configured AWS Secrets Manager to store the required secrets.
* link:https://docs.openshift.com/container-platform/latest/nodes/pods/nodes-pods-secrets-store.html#persistent-storage-csi-secrets-store-driver-install_nodes-pods-secrets-store[SSCSI Driver Operator is installed on your cluster].
* {gitops-title} Operator is installed on your cluster.
* You have a {gitops-shortname} repository ready to use the secrets.
* You are logged in to the Argo CD instance by using the Argo CD admin account.

.Procedure

. Install the AWS Secrets Manager provider and add resources:

.. In your {gitops-shortname} repository, create a directory and add `aws-provider.yaml` file in it with the following configuration to deploy resources for the AWS Secrets Manager provider:
+
[IMPORTANT]
====
The AWS Secrets Manager provider for the SSCSI driver is an upstream provider.

This configuration is modified from the configuration provided in the upstream link:https://github.com/aws/secrets-store-csi-driver-provider-aws#installing-the-aws-provider[AWS documentation] so that it works properly with {OCP}. Changes to this configuration might impact functionality.
====
+
.Example `aws-provider.yaml` file
[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: csi-secrets-store-provider-aws
  namespace: openshift-cluster-csi-drivers
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: csi-secrets-store-provider-aws-cluster-role
rules:
- apiGroups: [""]
  resources: ["serviceaccounts/token"]
  verbs: ["create"]
- apiGroups: [""]
  resources: ["serviceaccounts"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: csi-secrets-store-provider-aws-cluster-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: csi-secrets-store-provider-aws-cluster-role
subjects:
- kind: ServiceAccount
  name: csi-secrets-store-provider-aws
  namespace: openshift-cluster-csi-drivers
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  namespace: openshift-cluster-csi-drivers
  name: csi-secrets-store-provider-aws
  labels:
    app: csi-secrets-store-provider-aws
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: csi-secrets-store-provider-aws
  template:
    metadata:
      labels:
        app: csi-secrets-store-provider-aws
    spec:
      serviceAccountName: csi-secrets-store-provider-aws
      hostNetwork: false
      containers:
        - name: provider-aws-installer
          image: public.ecr.aws/aws-secrets-manager/secrets-store-csi-driver-provider-aws:1.0.r2-50-g5b4aca1-2023.06.09.21.19
          imagePullPolicy: Always
          args:
              - --provider-volume=/etc/kubernetes/secrets-store-csi-providers
          resources:
            requests:
              cpu: 50m
              memory: 100Mi
            limits:
              cpu: 50m
              memory: 100Mi
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: "/etc/kubernetes/secrets-store-csi-providers"
              name: providervol
            - name: mountpoint-dir
              mountPath: /var/lib/kubelet/pods
              mountPropagation: HostToContainer
      tolerations:
      - operator: Exists
      volumes:
        - name: providervol
          hostPath:
            path: "/etc/kubernetes/secrets-store-csi-providers"
        - name: mountpoint-dir
          hostPath:
            path: /var/lib/kubelet/pods
            type: DirectoryOrCreate
      nodeSelector:
        kubernetes.io/os: linux
----

.. Add a `secret-provider-app.yaml` file in your {gitops-shortname} repository to create an application and deploy resources for AWS Secrets Manager:
+
.Example `secret-provider-app.yaml` file
[source,yaml]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: secret-provider-app
  namespace: openshift-gitops
spec:
  destination:
    namespace: openshift-cluster-csi-drivers
    server: https://kubernetes.default.svc
  project: default
  source:
    path: path/to/aws-provider/resources
    repoURL: https://github.com/<my-domain>/<gitops>.git # <1>
  syncPolicy:
    automated:
    prune: true
    selfHeal: true
----
<1> Update the value of the `repoURL` field to point to your {gitops-shortname} repository.

. Synchronize resources with the default Argo CD instance to deploy them in the cluster:

.. Add a label to the `openshift-cluster-csi-drivers` namespace your application is deployed in so that the Argo CD instance in the `openshift-gitops` namespace can manage it:
+
[source,terminal]
----
$ oc label namespace openshift-cluster-csi-drivers argocd.argoproj.io/managed-by=openshift-gitops
----

.. Apply the resources in your {gitops-shortname} repository to your cluster, including the `aws-provider.yaml` file you just pushed:
+
.Example output
[source,terminal]
----
application.argoproj.io/argo-app created
application.argoproj.io/secret-provider-app created
...
----

In the Argo CD UI, you can observe that the `csi-secrets-store-provider-aws` daemonset continues to synchronize resources. To resolve this issue, you must configure the SSCSI driver to mount secrets from the AWS Secrets Manager.