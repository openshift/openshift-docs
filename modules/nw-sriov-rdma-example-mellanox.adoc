// Module included in the following assemblies:
//
// * networking/multiple_networks/configuring-sr-iov.adoc

[id="example-vf-use-in-rdma-mode-mellanox_{context}"]
= Example of a virtual function in RDMA mode with Mellanox NICs

RDMA over Converged Ethernet (RoCE) is the only supported mode when using RDMA
on {product-title}.

.Procedure

. Create the following SriovNetworkNodePolicy CR, and then save the YAML in the `mlx-rdma-node-policy.yaml` file.
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: mlx-rdma-node-policy
  namespace: openshift-sriov-network-operator
spec:
  resourceName: mlxnics
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  priority: <priority>
  numVfs: <num>
  nicSelector:
    vendor: "15b3"
    deviceID: "1015" <1>
    pfName: ["<pf_name>", ...]
    rootDevices: ["<pci_bus_id>", "..."]
  deviceType: netdevice <2>
  isRdma: true <3>
----

<1> Specify the device hex code of SR-IOV network device. The only allowed values for Mellanox cards are `1015`, `1017`.
<2> Specify the driver type for the virtual functions to `netdevice`.
<3> Enable RDMA mode.

[NOTE]
=====
Please refer to the `Configuring SR-IOV network devices` section for a detailed explanation on each option in `SriovNetworkNodePolicy`.

When applying the configuration specified in a SriovNetworkNodePolicy CR, the SR-IOV Operator may drain the nodes, and in some cases, reboot nodes.
It may take several minutes for a configuration change to apply.
Ensure that there are enough available nodes in your cluster to handle the evicted workload beforehand.

After the configuration update is applied, all the Pods in the `openshift-sriov-network-operator` namespace will change to a `Running` status.
=====

. Create the SriovNetworkNodePolicy CR by running the following command:
+
----
$ oc create -f mlx-rdma-node-policy.yaml
----

. Create the following SriovNetwork CR, and then save the YAML in the `mlx-rdma-network.yaml` file.
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: mlx-rdma-network
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: <target_namespace>
  ipam: |- <1>
    ...
  vlan: <vlan>
  resourceName: mlxnics
----

<1> Specify a configuration object for the ipam CNI plug-in as a YAML block scalar. The plug-in manages IP address assignment for the attachment definition.

[NOTE]
=====
Please refer to `Configuring SR-IOV additional network` section for detailed explanation on each option in `SriovNetwork`.
=====

. Create the SriovNetworkNodePolicy CR by running the following command:
+
----
$ oc create -f mlx-rdma-network.yaml
----

. Create the following Pod spec, and then save the YAML in the `mlx-rdma-pod.yaml` file.
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: rdma-app
  namespace: <target_namespace> <1>
  annotations:
    k8s.v1.cni.cncf.io/networks: mlx-rdma-network
spec:
  containers:
  - name: testpmd
    image: <RDMA_image> <2>
    securityContext:
     capabilities:
        add: ["IPC_LOCK"] <3>
    volumeMounts:
    - mountPath: /dev/hugepages <4>
      name: hugepage
    resources:
      limits:
        memory: "1Gi"
        cpu: "4" <5>
        hugepages-1Gi: "4Gi" <6>
      requests:
        memory: "1Gi"
        cpu: "4"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
----

<1> Specify the same `target_namespace` where SriovNetwork CR `mlx-rdma-network` is created. If you would like to create the Pod in a different namespace, change `target_namespace` in both Pod spec and SriovNetowrk CR.
<2> Specify the RDMA image which includes your application and RDMA library used by application.
<3> Specify the `IPC_LOCK` capability which is required by the application to allocate hugepage memory inside the container.
<4> Mount the hugepage volume to RDMA Pod under `/dev/hugepages`. The hugepage volume is backed by the emptyDir volume type with the medium being `Hugepages`.
<5> Specify number of CPUs. The RDMA Pod usually requires exclusive CPUs be allocated from the kubelet. This is achieved by setting CPU Manager policy to `static` and create Pod with `Guaranteed` QoS.
<6> Specify hugepage size `hugepages-1Gi` or `hugepages-2Mi` and the quantity of hugepages that will be allocated to the RDMA Pod. Configure `2Mi` and `1Gi` hugepages separately. Configuring `1Gi` hugepage requires adding kernel arguments to Nodes.

. Create the RDMA Pod by running the following command:
+
----
$ oc create -f mlx-rdma-pod.yaml
----
