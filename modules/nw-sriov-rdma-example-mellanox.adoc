// Module included in the following assemblies:
//
// * networking/multiple_networks/configuring-sr-iov.adoc

[id="example-vf-use-in-rdma-mode-mellanox_{context}"]
= Example of a virtual function in RDMA mode with Mellanox NICs

RDMA over Converged Ethernet (RoCE) is the only supported mode when using RDMA
on {product-title}.

.Procedure

. Create the following SriovNetworkNodePolicy CR, and then save the YAML in the `mlx-rdma-node-policy.yaml` file.
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: mlx-rdma-node-policy
  namespace: openshift-sriov-network-operator
spec:
  resourceName: mlxnics
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  priority: <priority>
  numVfs: <num>
  nicSelector:
    vendor: "15b3"
    deviceID: "1015" <1>
    pfNames: ["<pf_name>", ...]
    rootDevices: ["<pci_bus_id>", "..."]
  deviceType: netdevice <2>
  isRdma: true <3>
----
<1> Specify the device hex code of SR-IOV network device. The only allowed values for Mellanox cards are `1015`, `1017`.
<2> Specify the driver type for the virtual functions to `netdevice`.
<3> Enable RDMA mode.
+
[NOTE]
=====
Please refer to the `Configuring SR-IOV network devices` section for a detailed explanation on each option in `SriovNetworkNodePolicy`.
+
When applying the configuration specified in a SriovNetworkNodePolicy CR, the SR-IOV Operator may drain the nodes, and in some cases, reboot nodes.
It may take several minutes for a configuration change to apply.
Ensure that there are enough available nodes in your cluster to handle the evicted workload beforehand.
+
After the configuration update is applied, all the Pods in the `openshift-sriov-network-operator` namespace will change to a `Running` status.
=====

. Create the SriovNetworkNodePolicy CR by running the following command:
+
----
$ oc create -f mlx-rdma-node-policy.yaml
----

. Create the following SriovNetwork CR, and then save the YAML in the `mlx-rdma-network.yaml` file.
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: mlx-rdma-network
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: <target_namespace>
  ipam: |- <1>
    ...
  vlan: <vlan>
  resourceName: mlxnics
----
<1> Specify a configuration object for the ipam CNI plug-in as a YAML block scalar. The plug-in manages IP address assignment for the attachment definition.
+
[NOTE]
=====
Please refer to `Configuring SR-IOV additional network` section for detailed explanation on each option in `SriovNetwork`.
=====

. Create the SriovNetworkNodePolicy CR by running the following command:
+
----
$ oc create -f mlx-rdma-network.yaml
----

. Create the following Pod spec, and then save the YAML in the `mlx-rdma-pod.yaml` file.
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: rdma-app
  namespace: <target_namespace> <1>
  annotations:
    k8s.v1.cni.cncf.io/networks: mlx-rdma-network
spec:
  containers:
  - name: testpmd
    image: <RDMA_image> <2>
    securityContext:
     capabilities:
        add: ["IPC_LOCK"] <3>
    volumeMounts:
    - mountPath: /dev/hugepages <4>
      name: hugepage
    resources:
      limits:
        memory: "1Gi"
        cpu: "4" <5>
        hugepages-1Gi: "4Gi" <6>
      requests:
        memory: "1Gi"
        cpu: "4"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
----
<1> Specify the same `target_namespace` where SriovNetwork CR `mlx-rdma-network` is created. If you would like to create the Pod in a different namespace, change `target_namespace` in both Pod spec and SriovNetowrk CR.
<2> Specify the RDMA image which includes your application and RDMA library used by application.
<3> Specify the `IPC_LOCK` capability which is required by the application to allocate hugepage memory inside the container.
<4> Mount the hugepage volume to RDMA Pod under `/dev/hugepages`. The hugepage volume is backed by the emptyDir volume type with the medium being `Hugepages`.
<5> Specify number of CPUs. The RDMA Pod usually requires exclusive CPUs be allocated from the kubelet. This is achieved by setting CPU Manager policy to `static` and create Pod with `Guaranteed` QoS.
<6> Specify hugepage size `hugepages-1Gi` or `hugepages-2Mi` and the quantity of hugepages that will be allocated to the RDMA Pod. Configure `2Mi` and `1Gi` hugepages separately. Configuring `1Gi` hugepage requires adding kernel arguments to Nodes.

. Create the RDMA Pod by running the following command:
+
----
$ oc create -f mlx-rdma-pod.yaml
----
