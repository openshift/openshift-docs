:_mod-docs-content-type: PROCEDURE
[id="installing-logging-operator-web-console_{context}"]
= Installing {clo} by using the web console

Install {clo} on your {product-title} cluster to collect and forward logs to a log store from the OperatorHub by using the {product-title} web console.

.Prerequisites

* You have administrator permissions.
* You have access to the {product-title} web console.
* You installed and configured {loki-op}.

.Procedure

. In the {product-title} web console *Administrator* perspective, go to *Operators* -> *OperatorHub*.

. Type {clo} in the *Filter by keyword* field. Click *{clo}* in the list of available Operators, and then click *Install*.

. Select *stable-x.y* as the *Update channel*. The latest version is already selected in the *Version* field.
+
The {clo} must be deployed to the {logging} namespace `openshift-logging`, so the *Installation mode* and *Installed Namespace* are already selected. If this namespace does not already exist, it will be created for you.

. Select *Enable Operator-recommended cluster monitoring on this namespace.*
+
This option sets the `openshift.io/cluster-monitoring: "true"` label in the `Namespace` object. You must select this option to ensure that cluster monitoring scrapes the `openshift-logging` namespace.

. For *Update approval* select *Automatic*, then click *Install*.
+
If the approval strategy in the subscription is set to *Automatic*, the update process initiates as soon as a new operator version is available in the selected channel. If the approval strategy is set to *Manual*, you must manually approve pending updates.
+
[NOTE]
====
An Operator might display a `Failed` status before the installation completes. If the operator installation completes with an `InstallSucceeded` message, refresh the page.
====

. While the operator installs, create the service account that will be used by the log collector to collect the logs. 

.. Click the *+* in the top right of the screen to access the *Import YAML* page. 

.. Enter the YAML definition for the service account. 
+
.Example `ServiceAccount` object
[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: logging-collector # <1>
  namespace: openshift-logging # <2>
----
<1> Note down the name used for the service account `logging-collector` to use it later when creating the `ClusterLogForwarder` resource.
<2> Set the namespace to `openshift-logging` because that is the namespace for deploying the `ClusterLogForwarder` resource.

.. Click the *Create* button.

. Create the `ClusterRoleBinding` objects to grant the necessary permissions to the log collector for accessing the logs that you want to collect and to write the log store, for example infrastructure and application logs.

.. Click the *+* in the top right of the screen to access the *Import YAML* page. 

.. Enter the YAML definition for the `ClusterRoleBinding` resources.
+
.Example `ClusterRoleBinding` resources
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: logging-collector:write-logs
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: logging-collector-logs-writer # <1>
subjects:
- kind: ServiceAccount
  name: logging-collector
  namespace: openshift-logging
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: logging-collector:collect-application
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: collect-application-logs # <2>
subjects:
- kind: ServiceAccount
  name: logging-collector
  namespace: openshift-logging
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: logging-collector:collect-infrastructure
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: collect-infrastructure-logs # <3>
subjects:
- kind: ServiceAccount
  name: logging-collector
  namespace: openshift-logging
----
<1> The cluster role to allow the log collector to write logs to LokiStack.
<2> The cluster role to allow the log collector to collect logs from applications.
<3> The cluster role to allow the log collector to collect logs from infrastructure.

.. Click the *Create* button.

. Go to the *Operators* -> *Installed Operators* page. Select the  operator and click the *All instances* tab.

. After granting the necessary permissions to the service account, navigate to the *Installed Operators* page. Select the {clo} under the *Provided APIs*, find the *ClusterLogForwarder* resource and click *Create Instance*.

. Select *YAML view*, and then use the following template to create a `ClusterLogForwarder` CR:
+
.Example `ClusterLogForwarder` CR
[source,yaml]
----
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance
  namespace: openshift-logging # <1>
spec:
  serviceAccount:
    name: logging-collector # <2>
  outputs:
  - name: lokistack-out
    type: lokiStack # <3>
    lokiStack:
      target: # <4>
        name: logging-loki 
        namespace: openshift-logging
      authentication:
        token:
          from: serviceAccount
    tls:
      ca:
        key: service-ca.crt
        configMapName: openshift-service-ca.crt
  pipelines:
  - name: infra-app-logs
    inputRefs: # <5>
    - application
    - infrastructure
    outputRefs:
    - lokistack-out
----
<1> You must specify `openshift-logging` as the namespace.
<2> Specify the name of the service account created earlier.
<3> Select the `lokiStack` output type to send logs to the `LokiStack` instance.
<4> Point the `ClusterLogForwarder` to the `LokiStack` instance created earlier.
<5> Select the log output types you want to send to the `LokiStack` instance.

. Click *Create*.

.Verification
. In the *ClusterLogForwarder* tab verify that you see your `ClusterLogForwarder` instance.

. In the *Status* column, verify that you see the messages: 

* `Condition: observability.openshift.io/Authorized`
* `observability.openshift.io/Valid, Ready`
