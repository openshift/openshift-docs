// Module included in the following assemblies:
//
// * logging/config/cluster-logging-storage-considerations.adoc

[id="cluster-logging-deploy-storage-considerations_{context}"]
= Storage considerations for cluster logging and {product-title}


A persistent volume is required for each Elasticsearch deployment to have one data volume per data node. On {product-title} this is achieved using
Persistent Volume Claims.

The Elasticsearch Operator names the PVCs using the Elasticsearch resource name. Refer to
Persistent Elasticsearch Storage for more details.

////
Below are capacity planning guidelines for {product-title} aggregate logging.

*Example scenario*

Assumptions:

. Which application: Apache
. Bytes per line: 256
. Lines per second load on application: 1
. Raw text data -> JSON

Baseline (256 characters per minute -> 15KB/min)

[cols="3,4",options="header"]
|===
|Logging Pods
|Storage Throughput

|3 es
1 kibana
1 curator
1 fluentd
| 6 pods total: 90000 x 86400 = 7,7 GB/day

|3 es
1 kibana
1 curator
11 fluentd
| 16 pods total: 225000 x 86400 = 24,0 GB/day

|3 es
1 kibana
1 curator
20 fluentd
|25 pods total: 225000 x 86400 = 32,4 GB/day
|===


Calculating total logging throughput and disk space required for your {product-title} cluster requires knowledge of your applications. For example, if one of your
applications on average logs 10 lines-per-second, each 256 bytes-per-line,
calculate per-application throughput and disk space as follows:

----
 (bytes-per-line * (lines-per-second) = 2560 bytes per app per second
 (2560) * (number-of-pods-per-node,100) = 256,000 bytes per second per node
 256k * (number-of-nodes) = total logging throughput per cluster
----
////

Fluentd ships any logs from *systemd journal* and */var/log/containers/* to Elasticsearch.

Therefore, consider how much data you need in advance and that you are
aggregating application log data. Some Elasticsearch users have found that it
is necessary to keep absolute storage consumption around 50% and below 70% at all times. This
helps to avoid Elasticsearch becoming unresponsive during large merge
operations.

By default, at 85% Elasticsearch stops allocating new data to the node, at 90% Elasticsearch attempts to relocate
existing shards from that node to other nodes if possible. But if no nodes have free capacity below 85%, Elasticsearch effectively rejects creating new indices
and becomes RED.

[NOTE]
====
These low and high watermark values are Elasticsearch defaults in the current release. You can modify these values,
but you also must apply any modifications to the alerts also. The alerts are based
on these defaults.
====
