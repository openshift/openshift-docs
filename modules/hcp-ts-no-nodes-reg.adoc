// Module included in the following assemblies:
//
// * hosted_control_planes/hcp-troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="hcp-ts-no-nodes-reg_{context}"]
= No worker nodes are registered

If a hosted control plane is not coming fully online because the hosted control plane has no worker nodes registered, identify the problem by checking the status of various parts of the hosted control plane.

.Procedure

* View the `HostedCluster` and `NodePool` conditions for failures that indicate what the problem might be.

* Enter the following command to view the KubeVirt worker node virtual machine (VM) status for the `NodePool` resource:
+
[source,terminal]
----
$ oc get vm -n <namespace>
----

* If the VMs are stuck in the provisioning state, enter the following command to view the CDI import pods within the VM namespace for clues about why the importer pods have not completed:
+
[source,terminal]
----
$ oc get pods -n <namespace> | grep "import"
----

* If the VMs are stuck in the starting state, enter the following command to view the status of the virt-launcher pods:
+
[source,terminal]
----
$ oc get pods -n <namespace> -l kubevirt.io=virt-launcher
----
+
If the virt-launcher pods are in a pending state, investigate why the pods are not being scheduled. For example, not enough resources might exist to run the virt-launcher pods.

* If the VMs are running but they are not registered as worker nodes, use the web console to gain VNC access to one of the affected VMs. The VNC output indicates whether the ignition configuration was applied. If a VM cannot access the hosted control plane ignition server on startup, the VM cannot be provisioned correctly.

* If the ignition configuration was applied but the VM is still not registering as a node, see _Identifying the problem: Access the VM console logs_ to learn how to access the VM console logs during startup.