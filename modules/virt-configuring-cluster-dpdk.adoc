// Module included in the following assemblies:
//
// * virt/virtual_machines/vm_networking/virt-attaching-vm-to-sriov-network.adoc

:_content-type: PROCEDURE
[id="virt-configuring-cluster-dpdk_{context}"]
= Configuring a cluster for DPDK workloads

You can use the following procedure to configure an {product-title} cluster to run Data Plane Development Kit (DPDK) workloads.

:FeatureName: Configuring a cluster for DPDK workloads
include::snippets/technology-preview.adoc[]

.Prerequisites
* You have access to the cluster as a user with `cluster-admin` permissions.
* You have installed the OpenShift CLI (`oc`).
* You have installed the SR-IOV Network Operator.
* You have installed the Node Tuning Operator.

.Procedure 
. Map your compute nodes topology to determine which Non-Uniform Memory Access (NUMA) CPUs are isolated for DPDK applications and which ones are reserved for the operating system (OS).
. Label a subset of the compute nodes with a custom role; for example, `worker-dpdk`:
+
[source,terminal]
----
$ oc label node <node_name> node-role.kubernetes.io/worker-dpdk=""
---- 

. Create a new `MachineConfigPool` manifest that contains the `worker-dpdk` label in the `spec.machineConfigSelector` object:
+
.Example `MachineConfigPool` manifest
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: dpdk-workers
  labels:
    machineconfiguration.openshift.io/role: dpdk-workers
    pools.operator.machineconfiguration.openshift.io/dpdk-workers: ""
spec:
  configuration:
  machineConfigSelector:
    matchExpressions:
      - key: machineconfiguration.openshift.io/role
        operator: In
        values:
          - worker
          - worker-dpdk
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-dpdk: ""
----

. Create a `PerformanceProfile` manifest that applies to the labeled nodes and the machine config pool that you created in the previous steps. The performance profile specifies the CPUs that are isolated for DPDK applications and the CPUs that are reserved for house keeping.
+
.Example `PerformanceProfile` manifest
[source,yaml]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: profile-1
spec:
  cpu:
    isolated: 20,22,24,26,28,30,32,34,36,38,60,62,64,66,68,70,72,74,76,78
    reserved: 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,21,23,25,27,29,31,33,35,37,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,61,63,65,67,69,71,73,75,77,79
  hugepages:
    defaultHugepagesSize: 1G
    pages:
      - count: 32
        size: 1G
  numa:
    topologyPolicy: single-numa-node
  nodeSelector:
    node-role.kubernetes.io/worker-dpdk: "" 
  globallyDisableIrqLoadBalancing: true
----
+
[NOTE]
====
The compute nodes automatically restart after you apply the `MachineConfigPool` and `PerformanceProfile` manifests.
====

. Create an `SriovNetworkNodePolicy` object with the `spec.deviceType` field set to `vfio-pci`:
+
.Example `SriovNetworkNodePolicy` manifest
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: intel_nics_dpdk
  deviceType: vfio-pci
  mtu: 9000
  numVfs: 4
  priority: 99
  nicSelector:
    vendor: "8086"
    deviceID: "1572"
    pfNames:
      - eno3
    rootDevices:
      - "0000:19:00.2"
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
----
 