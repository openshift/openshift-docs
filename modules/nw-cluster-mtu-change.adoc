// Module included in the following assemblies:
//
// * networking/changing-cluster-network-mtu.adoc
// * installing/installing_aws/aws-compute-edge-zone-tasks.adoc 
// * installing/installing_aws/ipi/installing-aws-outposts.adoc

ifeval::["{context}" == "aws-compute-edge-tasks-local-zone"]
:local-zone:
endif::[]
ifeval::["{context}" == "aws-compute-edge-tasks-wavelength-zone"]
:wavelength-zone:
endif::[]
ifeval::["{context}" == "aws-compute-edge-zone-tasks"]
:post-aws-zones:
endif::[]
ifeval::["{context}" == "installing-aws-outposts"]
:outposts:
endif::[]

:_mod-docs-content-type: PROCEDURE
[id="nw-cluster-mtu-change_{context}"]
ifndef::outposts[= Changing the cluster network MTU]
ifdef::outposts[= Changing the cluster network MTU to support AWS Outposts]

ifdef::outposts[]
During installation, the maximum transmission unit (MTU) for the cluster network is detected automatically based on the MTU of the primary network interface of nodes in the cluster. You might need to decrease the MTU value for the cluster network to support an AWS Outposts subnet.
endif::outposts[]

ifndef::outposts[As a cluster administrator, you can increase or decrease the maximum transmission unit (MTU) for your cluster.]

[IMPORTANT]
====
The migration is disruptive and nodes in your cluster might be temporarily unavailable as the MTU update takes effect.
====

ifdef::outposts[For more details about the migration process, including important service interruption considerations, see "Changing the MTU for the cluster network" in the additional resources for this procedure.]

ifndef::local-zone,wavelength-zone,post-aws-zones,outposts[]
The following procedure describes how to change the cluster network MTU by using either machine configs, Dynamic Host Configuration Protocol (DHCP), or an ISO image. If you use either the DHCP or ISO approaches, you must refer to configuration artifacts that you kept after installing your cluster to complete the procedure.
endif::local-zone,wavelength-zone,post-aws-zones,outposts[]

.Prerequisites

* You have installed the {oc-first}.
* You have access to the cluster using an account with `cluster-admin` permissions.
* You have identified the target MTU for your cluster. The MTU for the OVN-Kubernetes network plugin must be set to `100` less than the lowest hardware MTU value in your cluster.

.Procedure

. To obtain the current MTU for the cluster network, enter the following command:
+
[source,terminal]
----
$ oc describe network.config cluster
----
+
.Example output
[source,text]
----
...
Status:
  Cluster Network:
    Cidr:               10.217.0.0/22
    Host Prefix:        23
  Cluster Network MTU:  1400
  Network Type:         OVNKubernetes
  Service Network:
    10.217.4.0/23
...
----

ifndef::local-zone,wavelength-zone,post-aws-zones,outposts[]
. Prepare your configuration for the hardware MTU by selecting one of the following methods:
+
.. If your hardware MTU is specified with DHCP, update your DHCP configuration similar to the following dnsmasq configuration:
+
[source,text]
----
dhcp-option-force=26,<mtu> <1>
----
<1> Where `<mtu>` specifies the hardware MTU for the DHCP server to advertise.
+
.. If your hardware MTU is specified with a kernel command line with PXE, update that configuration accordingly.
+
.. If your hardware MTU is specified in a NetworkManager connection configuration, complete the following steps. This method is the default for {product-title} if you do not explicitly specify your network configuration with DHCP, a kernel command line, or some other method. Your cluster nodes must all use the same underlying network configuration for the following procedure to work unmodified.
+
... Find the primary network interface by entering the following command:
+
[source,terminal]
----
$ oc debug node/<node_name> -- chroot /host nmcli -g connection.interface-name c show ovs-if-phys0 <1> <2>
----
<1> Where `<node_name>` specifies the name of a node in your cluster.
<2> Where `ovs-if-phys0` is the primary network interface. For nodes that use multiple NIC bonds, append `bond-sub0` for the primary NIC bond interface and `bond-sub1` for the secondary NIC bond interface.
+
... Create the following NetworkManager configuration in the `<interface>-mtu.conf` file.
+
.Example NetworkManager connection configuration
[source,ini]
----
[connection-<interface>-mtu]
match-device=interface-name:<interface> <1>
ethernet.mtu=<mtu> <2>
----
<1> Where `<interface>` specifies the primary network interface name.
<2> Where `<mtu>` specifies the new hardware MTU value.
+
[NOTE]
====
For nodes that use a network interface controller (NIC) bond interface, list the bond interface and any sub-interfaces in the `<bond-interface>-mtu.conf` file. 

.Example NetworkManager connection configuration
[source,ini]
----
[bond0-mtu] 
match-device=interface-name:bond0 
ethernet.mtu=9000 

[connection-eth0-mtu] 
match-device=interface-name:eth0 
ethernet.mtu=9000 

[connection-eth1-mtu] 
match-device=interface-name:eth1 
ethernet.mtu=9000
----
====
+
... Create the following Butane config in the `control-plane-interface.bu` file, which is the `MachineConfig` object for the control plane nodes:
+
[source,yaml,subs="attributes+"]
----
variant: openshift
version: {product-version}.0
metadata:
  name: 01-control-plane-interface
  labels:
    machineconfiguration.openshift.io/role: master
storage:
  files:
    - path: /etc/NetworkManager/conf.d/99-<interface>-mtu.conf <1>
      contents:
        local: <interface>-mtu.conf <2>
      mode: 0600
----
<1> Specify the NetworkManager connection name for the primary network interface.
<2> Specify the local filename for the updated NetworkManager configuration file from the previous step. For NIC bonds, specify the name for the `<bond-interface>-mtu.conf` file.
+
... Create the following Butane config in the `worker-interface.bu` file, which is the `MachineConfig` object for the compute nodes:
+
[source,yaml,subs="attributes+"]
----
variant: openshift
version: {product-version}.0
metadata:
  name: 01-worker-interface
  labels:
    machineconfiguration.openshift.io/role: worker
storage:
  files:
    - path: /etc/NetworkManager/conf.d/99-<interface>-mtu.conf <1>
      contents:
        local: <interface>-mtu.conf <2>
      mode: 0600
----
<1> Specify the NetworkManager connection name for the primary network interface.
<2> Specify the local filename for the updated NetworkManager configuration file from the previous step. 
+
... Create `MachineConfig` objects from the Butane configs by running the following command:
+
[source,terminal]
----
$ for manifest in control-plane-interface worker-interface; do
    butane --files-dir . $manifest.bu > $manifest.yaml
  done
----
+
[WARNING]
====
Do not apply these machine configs until explicitly instructed later in this procedure. Applying these machine configs now causes a loss of stability for the cluster.
====
endif::local-zone,wavelength-zone,post-aws-zones,outposts[]

. To begin the MTU migration, specify the migration configuration by entering the following command. The Machine Config Operator performs a rolling reboot of the nodes in the cluster in preparation for the MTU change.
+
[source,terminal]
----
$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": { "mtu": { "network": { "from": <overlay_from>, "to": <overlay_to> } , "machine": { "to" : <machine_to> } } } } }' <1> <2> <3>
----
<1> Where `<overlay_from>` specifies the current cluster network MTU value.
<2> Where `<overlay_to>` specifies the target MTU for the cluster network.
<3> Where `<machine_to>` specifies the MTU for the primary network interface on the underlying host network. For OVN-Kubernetes, this value must be `100` less than the value of `<machine_to>`.
+
ifndef::outposts[]
.Example that increases the cluster MTU
[source,terminal]
----
$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": { "mtu": { "network": { "from": 1400, "to": 8900 } , "machine": { "to" : 9000} } } } }'
----
endif::outposts[]
ifdef::outposts[]
.Example that decreases the cluster MTU
[source,terminal]
----
$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": { "mtu": { "network": { "from": 1400, "to": 1000 } , "machine": { "to" : 1100} } } } }'
----
endif::outposts[]

. As the Machine Config Operator updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:
+
[source,terminal]
----
$ oc get machineconfigpools
----
+
A successfully updated node has the following status: `UPDATED=true`, `UPDATING=false`, `DEGRADED=false`.
+
[NOTE]
====
By default, the Machine Config Operator updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.
====

. Confirm the status of the new machine configuration on the hosts:

.. To list the machine configuration state and the name of the applied machine configuration, enter the following command:
+
[source,terminal]
----
$ oc describe node | egrep "hostname|machineconfig"
----
+
.Example output
[source,text]
----
kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done
----

.. Verify that the following statements are true:
+
* The value of `machineconfiguration.openshift.io/state` field is `Done`.
* The value of the `machineconfiguration.openshift.io/currentConfig` field is equal to the value of the `machineconfiguration.openshift.io/desiredConfig` field.

.. To confirm that the machine config is correct, enter the following command:
+
[source,terminal]
----
$ oc get machineconfig <config_name> -o yaml | grep ExecStart <1>
----
<1> Where `<config_name>` is the name of the machine config from the `machineconfiguration.openshift.io/currentConfig` field.
+
The machine config must include the following update to the systemd configuration:
+
[source,plain]
----
ExecStart=/usr/local/bin/mtu-migration.sh
----

ifndef::local-zone,wavelength-zone,post-aws-zones,outposts[]
. Update the underlying network interface MTU value:
+
** If you are specifying the new MTU with a NetworkManager connection configuration, enter the following command. The MachineConfig Operator automatically performs a rolling reboot of the nodes in your cluster.
+
[source,terminal]
----
$ for manifest in control-plane-interface worker-interface; do
    oc create -f $manifest.yaml
  done
----
+
** If you are specifying the new MTU with a DHCP server option or a kernel command line and PXE, make the necessary changes for your infrastructure.

. As the Machine Config Operator updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:
+
[source,terminal]
----
$ oc get machineconfigpools
----
+
A successfully updated node has the following status: `UPDATED=true`, `UPDATING=false`, `DEGRADED=false`.
+
[NOTE]
====
By default, the Machine Config Operator updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.
====

. Confirm the status of the new machine configuration on the hosts:

.. To list the machine configuration state and the name of the applied machine configuration, enter the following command:
+
[source,terminal]
----
$ oc describe node | egrep "hostname|machineconfig"
----
+
.Example output
[source,text]
----
kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done
----
+
Verify that the following statements are true:
+
* The value of `machineconfiguration.openshift.io/state` field is `Done`.
* The value of the `machineconfiguration.openshift.io/currentConfig` field is equal to the value of the `machineconfiguration.openshift.io/desiredConfig` field.

.. To confirm that the machine config is correct, enter the following command:
+
[source,terminal]
----
$ oc get machineconfig <config_name> -o yaml | grep path: <1>
----
<1> Where `<config_name>` is the name of the machine config from the `machineconfiguration.openshift.io/currentConfig` field.
+
If the machine config is successfully deployed, the previous output contains the `/etc/NetworkManager/conf.d/99-<interface>-mtu.conf` file path and the `ExecStart=/usr/local/bin/mtu-migration.sh` line.
endif::local-zone,wavelength-zone,post-aws-zones,outposts[]

. To finalize the MTU migration, enter the following command for the OVN-Kubernetes network plugin.
+
[source,terminal]
----
$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": null, "defaultNetwork":{ "ovnKubernetesConfig": { "mtu": <mtu> }}}}' <1>
----
<1> Replace `<mtu>` with the new cluster network MTU that you specified with `<overlay_to>`.

. After finalizing the MTU migration, each machine config pool node is rebooted one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:
+
[source,terminal]
----
$ oc get machineconfigpools
----
+
A successfully updated node has the following status: `UPDATED=true`, `UPDATING=false`, `DEGRADED=false`.

.Verification

ifdef::local-zone,wavelength-zone,post-aws-zones,outposts[]
* Verify that the node in your cluster uses the MTU that you specified by entering the following command:
+
[source,terminal]
----
$ oc describe network.config cluster
----
endif::local-zone,wavelength-zone,post-aws-zones,outposts[]

ifndef::local-zone,wavelength-zone,post-aws-zones,outposts[]
. To get the current MTU for the cluster network, enter the following command:
+
[source,terminal]
----
$ oc describe network.config cluster
----

. Get the current MTU for the primary network interface of a node:

.. To list the nodes in your cluster, enter the following command:
+
[source,terminal]
----
$ oc get nodes
----

.. To obtain the current MTU setting for the primary network interface on a node, enter the following command:
+
[source,terminal]
----
$ oc debug node/<node> -- chroot /host ip address show <interface> <1> <2>
----
<1> Where `<node>` specifies a node from the output from the previous step.
<2> Where `<interface>` specifies the primary network interface name for the node.
+
.Example output
[source,text]
----
ens3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8051
----
endif::local-zone,wavelength-zone,post-aws-zones,outposts[]

ifeval::["{context}" == "aws-compute-edge-tasks-local-zone"]
:!local-zone:
endif::[]
ifeval::["{context}" == "aws-compute-edge-tasks-wavelength-zone"]
:!wavelength-zone:
endif::[]
ifeval::["{context}" == "aws-compute-edge-zone-tasks"]
:!post-aws-zones:
endif::[]
ifeval::["{context}" == "installing-aws-outposts"]
:!outposts:
endif::[]
