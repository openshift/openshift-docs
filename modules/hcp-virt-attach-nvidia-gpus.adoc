// Module included in the following assemblies:
//
// * hosted_control_planes/hcp-manage/hcp-manage-virt.adoc

:_mod-docs-content-type: PROCEDURE
[id="hcp-virt-attach-nvidia-gpus_{context}"]
= Attaching NVIDIA GPU devices by using the hcp CLI

You can attach one or more NVIDIA graphics processing unit (GPU) devices to node pools by using the `hcp` command-line interface (CLI) in a hosted cluster on {VirtProductName}.

:FeatureName: Attaching NVIDIA GPU devices to node pools
include::snippets/technology-preview.adoc[]

.Prerequisites

* You have exposed the NVIDIA GPU device as a resource on the node where the GPU device resides. For more information, see link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/openshift-virtualization.html[NVIDIA GPU Operator with {VirtProductName}].

* You have exposed the NVIDIA GPU device as an link:https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#extended-resources[extended resource] on the node to assign it to node pools.

.Procedure

* You can attach the GPU device to node pools during cluster creation by running the following command:
+
[source,terminal]
----
$ hcp create cluster kubevirt \
  --name <hosted_cluster_name> \// <1>
  --node-pool-replicas <worker_node_count> \// <2>
  --pull-secret <path_to_pull_secret> \// <3>
  --memory <memory> \// <4>
  --cores <cpu> \// <5>
  --host-device-name="<gpu_device_name>,count:<value>" <6>
----
<1> Specify the name of your hosted cluster, for instance, `example`.
<2> Specify the worker count, for example, `3`.
<3> Specify the path to your pull secret, for example, `/user/name/pullsecret`.
<4> Specify a value for memory, for example, `16Gi`.
<5> Specify a value for CPU, for example, `2`.
<6> Specify the GPU device name and the count, for example, `--host-device-name="nvidia-a100,count:2"`. The `--host-device-name` argument takes the name of the GPU device from the infrastructure node and an optional count that represents the number of GPU devices you want to attach to each virtual machine (VM) in node pools. The default count is `1`. For example, if you attach 2 GPU devices to 3 node pool replicas, all 3 VMs in the node pool are attached to the 2 GPU devices.
+
[TIP]
====
You can use the `--host-device-name` argument multiple times to attach multiple devices of different types.
====
