// Module included in the following assemblies:
//
// storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc

:_content-type: PROCEDURE
[id="lvms-creating-lvms-cluster_{context}"]
= Creating a Logical Volume Manager cluster on a {sno} worker node

You can configure a {sno} worker node as a Logical Volume Manager cluster.
On the control-plane {sno} node, {lvms} detects and uses the additional worker nodes when the new nodes become active in the cluster.

[NOTE]
====
When you create a Logical Volume Manager cluster, `StorageClass` and `LVMVolumeGroup` resources work together to provide dynamic provisioning of storage.
`StorageClass` CRs define the properties of the storage that you can dynamically provision.
`LVMVolumeGroup` is a specific type of persistent volume (PV) that is backed by an LVM Volume Group.
`LVMVolumeGroup` CRs provide the back-end storage for the persistent volumes that you create.
====

Perform the following procedure to create a Logical Volume Manager cluster on a {sno} worker node.

[NOTE]
====
You also can perform the same task by using the {product-title} web console.
====

.Prerequisites

* You have installed the OpenShift CLI (`oc`).

* You have logged in as a user with `cluster-admin` privileges.

* You installed {lvms} in a {sno} cluster and have installed a worker node for use in the {sno} cluster.

.Procedure

. Create the `LVMCluster` custom resource (CR).

.. Save the following YAML in the `lvmcluster.yaml` file:
+
[source,yaml]
----
apiVersion: lvm.topolvm.io/v1alpha1
kind: LVMCluster
metadata:
  name: lvmcluster
spec:
  storage:
    deviceClasses:  <1>
      - name: vg1
        default: true <2>
        deviceSelector:
          paths:
          - /dev/disk/by-path/pci-0000:87:00.0-nvme-1
          - /dev/disk/by-path/pci-0000:88:00.0-nvme-1
        thinPoolConfig:
          name: thin-pool-1
          sizePercent: 90
          overprovisionRatio: 10
        nodeSelector: <3>
          nodeSelectorTerms:
            - matchExpressions:
                - key: app
              operator: In
              Values:
                - test1
----
<1> To create multiple device storage classes in the cluster, create a YAML array under `deviceClasses` for each required storage class.
Configure the local device paths of the disks as an array of values in the `deviceSelector` field.
When configuring multiple device classes, you must specify the device path for each device.
<2> Mandatory: The `LVMCluster` resource must contain a single default storage class. Set `default: false` for secondary device storage classes.
If you are upgrading the `LVMCluster` resource from a previous version, you must specify a single default device class.
<3> Optional: To control what worker nodes the `LVMCluster` CR is applied to, specify a set of node selector labels.
The specified labels must be present on the node in order for the `LVMCluster` to be scheduled on that node.

.. Create the `LVMCluster` CR:
+
[source,terminal]
----
$ oc create -f lvmcluster.yaml
----
+
.Example output
[source,terminal]
----
lvmcluster/lvmcluster created
----
+
The `LVMCluster` resource creates the following system-managed CRs:
+
`LVMVolumeGroup`:: Tracks individual volume groups across multiple nodes.
`LVMVolumeGroupNodeStatus`:: Tracks the status of the volume groups on a node.

.Verification

Verify that the `LVMCluster` resource has created the `StorageClass`, `LVMVolumeGroup`, and `LVMVolumeGroupNodeStatus` CRs.

[IMPORTANT]
====
`LVMVolumeGroup` and `LVMVolumeGroupNodeStatus` are managed by {lvms}. Do not edit these CRs directly.
====

. Check that the `LVMCluster` CR is in a `ready` state by running the following command:
+
[source,terminal]
----
$ oc get lvmclusters.lvm.topolvm.io -o jsonpath='{.items[*].status.deviceClassStatuses[*]}'
----
+
.Example output
[source,json]
----
{
    "name": "vg1",
    "nodeStatus": [
        {
            "devices": [
                "/dev/nvme0n1",
                "/dev/nvme1n1",
                "/dev/nvme2n1"
            ],
            "node": "kube-node",
            "status": "Ready"
        }
    ]
}
----

. Check that the storage class is created:
+
[source,terminal]
----
$ oc get storageclass
----
+
.Example output
[source,terminal]
----
NAME          PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
lvms-vg1      topolvm.io           Delete          WaitForFirstConsumer   true                   31m
----

. Check that the volume snapshot class is created:
+
[source,terminal]
----
$ oc get volumesnapshotclass
----
+
.Example output
[source,terminal]
----
NAME          DRIVER               DELETIONPOLICY   AGE
lvms-vg1      topolvm.io           Delete           24h
----

. Check that the `LVMVolumeGroup` resource is created:
+
[source,terminal]
----
$ oc get lvmvolumegroup vg1 -o yaml
----
+
.Example output
[source,yaml]
----
apiVersion: lvm.topolvm.io/v1alpha1
kind: LVMVolumeGroup
metadata:
  creationTimestamp: "2022-02-02T05:16:42Z"
  generation: 1
  name: vg1
  namespace: lvm-operator-system
  resourceVersion: "17242461"
  uid: 88e8ad7d-1544-41fb-9a8e-12b1a66ab157
spec: {}
----

. Check that the `LVMVolumeGroupNodeStatus` resource is created:
+
[source,terminal]
----
$ oc get lvmvolumegroupnodestatuses.lvm.topolvm.io kube-node -o yaml
----
+
.Example output
[source,yaml]
----
apiVersion: lvm.topolvm.io/v1alpha1
kind: LVMVolumeGroupNodeStatus
metadata:
  creationTimestamp: "2022-02-02T05:17:59Z"
  generation: 1
  name: kube-node
  namespace: lvm-operator-system
  resourceVersion: "17242882"
  uid: 292de9bb-3a9b-4ee8-946a-9b587986dafd
spec:
  nodeStatus:
    - devices:
        - /dev/nvme0n1
        - /dev/nvme1n1
        - /dev/nvme2n1
      name: vg1
      status: Ready
----
