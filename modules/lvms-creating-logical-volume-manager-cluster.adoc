// Module included in the following assemblies:
//
// storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc

:_mod-docs-content-type: PROCEDURE
[id="lvms-creating-lvms-cluster_{context}"]
= Creating a Logical Volume Manager cluster on a worker node

You can configure a worker node as a Logical Volume Manager cluster.
On the control-plane node, {lvms} detects and uses the additional worker nodes when the new nodes become active in the cluster.

[NOTE]
====
When you create a Logical Volume Manager cluster, `StorageClass` and `LVMVolumeGroup` resources work together to provide dynamic provisioning of storage.
`StorageClass` CRs define the properties of the storage that you can dynamically provision.
`LVMVolumeGroup` is a specific type of persistent volume (PV) that is backed by an LVM Volume Group.
`LVMVolumeGroup` CRs provide the back-end storage for the persistent volumes that you create.
====

Perform the following procedure to create a Logical Volume Manager cluster on a worker node.

[NOTE]
====
You also can perform the same task by using the {product-title} web console.
====

.Prerequisites

* You have installed the OpenShift CLI (`oc`).

* You have logged in as a user with `cluster-admin` privileges.

* You installed {lvms} in a cluster and have installed a worker node for use in the cluster.

.Procedure

. Create the `LVMCluster` custom resource (CR).
+
[IMPORTANT]
=====
You can only create a single instance of the `LVMCluster` custom resource (CR) on an {product-title} cluster.
=====
+
.. Save the following YAML in the `lvmcluster.yaml` file:
+
[source,yaml]
----
apiVersion: lvm.topolvm.io/v1alpha1
kind: LVMCluster
metadata:
  name: lvmcluster
spec:
  storage:
    deviceClasses:  <1>
      - name: vg1 <2>
        fstype: ext4 <3>
        default: true <4>
        deviceSelector: <5>
          paths:
          - /dev/disk/by-path/pci-0000:87:00.0-nvme-1
          - /dev/disk/by-path/pci-0000:88:00.0-nvme-1
          - /dev/md/md-var
          optionalPaths:
          - /dev/disk/by-path/pci-0000:89:00.0-nvme-1
          - /dev/disk/by-path/pci-0000:90:00.0-nvme-1
          forceWipeDevicesAndDestroyAllData: true <6>
        thinPoolConfig:
          name: thin-pool-1
          sizePercent: 90
          overprovisionRatio: 10
        nodeSelector: <7>
          nodeSelectorTerms:
            - matchExpressions:
              - key: app
                operator: In
                values:
                - test1
----
<1> To create multiple device storage classes in the cluster, create a YAML array under `deviceClasses` for each required storage class.
If you add or remove a `deviceClass`, then the update reflects in the cluster only after deleting and recreating the `TopoLVM-Node` pod.
Configure the local device paths of the disks as an array of values in the `deviceSelector` field.
When configuring multiple device classes, you must specify the device path for each device.
<2> Specify a name for the LVM volume group (VG). To recover VGs from the previous {lvms} installation, you must set this field to the name of the VG that you want to recover. 
You can only recover the VGs but not the logical volume associated with the VG.
<3> Set `fstype` to `ext4` or `xfs`. By default, it is set to `xfs` if the setting is not specified.
<4> Mandatory: The `LVMCluster` resource must contain a single default storage class. Set `default: false` for secondary device storage classes.
If you are updating the `LVMCluster` resource from a previous version, you must specify a single default device class.
<5> Optional. To control or restrict the volume group to your preferred devices, you can manually specify the local paths of the devices in the `deviceSelector` section of the `LVMCluster` YAML. The `paths` section refers to devices the `LVMCluster` adds, which means those paths must exist. The `optionalPaths` section refers to devices the `LVMCluster` might add. You must specify at least one of `paths` or `optionalPaths` when specifying the `deviceSelector` section. If you specify `paths`, it is not mandatory to specify `optionalPaths`. If you specify `optionalPaths`, it is not mandatory to specify `paths` but at least one optional path must be present on the node. If you do not specify any paths, then the `LVMCluster` adds the unused devices on the node. After a device is added to the `LVMCluster`, it cannot be removed.
<6> Optional: To force wipe the selected disks, set `forceWipeDevicesAndDestroyAllData` to `true`. This parameter is set to `false` by default.
<7> Optional: To control what worker nodes the `LVMCluster` CR is applied to, specify a set of node selector labels.
The specified labels must be present on the node in order for the `LVMCluster` to be scheduled on that node.

.. Create the `LVMCluster` CR:
+
[source,terminal]
----
$ oc create -f lvmcluster.yaml
----
+
.Example output
[source,terminal]
----
lvmcluster/lvmcluster created
----
+
The `LVMCluster` resource creates the following system-managed CRs:
+
`LVMVolumeGroup`:: Tracks individual volume groups across multiple nodes.
`LVMVolumeGroupNodeStatus`:: Tracks the status of the volume groups on a node.

To recover VGs from the previous {lvms} installation, ensure that the following conditions are met:

* VGs must not be corrupted.
* VGs must have the `lvms` tag. For more information on adding tags to LVMS objects, see link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_logical_volumes/grouping-lvm-objects-with-tags_configuring-and-managing-logical-volumes#doc-wrapper[Grouping LVM objects with tags].

[NOTE]
====
Wiping the disk can lead to inconsistencies in data integrity if any of the following conditions is met:

* The disk is being used as swap space.
* The disk is part of the Redundant Array of Independent Disks (RAID).
* The disk is mounted.

In this case, you must manually wipe the disk.
====

.Verification

Verify that the `LVMCluster` resource has created the `StorageClass`, `LVMVolumeGroup`, and `LVMVolumeGroupNodeStatus` CRs.

[IMPORTANT]
====
`LVMVolumeGroup` and `LVMVolumeGroupNodeStatus` are managed by {lvms}. Do not edit these CRs directly.
====

. Check that the `LVMCluster` CR is in a `ready` state by running the following command:
+
[source,terminal]
----
$ oc get lvmclusters.lvm.topolvm.io -o jsonpath='{.items[*].status.deviceClassStatuses[*]}'
----
+
.Example output
[source,json]
----
{
    "name": "vg1",
    "nodeStatus": [
        {
            "devices": [
                "/dev/nvme0n1",
                "/dev/nvme1n1",
                "/dev/nvme2n1"
            ],
            "node": "kube-node",
            "status": "Ready"
        }
    ]
}
----

. Check that the storage class is created:
+
[source,terminal]
----
$ oc get storageclass
----
+
.Example output
[source,terminal]
----
NAME          PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
lvms-vg1      topolvm.io           Delete          WaitForFirstConsumer   true                   31m
----

. Check that the volume snapshot class is created:
+
[source,terminal]
----
$ oc get volumesnapshotclass
----
+
.Example output
[source,terminal]
----
NAME          DRIVER               DELETIONPOLICY   AGE
lvms-vg1      topolvm.io           Delete           24h
----

. Check that the `LVMVolumeGroup` resource is created:
+
[source,terminal]
----
$ oc get lvmvolumegroup vg1 -o yaml
----
+
.Example output
[source,yaml]
----
apiVersion: lvm.topolvm.io/v1alpha1
kind: LVMVolumeGroup
metadata:
  creationTimestamp: "2022-02-02T05:16:42Z"
  generation: 1
  name: vg1
  namespace: lvm-operator-system
  resourceVersion: "17242461"
  uid: 88e8ad7d-1544-41fb-9a8e-12b1a66ab157
spec: {}
----

. Check that the `LVMVolumeGroupNodeStatus` resource is created:
+
[source,terminal]
----
$ oc get lvmvolumegroupnodestatuses.lvm.topolvm.io kube-node -o yaml
----
+
.Example output
[source,yaml]
----
apiVersion: lvm.topolvm.io/v1alpha1
kind: LVMVolumeGroupNodeStatus
metadata:
  creationTimestamp: "2022-02-02T05:17:59Z"
  generation: 1
  name: kube-node
  namespace: lvm-operator-system
  resourceVersion: "17242882"
  uid: 292de9bb-3a9b-4ee8-946a-9b587986dafd
spec:
  nodeStatus:
    - devices:
        - /dev/nvme0n1
        - /dev/nvme1n1
        - /dev/nvme2n1
      name: vg1
      status: Ready
----