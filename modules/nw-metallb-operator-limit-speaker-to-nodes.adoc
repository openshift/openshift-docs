// Module included in the following assemblies:
//
// * networking/metallb/metallb-operator-install.adoc

[id="nw-metallb-operator-limit-speaker-to-nodes_{context}"]
= Limit speaker pods to specific nodes

By default, when you start MetalLB with the MetalLB Operator, the Operator starts an instance of a `speaker` pod on each node in the cluster.
Only the nodes with a `speaker` pod can advertise a load balancer IP address.
You can configure the `MetalLB` custom resource with a node selector to specify which nodes run the `speaker` pods.

The most common reason to limit the `speaker` pods to specific nodes is to ensure that only nodes with network interfaces on specific networks advertise load balancer IP addresses.
Only the nodes with a running `speaker` pod are advertised as destinations of the load balancer IP address.

If you limit the `speaker` pods to specific nodes and specify `local` for the external traffic policy of a service, then you must ensure that the application pods for the service are deployed to the same nodes.

.Example configuration to limit speaker pods to worker nodes
[source,yaml]
----
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  nodeSelector:  <1>
    node-role.kubernetes.io/worker: ""
  speakerTolerations:   <2>
  - key: "Example"
    operator: "Exists"
    effect: "NoExecute"
----
<1> The example configuration specifies to assign the speaker pods to worker nodes, but you can specify labels that you assigned to nodes or any valid node selector.
<2> In this example configuration, the pod that this toleration is attached to tolerates any taint that matches the `key` value and `effect` value using the `operator`.

After you apply a manifest with the `spec.nodeSelector` field, you can check the number of pods that the Operator deployed with the `oc get daemonset -n metallb-system speaker` command.
Similarly, you can display the nodes that match your labels with a command like `oc get nodes -l node-role.kubernetes.io/worker=`.

You can optionally allow the node to control which speaker pods should, or should not, be scheduled on them by using affinity rules. You can also limit these pods by applying a list of tolerations. For more information about affinity rules, taints, and tolerations, see the additional resources.
