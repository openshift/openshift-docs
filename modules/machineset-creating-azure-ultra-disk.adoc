// Module included in the following assemblies:
//
// * machine_management/creating_machinesets/creating-machineset-azure.adoc
// * storage/persistent_storage/persistent-storage-azure.adoc
// * storage/persistent_storage/persistent-storage-csi-azure.adoc
// * machine_management/control_plane_machine_management/cpmso-using.adoc

ifeval::["{context}" == "creating-machineset-azure"]
:mapi:
endif::[]
ifeval::["{context}" == "cpmso-using"]
:cpmso:
endif::[]
ifeval::["{context}" == "persistent-storage-azure"]
:pvc:
endif::[]
ifeval::["{context}" == "persistent-storage-csi-azure"]
:pvc:
endif::[]

ifdef::mapi[:machine-role: worker]
ifdef::cpmso[:machine-role: master]

:_mod-docs-content-type: PROCEDURE
[id="machineset-creating-azure-ultra-disk_{context}"]
= Creating machines with ultra disks by using machine sets

You can deploy machines with ultra disks on Azure by editing your machine set YAML file.

.Prerequisites

* Have an existing Microsoft Azure cluster.

.Procedure

ifdef::mapi,cpmso[]
. Create a custom secret in the `openshift-machine-api` namespace using the `{machine-role}` data secret by running the following command:
+
[source,terminal]
----
$ oc -n openshift-machine-api \
get secret <role>-user-data \ <1>
--template='{{index .data.userData | base64decode}}' | jq > userData.txt <2>
----
<1> Replace `<role>` with `{machine-role}`.
<2> Specify `userData.txt` as the name of the new custom secret.

. In a text editor, open the `userData.txt` file and locate the final `}` character in the file.

.. On the immediately preceding line, add a `,`.

.. Create a new line after the `,` and add the following configuration details:
+
[source,json]
----
"storage": {
  "disks": [ <1>
    {
      "device": "/dev/disk/azure/scsi1/lun0", <2>
      "partitions": [ <3>
        {
          "label": "lun0p1", <4>
          "sizeMiB": 1024, <5>
          "startMiB": 0
        }
      ]
    }
  ],
  "filesystems": [ <6>
    {
      "device": "/dev/disk/by-partlabel/lun0p1",
      "format": "xfs",
      "path": "/var/lib/lun0p1"
    }
  ]
},
"systemd": {
  "units": [ <7>
    {
      "contents": "[Unit]\nBefore=local-fs.target\n[Mount]\nWhere=/var/lib/lun0p1\nWhat=/dev/disk/by-partlabel/lun0p1\nOptions=defaults,pquota\n[Install]\nWantedBy=local-fs.target\n", <8>
      "enabled": true,
      "name": "var-lib-lun0p1.mount"
    }
  ]
}
----
<1> The configuration details for the disk that you want to attach to a node as an ultra disk.
<2> Specify the `lun` value that is defined in the `dataDisks` stanza of the machine set you are using. For example, if the machine set contains `lun: 0`, specify `lun0`. You can initialize multiple data disks by specifying multiple `"disks"` entries in this configuration file. If you specify multiple `"disks"` entries, ensure that the `lun` value for each matches the value in the machine set.
<3> The configuration details for a new partition on the disk.
<4> Specify a label for the partition. You might find it helpful to use hierarchical names, such as `lun0p1` for the first partition of `lun0`.
<5> Specify the total size in MiB of the partition.
<6> Specify the filesystem to use when formatting a partition. Use the partition label to specify the partition.
<7> Specify a `systemd` unit to mount the partition at boot. Use the partition label to specify the partition. You can create multiple partitions by specifying multiple `"partitions"` entries in this configuration file. If you specify multiple `"partitions"` entries, you must specify a `systemd` unit for each.
<8> For `Where`, specify the value of `storage.filesystems.path`. For `What`, specify the value of `storage.filesystems.device`.

. Extract the disabling template value to a file called `disableTemplating.txt` by running the following command:
+
[source,terminal]
----
$ oc -n openshift-machine-api get secret <role>-user-data \ <1>
--template='{{index .data.disableTemplating | base64decode}}' | jq > disableTemplating.txt
----
<1> Replace `<role>` with `{machine-role}`.

. Combine the `userData.txt` file and `disableTemplating.txt` file to create a data secret file by running the following command:
+
[source,terminal]
----
$ oc -n openshift-machine-api create secret generic <role>-user-data-x5 \ <1>
--from-file=userData=userData.txt \
--from-file=disableTemplating=disableTemplating.txt
----
<1> For `<role>-user-data-x5`, specify the name of the secret. Replace `<role>` with `{machine-role}`.
endif::mapi,cpmso[]

ifndef::cpmso[]
. Copy an existing Azure `MachineSet` custom resource (CR) and edit it by running the following command:
+
[source,terminal]
----
$ oc edit machineset <machine-set-name>
----
+
where `<machine-set-name>` is the machine set that you want to provision machines with ultra disks.

. Add the following lines in the positions indicated:
+
[source,yaml]
----
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
spec:
  template:
    spec:
      metadata:
        labels:
          disk: ultrassd <1>
      providerSpec:
        value:
          ultraSSDCapability: Enabled <2>
ifdef::mapi[]
          dataDisks: <2>
          - nameSuffix: ultrassd
            lun: 0
            diskSizeGB: 4
            deletionPolicy: Delete
            cachingType: None
            managedDisk:
              storageAccountType: UltraSSD_LRS
          userDataSecret:
            name: <role>-user-data-x5 <3>
endif::mapi[]
----
<1> Specify a label to use to select a node that is created by this machine set. This procedure uses `disk.ultrassd` for this value.
<2> These lines enable the use of ultra disks.
ifdef::mapi[]
For `dataDisks`, include the entire stanza.
<3> Specify the user data secret created earlier. Replace `<role>` with `{machine-role}`.
endif::mapi[]

. Create a machine set using the updated configuration by running the following command:
+
[source,terminal]
----
$ oc create -f <machine-set-name>.yaml
----
endif::cpmso[]

ifdef::cpmso[]
. Edit your control plane machine set CR by running the following command:
+
[source,terminal]
----
$ oc --namespace openshift-machine-api edit controlplanemachineset.machine.openshift.io cluster
----

. Add the following lines in the positions indicated:
+
[source,yaml]
----
apiVersion: machine.openshift.io/v1beta1
kind: ControlPlaneMachineSet
spec:
  template:
    spec:
      metadata:
        labels:
          disk: ultrassd <1>
      providerSpec:
        value:
          ultraSSDCapability: Enabled <2>
          dataDisks: <2>
          - nameSuffix: ultrassd
            lun: 0
            diskSizeGB: 4
            deletionPolicy: Delete
            cachingType: None
            managedDisk:
              storageAccountType: UltraSSD_LRS
          userDataSecret:
            name: <role>-user-data-x5 <3>
----
<1> Specify a label to use to select a node that is created by this machine set. This procedure uses `disk.ultrassd` for this value.
<2> These lines enable the use of ultra disks. For `dataDisks`, include the entire stanza.
<3> Specify the user data secret created earlier. Replace `<role>` with `{machine-role}`.

. Save your changes.

** For clusters that use the default `RollingUpdate` update strategy, the Operator automatically propagates the changes to your control plane configuration.

** For clusters that are configured to use the `OnDelete` update strategy, you must replace your control plane machines manually.
endif::cpmso[]

ifdef::pvc[]
. Create a storage class that contains the following YAML definition:
+
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ultra-disk-sc <1>
parameters:
  cachingMode: None
  diskIopsReadWrite: "2000" <2>
  diskMbpsReadWrite: "320" <3>
  kind: managed
  skuname: UltraSSD_LRS
provisioner: disk.csi.azure.com <4>
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer <5>
----
<1> Specify the name of the storage class. This procedure uses `ultra-disk-sc` for this value.
<2> Specify the number of IOPS for the storage class.
<3> Specify the throughput in MBps for the storage class.
<4> For Azure Kubernetes Service (AKS) version 1.21 or later, use `disk.csi.azure.com`. For earlier versions of AKS, use `kubernetes.io/azure-disk`.
<5> Optional: Specify this parameter to wait for the creation of the pod that will use the disk.

. Create a persistent volume claim (PVC) to reference the `ultra-disk-sc` storage class that contains the following YAML definition:
+
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ultra-disk <1>
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: ultra-disk-sc <2>
  resources:
    requests:
      storage: 4Gi <3>
----
<1> Specify the name of the PVC. This procedure uses `ultra-disk` for this value.
<2> This PVC references the `ultra-disk-sc` storage class.
<3> Specify the size for the storage class. The minimum value is `4Gi`.

. Create a pod that contains the following YAML definition:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nginx-ultra
spec:
  nodeSelector:
    disk: ultrassd <1>
  containers:
  - name: nginx-ultra
    image: alpine:latest
    command:
      - "sleep"
      - "infinity"
    volumeMounts:
    - mountPath: "/mnt/azure"
      name: volume
  volumes:
    - name: volume
      persistentVolumeClaim:
        claimName: ultra-disk <2>
----
<1> Specify the label of the machine set that enables the use of ultra disks. This procedure uses `disk.ultrassd` for this value.
<2> This pod references the `ultra-disk` PVC.
endif::pvc[]

.Verification

. Validate that the machines are created by running the following command:
+
[source,terminal]
----
$ oc get machines
----
+
The machines should be in the `Running` state.

. For a machine that is running and has a node attached, validate the partition by running the following command:
+
[source,terminal]
----
$ oc debug node/<node-name> -- chroot /host lsblk
----
+
In this command, `oc debug node/<node-name>` starts a debugging shell on the node `<node-name>` and passes a command with `--`. The passed command `chroot /host` provides access to the underlying host OS binaries, and `lsblk` shows the block devices that are attached to the host OS machine.

.Next steps

ifndef::cpmso[]
* To use an ultra disk from within a pod, create a workload that uses the mount point. Create a YAML file similar to the following example:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: ssd-benchmark1
spec:
  containers:
  - name: ssd-benchmark1
    image: nginx
    ports:
      - containerPort: 80
        name: "http-server"
    volumeMounts:
    - name: lun0p1
      mountPath: "/tmp"
  volumes:
    - name: lun0p1
      hostPath:
        path: /var/lib/lun0p1
        type: DirectoryOrCreate
  nodeSelector:
    disktype: ultrassd
----
endif::cpmso[]

ifdef::cpmso[]
* To use an ultra disk on the control plane, reconfigure your workload to use the control plane's ultra disk mount point.
endif::cpmso[]

ifeval::["{context}" == "creating-machineset-azure"]
:!mapi:
endif::[]
ifeval::["{context}" == "cpmso-using"]
:!cpmso:
endif::[]
ifeval::["{context}" == "persistent-storage-azure"]
:!pvc:
endif::[]
ifeval::["{context}" == "persistent-storage-csi-azure"]
:!pvc:
endif::[]
