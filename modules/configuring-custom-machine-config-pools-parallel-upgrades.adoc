// Module included in the following assemblies:
//
// * openshift-docs/updating/preparing_for_updates/updating-cluster-prepare.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-custom-machine-config-pools-parallel-upgrades_{context}"]
= Configuring custom machine config pools for parallel upgrades

You can accelerate cluster upgrades by partitioning worker nodes into custom Machine Config Pools (MCPs) that align with your Kubernetes failure domains (KFDs). By configuring these pools to update concurrently, you can perform parallel upgrades on specific zones while maintaining high availability.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.
* You have the OpenShift CLI (`oc`) installed.
* You have identified the failure domains (zones) in your cluster topology (for example, `worker-0`, `worker-1`, `worker-2`, `worker-3`).
* You have ensured that the cluster has sufficient spare capacity to support the workload if one failure domain is unavailable.

.Procedure

. Verify that the worker nodes have been partitioned out of the default worker pool and into new, custom MCPs. The default worker pool should display as empty, with the custom MCPs each containing their two respective nodes.
+
[source,terminal]
----
$ oc get mcp -o custom-columns=NAME:.metadata.name,MACHINECOUNT:.status.machineCount
----
+
.Example output
[source,terminal]
----
NAME       MACHINECOUNT
master     3
worker     0
worker-0   2
worker-1   2
worker-2   2
worker-3   2
----

. Create a YAML file for each custom MCP that corresponds to a failure domain in your cluster, as in the following example:
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-0
  labels:
    machineconfiguration.openshift.io/role: worker-0
spec:
  machineConfigSelector:
    matchExpressions:
      - key: machineconfiguration.openshift.io/role
        operator: In
        values: [ worker, worker-0 ]
  paused: true
  maxUnavailable: 100%
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-0: ""
----
+
Ensure the following configurations are present:

* *maxUnavailable*: Set this value to `100%`. This setting allows the Machine Config Operator (MCO) to update all nodes in the single failure zone simultaneously, significantly reducing upgrade time for the zone.

* *paused*: Set this value to `true`. Pausing the pool prevents unintended updates from starting immediately. You unpause the pool only when you are ready to upgrade that specific zone.

* *nodeSelector*: Define a label to identify the nodes that belong to this pool (for example, `node-role.kubernetes.io/worker-0`).

. Apply the `topology.kubernetes.io/zone` label to identify the KFD for the Kubernetes scheduler, and the custom node role label (for example, `worker-0`) to assign the node to the MCP by running the following command:
+
[source,terminal]
----
$ oc label node <node_name> node-role.kubernetes.io/worker-0="" topology.kubernetes.io/zone=<zone_name> --overwrite
----
+
[IMPORTANT]
====
The Kubernetes label `topology.kubernetes.io/zone` is recommended for addition during cluster installation or node scaling. It can also be added after installation.
====

. Verify that the worker nodes are correctly assigned to the custom MCPs and that the pools are created successfully.
+
[source,terminal]
----
$ oc get nodes --show-labels=false -L 'topology.kubernetes.io/zone'
----
+
.Example output
----
NAME                                        STATUS   ROLES                  AGE   VERSION   ZONE
master-0.topology.kubernetes.io/zone=kfd0   Ready    control-plane,master   27h   v1.31.13
worker-0.topology.kubernetes.io/zone=kfd0   Ready    worker,worker-0        27h   v1.31.13  kfd0
worker-1.topology.kubernetes.io/zone=kfd1   Ready    worker,worker-1        27h   v1.31.13  kfd0
worker-2.topology.kubernetes.io/zone=kfd2   Ready    worker,worker-2        27h   v1.31.13  kfd2
worker-3.topology.kubernetes.io/zone=kfd3   Ready    worker,worker-3        27h   v1.31.13  kfd3
----
