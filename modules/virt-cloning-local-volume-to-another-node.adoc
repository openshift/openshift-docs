// Module included in the following assemblies:
//
// * virt/virtual_machines/virtual_disks/virt-moving-local-vm-disk-to-different-node.adoc

[id="virt-cloning-local-volume-to-another-node_{context}"]
= Cloning a local volume to another node

You can move a virtual machine disk so that it runs on a specific node by cloning the underlying persistent volume claim (PVC).

To ensure the virtual machine disk is cloned to the correct node, you must either create a new persistent volume (PV) or identify one on the correct node.
Apply a unique label to the PV so that it can be referenced by the data volume.

[NOTE]
====
The destination PV must be the same size or larger than the source PVC.
If the destination PV is smaller than the source PVC, the cloning operation fails.
====

.Prerequisites

* The virtual machine must not be running. Power down the virtual machine before cloning the virtual machine disk.

.Procedure

. Either create a new local PV on the node, or identify a local PV already on the node:

* Create a local PV that includes the `nodeAffinity.nodeSelectorTerms` parameters. The following manifest creates a `10Gi` local PV on `node01`.
+
[source,yaml]
----
kind: PersistentVolume
apiVersion: v1
metadata:
  name: <destination-pv> <1>
  annotations:
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 10Gi <2>
  local:
    path: /mnt/local-storage/local/disk1 <3>
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node01 <4>
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local
  volumeMode: Filesystem
----
<1> The name of the PV.
<2> The size of the PV. You must allocate enough space, or the cloning operation fails. The size must be the same as or larger than the source PVC.
<3> The mount path on the node.
<4> The name of the node where you want to create the PV.

* Identify a PV that already exists on the target node. You can identify the node where a PV is provisioned by viewing the `nodeAffinity` field in its configuration:
+
[source,terminal]
----
$ oc get pv <destination-pv> -o yaml
----
+
The following snippet shows that the PV is on `node01`:
+
.Example output
[source,yaml]
----
...
spec:
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname <1>
          operator: In
          values:
          - node01 <2>
...
----
<1> The `kubernetes.io/hostname` key uses the node host name to select a node.
<2> The host name of the node.

. Add a unique label to the PV:
+
[source,terminal]
----
$ oc label pv <destination-pv> node=node01
----

. Create a data volume manifest that references the following:

* The PVC name and namespace of the virtual machine.
* The label you applied to the PV in the previous step.
* The size of the destination PV.
+
[source,yaml]
----
apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: <clone-datavolume> <1>
spec:
  source:
    pvc:
      name: "<source-vm-disk>" <2>
      namespace: "<source-namespace>" <3>
  pvc:
    accessModes:
      - ReadWriteOnce
    selector:
      matchLabels:
        node: node01 <4>
    resources:
      requests:
        storage: <10Gi> <5>
----
<1> The name of the new data volume.
<2> The name of the source PVC. If you do not know the PVC name, you can find it in the virtual machine configuration: `spec.volumes.persistentVolumeClaim.claimName`.
<3> The namespace where the source PVC exists.
<4> The label that you applied to the PV in the previous step.
<5> The size of the destination PV.

. Start the cloning operation by applying the data volume manifest to your cluster:
+
[source,terminal]
----
$ oc apply -f <clone-datavolume.yaml>
----

The data volume clones the PVC of the virtual machine into the PV on the specific node.
