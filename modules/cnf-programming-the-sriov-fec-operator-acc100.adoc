// CNF-1498 Validate and Document Intel SRO and SRIOV FEC Operator
// Module included in the following assemblies:
//
// *cnf-optimize-data-performance-n3000.adoc

:_content-type: PROCEDURE
[id="cnf-programming-the-sriov-operator-acc100_{context}"]
= Configuring the SR-IOV-FEC Operator for the Intel vRAN Dedicated Accelerator ACC100

Programming the Intel vRAN Dedicated Accelerator ACC100 exposes the Single Root I/O Virtualization (SRIOV) virtual function (VF) devices that are then used to accelerate the FEC in the vRAN workload.
The Intel vRAN Dedicated Accelerator ACC100 accelerates 4G and 5G Virtualized Radio Access Networks (vRAN) workloads. This in turn increases the overall compute capacity of a commercial, off-the-shelf platform.
This device is also known as Mount Bryce.

The SR-IOV-FEC Operator handles the management of the forward error correction (FEC) devices that are used to accelerate the FEC process in vRAN L1 applications.

Configuring the SR-IOV-FEC Operator involves:

* Creating the virtual functions (VFs) for the FEC device
* Binding the VFs to the appropriate drivers
* Configuring the VF queues for desired functionality in a 4G or 5G deployment

The role of forward error correction (FEC) is to correct transmission errors, where certain bits in a message can be lost or garbled. Messages can be lost or garbled due to noise in the transmission media, interference, or low signal strength.
Without FEC, a garbled message would have to be resent, adding to the network load and impacting throughput and latency.

.Prerequisites

* Intel FPGA ACC100 5G/4GÂ card
* Node or nodes installed with the OpenNESS Operator for Wireless FEC Accelerators
* Enable global SR-IOV and VT-d settings in the BIOS for the node
* RT kernel configured with Performance Addon Operator
* Log in as a user with `cluster-admin` privileges

.Procedure

. Change to the `vran-acceleration-operators` project:
+
[source,terminal]
----
$ oc project vran-acceleration-operators
----

. Verify that the SR-IOV-FEC Operator is installed:
+
[source,terminal]
----
$ oc get csv -o custom-columns=Name:.metadata.name,Phase:.status.phase
----
+
.Example output
[source,terminal]
----
Name                                        Phase
sriov-fec.v1.1.0                            Succeeded
----

. Verify that the `sriov-fec` pods are running:
+
[source,terminal]
----
$  oc get pods
----
+
.Example output
[source,terminal]
----
NAME                                            READY       STATUS      RESTARTS    AGE
sriov-device-plugin-j5jlv                       1/1         Running     1           15d
sriov-fec-controller-manager-85b6b8f4d4-gd2qg   1/1         Running     1           15d
sriov-fec-daemonset-kqqs6                       1/1         Running     1           15d
----
* `sriov-device-plugin` expose the FEC virtual functions as resources under the node
* `sriov-fec-controller-manager` applies CR to the node and maintains the operands containers
* `sriov-fec-daemonset` is responsible for:
** Discovering the SRIOV NICs on each node.
** Syncing the status of the custom resource (CR) defined in step 6.
** Taking the spec of the CR as input and configuring the discovered NICs.

. Retrieve all the nodes containing one of the supported vRAN FEC accelerator devices:
+
[source,terminal]
----
$ oc get sriovfecnodeconfig
----
+
.Example output
[source,terminal]
----
NAME             CONFIGURED
node1            Succeeded
----

. Find the physical function (PF) of the SR-IOV FEC accelerator device to configure:
+
[source,terminal]
----
$ oc get sriovfecnodeconfig node1 -o yaml
----
+
.Example output
[source,yaml]
----
status:
    conditions:
    - lastTransitionTime: "2021-03-19T17:19:37Z"
      message: Configured successfully
      observedGeneration: 1
      reason: ConfigurationSucceeded
      status: "True"
      type: Configured
    inventory:
       sriovAccelerators:
       - deviceID: 0d5c
         driver: ""
         maxVirtualFunctions: 16
         pciAddress: 0000:af:00.0 <1>
         vendorID: "8086"
         virtualFunctions: [] <2>
----
<1> This field indicates the PCI address of the card.
<2> This field shows that the virtual functions are empty.

. Configure the number of virtual functions and queue groups on the FEC device:

.. Create the following custom resource (CR) and save the YAML in the `sriovfec_acc100cr.yaml` file:
+
[NOTE]
====
This example configures the ACC100 8/8 queue groups for 5G, 4 queue groups for Uplink, and another 4 queue groups for Downlink.
====
+
[source,yaml]
----
apiVersion: sriovfec.intel.com/v1
kind: SriovFecClusterConfig
metadata:
  name: config <1>
spec:
  nodes:
   - nodeName: node1 <2>
     physicalFunctions:
       - pciAddress: 0000:af:00.0 <3>
         pfDriver: "pci-pf-stub"
         vfDriver: "vfio-pci"
         vfAmount: 16 <4>
         bbDevConfig:
           acc100:
             # Programming mode: 0 = VF Programming, 1 = PF Programming
             pfMode: false
             numVfBundles: 16
             maxQueueSize: 1024
             uplink4G:
               numQueueGroups: 0
               numAqsPerGroups: 16
               aqDepthLog2: 4
             downlink4G:
              numQueueGroups: 0
              numAqsPerGroups: 16
              aqDepthLog2: 4
             uplink5G:
              numQueueGroups: 4
              numAqsPerGroups: 16
              aqDepthLog2: 4
             downlink5G:
              numQueueGroups: 4
              numAqsPerGroups: 16
              aqDepthLog2: 4
----
<1> Specify a name for the CR object. The only name that can be specified is `config`.
<2> Specify the node name.
<3> Specify the PCI address of the card on which the SR-IOV-FEC Operator will be installed.
<4> Specify the number of virtual functions to create. For the Intel vRAN Dedicated Accelerator ACC100, create all 16 VFs.
+
[NOTE]
====
The card is configured to provide up to 8 queue groups with up to 16 queues per group. The queue groups can be divided between groups allocated to 5G and 4G and Uplink and Downlink.
The Intel vRAN Dedicated Accelerator ACC100 can be configured for:

* 4G or 5G only
* 4G and 5G at the same time

Each configured VF has access to all the queues. Each of the queue groups have a distinct priority level. The request for a given queue group is made from the application level that is, the vRAN application leveraging the FEC device.
====

.. Apply the CR:
+
[source,terminal]
----
$ oc apply -f sriovfec_acc100cr.yaml
----
+
After applying the CR, the SR-IOV FEC daemon starts configuring the FEC device.

.Verification
. Check the status:
+
[source,terminal]
----
$ oc get sriovfecclusterconfig config -o yaml
----
+
.Example output
[source,yaml]
----
status:
    conditions:
    - lastTransitionTime: "2021-03-19T11:46:22Z"
      message: Configured successfully
      observedGeneration: 1
      reason: Succeeded
      status: "True"
      type: Configured
    inventory:
      sriovAccelerators:
      - deviceID: 0d5c
        driver: pci-pf-stub
        maxVirtualFunctions: 16
        pciAddress: 0000:af:00.0
        vendorID: "8086"
        virtualFunctions:
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.0
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.1
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.2
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.3
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.4
----

. Check the logs:

.. Determine the pod name of the SR-IOV daemon:
+
[source,terminal]
----
$ oc get po -o wide | grep sriov-fec-daemonset | grep node1
----
+
.Example output

[source,terminal]
----
sriov-fec-daemonset-kqqs6                      1/1     Running   0          19h
----
.. View the logs:
+
[source,terminal]
----
$ oc logs sriov-fec-daemonset-kqqs6
----
+
.Example output

[source,terminal]
----
{"level":"Level(-2)","ts":1616794345.4786215,"logger":"daemon.drainhelper.cordonAndDrain()","msg":"node drained"}
{"level":"Level(-4)","ts":1616794345.4786265,"logger":"daemon.drainhelper.Run()","msg":"worker function - start"}
{"level":"Level(-4)","ts":1616794345.5762916,"logger":"daemon.NodeConfigurator.applyConfig","msg":"current node status","inventory":{"sriovAccelerat
ors":[{"vendorID":"8086","deviceID":"0b32","pciAddress":"0000:20:00.0","driver":"","maxVirtualFunctions":1,"virtualFunctions":[]},{"vendorID":"8086"
,"deviceID":"0d5c","pciAddress":"0000:af:00.0","driver":"","maxVirtualFunctions":16,"virtualFunctions":[]}]}}
{"level":"Level(-4)","ts":1616794345.5763638,"logger":"daemon.NodeConfigurator.applyConfig","msg":"configuring PF","requestedConfig":{"pciAddress":"
0000:af:00.0","pfDriver":"pci-pf-stub","vfDriver":"vfio-pci","vfAmount":2,"bbDevConfig":{"acc100":{"pfMode":false,"numVfBundles":16,"maxQueueSize":1
024,"uplink4G":{"numQueueGroups":4,"numAqsPerGroups":16,"aqDepthLog2":4},"downlink4G":{"numQueueGroups":4,"numAqsPerGroups":16,"aqDepthLog2":4},"uplink5G":{"numQueueGroups":0,"numAqsPerGroups":16,"aqDepthLog2":4},"downlink5G":{"numQueueGroups":0,"numAqsPerGroups":16,"aqDepthLog2":4}}}}}
{"level":"Level(-4)","ts":1616794345.5774765,"logger":"daemon.NodeConfigurator.loadModule","msg":"executing command","cmd":"/usr/sbin/chroot /host/ modprobe pci-pf-stub"}
{"level":"Level(-4)","ts":1616794345.5842702,"logger":"daemon.NodeConfigurator.loadModule","msg":"commands output","output":""}
{"level":"Level(-4)","ts":1616794345.5843055,"logger":"daemon.NodeConfigurator.loadModule","msg":"executing command","cmd":"/usr/sbin/chroot /host/ modprobe vfio-pci"}
{"level":"Level(-4)","ts":1616794345.6090655,"logger":"daemon.NodeConfigurator.loadModule","msg":"commands output","output":""}
{"level":"Level(-2)","ts":1616794345.6091156,"logger":"daemon.NodeConfigurator","msg":"device's driver_override path","path":"/sys/bus/pci/devices/0000:af:00.0/driver_override"}
{"level":"Level(-2)","ts":1616794345.6091807,"logger":"daemon.NodeConfigurator","msg":"driver bind path","path":"/sys/bus/pci/drivers/pci-pf-stub/bind"}
{"level":"Level(-2)","ts":1616794345.7488534,"logger":"daemon.NodeConfigurator","msg":"device's driver_override path","path":"/sys/bus/pci/devices/0000:b0:00.0/driver_override"}
{"level":"Level(-2)","ts":1616794345.748938,"logger":"daemon.NodeConfigurator","msg":"driver bind path","path":"/sys/bus/pci/drivers/vfio-pci/bind"}
{"level":"Level(-2)","ts":1616794345.7492096,"logger":"daemon.NodeConfigurator","msg":"device's driver_override path","path":"/sys/bus/pci/devices/0000:b0:00.1/driver_override"}
{"level":"Level(-2)","ts":1616794345.7492566,"logger":"daemon.NodeConfigurator","msg":"driver bind path","path":"/sys/bus/pci/drivers/vfio-pci/bind"}
{"level":"Level(-4)","ts":1616794345.74968,"logger":"daemon.NodeConfigurator.applyConfig","msg":"executing command","cmd":"/sriov_workdir/pf_bb_config ACC100 -c /sriov_artifacts/0000:af:00.0.ini -p 0000:af:00.0"}
{"level":"Level(-4)","ts":1616794346.5203931,"logger":"daemon.NodeConfigurator.applyConfig","msg":"commands output","output":"Queue Groups: 0 5GUL, 0 5GDL, 4 4GUL, 4 4GDL\nNumber of 5GUL engines 8\nConfiguration in VF mode\nPF ACC100 configuration complete\nACC100 PF [0000:af:00.0] configuration complete!\n\n"}
{"level":"Level(-4)","ts":1616794346.520459,"logger":"daemon.NodeConfigurator.enableMasterBus","msg":"executing command","cmd":"/usr/sbin/chroot /host/ setpci -v -s 0000:af:00.0 COMMAND"}
{"level":"Level(-4)","ts":1616794346.5458736,"logger":"daemon.NodeConfigurator.enableMasterBus","msg":"commands output","output":"0000:af:00.0 @04 = 0142\n"}
{"level":"Level(-4)","ts":1616794346.5459251,"logger":"daemon.NodeConfigurator.enableMasterBus","msg":"executing command","cmd":"/usr/sbin/chroot /host/ setpci -v -s 0000:af:00.0 COMMAND=0146"}
{"level":"Level(-4)","ts":1616794346.5795262,"logger":"daemon.NodeConfigurator.enableMasterBus","msg":"commands output","output":"0000:af:00.0 @04 0146\n"}
{"level":"Level(-2)","ts":1616794346.5795407,"logger":"daemon.NodeConfigurator.enableMasterBus","msg":"MasterBus set","pci":"0000:af:00.0","output":"0000:af:00.0 @04 0146\n"}
{"level":"Level(-4)","ts":1616794346.6867144,"logger":"daemon.drainhelper.Run()","msg":"worker function - end","performUncordon":true}
{"level":"Level(-4)","ts":1616794346.6867719,"logger":"daemon.drainhelper.Run()","msg":"uncordoning node"}
{"level":"Level(-4)","ts":1616794346.6896322,"logger":"daemon.drainhelper.uncordon()","msg":"starting uncordon attempts"}
{"level":"Level(-2)","ts":1616794346.69735,"logger":"daemon.drainhelper.uncordon()","msg":"node uncordoned"}
{"level":"Level(-4)","ts":1616794346.6973662,"logger":"daemon.drainhelper.Run()","msg":"cancelling the context to finish the leadership"}
{"level":"Level(-4)","ts":1616794346.7029872,"logger":"daemon.drainhelper.Run()","msg":"stopped leading"}
{"level":"Level(-4)","ts":1616794346.7030034,"logger":"daemon.drainhelper","msg":"releasing the lock (bug mitigation)"}
{"level":"Level(-4)","ts":1616794346.8040674,"logger":"daemon.updateInventory","msg":"obtained inventory","inv":{"sriovAccelerators":[{"vendorID":"8086","deviceID":"0b32","pciAddress":"0000:20:00.0","driver":"","maxVirtualFunctions":1,"virtualFunctions":[]},{"vendorID":"8086","deviceID":"0d5c","pciAddress":"0000:af:00.0","driver":"pci-pf-stub","maxVirtualFunctions":16,"virtualFunctions":[{"pciAddress":"0000:b0:00.0","driver":"vfio-pci","deviceID":"0d5d"},{"pciAddress":"0000:b0:00.1","driver":"vfio-pci","deviceID":"0d5d"}]}]}}
{"level":"Level(-4)","ts":1616794346.9058325,"logger":"daemon","msg":"Update ignored, generation unchanged"}
{"level":"Level(-2)","ts":1616794346.9065044,"logger":"daemon.Reconcile","msg":"Reconciled","namespace":"vran-acceleration-operators","name":"pg-itengdvs02r.altera.com"}
----

. Check the FEC configuration of the card:

+
[source,terminal]
----
$ oc get sriovfecnodeconfig node1 -o yaml
----
+
.Example output
[source,yaml]
----
status:
    conditions:
    - lastTransitionTime: "2021-03-19T11:46:22Z"
      message: Configured successfully
      observedGeneration: 1
      reason: Succeeded
      status: "True"
      type: Configured
    inventory:
      sriovAccelerators:
      - deviceID: 0d5c <1>
        driver: pci-pf-stub
        maxVirtualFunctions: 16
        pciAddress: 0000:af:00.0
        vendorID: "8086"
        virtualFunctions:
        - deviceID: 0d5d <2>
          driver: vfio-pci
          pciAddress: 0000:b0:00.0
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.1
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.2
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.3
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.4
----
<1> The value `0d5c` is the `deviceID` physical function of the FEC device.
<2> The value `0d5d` is the `deviceID` virtual function of the FEC device.
