// CNF-1498 Validate and Document Intel SRO and SRIOV FEC Operator
// Module included in the following assemblies:
//
// *cnf-optimize-data-performance-n3000.adoc

[id="configuring-the-sr-iov-fec-operator-intel-vran-dedicated-accelerator-acc100_{context}"]
= Configuring the SR-IOV FEC Operator for the Intel® vRAN Dedicated Accelerator ACC100

This section describes how to program the Intel® vRAN Dedicated Accelerator ACC100.
The Intel® vRAN Dedicated Accelerator ACC100 accelerates 4G and 5G Virtualized Radio Access Networks (vRAN) workloads increasing the overall compute capacity of a commercial, off-the-shelf platform.
This device is also known as Mount Bryce.

The OpenNESS SR-IOV Operator for Wireless FEC Accelerators handles the management of the Forward Error Correction (FEC) devices used to accelerate the FEC process in vRAN L1 applications.

Configuring the SR-IOV FEC Operator involves:

* Creating the desired virtual functions (VFs) for the FEC device
* Binding the VFs to the appropriate drivers
* Configuring the VF queues for desired functionality in a 4G or 5G deployment
+
[NOTE]
====
The role of Forward Error Correction (FEC) is to correct transmission errors, where certain bits in a message can be lost or garbled. Messages can be lost or garbled due to noise in the transmission media, interference, or low signal strength.
Without FEC, a garbled message would have to be resent, adding to the network load and impacting both throughput and latency.
====

.Prerequisites

* Intel® FPGA ACC100 5G/4G card
* Node or nodes installed with the OpenNESS SR-IOV Operator for Wireless FEC Accelerators
* RT kernel configured with the Performance Addon Operator
* Log in as a user with `cluster-admin` privileges
+
[NOTE]
====
All the commands run in the `vran-acceleration-operators` namespace. There is no requirement to specify the namespace as part of the `oc` commands if the user runs the `oc project vran-acceleration-operators` as described in the section Installing the SR-IOV FEC Operator.
====

.Procedure

. Verify that the SR-IOV FEC Operator is installed and that pods are running:
+
[source,terminal]
----
$ oc get csv -n sriov-fec -o custom-columns=Name:.metadata.name,Phase:.status.phase
----
+
.Example output
[source,terminal]
----
Name                                        Phase
SR-IOV FEC.v1.1.0                            Succeeded
----

. Verify that the pods are running:
+
[source,terminal]
----
$  oc get pods
----
+
.Example output
[source,terminal]
----
NAME                                            READY       STATUS      RESTARTS    AGE
sriov-device-plugin-j5jlv                       1/1         Running     1           15d
sriov-fec-controller-manager-85b6b8f4d4-gd2qg   1/1         Running     1           15d
sriov-fec-daemonset-kqqs6                       1/1         Running     1           15d
----
* `sriov-device-plugin` expose the FEC virtual functions as resources under the node
* `sriov-fec-controller-manager` applies CR to the node and maintains the operands containers
* `sriov-fec-daemonset` is the main worker application
+
[NOTE]
====
This output shows the `sriov-fec` pods.
====

. Retrieve all the nodes containing one of the supported vRAN FEC accelerator devices:
+
[source,terminal]
----
$ oc get sriovfecnodeconfig
----
+
.Example output
[source,terminal]
----
NAME             CONFIGURED
node1            Succeeded
----

. Find the physical function (PF) of the SR-IOV FEC accelerator device to be configured:

+
[source,terminal]
----
$ oc get sriovfecnodeconfig node1 -o yaml
----
+
.Example output
[source,yaml]
----
status:
    conditions:
    - lastTransitionTime: "2021-03-19T17:19:37Z"
      message: Configured successfully
      observedGeneration: 1
      reason: ConfigurationSucceeded
      status: "True"
      type: Configured
    inventory:
       sriovAccelerators:
       - deviceID: 0d5c
         driver: ""
         maxVirtualFunctions: 16
         pciAddress: 0000:af:00.0 <1>
         vendorID: "8086"
         virtualFunctions: [] <2>
----
<1> This field indicates the PCI address of the card.
<2> This field shows that the virtual functions are empty.

.  Configure the FEC device with the desired setting.

.. Create the following custom resource (CR) and save the YAML in the `sriovfec_acc100cr.yaml` file.
+
[NOTE]
====
This example configures the ACC100 8/8 queue groups for 5G, 4 queue groups for Uplink, and another 4 queue groups for Downlink.
====
+
[source,yaml]
----
apiVersion: sriovfec.intel.com/v1
kind: SriovFecClusterConfig
metadata:
  name: config <1>
spec:
  nodes:
   - nodeName: node1 <2>
     physicalFunctions:
       - pciAddress: 0000:af:00.0 <3>
         pfDriver: "pci-pf-stub"
         vfDriver: "vfio-pci"
         vfAmount: 16 <4>
         bbDevConfig:
           acc100:
             # Programming mode: 0 = VF Programming, 1 = PF Programming
             pfMode: false
             numVfBundles: 16
             maxQueueSize: 1024
             uplink4G:
               numQueueGroups: 0
               numAqsPerGroups: 16
               aqDepthLog2: 4
             downlink4G:
              numQueueGroups: 0
              numAqsPerGroups: 16
              aqDepthLog2: 4
             uplink5G:
              numQueueGroups: 4
              numAqsPerGroups: 16
              aqDepthLog2: 4
             downlink5G:
              numQueueGroups: 4
              numAqsPerGroups: 16
              aqDepthLog2: 4
----
<1> Specify a name for the CR object. The only name that can be specified is `config`.
<2> Specify the node name.
<3> Specify the PCI address of the card on which the SR-IOV FEC Operator will be installed.
<3> Specify the number of virtual functions to create. For the Intel® vRAN Dedicated Accelerator ACC100, create all 16 VFs.
+
[NOTE]
====
The card is configured to provide up to 8 queue groups with up to 16 queues per group. The queue groups can be divided between groups allocated to 5G and 4G and Uplink and Downlink.
The Intel® vRAN Dedicated Accelerator ACC100 can be configured for:

* 4G or 5G only
* 4G and 5G at the same time

Each configured VF has access to all the queues. Each of the queue groups have a distinct priority level. The request for a given queue group is made from the application level that is, the vRAN application leveraging the FEC device.
====

.. Apply the CR.
+
[source,terminal]
----
$ oc apply -f sriovfec_acc100cr.yaml
----
+
After creation of the CR, the SR-IOV FEC daemon starts configuring the FEC device.

.. Check the status:
+
[source,terminal]
----
$ oc get sriovfecclusterconfig config -o yaml
----
+
.Example output
[source,yaml]
----
status:
    conditions:
    - lastTransitionTime: "2021-03-19T11:46:22Z"
      message: Configured successfully
      observedGeneration: 1
      reason: Succeeded
      status: "True"
      type: Configured
    inventory:
      sriovAccelerators:
      - deviceID: 0d5c
        driver: pci-pf-stub
        maxVirtualFunctions: 16
        pciAddress: 0000:af:00.0
        vendorID: "8086"
        virtualFunctions:
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.0
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.1
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.2
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.3
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.4
----

. Check the logs:

.. Determine the pod name of the SR-IOV daemon:
+
[source,terminal]
----
$ oc get po -o wide | grep sriov-fec-daemonset | grep node01
----
+
.Example output

[source,terminal]
----
sriov-fec-daemonset-kqqs6                      1/1     Running   0          19h
----
.. View the logs:
+
[source,terminal]
----
$ oc logs sriov-fec-daemonset-kqqs6
----
+
.Example output

[source,terminal]
----
{"level":"Level(-2)","ts":1616794345.4786215,"logger":"daemon.drainhelper.cordonAndDrain()","msg":"node drained"}
{"level":"Level(-4)","ts":1616794345.4786265,"logger":"daemon.drainhelper.Run()","msg":"worker function - start"}
{"level":"Level(-4)","ts":1616794345.5762916,"logger":"daemon.NodeConfigurator.applyConfig","msg":"current node status","inventory":{"sriovAccelerat
ors":[{"vendorID":"8086","deviceID":"0b32","pciAddress":"0000:20:00.0","driver":"","maxVirtualFunctions":1,"virtualFunctions":[]},{"vendorID":"8086"
,"deviceID":"0d5c","pciAddress":"0000:af:00.0","driver":"","maxVirtualFunctions":16,"virtualFunctions":[]}]}}
{"level":"Level(-4)","ts":1616794345.5763638,"logger":"daemon.NodeConfigurator.applyConfig","msg":"configuring PF","requestedConfig":{"pciAddress":"
0000:af:00.0","pfDriver":"pci-pf-stub","vfDriver":"vfio-pci","vfAmount":2,"bbDevConfig":{"acc100":{"pfMode":false,"numVfBundles":16,"maxQueueSize":1
024,"uplink4G":{"numQueueGroups":4,"numAqsPerGroups":16,"aqDepthLog2":4},"downlink4G":{"numQueueGroups":4,"numAqsPerGroups":16,"aqDepthLog2":4},"uplink5G":{"numQueueGroups":0,"numAqsPerGroups":16,"aqDepthLog2":4},"downlink5G":{"numQueueGroups":0,"numAqsPerGroups":16,"aqDepthLog2":4}}}}}
{"level":"Level(-4)","ts":1616794345.5774765,"logger":"daemon.NodeConfigurator.loadModule","msg":"executing command","cmd":"/usr/sbin/chroot /host/ modprobe pci-pf-stub"}
{"level":"Level(-4)","ts":1616794345.5842702,"logger":"daemon.NodeConfigurator.loadModule","msg":"commands output","output":""}
{"level":"Level(-4)","ts":1616794345.5843055,"logger":"daemon.NodeConfigurator.loadModule","msg":"executing command","cmd":"/usr/sbin/chroot /host/ modprobe vfio-pci"}
{"level":"Level(-4)","ts":1616794345.6090655,"logger":"daemon.NodeConfigurator.loadModule","msg":"commands output","output":""}
{"level":"Level(-2)","ts":1616794345.6091156,"logger":"daemon.NodeConfigurator","msg":"device's driver_override path","path":"/sys/bus/pci/devices/0000:af:00.0/driver_override"}
{"level":"Level(-2)","ts":1616794345.6091807,"logger":"daemon.NodeConfigurator","msg":"driver bind path","path":"/sys/bus/pci/drivers/pci-pf-stub/bind"}
{"level":"Level(-2)","ts":1616794345.7488534,"logger":"daemon.NodeConfigurator","msg":"device's driver_override path","path":"/sys/bus/pci/devices/0000:b0:00.0/driver_override"}
{"level":"Level(-2)","ts":1616794345.748938,"logger":"daemon.NodeConfigurator","msg":"driver bind path","path":"/sys/bus/pci/drivers/vfio-pci/bind"}
{"level":"Level(-2)","ts":1616794345.7492096,"logger":"daemon.NodeConfigurator","msg":"device's driver_override path","path":"/sys/bus/pci/devices/0000:b0:00.1/driver_override"}
{"level":"Level(-2)","ts":1616794345.7492566,"logger":"daemon.NodeConfigurator","msg":"driver bind path","path":"/sys/bus/pci/drivers/vfio-pci/bind"}
{"level":"Level(-4)","ts":1616794345.74968,"logger":"daemon.NodeConfigurator.applyConfig","msg":"executing command","cmd":"/sriov_workdir/pf_bb_config ACC100 -c /sriov_artifacts/0000:af:00.0.ini -p 0000:af:00.0"}
{"level":"Level(-4)","ts":1616794346.5203931,"logger":"daemon.NodeConfigurator.applyConfig","msg":"commands output","output":"Queue Groups: 0 5GUL, 0 5GDL, 4 4GUL, 4 4GDL\nNumber of 5GUL engines 8\nConfiguration in VF mode\nPF ACC100 configuration complete\nACC100 PF [0000:af:00.0] configuration complete!\n\n"}
{"level":"Level(-4)","ts":1616794346.520459,"logger":"daemon.NodeConfigurator.enableMasterBus","msg":"executing command","cmd":"/usr/sbin/chroot /host/ setpci -v -s 0000:af:00.0 COMMAND"}
{"level":"Level(-4)","ts":1616794346.5458736,"logger":"daemon.NodeConfigurator.enableMasterBus","msg":"commands output","output":"0000:af:00.0 @04 = 0142\n"}
{"level":"Level(-4)","ts":1616794346.5459251,"logger":"daemon.NodeConfigurator.enableMasterBus","msg":"executing command","cmd":"/usr/sbin/chroot /host/ setpci -v -s 0000:af:00.0 COMMAND=0146"}
{"level":"Level(-4)","ts":1616794346.5795262,"logger":"daemon.NodeConfigurator.enableMasterBus","msg":"commands output","output":"0000:af:00.0 @04 0146\n"}
{"level":"Level(-2)","ts":1616794346.5795407,"logger":"daemon.NodeConfigurator.enableMasterBus","msg":"MasterBus set","pci":"0000:af:00.0","output":"0000:af:00.0 @04 0146\n"}
{"level":"Level(-4)","ts":1616794346.6867144,"logger":"daemon.drainhelper.Run()","msg":"worker function - end","performUncordon":true}
{"level":"Level(-4)","ts":1616794346.6867719,"logger":"daemon.drainhelper.Run()","msg":"uncordoning node"}
{"level":"Level(-4)","ts":1616794346.6896322,"logger":"daemon.drainhelper.uncordon()","msg":"starting uncordon attempts"}
{"level":"Level(-2)","ts":1616794346.69735,"logger":"daemon.drainhelper.uncordon()","msg":"node uncordoned"}
{"level":"Level(-4)","ts":1616794346.6973662,"logger":"daemon.drainhelper.Run()","msg":"cancelling the context to finish the leadership"}
{"level":"Level(-4)","ts":1616794346.7029872,"logger":"daemon.drainhelper.Run()","msg":"stopped leading"}
{"level":"Level(-4)","ts":1616794346.7030034,"logger":"daemon.drainhelper","msg":"releasing the lock (bug mitigation)"}
{"level":"Level(-4)","ts":1616794346.8040674,"logger":"daemon.updateInventory","msg":"obtained inventory","inv":{"sriovAccelerators":[{"vendorID":"8086","deviceID":"0b32","pciAddress":"0000:20:00.0","driver":"","maxVirtualFunctions":1,"virtualFunctions":[]},{"vendorID":"8086","deviceID":"0d5c","pciAddress":"0000:af:00.0","driver":"pci-pf-stub","maxVirtualFunctions":16,"virtualFunctions":[{"pciAddress":"0000:b0:00.0","driver":"vfio-pci","deviceID":"0d5d"},{"pciAddress":"0000:b0:00.1","driver":"vfio-pci","deviceID":"0d5d"}]}]}}
{"level":"Level(-4)","ts":1616794346.9058325,"logger":"daemon","msg":"Update ignored, generation unchanged"}
{"level":"Level(-2)","ts":1616794346.9065044,"logger":"daemon.Reconcile","msg":"Reconciled","namespace":"vran-acceleration-operators","name":"pg-itengdvs02r.altera.com"}
----

. Check the FEC configuration of the card:

+
[source,terminal]
----
$ oc get sriovfecnodeconfig node1 -o yaml
----
+
.Example output
[source,yaml]
----
status:
    conditions:
    - lastTransitionTime: "2021-03-19T11:46:22Z"
      message: Configured successfully
      observedGeneration: 1
      reason: Succeeded
      status: "True"
      type: Configured
    inventory:
      sriovAccelerators:
      - deviceID: 0d5c <1>
        driver: pci-pf-stub
        maxVirtualFunctions: 16
        pciAddress: 0000:af:00.0
        vendorID: "8086"
        virtualFunctions:
        - deviceID: 0d5d <2>
          driver: vfio-pci
          pciAddress: 0000:b0:00.0
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.1
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.2
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.3
        - deviceID: 0d5d
          driver: vfio-pci
          pciAddress: 0000:b0:00.4
----
<1> The value `0d5c` is the `deviceID` physical function of the FEC device.
<2> The value `0d5d` is the `deviceID` virtual function of the FEC device.
