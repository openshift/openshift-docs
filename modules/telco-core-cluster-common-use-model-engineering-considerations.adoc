// Module included in the following assemblies:
//
// * scalability_and_performance/telco_core_ref_design_specs/telco-core-rds.adoc

:_mod-docs-content-type: REFERENCE
[id="telco-core-cluster-common-use-model-engineering-considerations_{context}"]
= Telco core cluster common use model engineering considerations

* Cluster workloads are detailed in "Application workloads".
* Worker nodes should run on either of the following CPUs:
** Intel 3rd Generation Xeon (IceLake) CPUs or better when supported by {product-title}, or CPUs with the silicon security bug (Spectre and similar) mitigations turned off.
Skylake and older CPUs can experience 40% transaction performance drops when Spectre and similar mitigations are enabled.
** AMD EPYC Zen 4 CPUs (Genoa, Bergamo) or AMD EPYC Zen 5 CPUs (Turin) when supported by {product-title}.
** Intel Sierra Forest CPUs when supported by the {product-title}.
** IRQ balancing is enabled on worker nodes.
The `PerformanceProfile` CR sets the `globallyDisableIrqLoadBalancing` parameter to a value of `false`.
Guaranteed QoS pods are annotated to ensure isolation as described in "CPU partitioning and performance tuning".

* All cluster nodes should have the following features:
** Have Hyper-Threading enabled
** Have x86_64 CPU architecture
** Have the stock (non-realtime) kernel enabled
** Are not configured for workload partitioning

* The balance between power management and maximum performance varies between machine config pools in the cluster.
The following configurations should be consistent for all nodes in a machine config pools group.

** Cluster scaling.
See "Scalability" for more information.
** Clusters should be able to scale to at least 120 nodes.

* CPU partitioning is configured using a `PerformanceProfile` CR and is applied to nodes on a per `MachineConfigPool` basis.
See "CPU partitioning and performance tuning" for additional considerations.
* CPU requirements for {product-title} depend on the configured feature set and application workload characteristics.
For a cluster configured according to the reference configuration running a simulated workload of 3000 pods as created by the kube-burner node-density test, the following CPU requirements are validated:
** The minimum number of reserved CPUs for control plane and worker nodes is 2 CPUs (4 hyper-threads) per NUMA node.
** The NICs used for non-DPDK network traffic should be configured to use at most 32 RX/TX queues.
** Nodes with large numbers of pods or other resources might require additional reserved CPUs.
The remaining CPUs are available for user workloads.
+
[NOTE]
====
Variations in {product-title} configuration, workload size, and workload characteristics require additional analysis to determine the effect on the number of required CPUs for the OpenShift platform.
====

