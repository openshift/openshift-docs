// Module included in the following assemblies:
//
// * scalability_and_performance/telco_core_ref_design_specs/telco-core-rds.adoc

:_mod-docs-content-type: REFERENCE
[id="telco-core-cluster-common-use-model-engineering-considerations_{context}"]
= Telco core cluster common use model engineering considerations

* Cluster workloads are detailed in "Application workloads".
* Worker nodes should run on either of the following CPUs:
** Intel 3rd Generation Xeon (IceLake) CPUs or better when supported by {product-title}, or CPUs with the silicon security bug (Spectre and similar) mitigations turned off.
Skylake and older CPUs can experience 40% transaction performance drops when Spectre and similar mitigations are enabled.
** AMD EPYC Zen 4 CPUs (Genoa, Bergamo, or newer) or better when supported by {product-title}.
+
[NOTE]
====
Currently, per-pod power management is not available for AMD CPUs.
====
** IRQ balancing is enabled on worker nodes.
The `PerformanceProfile` CR sets `globallyDisableIrqLoadBalancing` to false.
Guaranteed QoS pods are annotated to ensure isolation as described in "CPU partitioning and performance tuning".

* All cluster nodes should have the following features:
** Have Hyper-Threading enabled
** Have x86_64 CPU architecture
** Have the stock (non-realtime) kernel enabled
** Are not configured for workload partitioning

* The balance between power management and maximum performance varies between machine config pools in the cluster.
The following configurations should be consistent for all nodes in a machine config pools group.
** Cluster scaling.
See "Scalability" for more information.
** Clusters should be able to scale to at least 120 nodes.

* CPU partitioning is configured using a `PerformanceProfile` CR and is applied to nodes on a per `MachineConfigPool` basis.
See "CPU partitioning and performance tuning" for additional considerations.
* CPU requirements for {product-title} depend on the configured feature set and application workload characteristics.
For a cluster configured according to the reference configuration running a simulated workload of 3000 pods as created by the kube-burner node-density test, the following CPU requirements are validated:
** The minimum number of reserved CPUs for control plane and worker nodes is 2 CPUs (4 hyper-threads) per NUMA node.
** The NICs used for non-DPDK network traffic should be configured to use at least 16 RX/TX queues.
** Nodes with large numbers of pods or other resources might require additional reserved CPUs.
The remaining CPUs are available for user workloads.

+
[NOTE]
====
Variations in {product-title} configuration, workload size, and workload characteristics require additional analysis to determine the effect on the number of required CPUs for the OpenShift platform.
====
