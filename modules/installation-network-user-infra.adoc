// Module included in the following assemblies:
//
// * installing/installing_bare_metal/installing-bare-metal.adoc
// * installing/installing_bare_metal/installing-bare-metal-network-customizations.adoc
// * installing/installing_bare_metal/installing-restricted-networks-bare-metal.adoc
// * installing/installing_platform_agnostic/installing-platform-agnostic.adoc
// * installing/installing_vmc/installing-restricted-networks-vmc-user-infra.adoc
// * installing/installing_vmc/installing-vmc-user-infra.adoc
// * installing/installing_vmc/installing-vmc-network-customizations-user-infra.adoc
// * installing/installing_vsphere/installing-restricted-networks-vsphere.adoc
// * installing/installing_vsphere/installing-vsphere.adoc
// * installing/installing_vsphere/installing-vsphere-network-customizations.adoc
// * installing/installing_ibm_z/installing-ibm-z.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc
// * installing/installing_ibm_z/installing-ibm-z-kvm.adoc
// * installing/installing_ibm_z/installing-ibm-power.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-power.adoc
// * installing/installing-rhv-restricted-network.adoc

ifeval::["{context}" == "installing-vsphere"]
:vsphere:
endif::[]

ifeval::["{context}" == "installing-restricted-networks-vsphere"]
:vsphere:
endif::[]

ifeval::["{context}" == "installing-vsphere-network-customizations"]
:vsphere:
endif::[]

ifeval::["{context}" == "installing-ibm-z"]
:ibm-z:
endif::[]
ifeval::["{context}" == "installing-ibm-z-kvm"]
:ibm-z-kvm:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z"]
:restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-power"]
:restricted:
endif::[]

[id="installation-network-user-infra_{context}"]
= Networking requirements for user-provisioned infrastructure

All the {op-system-first} machines require network in `initramfs` during boot
to fetch Ignition config from the machine config server.

ifdef::ibm-z[]
During the initial boot, the machines require an HTTP or HTTPS server to
establish a network connection to download their Ignition config files.

Ensure that the machines have persistent IP addresses and host names.
endif::ibm-z[]
ifndef::ibm-z[]
During the initial boot, the machines require either a DHCP server
or that static IP addresses be set on each host in the cluster to
establish a network connection, which allows them to download their Ignition config files.

It is recommended to use the DHCP server to manage the machines for the cluster
long-term. Ensure that the DHCP server is configured to provide persistent IP
addresses and host names to the cluster machines.
endif::ibm-z[]

The Kubernetes API server must be able to resolve the node names of the cluster
machines. If the API servers and worker nodes are in different zones, you can
configure a default DNS search zone to allow the API server to resolve the
node names. Another supported approach is to always refer to hosts by their
fully-qualified domain names in both the node objects and all DNS requests.

You must configure the network connectivity between machines to allow cluster
components to communicate. Each machine must be able to resolve the host names
of all other machines in the cluster.

ifdef::ibm-z-kvm[]
[NOTE]
====
The {op-system-base} KVM host must be configured to use bridged networking in libvirt or MacVTap to connect the network to the virtual machines. The virtual machines must have access to the network, which is attached to the {op-system-base} KVM host. Virtual Networks, for example network address translation (NAT), within KVM are not a supported configuration.
====
endif::ibm-z-kvm[]

.All machines to all machines
[cols="2a,2a,5a",options="header"]
|===

|Protocol
|Port
|Description

|ICMP
|N/A
|Network reachability tests

.4+|TCP
|`1936`
|Metrics

|`9000`-`9999`
|Host level services, including the node exporter on ports `9100`-`9101` and
the Cluster Version Operator on port `9099`.

|`10250`-`10259`
|The default ports that Kubernetes reserves

|`10256`
|openshift-sdn

.3+|UDP
|`4789`
|VXLAN and Geneve

|`6081`
|VXLAN and Geneve

|`9000`-`9999`
|Host level services, including the node exporter on ports `9100`-`9101`.

|TCP/UDP
|`30000`-`32767`
|Kubernetes node port

|===

.All machines to control plane
[cols="2a,2a,5a",options="header"]
|===

|Protocol
|Port
|Description

|TCP
|`6443`
|Kubernetes API

|===

.Control plane machines to control plane machines
[cols="2a,2a,5a",options="header"]
|===

|Protocol
|Port
|Description

|TCP
|`2379`-`2380`
|etcd server and peer ports

|===

[discrete]
== Network topology requirements

The infrastructure that you provision for your cluster must meet the following
network topology requirements.

ifndef::restricted,origin[]
[IMPORTANT]
====
{product-title} requires all nodes to have internet access to pull images
for platform containers and provide telemetry data to Red Hat.
====
endif::restricted,origin[]

[discrete]
=== Load balancers

Before you install {product-title}, you must provision two load balancers that meet the following requirements:

. *API load balancer*: Provides a common endpoint for users, both human and machine, to interact with and configure the platform. Configure the following conditions:
+
--
  ** Layer 4 load balancing only. This can be referred to as Raw TCP, SSL Passthrough, or SSL Bridge mode. If you use SSL Bridge mode, you must enable Server Name Indication (SNI) for the API routes.
  ** A stateless load balancing algorithm. The options vary based on the load balancer implementation.
--
+
[IMPORTANT]
====
Do not configure session persistence for an API load balancer.
====
+
Configure the following ports on both the front and back of the load balancers:
+
.API load balancer
[cols="2,5,^2,^2,2",options="header"]
|===

|Port
|Back-end machines (pool members)
|Internal
|External
|Description

|`6443`
|Bootstrap and control plane. You remove the bootstrap machine from the load
balancer after the bootstrap machine initializes the cluster control plane. You
must configure the `/readyz` endpoint for the API server health check probe.
|X
|X
|Kubernetes API server

|`22623`
|Bootstrap and control plane. You remove the bootstrap machine from the load
balancer after the bootstrap machine initializes the cluster control plane.
|X
|
|Machine config server

|===
+
[NOTE]
====
The load balancer must be configured to take a maximum of 30 seconds from the
time the API server turns off the `/readyz` endpoint to the removal of the API
server instance from the pool. Within the time frame after `/readyz` returns an
error or becomes healthy, the endpoint must have been removed or added. Probing
every 5 or 10 seconds, with two successful requests to become healthy and three
to become unhealthy, are well-tested values.
====

. *Application Ingress load balancer*: Provides an Ingress point for application traffic flowing in from outside the cluster. Configure the following conditions:
+
--
  ** Layer 4 load balancing only. This can be referred to as Raw TCP, SSL Passthrough, or SSL Bridge mode. If you use SSL Bridge mode, you must enable Server Name Indication (SNI) for the Ingress routes.
  ** A connection-based or session-based persistence is recommended, based on the options available and types of applications that will be hosted on the platform.
--
+
Configure the following ports on both the front and back of the load balancers:
+
.Application Ingress load balancer
[cols="2,5,^2,^2,2",options="header"]
|===

|Port
|Back-end machines (pool members)
|Internal
|External
|Description

|`443`
|The machines that run the Ingress router pods, compute, or worker, by default.
|X
|X
|HTTPS traffic

|`80`
|The machines that run the Ingress router pods, compute, or worker, by default.
|X
|X
|HTTP traffic

|===

[TIP]
====
If the true IP address of the client can be seen by the load balancer, enabling source IP-based session persistence can improve performance for applications that use end-to-end TLS encryption.
====

[NOTE]
====
A working configuration for the Ingress router is required for an
{product-title} cluster. You must configure the Ingress router after the control
plane initializes.
====

ifdef::vsphere[]
[discrete]
== Ethernet adaptor hardware address requirements

When provisioning VMs for the cluster, the ethernet interfaces configured for
each VM must use a MAC address from the VMware Organizationally Unique
Identifier (OUI) allocation ranges:

* `00:05:69:00:00:00` to `00:05:69:FF:FF:FF`
* `00:0c:29:00:00:00` to `00:0c:29:FF:FF:FF`
* `00:1c:14:00:00:00` to `00:1c:14:FF:FF:FF`
* `00:50:56:00:00:00` to `00:50:56:FF:FF:FF`

If a MAC address outside the VMware OUI is used, the cluster installation will
not succeed.
endif::vsphere[]

ifdef::vsphere[]
:!vsphere:
endif::[]

[discrete]
== NTP configuration

An {product-title} cluster that is installed in a restricted network is configured to use a public Network Time Protocol (NTP) server by default. To avoid clock skew, reconfigure the cluster to use a private NTP server instead. For more information, see the documentation for _Configuring chrony time service_.

ifeval::["{context}" == "installing-ibm-z"]
:!ibm-z:
endif::[]
ifeval::["{context}" == "installing-ibm-z-kvm"]
:!ibm-z-kvm:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z"]
:!restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-power"]
:!restricted:
endif::[]
