// Module included in the following assemblies:
//
// * logging/cluster-logging-log-forwarding.adoc

[id="cluster-logging-log-forwarding-configure_{context}"]
= Configuring the Log Forwarding feature

To configure the Log Forwarding, edit the Cluster Logging Custom Resource (CR) to add the `clusterlogging.openshift.io/logforwardingtechpreview: enabled` annotation and create a Log Forwarding Custom Resource to specify the outputs, pipelines, and enable log forwarding. 

If you enable Log Forwarding, you should define a pipeline all three source types. The logs from any undefined source type are dropped. For example, if you specified a pipeline for the `logs.app` and `log-audit` types, but did not specify a pipeline for the `logs.infra` type, `logs.infra` logs are dropped.

.Procedure

To configure the Log Forwarding feature:

. Edit the Cluster Logging Custom Resource (CR) in the `openshift-logging` project: 
+
----
$ oc edit ClusterLogging instance
----

. Add the `clusterlogging.openshift.io/logforwardingtechpreview` annotation and set to `enabled`:
+
[source,yaml]
----
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  annotations:
    clusterlogging.openshift.io/logforwardingtechpreview: enabled <1>
  name: "instance"
  namespace: "openshift-logging"
spec:

...

  collection: <2>
    logs:
      type: "fluentd"
      fluentd: {}
----
<1> Enables and disables the Log Forwarding feature. Set to `enable` to use log forwarding. To use the only the {product-title} Elasticsearch instance, set to disabled or do not add the annotation.
<2> The `spec.collection` block must be defined in the Cluster Logging CR for log forwarding to work.
 
. Create the Log Forwarding Custom Resource:
+
[source,yaml]
----
apiVersion: "logging.openshift.io/v1alpha1"
kind: "LogForwarding"
metadata:
  name: instance <1>
  namespace: openshift-logging <2>
spec:
  disableDefaultForwarding: true <3>
  outputs: <4>
   - type: "elasticsearch"
     name: elasticsearch
     endpoint: elasticsearch.openshift-logging.svc:9200
     secret:
        name: elasticsearch
   - type: "elasticsearch"
     name: elasticsearch-insecure
     endpoint: elasticsearch-insecure.svc.messaging.cluster.local
     insecure: true
   - type: "forward"
     name: secureforward-offcluster
     endpoint: https://secureforward.offcluster.com:9200
     secret:
        name: secureforward
  pipelines: <5>
   - name: container-logs
     inputSource: logs.app
     outputRefs:
     - elasticsearch
     - secureforward-offcluster
   - name: infra-logs
     inputSource: logs.infra
     outputRefs:
     - elasticsearch-insecure
   - name: audit-logs
     inputSource: logs.audit
     outputRefs:
     - secureforward-offcluster
----
<1> The name of the log forwarding CR must be `instance`.
<2> The namespace for the log forwarding CR must be `openshift-logging`.
<3> Set the annotation to `enabled` to enable log forwarding.
<4> Add one or more endpoints:
* Specify the type of output, either `elasticseach` or `forward`.
* Enter a name for the output.
* Enter the endpoint, either the server name or FQDN.
* Optionally, enter the name of the secret required by the endpoint for TLS communication. The secret must exist in the openshift-logging project.
* Specify `insecure: true` if the endpoint does not use a secret, resulting in insecure communication.
<5> Add one or more pipelines:
* Enter a name for the pipeline
* Specify the source type: `logs.app`, `logs.infra`, or `logs.audit`.
* Specify the name of one or more outputs configured in the CR.

[NOTE]
====
If you set `disableDefaultForwarding: true` you must configure a pipeline and output for all three types of logs, application, infrastructure, and audit. If you do not specify a pipeline and output for a log type, those logs are not stored and will be lost.
==== 

== Example log forwarding custom resources
 
A typical Log Forwarding configuration would be similar to the following examples.

The following Log Forwarding custom resource sends all logs to a secured external Elasticsearch instance:

.Sample custom resource to forward to an Elasticsearch instance

[source,yaml]
----
apiVersion: logging.openshift.io/v1alpha1
kind: LogForwarding
metadata:
  name: instance
  namespace: openshift-logging
spec:
  disableDefaultForwarding: true
  outputs:
    - name: user-created-es
      type: elasticsearch
      endpoint: 'elasticsearch-server.openshift-logging.svc:9200'
      secret:
        name: piplinesecret
  pipelines:
    - name: app-pipeline
      inputSource: logs.app
      outputRefs:
        - user-created-es
    - name: infra-pipeline
      inputSource: logs.infra
      outputRefs:
        - user-created-es
    - name: audit-pipeline
      inputSource: logs.audit
      outputRefs:
        - user-created-es
----

The following Log Forwarding custom resource sends all logs to a secured Fluentd instance using the Fluentd `out_forward` plug-in.

.Sample custom resource to use the`out_forward` plugin

[source,yaml]
----
apiVersion: logging.openshift.io/v1alpha1
kind: LogForwarding
metadata:
  name: instance
  namespace: openshift-logging
spec:
  disableDefaultForwarding: true
  outputs:
    - name: fluentd-created-by-user
      type: forward
      endpoint: 'fluentdserver.openshift-logging.svc:24224'
      secret:
        name: fluentdserver
  pipelines:
    - name: app-pipeline
      inputType: logs.app
      outputRefs:
        - fluentd-created-by-user
    - name: infra-pipeline
      inputType: logs.infra
      outputRefs:
        - fluentd-created-by-user
    - name: clo-default-audit-pipeline
      inputType: logs.audit
      outputRefs:
        - fluentd-created-by-user
----

The following Log Forwarding custom resource sends all logs to the internal {product-title} Elaticsearch instance, which is the default log forwarding method.

.Sample custom resource to use the default log forwarding

[source,yaml]
----
apiVersion: logging.openshift.io/v1alpha1
kind: LogForwarding
metadata:
  name: instance
  namespace: openshift-logging
spec:
  disableDefaultForwarding: false
----
////
The following Log Forwarding custom resource contains an incorrectly defined source type, `logs.app-logs`. The `status` block shows that the associated pipeline, `app-pipeline`, is dropped resulting in application logs not being stored.

.Sample custom resource with a dropped pipeline

[source,yaml]
----
apiVersion: logging.openshift.io/v1alpha1
kind: LogForwarding
metadata:
  annotations:
    clusterlogging.openshift.io/logforwardingtechpreview: enabled
  creationTimestamp: "2019-11-25T01:22:29Z"
  generation: 1
  name: instance
  namespace: openshift-logging
  resourceVersion: "30213"
  selfLink: /apis/logging.openshift.io/v1alpha1/namespaces/openshift-logging/logforwardings/instance
  uid: 06b40471-8fc1-4f80-8c1e-0117757c67f8
spec:
  outputs:
  - endpoint: fluentdserver1.openshift-logging.svc:24224
    insecure: true
    name: fluentd-created-by-user
    type: forward
  pipelines:
  - inputSource: logs.app-logs
    name: app-pipeline
    outputRefs:
    - fluentd-created-by-user
  - inputSource: logs.infra
    name: infra-pipeline
    outputRefs:
    - fluentd-created-by-user
  - inputSource: logs.audit
    name: audit-pipeline
    outputRefs:
    - fluentd-created-by-user
status:
  lastUpdated: null
  outputs:
  - lastUpdated: "2019-11-25T01:24:55Z"
    name: fluentd-created-by-user
    state: Accepted
  pipelines:
  - conditions:
    - reason: UnrecognizedSourceType
      status: "False"
      typ: SourceType
    lastUpdated: "2019-11-25T01:24:55Z"
    name: app-pipeline
    state: Dropped
  - lastUpdated: "2019-11-25T01:24:55Z"
    name: infra-pipeline
    state: Accepted
  - lastUpdated: "2019-11-25T01:24:55Z"
    name: audit-pipeline
    state: Accepted
  sources:
  - logs.infra
  - logs.audit
----
////
