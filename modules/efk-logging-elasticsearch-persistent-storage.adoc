// Module included in the following assemblies:
//
// * logging/efk-logging-elasticsearch.adoc

[id='efk-logging-elasticsearch-persistent-storage_{context}']
= Configuring persistent storage for Elasticsearch

By default, the `openshift_logging` Ansible role creates an ephemeral
deployment in which all of a pod's data is lost upon restart. 

For production environments, each Elasticsearch deployment configuration requires a persistent storage volume. You can specify an existing persistent
volume claim or allow {product-title} to create one. 

* *Use existing PVCs.* If you create your own PVCs for the deployment, {product-title} uses those PVCs. 
+
Name the PVCs to match the `openshift_logging_es_pvc_prefix` setting, which defaults to
`logging-es`. Assign each PVC a name with a sequence number added to it: `logging-es-0`,
`logging-es-1`, `logging-es-2`, and so on. 

* *Allow {product-title} to create a PVC.* If a PVC for Elsaticsearch does not exist, {product-title} creates the PVC based on parameters 
in the Ansible inventory file, by default *_/etc/ansible/hosts_*. 
+
[cols="3,7",options="header"]
|===
|Parameter
|Description

|`openshift_logging_es_pvc_size`
| Specify the size of the PVC request.

|`openshift_logging_elasticsearch_storage_type`
a|Specify the storage type as `pvc`. 
[NOTE]
====
This is an optional parameter. Setting the `openshift_logging_es_pvc_size` parameter to a value greater than 0 automatically sets this parameter to `pvc` by default. 
====

|`openshift_logging_es_pvc_prefix`
|Optionally, specify a custom prefix for the PVC.
|===
+
For example:
+
[source,bash]
----
openshift_logging_elasticsearch_storage_type=pvc 
openshift_logging_es_pvc_size=104802308Ki 
openshift_logging_es_pvc_prefix=es-logging
----

[WARNING]
====
Using NFS storage as a volume or a persistent volume (or via NAS such as
Gluster) is not supported for Elasticsearch storage, as Lucene relies on file
system behavior that NFS does not supply. Data corruption and other problems can
occur. If NFS storage is required, you can allocate a large file on a
volume to serve as a storage device and mount it locally on one host.
For example, if your NFS storage volume is mounted at *_/nfs/storage_*:

----
$ truncate -s 1T /nfs/storage/elasticsearch-1
$ mkfs.xfs /nfs/storage/elasticsearch-1
$ mount -o loop /nfs/storage/elasticsearch-1 /usr/local/es-storage
$ chown 1000:1000 /usr/local/es-storage
----

Then, use *_/usr/local/es-storage_* as a host-mount as described below.
Use a different backing file as storage for each Elasticsearch replica.

This loopback must be maintained manually outside of {product-title}, on the
node. You must not maintain it from inside a container.
====

It is possible to use a local disk volume (if available) on each
node host as storage for an Elasticsearch replica. Doing so requires
some preparation as follows.

. The relevant service account must be given the privilege to mount and edit a
local volume:
+
----
$ oc adm policy add-scc-to-user privileged  \
       system:serviceaccount:logging:aggregated-logging-elasticsearch <1>
----
<1> Use the project you created earlier (for example, *logging*) when running the
logging playbook.

. Each Elasticsearch replica definition must be patched to claim that privilege,
for example:
+
----
$ for dc in $(oc get deploymentconfig --selector logging-infra=elasticsearch -o name); do
    oc scale $dc --replicas=0
    oc patch $dc \
       -p '{"spec":{"template":{"spec":{"containers":[{"name":"elasticsearch","securityContext":{"privileged": true}}]}}}}'
  done
----

. The Elasticsearch replicas must be located on the correct nodes to use the local
storage, and should not move around even if those nodes are taken down for a
period of time. This requires giving each Elasticsearch replica a node selector
that is unique to a node where an administrator has allocated storage for it. To
configure a node selector, edit each Elasticsearch deployment configuration and
add or edit the *nodeSelector* section to specify a unique label that you have
applied for each desired node:
+
----
apiVersion: v1
kind: DeploymentConfig
spec:
  template:
    spec:
      nodeSelector:
        logging-es-node: "1" <1>
----
<1> This label should uniquely identify a replica with a single node that bears that
label, in this case `logging-es-node=1`. Use the `oc label` command to apply
labels to nodes as needed.
+
To automate applying the node selector you can instead use the `oc patch` command:
+
----
$ oc patch dc/logging-es-<suffix> \
   -p '{"spec":{"template":{"spec":{"nodeSelector":{"logging-es-node":"1"}}}}}'
----

. Once these steps are taken, a local host mount can be applied to each replica
as in this example (where we assume storage is mounted at the same path on each node):
+
----
$ for dc in $(oc get deploymentconfig --selector logging-infra=elasticsearch -o name); do
    oc set volume $dc \
          --add --overwrite --name=elasticsearch-storage \
          --type=hostPath --path=/usr/local/es-storage
    oc rollout latest $dc
    oc scale $dc --replicas=1
  done
----

