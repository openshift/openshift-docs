// Module included in the following assemblies:
//
// *scalability_and_performance/sno-du-deploying-clusters-on-single-nodes.adoc

:_content-type: PROCEDURE
[id="sno-du-configuring-logging-locally-and-forwarding_{context}"]
= Configuring logging locally and forwarding

To be able to debug a single node distributed unit (DU), logs need to be stored for further
analysis.

.Procedure

* Edit the `ClusterLogging` custom resource (CR) in the `openshift-logging` project:
+
[source,yaml]
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogging <1>
 metadata:
  name: instance
  namespace: openshift-logging
spec:
  collection:
    logs:
      fluentd: {}
      type: fluentd
  curation:
    type: "curator"
    curator:
      schedule: "30 3 * * *"
    managementState: Managed
---
apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder <2>
metadata:
  name: instance
  namespace: openshift-logging
spec:
  inputs:
    - infrastructure: {}
  outputs:
    - name: kafka-open
      type: kafka
      url: tcp://10.46.55.190:9092/test    <3>
  pipelines:
    - inputRefs:
      - audit
      name: audit-logs
      outputRefs:
      - kafka-open
    - inputRefs:
      - infrastructure
      name: infrastructure-logs
      outputRefs:
      - kafka-open
----
<1> Updates the existing instance or creates the instance if it does not exist.
<2> Updates the existing instance or creates the instance if it does not exist.
<3> Specifies the destination of the kafka server.
