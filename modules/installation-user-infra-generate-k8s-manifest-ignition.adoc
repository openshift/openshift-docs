// Module included in the following assemblies:
//
// * installing/installing_aws/installing-aws-user-infra.adoc
// * installing/installing_azure/installing-azure-user-infra.adoc
// * installing/installing_bare_metal/installing-bare-metal.adoc
// * installing/installing_gcp/installing-gcp-user-infra.adoc
// * installing/installing_gcp/installing-restricted-networks-gcp.adoc
// * installing/installing_aws/installing-restricted-networks-aws.adoc
// * installing/installing_bare_metal/installing-restricted-networks-bare-metal.adoc
// * installing/installing_vsphere/installing-restricted-networks-vsphere.adoc
// * installing/installing_vsphere/installing-vsphere.adoc
// * installing/installing_ibm_z/installing-ibm-z.adoc
// * installing/installing_openstack/installing-openstack-user.adoc


ifeval::["{context}" == "installing-aws-user-infra"]
:aws:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-aws"]
:aws:
:restricted:
endif::[]
ifeval::["{context}" == "installing-azure-user-infra"]
:azure:
:azure-user-infra:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-vsphere"]
:restricted:
endif::[]
ifeval::["{context}" == "installing-bare-metal"]
:baremetal:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-bare-metal"]
:baremetal-restricted:
endif::[]
ifeval::["{context}" == "installing-gcp-user-infra"]
:gcp:
endif::[]
ifeval::["{context}" == "installing-gcp-user-infra-vpc"]
:gcp:
:user-infra-vpc:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-gcp"]
:gcp:
:restricted:
endif::[]
ifeval::["{context}" == "installing-openstack-user"]
:osp:
endif::[]
ifeval::["{context}" == "installing-openstack-user-kuryr"]
:osp:
endif::[]
ifeval::["{context}" == "installing-vsphere"]
:vsphere:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-vsphere"]
:vsphere:
endif::[]

[id="installation-user-infra-generate-k8s-manifest-ignition_{context}"]
= Creating the Kubernetes manifest and Ignition config files

Because you must modify some cluster definition files and manually start the cluster machines, you must generate the Kubernetes manifest and Ignition config files that the cluster needs to make its machines.

[IMPORTANT]
====
The Ignition config files that the installation program generates contain
certificates that expire after 24 hours. You must complete your cluster
installation and keep the cluster running for 24 hours in a non-degraded state
to ensure that the first certificate rotation has finished.
====

.Prerequisites

* Obtain the {product-title} installation program.
ifdef::restricted,baremetal-restricted[]
For a restricted network installation, these files are on your mirror host.
endif::restricted,baremetal-restricted[]
* Create the `install-config.yaml` installation configuration file.

.Procedure

. Generate the Kubernetes manifests for the cluster:
+
[source,terminal]
----
$ ./openshift-install create manifests --dir=<installation_directory> <1>
----
+
.Example output
[source,terminal]
----
INFO Consuming Install Config from target directory
WARNING Making control-plane schedulable by setting MastersSchedulable to true for Scheduler cluster settings
----
<1> For `<installation_directory>`, specify the installation directory that
contains the `install-config.yaml` file you created.
+
Because you create your own compute machines later in the installation process,
you can safely ignore this warning.

ifdef::aws,azure,gcp[]
. Remove the Kubernetes manifest files that define the control plane machines:
+
[source,terminal]
----
$ rm -f <installation_directory>/openshift/99_openshift-cluster-api_master-machines-*.yaml
----
+
By removing these files, you prevent the cluster from automatically generating control plane machines.
endif::aws,azure,gcp[]

ifdef::gcp[]
ifndef::user-infra-vpc[]
. Optional: If you do not want the cluster to provision compute machines, remove
the Kubernetes manifest files that define the worker machines:
endif::user-infra-vpc[]
endif::gcp[]
ifdef::aws,azure,user-infra-vpc[]
. Remove the Kubernetes manifest files that define the worker machines:
endif::aws,azure,user-infra-vpc[]
ifdef::aws,azure,gcp[]
+
[source,terminal]
----
$ rm -f <installation_directory>/openshift/99_openshift-cluster-api_worker-machineset-*.yaml
----
+
Because you create and manage the worker machines yourself, you do not need
to initialize these machines.
endif::aws,azure,gcp[]

ifdef::osp,vsphere[]
. Remove the Kubernetes manifest files that define the control plane machines and compute machineSets:
+
[source,terminal]
----
$ rm -f openshift/99_openshift-cluster-api_master-machines-*.yaml openshift/99_openshift-cluster-api_worker-machineset-*.yaml
----
+
Because you create and manage these resources yourself, you do not have
to initialize them.
+
* You can preserve the MachineSet files to create compute machines by using the machine API, but you must update references to them to match your environment.
endif::osp,vsphere[]

ifdef::baremetal,baremetal-restricted[]
[WARNING]
====
If you are running a three-node cluster, skip the following step to allow the masters to be schedulable.
====
endif::baremetal,baremetal-restricted[]
. Modify the `<installation_directory>/manifests/cluster-scheduler-02-config.yml` Kubernetes manifest file to prevent pods from being scheduled on the control plane machines:
+
--
.. Open the `<installation_directory>/manifests/cluster-scheduler-02-config.yml` file.
.. Locate the `mastersSchedulable` parameter and set its value to `False`.
.. Save and exit the file.
--

ifdef::gcp,aws,azure[]
ifndef::user-infra-vpc[]
. Optional: If you do not want
link:https://github.com/openshift/cluster-ingress-operator[the Ingress Operator]
to create DNS records on your behalf, remove the `privateZone` and `publicZone`
sections from the `<installation_directory>/manifests/cluster-dns-02-config.yml` DNS configuration file:
endif::user-infra-vpc[]
ifdef::user-infra-vpc[]
. Remove the `privateZone`
sections from the `<installation_directory>/manifests/cluster-dns-02-config.yml` DNS configuration file:
endif::user-infra-vpc[]
+
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: DNS
metadata:
  creationTimestamp: null
  name: cluster
spec:
  baseDomain: example.openshift.com
  privateZone: <1>
    id: mycluster-100419-private-zone
ifndef::user-infra-vpc[]
  publicZone: <1>
    id: example.openshift.com
endif::user-infra-vpc[]
status: {}
----
<1> Remove this section completely.
+
ifndef::user-infra-vpc[]
If you do so, you must add ingress DNS records manually in a later step.
endif::user-infra-vpc[]
endif::gcp,aws,azure[]

ifdef::user-infra-vpc[]
. Configure the cloud provider for your VPC.
+
--
.. Open the `<installation_directory>/manifests/cloud-provider-config.yaml` file.
.. Add the `network-project-id` parameter and set its value to the ID of project that hosts the shared VPC network.
.. Add the `network-name` parameter and set its value to the name of the shared VPC network that hosts the {product-title} cluster.
.. Replace the value of the `subnetwork-name` parameter with the value of the shared VPC subnet that hosts your compute machines.
+
--
The contents of the `<installation_directory>/manifests/cloud-provider-config.yaml` resemble the following example:
+
[source,yaml]
----
config: |+
  [global]
  project-id      = example-project
  regional        = true
  multizone       = true
  node-tags       = opensh-ptzzx-master
  node-tags       = opensh-ptzzx-worker
  node-instance-prefix = opensh-ptzzx
  external-instance-groups-prefix = opensh-ptzzx
  network-project-id = example-shared-vpc
  network-name    = example-network
  subnetwork-name = example-worker-subnet
----

. If you deploy a cluster that is not on a private network, open the `<installation_directory>/manifests/cluster-ingress-default-ingresscontroller.yaml` file and replace the value of the `scope` parameter with `External`. The contents of the file resemble the following example:
+
[source,yaml]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: null
  name: default
  namespace: openshift-ingress-operator
spec:
  endpointPublishingStrategy:
    loadBalancer:
      scope: External
    type: LoadBalancerService
status:
  availableReplicas: 0
  domain: ''
  selector: ''
----

endif::user-infra-vpc[]

ifdef::azure-user-infra[]
. When configuring Azure on user-provisioned infrastructure, you must export
some common variables defined in the manifest files to use later in the Azure
Resource Manager (ARM) templates:
.. Export the infrastructure ID by using the following command:
+
[source,terminal]
----
$ export INFRA_ID=<infra_id> <1>
----
<1> The {product-title} cluster has been assigned an identifier (`INFRA_ID`) in the form of `<cluster_name>-<random_string>`. This will be used as the base name for most resources created using the provided ARM templates. This is the value of the `.status.infrastructureName` attribute from the `manifests/cluster-infrastructure-02-config.yml` file.

.. Export the resource group by using the following command:
+
[source,terminal]
----
$ export RESOURCE_GROUP=<resource_group> <1>
----
<1> All resources created in this Azure deployment exists as part of a link:https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/overview#resource-groups[resource group]. The resource group name is also based on the `INFRA_ID`, in the form of `<cluster_name>-<random_string>-rg`. This is the value of the `.status.platformStatus.azure.resourceGroupName` attribute from the `manifests/cluster-infrastructure-02-config.yml` file.
endif::azure-user-infra[]

. Obtain the Ignition config files:
+
[source,terminal]
----
$ ./openshift-install create ignition-configs --dir=<installation_directory> <1>
----
<1> For `<installation_directory>`, specify the same installation directory.
+
The following files are generated in the directory:
+
----
.
├── auth
│   ├── kubeadmin-password
│   └── kubeconfig
├── bootstrap.ign
├── master.ign
├── metadata.json
└── worker.ign
----

ifdef::osp[]
. Export the metadata file's `infraID` key as an environment variable:
+
[source,terminal]
----
$ export INFRA_ID=$(jq -r .infraID metadata.json)
----

[TIP]
Extract the `infraID` key from `metadata.json` and use it as a prefix for all of the {rh-openstack} resources that you create. By doing so, you avoid name conflicts when making multiple deployments in the same project.
endif::osp[]

ifeval::["{context}" == "installing-restricted-networks-aws"]
:!aws:
:!restricted:
endif::[]
ifeval::["{context}" == "installing-aws-user-infra"]
:!aws:
endif::[]
ifeval::["{context}" == "installing-azure-user-infra"]
:!azure:
:!azure-user-infra:
endif::[]
ifeval::["{context}" == "installing-gcp-user-infra"]
:!gcp:
endif::[]
ifeval::["{context}" == "installing-gcp-user-infra-vpc"]
:!gcp:
:!user-infra-vpc:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-vsphere"]
:!restricted:
endif::[]
ifeval::["{context}" == "installing-bare-metal"]
:!baremetal:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-bare-metal"]
:!baremetal-restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-gcp"]
:!gcp:
:!restricted:
endif::[]
ifeval::["{context}" == "installing-osp-user"]
:!osp:
endif::[]
ifeval::["{context}" == "installing-openstack-user-kuryr"]
:!osp:
endif::[]
ifeval::["{context}" == "installing-vsphere"]
:!vsphere:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-vsphere"]
:!vsphere:
endif::[]
