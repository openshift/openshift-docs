// Module included in the following assemblies:
//
// * installing/installing_aws/installing-aws-user-infra.adoc
// * installing/installing_azure/installing-azure-user-infra.adoc
// * installing/installing_azure_stack_hub/installing-azure-stack-hub-user-infra.adoc
// * installing/installing_bare_metal/installing-bare-metal.adoc
// * installing/installing_gcp/installing-gcp-user-infra.adoc
// * installing/installing_gcp/installing-restricted-networks-gcp.adoc
// * installing/installing_aws/installing-restricted-networks-aws.adoc
// * installing/installing_bare_metal/installing-restricted-networks-bare-metal.adoc
// * installing/installing_platform_agnostic/installing-platform-agnostic.adoc
// * installing/installing_vsphere/installing-restricted-networks-vsphere.adoc
// * installing/installing_vsphere/installing-vsphere.adoc
// * installing/installing_ibm_z/installing-ibm-z.adoc
// * installing/installing_ibm_z/installing-ibm-z-kvm.adoc
// * installing/installing_ibm_z/installing-ibm-z-lpar.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z-kvm.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z-lpar.adoc
// * installing/installing_ibm_power/installing-ibm-power.adoc
// * installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc
// * installing/installing_openstack/installing-openstack-user.adoc
// * installing/installing_azure/installing-restricted-networks-azure-user-provisioned.adoc


ifeval::["{context}" == "installing-aws-user-infra"]
:aws:
:three-node-cluster:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-aws"]
:aws:
:restricted:
endif::[]
ifeval::["{context}" == "installing-azure-user-infra"]
:azure:
:azure-user-infra:
:three-node-cluster:
endif::[]
ifeval::["{context}" == "installing-azure-stack-hub-user-infra"]
:ash:
:azure-user-infra:
endif::[]
ifeval::["{context}" == "installing-bare-metal"]
:baremetal:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-bare-metal"]
:baremetal-restricted:
endif::[]
ifeval::["{context}" == "installing-gcp-user-infra"]
:gcp:
:three-node-cluster:
endif::[]
ifeval::["{context}" == "installing-gcp-user-infra-vpc"]
:gcp:
:user-infra-vpc:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-gcp"]
:gcp:
:restricted:
endif::[]
ifeval::["{context}" == "installing-osp-user"]
:osp:
endif::[]
ifeval::["{context}" == "installing-openstack-user"]
:osp:
endif::[]
ifeval::["{context}" == "installing-openstack-user-sr-iov"]
:osp:
endif::[]
ifeval::["{context}" == "installing-vsphere"]
:vsphere:
:three-node-cluster:
endif::[]
ifeval::["{context}" == "installing-vsphere-installer-provisioned-network-customizations"]
:vsphere:
endif::[]
ifeval::["{context}" == "installing-vsphere-network-customizations"]
:vsphere:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-vsphere"]
:vsphere:
:restricted:
endif::[]
ifeval::["{context}" == "installing-platform-agnostic"]
:baremetal:
endif::[]
ifeval::["{context}" == "installing-ibm-z"]
:ibm-z:
endif::[]
ifeval::["{context}" == "installing-ibm-z-kvm"]
:ibm-z:
endif::[]
ifeval::["{context}" == "installing-ibm-z-lpar"]
:ibm-z:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z"]
:ibm-z:
:restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z-kvm"]
:ibm-z:
:restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z-lpar"]
:ibm-z:
:restricted:
endif::[]
ifeval::["{context}" == "installing-ibm-power"]
:ibm-power:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-power"]
:ibm-power:
:restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-azure-user-provisioned"]
:azure:
:azure-user-infra:
endif::[]

:_mod-docs-content-type: PROCEDURE
[id="installation-user-infra-generate-k8s-manifest-ignition_{context}"]
= Creating the Kubernetes manifest and Ignition config files

Because you must modify some cluster definition files and manually start the cluster machines, you must generate the Kubernetes manifest and Ignition config files that the cluster needs to configure the machines.

The installation configuration file transforms into the Kubernetes manifests. The manifests wrap into the Ignition configuration files, which are later used to configure the cluster machines.

[IMPORTANT]
====
* The Ignition config files that the {product-title} installation program generates contain certificates that expire after 24 hours, which are then renewed at that time. If the cluster is shut down before renewing the certificates and the cluster is later restarted after the 24 hours have elapsed, the cluster automatically recovers the expired certificates. The exception is that you must manually approve the pending `node-bootstrapper` certificate signing requests (CSRs) to recover kubelet certificates. See the documentation for _Recovering from expired control plane certificates_ for more information.

* It is recommended that you use Ignition config files within 12 hours after they are generated because the 24-hour certificate rotates from 16 to 22 hours after the cluster is installed. By using the Ignition config files within 12 hours, you can avoid installation failure if the certificate update runs during installation.
====

ifdef::ibm-z[]
[NOTE]
====
The installation program that generates the manifest and Ignition files is architecture specific and can be obtained from the
link:https://mirror.openshift.com/pub/openshift-v4/s390x/clients/ocp/latest/[client image mirror]. The Linux version of the installation program runs on s390x only. This installer program is also available as a Mac OS version.
====
endif::ibm-z[]
ifdef::ibm-power[]
[NOTE]
====
The installation program that generates the manifest and Ignition files is architecture specific and can be obtained from the
link:https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/latest/[client image mirror]. The Linux version of the installation program (without an architecture postfix) runs on ppc64le only. This installer program is also available as a Mac OS version.
====
endif::ibm-power[]

.Prerequisites

* You obtained the {product-title} installation program.
ifdef::restricted,baremetal-restricted[]
For a restricted network installation, these files are on your mirror host.
endif::restricted,baremetal-restricted[]
* You created the `install-config.yaml` installation configuration file.

.Procedure

. Change to the directory that contains the {product-title} installation program and generate the Kubernetes manifests for the cluster:
+
[source,terminal]
----
$ ./openshift-install create manifests --dir <installation_directory> <1>
----
+
<1> For `<installation_directory>`, specify the installation directory that
contains the `install-config.yaml` file you created.

ifdef::aws,azure,ash,gcp[]
. Remove the Kubernetes manifest files that define the control plane machines:
+
[source,terminal]
----
$ rm -f <installation_directory>/openshift/99_openshift-cluster-api_master-machines-*.yaml
----
+
By removing these files, you prevent the cluster from automatically generating control plane machines.
endif::aws,azure,ash,gcp[]

ifdef::aws,ash,azure,gcp[]
. Remove the Kubernetes manifest files that define the control plane machine set:
+
[source,terminal]
----
$ rm -f <installation_directory>/openshift/99_openshift-machine-api_master-control-plane-machine-set.yaml
----
endif::aws,ash,azure,gcp[]

ifdef::gcp[]
ifndef::user-infra-vpc[]
. Optional: If you do not want the cluster to provision compute machines, remove
the Kubernetes manifest files that define the worker machines:
endif::user-infra-vpc[]
endif::gcp[]
ifdef::aws,azure,ash,user-infra-vpc[]
. Remove the Kubernetes manifest files that define the worker machines:
endif::aws,azure,ash,user-infra-vpc[]
ifdef::aws,azure,ash,gcp[]
+
[source,terminal]
----
$ rm -f <installation_directory>/openshift/99_openshift-cluster-api_worker-machineset-*.yaml
----
ifndef::user-infra-vpc[]
+
[IMPORTANT]
====
If you disabled the `MachineAPI` capability when installing a cluster on user-provisioned infrastructure, you must remove the Kubernetes manifest files that define the worker machines. Otherwise, your cluster fails to install.
====
endif::user-infra-vpc[]
+
Because you create and manage the worker machines yourself, you do not need to initialize these machines.
endif::aws,azure,ash,gcp[]

ifdef::osp,vsphere[]
. Remove the Kubernetes manifest files that define the control plane machines, compute machine sets, and control plane machine sets:
+
[source,terminal]
----
$ rm -f openshift/99_openshift-cluster-api_master-machines-*.yaml openshift/99_openshift-cluster-api_worker-machineset-*.yaml openshift/99_openshift-machine-api_master-control-plane-machine-set.yaml
----
+
Because you create and manage these resources yourself, you do not have
to initialize them.
+
* You can preserve the compute machine set files to create compute machines by using the machine API, but you must update references to them to match your environment.
endif::osp,vsphere[]
ifdef::baremetal,baremetal-restricted,ibm-z,ibm-power,three-node-cluster[]
+
[WARNING]
====
If you are installing a three-node cluster, skip the following step to allow the control plane nodes to be schedulable.
====
+
[IMPORTANT]
====
When you configure control plane nodes from the default unschedulable to schedulable, additional subscriptions are required. This is because control plane nodes then become compute nodes.
====
endif::baremetal,baremetal-restricted,ibm-z,ibm-power,three-node-cluster[]

. Check that the `mastersSchedulable` parameter in the `<installation_directory>/manifests/cluster-scheduler-02-config.yml` Kubernetes manifest file is set to `false`. This setting prevents pods from being scheduled on the control plane machines:
+
--
.. Open the `<installation_directory>/manifests/cluster-scheduler-02-config.yml` file.
.. Locate the `mastersSchedulable` parameter and ensure that it is set to `false`.
.. Save and exit the file.
--

ifdef::gcp,aws,azure,ash[]
ifndef::user-infra-vpc[]
. Optional: If you do not want
link:https://github.com/openshift/cluster-ingress-operator[the Ingress Operator]
to create DNS records on your behalf, remove the `privateZone` and `publicZone`
sections from the `<installation_directory>/manifests/cluster-dns-02-config.yml` DNS configuration file:
endif::user-infra-vpc[]
ifdef::user-infra-vpc[]
. Remove the `privateZone`
sections from the `<installation_directory>/manifests/cluster-dns-02-config.yml` DNS configuration file:
endif::user-infra-vpc[]
+
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: DNS
metadata:
  creationTimestamp: null
  name: cluster
spec:
  baseDomain: example.openshift.com
  privateZone: <1>
    id: mycluster-100419-private-zone
ifndef::user-infra-vpc[]
  publicZone: <1>
    id: example.openshift.com
endif::user-infra-vpc[]
status: {}
----
<1> Remove this section completely.
+
ifndef::user-infra-vpc[]
If you do so, you must add ingress DNS records manually in a later step.
endif::user-infra-vpc[]
endif::gcp,aws,azure,ash[]

ifdef::user-infra-vpc[]
. Configure the cloud provider for your VPC.
+
--
.. Open the `<installation_directory>/manifests/cloud-provider-config.yaml` file.
.. Add the `network-project-id` parameter and set its value to the ID of project that hosts the shared VPC network.
.. Add the `network-name` parameter and set its value to the name of the shared VPC network that hosts the {product-title} cluster.
.. Replace the value of the `subnetwork-name` parameter with the value of the shared VPC subnet that hosts your compute machines.
+
--
The contents of the `<installation_directory>/manifests/cloud-provider-config.yaml` resemble the following example:
+
[source,yaml]
----
config: |+
  [global]
  project-id      = example-project
  regional        = true
  multizone       = true
  node-tags       = opensh-ptzzx-master
  node-tags       = opensh-ptzzx-worker
  node-instance-prefix = opensh-ptzzx
  external-instance-groups-prefix = opensh-ptzzx
  network-project-id = example-shared-vpc
  network-name    = example-network
  subnetwork-name = example-worker-subnet
----

. If you deploy a cluster that is not on a private network, open the `<installation_directory>/manifests/cluster-ingress-default-ingresscontroller.yaml` file and replace the value of the `scope` parameter with `External`. The contents of the file resemble the following example:
+
[source,yaml]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: null
  name: default
  namespace: openshift-ingress-operator
spec:
  endpointPublishingStrategy:
    loadBalancer:
      scope: External
    type: LoadBalancerService
status:
  availableReplicas: 0
  domain: ''
  selector: ''
----
endif::user-infra-vpc[]

ifdef::ash[]
. Optional: If your Azure Stack Hub environment uses an internal certificate authority (CA), you must update the `.spec.trustedCA.name` field in the `<installation_directory>/manifests/cluster-proxy-01-config.yaml` file to use `user-ca-bundle`:
+
[source,yaml]
----
...
spec:
  trustedCA:
    name: user-ca-bundle
...
----
+
Later, you must update your bootstrap ignition to include the CA.
endif::ash[]

ifdef::azure-user-infra[]
. When configuring Azure on user-provisioned infrastructure, you must export
some common variables defined in the manifest files to use later in the Azure
Resource Manager (ARM) templates:
.. Export the infrastructure ID by using the following command:
+
[source,terminal]
----
$ export INFRA_ID=<infra_id> <1>
----
<1> The {product-title} cluster has been assigned an identifier (`INFRA_ID`) in the form of `<cluster_name>-<random_string>`. This will be used as the base name for most resources created using the provided ARM templates. This is the value of the `.status.infrastructureName` attribute from the `manifests/cluster-infrastructure-02-config.yml` file.

.. Export the resource group by using the following command:
+
[source,terminal]
----
$ export RESOURCE_GROUP=<resource_group> <1>
----
<1> All resources created in this Azure deployment exists as part of a link:https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/overview#resource-groups[resource group]. The resource group name is also based on the `INFRA_ID`, in the form of `<cluster_name>-<random_string>-rg`. This is the value of the `.status.platformStatus.azure.resourceGroupName` attribute from the `manifests/cluster-infrastructure-02-config.yml` file.
endif::azure-user-infra[]

ifdef::ash[]
. Manually create your cloud credentials.

.. From the directory that contains the installation program, obtain details of the {product-title} release image that your `openshift-install` binary is built to use:
+
[source,terminal]
----
$ openshift-install version
----
+
.Example output
[source,text]
----
release image quay.io/openshift-release-dev/ocp-release:4.y.z-x86_64
----

.. Set a `$RELEASE_IMAGE` variable with the release image from your installation file by running the following command:
+
[source,terminal]
----
$ RELEASE_IMAGE=$(./openshift-install version | awk '/release image/ {print $3}')
----

.. Extract the list of `CredentialsRequest` custom resources (CRs) from the {product-title} release image by running the following command:
+
[source,terminal]
----
$ oc adm release extract \
  --from=$RELEASE_IMAGE \
  --credentials-requests \
  --included \// <1>
  --install-config=<path_to_directory_with_installation_configuration>/install-config.yaml \// <2>
  --to=<path_to_directory_for_credentials_requests> <3>
----
<1> The `--included` parameter includes only the manifests that your specific cluster configuration requires.
<2> Specify the location of the `install-config.yaml` file.
<3> Specify the path to the directory where you want to store the `CredentialsRequest` objects. If the specified directory does not exist, this command creates it.
+
This command creates a YAML file for each `CredentialsRequest` object.
+
.Sample `CredentialsRequest` object
[source,yaml]
----
apiVersion: cloudcredential.openshift.io/v1
kind: CredentialsRequest
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  name: openshift-image-registry-azure
  namespace: openshift-cloud-credential-operator
spec:
  secretRef:
    name: installer-cloud-credentials
    namespace: openshift-image-registry
  providerSpec:
    apiVersion: cloudcredential.openshift.io/v1
    kind: AzureProviderSpec
    roleBindings:
    - role: Contributor
----

.. Create YAML files for secrets in the `openshift-install` manifests directory that you generated previously. The secrets must be stored using the namespace and secret name defined in the `spec.secretRef` for each `CredentialsRequest` object. The format for the secret data varies for each cloud provider.
+
.Sample `secrets.yaml` file
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
    name: ${secret_name}
    namespace: ${secret_namespace}
stringData:
  azure_subscription_id: ${subscription_id}
  azure_client_id: ${app_id}
  azure_client_secret: ${client_secret}
  azure_tenant_id: ${tenant_id}
  azure_resource_prefix: ${cluster_name}
  azure_resourcegroup: ${resource_group}
  azure_region: ${azure_region}
----

.. Create a `cco-configmap.yaml` file in the manifests directory with the Cloud Credential Operator (CCO) disabled:
+
.Sample `ConfigMap` object
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
name: cloud-credential-operator-config
namespace: openshift-cloud-credential-operator
  annotations:
    release.openshift.io/create-only: "true"
data:
  disabled: "true"
----
endif::ash[]

. To create the Ignition configuration files, run the following command from the directory that contains the installation program:
+
[source,terminal]
----
$ ./openshift-install create ignition-configs --dir <installation_directory> <1>
----
<1> For `<installation_directory>`, specify the same installation directory.
+
Ignition config files are created for the bootstrap, control plane, and compute nodes in the installation directory. The `kubeadmin-password` and `kubeconfig` files are created in the `./<installation_directory>/auth` directory:
+
----
.
├── auth
│   ├── kubeadmin-password
│   └── kubeconfig
├── bootstrap.ign
├── master.ign
├── metadata.json
└── worker.ign
----

ifdef::osp[]
. Export the metadata file's `infraID` key as an environment variable:
+
[source,terminal]
----
$ export INFRA_ID=$(jq -r .infraID metadata.json)
----

[TIP]
Extract the `infraID` key from `metadata.json` and use it as a prefix for all of the {rh-openstack} resources that you create. By doing so, you avoid name conflicts when making multiple deployments in the same project.
endif::osp[]

ifeval::["{context}" == "installing-restricted-networks-aws"]
:!aws:
:!restricted:
endif::[]
ifeval::["{context}" == "installing-aws-user-infra"]
:!aws:
:!three-node-cluster:
endif::[]
ifeval::["{context}" == "installing-azure-user-infra"]
:!azure:
:!azure-user-infra:
:!three-node-cluster:
endif::[]
ifeval::["{context}" == "installing-azure-stack-hub-user-infra"]
:!ash:
:!azure-user-infra:
endif::[]
ifeval::["{context}" == "installing-gcp-user-infra"]
:!gcp:
:!three-node-cluster:
endif::[]
ifeval::["{context}" == "installing-gcp-user-infra-vpc"]
:!gcp:
:!user-infra-vpc:
endif::[]
ifeval::["{context}" == "installing-bare-metal"]
:!baremetal:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-bare-metal"]
:!baremetal-restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-gcp"]
:!gcp:
:!restricted:
endif::[]
ifeval::["{context}" == "installing-osp-user"]
:!osp:
endif::[]
ifeval::["{context}" == "installing-openstack-user"]
:!osp:
endif::[]
ifeval::["{context}" == "installing-openstack-user-sr-iov"]
:!osp:
endif::[]
ifeval::["{context}" == "installing-vsphere"]
:!vsphere:
:!three-node-cluster:
endif::[]
ifeval::["{context}" == "installing-vsphere-installer-provisioned-network-customizations"]
:!vsphere:
endif::[]
ifeval::["{context}" == "installing-vsphere-network-customizations"]
:!vsphere:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-vsphere"]
:!vsphere:
:!restricted:
endif::[]
ifeval::["{context}" == "installing-platform-agnostic"]
:!baremetal:
endif::[]
ifeval::["{context}" == "installing-ibm-z"]
:!ibm-z:
endif::[]
ifeval::["{context}" == "installing-ibm-z-kvm"]
:!ibm-z:
endif::[]
ifeval::["{context}" == "installing-ibm-z-lpar"]
:!ibm-z:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z"]
:!ibm-z:
:!restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z-kvm"]
:!ibm-z:
:!restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z-lpar"]
:!ibm-z:
:!restricted:
endif::[]
ifeval::["{context}" == "installing-ibm-power"]
:!ibm-power:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-power"]
:!ibm-power:
:!restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-azure-user-provisioned"]
:!azure:
:!azure-user-infra:
endif::[]
