// Module included in the following assemblies:
//
// * networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc

[id="nw-ovn-kubernetes-migration-about_{context}"]
= Offline migration to the OVN-Kubernetes network plugin overview

The offline migration method is a manual process that includes some downtime, during which your cluster is unreachable. You can use an Ansible playbook that automates the offline migration steps so that you can save time. These methods are primarily used for self-managed {product-title} deployments, and are an alternative to the limited live migration procedure. Use these methods only when you cannot perform a limited live migration to the OVN-Kubernetes network plugin.

[IMPORTANT]
====
Before you migrate your {product-title} cluster to use the OVN-Kubernetes network plugin, update your cluster to the latest z-stream release so that all the latest bug fixes apply to your cluster.
====

Although a rollback procedure is provided, the offline migration methods are intended as one-way processes.

////
[IMPORTANT]
====
Migrating to or from the OVN-Kubernetes network plugin is not supported for managed OpenShift cloud services such as {product-dedicated}, Azure Red Hat OpenShift(ARO), and Red Hat OpenShift Service on AWS (ROSA).
====
////
include::snippets/sdn-deprecation-statement.adoc[]

The following sections provide more information about the offline migration method.

[id="supported-platforms-offline-migrating-ovn-kubernetes"]
== Supported platforms when using the offline migration methods

The following table provides information about the supported platforms for the manual offline migration type.

.Supported platforms for the manual offline migration method
[cols="1,1", options="header"]
|===
| Platform              | Offline migration

| Bare-metal hardware   |&#10003;
| {aws-first}           |&#10003;
| {gcp-first}           |&#10003;
| {ibm-cloud-name}      |&#10003;
| {azure-first}         |&#10003;
| {rh-openstack-first}  |&#10003;
| {vmw-first}           |&#10003;
| Nutanix               |&#10003;
|===

The following table provides information about the supported platforms for the manual offline migration type.

.Supported platforms for the Ansible playbook offline migration method
[cols="1,1", options="header"]
|===
| Platform              | Ansible playbook offline migration

| Bare-metal hardware   |&#10003;
| {aws-first}           |&#10003;
| {gcp-first}           |&#10003;
| {ibm-cloud-name}      |&#10003;
| {azure-first}         |&#10003;
| {vmw-first}           |&#10003;
|===

[NOTE]
====
For each listed platform in these tables, the platform supports installing an {product-title} cluster on installer-provisioned infrastructure and user-provisioned infrastructure.
====

[id="best-practices-migrating-ovn-kubernetes-network-provider_{context}"]
== Best practices for manual offline migration and the Ansible playbook offline migration methods

For a list of best practices when migrating to the OVN-Kubernetes network plugin with the offline migration method, see link:https://access.redhat.com/articles/7076160[Best practices for OpenShift SDN to OVN Kubernetes CNI Offline migration].

[id="considerations-migrating-ovn-kubernetes-network-provider_{context}"]
== Considerations for the offline migration methods to the OVN-Kubernetes network plugin

If you have more than 150 nodes in your {product-title} cluster, then open a support case for consultation on your migration to the OVN-Kubernetes network plugin.

The subnets assigned to nodes and the IP addresses assigned to individual pods are not preserved during the migration.

While the OVN-Kubernetes network plugin implements many of the capabilities present in the OpenShift SDN network plugin, the configuration is not the same.

* If your cluster uses any of the following OpenShift SDN network plugin capabilities, you must manually configure the same capability in the OVN-Kubernetes network plugin:
+
--
* Namespace isolation
* Egress router pods
--

* Before migrating to OVN-Kubernetes, ensure that the following IP address ranges are not in use: `100.64.0.0/16`, `169.254.169.0/29`, `100.88.0.0/16`, `fd98::/64`, `fd69::/125`, and `fd97::/64`. OVN-Kubernetes uses these ranges internally. Do not include any of these ranges in any other CIDR definitions in your cluster or infrastructure.

* If your `openshift-sdn` cluster with Precision Time Protocol (PTP) uses the User Datagram Protocol (UDP) for hardware time stamping and you migrate to the OVN-Kubernetes plugin, the hardware time stamping cannot be applied to primary interface devices, such as an Open vSwitch (OVS) bridge. As a result, UDP version 4 configurations cannot work with a `br-ex` interface.

* Similar to OpenShift SDN, OVN-Kubernetes resources require `ClusterAdmin` privileges. Migrating from OpenShift SDN to OVN-Kubernetes does not automatically update role-base access control (RBAC) resources. OpenShift SDN resources granted to a project administrator through the `aggregate-to-admin` `ClusterRole` must be manually reviewed and adjusted, as these changes are not included in the migration process. After migration, manual verification of RBAC resources is required.

* To prevent traffic flow issues, check existing network policies in any namespaces that host applications that rely on system components. If a policy exists, enable traffic that originates from `openshift-ingress` or `openshift-kube-apiserver` system services to prevent the default setting from blocking this traffic.

The following sections highlight the differences in configuration between the aforementioned capabilities in OVN-Kubernetes and OpenShift SDN network plugins.

[id="migrating-sdn-primary-interface_{context}"]
=== Primary network interface

The OpenShift SDN plugin allows application of the `NodeNetworkConfigurationPolicy` (NNCP) custom resource (CR) to the primary interface on a node. The OVN-Kubernetes network plugin does not have this capability.

If you have an NNCP applied to the primary interface, you must delete the NNCP before migrating to the OVN-Kubernetes network plugin. Deleting the NNCP does not remove the configuration from the primary interface, but with OVN-Kubernetes, the Kubernetes NMState cannot manage this configuration. Instead, the `configure-ovs.sh` shell script manages the primary interface and the configuration attached to this interface.

[id="namespace-isolation_{context}"]
=== Namespace isolation

OVN-Kubernetes supports only the network policy isolation mode.

[IMPORTANT]
====
For a cluster using OpenShift SDN that is configured in either the multitenant or subnet isolation mode, you can still migrate to the OVN-Kubernetes network plugin. Note that after the migration operation, multitenant isolation mode is dropped, so you must manually configure network policies to achieve the same level of project-level isolation for pods and services.
====

[id="egress-ip-addresses_{context}"]
=== Egress IP addresses

OpenShift SDN supports two different Egress IP modes:

* In the _automatically assigned_ approach, an egress IP address range is assigned to a node.
* In the _manually assigned_ approach, a list of one or more egress IP addresses is assigned to a node.

The migration process supports migrating Egress IP configurations that use the automatically assigned mode.

The differences in configuring an egress IP address between OVN-Kubernetes and OpenShift SDN is described in the following table:

.Differences in egress IP address configuration
[cols="1a,1a",options="header"]
|===
|OVN-Kubernetes|OpenShift SDN

|
* Create an `EgressIPs` object
* Add an annotation on a `Node` object

|
* Patch a `NetNamespace` object
* Patch a `HostSubnet` object
|===

For more information on using egress IP addresses in OVN-Kubernetes, see "Configuring an egress IP address".

[id="egress-network-policies_{context}"]
=== Egress network policies

The difference in configuring an egress network policy, also known as an egress firewall, between OVN-Kubernetes and OpenShift SDN is described in the following table:

.Differences in egress network policy configuration
[cols="1a,1a",options="header"]
|===
|OVN-Kubernetes|OpenShift SDN

|
* Create an `EgressFirewall` object in a namespace

|
* Create an `EgressNetworkPolicy` object in a namespace
|===

[NOTE]
====
Because the name of an `EgressFirewall` object can only be set to `default`, after the migration all migrated `EgressNetworkPolicy` objects are named `default`, regardless of what the name was under OpenShift SDN.

If you subsequently rollback to OpenShift SDN, all `EgressNetworkPolicy` objects are named `default` as the prior name is lost.

For more information on using an egress firewall in OVN-Kubernetes, see "Configuring an egress firewall for a project".
====

[id="egress-router-pods_{context}"]
=== Egress router pods

OVN-Kubernetes supports egress router pods in redirect mode. OVN-Kubernetes does not support egress router pods in HTTP proxy mode or DNS proxy mode.

When you deploy an egress router with the Cluster Network Operator, you cannot specify a node selector to control which node is used to host the egress router pod.

[id="multicast_{context}"]
=== Multicast

The difference between enabling multicast traffic on OVN-Kubernetes and OpenShift SDN is described in the following table:

.Differences in multicast configuration
[cols="1a,1a",options="header"]
|===
|OVN-Kubernetes|OpenShift SDN

|
* Add an annotation on a `Namespace` object

|
* Add an annotation on a `NetNamespace` object
|===

For more information on using multicast in OVN-Kubernetes, see "Enabling multicast for a project".

[id="network-policies_{context}"]
=== Network policies

OVN-Kubernetes fully supports the Kubernetes `NetworkPolicy` API in the `networking.k8s.io/v1` API group. No changes are necessary in your network policies when migrating from OpenShift SDN.
