// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-hostpath.adoc

[id="update-upgrading-mcp_{context}"]
= Delaying the update for specific nodes

There might be some secenarios where you might not want to update specific nodes when you update a cluster. Updating nodes requires the nodes to be cordoned and drained, which marks the nodes as unschedulable and evicts all pods. For example, you can ensure that critical applications stay available during the update process, even if the update fails.
Dealying the update can also be helpful if you do not have a single maintenance window to complete the whole update process.

To deplay the update for specific nodes, before the update, create a custom machine config pool (MCP) and move the nodes the new MCP. Then, pause the MCP. The nodes associated with a paused MCP do not get updated. When you are ready to update those nodes, move the nodes back to the worker MCP, where they will be updated. If you have more than one custom MCP for updating, after the first set nodes are updated, verify that the applications on those nodes are functioning properly before updating the other nodes.

[NOTE]
====
Creating custom MCP using master nodes is not supported. The Machine Config Operator ignores the custom MCP created for the master nodes. This is required to make sure control plane nodes remain stable.
====

This slow rollout of an update to the worker nodes can be characterized as a _canary_ release.

.Procedure

To delay updating specific nodes when you update your cluster:

. Create one or more MCPs.
+
[NOTE]
====
In case of a failure i.e. if the MCP with the new version does not work as expected with the applications, the nodes in the pool can be cordoned and drained. So that the application will not run on these nodes and the extra capacity will help to maintain the quality of service of the applications.
====

.. Get the list of worker nodes.
+
[source,terminal]
----
$ oc get -l 'node-role.kubernetes.io/master!=' -o 'jsonpath={range .items[*]}{.metadata.name}{"\n"}{end}' nodes
----
+
.Example output
+
[source,terminal]
----
ci-ln-pwnll6b-f76d1-s8t9n-worker-a-s75z4
ci-ln-pwnll6b-f76d1-s8t9n-worker-b-dglj2
ci-ln-pwnll6b-f76d1-s8t9n-worker-c-lldbm
----

.. Add the MCP name as label to the worker node.
+
[source,terminal]
----
$ oc label node <node name> node-role.kubernetes.io/<mcp name>=
----
.Example output
+
[source,terminal]
----
node/ci-ln-gtrwm8t-f76d1-spbl7-worker-a-xk76k labeled
----

.. Create the machineconfig pool
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: mcp-noupdate | <1>
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,mcpfoo]} | <1>
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/mcp-noupdate: "" | <1>
----
<1> Name of the machine config pool
+
[source,terminal]
----
$ oc create -f <file_name>
----
+
.Example output
+
[source,terminal]
----
machineconfigpool.machineconfiguration.openshift.io/mcp-update created
----
+
.. To see the list of MCPs present in the cluster and their state
+
[source,terminal]
----
$ oc get machineconfigpool
----
+
.Example output
[source,terminal]
----
NAME       CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master     rendered-master-b0bb90c4921860f2a5d8a2f8137c1867   True      False      False      3              3                   3                     0                      97m
mcp-update rendered-mcpfoo-87ba3dec1ad78cb6aecebf7fbb476a36   True      False      False      1              1                   1                     0                      2m42s
worker     rendered-worker-87ba3dec1ad78cb6aecebf7fbb476a36   True      False      False      0              0                   0                     0                      97m
----

. Pause the MCPs you do not want to update as part of the default update process.
+
[NOTE]
====
Pausing the MCP will also pause the kube-apiserver-to-kubelet-signer automatic CA certificates rotation.
New CA certificates are generation and removal happens at 292 day (from the installation day) and 365 days respectively.
The renewal and removal of the certs need reboot of the nodes.
If the MCPs are paused then the node reboots will not happen and that would make the cluster degraded.
See the link:https://access.redhat.com/articles/5651701[article] to findout how much time you have before the next automatic CA certificate rotation.
Make sure the pools are unpaused when the CA cert rotation happens.
====
+
Pausing a MCP will keep it from updating to the new OS version by the machine config operator.
+
[source,terminal]
----
$ oc patch mcp/<mcp_name> --patch '{"spec":{"paused":true}}' --type=merge
----
+
For example:
+
[source,terminal]
----
$  oc patch mcp/mcp-noupdate --patch '{"spec":{"paused":true}}' --type=merge
----
+
.Example output
[source,terminal]
----
machineconfigpool.machineconfiguration.openshift.io/mcp-noupdate patched
----
+
. Start the update process. The update process will only update the MCPs which are not paused. That includes the control plane nodes (also known as master nodes).
+
. After the control plane update is completed, unpause the MCPs you paused one at a time. Unpausing the MCP starts the update process for the pool of nodes.
+
[NOTE]
====
You can change `maxUnavailable` in a MCP to specify the percentage or the number of machines that can be updating at any given time. The default is 1.
====
+
[source,terminal]
----
$ oc patch mcp/mcp-noupdate --patch '{"spec":{"paused":false}}' --type=merge
----
Example:
[source,terminal]
----
$  oc patch mcp/mcp-noupdate --patch '{"spec":{"paused":false}}' --type=merge
machineconfigpool.machineconfiguration.openshift.io/mcpfoo patched
----
+
You can check the progress of the update in the web console (In Administrator view -> Administration -> Cluster settings ) as well as running `$ oc get machineconfigpools` CLI command.

. Test if the applications are working as expected on the newly updated MCP.


== Steps to remove a node from a MCP

A node must have a role to be properly functioning with in the OpenShift cluster.
If you want to remove a node from a MCP, you should first relabel the node as a worker as it should be part of worker MCP if is not going to be part of any other MCP. Once it is labeled as worker only then proceed to unlabel it from the MCP.

. Label it as worker if it does not have worker label
+
[source,terminal]
----
$ oc label node <node name>  node-role.kubernetes.io/worker=
----
+
. Remove the MCP label
+
[source,terminal]
----
$oc label node <node name> node-role.kubernetes.io/<mcp name>-
----
+
. The machine config operator is then going to reconcile the node to the worker pool configuration. Check the output of `oc get mcp` to make sure the worker pool is updated before going to the next step.
+
. Delete the MCP
+
[source,terminal]
----
$ oc delete mcp mcpfoo
----

== In Case Of Failure

In case of failure, keep all the MCP paused and wait for the version with the bug fix and start the update process again.

[NOTE]
====
We do not recommend updating MCPs to different versions i.e. one MCP from 4.Y.100 to 4.Y+1.10 and another 4.Y.100 to 4.Y+1.20.
This scenario is never tested and may result in to undefined cluster state.
====
