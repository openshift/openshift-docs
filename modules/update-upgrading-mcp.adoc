// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-hostpath.adoc

[id="update-upgrading-mcp_{context}"]
= Delaying the update for specific nodes

There might be some secenarios where you might not want to update specific nodes when you update a cluster. Updating nodes requires the nodes to be cordoned and drained, which marks the nodes as unschedulable and evicts all pods. For example, you can ensure that critical applications stay available during the update process, even if the update fails.
Dealying the update can also be helpful if you do not have a single maintenance window to complete the whole update process.

To deplay the update for specific nodes, before the update, create a custom machine config pool (MCP) and move the nodes the new MCP. Then, pause the MCP. The nodes associated with a paused MCP do not get updated. When you are ready to update those nodes, move the nodes back to the worker MCP, where they will be updated. If you have more than one custom MCP for updating, after the first set nodes are updated, verify that the applications on those nodes are functioning properly before updating the other nodes.

[NOTE]
====
Creating custom MCP using master nodes is not supported. The Machine Config Operator ignores the custom MCP created for the master nodes. This is required to make sure control plane nodes remain stable.
====

This slow rollout of an update to the worker nodes can be characterized as a _canary_ release.

.Procedure

To delay updating specific nodes when you update your cluster:

. Create one or more MCPs.
+
[NOTE]
====
In case of a failure i.e. if the MCP with the new version does not work as expected with the applications, the nodes in the pool can be cordoned and drained. So that the application will not run on these nodes and the extra capacity will help to maintain the quality of service of the applications.
====

.. Get the list of worker nodes.
+
[source,terminal]
----
$ oc get -l 'node-role.kubernetes.io/master!=' -o 'jsonpath={range .items[*]}{.metadata.name}{"\n"}{end}' nodes
----
+
.Example output
+
[source,terminal]
----
ci-ln-pwnll6b-f76d1-s8t9n-worker-a-s75z4
ci-ln-pwnll6b-f76d1-s8t9n-worker-b-dglj2
ci-ln-pwnll6b-f76d1-s8t9n-worker-c-lldbm
----

.. Add a custom label, such as the name of the MCP you create, as label to the worker node.
+
[source,terminal]
----
$ oc label node <node name> node-role.kubernetes.io/<custom-label>=
----
+
For example:
+
[source,terminal]
----
$ oc label node ci-ln-0qv1yp2-f76d1-kl2tq-worker-a-j2ssz node-role.kubernetes.io/mcp-noupdate=
----
+
.Example output
+
[source,terminal]
----
node/ci-ln-gtrwm8t-f76d1-spbl7-worker-a-xk76k labeled
----

.. Create the machineconfig pool
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: mcp-noupdate <1>
spec:
  machineConfigSelector:
    matchExpressions: <2>
      - {
         key: machineconfiguration.openshift.io/role
         operator: In 
         values: [worker,mcp-noupdate]
        } 
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/mcp-noupdate: "" <3>
----
<1> Specify a name for the machine config pool
<2> Speify a selector to filter keys. This selects all nodes with the `machineconfiguration.openshift.io/role` value equal to `worker` and `mcp-noupdate`.
<3> Specify the custom label you added to the node(s) you want in this machine config pool.
+
[source,terminal]
----
$ oc create -f <file_name>
----
+
.Example output
+
[source,terminal]
----
machineconfigpool.machineconfiguration.openshift.io/mcp-noupdate created
----
+
.. View the list of machine config pools in the cluster and their current state:
+
[source,terminal]
----
$ oc get machineconfigpool
----
+
.Example output
[source,terminal]
----
NAME       CONFIG                                               UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master     rendered-master-b0bb90c4921860f2a5d8a2f8137c1867     True      False      False      3              3                   3                     0                      97m
mcp-noupdate rendered-mcpfoo-87ba3dec1ad78cb6aecebf7fbb476a36   True      False      False      1              1                   1                     0                      2m42s
worker     rendered-worker-87ba3dec1ad78cb6aecebf7fbb476a36     True      False      False      2              2                   2                     2                      97m
----
+
The new machine config pool, `mcp-noupdate rendered-mcpfoo-87ba3dec1ad78cb6aecebf7fbb476a36`, is created and the number of nodes to which you added the custom label are shown.

. Pause the machine config pools() you do not want to update as part of the default update process.
+
[NOTE]
====
Pausing a machine config pool pauses the `kube-apiserver-to-kubelet-signer` automatic CA certificates rotation. New CA certificates are generated and removal happens at 292 days from the installation date and 365 days respectively. The renewal and removal of the certs requires a reboot of the nodes. If the machine config pool is paused then the node reboots, certificates are not renewed, making the cluster degraded. To determine the next automatic CA certificate rotation, see the link:https://access.redhat.com/articles/5651701[Understand CA cert auto renewal in Red Hat OpenShift 4]. Ensure that the pools not unpaused when the CA cert rotation is scheduled to occur.
====
+
[source,terminal]
----
$ oc patch mcp/<mcp_name> --patch '{"spec":{"paused":true}}' --type=merge
----
+
For example:
+
[source,terminal]
----
$  oc patch mcp/mcp-noupdate --patch '{"spec":{"paused":true}}' --type=merge
----
+
.Example output
[source,terminal]
----
machineconfigpool.machineconfiguration.openshift.io/mcp-noupdate patched
----
+
. Start the update process. The update process will only update the MCPs which are not paused. That includes the control plane nodes (also known as master nodes).
+
. After the control plane update is completed, unpause the MCPs you paused one at a time. Unpausing the MCP starts the update process for the pool of nodes.
+
[NOTE]
====
You can change `maxUnavailable` in a MCP to specify the percentage or the number of machines that can be updating at any given time. The default is 1.
====
+
[source,terminal]
----
$ oc patch mcp/mcp-noupdate --patch '{"spec":{"paused":true}}' --type=merge
----
Example:
[source,terminal]
----
$  oc patch mcp/mcp-noupdate --patch '{"spec":{"paused":true}}' --type=merge
----
+
.Example output
[source,terminal]
----
machineconfigpool.machineconfiguration.openshift.io/mcpfoo patched
----
+
You can check the progress of the update in the web console (In Administrator view -> Administration -> Cluster settings ) as well as running `$ oc get machineconfigpools` CLI command.

. Test if the applications are working as expected on the newly updated MCP.

. Unpause the machine config pool:
+
[source,terminal]
----
$ oc patch mcp/mcp-noupdate --patch '{"spec":{"paused":false}}' --type=merge
----
Example:
[source,terminal]
----
$  oc patch mcp/mcp-noupdate --patch '{"spec":{"paused":false}}' --type=merge
----
+
.Example output
[source,terminal]
----
machineconfigpool.machineconfiguration.openshift.io/mcpfoo patched
----

. Move the node back to the original MCP:

.. Ensure that the node has a `worker` label or a label from an MCP that is updated, as a node must have a role to be properly functioning in the cluster.
+
Label the node if the node has only the custom label:
+
[source,terminal]
----
$ oc label node ci-ln-0qv1yp2-f76d1-kl2tq-worker-a-j2ssz node-role.kubernetes.io/worker=
----
+
.Example output if the `worker` label is present:
+
[source,terminal]
----
error: 'node-role.kubernetes.io/worker' already has a value (), and --overwrite is false
----

.. Remove the custom label from the node. 
+
[source,terminal]
----
$ oc label node <node name> node-role.kubernetes.io/<custom-label>-
----
+
For example:
+
[source,terminal]
----
$ oc label node ci-ln-0qv1yp2-f76d1-kl2tq-worker-a-j2ssz node-role.kubernetes.io/mcp-noupdate-
----
+
.Example output
+
----
node/ci-ln-0qv1yp2-f76d1-kl2tq-worker-a-j2ssz labeled
----
+
The MCO reconciles the node to the machine config pool configuration abd moves the node(s) back to the `worker` machine config pool:
+
[source,terminal]
----
NAME           CONFIG                                                   UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master         rendered-master-1203f157d053fd987c7cbd91e3fbc0ed         True      False      False      3              3                   3                     0                      61m
mcp-noupdate   rendered-mcp-noupdate-5ad4791166c468f3a35cd16e734c9028   True      False      False      1              1                   1                     0                      21m
worker         rendered-worker-5ad4791166c468f3a35cd16e734c9028         True      False      False      3              3                   3                     0                      61m
---- 

.. Verify that the  MCP is updated:
+
+
[source,terminal]
----
$ oc get mcp
---- 

. Delete the MCP
+
[source,terminal]
----
$ oc delete mcp <mcp_name>
----

== In Case Of Failure

In case of failure, keep all the MCP paused and wait for the version with the bug fix and start the update process again.

[NOTE]
====
We do not recommend updating MCPs to different versions i.e. one MCP from 4.Y.100 to 4.Y+1.10 and another 4.Y.100 to 4.Y+1.20.
This scenario is never tested and may result in to undefined cluster state.
====
