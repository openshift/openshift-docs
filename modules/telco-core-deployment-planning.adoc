// Module included in the following assemblies:
//
// * scalability_and_performance/telco_core_ref_design_specs/telco-core-rds.adoc

:_mod-docs-content-type: REFERENCE
[id="telco-core-deployment-planning_{context}"]
= Deployment planning

*Worker nodes and machine config pools*

`MachineConfigPools` (MCPs) custom resource (CR) enable the subdivision of worker nodes in telco core clusters into different node groups based on customer planning parameters.
Careful deployment planning using MCPs is crucial to minimize deployment and upgrade time and, more importantly, to minimize interruption of telco-grade services during cluster upgrades.

*Description*

Telco core clusters can use MCPs to split worker nodes into additional separate roles, for example, due to different hardware profiles. This allows custom tuning for each role and also plays a critical function in speeding up a telco core cluster deployment or upgrade. More importantly, multiple MCPs allow you to properly plan cluster upgrades across one or many maintenance windows. This is crucial because telco-grade services may otherwise be affected if careful planning is not considered.

During cluster upgrades, you can pause MCPs while you upgrade the control plane. See "Performing a canary rollout update" for more information. This ensures that worker nodes are not rebooted and running workloads remain unaffected until the MCP is unpaused.

Using careful MCP planning, you can control the timing and order of which set of nodes are upgraded at any time. For more information on how to use MCPs to plan telco upgrades, see "Applying MachineConfigPool labels to nodes before the update".

Before beginning the initial deployment, keep the following engineering considerations in mind regarding MCPs:

When using `PerformanceProfile` definitions, remember that each MCP must be linked to exactly one `PerformanceProfile` definition or tuned profile definition.
Consequently, even if the desired configuration is identical for multiple MCPs, each MCP still requires its own dedicated `PerformanceProfile` definition.

Plan your MCP labeling with an appropriate strategy to split your worker nodes depending on considerations such as:

*  The worker node type: identifying a group of nodes with equivalent hardware profile, for example, workers for control plane Network Functions (NFs) and workers for user data plane NFs.
* The number of worker nodes per worker node type.
* The minimum number of MCPs required for an equivalent hardware profile is 1, but could be larger for larger clusters.
  For example, you may design for more MCPs per hardware profile to support a more granular upgrade where a smaller percentage of the cluster capacity is affected with each step.
* The strategy for performing updates on nodes within an MCP is shaped by upgrade requirements and the chosen `maxUnavailable` value:
** Number of maintenance windows allowed.
** Duration of a maintenance window.
** Total number of worker nodes.
** Desired `maxUnavailable` (number of nodes updated concurrently) for the MCP.
* CNF requirements for worker nodes, in terms of:
** Minimum availability per Pod required during an upgrade, configured with a pod disruption budget (PDB). PDBs are crucial to maintain telco service level Agreements (SLAs) during upgrades. For more information about PDB, see "Understanding how to use pod disruption budgets to specify the number of pods that must be up".
** Minimum true high availability required per Pod, such that each replica runs on separate hardware.
** Pod affinity and anti-affinity link: For more information about how to use pod affinity and anti-affinity, see "Placing pods relative to other pods using affinity and anti-affinity rules".
* Duration and frequency of upgrade maintenance windows during which telco-grade services may be affected.

