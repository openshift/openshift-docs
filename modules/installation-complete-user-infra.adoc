// Module included in the following assemblies:
//
// * installing/installing_bare_metal/installing-bare-metal.adoc
// * installing/installing_bare_metal/installing-restricted-networks-bare-metal.adoc
// * installing/installing_vsphere/installing-restricted-networks-vsphere.adoc
// * installing/installing_vsphere/installing-vsphere.adoc
// * installing/installing_vsphere/installing-vsphere-network-customizations.adoc
// * installing/installing_ibm_z/installing-ibm-z.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc
// * installing/installing_ibm_z/installing-ibm-z-kvm.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z-kvm.adoc
// * installing/installing_ibm_z/installing-ibm-z-lpar.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z-lpar.adoc

ifeval::["{context}" == "installing-restricted-networks-vsphere"]
:restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-bare-metal"]
:restricted:
endif::[]
ifdef::openshift-origin[]
:restricted:
endif::[]
ifeval::["{context}" == "installing-ibm-z"]
:ibm-z:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z"]
:ibm-z:
:restricted:
endif::[]
ifeval::["{context}" == "installing-ibm-z-kvm"]
:ibm-z-kvm:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z-kvm"]
:ibm-z-kvm:
:restricted:
endif::[]
ifeval::["{context}" == "installing-ibm-z-lpar"]
:ibm-z-lpar:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z-lpar"]
:ibm-z-lpar:
:restricted:
endif::[]
ifeval::["{context}" == "installing-ibm-power"]
:ibm-power:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-power"]
:ibm-power:
:restricted:
endif::[]
:_mod-docs-content-type: PROCEDURE
[id="installation-complete-user-infra_{context}"]
= Completing installation on user-provisioned infrastructure

After you complete the Operator configuration, you can finish installing the
cluster on infrastructure that you provide.

.Prerequisites

* Your control plane has initialized.
* You have completed the initial Operator configuration.

.Procedure

. Confirm that all the cluster components are online with the following command:
+
[source,terminal]
----
$ watch -n5 oc get clusteroperators
----
+
.Example output
[source,terminal,subs="attributes+"]
----
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                             {product-version}.0    True        False         False      19m
baremetal                                  {product-version}.0    True        False         False      37m
cloud-credential                           {product-version}.0    True        False         False      40m
cluster-autoscaler                         {product-version}.0    True        False         False      37m
config-operator                            {product-version}.0    True        False         False      38m
console                                    {product-version}.0    True        False         False      26m
csi-snapshot-controller                    {product-version}.0    True        False         False      37m
dns                                        {product-version}.0    True        False         False      37m
etcd                                       {product-version}.0    True        False         False      36m
image-registry                             {product-version}.0    True        False         False      31m
ingress                                    {product-version}.0    True        False         False      30m
insights                                   {product-version}.0    True        False         False      31m
kube-apiserver                             {product-version}.0    True        False         False      26m
kube-controller-manager                    {product-version}.0    True        False         False      36m
kube-scheduler                             {product-version}.0    True        False         False      36m
kube-storage-version-migrator              {product-version}.0    True        False         False      37m
machine-api                                {product-version}.0    True        False         False      29m
machine-approver                           {product-version}.0    True        False         False      37m
machine-config                             {product-version}.0    True        False         False      36m
marketplace                                {product-version}.0    True        False         False      37m
monitoring                                 {product-version}.0    True        False         False      29m
network                                    {product-version}.0    True        False         False      38m
node-tuning                                {product-version}.0    True        False         False      37m
openshift-apiserver                        {product-version}.0    True        False         False      32m
openshift-controller-manager               {product-version}.0    True        False         False      30m
openshift-samples                          {product-version}.0    True        False         False      32m
operator-lifecycle-manager                 {product-version}.0    True        False         False      37m
operator-lifecycle-manager-catalog         {product-version}.0    True        False         False      37m
operator-lifecycle-manager-packageserver   {product-version}.0    True        False         False      32m
service-ca                                 {product-version}.0    True        False         False      38m
storage                                    {product-version}.0    True        False         False      37m
----
+
Alternatively, the following command notifies you when all of the clusters are available. It also retrieves and displays credentials:
+
[source,terminal]
----
$ ./openshift-install --dir <installation_directory> wait-for install-complete <1>
----
<1> For `<installation_directory>`, specify the path to the directory that you
stored the installation files in.
+
.Example output
[source,terminal]
----
INFO Waiting up to 30m0s for the cluster to initialize...
----
+
The command succeeds when the Cluster Version Operator finishes deploying the
{product-title} cluster from Kubernetes API server.
+
[IMPORTANT]
====
* The Ignition config files that the installation program generates contain certificates that expire after 24 hours, which are then renewed at that time. If the cluster is shut down before renewing the certificates and the cluster is later restarted after the 24 hours have elapsed, the cluster automatically recovers the expired certificates. The exception is that you must manually approve the pending `node-bootstrapper` certificate signing requests (CSRs) to recover kubelet certificates. See the documentation for _Recovering from expired control plane certificates_ for more information.

* It is recommended that you use Ignition config files within 12 hours after they are generated because the 24-hour certificate rotates from 16 to 22 hours after the cluster is installed. By using the Ignition config files within 12 hours, you can avoid installation failure if the certificate update runs during installation.
====

. Confirm that the Kubernetes API server is communicating with the pods.
.. To view a list of all pods, use the following command:
+
[source,terminal]
----
$ oc get pods --all-namespaces
----
+
.Example output
[source,terminal]
----
NAMESPACE                         NAME                                            READY   STATUS      RESTARTS   AGE
openshift-apiserver-operator      openshift-apiserver-operator-85cb746d55-zqhs8   1/1     Running     1          9m
openshift-apiserver               apiserver-67b9g                                 1/1     Running     0          3m
openshift-apiserver               apiserver-ljcmx                                 1/1     Running     0          1m
openshift-apiserver               apiserver-z25h4                                 1/1     Running     0          2m
openshift-authentication-operator authentication-operator-69d5d8bf84-vh2n8        1/1     Running     0          5m
...
----

.. View the logs for a pod that is listed in the output of the previous command by using the following command:
+
[source,terminal]
----
$ oc logs <pod_name> -n <namespace> <1>
----
<1> Specify the pod name and namespace, as shown in the output of the previous
command.
+
If the pod logs display, the Kubernetes API server can communicate with the
cluster machines.

ifndef::ibm-power[]
. For an installation with Fibre Channel Protocol (FCP), additional steps are required to enable multipathing. Do not enable multipathing during installation.
endif::ibm-power[]
ifdef::ibm-power[]
. Additional steps are required to enable multipathing. Do not enable multipathing during installation.
endif::ibm-power[]
+
See "Enabling multipathing with kernel arguments on {op-system}" in the _Postinstallation machine configuration tasks_ documentation for more information.

ifdef::restricted[]
. Register your cluster on the link:https://console.redhat.com/openshift/register[Cluster registration] page.
endif::restricted[]

ifdef::ibm-z,ibm-z-lpar[]
.Verification

If you have enabled secure boot during the {product-title} bootstrap process, the following verification steps are required:

. Debug the node by running the following command:
+
[source,terminal]
----
$ oc debug node/<node_name>
chroot /host
----
+
. Confirm that secure boot is enabled by running the following command:
+
[source,terminal]
----
$ cat /sys/firmware/ipl/secure
----
+
.Example output
[source,terminal]
----
1 <1>
----
<1> The value is `1` if secure boot is enabled and `0` if secure boot is not enabled.
endif::ibm-z,ibm-z-lpar[]
ifdef::ibm-z-lpar[]
. List the re-IPL configuration by running the following command:
+
[source,terminal]
----
# lsreipl
----
+
.Example output for an FCP disk
[source,terminal]
----
Re-IPL type: fcp
WWPN: 0x500507630400d1e3
LUN: 0x4001400e00000000
Device: 0.0.810e
bootprog: 0
br_lba: 0
Loadparm: ""
Bootparms: ""
clear: 0
----
+
.Example output for a DASD disk
[source,terminal]
----
for DASD output:
Re-IPL type: ccw
Device: 0.0.525d
Loadparm: ""
clear: 0
----

. Shut down the node by running the following command:
+
[source,terminal]
----
sudo shutdown -h
----

. Initiate a boot from LPAR from the Hardware Management Console (HMC). See link:https://www.ibm.com/docs/en/linux-on-systems?topic=boot-lpar[Initiating a secure boot from an LPAR] in IBM documentation.

. When the node is back, check the secure boot status again.
endif::ibm-z-lpar[]

ifeval::["{context}" == "installing-restricted-networks-vsphere"]
:!restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-bare-metal"]
:!restricted:
endif::[]
ifdef::openshift-origin[]
:!restricted:
endif::[]
ifeval::["{context}" == "installing-ibm-z"]
:!ibm-z:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z"]
:!ibm-z:
:!restricted:
endif::[]
ifeval::["{context}" == "installing-ibm-power"]
:!ibm-power:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-power"]
:!ibm-power:
:!restricted:
endif::[]
ifeval::["{context}" == "installing-ibm-z-kvm"]
:!ibm-z-kvm:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z-kvm"]
:!ibm-z-kvm:
:!restricted:
endif::[]
ifeval::["{context}" == "installing-ibm-z-lpar"]
:!ibm-z-lpar:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z-lpar"]
:!ibm-z-lpar:
:!restricted:
endif::[]
