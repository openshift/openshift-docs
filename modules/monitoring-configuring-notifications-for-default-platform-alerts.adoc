// Module included in the following assemblies:
//
// * observability/monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-notifications-for-default-platform-alerts_{context}"]
= Configuring notifications for default platform alerts

You can configure Alertmanager to send notifications. Customize where and how Alertmanager sends notifications about default platform alerts by editing the default configuration in the `alertmanager-main` secret in the `openshift-monitoring` namespace.

[IMPORTANT]
====
Alertmanager does not send notifications by default. It is recommended to configure Alertmanager to receive notifications by setting up notifications details in the `alertmanager-main` secret configuration file.
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.

.Procedure

. Open the Alertmanager YAML configuration file:

** To open the Alertmanager configuration from the CLI:

.. Print the currently active Alertmanager configuration from the `alertmanager-main` secret into `alertmanager.yaml` file:
+
[source,terminal]
----
$ oc -n openshift-monitoring get secret alertmanager-main --template='{{ index .data "alertmanager.yaml" }}' | base64 --decode > alertmanager.yaml
----

.. Open the `alertmanager.yaml` file.

** To open the Alertmanager configuration from the {product-title} web console:

.. Go to the *Administration* -> *Cluster Settings* -> *Configuration* -> *Alertmanager* -> *YAML* page of the web console.

. Edit the Alertmanager configuration by updating parameters in the YAML:
+
[source,yaml]
----
global:
  resolve_timeout: 5m
route:
  group_wait: 30s #<1>
  group_interval: 5m #<2>
  repeat_interval: 12h #<3>
  receiver: default
  routes:
  - matchers:
    - "alertname=Watchdog"
    repeat_interval: 2m
    receiver: watchdog
  - matchers:
    - "service=<your_service>" #<4>
    routes:
    - matchers:
      - <your_matching_rules> #<5>
      receiver: <receiver> #<6>
receivers:
- name: default
- name: watchdog
- name: <receiver>
  <receiver_configuration> #<7>
----
<1> Specify how long Alertmanager waits while collecting initial alerts for a group of alerts before sending a notification.
<2> Specify how much time must elapse before Alertmanager sends a notification about new alerts added to a group of alerts for which an initial notification was already sent.
<3> Specify the minimum amount of time that must pass before an alert notification is repeated.
If you want a notification to repeat at each group interval, set the `repeat_interval` value to less than the `group_interval` value.
The repeated notification can still be delayed, for example, when certain Alertmanager pods are restarted or rescheduled.
<4> Specify the name of the service that fires the alerts.
<5> Specify labels to match your alerts.
<6> Specify the name of the receiver to use for the alerts.
<7> Specify the receiver configuration.
+
[IMPORTANT]
====
* Use the `matchers` key name to indicate the matchers that an alert has to fulfill to match the node.
Do not use the `match` or `match_re` key names, which are both deprecated and planned for removal in a future release.

* If you define inhibition rules, use the following key names:
+
--
** `target_matchers`: to indicate the target matchers
** `source_matchers`: to indicate the source matchers
--
+
Do not use the `target_match`, `target_match_re`, `source_match`, or `source_match_re` key names, which are deprecated and planned for removal in a future release.
====
+
The following Alertmanager configuration example configures PagerDuty as an alert receiver:
+
[source,yaml]
----
global:
  resolve_timeout: 5m
route:
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: default
  routes:
  - matchers:
    - "alertname=Watchdog"
    repeat_interval: 2m
    receiver: watchdog
  - matchers:
    - "service=example-app"
    routes:
    - matchers:
      - "severity=critical"
      receiver: team-frontend-page
receivers:
- name: default
- name: watchdog
- name: team-frontend-page
  pagerduty_configs:
  - service_key: "<your_key>"
----
+
With this configuration, alerts of `critical` severity that are fired by the `example-app` service are sent through the `team-frontend-page` receiver. Typically, these types of alerts would be paged to an individual or a critical response team.

. Apply the new configuration in the file:

** To apply the changes from the CLI, run the following command:
+
[source,terminal]
----
$ oc -n openshift-monitoring create secret generic alertmanager-main --from-file=alertmanager.yaml --dry-run=client -o=yaml |  oc -n openshift-monitoring replace secret --filename=-
----

** To apply the changes from the {product-title} web console, click *Save*.
