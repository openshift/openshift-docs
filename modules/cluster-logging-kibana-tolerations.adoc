// Module included in the following assemblies:
//
// * logging/cluster-logging-kibana.adoc

[id="cluster-logging-kibana-tolerations_{context}"]
= Using tolerations to control the Kibana Pod placement

You can control which nodes the Kibana Pods run on and prevent 
other workloads from using those nodes by using tolerations on the Pods.

You apply tolerations to the Kibana Pods through the Cluster Logging Custom Resource (CR)
and apply taints to a node through the node specification. A taint on a node is a `key:value pair` that 
instructs the node to repel all Pods that do not tolerate the taint. Using a specific `key:value` pair
that is not on other Pods ensures only the Kibana Pod can run on that node.

.Prerequisites

* Cluster logging and Elasticsearch must be installed.

.Procedure

. Use the following command to add a taint to a node where you want to schedule the Kibana Pod:
+
----
$ oc adm taint nodes <node-name> <key>=<value>:<effect>
----
+
For example:
+
----
$ oc adm taint nodes node1 kibana=node:NoExecute
----
+
This example places a taint on `node1` that has key `kibana`, value `node`, and taint effect `NoExecute`.
You must use the `NoExecute` taint effect. `NoExecute` schedules only Pods that match the taint and remove existing Pods
that do not match.

. Edit the `visualization` section of the Cluster Logging Custom Resource (CR) to configure a toleration for the Kibana Pod:
+
[source, yaml]
----
  visualization:
    type: "kibana" 
    kibana:
      tolerations: 
      - key: "kibana"  <1>
        operator: "Exists"  <2>
        effect: "NoExecute"  <3>
        tolerationSeconds: 6000 <4>
----
<1> Specify the key that you added to the node.
<2> Specify the `Exists` operator to require the `key`/`value`/`effect` parameters to match. 
<3> Specify the `NoExecute` effect.
<4> Optionally, specify the `tolerationSeconds` parameter to set how long a Pod can remain bound to a node before being evicted.


This toleration matches the taint created by the `oc adm taint` command. A Pod with this toleration would be able to schedule onto `node1`.

