// Module included in the following assemblies:
//
// * logging/cluster-logging-log-store.adoc

[id="cluster-logging-log-store-comp-viewing_{context}"]
= Viewing the status of the log store

You can view the status of your log store.

.Prerequisites

* Cluster logging and Elasticsearch must be installed.

.Procedure

. Change to the `openshift-logging` project.
+
[source,terminal]
----
$ oc project openshift-logging
----

. To view the status:

.. Get the name of the log store instance:
+
[source,terminal]
----
$ oc get Elasticsearch
----
+
.Example output
[source,terminal]
----
NAME            AGE
elasticsearch   5h9m
----

.. Get the log store status:
+
[source,terminal]
----
$ oc get Elasticsearch <Elasticsearch-instance> -o yaml
----
+
For example:
+
[source,terminal]
----
$ oc get Elasticsearch elasticsearch -n openshift-logging -o yaml
----
+
The output includes information similar to the following:
+
.Example output
[source,terminal]
----
status: <1>
  cluster: <2>
    activePrimaryShards: 30
    activeShards: 60
    initializingShards: 0
    numDataNodes: 3
    numNodes: 3
    pendingTasks: 0
    relocatingShards: 0
    status: green
    unassignedShards: 0
  clusterHealth: ""
  conditions: [] <3>
  nodes: <4>
  - deploymentName: elasticsearch-cdm-zjf34ved-1
    upgradeStatus: {}
  - deploymentName: elasticsearch-cdm-zjf34ved-2
    upgradeStatus: {}
  - deploymentName: elasticsearch-cdm-zjf34ved-3
    upgradeStatus: {}
  pods: <5>
    client:
      failed: []
      notReady: []
      ready:
      - elasticsearch-cdm-zjf34ved-1-6d7fbf844f-sn422
      - elasticsearch-cdm-zjf34ved-2-dfbd988bc-qkzjz
      - elasticsearch-cdm-zjf34ved-3-c8f566f7c-t7zkt
    data:
      failed: []
      notReady: []
      ready:
      - elasticsearch-cdm-zjf34ved-1-6d7fbf844f-sn422
      - elasticsearch-cdm-zjf34ved-2-dfbd988bc-qkzjz
      - elasticsearch-cdm-zjf34ved-3-c8f566f7c-t7zkt
    master:
      failed: []
      notReady: []
      ready:
      - elasticsearch-cdm-zjf34ved-1-6d7fbf844f-sn422
      - elasticsearch-cdm-zjf34ved-2-dfbd988bc-qkzjz
      - elasticsearch-cdm-zjf34ved-3-c8f566f7c-t7zkt
  shardAllocationEnabled: all
----
<1> In the output, the cluster status fields appear in the `status` stanza.
<2> The status of the log store: 
+
* The number of active primary shards.
* The number of active shards. 
* The number of shards that are initializing.
* The number of log store data nodes.
* The total number of log store nodes.
* The number of pending tasks.
* The log store status: `green`, `red`, `yellow`.
* The number of unassigned shards.
<3> Any status conditions, if present. The log store status indicates the reasons from the scheduler if a pod could not be placed. Any events related to the following conditions are shown:
* Container Waiting for both the log store and proxy containers.
* Container Terminated for both the log store and proxy containers.
* Pod unschedulable.
Also, a condition is shown for a number of issues, see *Example condition messages*.
<4> The log store nodes in the cluster, with `upgradeStatus`.  
<5> The log store client, data, and master pods in the cluster, listed under 'failed`, `notReady` or `ready` state.

[id="cluster-logging-elasticsearch-status-message_{context}"]
== Example condition messages

The following are examples of some condition messages from the `Status` section of the Elasticsearch instance.

// https://github.com/openshift/elasticsearch-operator/pull/92

This status message indicates a node has exceeded the configured low watermark and no shard will be allocated to this node.

[source,yaml]
----
status:
  nodes:
  - conditions:
    - lastTransitionTime: 2019-03-15T15:57:22Z
      message: Disk storage usage for node is 27.5gb (36.74%). Shards will be not
        be allocated on this node.
      reason: Disk Watermark Low
      status: "True"
      type: NodeStorage
    deploymentName: example-elasticsearch-cdm-0-1
    upgradeStatus: {}
----

This status message indicates a node has exceeded the configured high watermark and shards will be relocated to other nodes.

[source,yaml]
----
status:
  nodes:
  - conditions:
    - lastTransitionTime: 2019-03-15T16:04:45Z
      message: Disk storage usage for node is 27.5gb (36.74%). Shards will be relocated
        from this node.
      reason: Disk Watermark High
      status: "True"
      type: NodeStorage
    deploymentName: example-elasticsearch-cdm-0-1
    upgradeStatus: {}
----

This status message indicates the log store node selector in the CR does not match any nodes in the cluster:

[source,yaml]
----
status:
    nodes:
    - conditions:
      - lastTransitionTime: 2019-04-10T02:26:24Z
        message: '0/8 nodes are available: 8 node(s) didn''t match node selector.'
        reason: Unschedulable
        status: "True"
        type: Unschedulable
----

This status message indicates that the log store CR uses a non-existent PVC.

[source,yaml]
----
status:
   nodes:
   - conditions:
     - last Transition Time:  2019-04-10T05:55:51Z
       message:               pod has unbound immediate PersistentVolumeClaims (repeated 5 times)
       reason:                Unschedulable
       status:                True
       type:                  Unschedulable
----

This status message indicates that your log store cluster does not have enough nodes to support your log store redundancy policy.

[source,yaml]
----
status:
  clusterHealth: ""
  conditions:
  - lastTransitionTime: 2019-04-17T20:01:31Z
    message: Wrong RedundancyPolicy selected. Choose different RedundancyPolicy or
      add more nodes with data roles
    reason: Invalid Settings
    status: "True"
    type: InvalidRedundancy
----

This status message indicates your cluster has too many master nodes:

[source,yaml]
----
status:
  clusterHealth: green
  conditions:
    - lastTransitionTime: '2019-04-17T20:12:34Z'
      message: >-
        Invalid master nodes count. Please ensure there are no more than 3 total
        nodes with master roles
      reason: Invalid Settings
      status: 'True'
      type: InvalidMasters
----
