[id="cluster-logging-release-notes-5-1-0"]
= OpenShift Logging 5.1.0

This release includes link:https://access.redhat.com/errata/RHSA-2021:2112[RHSA-2021:2112 OpenShift Logging Bug Fix Release 5.1.0]


[id="openshift-logging-5-1-0-bug-fixes"]
== Bug fixes

* Previously, the `ClusterLogForwarder` CR did not show the `input[].selector` element after it had been created. The current release fixes this issue. When you specify a `selector` in the `ClusterLogForwarder` CR, it remains. Fixing this bug was necessary for link:https://issues.redhat.com/browse/LOG-883[LOG-883], which enables using pod label selectors to forward application log data.
(link:https://issues.redhat.com/browse/LOG-1338[*LOG-1338*])

* Previously, an update in the cluster service version (CSV) accidentally introduced resources and limits for the OpenShift Elasticsearch operator container. Under specific conditions, this caused an out-of-memory condition that terminated the Elasticsearch operator pod. The current release fixes this issue by removing the CSV resources and limits for the operator container. Now, the operator gets scheduled without issues.
(link:https://issues.redhat.com/browse/LOG-1254[*LOG-1254*])

* Previously, forwarding logs to Kafka using chained certificates failed with the following error message:
+
`state=error: certificate verify failed (unable to get local issuer certificate)`
+
Logs could not be forwarded to a Kafka broker with a certificate signed by an intermediate CA. This happened because fluentd Kafka plugin could only handle a single CA certificate supplied in the `ca-bundle.crt` entry of the corresponding secret. The current release fixes this issue by enabling the fluentd Kafka plugin to handle multiple CA certificates supplied in the `ca-bundle.crt` entry of the corresponding secret. Now, logs can be forwarded to a Kafka broker with a certificate signed by an intermediate CA.
(link:https://issues.redhat.com/browse/LOG-1218[*LOG-1218*], link:https://issues.redhat.com/browse/LOG-1216[*LOG-1216*])

* Previously, while under load, Elasticsearch responded to some requests with an HTTP 500 error, even though there was nothing wrong with the cluster. Retrying the request was successful. This release fixes the issue by updating the cron jobs to be more resilient when they encounter temporary HTTP 500 errors. The updated cron jobs will first retry a request multiple times, before failing.
(link:https://issues.redhat.com/browse/LOG-1215[*LOG-1215*])

* If you did not set the `.proxy` value in the cluster installation configuration, and then configured a global proxy on the installed cluster, a bug prevented Fluentd from forwarding logs to Elasticsearch. To work around this issue, in the proxy or cluster configuration, set the `no_proxy` value to `.svc.cluster.local` so it skips internal traffic. The current release fixes the proxy configuration issue. Now, if you configure the global proxy after installing an OpenShift cluster, Fluentd forwards logs to Elasticsearch.
(link:https://issues.redhat.com/browse/LOG-1187[*LOG-1187*], link:https://bugzilla.redhat.com/show_bug.cgi?id=1915448[*BZ#1915448*])

* Previously, the logging collector was creating more socket connections than necessary. With this fix, the logging collector re-uses the existing socket connection to send logs.
(link:https://issues.redhat.com/browse/LOG-1186[*LOG-1186*])

* As a cluster administrator, you can use Kubernetes pod labels to gather log data from an application and send it to a specific log collector. You can gather log data by configuring the `inputs[].name.application.selector.matchLabels` element in the `ClusterLogForwarder` custom resource (CR) YAML file. You can also filter the gathered log data by namespace.
(link:https://issues.redhat.com/browse/LOG-883[*LOG-883*])
