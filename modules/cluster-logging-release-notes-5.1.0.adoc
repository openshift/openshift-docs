[id="cluster-logging-release-notes-5-1-0"]
= OpenShift Logging 5.1.0

This release includes link:https://access.redhat.com/errata/RHSA-2021:NNNN[RHSA-2021:NNNN Bug Fix Advisory for OpenShift Logging 5.1.0]


[id="openshift-logging-5-1-0-bug-fixes"]
== Bug fixes

* Related to 5.1 Epic - Expose ability to forward logs only from specific pods via a label selector inside the Log Forwarding API.
(link:https://issues.redhat.com/browse/LOG-1338[*LOG-1338*])

* Previously, an update in the cluster service version (CSV) accidentally introduced resources and limits for the OpenShift Elasticsearch operator container. Under specific conditions, this caused an out-of-memory condition that terminated the Elasticsearch operator pod. The current release fixes this issue by removing the CSV resources and limits for the operator container. Now, the operator gets scheduled without issues.
(link:https://issues.redhat.com/browse/LOG-1254[*LOG-1254*])

* Previously, forwarding logs to Kafka using chained certificates failed with error "state=error: certificate verify failed (unable to get local issuer certificate)." Logs could not be forwarded to a Kafka broker with a certificate signed by an intermediate CA. This happened because fluentd Kafka plugin could only handle a single CA certificate supplied in the ca-bundle.crt entry of the corresponding secret. The current release fixes this issue. It enables the fluentd Kafka plugin to handle multiple CA certificates supplied in the ca-bundle.crt entry of the corresponding secret. Now, logs can be forwarded to a Kafka broker with a certificate signed by an intermediate CA.
(link:https://issues.redhat.com/browse/LOG-1218[*LOG-1218*], link:https://issues.redhat.com/browse/LOG-1216[*LOG-1216*])

* Previously, while under load, Elasticsearch responded to some requests with an HTTP 500 error, even though there was nothing wrong with the cluster. Retrying the request was successful. This release fixes the issue by updating the cron jobs to be more resilient when they encounter temporary HTTP 500 errors. They will retry a request multiple times first before failing.
(link:https://issues.redhat.com/browse/LOG-1215[*LOG-1215*])

*
(link:https://issues.redhat.com/browse/LOG-1192[*LOG-1192*])

* If you did not set `.proxy` in the cluster installation configuration, and then configured a global proxy on the installed cluster, a bug prevented Fluentd from forwarding logs to Elasticsearch. To work around this issue, in the proxy/cluster configuration, set `no_proxy` to `.svc.cluster.local` so it skips internal traffic. The current release fixes the proxy configuration issue. Now, if you configure the global proxy after installing an OpenShift cluster, Fluentd forwards logs to Elasticsearch.
(link:https://issues.redhat.com/browse/LOG-1187[*LOG-1187*], link:https://bugzilla.redhat.com/show_bug.cgi?id=1915448[*BZ#1915448*])

* Previously, the logging collector is creating more socket connections than necessary. With this fix, the logging collector uses the existing socket connection to send logs.
(link:https://issues.redhat.com/browse/LOG-1186[*LOG-1186*])

[id="openshift-logging-5-1-0-known-issues"]
== Known issues

* Fluentd pods with the `ruby-kafka-1.1.0` and `fluent-plugin-kafka-0.13.1` gems are not compatible with Apache Kafka version 0.10.1.0.
+
As a result, log forwarding to Kafka fails with a message: `error_class=Kafka::DeliveryFailed error="Failed to send messages to flux-openshift-v4/1"`
+
The `ruby-kafka-0.7` gem dropped support for Kafka 0.10 in favor of native support for Kafka 0.11. The `ruby-kafka-1.0.0` gem added support for Kafka 2.3 and 2.4. The current version of OpenShift Logging tests and therefore supports Kafka version 2.4.1.
+
To work around this issue, upgrade to a supported version of Apache Kafka.
