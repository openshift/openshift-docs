// Module included in the following assemblies:
// Epic CNF-3901 (CNF-2133) (4.11), Story TELCODOCS-339
// * scalability_and_performance/cnf-talm-for-cluster-upgrades.adoc

:_content-type: PROCEDURE
[id="talo-backup-start_and_update_{context}"]
= Creating a ClusterGroupUpgrade CR with backup

For {sno}, you can create a backup of a deployment before an upgrade. If the upgrade fails you can use the `upgrade-recovery.sh` script generated by {cgu-operator-first} to return the system to its preupgrade state.
The backup consists of the following items:

Cluster backup:: A snapshot of `etcd` and static pod manifests.
Content backup:: Backups of folders, for example, `/etc`, `/usr/local`, `/var/lib/kubelet`.
Changed files backup:: Any files managed by `machine-config` that have been changed.
Deployment:: A pinned `ostree` deployment.
Images (Optional):: Any container images that are in use.


.Prerequisites

* Install the {cgu-operator-first}.
* Provision one or more managed clusters.
* Log in as a user with `cluster-admin` privileges.
* Install {rh-rhacm-first}.

[NOTE]
====
It is highly recommended that you create a recovery partition.
The following is an example `SiteConfig` custom resource (CR) for a recovery partition of 50 GB:

[source,yaml]
----
nodes:
    - hostName: "snonode.sno-worker-0.e2e.bos.redhat.com"
    role: "master"
    rootDeviceHints:
        hctl: "0:2:0:0"
        deviceName: /dev/sda
........
........
    #Disk /dev/sda: 893.3 GiB, 959119884288 bytes, 1873281024 sectors
    diskPartition:
        - device: /dev/sda
        partitions:
        - mount_point: /var/recovery
            size: 51200
            start: 800000
----
====

.Procedure

. Save the contents of the `ClusterGroupUpgrade` CR with the `backup` and `enable` fields set to `true` in the `clustergroupupgrades-group-du.yaml` file:
+
[source,yaml]
----
apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: du-upgrade-4918
  namespace: ztp-group-du-sno
spec:
  preCaching: true
  backup: true
  clusters:
  - cnfdb1
  - cnfdb2
  enable: true
  managedPolicies:
  - du-upgrade-platform-upgrade
  remediationStrategy:
    maxConcurrency: 2
    timeout: 240
----

. To start the update, apply the `ClusterGroupUpgrade` CR by running the following command:
+
[source,terminal]
----
$ oc apply -f clustergroupupgrades-group-du.yaml
----

.Verification

* Check the status of the upgrade in the hub cluster by running the following command:
+
[source,terminal]
----
$ oc get cgu -n ztp-group-du-sno du-upgrade-4918 -o jsonpath='{.status}'
----
+
.Example output
+
[source,json]
----
{
    "backup": {
        "clusters": [
            "cnfdb2",
            "cnfdb1"
    ],
    "status": {
        "cnfdb1": "Succeeded",
        "cnfdb2": "Failed" <1>
    }
},
"computedMaxConcurrency": 1,
"conditions": [
    {
        "lastTransitionTime": "2022-04-05T10:37:19Z",
        "message": "Backup failed for 1 cluster", <2>
        "reason": "PartiallyDone", <3>
        "status": "True", <4>
        "type": "Succeeded"
    }
],
"precaching": {
    "spec": {}
},
"status": {}
----
<1> Backup has failed for one cluster.
<2> The message confirms that the backup failed for one cluster.
<3> The backup was partially successful.
<4> The backup process has finished.
