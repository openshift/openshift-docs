// Module included in the following assemblies:
//
// * logging/cluster-logging-external.adoc

[id="cluster-logging-collector-legacy_{context}"]
= Forwarding logs using the legacy configuration method

You can configure cluster logging to send logs to destinations outside of your {product-title} cluster instead of the default Elasticsearch log store using Fluentd or syslog by creating a configuration file and ConfigMap.

[IMPORTANT]
====
These methods for forwarding logs are deprecated in {product-title} and will be removed in a future release.
====

Sending logs using the Fluentd forward protocol::

You can use the Fluentd *forward* protocol to send a copy of your logs to an external log aggregator configured to accept the protocol, instead of the default Elasticsearch log store. You are responsible for configuring the external log aggregator to receive the logs from {product-title}. 

ifdef::openshift-origin[]
The *forward* protocols are provided with the Fluentd image as of v1.4.0.
endif::openshift-origin[]

To configure {product-title} to send logs using the Fluentd *forward* protocol, create a configuration file called `secure-forward.conf`, that points to an external log aggregator. Then, use that file to create a ConfigMap called called `secure-forward` in the `openshift-logging` namespace, which {product-title} uses when forwarding the logs. 

.Sample Fluentd configuration file

[source,yaml]
----
<store>
  @type forward
  <security>
    self_hostname fluentd.example.com 
    shared_key "fluent-receiver"
  </security>
  transport tls
  tls_verify_hostname false
  tls_cert_path '/etc/ocp-forward/ca-bundle.crt'
  <buffer>
    @type file
    path '/var/lib/fluentd/secureforwardlegacy'
    queued_chunks_limit_size "1024"
    chunk_limit_size "1m"
    flush_interval "5s"
    flush_at_shutdown "false"
    flush_thread_count "2"
    retry_max_interval "300"
    retry_forever true
    overflow_action "exception"
  </buffer>
  <server>
    host fluent-receiver.example.com
    port 24224
  </server>
</store>
----

Sending logs using syslog RFC3164 or RFC5424 protocol:: 

You can use the Fluentd *syslog* RFC3164 or RFC5424 protocol to send a copy of your logs to an external log aggregator configured to accept the protocol, instead of the default Elasticsearch log store. You are responsible for configuring the external log aggregator to receive the logs from {product-title}. 

[NOTE]
====
The *syslog* RFC3164 protocol does not provide Kubernetes metadata, systemd data, or other metadata.
====

There are two versions of the *syslog* protocol:

* *out_syslog*: The non-buffered implementation, which communicates through UDP, does not buffer data and writes out results immediately.
* *out_syslog_buffered*: The buffered implementation, which communicates through TCP and link:https://docs.fluentd.org/buffer[buffers data into chunks].

To configure log forwarding using the *syslog* protocol, create a configuration file called `syslog.conf`, with the information needed to forward the logs. Then, use that file to create a ConfigMap called `syslog` in the `openshift-logging` namespace, which {product-title} uses when forwarding the logs.

.Sample syslog configuration file
[source,yaml]
----
<store>
@type syslog_buffered
remote_syslog rsyslogserver.example.com
port 514
hostname fluentd-4nzfz
remove_tag_prefix tag
tag_key ident,systemd.u.SYSLOG_IDENTIFIER
facility local0
severity info
use_record true
payload_key message
rfc 3164
</store>
----

You can configure the following for the `syslog` outputs. For more information, see the syslog link:https://tools.ietf.org/html/rfc3164[RFC3164] or link:https://tools.ietf.org/html/rfc5424[RFC5424] RFC. 

* facility: The link:https://tools.ietf.org/html/rfc5424#section-6.2.1[syslog facility]. The value can be a decimal integer or a case-insensitive keyword:
** `0` or `kern` for kernel messages.
** `1` or `user` for user-level messages. This is the default.
** `2` or `mail` for the mail system..
** `3` or `daemon` for system daemons.
** `4` or `auth` for security/authentication messages.
** `5` or `syslog` for messages generated internally by syslogd.
** `6` or `lpr` for line printer subsystem.
** `7` or `news` for the network news subsystem.
** `8` or `uucp` for the UUCP subsystem.
** `9` or `cron` for the clock daemon.
** `10` or `authpriv` for security authentication messages.
** `11` or `ftp` for the FTP daemon.
** `12` or `ntp` for the NTP subsystem.
** `13` or `security` for log audit.
** `14` or `console` for log alert.
** `15` or `solaris-cron` for the scheduling daemon.
** `16`–`23` or `local0` – `local7` for locally used facilities
* payloadKey: The record field to use as payload for the syslog message.
* rfc: The RFC to be used for sending log using syslog. The default is RFC5424.
* severity: The link:https://tools.ietf.org/html/rfc5424#section-6.2.1[syslog severity] to set on outgoing syslog records. The value can be a decimal integer or a case-insensitive keyword:
** `0` or `Emergency` for messages indicating the system is unusable.
** `1` or `Alert` for messages indicating action must be taken immediately.
** `2` or `Critical` for messages indicating critical conditions.
** `3` or `Error` for messages indicating error conditions.
** `4` or `Warning` for messages indicating warning conditions.
** `5` or `Notice` for messages indicating normal but significant condition.
** `6` or `Informational` for messages indicating informational messages.
** `7` or `Debug` for messages indicating debug-level messages. This is the default.
* tag: Tag specifies a record field to use as tag on the syslog message.
* trimPrefix: Remove the specified prefix from the tag.

The following parameters apply to RFC5424:

* appName: The APP-NAME part of the syslog-msg header. Must be specified for `RFC5424`.
* msgID: The MSGID part of the syslog-msg header. Must be specified for `RFC5424`.
* procID: The PROCID part of the syslog-msg header. Must be specified for `RFC5424`.


.Procedure

To configure {product-title} to forward logs using the legacy configuration methods:

. Create a configuration file: 

* For syslog, name the configuration file `syslog.conf` and specify parameters similar to the following within the `<store>` stanza:
+
----
<store>
@type <tpye> <1>
remote_syslog <syslog-server> <2>
port 514 <3>
hostname <host> <4>
tag_key <key> <5>
remove_tag_prefix <prefix>
tag_key <key>
facility <value>
severity <value>
use_record <value>
payload_key message
rfc <value> <6> 
</store>
----
<1> Specify the protocol to use, either: `syslog` or `syslog_buffered`. 
<2> Specify the FQDN or IP address of the syslog server.
<3> Specify the port of the syslog server.
<4> Specify a name for this syslog server.
<5> Optional. Specify the appropriate syslog parameters, for example:
** Parameter to remove the specified prefix from the tag. Defaults to `''` (empty).
** Parameter to  remove the specified `tag` field from the syslog prefix.
** Parameter to set the specified field as the syslog key.
** Parameter to specify the syslog log facility or source.
** Parameter to specify the syslog log severity. 
** Parameter to use the severity and facility from the record if available. If `true`, the `container_name`, `namespace_name`, and `pod_name` are included in the output content.
** Parameter to specify the key to set the payload of the syslog message. Defaults to `message`.
<6> Specify the RFC protocol to use: either `3164` or `5124`.

* For Fluentd, name the configuration file `secure-forward` and specify parameters similar to the following within the `<store>` stanza:
+
[source,yaml]
----
<store>
  @type forward
  <security>
    self_hostname <common-name> <1>
    shared_key <key> <2>
  </security>
  transport tls <3>
  tls_verify_hostname <value> <4>
  tls_cert_path <path_to_file> <5>
  <buffer> <6>
    @type file 
    path '/var/lib/fluentd/secureforwardlegacy'
    queued_chunks_limit_size "#{ENV['BUFFER_QUEUE_LIMIT'] || '1024' }"
    chunk_limit_size "#{ENV['BUFFER_SIZE_LIMIT'] || '1m' }"
    flush_interval "#{ENV['FORWARD_FLUSH_INTERVAL'] || '5s'}"
    flush_at_shutdown "#{ENV['FLUSH_AT_SHUTDOWN'] || 'false'}"
    flush_thread_count "#{ENV['FLUSH_THREAD_COUNT'] || 2}"
    retry_max_interval "#{ENV['FORWARD_RETRY_WAIT'] || '300'}"
    retry_forever true
    overflow_action "#{ENV['BUFFER_QUEUE_FULL_ACTION'] || 'exception'}"
  </buffer>
  <server>
    name <7>
    host <8>
    hostlabel <9>
    port <10>
  </server>
  <server> <11>
    name
    host
  </server>
----
<1> Specify the default value of the auto-generated certificate common name (CN).
<2> Enter the Shared key between nodes
<3> Specify `tls` to enable TLS validation.
<4> Set to `true` to verify the server cert hostname. Set to `false` to ignore server cert hostname.
<5> Specify the path to private CA certificate file as `/etc/ocp-forward/ca_cert.pem`.
<6> Specify the link:https://docs.fluentd.org/configuration/buffer-section[Fluentd buffer parameters] as needed.
<7> Optionally, enter a name for this server.
<8> Specify the host name or IP of the server.
<9> Specify the host label of the server.
<10> Specify the port of the server.
<11> Optionally, add additional servers. 
If you specify two or more servers, *forward* uses these server nodes in a round-robin order.
+
To use mTLS, see the link:https://docs.fluentd.org/output/forward#tips-and-tricks[Fluentd documentation] for information about client certificate, key parameters, and other settings.


. Create a ConfigMap in the `openshift-logging` namespace from the configuration file:
+
* For syslog, name the ConfigMap `syslog`:
+
[source,terminal]
----
$ oc create configmap syslog --from-file=syslog.conf -n openshift-logging
----
+
* For Fluentd, name the ConfigMap `secure-forward`:
+
[source,terminal]
----
$ oc create configmap secure-forward --from-file=secure-forward.conf -n openshift-logging
----

The Cluster Logging Operator redeploys the Fluentd Pods. If the Pods do not redeploy, you can delete the Fluentd
Pods to force them to redeploy.

[source,terminal]
----
$ oc delete pod --selector logging-infra=fluentd
----
