// Module included in the following assemblies:
//
// * architecture/nvidia-gpu-architecture-overview.adoc

:_mod-docs-content-type: CONCEPT
[id="nvidia-gpu-vsphere_{context}"]
= GPUs and vSphere

You can deploy {product-title} on an NVIDIA-certified VMware vSphere server that can host different GPU types.

An NVIDIA GPU driver must be installed in the hypervisor in case vGPU instances are used by the VMs. For VMware vSphere, this host driver is provided in the form of a VIB file.

The maximum number of vGPUS that can be allocated to worker node VMs depends on the version of vSphere:

* vSphere 7.0: maximum 4 vGPU per VM
* vSphere 8.0: maximum 8 vGPU per VM
+
[NOTE]
====
vSphere 8.0 introduced support for multiple full or fractional heterogenous profiles associated with a VM.
====

You can choose one of the following methods to attach the worker nodes to the GPUs:

* GPU passthrough for accessing and using GPU hardware within a virtual machine (VM)

* GPU (vGPU) time-slicing, when not all of the GPU is needed

Similar to bare metal deployments, one or three or more servers are required. Clusters with two servers are not supported.
