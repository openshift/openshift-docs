// Module included in the following assemblies:
//
// * observability/logging/logging-6.0/log6x-about.adoc

:_mod-docs-content-type: PROCEDURE
[id="quick-start-opentelemetry_{context}"]
= Quick start with OpenTelemetry

:FeatureName: The OpenTelemetry Protocol (OTLP) output log forwarder
include::snippets/technology-preview.adoc[]

To configure OTLP ingestion and enable the OpenTelemetry data model, follow these steps:

.Prerequisites
* Cluster administrator permissions

.Procedure

. Install the {clo}, {loki-op}, and {coo-first} from OperatorHub.

. Create a `LokiStack` custom resource (CR) in the `openshift-logging` namespace:
+
[source,yaml]
----
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki
  namespace: openshift-logging
spec:
  managementState: Managed
  size: 1x.extra-small
  storage:
    schemas:
    - effectiveDate: '2024-10-01'
      version: v13
    secret:
      name: logging-loki-s3
      type: s3
  storageClassName: gp3-csi
  tenants:
    mode: openshift-logging
----
+
[NOTE]
====
Ensure that the `logging-loki-s3` secret is created beforehand. The contents of this secret vary depending on the object storage in use. For more information, see "Secrets and TLS Configuration".
====

. Create a service account for the collector:
+
[source,terminal]
----
$ oc create sa collector -n openshift-logging
----

. Allow the collector's service account to write data to the `LokiStack` CR:
+
[source,terminal]
----
$ oc adm policy add-cluster-role-to-user logging-collector-logs-writer -z collector
----
+
[NOTE]
====
The `ClusterRole` resource is created automatically during the Cluster Logging Operator installation and does not need to be created manually.
====

. Allow the collector's service account to collect logs:
+
[source,terminal]
----
$ oc project openshift-logging
----
+
[source,terminal]
----
$ oc adm policy add-cluster-role-to-user collect-application-logs -z collector
----
+
[source,terminal]
----
$ oc adm policy add-cluster-role-to-user collect-audit-logs -z collector
----
+
[source,terminal]
----
$ oc adm policy add-cluster-role-to-user collect-infrastructure-logs -z collector
----
+
[NOTE]
====
The example binds the collector to all three roles (application, infrastructure, and audit). By default, only application and infrastructure logs are collected. To collect audit logs, update your `ClusterLogForwarder` configuration to include them. Assign roles based on the specific log types required for your environment.
====

. Create a `UIPlugin` CR to enable the *Log* section in the *Observe* tab:
+
[source,yaml]
----
apiVersion: observability.openshift.io/v1alpha1
kind: UIPlugin
metadata:
  name: logging
spec:
  type: Logging
  logging:
    lokiStack:
      name: logging-loki
----

. Create a `ClusterLogForwarder` CR to configure log forwarding:
+
[source,yaml]
----
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: collector
  namespace: openshift-logging
  annotations:
    observability.openshift.io/tech-preview-otlp-output: "enabled" # <1>
spec:
  serviceAccount:
    name: collector
  outputs:
  - name: loki-otlp
    type: lokiStack # <2>
    lokiStack:
      target:
        name: logging-loki
        namespace: openshift-logging
      dataModel: Otel # <3>
      authentication:
        token:
          from: serviceAccount
    tls:
      ca:
        key: service-ca.crt
        configMapName: openshift-service-ca.crt
  pipelines:
  - name: my-pipeline
    inputRefs:
    - application
    - infrastructure
    outputRefs:
    - loki-otlp
----
<1> Use the annotation to enable the `Otel` data model, which is a Technology Preview feature.
<2> Define the output type as `lokiStack`.
<3> Specifies the OpenTelemetry data model.
+
[NOTE]
====
You cannot use `lokiStack.labelKeys` when `dataModel` is `Otel`. To achieve similar functionality when `dataModel` is `Otel`, refer to "Configuring LokiStack for OTLP data ingestion".
====

.Verification
* Verify that OTLP is functioning correctly by going to *Observe* -> *OpenShift Logging* -> *LokiStack* -> *Writes* in the OpenShift web console, and checking *Distributor - Structured Metadata*.