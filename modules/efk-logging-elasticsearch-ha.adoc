// Module included in the following assemblies:
//
// * logging/efk-logging-elasticsearch.adoc

[id='efk-logging-elasticsearch-ha_{context}']
= Configuring Elasticsearch replication policy

You can define how Elasticsearch shards are replicated across data nodes in the cluster:

.Prerequisites

* Cluster logging and Elasticsearch must be installed.

* Set cluster logging to the unmanaged state.

* If needed, get the name of the Cluster Logging Custom Resource in the `openshift-logging` project:
+
----
$ oc get ClusterLogging
NAME       AGE
instance   112m
----

.Procedure

Edit the Cluster Logging Custom Resource (CR) in the `openshift-logging` project: 

[source,yaml]
----
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "clusterlogging"

....

spec:
  logStore:
    type: "elasticsearch"
    elasticsearch: 
      redundancyPolicy: "SingleRedundancy" <1>
----
<1> Specify a redundancy policy for the shards. The change is applied upon saving the changes.

* *FullRedundancy*. Elasticsearch fully replicates the primary shards for each index 
to every data node. This provides the highest safety, but requires the highest amount of disk and results in the poorest performance.
* *MultipleRedundancy*. Elasticsearch fully replicates the primary shards for each index to half of the data nodes.
This provides a good tradeoff between safety and performance.
* *SingleRedundancy*. Elasticsearch makes one copy of the primary shards for each index. 
Logs are always available and recoverable as long as at least two data nodes exist.
This provides better performance than MultipleRedundancy, when using 5 or more nodes.  You cannot 
apply this policy on deployments of single Elasticsearch node.
* *ZeroRedundancy*. Elasticsearch does not make copies of the primary shards. 
Logs might be unavailable or lost in the event a node is down or fails.
Use this mode when you are more concerned with performance than safety, or have 
implemented your own disk/PVC backup/restore strategy.


