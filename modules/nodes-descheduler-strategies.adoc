// Module included in the following assemblies:
//
// * nodes/scheduling/nodes-descheduler.adoc

[id="nodes-descheduler-strategies_{context}"]
= Descheduler strategies

The following descheduler strategies are available:

Low node utilization::
The `LowNodeUtilization` strategy finds nodes that are underutilized and evicts Pods, if possible, from other nodes in the hope that recreation of evicted Pods will be scheduled on these underutilized nodes.
+
The underutilization of nodes is determined by several configurable threshold parameters: CPU, memory, and number of Pods. If a node's usage is below the configured thresholds for all parameters (CPU, memory, and number of Pods), then the node is considered to be underutilized.
+
You can also set a target threshold for CPU, memory, and number of Pods. If a node's usage is above the configured target thresholds for all parameters, then the node's Pods might be considered for eviction.
+
Additionally, you can use the `NumberOfNodes` parameter to set the strategy to activate only when the number of underutilized nodes is above the configured value. This can be helpful in large clusters where a few nodes might be underutilized frequently or for a short period of time.

Duplicate Pods::
The `RemoveDuplicates` strategy ensures that there is only one Pod associated with a ReplicaSet, ReplicationController, Deployment, or Job running on same node. If there are more, then those duplicate Pods are evicted for better spreading of Pods in a cluster.
+
This situation could occur after a node failure, when a Pod is moved to another node, leading to more than one Pod associated with a ReplicaSet, ReplicationController, Deployment, or Job on that node. After the failed node is ready again, this strategy evicts the duplicate Pod.

Violation of inter-pod anti-affinity::
The `RemovePodsViolatingInterPodAntiAffinity` strategy ensures that Pods violating inter-pod anti-affinity are removed from nodes.
+
This situation could occur when anti-affinity rules are created for Pods that are already running on the same node.

Violation of node affinity::
The `RemovePodsViolatingNodeAffinity` strategy ensures that Pods violating node affinity are removed from nodes.
+
This situation could occur if a node no longer satisfies a Pod's affinity rule. If another node is available that satisfies the affinity rule, then the Pod is evicted.

Violation of node taints::
The `RemovePodsViolatingNodeTaints` strategy ensures that Pods violating `NoSchedule` taints on nodes are removed.
+
This situation could occur if a Pod is set to tolerate a taint `key=value:NoSchedule` and is running on a tainted node. If the node's taint is updated or removed, the taint is no longer satisfied by the Pod's tolerations and the Pod is evicted.

Too many restarts::
The `RemovePodsHavingTooManyRestarts` strategy ensures that Pods that have been restarted too many times are removed from nodes.
+
This situation could occur if a Pod is scheduled on a node that is unable to start it. For example, if the node is having network issues and is unable to mount a networked persistent volume, then the Pod should be evicted so that it can be scheduled on another node. Another example is if the Pod is crashlooping.
+
This strategy has two configurable parameters: `PodRestartThreshold` and `IncludingInitContainers`. If a Pod is restarted more than the configured `PodRestartThreshold` value, then the Pod is evicted. You can use the `IncludingInitContainers` parameter to specify whether restarts for Init Containers should be calculated into the `PodRestartThreshold` value.
