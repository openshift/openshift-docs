// Module included in the following assemblies:
//
// * installing-byoh/installing-existing-hosts.adoc

[id="installation-planning_{context}"]
= Planning your installation

You install {product-title} by running a series of Ansible playbooks. As you
prepare to install your cluster, you create an inventory file that
represents your environment and {product-title} cluster configuration. While
familiarity with Ansible might make this process easier, it is not required.

You can read more about Ansible and its basic usage in the
link:http://docs.ansible.com/ansible/[official documentation].

[id="inital-planning_{context}"]
== Initial planning

Before you install your production {product-title} cluster, you need answers to
the following questions:

ifdef::openshift-origin[]
* _Do you install on-premise or in a public or private cloud?_ Review the installation methods
for more information about the cloud providers options available.
endif::[]

* _How many pods are required in your cluster?_ Review the sizing considerations
for the limits for nodes and pods so you can calculate how large your
environment needs to be.

* _How many hosts do you require in the cluster?_ Review the environment scenarios
for multiple examples of Single Master and Multiple Master
configurations.

* _Do you need a high availability cluster?_
High availability configurations improve fault tolerance. In this situation, you
might use the Multiple Masters Using Native HA
example to set up your environment.

* _Is cluster monitoring required?_
The monitoring stack requires additional system resources.
Note that the monitoring stack is installed by default.

* _Do you want to use your own infrastructure or let {product-title} automatically provision and manage your infrastructure on a cloud?_
If you want to your use own infrastructure to host your cluster nodes, you can use the BYO installation
method. If you want to let {product-title} manage your infrastructure through a
cloud provider, you can install on a cloud.

* _Which identity provider do you use for authentication?_
If you already use a supported identity provider, configure {product-title} to
use that identity provider during installation.

ifdef::openshift-enterprise[]
* _Is my installation supported if I integrate it with other technologies?_
See the link:https://access.redhat.com/articles/2176281[OpenShift Container Platform Tested Integrations]
for a list of tested integrations.
endif::[]

ifdef::openshift-origin[]
[id="planning-cloud-providers_{context}"]
== On-premise versus cloud providers

You can install {product-title} on-premise or host it on public or private
clouds. You can use the provided Ansible playbooks to help you automate
the provisioning and installation processes.
endif::[]

[id="sizing_{context}"]
== Sizing considerations

Determine how many nodes and pods you require for your {product-title} cluster.
Cluster scalability correlates to the number of pods in a cluster environment.
That number influences the other numbers in your setup

[id="environment-scenarios_{context}"]
== Environment scenarios

Use these environment scenarios to help plan your {product-title} cluster
based on your sizing needs.

[NOTE]
====
Moving from a single master cluster to multiple masters after installation is
not supported.
====

In all environments, if your etcd hosts are co-located with master hosts, etcd
runs as a static pod on the host. If your etcd hosts are not co-located with
master hosts, they run etcd as standalone processes.

[id="single-master-single-box_{context}"]
=== Single master and node on one system

You can install {product-title} on a single system for only a development
environment. You cannot use an _all-in-one environment_ as a production
environment.

[id="single-master-multi-node_{context}"]
=== Single master and multiple nodes

The following table describes an example environment for a single
master (with etcd installed on the same host)
and two nodes:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|`master.example.com`
|Master, etcd, and node

|`node1.example.com`
.2+.^|Node

|`node2.example.com`
|===

[id="multi-masters-using-native-ha-colocated_{context}"]
=== Multiple masters using native HA

The following describes an example environment for three masters,
one HAProxy load balancer, and two nodes
using the `native` HA method. etcd runs as static pods on the master nodes:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|`master1.example.com`
.3+.^|Master (clustered using native HA) and node and clustered etcd

|`master2.example.com`

|`master3.example.com`

|`lb.example.com`
|HAProxy to load balance API master endpoints

|`node1.example.com`
.2+.^|Node

|`node2.example.com`
|===

[id="multi-masters-using-native-ha_{context}"]
=== Multiple Masters Using Native HA with External Clustered etcd

The following describes an example environment for three
masters,
one HAProxy load balancer, three external clustered etcd
hosts, and two nodes using the `native` HA method:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|`master1.example.com`
.3+.^|Master (clustered using native HA) and node

|`master2.example.com`

|`master3.example.com`

|`lb.example.com`
|HAProxy to load balance API master endpoints

|`etcd1.example.com`
.3+.^|Clustered etcd

|`etcd2.example.com`

|`etcd3.example.com`

|`node1.example.com`
.2+.^|Node

|`node2.example.com`
|===

[id="planning-stand-alone-registry_{context}"]
=== Stand-alone registry

You can also install {product-title} to act as a stand-alone registry using the
{product-title}'s integrated registry.

[id="planning-installation-types_{context}"]
== Installation types for supported infrastructure

Starting in {product-title} {product-version}, you can install the cluster on your own
infrastructure nodes or install a fully-managed cluster on certain cloud providers.

.Differences between installation types
[cols="h,2*",options="header"]
|===
| |BYO | Managed

|Installation type |RPM-based |System container
|Delivery Mechanism |RPM packages using `yum` |System container images using `docker`
|Service Management |`systemd` |`docker` and `systemd` units
|===

////
[id="containerized-required-images_{context}"]
=== Required images for system containers

The system container installation type makes use of the following images:

ifdef::openshift-origin[]
- `openshift/origin-node`
endif::[]
ifdef::openshift-enterprise[]
- `openshift3/ose-node`

By default, all of the above images are pulled from the Red Hat Registry at
https://registry.redhat.io[registry.redhat.io].
endif::[]

If you need to use a private registry to pull these images during the
installation, you can specify the registry information ahead of time. Set the
following Ansible variables in your inventory file, as required:

----
ifdef::openshift-origin[]
oreg_url='<registry_hostname>/openshift/origin-${component}:${version}'
endif::[]
ifdef::openshift-enterprise[]
oreg_url='<registry_hostname>/openshift3/ose-${component}:${version}'
endif::[]
openshift_docker_insecure_registries=<registry_hostname>
openshift_docker_blocked_registries=<registry_hostname>
----

[NOTE]
====
You can also set the `openshift_docker_insecure_registries` variable to the IP
address of the host. `0.0.0.0/0` is not a valid setting.
====

The default component inherits the image prefix and version from the `oreg_url`
value.

The configuration of additional, insecure, and blocked container registries occurs
at the beginning of the installation process to ensure that these settings are
applied before attempting to pull any of the required images.

////

[id="planning-installation-types-service-names_{context}"]
=== systemd service names

The installation process creates relevant `systemd` units which can be used to
start, stop, and poll services using normal `systemctl` commands. For system
container installations, these unit names match those of an RPM installation.

[id="containerized-file-paths_{context}"]
=== File path locations

All {product-title} configuration files are placed in the same locations during
containerized installation as RPM based installations and will survive `os-tree`
upgrades.
