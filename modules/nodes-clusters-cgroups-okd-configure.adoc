// Module included in the following assemblies:
//
// * nodes/clusters/nodes-cluster-cgroups-okd.adoc
// * post_installation_configuration/cluster-tasks.adoc


ifeval::["{context}" == "nodes-cluster-cgroups-2"]
:node:
endif::[]
ifeval::["{context}" == "post-install-cluster-tasks"]
:post:
endif::[]

ifdef::post[]
:_mod-docs-content-type: PROCEDURE
[id="nodes-clusters-cgroups-okd-configure_{context}"]
= Configuring the Linux cgroup version on your nodes

As of {product-title} 4.14, {product-title} uses link:https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html[Linux control group version 2] (cgroup v2) in your cluster. If you are using cgroup v1 on {product-title} 4.13 or earlier, migrating to {product-title} 4.14 will not automatically update your cgroup configuration to version 2. A fresh installation of {product-title} 4.14 will use cgroup v2 by default. However, you can enable link:https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/index.html[Linux control group version 1] (cgroup v1) upon installation. Enabling cgroup v1 in {product-title} disables all cgroup v2 controllers and hierarchies in your cluster.

cgroup v2 is the current version of the Linux cgroup API. cgroup v2 offers several improvements over cgroup v1, including a unified hierarchy, safer sub-tree delegation, new features such as link:https://www.kernel.org/doc/html/latest/accounting/psi.html[Pressure Stall Information], and enhanced resource management and isolation.
endif::post[]

ifdef::node[]
:_mod-docs-content-type: PROCEDURE
[id="nodes-clusters-cgroups-okd-configure_{context}"]
= Configuring Linux cgroup

You can switch to Linux control group version 1 (cgroup v1), if needed, by using a machine config. Enabling cgroup v1 in {product-title} disables the cgroup v2 controllers and hierarchies in your cluster.
endif::node[]

.Prerequisites
* Have administrative privilege to a working {product-title} cluster.

.Procedure

. Create a `MachineConfig` object file that identifies the kernel argument (for example, `worker-cgroup-v1.yaml`)
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker <1>
  name: worker-cgroup-v1 <2>
spec:
  config:
    ignition:
      version: 3.2.0
  kernelArguments:
    - systemd.unified_cgroup_hierarchy=0 <3>
----
+
<1> Applies the new kernel argument only to worker nodes.
<2> Applies a name to the machine config.
<3> Configures cgroup v1 on the associated nodes.

. Create the new machine config:
+
[source,terminal]
----
$ oc create -f 05-worker-cgroup-v1.yaml
----

. Check to see that the new machine config was added:
+
[source,terminal]
----
$ oc get MachineConfig
----
+
.Example output
[source,terminal]
----
NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-cgroup-v1                                                                           3.2.0             105s
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-master-c5e92d98103061c4818cfcefcf462770   60746a843e7ef8855ae00f2ffcb655c53e0e8296   3.2.0             115s
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
----

. Check the nodes:
+
[source,terminal]
----
$ oc get nodes
----
+
.Example output
[source,terminal]
----
NAME                           STATUS                     ROLES    AGE   VERSION
ip-10-0-136-161.ec2.internal   Ready                      worker   28m   v1.27.3
ip-10-0-136-243.ec2.internal   Ready                      master   34m   v1.27.3
ip-10-0-141-105.ec2.internal   Ready,SchedulingDisabled   worker   28m   v1.27.3
ip-10-0-142-249.ec2.internal   Ready                      master   34m   v1.27.3
ip-10-0-153-11.ec2.internal    Ready                      worker   28m   v1.27.3
ip-10-0-153-150.ec2.internal   Ready                      master   34m   v1.27.3
----
+
You can see that the command disables scheduling on each worker node.

. After a node returns to the `Ready` state, start a debug session for that node:
+
[source,terminal]
----
$ oc debug node/<node_name>
----

. Set `/host` as the root directory within the debug shell:
+
[source,terminal]
----
sh-4.4# chroot /host
----

. Check that the `sys/fs/cgroup/cgroup2fs` file has been moved to the `tmpfs` file system:
+
[source,terminal]
----
$ stat -c %T -f /sys/fs/cgroup
----
+
.Example output
+
[source,terminal]
----
tmpfs
----

ifeval::["{context}" == "nodes-cluster-cgroups-2"]
:!node:
endif::[]
ifeval::["{context}" == "post-install-cluster-tasks"]
:!post:
endif::[]
