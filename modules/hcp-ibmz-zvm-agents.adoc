// Module included in the following assemblies:
//
// * hosted_control_planes/hcp-deploy/hcp-deploy-ibmz.adoc

:_mod-docs-content-type: PROCEDURE
[id="hcp-ibmz-zvm-agents_{context}"]
= Adding {ibm-title} z/VM as agents

If you want to use a static IP for z/VM guest, you must configure the `NMStateConfig` attribute for the z/VM agent so that the IP parameter persists in the second start.

Complete the following steps to start your {ibm-z-title} environment with the downloaded PXE images from the `InfraEnv` resource. After the Agents are created, the host communicates with the Assisted Service and registers in the same namespace as the `InfraEnv` resource on the management cluster.

.Procedure

. Update the parameter file to add the `rootfs_url`, `network_adaptor` and `disk_type` values. 
+
.Example parameter file
[source,yaml]
----
rd.neednet=1 cio_ignore=all,!condev \
console=ttysclp0  \
ignition.firstboot ignition.platform.id=metal \
coreos.live.rootfs_url=http://<http_server>/rhcos-<version>-live-rootfs.<architecture>.img \// <1>
coreos.inst.persistent-kargs=console=ttysclp0
ip=<ip>::<gateway>:<netmask>:<hostname>::none nameserver=<dns> \// <2>
rd.znet=qeth,<network_adaptor_range>,layer2=1
rd.<disk_type>=<adapter> \// <3>
zfcp.allow_lun_scan=0
ai.ip_cfg_override=1 \// <4>
----
<1> For the `coreos.live.rootfs_url` artifact, specify the matching `rootfs` artifact for the `kernel` and `initramfs` that you are starting. Only HTTP and HTTPS protocols are supported.
<2> For the `ip` parameter, manually assign the IP address, as described in _Installing a cluster with z/VM on {ibm-z-title} and {ibm-linuxone-title}_.
<3> For installations on DASD-type disks, use `rd.dasd` to specify the DASD where Red Hat Enterprise Linux CoreOS (RHCOS) is to be installed. For installations on FCP-type disks, use `rd.zfcp=<adapter>,<wwpn>,<lun>` to specify the FCP disk where RHCOS is to be installed.
<4> Specify this parameter when you use an Open Systems Adapter (OSA) or HiperSockets.

. Move `initrd`, kernel images, and the parameter file to the guest VM by running the following commands:
+
[source,terminal]
----
vmur pun -r -u -N kernel.img $INSTALLERKERNELLOCATION/<image name>
----
+
[source,terminal]
----
vmur pun -r -u -N generic.parm $PARMFILELOCATION/paramfilename
----
+
[source,terminal]
----
vmur pun -r -u -N initrd.img $INSTALLERINITRAMFSLOCATION/<image name>
----

. Run the following command from the guest VM console:
+
[source,terminal]
----
cp ipl c
----

. To list the agents and their properties, enter the following command:
+
[source,terminal]
----
$ oc -n <hosted_control_plane_namespace> get agents
----
+
.Example output
[source,terminal]
----
NAME    CLUSTER APPROVED    ROLE    STAGE
50c23cda-cedc-9bbd-bcf1-9b3a5c75804d    auto-assign
5e498cd3-542c-e54f-0c58-ed43e28b568a    auto-assign
----

. Run the following command to approve the agent. 
+
[source,terminal]
----
$ oc -n <hosted_control_plane_namespace> patch agent \
  50c23cda-cedc-9bbd-bcf1-9b3a5c75804d -p \
  '{"spec":{"installation_disk_id":"/dev/sda","approved":true,"hostname":"worker-zvm-0.hostedn.example.com"}}' \// <1>
  --type merge
----
<1> Optionally, you can set the agent ID `<installation_disk_id>` and `<hostname>` in the specification.

. Run the following command to verify that the agents are approved:
+
[source,terminal]
----
$ oc -n <hosted_control_plane_namespace> get agents
----
+
.Example output
[source,terminal]
----
NAME                                            CLUSTER     APPROVED   ROLE          STAGE
50c23cda-cedc-9bbd-bcf1-9b3a5c75804d             true       auto-assign
5e498cd3-542c-e54f-0c58-ed43e28b568a             true       auto-assign
----