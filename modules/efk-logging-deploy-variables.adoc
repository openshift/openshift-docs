// Module included in the following assemblies:
//
// * logging/efk-logging-deploy.adoc

[id='efk-logging-deploy-variables-{context}']
= Specifying Ansible variables for deploying the EFK stack

When deploying the EFK stack into {product-title} you can specify parameters for the EFK deployment in the 
Ansible inventory file. 

Because Elasticsearch can use a lot of resources, all members of a cluster
should have low latency network connections to each other and to any remote
storage. Ensure this by directing the instances to dedicated nodes, or a
dedicated region within your cluster, using a node selector.

To configure a node selector, specify the `openshift_logging_es_nodeselector`
configuration option in the inventory file. This applies to all Elasticsearch
deployments; if you need to individualize the node selectors, you must manually
edit each deployment configuration after deployment. The node selector is
specified as a python compatible dict. For example, `{"node-type":"infra",
"region":"east"}`.

[NOTE]
====
By default the Elasticsearch service uses port 9300 for TCP communication
between nodes in a cluster.
====
.Procedure

. Edit the Ansible inventory file, by default *_/etc/Ansible/hosts_*, specifying variables as described below.
+
* Set `openshift_logging_install_logging=true`
* Set `openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra": "true"}`

. Save and close the file.

== EFK Ansible variables

The following tables describe the Ansible variables you can use to configure your EFK stack.

.General EFK parameters
[cols="3,7",options="header"]
|===
|Parameter
|Description

|`openshift_logging_install_logging`
|Set to `true` to install logging. Set to `false` to uninstall logging.

|`openshift_logging`
|The namespace where Aggregated Logging is deployed.

|`openshift_logging_master_url`
|The URL for the Kubernetes master, this does not need to be public facing but
should be accessible from within the cluster. For example,
https://<PRIVATE-MASTER-URL>:8443.

|`openshift_logging_master_public_url`
|The public facing URL for the Kubernetes master. This is used for Authentication
redirection by the Kibana proxy. For example,
https://<CONSOLE-PUBLIC-URL-MASTER>:8443.

|`openshift_logging_purge_logging`
|The common uninstall keeps PVC to prevent unwanted data loss during
reinstalls. To ensure that the Ansible playbook completely and irreversibly
removes all logging persistent data including PVC, set
`openshift_logging_install_logging` to 'false' to trigger uninstallation and
`openshift_logging_purge_logging` to 'true'. The default is set to 'false'.

|`openshift_logging_image_pull_secret`
|Specify the name of an existing pull
secret to be used for pulling component images from an authenticated registry.
|===

//tag::eventrouter-vars[]
.Parameters for configuring eventrouter
[cols="3,7",options="header"]
|===
|Parameter
|Description

|`openshift_logging_install_eventrouter`
|Coupled with `openshift_logging_install_logging`. When both are set to 'true',
*eventrouter* will be installed. When both are 'false', *eventrouter* will be
uninstalled.

|`openshift_logging_eventrouter_image_prefix`
|The prefix for the *eventrouter* logging image.

|`openshift_logging_eventrouter_image_version`
|The image version for the logging *eventrouter*.

|`openshift_logging_eventrouter_sink`
|Select a sink for *eventrouter*, supported `stdout` and `glog`. The default is set
to `stdout`.

|`openshift_logging_eventrouter_nodeselector`
|A map of labels, such as `"node":"infra"`,`"region":"west"`, to select the nodes
where the pod will land.

|`openshift_logging_eventrouter_replicas`
|The default is set to '1'.

|`openshift_logging_eventrouter_cpu_limit`
|The minimum amount of CPU to allocate to *eventrouter*. The default is set to '100m'.

|`openshift_logging_eventrouter_memory_limit`
|The memory limit for *eventrouter* pods. The default is set to '128Mi'.

|`openshift_logging_eventrouter_namespace`
a|The project where *eventrouter* is deployed. The default is set to 'default'.

[IMPORTANT]
====
Do not set the project to anything other than `default` or `openshift-*`. If you specify a different project, 
event information from the other project can leak into indices that are not restricted to operations users.
To use a non-default project, create the project as usual using `oc new-project`.
====
|===
//end::eventrouter-vars[]

.Parameters for configuring Kibana
[cols="3,7",options="header"]
|===
|Parameter
|Description

|`openshift_logging_kibana_hostname`
|The external host name for web clients to reach Kibana.

|`openshift_logging_kibana_cpu_limit`
|The amount of CPU to allocate to Kibana.

|`openshift_logging_kibana_memory_limit`
|The amount of memory to allocate to Kibana.

|`openshift_logging_kibana_proxy_debug`
|When `true`, set the Kibana Proxy log level to DEBUG.

|`openshift_logging_kibana_proxy_cpu_limit`
|The amount of CPU to allocate to Kibana proxy.

|`openshift_logging_kibana_proxy_memory_limit`
|The amount of memory to allocate to Kibana proxy.

|`openshift_logging_kibana_replica_count`
|The number of to which Kibana should be scaled up.

|`openshift_logging_kibana_nodeselector`
|A node selector that specifies
which nodes are eligible targets for deploying Kibana instances.

|`openshift_logging_kibana_env_vars`
|A map of environment variables to add to the Kibana deployment configuration.
For example, {"ELASTICSEARCH_REQUESTTIMEOUT":"30000"}.

|`openshift_logging_kibana_key`
|The public facing key to use when creating
the Kibana route.

|`openshift_logging_kibana_cert`
|The cert that matches
the key when creating the Kibana route.

|`openshift_logging_kibana_ca`
|Optional. The CA to goes with the key and cert used when creating the Kibana
route.

|`openshift_logging_elasticsearch_kibana_index_mode`
a|The default value, `unique`, allows users to each have their own Kibana index. In
this mode, their saved queries, visualizations, and dashboards are not shared.

You may also set the value `shared_ops`. In this mode, all operations users
share a Kibana index which allows each operations user to see the same
queries, visualizations, and dashboards. To determine if you are an operations user:

----
#oc auth can-i view pod/logs -n default
yes
----

If you do not have appropriate access, contact your cluster administrator.

|`openshift_logging_kibana_ops_nodeselector`
|A node selector that specifies which nodes are eligible targets
for deploying Kibana instances.
|===

//tag::ops-cluster-vars[]
.Parameters for configuring Ops cluster for use when `openshift_logging_use_ops` is set to `true`
[cols="3,7",options="header"]
|===
|Parameter
|Description

|`openshift_logging_use_ops`
|If set to `true`, configures a second Elasticsearch cluster and Kibana for
operations logs. Fluentd splits logs between the main cluster and a cluster
reserved for operations logs, which consists of the logs from the projects
*default*, *openshift*, and *openshift-infra*, as well as Docker, OpenShift, and
system logs from the journal. This means a second Elasticsearch cluster and
Kibana are deployed. The deployments are distinguishable by the *-ops* suffix
included in their names and have parallel deployment options listed below.

|`openshift_logging_kibana_ops_hostname`
|Equivalent to `openshift_logging_kibana_hostname` for Ops cluster.

|`openshift_logging_kibana_ops_cpu_limit`
|Equivalent to `openshift_logging_kibana_cpu_limit` for Ops cluster.

|`openshift_logging_kibana_ops_memory_limit`
|Equivalent to `openshift_logging_kibana_memory_limit` for Ops cluster.

|`openshift_logging_kibana_ops_proxy_debug`
|Equivalent to `openshift_logging_kibana_proxy_debug` for Ops cluster.

|`openshift_logging_kibana_ops_proxy_cpu_limit`
|Equivalent to `openshift_logging_kibana_proxy_cpu_limit` for Ops cluster.

|`openshift_logging_kibana_ops_proxy_memory_limit`
|Equivalent to `openshift_logging_kibana_proxy_memory_limit` for Ops cluster.

|`openshift_logging_kibana_ops_replica_count`
|Equivalent to `openshift_logging_kibana_replica_count` for Ops cluster.

|`openshift_logging_es_ops_host`
|Equivalent to `openshift_logging_es_host` for Ops cluster.

|`openshift_logging_es_ops_hostname`
|The external-facing hostname to use for the route and the TLS server certificate.
The default is set to `es-ops`.

|`openshift_logging_es_ops_port`
|Equivalent to `openshift_logging_es_port` for Ops cluster.

|`openshift_logging_es_ops_ca`
|Equivalent to `openshift_logging_es_ca` for Ops cluster.

|`openshift_logging_es_ops_allow_external`
|Set to `true` to expose Elasticsearch as a reencrypt route. Set to `false` by
defaut.

For example, if `openshift_master_default_subdomain` is set to `=example.test`,
then the default value of `openshift_logging_es_ops_hostname` will be
`es-ops.example.test`.

|`openshift_logging_es_ops_cert`
|The location of the certificate Elasticsearch uses for the external TLS server
cert. The default is a generated cert.

|`openshift_logging_es_ops_key`
|The location of the key Elasticsearch uses for the external TLS server cert.
The default is a generated key.

|`openshift_logging_es_ops_ca_ext`
|The location of the CA cert Elasticsearch uses for the external TLS
server cert. The default is the internal CA.

|`openshift_logging_es_ops_client_cert`
|Equivalent to `openshift_logging_es_client_cert` for Ops cluster.

|`openshift_logging_es_ops_client_key`
|Equivalent to `openshift_logging_es_client_key` for Ops cluster.

|`openshift_logging_es_ops_cluster_size`
|Equivalent to `openshift_logging_es_cluster_size` for Ops cluster.

|`openshift_logging_es_ops_cpu_limit`
|Equivalent to `openshift_logging_es_cpu_limit` for Ops cluster.

|`openshift_logging_es_ops_memory_limit`
|Equivalent to `openshift_logging_es_memory_limit` for Ops cluster.

|`openshift_logging_es_ops_pv_selector`
|Equivalent to `openshift_logging_es_pv_selector` for Ops cluster.

|`openshift_logging_es_ops_pvc_dynamic`
|Equivalent to `openshift_logging_es_pvc_dynamic` for Ops cluster.

|`openshift_logging_es_ops_pvc_size`
|Equivalent to `openshift_logging_es_pvc_size` for Ops cluster.

|`openshift_logging_es_ops_pvc_prefix`
|Equivalent to `openshift_logging_es_pvc_prefix` for Ops cluster.

|`openshift_logging_es_ops_recover_after_time`
|Equivalent to `openshift_logging_es_recovery_after_time` for Ops cluster.

|`openshift_logging_es_ops_storage_group`
|Equivalent to `openshift_logging_es_storage_group` for Ops cluster.

|`openshift_logging_es_ops_nodeselector`
|A node selector that specifies which nodes are eligible targets
for deploying Elasticsearch instances. This can be used to place
these instances on nodes reserved or optimized for running them.
For example, the selector could be `node-type=infrastructure`. At least
one active node must have this label before Elasticsearch will deploy.
|===
//end::ops-cluster-vars[]

.Parameters for configuring Elasticsearch
[cols="3,7",options="header"]
|===
|Parameter
|Description

|`openshift_logging_es_allow_external`
|Set to `true` to expose Elasticsearch as a reencrypt route. Set to `false` by
default.

|`openshift_logging_es_hostname`
|The external-facing hostname to use for the route and the TLS server
certificate. The default is set to `es`.

For example, if `openshift_master_default_subdomain` is set to `=example.test`,
then the default value of `openshift_logging_es_hostname` will be
`es.example.test`.

|`openshift_logging_es_cert`
|The location of the certificate Elasticsearch uses for the external TLS server
cert. The default is a generated cert.

|`openshift_logging_es_key`
|The location of the key Elasticsearch uses for the external TLS server cert.
The default is a generated key.

|`openshift_logging_es_ca_ext`
|The location of the CA cert Elasticsearch uses for the external TLS
server cert. The default is the internal CA.

|`openshift_logging_es_host`
|The name of the ES service where Fluentd should send logs.

|`openshift_logging_es_port`
|The port for the ES service where Fluentd should send logs.

|`openshift_logging_es_ca`
|The location of the CA Fluentd uses to communicate with `openshift_logging_es_host`.

|`openshift_logging_es_client_cert`
|The location of the client certificate Fluentd uses for `openshift_logging_es_host`.

|`openshift_logging_es_client_key`
|The location of the client key Fluentd uses for `openshift_logging_es_host`.

|`openshift_logging_es_cluster_size`
|Elasticsearch replicas to deploy. Redundancy requires at least three or more.

|`openshift_logging_es_cpu_limit`
|The amount of CPU limit for the ES cluster.

|`openshift_logging_es_memory_limit`
|Amount of RAM to reserve per Elasticsearch instance. It
must be at least 512M. Possible suffixes are G,g,M,m.

|`openshift_logging_es_number_of_replicas`
|The number of replica shards per primary shard for every new index. Defaults to '0'. A minimum of `1` is advisable for production clusters.

|`openshift_logging_es_number_of_shards`
|The number of primary shards for every new index created in ES. Defaults to '1'.

|`openshift_logging_es_pv_selector`
|A key/value map added to a PVC in order to select specific PVs.

|`openshift_logging_es_pvc_dynamic`
|To dynamically provision the backing storage, set the parameter value to `true`.
When set to `true`, the storageClass spec is omitted from the PVC definition.
If you set a `openshift_logging_es_pvc_storage_class_name` parameter value,
its value overrides the value of the the `openshift_logging_es_pvc_dynamic` parameter.

|`openshift_logging_es_pvc_storage_class_name`
|To use a non-default storage class, specify the storage class name, such as
`glusterprovisioner` or `cephrbdprovisioner`. After you specify
the storage class name, dynamic volume provisioning is active regardless of the
openshift_logging_es_pvc_dynamic value.

|`openshift_logging_es_pvc_size`
|Size of the persistent volume claim to
create per Elasticsearch instance. For example, 100G. If omitted, no PVCs are
created and ephemeral volumes are used instead. If this parameter is set, `openshift_logging_elasticsearch_storage_type` is set to `pvc`.

|`openshift_logging_elasticsearch_storage_type`
|Sets the Elasticsearch storage type. If you are using Persistent Elasticsearch Storage, set to `pvc`.

|`openshift_logging_es_pvc_prefix`
a|Prefix for the names of persistent volume claims to be used as storage for
Elasticsearch instances. A number is appended per instance, such as
*logging-es-1*. If they do not already exist, they are created with size
`_es-pvc-size_`.

When `openshift_logging_es_pvc_prefix` is set, and:

* `openshift_logging_es_pvc_dynamic`=`true`, the value for `openshift_logging_es_pvc_size` is optional.
* `openshift_logging_es_pvc_dynamic`=`false`, the value for `openshift_logging_es_pvc_size` must be set.

|`openshift_logging_es_recover_after_time`
|The amount of time ES will wait before it tries to recover.

|`openshift_logging_es_storage_group`
|Number of a supplemental group ID for access to Elasticsearch storage volumes.
Backing volumes should allow access by this group ID.

|`openshift_logging_es_nodeselector`
|A node selector specified as a map that determines which nodes are eligible targets
for deploying Elasticsearch instances. This can be used to place
these instances on nodes reserved or optimized for running them.
For example, the selector could be `{"node-type":"infrastructure"}`. At least
one active node must have this label before Elasticsearch will deploy.
|===

.Parameters for configuring Curator
[cols="3,7",options="header"]
|===
|Parameter
|Description

|`openshift_logging_curator_default_days`
|The default minimum age (in days) Curator uses for deleting log records.

|`openshift_logging_curator_run_hour`
|The hour of the day Curator will run.

|`openshift_logging_curator_run_minute`
| The minute of the hour Curator will run.

|`openshift_logging_curator_run_timezone`
|The timezone Curator uses for figuring out its run time. Provide the
timezone as a string in the tzselect(8) or timedatectl(1) "Region/Locality"
format, for example `America/New_York` or `UTC`.

|`openshift_logging_curator_script_log_level`
|The script log level for Curator.

|`openshift_logging_curator_log_level`
|The log level for the Curator process.

|`openshift_logging_curator_cpu_limit`
|The amount of CPU to allocate to Curator.

|`openshift_logging_curator_memory_limit`
|The amount of memory to allocate to Curator.

|`openshift_logging_curator_nodeselector`
|A node selector that specifies
which nodes are eligible targets for deploying Curator instances.

|`openshift_logging_curator_ops_cpu_limit`
|Equivalent to `openshift_logging_curator_cpu_limit` for Ops cluster.

|`openshift_logging_curator_ops_memory_limit`
|Equivalent to `openshift_logging_curator_memory_limit` for Ops cluster.

|`openshift_logging_curator_ops_nodeselector`
|A node selector that specifies which nodes are eligible targets
for deploying Curator instances.
|===

.Parameters for configuring Fluentd
[cols="3,7",options="header"]
|===
|Parameter
|Description

|`openshift_logging_fluentd_nodeselector`
|A node selector that specifies which nodes are eligible targets
for deploying Fluentd instances.
Any node where Fluentd should run (typically, all) must have this label
before Fluentd is able to run and collect logs.

When scaling up the Aggregated Logging cluster after installation,
the `openshift_logging` role labels nodes provided by
`openshift_logging_fluentd_hosts` with this node selector.

As part of the installation, it is recommended that you add the Fluentd node
selector label to the list of persisted node labels.

|`openshift_logging_fluentd_cpu_limit`
|The CPU limit for Fluentd pods.

|`openshift_logging_fluentd_memory_limit`
|The memory limit for Fluentd pods.

|`openshift_logging_fluentd_journal_read_from_head`
|Set to `true` if Fluentd should read from the head of Journal when first
starting up, using this may cause a delay in ES receiving current log records.

|`openshift_logging_fluentd_hosts`
|List of nodes that should be labeled for Fluentd to be deployed. The default is
to label all nodes with ['--all']. The null value is
`openshift_logging_fluentd_hosts={}`.
To spin up Fluentd pods update the daemonset's `nodeSelector` to a valid label. For
example, ['host1.example.com', 'host2.example.com'].

|`openshift_logging_fluentd_audit_container_engine`
|When `openshift_logging_fluentd_audit_container_engine` is set to `true`, the
audit log of the container engine is collected and stored in ES. Enabling this
variable allows the EFK to watch the specified audit log file or the
default `/var/log/audit.log` file, collects audit information for the container
engine for the platform, then puts it into Kibana.

|`openshift_logging_fluentd_audit_file`
|Location of audit log file. The default is `/var/log/audit/audit.log`. Enabling this
variable allows the EFK to watch the specified audit log file or the
default `/var/log/audit.log` file, collects audit information for the container
engine for the platform, then puts it into Kibana.

|`openshift_logging_fluentd_audit_pos_file`
|Location of the Fluentd `in_tail` position file for the audit log file. The default is
`/var/log/audit/audit.log.pos`. Enabling this
variable allows the EFK to watch the specified audit log file or the
default `/var/log/audit.log` file, collects audit information for the container
engine for the platform, then puts it into Kibana.
|===
