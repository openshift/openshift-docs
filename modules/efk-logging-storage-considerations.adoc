// Module included in the following assemblies:
//
// * logging/efk-logging-deploy.adoc

[id='efk-logging-storage-considerations_{context}']
= Storage considerations for cluster logging and {product-title}

An Elasticsearch index is a collection of shards and its corresponding replica
shards. This is how ES implements high availability internally, therefore there
is little need to use hardware based mirroring RAID variants. RAID 0 can still
be used to increase overall disk performance.

Every search request needs to hit a copy of every shard in the index. Each ES
instance requires its own individual storage, but an {product-title} deployment
can only provide volumes shared by all of its pods, which again means that
Elasticsearch shouldn't be implemented with a single node.

A persistent volume should be added to each Elasticsearch deployment configuration so that
we have one volume per replica shard. On {product-title} this is often achieved through
Persistent Volume Claims:

* 1 volume per shard

* 1 volume per replica shard

The PVCs must be named based on the *openshift_logging_es_pvc_prefix* setting. Refer to
Persistent Elasticsearch Storage for more details.

Below are capacity planning guidelines for {product-title} aggregate logging.

*Example scenario*

Assumptions:

. Which application: Apache
. Bytes per line: 256
. Lines per second load on application: 1
. Raw text data -> JSON

Baseline (256 characters per minute -> 15KB/min)

[cols="3,4",options="header"]
|===
|Logging Infra Pods
|Storage Throughput

|3 es
1 kibana
1 curator
1 fluentd
| 6 pods total: 90000 x 86400 = 7,7 GB/day

|3 es
1 kibana
1 curator
11 fluentd
| 16 pods total: 225000 x 86400 = 24,0 GB/day

|3 es
1 kibana
1 curator
20 fluentd
|25 pods total: 225000 x 86400 = 32,4 GB/day
|===


Calculating total logging throughput and disk space required for your logging
environment requires knowledge of your application. For example, if one of your
applications on average logs 10 lines-per-second, each 256 bytes-per-line,
calculate per-application throughput and disk space as follows:
----
 (bytes-per-line * (lines-per-second) = 2560 bytes per app per second
 (2560) * (number-of-pods-per-node,100) = 256,000 bytes per second per node
 256k * (number-of-nodes) = total logging throughput per cluster
----

Fluentd ships any logs from *systemd journal* and */var/lib/docker/containers/* to Elasticsearch.

Local SSD drives are recommended in order to achieve the best performance. In
Red Hat Enterprise Linux (RHEL) 7, the
link:https://access.redhat.com/articles/425823[deadline] IO scheduler is the
default for all block devices except SATA disks. For SATA disks, the default IO
scheduler is *cfq*.

Sizing storage for ES is greatly dependent on how you optimize your indices.
Therefore, consider how much data you need in advance and that you are
aggregating application log data. Some Elasticsearch users have found that it
is necessary to
link:https://signalfx.com/blog/how-we-monitor-and-run-elasticsearch-at-scale/[keep
absolute storage consumption around 50% and below 70% at all times]. This
helps to avoid Elasticsearch becoming unresponsive during large merge
operations.
