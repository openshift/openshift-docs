// Module included in the following assemblies:
//
// * observability/monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-alert-routing-default-platform-alerts_{context}"]
= Configuring alert routing for default platform alerts

[role="_abstract"]
You can configure Alertmanager to send notifications to receive important alerts coming from the cluster. Customize where and how Alertmanager sends notifications about default platform alerts by editing the default configuration in the `alertmanager-main` secret in the `openshift-monitoring` project.

[NOTE]
====
All features of a supported version of upstream Alertmanager are also supported in an {ocp} Alertmanager configuration. To check all the configuration options of a supported version of upstream Alertmanager, see link:https://prometheus.io/docs/alerting/{alertmanager-version}/configuration/[Alertmanager configuration] (Prometheus documentation).
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have installed the {oc-first}.

.Procedure

. Extract the currently active Alertmanager configuration from the `alertmanager-main` secret and save it as a local `alertmanager.yaml` file:
+
[source,terminal]
----
$ oc -n openshift-monitoring get secret alertmanager-main --template='{{ index .data "alertmanager.yaml" }}' | base64 --decode > alertmanager.yaml
----

. Open the `alertmanager.yaml` file.

. Edit the Alertmanager configuration:

.. Optional: Change the default Alertmanager configuration:
+
.Example of the default Alertmanager secret YAML
[source,yaml]
----
global:
  resolve_timeout: 5m
  http_config:
    proxy_from_environment: true
route:
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: default
  routes:
  - matchers:
    - "alertname=Watchdog"
    repeat_interval: 2m
    receiver: watchdog
receivers:
- name: default
- name: watchdog
----
+
where:

`proxy_from_environment`::
Specifies whether to enable proxying for all alert receivers. If you configured an HTTP cluster-wide proxy, set this parameter to `true` to enable proxying for all alert receivers.

`group_wait`::
Specifies how long Alertmanager waits while collecting initial alerts for a group of alerts before sending a notification.

`group_interval`::
Specifies how much time must elapse before Alertmanager sends a notification about new alerts added to a group of alerts for which an initial notification was already sent.

`repeat_interval`::
Specifies the minimum amount of time that must pass before an alert notification is repeated. If you want a notification to repeat at each group interval, set the `repeat_interval` value to less than the `group_interval` value. The repeated notification can still be delayed, for example, when certain Alertmanager pods are restarted or rescheduled.

.. Add your alert receiver configuration:
+
[source,yaml]
----
# ...
receivers:
- name: default
- name: watchdog
- name: <receiver>
  <receiver_configuration>
# ...
----
+
--
where:

`<receiver>`::
Specifies the name of the receiver.

`<receiver_configuration>`::
Specifies the receiver configuration. The supported receivers are PagerDuty, webhook, email, Slack, and Microsoft Teams.
--
+
.Example of configuring PagerDuty as an alert receiver
[source,yaml]
----
# ...
receivers:
- name: default
- name: watchdog
- name: team-frontend-page
  pagerduty_configs:
  - routing_key: xxxxxxxxxx
    http_config:
      proxy_from_environment: true
      authorization:
        credentials: xxxxxxxxxx
# ...
----
+
--
where:

`routing_key`::
Specifies the PagerDuty integration key.

`http_config`::
(Optional) Adds the custom HTTP configuration for a specific receiver. That receiver does not inherit the global HTTP configuration settings.
--
+
.Example of configuring email as an alert receiver
[source,yaml]
----
# ...
receivers:
- name: default
- name: watchdog
- name: team-frontend-page
  email_configs:
    - to: myemail@example.com
      from: alertmanager@example.com
      smarthost: 'smtp.example.com:587'
      auth_username: alertmanager@example.com
      auth_password: password
      hello: alertmanager
# ...
----
+
--
where:

`to`::
Specifies an email address to send notifications to.

`from`::
Specifies an email address to send notifications from.

`smarthost`::
Specifies the SMTP server address used for sending emails, including the port number.

`auth_username`, `auth_password`::
Specify the authentication credentials that Alertmanager uses to connect to the SMTP server. This example uses username and password.

`hello`::
Specifies the hostname to identify to the SMTP server. If you do not include this parameter, the hostname defaults to `localhost`.
--
+
[IMPORTANT]
====
Alertmanager requires an external SMTP server to send email alerts. To configure email alert receivers, ensure you have the necessary connection details for an external SMTP server.
====
+
[WARNING]
====
Alertmanager requires an explicit TLS connection using STARTTLS. This results in the following behavior:

* Connections that transmit data without encryption to remote SMTP servers (unencrypted SMTP) are not supported.
* Implicit TLS connections are not supported, for example using `smtps://` on port 465.
* Only explicit TLS connections using STARTTLS are supported, for example using `smtp://` on port 587 with STARTTLS enabled.
====

.. Add the routing configuration:
+
[source,yaml]
----
# ...
route:
  group_wait: 30s 
  group_interval: 5m 
  repeat_interval: 12h
  receiver: default
  routes:
  - matchers:
    - "alertname=Watchdog"
    repeat_interval: 2m
    receiver: watchdog
  - matchers:
    - "<your_matching_rules>"
    receiver: <receiver>
# ...
----
+
--
where:

`matchers`::
Defines the matching rules that an alert has to fulfill to match the node. If you define inhibition rules, use `target_matchers` key name for target matchers and `source_matchers` key name for source matchers.

`<your_matching_rules>`::
Specifies labels to match your alerts.

`<receiver>`::
Specifies the name of the receiver to use for the alerts.
--
+
[WARNING]
====
Do not use the `match`, `match_re`, `target_match`, `target_match_re`, `source_match`, and `source_match_re` key names, which are deprecated and planned for removal in a future release.
====
+
.Example of alert routing 
[source,yaml]
----
# ...
route:
  group_wait: 30s 
  group_interval: 5m 
  repeat_interval: 12h
  receiver: default
  routes:
  # ...
  - matchers:
    - "service=example-app"
    routes:
    - matchers:
      - "severity=critical"
      receiver: team-frontend-page
# ...
----

* `"service=example-app"` matching rule matches alerts from the `example-app` service.
* You can specify routes within other routes for more complex alert routing. In this example, `route.routes.matchers.routes` matches alerts of `critical` severity that are fired by the `example-app` service to the `team-frontend-page` receiver. Typically, these types of alerts are paged to an individual or a critical response team.

. Apply the new configuration in the file:
+
[source,terminal]
----
$ oc -n openshift-monitoring create secret generic alertmanager-main --from-file=alertmanager.yaml --dry-run=client -o=yaml |  oc -n openshift-monitoring replace secret --filename=-
----

.Verification

* Verify your routing configuration by visualizing the routing tree:
+
[source,terminal]
----
$ oc exec alertmanager-main-0 -n openshift-monitoring -- amtool config routes show --alertmanager.url http://localhost:9093
----
+
.Example output
[source,terminal]
----
Routing tree:
.
└── default-route  receiver: default
    ├── {alertname="Watchdog"}  receiver: Watchdog
    └── {service="example-app"}  receiver: default
        └── {severity="critical"}  receiver: team-frontend-page
----
