// Module included in the following assemblies:
//
// * observability/monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-alert-routing-default-platform-alerts_{context}"]
= Configuring alert routing for default platform alerts

You can configure Alertmanager to send notifications. Customize where and how Alertmanager sends notifications about default platform alerts by editing the default configuration in the `alertmanager-main` secret in the `openshift-monitoring` namespace.

[NOTE]
====
All features of a supported version of upstream Alertmanager are also supported in an {product-title} Alertmanager configuration. To check all the configuration options of a supported version of upstream Alertmanager, see link:https://prometheus.io/docs/alerting/0.27/configuration/[Alertmanager configuration] (Prometheus documentation).
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.

.Procedure

. Open the Alertmanager YAML configuration file:

** To open the Alertmanager configuration from the CLI:

.. Print the currently active Alertmanager configuration from the `alertmanager-main` secret into `alertmanager.yaml` file:
+
[source,terminal]
----
$ oc -n openshift-monitoring get secret alertmanager-main --template='{{ index .data "alertmanager.yaml" }}' | base64 --decode > alertmanager.yaml
----

.. Open the `alertmanager.yaml` file.

** To open the Alertmanager configuration from the {product-title} web console:

.. Go to the *Administration* -> *Cluster Settings* -> *Configuration* -> *Alertmanager* -> *YAML* page of the web console.

. Edit the Alertmanager configuration by updating parameters in the YAML:
+
[source,yaml]
----
global:
  resolve_timeout: 5m
  http_config:
    proxy_from_environment: true # <1>
route:
  group_wait: 30s # <2>
  group_interval: 5m # <3>
  repeat_interval: 12h # <4>
  receiver: default
  routes:
  - matchers:
    - "alertname=Watchdog"
    repeat_interval: 2m
    receiver: watchdog
  - matchers:
    - "service=<your_service>" # <5>
    routes:
    - matchers:
      - <your_matching_rules> # <6>
      receiver: <receiver> # <7>
receivers:
- name: default
- name: watchdog
- name: <receiver>
  <receiver_configuration> # <8>
----
<1> If you configured an HTTP cluster-wide proxy, set the `proxy_from_environment` parameter to `true` to enable proxying for all alert receivers.
<2> Specify how long Alertmanager waits while collecting initial alerts for a group of alerts before sending a notification.
<3> Specify how much time must elapse before Alertmanager sends a notification about new alerts added to a group of alerts for which an initial notification was already sent.
<4> Specify the minimum amount of time that must pass before an alert notification is repeated.
If you want a notification to repeat at each group interval, set the `repeat_interval` value to less than the `group_interval` value.
The repeated notification can still be delayed, for example, when certain Alertmanager pods are restarted or rescheduled.
<5> Specify the name of the service that fires the alerts.
<6> Specify labels to match your alerts.
<7> Specify the name of the receiver to use for the alerts.
<8> Specify the receiver configuration.
+
[IMPORTANT]
====
* Use the `matchers` key name to indicate the matchers that an alert has to fulfill to match the node.
Do not use the `match` or `match_re` key names, which are both deprecated and planned for removal in a future release.

* If you define inhibition rules, use the following key names:
+
--
** `target_matchers`: to indicate the target matchers
** `source_matchers`: to indicate the source matchers
--
+
Do not use the `target_match`, `target_match_re`, `source_match`, or `source_match_re` key names, which are deprecated and planned for removal in a future release.
====
+
.Example of Alertmanager configuration with PagerDuty as an alert receiver
[source,yaml]
----
global:
  resolve_timeout: 5m
  http_config:
    proxy_from_environment: true
route:
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: default
  routes:
  - matchers:
    - "alertname=Watchdog"
    repeat_interval: 2m
    receiver: watchdog
  - matchers: # <1>
    - "service=example-app"
    routes:
    - matchers:
      - "severity=critical"
      receiver: team-frontend-page
receivers:
- name: default
- name: watchdog
- name: team-frontend-page
  pagerduty_configs:
  - service_key: "<your_key>"
    http_config: # <2> 
      proxy_from_environment: true
      authorization:
        credentials: xxxxxxxxxx
----
<1> Alerts of `critical` severity that are fired by the `example-app` service are sent through the `team-frontend-page` receiver. Typically, these types of alerts would be paged to an individual or a critical response team.
<2> Custom HTTP configuration for a specific receiver. If you configure the custom HTTP configuration for a specific alert receiver, that receiver does not inherit the global HTTP config settings.

. Apply the new configuration in the file:

** To apply the changes from the CLI, run the following command:
+
[source,terminal]
----
$ oc -n openshift-monitoring create secret generic alertmanager-main --from-file=alertmanager.yaml --dry-run=client -o=yaml |  oc -n openshift-monitoring replace secret --filename=-
----

** To apply the changes from the {product-title} web console, click *Save*.
