// Module included in the following assembly:
//
// * hcp-backup-restore-dr.adoc

:_mod-docs-content-type: PROCEDURE
[id="hosted-cluster-etcd-backup-restore-on-prem_{context}"]
= Backing up and restoring etcd on a hosted cluster in an on-premise environment

By backing up and restoring etcd on a hosted cluster, you can fix failures, such as corrupted or missing data in an etcd member of a three node cluster. If multiple members of the etcd cluster encounter data loss or have a `CrashLoopBackOff` status, this approach helps prevent an etcd quorum loss.

[IMPORTANT]
====
This procedure requires API downtime.
====

.Prerequisites

* The `oc` and `jq` binaries have been installed.

.Procedure

. First, set up your environment variables and scale down the API servers:

.. Set up environment variables for your hosted cluster by entering the following commands, replacing values as necessary:
+
[source,terminal]
----
$ CLUSTER_NAME=my-cluster
----
+
[source,terminal]
----
$ HOSTED_CLUSTER_NAMESPACE=clusters
----
+
[source,terminal]
----
$ CONTROL_PLANE_NAMESPACE="${HOSTED_CLUSTER_NAMESPACE}-${CLUSTER_NAME}"
----

.. Pause reconciliation of the hosted cluster by entering the following command, replacing values as necessary:
+
[source,terminal]
----
$ oc patch -n <hosted_cluster_namespace> hostedclusters/<hosted_cluster_name> -p '{"spec":{"pausedUntil":"true"}}' --type=merge
----

.. Scale down the API servers by entering the following commands:
+
... Scale down the `kube-apiserver` deployment:
+
[source,terminal]
----
$ oc scale -n <control_plane_namespace> deployment/kube-apiserver --replicas=0
----

... Scale down the `openshift-apiserver` deployment:
+
[source,terminal]
----
$ oc scale -n <control_plane_namespace> deployment/openshift-apiserver --replicas=0
----

... Scale down the `openshift-oauth-apiserver` deployment:
+
[source,terminal]
----
$ oc scale -n <control_plane_namespace> deployment/openshift-oauth-apiserver --replicas=0
----

. Next, take a snapshot of etcd by using one of the following methods:

.. Use a previously backed-up snapshot of etcd.
.. If you have an available etcd pod, take a snapshot from the active etcd pod by completing the following steps:

... List etcd pods by entering the following command:
+
[source,terminal]
----
$ oc get -n <control_plane_namespace> pods -l app=etcd
----

... Take a snapshot of the pod database and save it locally to your machine by entering the following command:
+
[source,terminal]
----
$ oc exec -n <control_plane_namespace> -c etcd -t <etcd_pod_name> -- env ETCDCTL_API=3 /usr/bin/etcdctl \
--cacert /etc/etcd/tls/etcd-ca/ca.crt \
--cert /etc/etcd/tls/client/etcd-client.crt \
--key /etc/etcd/tls/client/etcd-client.key \
--endpoints=https://localhost:2379 \
snapshot save /var/lib/snapshot.db
----

... Verify that the snapshot is successful by entering the following command:
+
[source,terminal]
----
$ oc exec -n <control_plane_namespace> -c etcd -t <etcd_pod_name> -- env ETCDCTL_API=3 /usr/bin/etcdctl -w table snapshot status /var/lib/snapshot.db
----

.. Make a local copy of the snapshot by entering the following command:
+
[source,terminal]
----
$ oc cp -c etcd <control_plane_namespace>/<etcd_pod_name>:/var/lib/snapshot.db /tmp/etcd.snapshot.db
----

... Make a copy of the snapshot database from etcd persistent storage:
+
.... List etcd pods by entering the following command:
+
[source,terminal]
----
$ oc get -n <control_plane_namespace> pods -l app=etcd
----

.... Find a pod that is running and set its name as the value of `ETCD_POD: ETCD_POD=etcd-0`, and then copy its snapshot database by entering the following command:
+
[source,terminal]
----
$ oc cp -c etcd <control_plane_namespace>/${ETCD_POD}:/var/lib/data/member/snap/db /tmp/etcd.snapshot.db
----

. Next, scale down the etcd statefulset by entering the following command:
+
[source,terminal]
----
$ oc scale -n <control_plane_namespace> statefulset/etcd --replicas=0
----

.. Delete volumes for second and third members by entering the following command:
+
[source,terminal]
----
$ oc delete -n <control_plane_namespace> pvc/<etcd_pvc_name_1> pvc/<etcd_pvc_name_2>
----

.. Create a pod to access the first etcd member's data:

... Get the etcd image by entering the following command:
+
[source,terminal]
----
$ oc get -n <control_plane_namespace> statefulset/etcd -o jsonpath='{ .spec.template.spec.containers[0].image }')
----
+
Note the etcd image value to use in the next step.

... Create a pod that allows access to etcd data:
+
[source,yaml]
----
$ cat << EOF | oc apply -n <control_plane_namespace> -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: etcd-data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: etcd-data
  template:
    metadata:
      labels:
        app: etcd-data
    spec:
      containers:
      - name: access
        image: <etcd_image>
        volumeMounts:
        - name: data
          mountPath: /var/lib
        command:
        - /usr/bin/bash
        args:
        - -c
        - |-
          while true; do
            sleep 1000
          done
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: <etcd_pvc_name>
EOF
----

... Check the status of the `etcd-data` pod and wait for it to be running by entering the following command:
+
[source,terminal]
----
$ oc get -n <control_plane_namespace> pods -l app=etcd-data
----

... Get the name of the `etcd-data` pod by entering the following command:
+
[source,terminal]
----
$ oc get -n <control_plane_namespace> pods --no-headers -l app=etcd-data -o name | cut -d/ -f2)
----
+
Note the `etcd-data` pod name to use in the next steps.

.. Copy an etcd snapshot into the pod by entering the following command:
+
[source,terminal]
----
$ oc cp /tmp/etcd.snapshot.db <control_plane_namespace>/<etcd_data_pod_name>:/var/lib/restored.snap.db
----

.. Remove old data from the `etcd-data` pod by entering the following commands:
+
[source,terminal]
----
$ oc exec -n <control_plane_namespace> <etcd_data_pod_name> -- rm -rf /var/lib/data
----
+
[source,terminal]
----
$ oc exec -n <control_plane_namespace> <etcd_data_pod_name> -- mkdir -p /var/lib/data
----

.. Restore the etcd snapshot by entering the following command:
+
[source,terminal]
----
$ oc exec -n <control_plane_namespace> <etcd_data_pod_name> -- etcdutl snapshot restore /var/lib/restored.snap.db \
     --data-dir=/var/lib/data --skip-hash-check \
     --name etcd-0 \
     --initial-cluster-token=etcd-cluster \
     --initial-cluster etcd-0=https://etcd-0.etcd-discovery.<control_plane_namespace>.svc:2380,etcd-1=https://etcd-1.etcd-discovery.<control_plane_namespace>.svc:2380,etcd-2=https://etcd-2.etcd-discovery.<control_plane_namespace>.svc:2380 \
     --initial-advertise-peer-urls https://etcd-0.etcd-discovery.<control_plane_namespace>.svc:2380
----

.. Remove the temporary etcd snapshot from the pod by entering the following command:
+
[source,terminal]
----
$ oc exec -n <control_plane_namespace> <etcd_data_pod_name> -- rm /var/lib/restored.snap.db
----

.. Delete data access deployment by entering the following command:
+
[source,terminal]
----
$ oc delete -n <control_plane_namespace> deployment/etcd-data
----

.. Scale up the etcd cluster by entering the following command:
+
[source,terminal]
----
$ oc scale -n <control_plane_namespace> statefulset/etcd --replicas=3
----

.. Wait for the etcd member pods to return and report as available by entering the following command:
+
[source,terminal]
----
$ oc get -n <control_plane_namespace> pods -l app=etcd -w
----

.. Scale up all etcd-writer deployments by entering the following command:
+
[source,terminal]
----
$ oc scale deployment -n <control_plane_namespace> --replicas=3 kube-apiserver openshift-apiserver openshift-oauth-apiserver
----

. Restore reconciliation of the hosted cluster by entering the following command:
+
[source,terminal]
----
$ oc patch -n <hosted_cluster_namespace> hostedclusters/<hosted_cluster_name> -p '{"spec":{"pausedUntil":""}}' --type=merge
----
