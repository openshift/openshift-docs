// Module included in the following assemblies:
//
// network_observability/configuring-operator.adoc

:_content-type: PROCEDURE
[id="network-observability-enriched-flows-kafka_{context}"]
= Export enriched network flow data

You can send network flows to Kafka, so that they can be consumed by any processor or storage that supports Kafka input, such as Splunk, Elasticsearch, or Fluentd.

.Prerequisites
* Installed Kafka

.Procedure

. In the web console, navigate to *Operators* -> *Installed Operators*.
. Under the *Provided APIs* heading for the *NetObserv Operator*, select *Flow Collector*. 
. Select *cluster* and then select the *YAML* tab.
. Edit the `FlowCollector` to configure `spec.exporters` as follows:
+
[source,yaml]
----
apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
spec:
  exporters:
  - type: KAFKA
      kafka:
        address: "kafka-cluster-kafka-bootstrap.netobserv"
        topic: netobserv-flows-export   <1>
        tls: 
          enable: false                 <2>
  
----
<1> The Network Observability Operator exports all flows to the configured Kafka topic.
<2> You can encrypt all communications to and from Kafka with SSL/TLS or mTLS. When enabled, the Kafka CA certificate must be available as a ConfigMap or a Secret, both in the namespace where the `flowlogs-pipeline` processor component is deployed (default: netobserv). It must be referenced with `spec.exporters.tls.caCert`. When using mTLS, client secrets must be available in these namespaces as well (they can be generated for instance using the AMQ Streams User Operator) and referenced with `spec.exporters.tls.userCert`.
. After configuration, network flows data can be sent to an available output in a JSON format. For more information, see _Network flows format reference_