// Module included in the following assemblies:
//
// * installing/installing_ibm_z/installing-ibm-z.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc

:_mod-docs-content-type: CONCEPT
[id="preferred-ibm-z-system-requirements_{context}"]
= Preferred {ibm-z-title} system environment

[discrete]
== Hardware requirements

* Three LPARS that each have the equivalent of six IFLs, which are SMT2 enabled, for each cluster.
* Two network connections to both connect to the `LoadBalancer` service and to serve data for traffic outside the cluster.
* HiperSockets, which are attached to a node either directly as a device or by bridging with one z/VM VSWITCH to be transparent to the z/VM guest. To directly connect HiperSockets to a node, you must set up a gateway to the external network via a {op-system-base} 8 guest to bridge to the HiperSockets network.

[discrete]
== Operating system requirements

* Two or three instances of z/VM 7.2 or later for high availability

On your z/VM instances, set up:

* Three guest virtual machines for {product-title} control plane machines, one per z/VM instance.
* At least six guest virtual machines for {product-title} compute machines, distributed across the z/VM instances.
* One guest virtual machine for the temporary {product-title} bootstrap machine.
* To ensure the availability of integral components in an overcommitted environment, increase the priority of the control plane by using the CP command `SET SHARE`. Do the same for infrastructure nodes, if they exist. See link:https://www.ibm.com/docs/en/zvm/latest?topic=commands-set-share[SET SHARE] in {ibm-name} Documentation.

[discrete]
== {ibm-z-title} network connectivity requirements

To install on {ibm-z-name} under z/VM, you require a single z/VM virtual NIC in layer 2 mode. You also need:

*   A direct-attached OSA or RoCE network adapter
*   A z/VM VSwitch set up. For a preferred setup, use OSA link aggregation.

[discrete]
=== Disk storage for the z/VM guest virtual machines

* FICON attached disk storage (DASDs). These can be z/VM minidisks, fullpack minidisks, or dedicated DASDs, all of which must be formatted as CDL, which is the default. To reach the minimum required DASD size for {op-system-first} installations, you need extended address volumes (EAV). If available, use HyperPAV and High Performance FICON (zHPF) to ensure optimal performance.
* FCP attached disk storage

[discrete]
=== Storage / Main Memory

* 16 GB for {product-title} control plane machines
* 8 GB for {product-title} compute machines
* 16 GB for the temporary {product-title} bootstrap machine
