// Module included in the following assemblies:
//
// * serverless/develop/serverless-kafka-developer.adoc

:_mod-docs-content-type: PROCEDURE
[id="serverless-kafka-source-odc_{context}"]
= Creating an Apache Kafka event source by using the web console

After the Knative broker implementation for Apache Kafka is installed on your cluster, you can create an Apache Kafka source by using the web console. Using the {product-title} web console provides a streamlined and intuitive user interface to create a Kafka source.

.Prerequisites

* The {ServerlessOperatorName}, Knative Eventing, and the `KnativeKafka` custom resource are installed on your cluster.
* You have logged in to the web console.
* You have access to a Red Hat AMQ Streams (Kafka) cluster that produces the Kafka messages you want to import.
* You have created a project or have access to a project with the appropriate roles and permissions to create applications and other workloads in {product-title}.

.Procedure

. In the *Developer* perspective, navigate to the *+Add* page and select *Event Source*.
. In the *Event Sources* page, select *Kafka Source* in the *Type* section.
. Configure the *Kafka Source* settings:
.. Add a comma-separated list of *Bootstrap Servers*.
.. Add a comma-separated list of *Topics*.
.. Add a *Consumer Group*.
.. Select the *Service Account Name* for the service account that you created.
.. Select the *Sink* for the event source. A *Sink* can be either a *Resource*, such as a channel, broker, or service, or a *URI*.
.. Enter a *Name* for the Kafka event source.
. Click *Create*.

.Verification

You can verify that the Kafka event source was created and is connected to the sink by viewing the *Topology* page.

. In the *Developer* perspective, navigate to *Topology*.
. View the Kafka event source and sink.
+
image::verify-kafka-ODC.png[View the Kafka source and service in the Topology view]
