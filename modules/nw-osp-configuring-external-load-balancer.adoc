// Module included in the following assemblies:
// TODO
// * networking/TBD
// * networking/load-balancing-openstack.adoc
// * installing/installing_bare_metal_ipi/ipi-install-post-installation-configuration.adoc jowilkin
// * installing/installing-vsphere-installer-provisioned.adoc
// * installing/installing-vsphere-installer-provisioned-customizations.adoc
// * installing/installing-vsphere-installer-provisioned-network-customizations.adoc
// * installing/installing-restricted-networks-installer-provisioned-vsphere.adoc


ifeval::["{context}" == "installing-vsphere-installer-provisioned"]
:vsphere:
endif::[]
ifeval::["{context}" == "installing-vsphere-installer-provisioned-customizations"]
:vsphere:
endif::[]
ifeval::["{context}" == "installing-vsphere-installer-provisioned-network-customizations"]
:vsphere:
endif::[]
ifeval::["{context}" == installing-restricted-networks-installer-provisioned-vsphere]
:vsphere:
endif::[]

:_content-type: PROCEDURE
[id="nw-osp-configuring-external-load-balancer_{context}"]
= Configuring an external load balancer

You can configure an {product-title} cluster
ifeval::["{context}" == "load-balancing-openstack"]
on {rh-openstack-first}
endif::[]
to use an external load balancer in place of the default load balancer.

You can also configure an {product-title} cluster to use an external load balancer that supports multiple subnets. If you use multiple subnets, you can explicitly list all the IP addresses in any networks that are used by your load balancer targets. This configuration can reduce maintenance overhead because you can create and destroy nodes within those networks without reconfiguring the load balancer targets.

If you deploy your ingress pods by using a machine set on a smaller network, such as a `/27` or `/28`, you can simplify your load balancer targets.

[NOTE]
====
You do not need to specify API and Ingress static addresses for your installation program. If you choose this configuration, you must take additional actions to define network targets that accept an IP address from each referenced vSphere subnet.
====

.Prerequisites

* On your load balancer, TCP over ports 6443, 443, and 80 must be reachable by all users of your system that are located outside the cluster.

* Load balance the application ports, 443 and 80, between all the compute nodes.

* Load balance the API port, 6443, between each of the control plane nodes.

* On your load balancer, port 22623, which is used to serve ignition startup configurations to nodes, is not exposed outside of the cluster.

* Your load balancer can access the required ports on each node in your cluster. You can ensure this level of access by completing the following actions:
** The API load balancer can access ports 22623 and 6443 on the control plane nodes.
** The ingress load balancer can access ports 443 and 80 on the nodes where the ingress pods are located.

ifdef::vsphere[]
* Optional: If you are using multiple networks, you can create targets for every IP address in the network that can host nodes. This configuration can reduce the maintenance overhead of your cluster.
endif::vsphere[]

.Procedure

. Enable access to the cluster from your load balancer on ports 6443, 443, and 80.
+
As an example, note this HAProxy configuration:
+
.A section of a sample HAProxy configuration
[source,text]
----
...
listen my-cluster-api-6443
    bind 0.0.0.0:6443
    mode tcp
    balance roundrobin
    server my-cluster-master-2 192.0.2.2:6443 check
    server my-cluster-master-0 192.0.2.3:6443 check
    server my-cluster-master-1 192.0.2.1:6443 check
listen my-cluster-apps-443
        bind 0.0.0.0:443
        mode tcp
        balance roundrobin
        server my-cluster-worker-0 192.0.2.6:443 check
        server my-cluster-worker-1 192.0.2.5:443 check
        server my-cluster-worker-2 192.0.2.4:443 check
listen my-cluster-apps-80
        bind 0.0.0.0:80
        mode tcp
        balance roundrobin
        server my-cluster-worker-0 192.0.2.7:80 check
        server my-cluster-worker-1 192.0.2.9:80 check
        server my-cluster-worker-2 192.0.2.8:80 check
----

. Add records to your DNS server for the cluster API and apps over the load balancer. For example:
+
[source,dns]
----
<load_balancer_ip_address> api.<cluster_name>.<base_domain>
<load_balancer_ip_address> apps.<cluster_name>.<base_domain>
----

. From a command line, use `curl` to verify that the external load balancer and DNS configuration are operational.

.. Verify that the cluster API is accessible:
+
[source,terminal]
----
$ curl https://<loadbalancer_ip_address>:6443/version --insecure
----
+
If the configuration is correct, you receive a JSON object in response:
+
[source,json]
----
{
  "major": "1",
  "minor": "11+",
  "gitVersion": "v1.11.0+ad103ed",
  "gitCommit": "ad103ed",
  "gitTreeState": "clean",
  "buildDate": "2019-01-09T06:44:10Z",
  "goVersion": "go1.10.3",
  "compiler": "gc",
  "platform": "linux/amd64"
}
----

.. Verify that cluster applications are accessible:
+
[NOTE]
====
You can also verify application accessibility by opening the {product-title} console in a web browser.
====
+
[source, terminal]
----
$ curl http://console-openshift-console.apps.<cluster_name>.<base_domain> -I -L --insecure
----
+
If the configuration is correct, you receive an HTTP response:
+
[source,terminal]
----
HTTP/1.1 302 Found
content-length: 0
location: https://console-openshift-console.apps.<cluster-name>.<base domain>/
cache-control: no-cacheHTTP/1.1 200 OK
referrer-policy: strict-origin-when-cross-origin
set-cookie: csrf-token=39HoZgztDnzjJkq/JuLJMeoKNXlfiVv2YgZc09c3TBOBU4NI6kDXaJH1LdicNhN1UsQWzon4Dor9GWGfopaTEQ==; Path=/; Secure
x-content-type-options: nosniff
x-dns-prefetch-control: off
x-frame-options: DENY
x-xss-protection: 1; mode=block
date: Tue, 17 Nov 2020 08:42:10 GMT
content-type: text/html; charset=utf-8
set-cookie: 1e2670d92730b515ce3a1bb65da45062=9b714eb87e93cf34853e87a92d6894be; path=/; HttpOnly; Secure; SameSite=None
cache-control: private
----
