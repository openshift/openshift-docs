// This is included in the following assemblies:
//
// ipi-install-configuration-files.adoc
ifeval::["{context}" == "ipi-install-installation-workflow"]
:bare:
endif::[]
ifeval::["{context}" == "installing-vsphere-installer-provisioned-network-customizations"]
:vSphere:
endif::[]

:_mod-docs-content-type: PROCEDURE
[id='configure-network-components-to-run-on-the-control-plane_{context}']
= Configuring network components to run on the control plane

You can configure networking components to run exclusively on the control plane nodes. By default, {product-title} allows any node in the machine config pool to host the `ingressVIP` virtual IP address. However, some environments deploy worker nodes in separate subnets from the control plane nodes, which requires configuring the `ingressVIP` virtual IP address to run on the control plane nodes.

ifdef::vSphere[]
[NOTE]
====
You can scale the remote workers by creating a worker machineset in a separate subnet.
====
endif::vSphere[]

[IMPORTANT]
====
When deploying remote workers in separate subnets, you must place the `ingressVIP` virtual IP address exclusively with the control plane nodes.
====

ifdef::bare[]
image::161_OpenShift_Baremetal_IPI_Deployment_updates_0521.png[Installer-provisioned networking]
endif::bare[]
ifdef::vSphere[]
image::325_OpenShift_vSphere_Deployment_updates_0323.png[Installer-provisioned networking]
endif::vSphere[]

.Procedure

. Change to the directory storing the `install-config.yaml` file:
+
[source,terminal]
----
$ cd ~/clusterconfigs
----

. Switch to the `manifests` subdirectory:
+
[source,terminal]
----
$ cd manifests
----

. Create a file named `cluster-network-avoid-workers-99-config.yaml`:
+
[source,terminal]
----
$ touch cluster-network-avoid-workers-99-config.yaml
----

. Open the `cluster-network-avoid-workers-99-config.yaml` file in an editor and enter a custom resource (CR) that describes the Operator configuration:
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 50-worker-fix-ipi-rwn
  labels:
    machineconfiguration.openshift.io/role: worker
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/kubernetes/manifests/keepalived.yaml
          mode: 0644
          contents:
            source: data:,
----
+
This manifest places the `ingressVIP` virtual IP address on the control plane nodes. Additionally, this manifest deploys the following processes on the control plane nodes only:
+
* `openshift-ingress-operator`
+
* `keepalived`

. Save the `cluster-network-avoid-workers-99-config.yaml` file.

. Create a `manifests/cluster-ingress-default-ingresscontroller.yaml` file:
+
[source,yaml]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/master: ""
----

. Consider backing up the `manifests` directory. The installer deletes the `manifests/` directory when creating the cluster.

. Modify the `cluster-scheduler-02-config.yml` manifest to make the control plane nodes schedulable by setting the `mastersSchedulable` field to `true`. Control plane nodes are not schedulable by default. For example:
+
----
$ sed -i "s;mastersSchedulable: false;mastersSchedulable: true;g" clusterconfigs/manifests/cluster-scheduler-02-config.yml
----
+
[NOTE]
====
If control plane nodes are not schedulable after completing this procedure, deploying the cluster will fail.
====

ifeval::["{context}" == "ipi-install-installation-workflow"]
:!bare:
endif::[]
ifeval::["{context}" == "installing-vsphere-installer-provisioned-network-customizations"]
:!vSphere:
endif::[]
