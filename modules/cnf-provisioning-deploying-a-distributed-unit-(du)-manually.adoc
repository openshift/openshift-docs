// CNF-950 4.7 Provisioning and deploying a Distributed Unit (DU) manually
// Module included in the following assemblies:
//
// *cnf-provisioning-and-deploying-a-distributed-unit.adoc

[id="cnf-provisioning-deploying-a-distributed-unit-manually_{context}"]
= Provisioning and deploying a Distributed Unit (DU) manually

Radio access network (RAN) is composed of Centralized Units (CU), Distributed Units (DU) and Radio Units (RU).
RAN from the telecommunications standard perspective is shown below:

image::135_OpenShift_Distributed_Unit.svg[High level RAN overview]

From the three components composing RAN, the CU and DU can be virtualized and implemented as cloud-native functions.

The CU and DU split architecture is driven by real-time computing and networking requirements.
A DU can be seen as a real-time part of a telecommunication baseband unit.
One Distributed Unit may aggregate several cells. A CU can be seen as a non-realtime part of a baseband unit, aggregating
traffic from one or more Distributed Units.

A cell in the context of a DU can be seen as a real-time application performing intensive digital signal processing, data transfer,
and algorithmic tasks.
Cells often use hardware acceleration (FPGA, GPU, eASIC) for DSP processing offload, but there are also software-only implementations
(FlexRAN), based on AVX-512 instructions.

Running cell application on COTS hardware requires the following features to be enabled:

* Real-time kernel
* CPU isolation
* NUMA awareness
* HUGEPAGES memory management
* Precision timing synchronization using PTP
* AVX-512 instruction set (for Flexran and / or FPGA implementation)
* Additional features depending on the RAN operator requirements

Accessing hardware acceleration devices and high throughput network interface cards by virtualized software applications
requires use of SR-IOV and Passthrough PCI device virtualization.

In addition to the compute and acceleration requirements, DUs operate on multiple internal and external networks.

== The manifest structure

The profile is built from one cluster specific folder and one or more site-specific folders.
This is done to address a deployment that includes remote worker nodes (several sites belonging to the same cluster).

The [`cluster-config`](ran-profile/cluster-config) directory contains performance and PTP customizations based upon
operator deployments in [`deploy`](../feature-configs/deploy) folder.

The [`site.1.fqdn`](site.1.fqdn) folder contains site-specific network customizations.

== Prerequisites

Before installing the operators and deploying the DU, perform the following steps.

. Clone the files in the `ran-profile` folder here:
link:https://github.com/openshift-kni/cnf-features-deploy/tree/master/feature-configs/cn-ran-overlays/ran-profile[ran-profile].

. Create a machine config pool for the RAN worker nodes. For example:
+
[source,yaml]
----
cat <<EOF | oc apply -f -
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-cnf
  labels:
    machineconfiguration.openshift.io/role: worker-cnf
spec:
  machineConfigSelector:
    matchExpressions:
      - {
          key: machineconfiguration.openshift.io/role,
          operator: In,
          values: [worker-cnf, worker],
        }
  paused: false
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-cnf: ""
----

. Include the worker node in the above machine config pool by labelling it with the `node-role.kubernetes.io/worker-cnf` label:
+
----
oc label --overwrite node/<your node name> node-role.kubernetes.io/worker-cnf=""
----

. Label the node as ptp slave (DU only):
+
----
oc label --overwrite node/<your node name> ptp/slave=""
----

An example of labeling the nodes used in CI/CD can be seen here:
link:https://raw.githubusercontent.com/openshift-kni/cnf-features-deploy/master/hack/setup-test-cluster.sh[setup-test-cluster.sh].


== Deployment

The profile is built in layers with `kustomize`. To get the profile output, run:

----
oc kustomize ran-profile
----

It can be applied manually or with the toolset of your choice (for example, ArgoCD).

This project contains makefile based tooling, that can be used as follows (from the project root):

----
FEATURES_ENVIRONMENT=cn-ran-overlays FEATURES=ran-profile make feature-deploy
----

== SR-IOV configuration notes

The SriovNetworkNodePolicy object must be configured differently for different NIC models and placements.

|====================
|*Manufacturer* |*deviceType* |*isRdma*
|Intel        |vfio-pci or netdevice |false
|Mellanox     |netdevice |structure
|====================

In addition, when configuring the `nicSelector`, the `pfNames` value must match the intended interface name on the specific host.

If there is a mixed cluster where some of the nodes are deployed with Intel NICs and some with Mellanox, several

SR-IOV configurations can be created with the same `resourceName`.
The device plugin will discover only the available ones and will put the capacity on the node accordingly.
