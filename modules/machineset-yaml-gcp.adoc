// Module included in the following assemblies:
//
// * machine_management/creating-infrastructure-machinesets.adoc
// * machine_management/creating-machineset-gcp.adoc

ifeval::["{context}" == "creating-infrastructure-machinesets"]
:infra:
endif::[]

:_mod-docs-content-type: REFERENCE
[id="machineset-yaml-gcp_{context}"]
=  Sample YAML for a compute machine set custom resource on GCP

This sample YAML defines a compute machine set that runs in Google Cloud Platform (GCP) and creates nodes that are labeled with
ifndef::infra[`node-role.kubernetes.io/<role>: ""`,]
ifdef::infra[`node-role.kubernetes.io/infra: ""`,]
where
ifndef::infra[`<role>`]
ifdef::infra[`infra`]
is the node label to add.

[discrete]
[id="cpmso-yaml-provider-spec-gcp-oc_{context}"]
== Values obtained by using the  OpenShift CLI

In the following example, you can obtain some of the values for your cluster by using the OpenShift CLI.

Infrastructure ID:: The `<infrastructure_id>` string is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
+
[source,terminal]
----
$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster
----

Image path:: The `<path_to_image>` string is the path to the image that was used to create the disk. If you have the OpenShift CLI installed, you can obtain the path to the image by running the following command:
+
[source,terminal]
----
$ oc -n openshift-machine-api \
  -o jsonpath='{.spec.template.spec.providerSpec.value.disks[0].image}{"\n"}' \
  get machineset/<infrastructure_id>-worker-a
----

.Sample GCP `MachineSet` values
[source,yaml]
----
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: <infrastructure_id> <1>
  name: <infrastructure_id>-w-a
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: <infrastructure_id>
      machine.openshift.io/cluster-api-machineset: <infrastructure_id>-w-a
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: <infrastructure_id>
ifndef::infra[]
        machine.openshift.io/cluster-api-machine-role: <role> <2>
        machine.openshift.io/cluster-api-machine-type: <role>
endif::infra[]
ifdef::infra[]
        machine.openshift.io/cluster-api-machine-role: <infra> <2>
        machine.openshift.io/cluster-api-machine-type: <infra>
endif::infra[]
        machine.openshift.io/cluster-api-machineset: <infrastructure_id>-w-a
    spec:
      metadata:
        labels:
ifndef::infra[]
          node-role.kubernetes.io/<role>: ""
endif::infra[]
ifdef::infra[]
          node-role.kubernetes.io/infra: ""
endif::infra[]
      providerSpec:
        value:
          apiVersion: gcpprovider.openshift.io/v1beta1
          canIPForward: false
          credentialsSecret:
            name: gcp-cloud-credentials
          deletionProtection: false
          disks:
          - autoDelete: true
            boot: true
            image: <path_to_image> <3>
            labels: null
            sizeGb: 128
            type: pd-ssd
          gcpMetadata: <4>
          - key: <custom_metadata_key>
            value: <custom_metadata_value>
          kind: GCPMachineProviderSpec
          machineType: n1-standard-4
          metadata:
            creationTimestamp: null
          networkInterfaces:
          - network: <infrastructure_id>-network
            subnetwork: <infrastructure_id>-worker-subnet
          projectID: <project_name> <5>
          region: us-central1
          serviceAccounts:
          - email: <infrastructure_id>-w@<project_name>.iam.gserviceaccount.com
            scopes:
            - https://www.googleapis.com/auth/cloud-platform
          tags:
            - <infrastructure_id>-worker
          userDataSecret:
            name: worker-user-data
          zone: us-central1-a
ifdef::infra[]
      taints: <6>
      - key: node-role.kubernetes.io/infra
        effect: NoSchedule
endif::infra[]
----
<1> For `<infrastructure_id>`, specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster.
ifndef::infra[]
<2> For `<node>`, specify the node label to add.
endif::infra[]
ifdef::infra[]
<2> For `<infra>`, specify the `<infra>` node label.
endif::infra[]
<3> Specify the path to the image that is used in current compute machine sets.
+
To use a GCP Marketplace image, specify the offer to use:
+
--
* {product-title}: `\https://www.googleapis.com/compute/v1/projects/redhat-marketplace-public/global/images/redhat-coreos-ocp-48-x86-64-202210040145`
* {opp}: `\https://www.googleapis.com/compute/v1/projects/redhat-marketplace-public/global/images/redhat-coreos-opp-48-x86-64-202206140145`
* {oke}: `\https://www.googleapis.com/compute/v1/projects/redhat-marketplace-public/global/images/redhat-coreos-oke-48-x86-64-202206140145`
--
<4> Optional: Specify custom metadata in the form of a `key:value` pair. For example use cases, see the GCP documentation for link:https://cloud.google.com/compute/docs/metadata/setting-custom-metadata[setting custom metadata].
<5> For `<project_name>`, specify the name of the GCP project that you use for your cluster.
ifdef::infra[]
<6> Specify a taint to prevent user workloads from being scheduled on infra nodes.
+
[NOTE]
====
After adding the `NoSchedule` taint on the infrastructure node, existing DNS pods running on that node are marked as `misscheduled`. You must either delete or link:https://access.redhat.com/solutions/6592171[add toleration on `misscheduled` DNS pods].
====
endif::infra[]

ifeval::["{context}" == "creating-infrastructure-machinesets"]
:!infra:
endif::[]
