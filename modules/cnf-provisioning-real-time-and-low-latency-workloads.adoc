// CNF-489 Real time and low latency workload provisioning
// Module included in the following assemblies:
//
// *cnf-performance-addon-operator-for-low-latency-nodes.adoc

[id="cnf-provisioning-real-time-and-low-latency-workloads_{context}"]
= Provisioning real-time and low latency workloads

Many industries and organizations need extremely high performance computing and may require low and predictable latency,
especially in the financial and telecommunications industries.
For these industries, with their unique requirements, OpenShift Container Platform (OCP) provides a
Performance Addon Operator to implement automatic tuning in order to achieve low latency performance and
consistent response time for OpenShift applications.
The cluster administrator uses this performance profile configuration that makes it easier to make these changes
in a more reliable way.
The administrator can specify whether to update the kernel to kernel-rt (real-time), the CPUs that will be reserved
for housekeeping, and the CPUs are used for running the workloads.

== Known limitations for real-time

To fully utilize the real-time mode, the containers need to run with elevated privileges.
See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container[Set capabilities for a Container] for information granting privileges.

OpenShift restricts the allowed capabilities, so you might need to create a SecurityContext as well, as explained here:
https://docs.openshift.com/container-platform/4.1/authentication/managing-security-context-constraints.html#security-context-constraints-creating_configuring-internal-oauth[Creating Security Context Constraints].

[NOTE]
====
This procedure is fully supported with bare metal installations using RHCOS systems.
====

Establishing the right performance expectations refers to the fact that the realtime kernel is not a panacea.
Its objective is consistent, low-latency determinism offering predictable response times.
There is some additional kernel overhead associated with the real-time kernel.
This is due primarily to handling hardware interrupts in separately scheduled threads.
The increased overhead in some workloads results in some degradation in overall throughput.
The exact amount is very workload dependent, ranging from 0% to 30%. However, it is the cost of determinism.

== Provisioning a worker with real-time capabilities


. Install Performance Addon Operator to the cluster.
See https://docs.openshift.com/container-platform-ocp/4.5/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html#installing-the-performance-addon-operator_cnf-master[Installing the Performance Addon Operator].
. (Optional) Add a node to the OCP cluster.
See https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/8/html-single/tuning_guide/index#Setting_BIOS_parameters[Setting BIOS parameters].
. (Optional) Create a new MachineConfigPool for RT nodes.
. Add the node to the proper MachineConfigPool, using node role labels.
+
You need to decide which nodes will be configured with real-time workloads.
It could be all of the nodes in the cluster or a subset of the nodes.
The Performance Addon Operator expects all of the nodes are part of a dedicated MachineConfigPool.
If you use all of the nodes, you just point the Performance Addon Operator to the worker node role label.
If you use a subset, you need to group the nodes into a new MachineConfigPool.

. Create the PerformanceProfile with the proper set of housekeeping cores and `realTimeKernel: enabled: true`.

. Specify a node selector in the PerformanceProfile, as shown here:
+
[source,yaml]
----
apiVersion: performance.openshift.io/v1
kind: PerformanceProfile
metadata:
 name: example-performanceprofile
spec:
 [...]
  realTimeKernel:
    enabled: true
  nodeSelector:
   node-role.kubernetes.io/worker-rt: ""
----

. Verify that a matching MachineConfigPool exists with a label:
+
----
machineconfiguration.openshift.io/role=worker-rt
----

. OpenShift will start configuring the nodes, which may involve multiple reboots. Wait for the nodes to settle.
This can take a long time depending on the specific hardware you use. We observed total time of about 20 minutes per node.

. Verify everything is working as expected.

== Changing to a real-time kernel

Use this command to verify that the real-time kernel is installed:

----
oc get node -o wide
----

Note the worker with the role `worker-rt` that contains the string `4.18.0-211.rt5.23.el8.x86_64`:
----
NAME                               	STATUS   ROLES           	AGE 	VERSION                  	INTERNAL-IP
EXTERNAL-IP   OS-IMAGE                                       	KERNEL-VERSION
CONTAINER-RUNTIME
cnf-worker-0.example.com	          Ready	 worker,worker-rt   5d17h   v1.19.0-rc.2+aaf4ce1-dirty
128.66.135.107   <none>    	        Red Hat Enterprise Linux CoreOS 46.82.202008252340-0 (Ootpa)
4.18.0-211.rt5.23.el8.x86_64   cri-o://1.19.0-90.rhaos4.6.git4a0ac05.el8-rc.1
[...]
----

== Creating a workload that works in real-time

Use the following procedures for preparing a workload that will use real-time capabilities.

* Create a Pod with a QoS class of Guaranteed
* (Optional) Disable CPU load balancing for DPDK
* Assign a proper node selector

When writing your applications, follow the general recommendations described here:
https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/8/html-single/tuning_guide/index#chap-Application_Tuning_and_Deployment[Application tuning and deployment].

== Creating a Pod with a QoS class of Guaranteed

Keep the following in mind when you create a Pod that is given a QoS class of Guaranteed:

* Every Container in the Pod must have a memory limit and a memory request, and they must be the same.
* Every Container in the Pod must have a CPU limit and a CPU request, and they must be the same.

The following example shows the configuration file for a Pod that has one Container.
The Container has a memory limit and a memory request, both equal to 200 MiB.
The Container has a CPU limit and a CPU request, both equal to 1 CPU.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-ctr
    image: <image-pull-spec>
    resources:
      limits:
        memory: "200Mi"
        cpu: "1"
      requests:
        memory: "200Mi"
        cpu: "1"
----

. Create the pod:
+
----
oc  apply -f qos-pod.yaml --namespace=qos-example
----

. View detailed information about the Pod:
+
----
oc get pod qos-demo --namespace=qos-example --output=yaml
----
+
[source,yaml]
----
spec:
  containers:
    ...
status:
  qosClass: Guaranteed
----
+
[NOTE]
====
If a Container specifies its own memory limit, but does not specify a memory request, OCP automatically assigns a memory
request that matches the limit.
Similarly, if a Container specifies its own CPU limit, but does not specify a CPU request, OCP automatically assigns a
CPU request that matches the limit.
====

== (Optional) Disabling CPU load balancing for DPDK

Functionality to disable/enable the CPU load balancing is implemented on the CRI-O level.
The code under the CRI-O disables or enables CPU load balancing only when:

* The pod uses the `performance-<profile-name>` runtime class. You can get the performance-<profile-name>
by looking at the status of the performance profile, as shown here:
+
----
apiVersion: performance.openshift.io/v1
kind: PerformanceProfile
...
status:
  ...
  runtimeClass: performance-manual
----

* The pod has `cpu-load-balancing.crio.io: true` annotation.

The Performance Addon Operator is responsible for the creation of the high-performance runtime handler config snippet.
It will have the same content as default runtime handler, under relevant nodes, and for creation of the high-performance
runtime class under the cluster.

To disable the CPU load balancing for the pod, the pod specification needs to include the following fields:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  ...
  annotations:
    ...
    cpu-load-balancing.crio.io: "true"
    ...
  ...
spec:
  ...
  runtimeClassName: performance-<profile_name>
  ...
----

[NOTE]
====
It is important to be aware that disabling CPU load balancing should be done only when the CPU manager static policy is enabled
and for pods with guaranteed QoS that use whole CPUs.
Otherwise, disabling CPU load balancing can affect the performance of other containers in the cluster.
====

== Assigning a proper node selector
The preferred way to assign a node selector is to use the same node selector the performance profile used, as shown here:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: example
spec:
  [...]
  nodeSelector:
     node-role.kubernetes.io/worker-rt: ""
----

For more information, see
https://access.redhat.com/documentation/en-us/openshift_container_platform/4.4/html-single/nodes/index#nodes-scheduler-node-selectors[Placing pods on specific nodes using node selectors].

== Scheduling a workload onto a worker with real-time capabilities
Use label selectors that match the nodes attached to the MachineConfigPool that was configured for low latency by
Performance Addon Operator.
For more information, see https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/[Assigning Pods to Nodes].
