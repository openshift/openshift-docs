// CNF-489 Real time and low latency workload provisioning
// Module included in the following assemblies:
//
// *cnf-low-latency-tuning.adoc

:_mod-docs-content-type: PROCEDURE
[id="cnf-provisioning-real-time-and-low-latency-workloads_{context}"]
= Provisioning real-time and low latency workloads

Many industries and organizations need extremely high performance computing and might require low and predictable latency, especially in the financial and telecommunications industries. For these industries, with their unique requirements, {product-title} provides the Node Tuning Operator to implement automatic tuning to achieve low latency performance and consistent response time for {product-title} applications.

The cluster administrator can use this performance profile configuration to make these changes in a more reliable way. The administrator can specify whether to update the kernel to kernel-rt (real-time), reserve CPUs for cluster and operating system housekeeping duties, including pod infra containers, isolate CPUs for application containers to run the workloads, and disable unused CPUs to reduce power consumption.

[WARNING]
====
The usage of execution probes in conjunction with applications that require guaranteed CPUs can cause latency spikes. It is recommended to use other probes, such as a properly configured set of network probes, as an alternative.
====

[NOTE]
====
In earlier versions of {product-title}, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance for OpenShift applications. In {product-title} 4.11 and later, these functions are part of the Node Tuning Operator.
====

[id="node-tuning-operator-known-limitations-for-real-time_{context}"]
== Known limitations for real-time

[NOTE]
====
In most deployments, kernel-rt is supported only on worker nodes when you use a standard cluster with three control plane nodes and three worker nodes. There are exceptions for compact and single nodes on {product-title} deployments. For installations on a single node, kernel-rt is supported on the single control plane node.
====

To fully utilize the real-time mode, the containers must run with elevated privileges.
See link:https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container[Set capabilities for a Container] for information on granting privileges.

{product-title} restricts the allowed capabilities, so you might need to create a `SecurityContext` as well.

[NOTE]
====
This procedure is fully supported with bare metal installations using {op-system-first} systems.
====

Establishing the right performance expectations refers to the fact that the real-time kernel is not a panacea. Its objective is consistent, low-latency determinism offering predictable response times. There is some additional kernel overhead associated with the real-time kernel. This is due primarily to handling hardware interruptions in separately scheduled threads. The increased overhead in some workloads results in some degradation in overall throughput. The exact amount of degradation is very workload dependent, ranging from 0% to 30%. However, it is the cost of determinism.

[id="node-tuning-operator-provisioning-worker-with-real-time-capabilities_{context}"]
== Provisioning a worker with real-time capabilities

. Optional: Add a node to the {product-title} cluster.
See link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/8/html/optimizing_rhel_8_for_real_time_for_low_latency_operation/setting-bios-parameters-for-system-tuning_optimizing-rhel8-for-real-time-for-low-latency-operation[Setting BIOS parameters for system tuning].

. Add the label `worker-rt` to the worker nodes that require the real-time capability by using the `oc` command.

. Create a new machine config pool for real-time nodes:
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-rt
  labels:
    machineconfiguration.openshift.io/role: worker-rt
spec:
  machineConfigSelector:
    matchExpressions:
      - {
           key: machineconfiguration.openshift.io/role,
           operator: In,
           values: [worker, worker-rt],
        }
  paused: false
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-rt: ""
----
Note that a machine config pool worker-rt is created for group of nodes that have the label `worker-rt`.

. Add the node to the proper machine config pool by using node role labels.
+
[NOTE]
====
You must decide which nodes are configured with real-time workloads. You could configure all of the nodes in the cluster, or a subset of the nodes. The Node Tuning Operator that expects all of the nodes are part of a dedicated machine config pool. If you use all of the nodes, you must point the Node Tuning Operator to the worker node role label. If you use a subset, you must group the nodes into a new machine config pool.
====
. Create the `PerformanceProfile` with the proper set of housekeeping cores and `realTimeKernel: enabled: true`.

. You must set `machineConfigPoolSelector` in `PerformanceProfile`:
+
[source,yaml]
----
  apiVersion: performance.openshift.io/v2
  kind: PerformanceProfile
  metadata:
   name: example-performanceprofile
  spec:
  ...
    realTimeKernel:
      enabled: true
    nodeSelector:
       node-role.kubernetes.io/worker-rt: ""
    machineConfigPoolSelector:
       machineconfiguration.openshift.io/role: worker-rt
----
. Verify that a matching machine config pool exists with a label:
+
[source,terminal]
----
$ oc describe mcp/worker-rt
----
+
.Example output
[source,yaml]
----
Name:         worker-rt
Namespace:
Labels:       machineconfiguration.openshift.io/role=worker-rt
----

. {product-title} will start configuring the nodes, which might involve multiple reboots. Wait for the nodes to settle. This can take a long time depending on the specific hardware you use, but 20 minutes per node is expected.

. Verify everything is working as expected.

[id="node-tuning-operator-verifying-real-time-kernel-installation_{context}"]
== Verifying the real-time kernel installation

Use this command to verify that the real-time kernel is installed:

[source,terminal]
----
$ oc get node -o wide
----

Note the worker with the role `worker-rt` that contains the string `4.18.0-305.30.1.rt7.102.el8_4.x86_64   cri-o://1.29.4-99.rhaos4.10.gitc3131de.el8`:

[source,terminal]
----
NAME                               	STATUS   ROLES           	AGE 	VERSION                  	INTERNAL-IP
EXTERNAL-IP   OS-IMAGE                                       	KERNEL-VERSION
CONTAINER-RUNTIME
rt-worker-0.example.com	          Ready	 worker,worker-rt   5d17h   v1.29.4
128.66.135.107   <none>    	        Red Hat Enterprise Linux CoreOS 46.82.202008252340-0 (Ootpa)
4.18.0-305.30.1.rt7.102.el8_4.x86_64   cri-o://1.29.4-99.rhaos4.10.gitc3131de.el8
[...]
----

[id="node-tuning-operator-creating-workload-that-works-in-real-time_{context}"]
== Creating a workload that works in real-time

Use the following procedures for preparing a workload that will use real-time capabilities.

.Procedure

. Create a pod with a QoS class of `Guaranteed`.
. Optional: Disable CPU load balancing for DPDK.
. Assign a proper node selector.

When writing your applications, follow the general recommendations described in
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/8/html-single/tuning_guide/index#chap-Application_Tuning_and_Deployment[Application tuning and deployment].

[id="node-tuning-operator-creating-pod-with-guaranteed-qos-class_{context}"]
== Creating a pod with a QoS class of `Guaranteed`

Keep the following in mind when you create a pod that is given a QoS class of `Guaranteed`:

* Every container in the pod must have a memory limit and a memory request, and they must be the same.
* Every container in the pod must have a CPU limit and a CPU request, and they must be the same.

The following example shows the configuration file for a pod that has one container. The container has a memory limit and a memory request, both equal to 200 MiB. The container has a CPU limit and a CPU request, both equal to 1 CPU.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo
  namespace: qos-example
spec:
  securityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: qos-demo-ctr
    image: <image-pull-spec>
    resources:
      limits:
        memory: "200Mi"
        cpu: "1"
      requests:
        memory: "200Mi"
        cpu: "1"
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop: [ALL]
----

. Create the pod:
+
[source,terminal]
----
$ oc  apply -f qos-pod.yaml --namespace=qos-example
----

. View detailed information about the pod:
+
[source,terminal]
----
$ oc get pod qos-demo --namespace=qos-example --output=yaml
----
+
.Example output
[source,yaml]
----
spec:
  containers:
    ...
status:
  qosClass: Guaranteed
----
+
[NOTE]
====
If a container specifies its own memory limit, but does not specify a memory request, {product-title} automatically assigns a memory request that matches the limit. Similarly, if a container specifies its own CPU limit, but does not specify a CPU request, {product-title} automatically assigns a CPU request that matches the limit.
====

[id="node-tuning-operator-disabling-cpu-load-balancing-for-dpdk_{context}"]
== Optional: Disabling CPU load balancing for DPDK

Functionality to disable or enable CPU load balancing is implemented on the CRI-O level. The code under the CRI-O disables or enables CPU load balancing only when the following requirements are met.

* The pod must use the `performance-<profile-name>` runtime class. You can get the proper name by looking at the status of the performance profile, as shown here:
+
[source,yaml]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
...
status:
  ...
  runtimeClass: performance-manual
----

[NOTE]
====
Currently, disabling CPU load balancing is not supported with cgroup v2.
====

The Node Tuning Operator is responsible for the creation of the high-performance runtime handler config snippet under relevant nodes and for creation of the high-performance runtime class under the cluster. It will have the same content as default runtime handler except it enables the CPU load balancing configuration functionality.

To disable the CPU load balancing for the pod, the `Pod` specification must include the following fields:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  ...
  annotations:
    ...
    cpu-load-balancing.crio.io: "disable"
    ...
  ...
spec:
  ...
  runtimeClassName: performance-<profile_name>
  ...
----

[NOTE]
====
Only disable CPU load balancing when the CPU manager static policy is enabled and for pods with guaranteed QoS that use whole CPUs. Otherwise, disabling CPU load balancing can affect the performance of other containers in the cluster.
====

[id="node-tuning-operator-assigning-proper-node-selector_{context}"]
== Assigning a proper node selector

The preferred way to assign a pod to nodes is to use the same node selector the performance profile used, as shown here:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: example
spec:
  # ...
  nodeSelector:
    node-role.kubernetes.io/worker-rt: ""
----

For more information, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.5/html-single/nodes/index#nodes-scheduler-node-selectors[Placing pods on specific nodes using node selectors].

[id="node-tuning-operator-scheduling-workload-onto-worker-with-real-time-capabilities_{context}"]
== Scheduling a workload onto a worker with real-time capabilities

Use label selectors that match the nodes attached to the machine config pool that was configured for low latency by the Node Tuning Operator. For more information, see link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/[Assigning pods to nodes].

[id="node-tuning-operator-disabling-CPUs-for-power-consumption_{context}"]
== Reducing power consumption by taking CPUs offline

You can generally anticipate telecommunication workloads. When not all of the CPU resources are required, the Node Tuning Operator allows you take unused CPUs offline to reduce power consumption by manually updating the performance profile.

To take unused CPUs offline, you must perform the following tasks:

. Set the offline CPUs in the performance profile and save the contents of the YAML file:
+
.Example performance profile with offlined CPUs
[source,yaml]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: performance
spec:
  additionalKernelArgs:
  - nmi_watchdog=0
  - audit=0
  - mce=off
  - processor.max_cstate=1
  - intel_idle.max_cstate=0
  - idle=poll
  cpu:
    isolated: "2-23,26-47"
    reserved: "0,1,24,25"
    offlined: "48-59" <1>
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  numa:
    topologyPolicy: single-numa-node
  realTimeKernel:
    enabled: true
----
<1> Optional. You can list CPUs in the `offlined` field to take the specified CPUs offline.

. Apply the updated profile by running the following command:
+
[source,terminal]
----
$ oc apply -f my-performance-profile.yaml
----

[id="node-tuning-operator-pod-power-saving-config_{context}"]
== Optional: Power saving configurations

You can enable power savings for a node that has low priority workloads that are colocated with high priority workloads without impacting the latency or throughput of the high priority workloads. Power saving is possible without modifications to the workloads themselves.

[IMPORTANT]
====
The feature is supported on Intel Ice Lake and later generations of Intel CPUs. The capabilities of the processor might impact the latency and throughput of the high priority workloads.
====

When you configure a node with a power saving configuration, you must configure high priority workloads with performance configuration at the pod level, which means that the configuration applies to all the cores used by the pod.

By disabling P-states and C-states at the pod level, you can configure high priority workloads for best performance and lowest latency.

.Configuration for high priority workloads
[cols="1,2,3", options="header"]

|===
| Annotation | Possible Values | Description

|`cpu-c-states.crio.io:` a|  * `"enable"`
* `"disable"`
* `"max_latency:microseconds"` | This annotation allows you to enable or disable C-states for each CPU. Alternatively, you can also specify a maximum latency in microseconds for the C-states. For example, enable C-states with a maximum latency of 10 microseconds with the setting `cpu-c-states.crio.io`: `"max_latency:10"`. Set the value to `"disable"` to provide the best performance for a pod.

| `cpu-freq-governor.crio.io:` | Any supported `cpufreq governor`. | Sets the `cpufreq` governor for each CPU. The `"performance"` governor is recommended for high priority workloads.

|===

.Prerequisites

* You enabled C-states and OS-controlled P-states in the BIOS

.Procedure

. Generate a `PerformanceProfile` with `per-pod-power-management` set to `true`:
+
[source,terminal,subs="attributes+"]
----
$ podman run --entrypoint performance-profile-creator -v \
/must-gather:/must-gather:z registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v{product-version} \
--mcp-name=worker-cnf --reserved-cpu-count=20 --rt-kernel=true \
--split-reserved-cpus-across-numa=false --topology-manager-policy=single-numa-node \
--must-gather-dir-path /must-gather -power-consumption-mode=low-latency \ <1>
--per-pod-power-management=true > my-performance-profile.yaml
----
<1> The `power-consumption-mode` must be `default` or `low-latency` when the `per-pod-power-management` is set to `true`.

+
.Example `PerformanceProfile` with `perPodPowerManagement`

[source,yaml]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
     name: performance
spec:
    [.....]
    workloadHints:
        realTime: true
        highPowerConsumption: false
        perPodPowerManagement: true
----

. Set the default `cpufreq` governor as an additional kernel argument in the `PerformanceProfile` custom resource (CR):
+
[source,yaml]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
     name: performance
spec:
    ...
    additionalKernelArgs:
    - cpufreq.default_governor=schedutil <1>
----
<1> Using the `schedutil` governor is recommended, however, you can use other governors such as the `ondemand` or `powersave` governors.

. Set the maximum CPU frequency in the `TunedPerformancePatch` CR:
+
[source,yaml]
----
spec:
  profile:
  - data: |
      [sysfs]
      /sys/devices/system/cpu/intel_pstate/max_perf_pct = <x> <1>
----
<1> The `max_perf_pct` controls the maximum frequency the `cpufreq` driver is allowed to set as a percentage of the maximum supported cpu frequency. This value applies to all CPUs. You can check the maximum supported frequency in `/sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq`. As a starting point, you can use a percentage that caps all CPUs at the `All Cores Turbo` frequency. The `All Cores Turbo` frequency is the frequency that all cores will run at when the cores are all fully occupied.

. Add the desired annotations to your high priority workload pods. The annotations override the `default` settings.
+
.Example high priority workload annotation
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  ...
  annotations:
    ...
    cpu-c-states.crio.io: "disable"
    cpu-freq-governor.crio.io: "performance"
    ...
  ...
spec:
  ...
  runtimeClassName: performance-<profile_name>
  ...
----

. Restart the pods.
