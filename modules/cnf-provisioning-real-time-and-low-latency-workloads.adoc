// CNF-489 Real time and low latency workload provisioning
// Module included in the following assemblies:
//
// *cnf-performance-addon-operator-for-low-latency-nodes.adoc

[id="cnf-provisioning-real-time-and-low-latency-workloads_{context}"]
= Provisioning real-time and low latency workloads

Many industries and organizations need extremely high performance computing and might require low and predictable latency, especially in the financial and telecommunications industries. For these industries, with their unique requirements, {product-title} provides a Performance Addon Operator to implement automatic tuning to achieve low latency performance and consistent response time for {product-title} applications.

The cluster administrator uses this performance profile configuration that makes it easier to make these changes in a more reliable way. The administrator can specify whether to update the kernel to kernel-rt (real-time), the CPUs that will be reserved for housekeeping, and the CPUs that are used for running the workloads.

[id="performance-addon-operator-known-limitations-for-real-time_{context}"]
== Known limitations for real-time

[NOTE]
====
The RT kernel is only supported on worker nodes.
====

To fully utilize the real-time mode, the containers must run with elevated privileges.
See link:https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container[Set capabilities for a Container] for information on granting privileges.

{product-title} restricts the allowed capabilities, so you might need to create a `SecurityContext` as well, as explained in
link:https://docs.openshift.com/container-platform/4.1/authentication/managing-security-context-constraints.html#security-context-constraints-creating_configuring-internal-oauth[Creating Security Context Constraints].

[NOTE]
====
This procedure is fully supported with bare metal installations using {op-system-first} systems.
====

Establishing the right performance expectations refers to the fact that the real-time kernel is not a panacea. Its objective is consistent, low-latency determinism offering predictable response times. There is some additional kernel overhead associated with the real-time kernel. This is due primarily to handling hardware interruptions in separately scheduled threads. The increased overhead in some workloads results in some degradation in overall throughput. The exact amount of degradation is very workload dependent, ranging from 0% to 30%. However, it is the cost of determinism.

[id="performance-addon-operator-provisioning-worker-with-real-time-capabilities_{context}"]
== Provisioning a worker with real-time capabilities

. Install Performance Addon Operator to the cluster.
. (Optional) Add a node to the {product-title} cluster.
See link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/8/html-single/tuning_guide/index#Setting_BIOS_parameters[Setting BIOS parameters].
. (Optional) Create a new machine config pool for real-time nodes.
. Add the node to the proper machine config pool, using node role labels.
+
You must decide which nodes will be configured with real-time workloads. It could be all of the nodes in the cluster or a subset of the nodes. The Performance Addon Operator expects all of the nodes are part of a dedicated machine config pool. If you use all of the nodes, you just point the Performance Addon Operator to the worker node role label. If you use a subset, you must group the nodes into a new machine config pool.

. Create the `PerformanceProfile` with the proper set of housekeeping cores and `realTimeKernel: enabled: true`.

. Specify a node selector in the `PerformanceProfile`, as shown here:
+
[source,yaml]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
 name: example-performanceprofile
spec:
...
  realTimeKernel:
    enabled: true
  nodeSelector:
   node-role.kubernetes.io/worker-rt: ""
----

. Verify that a matching machine config pool exists with a label:
+
[source,bash]
----
machineconfiguration.openshift.io/role=worker-rt
----

. {product-title} will start configuring the nodes, which might involve multiple reboots. Wait for the nodes to settle. This can take a long time depending on the specific hardware you use, but 20 minutes per node is expected.

. Verify everything is working as expected.

[id="performance-addon-operator-verifying-real-time-kernel-installation_{context}"]
== Verifying the real-time kernel installation

Use this command to verify that the real-time kernel is installed:

[source,terminal]
----
$ oc get node -o wide
----

Note the worker with the role `worker-rt` that contains the string `4.18.0-211.rt5.23.el8.x86_64`:

[source,terminal]
----
NAME                               	STATUS   ROLES           	AGE 	VERSION                  	INTERNAL-IP
EXTERNAL-IP   OS-IMAGE                                       	KERNEL-VERSION
CONTAINER-RUNTIME
cnf-worker-0.example.com	          Ready	 worker,worker-rt   5d17h   v1.20.0
128.66.135.107   <none>    	        Red Hat Enterprise Linux CoreOS 46.82.202008252340-0 (Ootpa)
4.18.0-211.rt5.23.el8.x86_64   cri-o://1.20.0-90.rhaos4.6.git4a0ac05.el8-rc.1
[...]
----

[id="performance-addon-operator-creating-workload-that-works-in-real-time_{context}"]
== Creating a workload that works in real-time

Use the following procedures for preparing a workload that will use real-time capabilities.

.Procedure

. Create a pod with a QoS class of `Guaranteed`.
. (Optional) Disable CPU load balancing for DPDK.
. Assign a proper node selector.

When writing your applications, follow the general recommendations described in
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/8/html-single/tuning_guide/index#chap-Application_Tuning_and_Deployment[Application tuning and deployment].

[id="performance-addon-operator-creating-pod-with-guaranteed-qos-class_{context}"]
== Creating a pod with a QoS class of `Guaranteed`

Keep the following in mind when you create a pod that is given a QoS class of `Guaranteed`:

* Every container in the pod must have a memory limit and a memory request, and they must be the same.
* Every container in the pod must have a CPU limit and a CPU request, and they must be the same.

The following example shows the configuration file for a pod that has one container. The container has a memory limit and a memory request, both equal to 200 MiB. The container has a CPU limit and a CPU request, both equal to 1 CPU.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-ctr
    image: <image-pull-spec>
    resources:
      limits:
        memory: "200Mi"
        cpu: "1"
      requests:
        memory: "200Mi"
        cpu: "1"
----

. Create the pod:
+
[source,terminal]
----
$ oc  apply -f qos-pod.yaml --namespace=qos-example
----

. View detailed information about the pod:
+
[source,terminal]
----
$ oc get pod qos-demo --namespace=qos-example --output=yaml
----
+
.Example output
[source,yaml]
----
spec:
  containers:
    ...
status:
  qosClass: Guaranteed
----
+
[NOTE]
====
If a container specifies its own memory limit, but does not specify a memory request, {product-title} automatically assigns a memory request that matches the limit. Similarly, if a container specifies its own CPU limit, but does not specify a CPU request, {product-title} automatically assigns a CPU request that matches the limit.
====

[id="performance-addon-operator-disabling-cpu-load-balancing-for-dpdk_{context}"]
== (Optional) Disabling CPU load balancing for DPDK

Functionality to disable or enable CPU load balancing is implemented on the CRI-O level. The code under the CRI-O disables or enables CPU load balancing only when the following requirements are met.

* The pod must use the `performance-<profile-name>` runtime class. You can get the proper name by looking at the status of the performance profile, as shown here:
+
[source,yaml]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
...
status:
  ...
  runtimeClass: performance-manual
----

* The pod must have the `cpu-load-balancing.crio.io: true` annotation.

The Performance Addon Operator is responsible for the creation of the high-performance runtime handler config snippet under relevant nodes and for creation of the high-performance runtime class under the cluster. It will have the same content as default runtime handler except it enables the CPU load balancing configuration functionality.

To disable the CPU load balancing for the pod, the `Pod` specification must include the following fields:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  ...
  annotations:
    ...
    cpu-load-balancing.crio.io: "true"
    ...
  ...
spec:
  ...
  runtimeClassName: performance-<profile_name>
  ...
----

[NOTE]
====
Only disable CPU load balancing when the CPU manager static policy is enabled and for pods with guaranteed QoS that use whole CPUs. Otherwise, disabling CPU load balancing can affect the performance of other containers in the cluster.
====

[id="performance-addon-operator-assigning-proper-node-selector_{context}"]
== Assigning a proper node selector

The preferred way to assign a pod to nodes is to use the same node selector the performance profile used, as shown here:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: example
spec:
  [...]
  nodeSelector:
     node-role.kubernetes.io/worker-rt: ""
----

For more information, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.5/html-single/nodes/index#nodes-scheduler-node-selectors[Placing pods on specific nodes using node selectors].

[id="performance-addon-operator-scheduling-workload-onto-worker-with-real-time-capabilities_{context}"]
== Scheduling a workload onto a worker with real-time capabilities

Use label selectors that match the nodes attached to the machine config pool that was configured for low latency by the Performance Addon Operator. For more information, see link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/[Assigning pods to nodes].
