// Module file include in the following assemblies:
// logging/cluster-logging-upgrading.adoc

:_mod-docs-content-type: PROCEDURE
[id="cluster-logging-upgrading-elasticsearch_{context}"]
= Updating the OpenShift Elasticsearch Operator

To update the OpenShift Elasticsearch Operator to the current version, you must modify the subscription.

include::snippets/logging-elastic-dep-snip.adoc[]

.Prerequisites

* If you are using Elasticsearch as the default log store, and Kibana as the UI, update the OpenShift Elasticsearch Operator before you update the Cluster Logging Operator.
+
[IMPORTANT]
====
If you update the Operators in the wrong order, Kibana does not update and the Kibana custom resource (CR) is not created. To fix this issue, delete the Red Hat OpenShift Logging Operator pod. When the Red Hat OpenShift Logging Operator pod redeploys, it creates the Kibana CR and Kibana becomes available again.
====

* The Logging status is healthy:
** All pods have a `ready` status.
** The Elasticsearch cluster is healthy.

* Your link:https://www.elastic.co/guide/en/elasticsearch/reference/current/snapshot-restore.html[Elasticsearch and Kibana data is backed up].
* You have administrator permissions.
* You have installed the {oc-first} for the verification steps.

.Procedure

ifndef::openshift-rosa,openshift-dedicated[]
. In the {product-title} web console, click *Operators* -> *Installed Operators*.
endif::[]
ifdef::openshift-rosa,openshift-dedicated[]
. In the {hybrid-console}, click *Operators* -> *Installed Operators*.
endif::[]

. Select the *openshift-operators-redhat* project.

. Click *OpenShift Elasticsearch Operator*.

. Click *Subscription* -> *Channel*.

. In the *Change Subscription Update Channel* window, select *stable-5.y* and click *Save*. Note the `elasticsearch-operator.v5.y.z` version.

. Wait for a few seconds, then click *Operators* -> *Installed Operators*. Verify that the OpenShift Elasticsearch Operator version matches the latest `elasticsearch-operator.v5.y.z` version.

. On the *Operators* -> *Installed Operators* page, wait for the *Status* field to report *Succeeded*.

.Verification

. Verify that all Elasticsearch pods have a *Ready* status by entering the following command and observing the output:
+
[source,terminal]
----
$ oc get pod -n openshift-logging --selector component=elasticsearch
----
+
.Example output
[source,terminal]
----
NAME                                            READY   STATUS    RESTARTS   AGE
elasticsearch-cdm-1pbrl44l-1-55b7546f4c-mshhk   2/2     Running   0          31m
elasticsearch-cdm-1pbrl44l-2-5c6d87589f-gx5hk   2/2     Running   0          30m
elasticsearch-cdm-1pbrl44l-3-88df5d47-m45jc     2/2     Running   0          29m
----

. Verify that the Elasticsearch cluster status is `green` by entering the following command and observing the output:
+
[source,terminal]
----
$ oc exec -n openshift-logging -c elasticsearch elasticsearch-cdm-1pbrl44l-1-55b7546f4c-mshhk -- health
----
+
.Example output
[source,json]
----
{
  "cluster_name" : "elasticsearch",
  "status" : "green",
}
----

. Verify that the Elasticsearch cron jobs are created by entering the following commands and observing the output:
+
[source,terminal]
----
$ oc project openshift-logging
----
+
[source,terminal]
----
$ oc get cronjob
----
+
.Example output
[source,terminal]
----
NAME                     SCHEDULE       SUSPEND   ACTIVE   LAST SCHEDULE   AGE
elasticsearch-im-app     */15 * * * *   False     0        <none>          56s
elasticsearch-im-audit   */15 * * * *   False     0        <none>          56s
elasticsearch-im-infra   */15 * * * *   False     0        <none>          56s
----

. Verify that the log store is updated to the correct version and the indices are `green` by entering the following command and observing the output:
+
[source,terminal]
----
$ oc exec -c elasticsearch <any_es_pod_in_the_cluster> -- indices
----
+
Verify that the output includes the `app-00000x`, `infra-00000x`, `audit-00000x`, `.security` indices:
+
.Sample output with indices in a green status
[%collapsible]
====
[source,terminal]
----
Tue Jun 30 14:30:54 UTC 2020
health status index                                                                 uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   infra-000008                                                          bnBvUFEXTWi92z3zWAzieQ   3 1       222195            0        289            144
green  open   infra-000004                                                          rtDSzoqsSl6saisSK7Au1Q   3 1       226717            0        297            148
green  open   infra-000012                                                          RSf_kUwDSR2xEuKRZMPqZQ   3 1       227623            0        295            147
green  open   .kibana_7                                                             1SJdCqlZTPWlIAaOUd78yg   1 1            4            0          0              0
green  open   infra-000010                                                          iXwL3bnqTuGEABbUDa6OVw   3 1       248368            0        317            158
green  open   infra-000009                                                          YN9EsULWSNaxWeeNvOs0RA   3 1       258799            0        337            168
green  open   infra-000014                                                          YP0U6R7FQ_GVQVQZ6Yh9Ig   3 1       223788            0        292            146
green  open   infra-000015                                                          JRBbAbEmSMqK5X40df9HbQ   3 1       224371            0        291            145
green  open   .orphaned.2020.06.30                                                  n_xQC2dWQzConkvQqei3YA   3 1            9            0          0              0
green  open   infra-000007                                                          llkkAVSzSOmosWTSAJM_hg   3 1       228584            0        296            148
green  open   infra-000005                                                          d9BoGQdiQASsS3BBFm2iRA   3 1       227987            0        297            148
green  open   infra-000003                                                          1-goREK1QUKlQPAIVkWVaQ   3 1       226719            0        295            147
green  open   .security                                                             zeT65uOuRTKZMjg_bbUc1g   1 1            5            0          0              0
green  open   .kibana-377444158_kubeadmin                                           wvMhDwJkR-mRZQO84K0gUQ   3 1            1            0          0              0
green  open   infra-000006                                                          5H-KBSXGQKiO7hdapDE23g   3 1       226676            0        295            147
green  open   infra-000001                                                          eH53BQ-bSxSWR5xYZB6lVg   3 1       341800            0        443            220
green  open   .kibana-6                                                             RVp7TemSSemGJcsSUmuf3A   1 1            4            0          0              0
green  open   infra-000011                                                          J7XWBauWSTe0jnzX02fU6A   3 1       226100            0        293            146
green  open   app-000001                                                            axSAFfONQDmKwatkjPXdtw   3 1       103186            0        126             57
green  open   infra-000016                                                          m9c1iRLtStWSF1GopaRyCg   3 1        13685            0         19              9
green  open   infra-000002                                                          Hz6WvINtTvKcQzw-ewmbYg   3 1       228994            0        296            148
green  open   infra-000013                                                          KR9mMFUpQl-jraYtanyIGw   3 1       228166            0        298            148
green  open   audit-000001                                                          eERqLdLmQOiQDFES1LBATQ   3 1            0            0          0              0
----
====

. Verify that the log visualizer is updated to the correct version by entering the following command and observing the output:
+
[source,terminal]
----
$ oc get kibana kibana -o json
----
+
Verify that the output includes a Kibana pod with the `ready` status:
+
.Sample output with a ready Kibana pod
[%collapsible]
====
[source,json]
----
[
{
"clusterCondition": {
"kibana-5fdd766ffd-nb2jj": [
{
"lastTransitionTime": "2020-06-30T14:11:07Z",
"reason": "ContainerCreating",
"status": "True",
"type": ""
},
{
"lastTransitionTime": "2020-06-30T14:11:07Z",
"reason": "ContainerCreating",
"status": "True",
"type": ""
}
]
},
"deployment": "kibana",
"pods": {
"failed": [],
"notReady": []
"ready": []
},
"replicaSets": [
"kibana-5fdd766ffd"
],
"replicas": 1
}
]
----
====
