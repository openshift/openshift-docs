// Module included in the following assemblies:
//
// * edge_computing/ztp-advanced-install-ztp.adoc

:_mod-docs-content-type: PROCEDURE
[id="ztp-deleting-node-clusterinstance_{context}"]
= Deleting a node by using the ClusterInstance CR

By using a `ClusterInstance` custom resource (CR), you can delete and reprovision a node.
This method is more efficient than manually deleting the node.

.Prerequisites

* You have configured the hub cluster to generate the required installation and policy CRs.

* You have created a Git repository in which you can manage your custom site configuration data. The repository must be accessible from the hub cluster and be defined as the source repository for the Argo CD application.


.Procedure

. Update the `ClusterInstance` CR to add the `bmac.agent-install.openshift.io/remove-agent-and-node-on-delete=true` annotation to the `BareMetalHost` resource for the node, and push the changes to the Git repository:
+
[source,yaml]
----
apiVersion: siteconfig.open-cluster-management.io/v1alpha1
kind: ClusterInstance
metadata:
  name: "example-cluster"
  namespace: "example-cluster"
spec:
  # ...
  nodes:
    - hostName: "worker-node2.example.com"
      role: "worker"
      extraAnnotations:
        BareMetalHost:
          bmac.agent-install.openshift.io/remove-agent-and-node-on-delete: "true"
# ...
----

. Verify that the `BareMetalHost` object is annotated by running the following command:
+
[source,terminal]
----
$ oc get bmh -n <cluster_namespace> <bmh_name> -ojsonpath='{.metadata}' | jq -r '.annotations["bmac.agent-install.openshift.io/remove-agent-and-node-on-delete"]'
----
+
.Example output
[source,terminal]
----
true
----

. Delete the `BareMetalHost` CR by configuring the `pruneManifests` field in the `ClusterInstance` CR to remove the target `BareMetalHost` resource:
+
[source,yaml]
----
apiVersion: siteconfig.open-cluster-management.io/v1alpha1
kind: ClusterInstance
metadata:
  name: "example-cluster"
  namespace: "example-cluster"
spec:
  # ...
  nodes:
    - hostName: "worker-node2.example.com"
      role: "worker"
      pruneManifests:
        - apiVersion: metal3.io/v1alpha1
          kind: BareMetalHost
# ...
----

. Push the changes to the Git repository and wait for deprovisioning to start.
The status of the `BareMetalHost` CR should change to `deprovisioning`. Wait for the `BareMetalHost` to finish deprovisioning, and be fully deleted.

.Verification

. Verify that the `BareMetalHost` and `Agent` CRs for the worker node have been deleted from the hub cluster by running the following commands:
+
[source,terminal]
----
$ oc get bmh -n <cluster_namespace>
----
+
[source,terminal]
----
$ oc get agent -n <cluster_namespace>
----

. Verify that the node record has been deleted from the spoke cluster by running the following command:
+
[source,terminal]
----
$ oc get nodes
----
+
[NOTE]
====
If you are working with secrets, deleting a secret too early can cause an issue because ArgoCD needs the secret to complete resynchronization after deletion.
Delete the secret only after the node cleanup, when the current ArgoCD synchronization is complete.
====

. After the `BareMetalHost` object is successfully deleted, remove the worker node definition from the `spec.nodes` section in the `ClusterInstance` CR and push the changes to the Git repository.

.Next steps

To reprovision a node, add the node definition back to the `spec.nodes` section in the `ClusterInstance` CR, push the changes to the Git repository, and wait for the synchronization to complete.
This regenerates the `BareMetalHost` CR of the worker node and triggers the re-install of the node.
