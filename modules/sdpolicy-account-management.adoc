
// Module included in the following assemblies:
//
// * assemblies/osd-service-definition.adoc

[id="sdpolicy-account-management_{context}"]
= Account management

[id="billing_{context}"]
== Billing
Each {product-title} cluster requires a minimum annual base cluster purchase and there are two billing options available for each cluster: Standard and Customer Cloud Subscription (CCS).

Standard {product-title} clusters are deployed in to their own cloud infrastructure accounts, each owned by Red Hat. Red Hat is responsible for this account, and cloud infrastructure costs are paid directly by Red Hat. The customer only pays the Red Hat subscription costs.

In the CCS model, the customer pays the cloud infrastructure provider directly for cloud costs and the cloud infrastructure account is part of a customer’s Organization, with specific access granted to Red Hat. In this model, the customer pays Red Hat for the CCS subscription and pays the cloud provider for the cloud costs. It is the customer's responsibility to pre-purchase or provide Reserved Instance (RI) compute instances to ensure lower cloud infrastructure costs.

Additional resources can be purchased for an OpenShift Dedicated Cluster, including:

* Additional nodes (can be different types and sizes through the use of machine pools)
* Middleware (JBoss EAP, JBoss Fuse, and so on) - additional pricing based on specific middleware component
* Additional storage in increments of 500 GB (standard only; 100 GB included)
* Additional 12 TiB Network I/O (standard only; 12 TB included)
* Load Balancers for Services are available in bundles of 4; enables non-HTTP/SNI traffic or non-standard ports (standard only)

[id="cluster-self-service_{context}"]
== Cluster self-service

Customers can create, scale, and delete their clusters from {cluster-manage-url}, provided that they have pre-purchased the necessary subscriptions.

Actions available in {cluster-manager-first} must not be directly performed from within the cluster as this might cause adverse affects, including having all actions automatically reverted.

[id="cloud-providers_{context}"]
== Cloud providers

{product-title} offers OpenShift Container Platform clusters as a managed service on the following cloud providers:

* Amazon Web Services (AWS)
* Google Cloud Platform (GCP)

[id="compute_{context}"]
== Compute

Single availability zone clusters require a minimum of 2 worker nodes for Customer Cloud Subscription (CCS) clusters deployed to a single availability zone. A minimum of 4 worker nodes is required for standard clusters. These 4 worker nodes are included in the base subscription.

Multiple availability zone clusters require a minimum of 3 worker nodes for Customer Cloud Subscription (CCS) clusters, 1 deployed to each of 3 availability zones. A minimum of 9 worker nodes are required for standard clusters. These 9 worker nodes are included in the base subscription, and additional nodes must be purchased in multiples of 3 to maintain proper node distribution.

Worker nodes must all be the same type and size within a single {product-title} cluster.

[NOTE]
====
The default machine pool node type and size cannot be changed after the cluster has been created.
====

Control and infrastructure nodes are also provided by Red Hat. There are at least 3 control planenodes that handle etcd and API-related workloads. There are at least 2 infrastructure nodes that handle metrics, routing, the web console, and other workloads. Control and infrastructure nodes are strictly for Red Hat workloads to operate the service, and customer workloads are not permitted to be deployed on these nodes.

[NOTE]
====
Approximately 1 vCPU core and 1 GiB of memory are reserved on each worker node and removed from allocatable resources. This is necessary to run link:https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved[processes required by the underlying platform]. This includes system daemons such as udev, kubelet, container runtime, and so on, and also accounts for kernel reservations. {OCP} core systems such as audit log aggregation, metrics collection, DNS, image registry, SDN, and so on might consume additional allocatable resources to maintain the stability and maintainability of the cluster. The additional resources consumed might vary based on usage.
====

[id="aws-compute-types-ccs_{context}"]
== AWS compute types for Customer Cloud Subscription clusters

{product-title} offers the following worker node types and sizes on AWS:

.General purpose
[%collapsible]
====
- m5.xlarge (4 vCPU, 16 GiB)
- m5.2xlarge (8 vCPU, 32 GiB)
- m5.4xlarge (16 vCPU, 64 GiB)
- m5.8xlarge (32 vCPU, 128 GiB)
- m5.12xlarge (48 vCPU, 192 GiB)
- m5.16xlarge (64 vCPU, 256 GiB)
- m5.24xlarge (96 vCPU, 384 GiB)
====

.Memory-optimized
[%collapsible]
====
- r5.xlarge (4 vCPU, 32 GiB)
- r5.2xlarge (8 vCPU, 64 GiB)
- r5.4xlarge (16 vCPU, 128 GiB)
- r5.8xlarge (32 vCPU, 256 GiB)
- r5.12xlarge (48 vCPU, 384 GiB)
- r5.16xlarge (64 vCPU, 512 GiB)
- r5.24xlarge (96 vCPU, 768 GiB)
- r6i.xlarge (4 vCPU, 32 GiB)
- r6i.2xlarge (8 vCPU, 64 GiB)
- r6i.4xlarge (16 vCPU, 128 GiB)
- r6i.8xlarge (32 vCPU, 256 GiB)
- r6i.12xlarge (48 vCPU, 384 GiB)
- r6i.16xlarge (64 vCPU, 512 GiB)
- r6i.24xlarge (96 vCPU, 768 GiB)
- r6i.32xlarge (128 vCPU, 1,024 GiB)
- z1d.xlarge (4 vCPU, 32 GiB)
- z1d.2xlarge (8 vCPU, 64 GiB)
- z1d.3xlarge (12 vCPU, 96 GiB)
- z1d.6xlarge (24 vCPU, 192 GiB)
- z1d.12xlarge (48 vCPU, 384 GiB)
====

.Compute-optimized
[%collapsible]
====
- c5.xlarge (4 vCPU, 8 GiB)
- c5.2xlarge (8 vCPU, 16 GiB)
- c5.4xlarge (16 vCPU, 32 GiB)
- c5.9xlarge (36 vCPU, 72 GiB)
- c5.12xlarge (48 vCPU, 96 GiB)
- c5.18xlarge (72 vCPU, 144 GiB)
- c5.24xlarge (96 vCPU, 192 GiB)
====

.Storage-optimized compute types
[%collapsible]
====
- i3.xlarge	(4 vCPU, 30.5 GiB)
- i3.2xlarge (8 vCPU, 61 GiB)
- i3.4xlarge (16 vCPU, 122 GiB)
- i3.8xlarge (32 vCPU, 244 GiB)
- i3.16xlarge (64 vCPU, 488 GiB)
- i3en.xlarge (4 vCPU, 32 GiB)
- i3en.2xlarge (8 vCPU, 64 GiB)
- i3en.3xlarge (12 vCPU, 96 GiB)
- i3en.6xlarge (24 vCPU, 192 GiB)
- i3en.12xlarge (48 vCPU, 384 GiB)
- i3en.24xlarge (96 vCPU, 768 GiB)
====

[id="aws-compute-types-non-ccs_{context}"]
== AWS compute types for standard clusters

{product-title} offers the following worker node types and sizes on AWS:

.General purpose
[%collapsible]
====
- m5.xlarge (4 vCPU, 16 GiB)
- m5.2xlarge (8 vCPU, 32 GiB)
- m5.4xlarge (16 vCPU, 64 GiB)
====

.Memory-optimized
[%collapsible]
====
- r5.xlarge (4 vCPU, 32 GiB)
- r5.2xlarge (8 vCPU, 64 GiB)
- r5.4xlarge (16 vCPU, 128 GiB)
====

.Compute-optimized
[%collapsible]
====
- c5.2xlarge (8 vCPU, 16 GiB)
- c5.4xlarge (16 vCPU, 32 GiB)
====

[id="gcp-compute-types_{context}"]
== Google Cloud compute types

{product-title} offers the following worker node types and sizes on Google Cloud that are chosen to have a common CPU and memory capacity that are the same as other cloud instance types:

.General purpose
[%collapsible]
====
* custom-4-16384 (4 vCPU, 16 GiB)
* custom-8-32768 (8 vCPU, 32 GiB)
* custom-16-65536 (16 vCPU, 64 GiB)
====

.Memory-optimized
[%collapsible]
====
* custom-4-32768-ext (4 vCPU, 32 GiB)
* custom-8-65536-ext (8 vCPU, 64 GiB)
* custom-16-131072-ext (16 vCPU, 128 GiB)
====

.Compute-optimized
[%collapsible]
====
* custom-8-16384 (8 vCPU, 16 GiB)
* custom-16-32768 (16 vCPU, 32 GiB)
====

[id="regions-availability-zones_{context}"]
== Regions and availability zones
The following AWS regions are supported by {OCP} 4 and are supported for {product-title}:

* af-south-1 (Cape Town, AWS opt-in required)
* ap-east-1 (Hong Kong, AWS opt-in required)
* ap-northeast-1 (Tokyo)
* ap-northeast-2 (Seoul)
* ap-northeast-3 (Osaka)
* ap-south-1 (Mumbai)
* ap-southeast-1 (Singapore)
* ap-southeast-2 (Sydney)
* ca-central-1 (Central Canada)
* eu-central-1 (Frankfurt)
* eu-north-1 (Stockholm)
* eu-south-1 (Milan, AWS opt-in required)
* eu-west-1 (Ireland)
* eu-west-2 (London)
* eu-west-3 (Paris)
* me-south-1 (Bahrain, AWS opt-in required)
* sa-east-1 (São Paulo)
* us-east-1 (N. Virginia)
* us-east-2 (Ohio)
* us-west-1 (N. California)
* us-west-2 (Oregon)

The following Google Cloud regions are currently supported:

* asia-east1, Changhua County, Taiwan
* asia-east2, Hong Kong
* asia-northeast1, Tokyo, Japan
* asia-northeast2, Osaka, Japan
* asia-northeast3, Seoul, Korea
* asia-south1, Mumbai, India
* asia-southeast1, Jurong West, Singapore
* asia-southeast2, Jakarta, Indonesia
* europe-north1, Hamina, Finland
* europe-west1, St. Ghislain, Belgium
* europe-west2, London, England, UK
* europe-west3, Frankfurt, Germany
* europe-west4, Eemshaven, Netherlands
* europe-west6, Zürich, Switzerland
* northamerica-northeast1, Montréal, Québec, Canada
* southamerica-east1, Osasco (São Paulo), Brazil
* us-central1, Council Bluffs, Iowa, USA
* us-east1, Moncks Corner, South Carolina, USA
* us-east4, Ashburn, Northern Virginia, USA
* us-west1, The Dalles, Oregon, USA
* us-west2, Los Angeles, California, USA
* us-west3, Salt Lake City, Utah, USA
* us-west4, Las Vegas, Nevada, USA

Multi-AZ clusters can only be deployed in regions with at least 3 availability zones (see link:https://aws.amazon.com/about-aws/global-infrastructure/regions_az/[AWS] and link:https://cloud.google.com/compute/docs/regions-zones[Google Cloud]).

Each new {product-title} cluster is installed within a dedicated Virtual Private Cloud (VPC) in a single Region, with the option to deploy into a single Availability Zone (Single-AZ) or across multiple Availability Zones (Multi-AZ). This provides cluster-level network and resource isolation, and enables cloud-provider VPC settings, such as VPN connections and VPC Peering. Persistent volumes are backed by cloud block storage and are specific to the availability zone in which they are provisioned. Persistent volumes do not bind to a volume until the associated pod resource is assigned into a specific availability zone in order to prevent unschedulable pods. Availability zone-specific resources are only usable by resources in the same availability zone.

[WARNING]
====
The region and the choice of single or multi availability zone cannot be changed once a cluster has been deployed.
====

[id="sla_{context}"]
== Service level agreement (SLA)
Any SLAs for the service itself are defined in Appendix 4 of the link:https://www.redhat.com/en/about/agreements[Red Hat Enterprise Agreement Appendix 4 (Online Subscription Services)].

[id="limited-support_{context}"]
== Limited support status

When a cluster transitions to a _Limited Support_ status, Red Hat no longer proactively monitors the cluster, the SLA is no longer applicable, and credits requested against the SLA are denied. It does not mean that you no longer have product support. In some cases, the cluster can return to a fully-supported status if you remediate the violating factors. However, in other cases, you might have to delete and recreate the cluster.

A cluster might transition to a Limited Support status for many reasons, including the following scenarios:

If you do not upgrade a cluster to a supported version before the end-of-life date:: Red Hat does not make any runtime or SLA guarantees for versions after their end-of-life date. To receive continued support, upgrade the cluster to a supported version prior to the end-of-life date. If you do not upgrade the cluster prior to the end-of-life date, the cluster transitions to a Limited Support status until it is upgraded to a supported version.
+
Red Hat provides commercially reasonable support to upgrade from an unsupported version to a supported version. However, if a supported upgrade path is no longer available, you might have to create a new cluster and migrate your workloads.

If you remove or replace any native {product-title} components or any other component that is installed and managed by Red Hat:: If cluster administrator permissions were used, Red Hat is not responsible for any of your or your authorized users’ actions, including those that affect infrastructure services, service availability, or data loss. If Red Hat detects any such actions, the cluster might transition to a Limited Support status. Red Hat notifies you of the status change and you should either revert the action or create a support case to explore remediation steps that might require you to delete and recreate the cluster.

If you have questions about a specific action that might cause a cluster to transition to a Limited Support status or need further assistance, open a support ticket.

[id="support_{context}"]
== Support
{product-title} includes Red Hat Premium Support, which can be accessed by using the link:https://access.redhat.com/support?extIdCarryOver=true&sc_cid=701f2000001Css5AAC[Red Hat Customer Portal].

See the link:https://access.redhat.com/support/offerings/production/soc[Scope of Coverage Page] for link:https://access.redhat.com/support/offerings/production/scope_moredetail[more details] on what is covered with included support for {product-title}.

See {product-title} link:https://access.redhat.com/support/offerings/openshift/sla?extIdCarryOver=true&sc_cid=701f2000001Css5AAC[SLAs] for support response times.
