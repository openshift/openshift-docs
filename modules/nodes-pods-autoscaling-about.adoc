// Module included in the following assemblies:
//
// * nodes/nodes-pods-autoscaling-about.adoc

:_mod-docs-content-type: CONCEPT
[id="nodes-pods-autoscaling-about_{context}"]
= Understanding horizontal pod autoscalers

You can create a horizontal pod autoscaler to specify the minimum and maximum number of pods
you want to run, as well as the CPU utilization or memory utilization your pods should target.

After you create a horizontal pod autoscaler, {product-title} begins to query the CPU and/or memory resource metrics on the pods.
When these metrics are available, the horizontal pod autoscaler computes
the ratio of the current metric utilization with the desired metric utilization,
and scales up or down accordingly. The query and scaling occurs at a regular interval,
but can take one to two minutes before metrics become available.

For replication controllers, this scaling corresponds directly to the replicas
of the replication controller. For deployment configurations, scaling corresponds
directly to the replica count of the deployment configuration. Note that autoscaling
applies only to the latest deployment in the `Complete` phase.

{product-title} automatically accounts for resources and prevents unnecessary autoscaling
during resource spikes, such as during start up. Pods in the `unready` state
have `0 CPU` usage when scaling up and the autoscaler ignores the pods when scaling down.
Pods without known metrics have `0% CPU` usage when scaling up and `100% CPU` when scaling down.
This allows for more stability during the HPA decision. To use this feature, you must configure
readiness checks to determine if a new pod is ready for use.

ifdef::openshift-origin,openshift-enterprise,openshift-webscale[]
To use horizontal pod autoscalers, your cluster administrator must have
properly configured cluster metrics.
endif::openshift-origin,openshift-enterprise,openshift-webscale[]

== Supported metrics

The following metrics are supported by horizontal pod autoscalers:

.Metrics
[cols="3a,5a,5a",options="header"]
|===

|Metric |Description |API version

|CPU utilization
|Number of CPU cores used. Can be used to calculate a percentage of the pod's requested CPU.
|`autoscaling/v1`, `autoscaling/v2`

|Memory utilization
|Amount of memory used. Can be used to calculate a percentage of the pod's requested memory.
|`autoscaling/v2`
|===

[IMPORTANT]
====
For memory-based autoscaling, memory usage must increase and decrease
proportionally to the replica count. On average:

* An increase in replica count must lead to an overall decrease in memory
(working set) usage per-pod.
* A decrease in replica count must lead to an overall increase in per-pod memory
usage.

Use the {product-title} web console to check the memory behavior of your application
and ensure that your application meets these requirements before using
memory-based autoscaling.
====

The following example shows autoscaling for the `image-registry` `Deployment` object. The initial deployment requires 3 pods. The HPA object increases the minimum to 5. If CPU usage on the pods reaches 75%, the pods increase to 7:

[source,terminal]
----
$ oc autoscale deployment/image-registry --min=5 --max=7 --cpu-percent=75
----

.Example output
[source,terminal]
----
horizontalpodautoscaler.autoscaling/image-registry autoscaled
----

.Sample HPA for the `image-registry` `Deployment` object with `minReplicas` set to 3
[source,yaml]
----
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: image-registry
  namespace: default
spec:
  maxReplicas: 7
  minReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: image-registry
  targetCPUUtilizationPercentage: 75
status:
  currentReplicas: 5
  desiredReplicas: 0
----

. View the new state of the deployment:
+
[source,terminal]
----
$ oc get deployment image-registry
----
+
There are now 5 pods in the deployment:
+
.Example output
[source,terminal]
----
NAME             REVISION   DESIRED   CURRENT   TRIGGERED BY
image-registry   1          5         5         config
----
