// Module included in the following assemblies:
// Epic CNF-290 (4.5)
// scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc

[id="cnf-performing-end-to-end-tests-for-platform-verification_{context}"]
= Performing end-to-end tests for platform verification

The Cloud-native Network Functions link:https://quay.io/repository/openshift-kni/cnf-tests?tag=latest&tab=tags[(CNF)] tests image is a containerized test suite that validates features required to run CNF payloads. You can use this image to validate a CNF-enabled OpenShift cluster where all the components required for running CNF workloads are installed.

The tests run by the image are split into three different phases:

* Simple cluster validation
* Setup
* End to end tests

The validation phase checks that all the features required to be tested are deployed correctly on the cluster.

Validations include:

* Targeting a machine config pool that belong to the machines to be tested
* Enabling SCTP on the nodes
* Enabling xt_u32 kernel module via machine config
* Having the Performance Addon Operator installed
* Having the SR-IOV Operator installed
* Having the PTP Operator installed
* Enabling the `contain-mount-namespace` mode via machine config
* Using OVN-kubernetes as the cluster network provider

Latency tests, a part of the CNF-test container, also require the same validations. For more information about running a latency test, see the Running the latency tests section.

The tests need to perform an environment configuration every time they are executed. This involves items such as creating SR-IOV node policies, performance profiles, or PTP profiles. Allowing the tests to configure an already configured cluster might affect the functionality of the cluster. Also, changes to configuration items such as SR-IOV node policy might result in the environment being temporarily unavailable until the configuration change is processed.

[id="cnf-performing-end-to-end-tests-prerequisites_{context}"]
== Prerequisites

* The test entrypoint is `/usr/bin/test-run.sh`. It runs both a setup test set and the real conformance test suite. The minimum requirement is to provide it with a kubeconfig file and its related `$KUBECONFIG` environment variable, mounted through a volume.

* The tests assumes that a given feature is already available on the cluster in the form of an Operator, flags enabled on the cluster, or machine configs.

* Some tests require a pre-existing machine config pool to append their changes to. This must be created on the cluster before running the tests.
+
The default worker pool is `worker-cnf` and can be created with the following manifest:
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-cnf
  labels:
    machineconfiguration.openshift.io/role: worker-cnf
spec:
  machineConfigSelector:
    matchExpressions:
      - {
          key: machineconfiguration.openshift.io/role,
          operator: In,
          values: [worker-cnf, worker],
        }
  paused: false
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-cnf: ""
----
+
You can use the `ROLE_WORKER_CNF` variable to override the worker pool name:
+
[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -e KUBECONFIG=/kubeconfig/kubeconfig -e
ROLE_WORKER_CNF=custom-worker-pool registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/test-run.sh
----
+
[NOTE]
====
Currently, not all tests run selectively on the nodes belonging to the pool.
====

[id="cnf-performing-end-to-end-tests-running-the-tests_{context}"]
== Running the tests
Assuming the `kubeconfig` file is in the current folder, the command for running the test suite is:

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -e KUBECONFIG=/kubeconfig/kubeconfig registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/test-run.sh
----

This allows your `kubeconfig` file to be consumed from inside the running container.

[id="cnf-performing-end-to-end-tests-running-the-latency_tests"]
=== Running the latency tests
In {product-title} {product-version}, you can also run latency tests from the CNF-test container. The latency test allows you to set a latency limit so that you can determine performance, throughput, and latency.

The latency test runs the `oslat` tool, which is an open source program to detect OS level latency. For more information, see the Red Hat Knowledgebase solution link:https://access.redhat.com/solutions/5315541[How to measure OS and hardware latency on isolated CPUs?].

By default, the latency tests are disabled. To enable the latency test, you must add the `LATENCY_TEST_RUN` variable and set its value to `true`. For example, `LATENCY_TEST_RUN=true`.

Additionally, you can set the following environment variables for latency tests:

* `LATENCY_TEST_RUNTIME` - Specifies the amount of time (in seconds) that the latency test must run.
* `OSLAT_MAXIMUM_LATENCY` - Specifies the maximum latency (in microseconds) that is expected from all buckets during the `oslat` test run.

To perform the latency tests, run the following command:

[source,termina]
----
$ docker run -v $(pwd)/:/kubeconfig -e KUBECONFIG=/kubeconfig/kubeconfig -e LATENCY_TEST_RUN=true -e LATENCY_TEST_RUNTIME=600 -e OSLAT_MAXIMUM_LATENCY=20 registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/test-run.sh
----
[NOTE]
====
You must run the latency test in discovery mode. For more information, see the Discovery mode section.
====

.Excerpt of a sample result of a 10-second latency test using the following command:
[source,terminal]
----
[root@cnf12-installer ~]# podman run --rm -v $KUBECONFIG:/kubeconfig:Z -e PERF_TEST_PROFILE=worker-cnf-2 -e KUBECONFIG=/kubeconfig -e LATENCY_TEST_RUN=true -e LATENCY_TEST_RUNTIME=10 -e OSLAT_MAXIMUM_LATENCY=20 -e DISCOVERY_MODE=true registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/test-run.sh
-ginkgo.focus="Latency"
running /0_config.test -ginkgo.focus=Latency
----
.Example output
[source,terminal]
----
I1106 15:09:08.087085       7 request.go:621] Throttling request took 1.037172581s, request: GET:https://api.cnf12.kni.lab.eng.bos.redhat.com:6443/apis/autoscaling.openshift.io/v1?timeout=32s
Running Suite: Performance Addon Operator configuration

Random Seed: 1604675347
Will run 0 of 1 specs

JUnit report was created: /unit_report_performance_config.xml

Ran 0 of 1 Specs in 0.000 seconds
SUCCESS! -- 0 Passed | 0 Failed | 0 Pending | 1 Skipped
PASS
running /4_latency.test -ginkgo.focus=Latency
I1106 15:09:10.735795      23 request.go:621] Throttling request took 1.037276624s, request: GET:https://api.cnf12.kni.lab.eng.bos.redhat.com:6443/apis/certificates.k8s.io/v1?timeout=32s
Running Suite: Performance Addon Operator latency e2e tests

Random Seed: 1604675349
Will run 1 of 1 specs

I1106 15:10:06.401180      23 nodes.go:86] found mcd machine-config-daemon-r78qc for node cnfdd8.clus2.t5g.lab.eng.bos.redhat.com
I1106 15:10:06.738120      23 utils.go:23] run command 'oc [exec -i -n openshift-machine-config-operator -c machine-config-daemon --request-timeout 30 machine-config-daemon-r78qc -- cat /rootfs/var/log/oslat.log]' (err=<nil>):
  stdout=
Version: v0.1.7

Total runtime: 		10 seconds
Thread priority: 	SCHED_FIFO:1
CPU list: 		3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50
CPU for main thread: 	2
Workload: 		no
Workload mem: 		0 (KiB)
Preheat cores: 		48

Pre-heat for 1 seconds...
Test starts...
Test completed.

Core: 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50
CPU Freq: 2096 2096 2096 2096 2096 2096 2096 2096 2096 2096 2096 2096 2096 2092 2096 2096 2096 2092 2092 2096 2096 2096 2096 2096 2096 2096 2096 2096 2096 2092 2096 2096 2092 2096 2096 2096 2096 2092 2096 2096 2096 2092 2096 2096 2096 2096 2096 2096 (Mhz)
...
Maximum: 3 4 3 3 3 3 3 3 4 3 3 3 3 4 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 4 3 3 3 3 3 4 3 3 3 4 (us)
----

[id="cnf-performing-end-to-end-tests-image-parameters_{context}"]
== Image parameters

Depending on the requirements, the tests can use different images. There are two images used by the tests that can be changed using the following environment variables:

* `CNF_TESTS_IMAGE`
* `DPDK_TESTS_IMAGE`

For example, to change the `CNF_TESTS_IMAGE` with a custom registry run the following command:

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -e KUBECONFIG=/kubeconfig/kubeconfig -e CNF_TESTS_IMAGE="custom-cnf-tests-image:latests" registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/test-run.sh
----

[id="cnf-performing-end-to-end-tests-ginko-parameters_{context}"]
=== Ginkgo parameters

The test suite is built upon the ginkgo BDD framework. This means that it accepts parameters for filtering or skipping tests.

You can use the `-ginkgo.focus` parameter to filter a set of tests:

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -e KUBECONFIG=/kubeconfig/kubeconfig registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/test-run.sh -ginkgo.focus="performance|sctp"
----

You can run only the latency test using the `-ginkgo.focus` parameter.

To run only the latency test, you must provide the `-ginkgo.focus` parameter and the `PERF_TEST_PROFILE` environment variable that contains the name of the performance profile that needs to be tested. For example:

[source,terminal]
----
$ docker run --rm -v $KUBECONFIG:/kubeconfig -e KUBECONFIG=/kubeconfig -e LATENCY_TEST_RUN=true -e LATENCY_TEST_RUNTIME=600 -e OSLAT_MAXIMUM_LATENCY=20 -e PERF_TEST_PROFILE=<performance_profile_name> registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/test-run.sh -ginkgo.focus="\[performance\]\[config\]|\[performance\]\ Latency\ Test"
----

[NOTE]
====
There is a particular test that requires both SR-IOV and SCTP. Given the selective nature of the `focus` parameter, this test is triggered by only placing the `sriov` matcher. If the tests are executed against a cluster where SR-IOV is installed but SCTP is not, adding the `-ginkgo.skip=SCTP` parameter causes the tests to skip SCTP testing.
====

[id="cnf-performing-end-to-end-tests-available-features_{context}"]
=== Available features

The set of available features to filter are:

* `performance`
* `sriov`
* `ptp`
* `sctp`
* `xt_u32`
* `dpdk`
* `container-mount-namespace`

[id="cnf-performing-end-to-end-tests-dry-run_{context}"]
== Dry run

Use this command to run in dry-run mode. This is useful for checking what is in the test suite and provides output for all of the tests the image would run.

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -e KUBECONFIG=/kubeconfig/kubeconfig registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/test-run.sh -ginkgo.dryRun -ginkgo.v
----

[id="cnf-performing-end-to-end-tests-disconnected-mode_{context}"]
== Disconnected mode

The CNF tests image support running tests in a disconnected cluster, meaning a cluster that is not able to reach outer registries. This is done in two steps:

. Performing the mirroring.

. Instructing the tests to consume the images from a custom registry.

[id="cnf-performing-end-to-end-tests-mirroring-images-to-custom-registry_{context}"]
=== Mirroring the images to a custom registry accessible from the cluster

A `mirror` executable is shipped in the image to provide the input required by `oc` to mirror the images needed to run the tests to a local registry.

Run this command from an intermediate machine that has access both to the cluster and to link:https://catalog.redhat.com/software/containers/explore[registry.redhat.io] over the Internet:

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -e KUBECONFIG=/kubeconfig/kubeconfig registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/mirror -registry my.local.registry:5000/ |  oc image mirror -f -
----

Then, follow the instructions in the following section about overriding the registry used to fetch the images.

[id="instruct-the-tests-to-consume-images-from-a-custom-registry_{context}"]
=== Instruct the tests to consume those images from a custom registry

This is done by setting the `IMAGE_REGISTRY` environment variable:

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -e KUBECONFIG=/kubeconfig/kubeconfig -e IMAGE_REGISTRY="my.local.registry:5000/" -e CNF_TESTS_IMAGE="custom-cnf-tests-image:latests" registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/test-run.sh
----

[id="cnf-performing-end-to-end-tests-mirroring-to-cluster-internal-registry_{context}"]
=== Mirroring to the cluster internal registry

{product-title} provides a built-in container image registry, which runs as a standard workload on the cluster.

.Procedure

. Gain external access to the registry by exposing it with a route:
+
[source,terminal]
----
$ oc patch configs.imageregistry.operator.openshift.io/cluster --patch '{"spec":{"defaultRoute":true}}' --type=merge
----

. Fetch the registry endpoint:
+
[source,terminal]
----
REGISTRY=$(oc get route default-route -n openshift-image-registry --template='{{ .spec.host }}')
----

. Create a namespace for exposing the images:
+
[source,terminal]
----
$ oc create ns cnftests
----

. Make that image stream available to all the namespaces used for tests. This is required to allow the tests namespaces to fetch the images from the `cnftests` image stream.
+
[source,terminal]
----
$ oc policy add-role-to-user system:image-puller system:serviceaccount:sctptest:default --namespace=cnftests
----
+
[source,terminal]
----
$ oc policy add-role-to-user system:image-puller system:serviceaccount:cnf-features-testing:default --namespace=cnftests
----
+
[source,terminal]
----
$ oc policy add-role-to-user system:image-puller system:serviceaccount:performance-addon-operators-testing:default --namespace=cnftests
----
+
[source,terminal]
----
$ oc policy add-role-to-user system:image-puller system:serviceaccount:dpdk-testing:default --namespace=cnftests
----
+
[source,terminal]
----
$ oc policy add-role-to-user system:image-puller system:serviceaccount:sriov-conformance-testing:default --namespace=cnftests
----

. Retrieve the docker secret name and auth token:
+
[source,bash]
----
SECRET=$(oc -n cnftests get secret | grep builder-docker | awk {'print $1'}
TOKEN=$(oc -n cnftests get secret $SECRET -o jsonpath="{.data['\.dockercfg']}" | base64 --decode | jq '.["image-registry.openshift-image-registry.svc:5000"].auth')
----

. Write a `dockerauth.json` similar to this:
+
[source,bash]
----
echo "{\"auths\": { \"$REGISTRY\": { \"auth\": $TOKEN } }}" > dockerauth.json
----

. Do the mirroring:
+
[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -e KUBECONFIG=/kubeconfig/kubeconfig registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/mirror -registry $REGISTRY/cnftests |  oc image mirror --insecure=true -a=$(pwd)/dockerauth.json -f -
----

. Run the tests:
+
[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -e KUBECONFIG=/kubeconfig/kubeconfig -e IMAGE_REGISTRY=image-registry.openshift-image-registry.svc:5000/cnftests cnf-tests-local:latest /usr/bin/test-run.sh
----

[id="mirroring-different-set-of-images_{context}"]
=== Mirroring a different set of images

.Procedure

. The `mirror` command tries to mirror the u/s images by default. This can be overridden by passing a file with the following format to the image:
+
[source,yaml]
----
[
    {
        "registry": "public.registry.io:5000",
        "image": "imageforcnftests:4.8"
    },
    {
        "registry": "public.registry.io:5000",
        "image": "imagefordpdk:4.8"
    }
]
----

. Pass it to the `mirror` command, for example saving it locally as `images.json`. With the following command, the local path is mounted in `/kubeconfig` inside the container and that can be passed to the mirror command.
+
[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -e KUBECONFIG=/kubeconfig/kubeconfig registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/mirror --registry "my.local.registry:5000/" --images "/kubeconfig/images.json" |  oc image mirror -f -
----

[id="discovery-mode_{context}"]
== Discovery mode

Discovery mode allows you to validate the functionality of a cluster without altering its configuration. Existing environment configurations are used for the tests. The tests attempt to find the configuration items needed and use those items to execute the tests. If resources needed to run a specific test are not found, the test is skipped, providing an appropriate message to the user. After the tests are finished, no cleanup of the pre-configured configuration items is done, and the test environment can be immediately used for another test run.

Some configuration items are still created by the tests. These are specific items needed for a test to run; for example, a SR-IOV Network. These configuration items are created in custom namespaces and are cleaned up after the tests are executed.

An additional bonus is a reduction in test run times. As the configuration items are already there, no time is needed for environment configuration and stabilization.

To enable discovery mode, the tests must be instructed by setting the `DISCOVERY_MODE` environment variable as follows:

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig -e
DISCOVERY_MODE=true registry.redhat.io/openshift-kni/cnf-tests /usr/bin/test-run.sh
----

[id="required-environment-config-prereqs_{context}"]
=== Required environment configuration prerequisites

.SR-IOV tests

Most SR-IOV tests require the following resources:

* `SriovNetworkNodePolicy`.
* At least one with the resource specified by `SriovNetworkNodePolicy` being allocatable; a resource count of at least 5 is considered sufficient.

Some tests have additional requirements:

* An unused device on the node with available policy resource, with link state `DOWN` and not a bridge slave.
* A `SriovNetworkNodePolicy` with a MTU value of `9000`.

.DPDK tests

The DPDK related tests require:

* A performance profile.
* A SR-IOV policy.
* A node with resources available for the SR-IOV policy and available with the `PerformanceProfile` node selector.

.PTP tests

* A slave `PtpConfig` (`ptp4lOpts="-s" ,phc2sysOpts="-a -r"`).
* A node with a label matching the slave `PtpConfig`.

.SCTP tests

* `SriovNetworkNodePolicy`.
* A node matching both the `SriovNetworkNodePolicy` and a `MachineConfig` that enables SCTP.

.XT_U32 tests

* A node with a machine config that enables XT_U32.

.Performance Operator tests

Various tests have different requirements. Some of them are:

* A performance profile.
* A performance profile having `profile.Spec.CPU.Isolated = 1`.
* A performance profile having `profile.Spec.RealTimeKernel.Enabled == true`.
* A node with no huge pages usage.

.Container-mount-namespace tests

* A node with a machine config which enables `container-mount-namespace` mode

[id="limiting-nodes-used-during-tests_{context}"]
=== Limiting the nodes used during tests

The nodes on which the tests are executed can be limited by specifying a `NODES_SELECTOR` environment variable. Any resources created by the test are then limited to the specified nodes.

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig -e
NODES_SELECTOR=node-role.kubernetes.io/worker-cnf registry.redhat.io/openshift-kni/cnf-tests /usr/bin/test-run.sh
----

[id="using-single-performance-profile_{context}"]
=== Using a single performance profile

The resources needed by the DPDK tests are higher than those required by the performance test suite. To make the execution faster, the performance profile used by tests can be overridden using one that also serves the DPDK test suite.

To do this, a profile like the following one can be mounted inside the container, and the performance tests can be instructed to deploy it.

[source,yaml]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: performance
spec:
  cpu:
    isolated: "4-15"
    reserved: "0-3"
  hugepages:
    defaultHugepagesSize: "1G"
    pages:
    - size: "1G"
      count: 16
      node: 0
  realTimeKernel:
    enabled: true
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
----

[NOTE]
====
When you configure reserved and isolated CPUs, the infra containers in pods use the reserved CPUs and the application containers use the isolated CPUs.
====

To override the performance profile used, the manifest must be mounted inside the container and the tests must be instructed by setting the `PERFORMANCE_PROFILE_MANIFEST_OVERRIDE` parameter as follows:

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig -e
PERFORMANCE_PROFILE_MANIFEST_OVERRIDE=/kubeconfig/manifest.yaml registry.redhat.io/openshift-kni/cnf-tests /usr/bin/test-run.sh
----

[id="disabling-performance-profile-cleanup_{context}"]
=== Disabling the performance profile cleanup

When not running in discovery mode, the suite cleans up all the created artifacts and configurations. This includes the performance profile.

When deleting the performance profile, the machine config pool is modified and nodes are rebooted. After a new iteration, a new profile is created. This causes long test cycles between runs.

To speed up this process, set `CLEAN_PERFORMANCE_PROFILE="false"` to instruct the tests not to clean the performance profile. In this way, the next iteration will not need to create it and wait for it to be applied.

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig -e
CLEAN_PERFORMANCE_PROFILE="false" registry.redhat.io/openshift-kni/cnf-tests /usr/bin/test-run.sh
----

[id="cnf-performing-end-to-end-tests-running-in-single-node-cluster_{context}"]
== Running in a single node cluster

Running tests on a single node cluster causes the following limitations to be imposed:

* Longer timeouts for certain tests, including SR-IOV and SCTP tests
* Tests requiring master and worker nodes are skipped

Longer timeouts concern SR-IOV and SCTP tests. Reconfiguration requiring node reboots cause a reboot of the entire environment, including the OpenShift control plane, and therefore takes longer to complete. All PTP tests requiring a master and worker node are skipped. No additional configuration is needed because the tests check for the number of nodes at startup and adjust test behavior accordingly.

PTP tests can run in Discovery mode. The tests look for a PTP master configured outside of the cluster.

For more information, see the Discovery mode section.
// TODO update to xref

To enable Discovery mode, the tests must be instructed by setting the `DISCOVERY_MODE` environment variable as follows:

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig -e
DISCOVERY_MODE=true registry.redhat.io/openshift-kni/cnf-tests /usr/bin/test-run.sh
----

[discrete]
=== Required parameters

* `ROLE_WORKER_CNF=master` - Required because master is the only machine pool to which the node will belong.
* `XT_U32TEST_HAS_NON_CNF_WORKERS=false` - Required to instruct the xt_u32 negative test to skip because there are only nodes where the module is loaded.
* `SCTPTEST_HAS_NON_CNF_WORKERS=false` - Required to instruct the SCTP negative test to skip because there are only nodes where the module is loaded.

[id="cnf-performing-end-to-end-tests-troubleshooting_{context}"]
== Troubleshooting

The cluster must be reached from within the container. You can verify this by running:

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -e KUBECONFIG=/kubeconfig/kubeconfig
registry.redhat.io/openshift-kni/cnf-tests oc get nodes
----

If this does not work, it could be caused by spanning across DNS, MTU size, or firewall issues.

[id="cnf-performing-end-to-end-tests-test-reports_{context}"]
== Test reports

CNF end-to-end tests produce two outputs: a JUnit test output and a test failure report.

[id="cnf-performing-end-to-end-tests-junit-test-output_{context}"]
=== JUnit test output

A JUnit-compliant XML is produced by passing the `--junit` parameter together with the path where the report is dumped:

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -v $(pwd)/junitdest:/path/to/junit -e KUBECONFIG=/kubeconfig/kubeconfig registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/test-run.sh --junit /path/to/junit
----

[id="cnf-performing-end-to-end-tests-test-failure-report_{context}"]
=== Test failure report

A report with information about the cluster state and resources for troubleshooting can be produced by passing the `--report` parameter with the path where the report is dumped:

[source,terminal]
----
$ docker run -v $(pwd)/:/kubeconfig -v $(pwd)/reportdest:/path/to/report -e KUBECONFIG=/kubeconfig/kubeconfig registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/test-run.sh --report /path/to/report
----

[id="cnf-performing-end-to-end-tests-podman_{context}"]
=== A note on podman

When executing podman as non root and non privileged, mounting paths can fail with "permission denied" errors. To make it work, append `:Z` to the volumes creation; for example, `-v $(pwd)/:/kubeconfig:Z` to allow podman to do the proper SELinux relabeling.

[id="cnf-performing-end-to-end-tests-running-on-4-4_{context}"]
=== Running on {product-title} 4.4

With the exception of the following, the CNF end-to-end tests are compatible with {product-title} 4.4:

[source,bash]
----
[test_id:28466][crit:high][vendor:cnf-qe@redhat.com][level:acceptance] Should contain configuration injected through openshift-node-performance profile
[test_id:28467][crit:high][vendor:cnf-qe@redhat.com][level:acceptance] Should contain configuration injected through the openshift-node-performance profile
----

You can skip these tests by adding the `-ginkgo.skip â€œ28466|28467"` parameter.

[id="cnf-performing-end-to-end-tests-using-single-performance-profile_{context}"]
=== Using a single performance profile

The DPDK tests require more resources than what is required by the performance test suite. To make the execution faster, you can override the performance profile used by the tests using a profile that also serves the DPDK test suite.

To do this, use a profile like the following one that can be mounted inside the container, and the performance tests can be instructed to deploy it.

[source,yaml]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
 name: performance
spec:
 cpu:
  isolated: "5-15"
  reserved: "0-4"
 hugepages:
  defaultHugepagesSize: "1G"
  pages:
  - size: "1G"
    count: 16
    node: 0
 realTimeKernel:
  enabled: true
 numa:
  topologyPolicy: "best-effort"
 nodeSelector:
  node-role.kubernetes.io/worker-cnf: ""
----

[NOTE]
====
When you configure reserved and isolated CPUs, the infra containers in pods use the reserved CPUs and the application containers use the isolated CPUs.
====

To override the performance profile, the manifest must be mounted inside the container and the tests must be instructed by setting the `PERFORMANCE_PROFILE_MANIFEST_OVERRIDE`:

[source,termal]
----
$ docker run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig -e PERFORMANCE_PROFILE_MANIFEST_OVERRIDE=/kubeconfig/manifest.yaml registry.redhat.io/openshift4/cnf-tests-rhel8:v4.8 /usr/bin/test-run.sh
----

[id="cnf-performing-end-to-end-tests-cluster-impacts_{context}"]
== Impacts on the cluster

Depending on the feature, running the test suite could cause different impacts on the cluster. In general, only the SCTP tests do not change the cluster configuration. All of the other features have various impacts on the configuration.

[id="cnf-performing-end-to-end-tests-sctp_{context}"]
=== SCTP

SCTP tests just run different pods on different nodes to check connectivity. The impacts on the cluster are related to running simple pods on two nodes.

[id="cnf-performing-end-to-end-tests-xtu32_{context}"]
=== XT_U32

XT_U32 tests run pods on different nodes to check iptables rule that utilize xt_u32. The impacts on the cluster are related to running simple pods on two nodes.

[id="cnf-performing-end-to-end-tests-sr-iov_{context}"]
=== SR-IOV

SR-IOV tests require changes in the SR-IOV network configuration, where the tests create and destroy different types of configuration.

This might have an impact if existing SR-IOV network configurations are already installed on the cluster, because there may be conflicts depending on the priority of such configurations.

At the same time, the result of the tests might be affected by existing configurations.

[id="cnf-performing-end-to-end-tests-ptp_{context}"]
=== PTP

PTP tests apply a PTP configuration to a set of nodes of the cluster. As with SR-IOV, this might conflict with any existing PTP configuration already in place, with unpredictable results.

[id="cnf-performing-end-to-end-tests-performance_{context}"]
=== Performance

Performance tests apply a performance profile to the cluster. The effect of this is changes in the node configuration, reserving CPUs, allocating memory huge pages, and setting the kernel packages to be realtime. If an existing profile named `performance` is already available on the cluster, the tests do not deploy it.

[id="cnf-performing-end-to-end-tests-dpdk_{context}"]
=== DPDK

DPDK relies on both performance and SR-IOV features, so the test suite configures both a performance profile and SR-IOV networks, so the impacts are the same as those described in SR-IOV testing and performance testing.

[id="cnf-performing-end-to-end-tests-container-mount-namespace_{context}"]
=== Container-mount-namespace

The validation test for `container-mount-namespace` mode only checks that the appropriate `MachineConfig` objects are present and active, and has no additional impact on the node.

[id="cnf-performing-end-to-end-tests-cleaning-up_{context}"]
=== Cleaning up

After running the test suite, all the dangling resources are cleaned up.
