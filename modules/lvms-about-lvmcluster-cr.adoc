// Module included in the following assemblies:
//
// storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc

:_mod-docs-content-type: CONCEPT
[id="about-lvmcluster_{context}"]
= About the LVMCluster custom resource

After you have installed {lvms}, you must create an `LVMCluster` custom resource (CR) on a worker node.

You can configure the `LVMCluster` CR to perform the following actions:

* Create LVM volume groups that you can use to provision persistent volume claims (PVCs).
* Configure a list of devices that you want to add to the LVM volume groups. 
* Configure the requirements to select the nodes on which you want to create an LVM volume group, and the thin pool configuration for the volume group.
* Force wipe the selected devices.

You can create an `LVMCluster` CR using the OpenShift CLI (`oc`) or the {product-title} web console.

include::snippets/lvms-creating-lvmcluster.adoc[]

[discrete]
== LVMCluster custom resource configuration

The `LVMCluster` CR fields are described in the following table:

.`LVMCluster` CR fields
[cols=".^2,.^2,.^6a",options="header"]
|====

|Field|Type|Description

|`spec.storage.deviceClasses`
|`array`
|Contains the configuration to assign the local storage devices to the LVM volume groups. 

To create multiple device storage classes, create a device class for each storage class. 

After creating an `LVMCluster` CR, if you add or remove a device class, the update reflects in the `LVMCluster` CR only after deleting and recreating the `TopoLVM-Node` pod. 

|`deviceClasses.name`
|`string`
|Specify a name for the LVM volume group (VG). 

If this field is set to the name of a VG from the previous {lvms} installation, {lvms} recreates the resources related to that volume group in the current {lvms} installation. 

You can only recover the VGs but not the logical volume associated with the VG.

[IMPORTANT]
====
Before recovering the VGs from the previous {lvms} installation, ensure that the following conditions are met:

* VGs must not be corrupted.
* VGs must have the `lvms` tag. For more information on adding tags to LVMS objects, see link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_logical_volumes/grouping-lvm-objects-with-tags_configuring-and-managing-logical-volumes#doc-wrapper[Grouping LVM objects with tags].
====

|`deviceClasses.fstype`
|`string`
|Set this field to `ext4` or `xfs`. By default, this field is set to `xfs`.

|`deviceClasses.default`
|`boolean`
|Set this field to `true` to indicate that a device class is the default. Otherwise, you can set it to `false`. You can only configure a single default device class. 

|`deviceClasses.nodeSelector`
|`object`
|Contains the configuration to choose the nodes on which you want to create the LVM volume group. If this field is empty, all nodes without no-schedule taints are considered.

[NOTE]
====
On the control-plane node, {lvms} detects and uses the additional worker nodes when the new nodes become active in the cluster. 
====

|`nodeSelector.nodeSelectorTerms`
|`array`
|Configure the requirements that are used to select the node.

|`deviceClasses.deviceSelector`
|`object`
|Contains the configuration to specify the paths to the devices that you want to add to the LVM volume group, and force wipe the devices that are added to the LVM volume group. 

You can specify the device paths in the `paths` field, the `optionalPaths` field, or both. If you do not specify the device paths in both `paths` and `optionalPaths`, {lvms} adds the supported unused devices to the LVM volume group.

[IMPORTANT]
====
After a device is added to the LVM volume group, it cannot be removed.
==== 

{lvms} adds the devices to the LVM volume group only if the following conditions are met:

* The device path exists.

* The device is supported by {lvms}.

[NOTE]
====
The devices that you want to add to the volume group must be supported by {lvms}. For information about unsupported devices, see _Devices not supported by {lvms}_ in _Additional resources_.
====

You can also add the path to the RAID arrays to integrate the RAID arrays with {lvms}. For more information, see _Integrating RAID arrays with {lvms}_ in _Additional resources_. 

|`deviceSelector.paths`
|`array`
|Specify the device paths.

If the device path specified in this field does not exist, or the device is not supported by {lvms}, the `LVMCluster` CR moves to the `Failed` state.

|`deviceSelector.optionalPaths`
|`array`
| Specify the optional device paths.

If the device path specified in this field does not exist, or the device is not supported by {lvms}, {lvms} ignores the device without causing an error.

|`deviceSelector.
forceWipeDevicesAndDestroyAllData`
|`boolean`
|To force wipe the selected devices, set this field to `true`. By default, this field is set to `false`.

[IMPORTANT]
====
Wiping the device can lead to inconsistencies in data integrity if any of the following conditions are met:

* The device is being used as swap space.
* The device is part of a RAID array.
* The device is mounted.

If any of these conditions are true, do not force wipe the disk. Instead, you must manually wipe the disk.
====

[WARNING]
====
If this field is set to `true`, {lvms} wipes all previous data on the devices. Use this feature with caution.
====

|`deviceClasses.thinPoolConfig`
|`object`
|Contains the configuration to create a thin pool in the LVM volume group.

|`thinPoolConfig.name`
|`string`
|Specify a name for the thin pool.

|`thinPoolConfig.sizePercent`
|`integer`
|Specify the percentage of space in the LVM volume group for creating the thin pool. 

By default, this field is set to 90. The minimum value that you can set is 10, and the maximum value is 90.

|`thinPoolConfig.overprovisionRatio`
|`integer`
|Specify a factor by which you can provision additional storage based on the available storage in the thin pool.

For example, if this field is set to 10, you can provision up to 10 times the amount of available storage in the thin pool.

To disable over-provisioning, set this field to 1.

|====

Upon creating the `LVMCluster` CR, {lvms} creates a storage class and volume snapshot class for each device class.

[NOTE]
====
{lvms} configures the name of the storage class and volume snapshot class in the format `lvms-<device-class-name>`. Where, `<device-class-name>` is the value of the `deviceClasses.name` field in the `LVMCluster` CR. For example, if the `deviceClasses.name` field is set to vg1, the name of the storage class and volume snapshot class is `lvms-vg1`.

To get the list of storage classes, run the following command:
[source,terminal]
----
$ oc get storageclass
----

To get the list of volume snapshot classes, run the following command:
[source,terminal]
----
$ oc get volumesnapshotclass
----
====

Upon creating the `LVMCluster` CR, {lvms} creates the following system-managed CRs:

* `LVMVolumeGroup`: This CR is a specific type of persistent volume (PV) that is backed by an LVM volume group. It tracks the individual volume groups across multiple nodes.
* `LVMVolumeGroupNodeStatus`: This CR tracks the status of the volume groups on a node.