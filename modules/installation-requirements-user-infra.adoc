// Module included in the following assemblies:
//
// * installing/installing_bare_metal/installing-bare-metal.adoc
// * installing/installing_bare_metal/installing-bare-metal-network-customizations.adoc
// * installing/installing_bare_metal/installing-restricted-networks-bare-metal.adoc
// * installing/installing_platform_agnostic/installing-platform-agnostic.adoc
// * installing/installing_vmc/installing-restricted-networks-vmc-user-infra.adoc
// * installing/installing_vmc/installing-vmc-user-infra.adoc
// * installing/installing_vmc/installing-vmc-network-customizations-user-infra.adoc
// * installing/installing_vsphere/installing-restricted-networks-vsphere.adoc
// * installing/installing_vsphere/installing-vsphere.adoc
// * installing/installing_vsphere/installing-vsphere-network-customizations.adoc
// * installing/installing_ibm_power/installing-ibm-power.adoc
// * installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc
// * installing/installing_ibm_z/installing-ibm-z.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc

ifeval::["{context}" == "installing-ibm-z"]
:ibm-z:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z"]
:ibm-z:
endif::[]
ifeval::["{context}" == "installing-ibm-power"]
:ibm-power:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-power"]
:ibm-power:
endif::[]
ifeval::["{context}" == "installing-bare-metal"]
:bare-metal:
endif::[]
ifeval::["{context}" == "installing-bare-metal-network-customizations"]
:bare-metal:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-bare-metal"]
:bare-metal:
endif::[]
ifeval::["{context}" == "installing-vsphere"]
:vsphere:
endif::[]
ifeval::["{context}" == "installing-vsphere-network-customizations"]
:vsphere:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-vsphere"]
:vsphere:
endif::[]
ifeval::["{context}" == "installing-platform-agnostic"]
:agnostic:
endif::[]

[id="installation-requirements-user-infra_{context}"]
= Machine requirements for a cluster with user-provisioned infrastructure

For a cluster that contains user-provisioned infrastructure, you must deploy all
of the required machines.

[id="machine-requirements_{context}"]
== Required machines

The smallest {product-title} clusters require the following hosts:

* One temporary bootstrap machine

* Three control plane, or master, machines

* At least two compute machines, which are also known as worker machines.
ifdef::bare-metal[]
If you are running a three-node cluster, running zero compute machines is supported. Running one compute machine is not supported.
endif::bare-metal[]

[NOTE]
====
The cluster requires the bootstrap machine to deploy the {product-title} cluster
on the three control plane machines. You can remove the bootstrap machine after
you install the cluster.
====

[IMPORTANT]
====
ifdef::ibm-z[]
To improve high availability of your cluster, distribute the control plane machines over different z/VM instances on at least two physical machines.
endif::ibm-z[]
ifndef::ibm-z[]
To maintain high availability of your cluster, use separate physical hosts for
these cluster machines.
endif::ibm-z[]
====

The bootstrap and control plane machines must use {op-system-first} as the operating system.

ifdef::bare-metal,agnostic[]
[IMPORTANT]
====
If the `platform: none` field is defined in the `install-config.yaml` file, virtual machines (VMs) configured to use virtual hardware version 14 or greater might result in a failed installation. It is recommended to configure VMs with virtual hardware version 13. For more information, see link:https://bugzilla.redhat.com/show_bug.cgi?id=1935539[BZ#1935539].
====
endif::bare-metal,agnostic[]

ifndef::openshift-origin[]
Note that {op-system} is based on {op-system-base-full} 8 and inherits all of its hardware certifications and requirements.
endif::[]
See link:https://access.redhat.com/articles/rhel-limits[Red Hat Enterprise Linux technology capabilities and limits].

ifdef::vsphere[]
[IMPORTANT]
====
All virtual machines must reside in the same datastore and in the same folder as the installer.
====
endif::vsphere[]


[id="network-connectivity_{context}"]
== Network connectivity requirements

All the {op-system-first} machines require network in `initramfs` during boot to fetch Ignition config files from the Machine Config Server.
ifndef::ibm-z[]
During the initial boot, the machines require either a DHCP server or that static IP addresses be set in order to establish a network connection to download their Ignition config files.
endif::ibm-z[]
ifdef::ibm-z[]
The machines are configured with static IP addresses. No DHCP server is required.
endif::ibm-z[]
Additionally, each {product-title} node in the cluster must have access to a Network Time Protocol (NTP) server.
ifndef::ibm-z[]
If a DHCP server provides NTP servers information, the chrony time service on the {op-system-first} machines read the information and can sync the clock with the NTP servers.
endif::ibm-z[]

ifdef::ibm-z[]
[id="ibm-z-network-connectivity_{context}"]
== IBM Z network connectivity requirements

To install on IBM Z under z/VM, you require a single z/VM virtual NIC in layer 2 mode. You also need:

*   A direct-attached OSA or RoCE network adapter
*   A z/VM VSwitch set up. For a preferred setup, use OSA link aggregation.
endif::ibm-z[]

[id="minimum-resource-requirements_{context}"]
== Minimum resource requirements

Each cluster machine must meet the following minimum requirements:

[cols="2,2,2,2,2",options="header"]
|===

|Machine
|Operating System
|vCPU ^[1]^
|Virtual RAM
|Storage

|Bootstrap
|{op-system}
ifdef::ibm-power[|2]
ifndef::ibm-power[|4]
|16 GB
|120 GB

|Control plane
|{op-system}
ifdef::ibm-power[|2]
ifndef::ibm-power[|4]
|16 GB
|120 GB

ifndef::openshift-origin[]
|Compute
ifdef::ibm-z,ibm-power[|{op-system}]
ifndef::ibm-z,ibm-power[|{op-system} or RHEL 7.9]
|2
|8 GB
|120 GB
endif::openshift-origin[]

ifdef::openshift-origin[]
|Compute
|{op-system}
|2
|8 GB
|120 GB
endif::openshift-origin[]
|===
[.small]
--
ifdef::ibm-z[]
1. 1 physical core (IFL) provides 2 logical cores (threads) when SMT-2 is enabled. The hypervisor can provide 2 or more vCPUs.
endif::ibm-z[]
ifndef::ibm-z[]
1. 1 vCPU is equivalent to 1 physical core when simultaneous multithreading (SMT), or hyperthreading, is not enabled. When enabled, use the following formula to calculate the corresponding ratio: (threads per core × cores) × sockets = vCPUs.
endif::ibm-z[]
--

ifdef::ibm-z[]
[id="minimum-ibm-z-system-requirements_{context}"]
== Minimum IBM Z system requirements

You can install {product-title} version {product-version} on the following IBM hardware:

* IBM Z, versions 13, 14, or 15
* LinuxONE, any version

[discrete]
=== Hardware requirements

* 1 LPAR with 6 IFLs that supports SMT2
* 1 OSA or RoCE network adapter

[discrete]
=== Operating system requirements

* One instance of z/VM 7.1 or later

On your z/VM instance, set up:

* 3 guest virtual machines for {product-title} control plane machines
* 2 guest virtual machines for {product-title} compute machines
* 1 guest virtual machine for the temporary {product-title} bootstrap machine

[discrete]
=== Disk storage for the z/VM guest virtual machines

* FICON attached disk storage (DASDs). These can be z/VM minidisks, fullpack minidisks, or dedicated DASDs, all of which must be formatted as CDL, which is the default. To reach the minimum required DASD size for {op-system-first} installations, you need extended address volumes (EAV). If available, use HyperPAV to ensure optimal performance.
* FCP attached disk storage

[discrete]
=== Storage / Main Memory

* 16 GB for {product-title} control plane machines
* 8 GB for {product-title} compute machines
* 16 GB for the temporary {product-title} bootstrap machine

[id="preferred-ibm-z-system-requirements_{context}"]
== Preferred IBM Z system requirements

[discrete]
=== Hardware requirements

* 3 LPARs with 6 IFLs each that support SMT2
* 1 or 2 OSA or RoCE network adapters, or both
* Hipersockets, which are attached to a node either directly as a device or by bridging with one z/VM VSWITCH to be transparent to the z/VM guest. To directly connect Hipersockets to a node, you must set up a gateway to the external network via a RHEL 8 guest to bridge to the Hipersockets network.

[discrete]
=== Operating system requirements

* 2 or 3 instances of z/VM 7.1 or later for high availability

On your z/VM instances, set up:

* 3 guest virtual machines for {product-title} control plane machines, one per z/VM instance
* At least 6 guest virtual machines for {product-title} compute machines, distributed across the z/VM instances
* 1 guest virtual machine for the temporary {product-title} bootstrap machine
* To ensure the availability of integral components in an overcommitted environment, increase the priority of the control plane using the CP command `SET SHARE`. Do the same for infrastructure plane machines if they exist. See link:https://www.ibm.com/docs/en/zvm/7.1?topic=commands-set-share[SET SHARE] in IBM Documentation.

[discrete]
=== Disk storage for the z/VM guest virtual machines

* FICON attached disk storage (DASDs). These can be z/VM minidisks, fullpack minidisks, or dedicated DASDs, all of which must be formatted as CDL, which is the default. To reach the minimum required DASD size for {op-system-first} installations, you need extended address volumes (EAV). If available, use HyperPAV and High Performance FICON (zHPF) to ensure optimal performance.
* FCP attached disk storage

[discrete]
=== Storage / Main Memory

* 16 GB for {product-title} control plane machines
* 8 GB for {product-title} compute machines
* 16 GB for the temporary {product-title} bootstrap machine
endif::ibm-z[]

ifdef::ibm-power[]

[id="minimum-ibm-power-system-requirements_{context}"]
== Minimum IBM Power Systems requirements

You can install {product-title} version {product-version} on the following IBM hardware:

* IBM POWER8 or POWER9 processor-based systems

[discrete]
=== Hardware requirements

* 6 IBM Power bare metal servers or 6 LPARs across multiple PowerVM servers

[discrete]
=== Operating system requirements

* One instance of an IBM POWER8 or POWER9 processor-based system

On your IBM Power instance, set up:

* 3 guest virtual machines for {product-title} control plane machines
* 2 guest virtual machines for {product-title} compute machines
* 1 guest virtual machine for the temporary {product-title} bootstrap machine

[discrete]
=== Disk storage for the IBM Power guest virtual machines

* Storage provisioned by the Virtual I/O Server using vSCSI, NPIV (N-Port ID Virtualization) or SSP (shared storage pools)

[discrete]
=== Network for the PowerVM guest virtual machines

* Virtualized by the Virtual I/O Server using Shared Ethernet Adapter
* Virtualized by the Virtual I/O Server using IBM VNIC

[discrete]
=== Storage / main memory

* 120 GB / 16 GB for {product-title} control plane machines
* 120 GB / 8 GB for {product-title} compute machines
* 120 GB / 16 GB for the temporary {product-title} bootstrap machine

[id="recommended-ibm-Power-system-requirements_{context}"]

== Recommended IBM Power system requirements
[discrete]
=== Hardware requirements

* 6 IBM Power bare metal servers or 6 LPARs across multiple PowerVM servers

[discrete]
=== Operating system requirements

* One instance of an IBM POWER8 or POWER9 processor-based system

On your IBM Power instance, set up:

* 3 guest virtual machines for {product-title} control plane machines
* 2 guest virtual machines for {product-title} compute machines
* 1 guest virtual machine for the temporary {product-title} bootstrap machine

[discrete]
=== Disk storage for the IBM Power guest virtual machines

* Storage provisioned by the Virtual I/O Server using vSCSI, NPIV (N-Port ID Virtualization) or SSP (shared storage pools)

[discrete]
=== Network for the PowerVM guest virtual machines

* Virtualized by the Virtual I/O Server using Shared Ethernet Adapter
* Virtualized by the Virtual I/O Server using IBM VNIC

[discrete]
=== Storage / main memory

* 120 GB / 32 GB for {product-title} control plane machines
* 120 GB / 32 GB for {product-title} compute machines
* 120 GB / 16 GB for the temporary {product-title} bootstrap machine

endif::ibm-power[]

[id="csr_management_{context}"]
== Certificate signing requests management

Because your cluster has limited access to automatic machine management when you
use infrastructure that you provision, you must provide a mechanism for approving
cluster certificate signing requests (CSRs) after installation. The
`kube-controller-manager` only approves the kubelet client CSRs. The
`machine-approver` cannot guarantee the validity of a serving certificate
that is requested by using kubelet credentials because it cannot confirm that
the correct machine issued the request. You must determine and implement a
method of verifying the validity of the kubelet serving certificate requests
and approving them.

ifeval::["{context}" == "installing-ibm-z"]
:!ibm-z:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z"]
:!ibm-z:
endif::[]
ifeval::["{context}" == "installing-ibm-power"]
:!ibm-power:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-power"]
:!ibm-power:
endif::[]
ifeval::["{context}" == "installing-bare-metal"]
:!bare-metal:
endif::[]
ifeval::["{context}" == "installing-bare-metal-network-customizations"]
:!bare-metal:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-bare-metal"]
:!bare-metal:
endif::[]
ifeval::["{context}" == "installing-platform-agnostic"]
:!agnostic:
endif::[]
