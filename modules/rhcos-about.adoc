// Module included in the following assemblies:
//
// * architecture/architecture_rhcos.adoc

[id="rhcos-about_{context}"]
= About {op-system}

{op-system-first} represents the next generation of single-purpose
container operating system technology. Created by the same development teams
that created Red Hat Enterprise Linux Atomic Host and CoreOS Container Linux,
{op-system} combines the quality standards of Red Hat Enterprise Linux (RHEL)
with the automated, remote upgrade features from Container Linux.

{op-system} is supported only as a component of {product-title}
{product-version} for all {product-title} machines. {op-system} is the only
supported operating system for {product-title} control plane, or master,
machines. While {op-system} is the default operating system for all cluster
machines, you can create compute, or worker, machines that use RHEL as their
operating system. There are two general ways {op-system} is deployed in
{product-title} {product-version}:

* If you install your cluster on infrastructure that the cluster provisions,
{op-system} images are downloaded to the target platform during installation,
and suitable Ignition config files, which control the {op-system} configuration,
are used to deploy the machines.

* If you install your cluster on infrastructure
that you manage, you must follow the installation documentation to obtain the
{op-system} images, generate Ignition config files, and use the Ignition config
files to provision your machines.

[id="rhcos-key-features_{context}"]
== Key {op-system} features

The following list describes key features of the {op-system} operating system:

* **Based on RHEL**: The underlying operating system consists primarily of RHEL components.
The same quality, security, and control measures that support RHEL also support
{op-system}. For example, {op-system} software is in
RPM packages, and each {op-system} system starts up with a RHEL kernel and a set
of services that are managed by the systemd init system.

* **Controlled immutability**: Although it contains RHEL components, {op-system}
is designed to be managed
more tightly than a default RHEL installation. Management is
performed remotely from the {product-title} cluster. When you set up your
{op-system} machines, you can modify only a few system settings. This controlled
immutability allows {product-title} to
store the latest state of {op-system} systems in the cluster so it is always
able to create additional machines and perform updates based on the latest {op-system}
configurations.

* **CRI-O container runtime**: Although {op-system} contains features for running the
OCI- and libcontainer-formatted containers that Docker requires, it incorporates
the CRI-O container engine
instead of the Docker container engine. By focusing on features needed by
Kubernetes platforms, such as {product-title}, CRI-O can offer specific
compatibility with different Kubernetes versions. CRI-O also offers a smaller
footprint and reduced attack surface than is possible with container engines
that offer a larger feature set. At the moment, CRI-O is only available as a
container engine within {product-title} clusters.

* **Set of container tools**: For tasks such as building, copying, and otherwise
managing containers, {op-system} replaces the Docker CLI tool with a compatible
set of container tools. The podman CLI tool supports many container runtime
features, such as running, starting, stopping, listing, and removing containers
and container images. The skopeo CLI tool can copy, authenticate, and sign
images. You can use the crictl CLI tool to work with containers and pods from the
CRI-O container engine. While direct use of these tools in {op-system} is
discouraged, you can use them for debugging purposes.

* **rpm-ostree upgrades**: {op-system} features transactional upgrades using
the `rpm-ostree` system.
Updates are delivered via container images and are part of the
OpenShift update process. When deployed, the container image is pulled,
extracted, and written to disk, then the bootloader is modified to boot into
the new version. The machine will reboot into the update in a rolling manner to
ensure cluster capacity is minimally impacted.

* **Updated via MachineConfigOperator**:
In {product-title}, the Machine Config Operator handles operating system upgrades.
Instead of upgrading individual packages, as is done with `yum`
upgrades, `rpm-ostree` delivers upgrades of the OS as an atomic unit. The
new OS deployment is staged during upgrades and goes into effect on the next reboot.
If something goes wrong with the upgrade, a single rollback and reboot returns the
system to the previous state. {op-system} upgrades in {product-title} are performed
during cluster updates.

For {op-system} systems, the layout of the `rpm-ostree` file system has the
 following characteristics:

* `/usr` is where the operating system binaries and libraries are stored and is
 read-only. We do not support altering this.
* `/etc`, `/boot`, `/var` are writable on the system but only intended to be altered
 by the Machine Config Operator.
* `/var/lib/containers` is the graph storage location for storing container
 images.

[id="rhcos-configured_{context}"]
== How {op-system} systems are configured

{op-system} is designed to deploy on a {product-title} cluster with a minimal amount
of user configuration. In its most basic form, this consists of:

* Starting with a provisioned infrastructure (such as on AWS) or provisioning
the infrastructure yourself

* Supplying a few pieces of information, such as credentials and cluster name,
in an `install-config.yaml` file when running `openshift-install`.

Because {op-system} systems in {product-title} are designed to be fully managed
from the {product-title} cluster after that, directly changing an {op-system} machine is
discouraged. Although limited direct access to {op-system} machines
cluster can be accomplished for debugging purposes, you should not directly configure
{op-system} systems. 
Instead, if you need to add or change features on your {product-title} nodes, 
consider making changes in the following ways:

* **Kubernetes workload objects (DaemonSets, Deployments, etc.)**: If you need to
add services or other user-level features to your cluster, consider adding them as
Kubernetes workload objects. Keeping those features outside of specific node
configurations is the best way to reduce the risk of breaking the cluster on
subsequent upgrades.

* **Day-2 customizations**: If possible, bring up a cluster without doing any
customizations to cluster nodes and make needed node changes after the cluster is up.
Those changes will be easier to track later and less likely to break updates.
Creating MachineConfigs or modifying Operator custom resources
are ways of making these customizations.

* **Day-1 customizations**: For customizations that you must implement when the
cluster first comes up, there are ways of modifying your cluster so changes are
implemented on first boot.
Day-1 customizations can be done through Ignition configs and manifest files
during `openshift-install` or by adding boot options during ISO installs
provisioned by the user.

Here are examples of customizations you could do on day-1:

* **Kernel arguments**: If particular kernel features or tuning is needed on nodes when the cluster first boots

* **Disk encryption**: If your security needs require that the root filesystem on the nodes are encrypted (such as with FIPS compliance)

* **Kernel modules**: If a particular hardware device, such as a network card or video card, doesn’t have a usable module available by default in the Linux kernel

* **Chronyd**: If you want to provide specific clock settings to your nodes,
such as the location of time servers

To accomplish these tasks, you can interrupt the installation process to provide your own
Ignition configs through manifest files during installation.
Manifests that produce MachineConfigs can instead be passed to the Machine Config Operator
after the cluster is up.

[NOTE]
====
The Ignition config files that the installation program generates contain certificates that expire after 24 hours. You must complete your cluster installation and keep the cluster running for 24 hours in a non-degraded state to ensure that the first certificate rotation has finished.
====

To better understand your opportunities to configure {op-system} systems,
it helps to learn how {op-system} systems are deployed in {product-title}.

[id="rhcos-deployed_{context}"]
== How {op-system} systems are deployed

Differences between {op-system} deployments for {product-title} are based on
whether you are deploying on an infrastructure provisioned by the installer or by the user:

* **Installer provisioned**: Some cloud environments offer pre-configured infrastructures
that allow you to bring up an {product-title} cluster with minimal configuration.
For these types of deployments, you can supply Ignition configs
that place content on each node so it is there when the cluster first boots.

* **User provisioned**: If you are provisioning your own infrastructure, you have more flexibility
in how you add content to a {op-system} node. For example, you could add kernel
options when you boot the {op-system} ISO installer to install each system.
However, in most cases where configuration is required on the operating system
itself, it is best to provide that configuration through an Ignition config.

The Ignition facility runs only when the {op-system} system is first set up.
After that, Ignition configs can be supplied later using the MachineConfigs.

For information on specific changes you can make to RHCOS nodes using
Ignition configs, see Customizing Nodes.


[id="rhcos-about-ignition_{context}"]
= About Ignition

Ignition is the utility that is used by {op-system} to manipulate disks during
initial configuration. It completes common disk tasks, including partitioning
disks, formatting partitions, writing files, and configuring users. On first
boot, Ignition reads its configuration from the installation media or the
location that you specify and applies the configuration to the machines.

Whether you are installing your cluster or adding machines to it, Ignition
always performs the initial configuration of the {product-title}
cluster machines. Most of the actual system setup happens on each machine
itself. For each machine,
Ignition takes the {op-system} image and boots the {op-system} kernel. Options
on the kernel command line, identify the type of deployment and the location of
the Ignition-enabled initial Ram Disk (initramfs).

{product-title} uses Ignition version 3.

////
Due to some quirks in the tooling used to create Ignition, some explanation is
needed for its version numbers:

* Ignition version: {product-title} {product-version} uses Ignition v3. The
previous versions were all 0.x and 2.x. There is no Ignition version 1.
* Ignition configs version: {product-title} {product-version} uses v2.3 and
v3.0 Ignition configs and only supports those versions. Previous Ignition config versions
included v1, v2, v2.1, v2.2, and v2.3. If presented with those earlier versions,
Ignition upgrades that Ignition config through each version until it reaches
v2.3, then runs the resulting Ignition config. +
 +
One of the new features of Ignition v3 is that it 
allows a child Ignition config that merges with a parent config to overwrite any
file on the parent for which there is a conflict. The merge and replace features
of the
https://github.com/coreos/ignition/blob/master/doc/configuration-v3_0.md[Ignition config v3 spec]
provides cleaner ways of managing these conflicts. This feature also allows many
system types to share a common Ignition config, while differences, such as specific
hardware or cloud features, can be added with child configs. +
 +
All v3 versions (v3.1, v3.2, etc.) will be guaranteed to be supported until v4
comes out. At that point, fields deprecated in later v3.x versions could be removed in v4.
////

[id="about-ignition_{context}"]
== How Ignition works

To create machines by using Ignition, you need Ignition config files. The
{product-title} installation program creates the Ignition config files that you
need to deploy your cluster. These files are based on the information that you
provide to the installation program directly or through an `install-config.yaml`
file.

The way that Ignition configures machines is similar to how tools like
https://cloud-init.io/[cloud-init] or Linux Anaconda
https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/installation_guide/index#chap-kickstart-installations[kickstart]
configure systems, but with some important differences:

////
The order
of information in those files doesn’t matter. For example, if a file needs a
directory several levels deep, if another file needs a directory along that
path, the later file is created first. Ignition sorts and creates all files,
directories, and links by depth.
////

* Ignition runs from an initial RAM disk that is separate
from the system you are installing to. Because of that, Ignition can
repartition disks, set up file systems, and perform other changes to the
machine’s permanent file system. In contrast, cloud-init runs as part of a
machine’s init system when
the system boots, so making foundational changes to things like disk partitions
cannot be done as easily. With cloud-init, it is also difficult to reconfigure
the boot process while you are in the middle of the node's boot process.

* Ignition is meant to initialize systems, not change existing systems. After a
machine initializes and the kernel is running from the installed system, the
Machine Config Operator from the {product-title} cluster completes all future
machine configuration.

* Instead of completing a defined set of actions, Ignition implements
a declarative configuration. It checks that all partitions, files, services,
and other items are in place before the new machine starts. It then makes the
changes, like copying files to disk that are necessary for the new machine to
meet the specified configuration.

* After Ignition finishes configuring a machine, the kernel keeps running but
discards the initial RAM disk and pivots to the installed system on disk. All of
the new system services and other features start without requiring a system
reboot.

* Because Ignition confirms that all new machines meet the declared configuration,
you cannot have a partially-configured machine. If a machine’s setup fails,
the initialization process does not finish, and Ignition does not start the new
machine. Your cluster will never contain partially-configured machines. If
Ignition cannot complete, the machine is not added to the cluster. You must add
a new machine instead. This behavior prevents the difficult case of debugging a machine when the results of a
failed configuration task are not known until something that depended on it
fails at a later date.

* If there is a problem with an
Ignition config that causes the setup of a machine to fail, Ignition will not try
to use the same config to set up another machine. For example, a failure could
result from an Ignition config made up of a parent and child config that both
want to create the same file. A failure in such a case would prevent that
Ignition config from being used again to set up an other machines, until the
problem is resolved.

* If you have multiple Ignition config files, you get a union of that set of
configs.  Because Ignition is declarative, conflicts between the configs could
cause Ignition to fail to set up the machine. The order of information in those
files doesn’t matter. Ignition will sort and implement each setting in ways that
 make the most sense. For example, if a file needs a directory several levels
 deep, if another file needs a directory along that path, the later file is
 created first. Ignition sorts and creates all files, directories, and
 links by depth.

* Because Ignition can start with a completely empty hard disk, it can do
something cloud-init can’t do: set up systems on bare metal from scratch
(using features such as PXE boot). In the bare metal case, the Ignition config
is injected into the boot partition so Ignition can find it and configure
the system correctly.


[id="ignition-sequence_{context}"]
== The Ignition sequence

The Ignition process for an {op-system} machine in an {product-title} cluster
involves the following steps:

* The machine gets its Ignition config file. Master machines get their Ignition
config files from the bootstrap machine, and worker machines get Ignition config
files from a master.
* Ignition creates disk partitions, file systems, directories, and links on the
machine. It supports RAID arrays but does not support LVM volumes
* Ignition mounts the root of the permanent file system to the `/sysroot`
directory in the
initramfs and starts working in that `/sysroot` directory.
* Ignition configures all defined file systems and sets them up to mount appropriately
at runtime.
* Ignition runs `systemd` temporary files to populate required files in the
`/var` directory.
* Ignition runs the Ignition config files to set up users, systemd unit files,
and other configuration files.
* Ignition unmounts all components in the permanent system that were mounted in
the initramfs.
* Ignition starts up new machine’s init process which, in turn, starts up all other
services on the machine that run during system boot.

The machine is then ready to join the cluster and does not require a reboot.

////
After Ignition finishes its work on an individual machine, the kernel pivots to the
installed system. The initial RAM disk is no longer used and the kernel goes on
to run the init service to start up everything on the host from the installed
disk. When the last machine under the bootstrap machine’s control is completed, and
the services on those machines come up, the work of the bootstrap machine is over.
////
