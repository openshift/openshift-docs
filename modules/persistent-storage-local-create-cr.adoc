// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-local.adoc

:_mod-docs-content-type: PROCEDURE
[id="local-volume-cr_{context}"]
= Provisioning local volumes by using the Local Storage Operator

Local volumes cannot be created by dynamic provisioning. Instead, persistent volumes can be created by the Local Storage Operator. The local volume provisioner looks for any file system or block volume devices at the paths specified in the defined resource.

.Prerequisites

* The Local Storage Operator is installed.
* You have a local disk that meets the following conditions:
** It is attached to a node.
** It is not mounted.
** It does not contain partitions.

.Procedure

. Create the local volume resource. This resource must define the nodes and paths to the local volumes.
+
[NOTE]
====
Do not use different storage class names for the same device. Doing so will create multiple persistent volumes (PVs).
====
+
--
.Example: Filesystem
[source,yaml]
----
apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-disks"
  namespace: "openshift-local-storage" <1>
spec:
  nodeSelector: <2>
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - ip-10-0-140-183
          - ip-10-0-158-139
          - ip-10-0-164-33
  storageClassDevices:
    - storageClassName: "local-sc" <3>
      forceWipeDevicesAndDestroyAllData: false <4>
      volumeMode: Filesystem <5>
      fsType: xfs <6>
      devicePaths: <7>
        - /path/to/device <8>
----
<1> The namespace where the Local Storage Operator is installed.
<2> Optional: A node selector containing a list of nodes where the local storage volumes are attached. This example uses the node hostnames, obtained from `oc get node`. If a value is not defined, then the Local Storage Operator will attempt to find matching disks on all available nodes.
<3> The name of the storage class to use when creating persistent volume objects. The Local Storage Operator automatically creates the storage class if it does not exist. Be sure to use a storage class that uniquely identifies this set of local volumes.
<4> This setting defines whether or not to call `wipefs`, which removes partition table signatures (magic strings) making the disk ready to use for Local Storage Operator (LSO) provisioning. No other data besides signatures is erased. The default is "false" (`wipefs` is not invoked). Setting `forceWipeDevicesAndDestroyAllData` to "true" can be useful in scenarios where previous data can remain on disks that need to be re-used. In these scenarios, setting this field to true eliminates the need for administrators to erase the disks manually. Such cases can include single-node OpenShift (SNO) cluster environments where a node can be redeployed multiple times or when using OpenShift Data Foundation (ODF), where previous data can remain on the disks planned to be consumed as object storage devices (OSDs).
<5> The volume mode, either `Filesystem` or `Block`, that defines the type of local volumes.
+
[NOTE]
====
A raw block volume (`volumeMode: Block`) is not formatted with a file system. Use this mode only if any application running on the pod can use raw block devices.
====
<6> The file system that is created when the local volume is mounted for the first time.
<7> The path containing a list of local storage devices to choose from.
<8> Replace this value with your actual local disks filepath to the `LocalVolume` resource `by-id`, such as `/dev/disk/by-id/wwn`. PVs are created for these local disks when the provisioner is deployed successfully.
+
[NOTE]
====
If you are running {product-title} with {op-system-base} KVM, you must assign a serial number to your VM disk. Otherwise, the VM disk can not be identified after reboot. You can use the `virsh edit <VM>` command to add the `<serial>mydisk</serial>` definition.
====
--
+
.Example: Block
[source,yaml]
----
apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-disks"
  namespace: "openshift-local-storage" <1>
spec:
  nodeSelector: <2>
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - ip-10-0-136-143
          - ip-10-0-140-255
          - ip-10-0-144-180
  storageClassDevices:
    - storageClassName: "local-sc" <3>
      forceWipeDevicesAndDestroyAllData: false <4>
      volumeMode: Block <5>
      devicePaths: <6>
        - /path/to/device <7>
----
<1> The namespace where the Local Storage Operator is installed.
<2> Optional: A node selector containing a list of nodes where the local storage volumes are attached. This example uses the node hostnames, obtained from `oc get node`. If a value is not defined, then the Local Storage Operator will attempt to find matching disks on all available nodes.
<3> The name of the storage class to use when creating persistent volume objects.
<4> This setting defines whether or not to call `wipefs`, which removes partition table signatures (magic strings) making the disk ready to use for Local Storage Operator (LSO) provisioning. No other data besides signatures is erased. The default is "false" (`wipefs` is not invoked). Setting `forceWipeDevicesAndDestroyAllData` to "true" can be useful in scenarios where previous data can remain on disks that need to be re-used. In these scenarios, setting this field to true eliminates the need for administrators to erase the disks manually. Such cases can include single-node OpenShift (SNO) cluster environments where a node can be redeployed multiple times or when using OpenShift Data Foundation (ODF), where previous data can remain on the disks planned to be consumed as object storage devices (OSDs).
<5> The volume mode, either `Filesystem` or `Block`, that defines the type of local volumes.
<6> The path containing a list of local storage devices to choose from.
<7> Replace this value with your actual local disks filepath to the `LocalVolume` resource `by-id`, such as `dev/disk/by-id/wwn`. PVs are created for these local disks when the provisioner is deployed successfully.
+
[NOTE]
====
If you are running {product-title} with {op-system-base} KVM, you must assign a serial number to your VM disk. Otherwise, the VM disk can not be identified after reboot. You can use the `virsh edit <VM>` command to add the `<serial>mydisk</serial>` definition.
====

. Create the local volume resource in your {product-title} cluster. Specify the file you just created:
+
[source,terminal]
----
$ oc create -f <local-volume>.yaml
----

. Verify that the provisioner was created and that the corresponding daemon sets were created:
+
[source,terminal]
----
$ oc get all -n openshift-local-storage
----
+
.Example output
[source,terminal]
----
NAME                                          READY   STATUS    RESTARTS   AGE
pod/diskmaker-manager-9wzms                   1/1     Running   0          5m43s
pod/diskmaker-manager-jgvjp                   1/1     Running   0          5m43s
pod/diskmaker-manager-tbdsj                   1/1     Running   0          5m43s
pod/local-storage-operator-7db4bd9f79-t6k87   1/1     Running   0          14m

NAME                                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
service/local-storage-operator-metrics   ClusterIP   172.30.135.36   <none>        8383/TCP,8686/TCP   14m

NAME                               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/diskmaker-manager   3         3         3       3            3           <none>          5m43s

NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/local-storage-operator   1/1     1            1           14m

NAME                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/local-storage-operator-7db4bd9f79   1         1         1       14m
----
+
Note the desired and current number of daemon set processes. A desired count of `0` indicates that the label selectors were invalid.

. Verify that the persistent volumes were created:
+
[source,terminal]
----
$ oc get pv
----
+
.Example output
[source,terminal]
----
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
local-pv-1cec77cf   100Gi      RWO            Delete           Available           local-sc                88m
local-pv-2ef7cd2a   100Gi      RWO            Delete           Available           local-sc                82m
local-pv-3fa1c73    100Gi      RWO            Delete           Available           local-sc                48m
----

[IMPORTANT]
====
Editing the `LocalVolume` object does not change the `fsType` or `volumeMode` of existing persistent volumes because doing so might result in a destructive operation.
====
