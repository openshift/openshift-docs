// Module included in the following assemblies:
//
// * migration/migrating_3_4/troubleshooting-3-4.adoc
// * migration/migrating_4_1_4/troubleshooting-4-1-4.adoc
// * migration/migrating_4_2_4/troubleshooting-4-2-4.adoc

[id='migration-known-issues_{context}']
= Known issues

This release has the following known issues:

* During migration, the {mtc-full} ({mtc-short}) preserves the following namespace annotations:

** `openshift.io/sa.scc.mcs`
** `openshift.io/sa.scc.supplemental-groups`
** `openshift.io/sa.scc.uid-range`
+
These annotations preserve the UID range, ensuring that the containers retain their file system permissions on the target cluster. There is a risk that the migrated UIDs could duplicate UIDs within an existing or future namespace on the target cluster. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1748440[*BZ#1748440*])

* If an AWS bucket is added to the {mtc-short} web console and then deleted, its status remains `True` because the `MigStorage` CR is not updated. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1738564[*BZ#1738564*])

* Most cluster-scoped resources are not yet handled by {mtc-short}. If your applications require cluster-scoped resources, you might have to create them manually on the target cluster.

* If a migration fails, the migration plan does not retain custom PV settings for quiesced pods. You must manually roll back the migration, delete the migration plan, and create a new migration plan with your PV settings. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1784899[*BZ#1784899*])

* If a large migration fails because Restic times out, you can increase the `restic_timeout` parameter value (default: `1h`) in the `MigrationController` CR.

* If you select the data verification option for PVs that are migrated with the file system copy method, performance is significantly slower.

ifeval::["{mtc-version}" < "1.4"]
* If you are migrating data from NFS storage and `root_squash` is enabled, `Restic` maps to `nfsnobody`. The migration fails and a permission error is displayed in the `Restic` pod log. You can resolve this issue by creating a supplemental group for Restic. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1873641[*BZ#1873641*])

* If Velero has an invalid `BackupStorageLocation` during start-up, it will crash-loop until the invalid `BackupStorageLocation` is removed. This scenario is triggered by incorrect credentials, a non-existent S3 bucket, and other configuration errors. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1881707[*BZ#1881707*])
endif::[]
