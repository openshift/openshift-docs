// Module included in the following assemblies:
//
// * logging/cluster-logging-external.adoc

[id="cluster-logging-log-forwarding-about_{context}"]
= Understanding the Log Forwarding API

Forwarding cluster logs using the Log Forwarding API requires a combination of _outputs_ and _pipelines_ to send logs to specific endpoints inside and outside of your {product-title} cluster. 

[NOTE]
====
If you want to use only the default internal {product-title} Elasticsearch logstore, do not configure the Log Forwarding feature. 
==== 

By default, the Cluster Logging Operator sends logs to the default internal Elasticsearch logstore, as defined in the `ClusterLogging` custom resource. To use the Log Forwarding feature, you create a custom `logforwarding` configuration file to send logs to endpoints you specify.  

An _output_ is the destination for log data and a _pipeline_ defines simple routing for one source to one or more outputs. 

An output can be either:

* `elasticsearch` to forward logs to an external Elasticsearch v5.x cluster and/or the internal {product-title} Elasticsearch instance. 
* `forward` to forward logs to an external log aggregation solution. This option uses the Fluentd *forward* protocols. 

[NOTE]
====
The endpoint must be a server name or FQDN, not an IP Address, if the cluster-wide proxy using the CIDR annotation is enabled.
====

A _pipeline_ associates the source of the data to an output. The source of the data is one of the following:

* `logs.app` - Container logs generated by user applications running in the cluster, except infrastructure container applications.
* `logs.infra` - Logs generated by infrastructure components running in the cluster and {product-title} nodes, such as journal logs. Infrastructure components are pods that run in the `openshift*`, `kube*`, or `default` projects.
* `logs.audit` - Logs generated by the node audit system (auditd), which are stored in the  */var/log/audit/audit.log* file, and the audit logs from the Kubernetes apiserver and the OpenShift apiserver. 

Note the following:

* The internal {product-title} Elasticsearch instance does not provide secure storage for audit logs. We recommend you ensure that the system to which you forward audit logs is compliant with your organizational and governmental regulations and is properly secured. {product-title} cluster logging does not comply with those regulations.

* An output supports TLS communication using a secret. Secrets must have keys of: *tls.crt*, *tls.key*, and *ca-bundler.crt* which point to the respective certificates for which they represent. Secrets must have the key *shared_key* for use when using forward in a secure manner.

* You are responsible to create and maintain any additional configurations that external destinations might require, such as keys and secrets, service accounts, port opening, or global proxy configuration.

The following example creates three outputs: 

* the internal {product-title} Elasticsearch instance, 
* an unsecured externally-managed Elasticsearch instance, 
* a secured external log aggregator using the *forward* protocols. 

Three pipelines send:

* the application logs to the internal {product-title} Elasticsearch, 
* the infrastructure logs to an external Elasticsearch instance,
* the audit logs to the secured device over the *forward* protocols.

.Sample log forwarding outputs and pipelines
[source,yaml]
----
apiVersion: "logging.openshift.io/v1alpha1"
kind: "LogForwarding"
metadata:
  name: instance <1>
  namespace: openshift-logging
spec:
  disableDefaultForwarding: true <2>
  outputs: <3>
   - name: elasticsearch <4>
     type: "elasticsearch"  <5>
     endpoint: elasticsearch.openshift-logging.svc:9200 <6>
     secret: <7>
        name: fluentd
   - name: elasticsearch-insecure
     type: "elasticsearch"
     endpoint: elasticsearch-insecure.svc.messaging.cluster.local
     insecure: true <8>
   - name: secureforward-offcluster
     type: "forward"
     endpoint: https://secureforward.offcluster.com:24224
     secret:
        name: secureforward
  pipelines: <9>
   - name: container-logs <10>
     inputSource: logs.app <11>
     outputRefs: <12>
     - elasticsearch
     - secureforward-offcluster
   - name: infra-logs
     inputSource: logs.infra
     outputRefs:
     - elasticsearch-insecure
   - name: audit-logs
     inputSource: logs.audit
     outputRefs:
     - secureforward-offcluster
----
<1> The name of the log forwarding CR must be `instance`.
<2> Parameter to disable the default log forwarding behavior.
<3> Configuration for the outputs.
<4> A name to describe the output.
<5> The type of output, either `elasticsearch` or `forward`.
<6> Enter the endpoint, either the server name, FQDN, or IP address. If the cluster-wide proxy using the CIDR annotation is enabled, the endpoint must be a server name or FQDN, not an IP Address. For the internal {product-title} Elasticsearch instance, specify `elasticsearch.openshift-logging.svc:9200`.
<7> Optional name of the secret required by the endpoint for TLS communication. The secret must exist in the `openshift-logging` project.
<8> Optional setting if the endpoint does not use a secret, resulting in insecure communication. 
<9> Configuration for the pipelines.
<10> A name to describe the pipeline.
<11> The data source: `logs.app`, `logs.infra`, or `logs.audit`.
<12> The name of one or more outputs configured in the CR.

