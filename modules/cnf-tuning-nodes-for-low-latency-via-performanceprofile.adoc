// Module included in the following assemblies:
// Epic CNF-78 (4.4)
// Epic CNF-422 (4.5)
// scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc

[IMPORTANT]
====
The feature described in this document is for *Developer Preview* purposes and is *not supported* by Red Hat at this time.
This feature could cause nodes to reboot and not be available.
====

[id="cnf-tuning-nodes-for-low-latency-via-performanceprofile_{context}"]
= Tuning nodes for low latency with the performance profile

The performance profile lets you control latency tuning aspects of nodes that belong to a certain machine config pool. After you specify your settings, the `PerformanceProfile` object is compiled into multiple objects that perform the actual node level tuning:

* A `MachineConfig` file that manipulates the nodes.
* A `KubeletConfig` file that configures the Topology Manager, the CPU Manager, and the {product-title} nodes.
* The Tuned profile that configures the Node Tuning Operator.

.Procedure

. Prepare a cluster.

. Create a machine config pool.

. Install the Performance Addon Operator.

. Create a performance profile that is appropriate for your hardware and topology. In the performance profile, you can specify whether to update the kernel to kernel-rt, allocation of huge pages, the CPUs that will be reserved for operating system housekeeping processes and CPUs that will be used for running the workloads.
+
This is a typical performance profile:
+
[source,yaml]
----
apiVersion: performance.openshift.io/v1
kind: PerformanceProfile
metadata:
 name: performance
spec:
 cpu:
  isolated: "5-15"
  reserved: "0-4"
 hugepages:
  defaultHugepagesSize: "1G"
  pages:
  -size: "1G"
   count: 16
   node: 0
 realTimeKernel:
  enabled: true  <1>
 numa:  <2>
  topologyPolicy: "best-effort"
 nodeSelector:
  node-role.kubernetes.io/worker-cnf: ""
----

<1> Valid values are `true` or `false`. Setting the `true` value installs the real-time kernel on the node.
<2> Use this field to configure the topology manager policy. Valid values are `none` (default), `best-effort`, `restricted`, and `single-numa-node`. For more information, see link:https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/#topology-manager-policies[Topology Manager Policies].

[id="cnf-partitioning-the-cpus_{context}"]
== Partitioning the CPUs

You can reserve cores, or threads, for operating system housekeeping tasks from a single NUMA node and put your workloads on another NUMA node. The reason for this is that the housekeeping processes might be using the CPUs in a way that would impact latency sensitive processes running on those same CPUs. Keeping your workloads on a separate NUMA node prevents the processes from interfering with each other. Additionally, each NUMA node has its own memory bus that is not shared.

Specify two groups of CPUs in the `spec` section:

* `isolated` - Has the lowest latency. Processes in this group have no interruptions and so can, for example, reach much higher DPDK zero packet loss bandwidth.

* `reserved` - The housekeeping CPUs. Threads in the reserved group tend to be very busy, so latency-sensitive applications should be run in the isolated group. See link:https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed[Create a pod that gets assigned a QoS class of `Guaranteed`].
