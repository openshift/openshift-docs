// Module included in the following assemblies:
// Epic CNF-78
// scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc

[IMPORTANT]
====
The feature described in this document is for *Developer Preview* purposes and is *not supported* by Red Hat at this time.
This feature could cause nodes to reboot and not come up.
====

[id="cnf-tuning-nodes-for-low-latency-via-performanceprofile_{context}"]
= Tuning nodes for low latency via PerformanceProfile

The PerformanceProfile lets you control latency tuning aspects of nodes that belong to a certain MachineConfigPool.
After you have specified your settings, the `PerformanceProfile` object is compiled into multiple objects that perform the actual node level tuning:

* A `MachineConfig` file that manipulates the nodes.
* A `KubeletConfig` file that configures the Topology Manager, the CPU Manager, and the {product-title} nodes.
* The Tuned profile that configures the Node Tuning Operator.

.Procedure

. Prepare a cluster.

. Create a Machine Config Pool. See xref:../nodes/nodes/nodes-nodes-managing.adoc#nodes-nodes-managing[Managing nodes].

. Install the Performance Addon Operator.
See xref:./modules/cnf-installing-the-performance-addon-operator.adoc[Installing the Performance Addon Operator].

. Create a performance profile that is appropriate for your hardware and topology.
In the PerformanceProfile, you can specify whether to update the kernel to kernel-rt, allocation of hugepages, the CPUs that
will be reserved for housekeeping, and CPUs that will be used for running the workloads.
+
This is a typical performance profile:
+
----
apiversion: performance.openshift.io/v1alpha1
kind: PerformanceProfile
metadata:
    name: <unique-name>
spec:
   cpu:
       isolated: "1-3"
       reserved: "0"
   hugepages: <1>
      defaultHugepagesSize: "1Gi"
      pages:
      - size:  "1Gi"
        count:  4
        node:  0
realTimeKernel:
      enabled: true  <2>
   numa:
       topologyPolicy: "best-effort"
   nodeSelector:
         node-role.kubernetes.io/worker-cnf: ""
----

<1> See xref:../scalability_and_performance/cnf-configuring-huge-pages[Configuring huge pages].

<2> Valid values are `true` or `false`. Setting the `true` value installs the real-time kernel on the node.

== Partitioning the CPUs

You can reserve cores (threads) for housekeeping tasks from a single NUMA node and put your workloads on another NUMA node.
The reason for this is that the housekeeping processes may be using the CPUs in a way that would impact latency sensitive processes
running on those same CPUs.
Keeping your workloads on a separate NUMA node prevents the processes from interfering with each other.
Additionally, each NUMA node has its own memory bus that is not shared.

Specify two groups of CPUs in the `spec` section:
+
`isolated` - Has the lowest latency. Processes in this group have no interruptions and so can, for example,
reach much higher DPDK zero packet loss bandwidth.
+
`reserved` - The housekeeping CPUs. Threads in the reserved group tend to be very busy, so latency-sensitive
applications should be run in the isolated group.
See link:https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed[Create a Pod that gets assigned a QoS class of `Guaranteed`].

.Additional resources

For more information about MachineConfig and KubeletConfig,
see xref:../nodes/nodes/nodes-nodes-managing.adoc#nodes-nodes-managing[Managing nodes].

For more information about the Node Tuning Operator,
see xref:../scalability_and_performance/using-node-tuning-operator.adoc#using-node-tuning-operator[Using the Node Tuning Operator].
