// Module included in the following assemblies:
//
// * scalability_and_performance/recommended-host-practices.adoc
// * post_installation_configuration/node-tasks.adoc
// * post_installation_configuration/machine-configuration-tasks.adoc

:_content-type: PROCEDURE
[id="create-a-kubeletconfig-crd-to-edit-kubelet-parameters_{context}"]
= Creating a KubeletConfig CRD to edit kubelet parameters

The kubelet configuration is currently serialized as an Ignition configuration, so it can be directly edited. However, there is also a new
`kubelet-config-controller` added to the Machine Config Controller (MCC). This allows you to create a `KubeletConfig` custom resource (CR) to edit the kubelet parameters.

[NOTE]
====
As the fields in the `kubeletConfig` object are passed directly to the kubelet from upstream Kubernetes, the kubelet validates those values directly. Invalid values in the `kubeletConfig` object might cause cluster nodes to become unavailable. For valid values, see the link:https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/[Kubernetes documentation].
====

.Procedure

. View the available machine configuration objects that you can select:
+
[source,terminal]
----
$ oc get machineconfig
----
+
By default, the two kubelet-related configs are `01-master-kubelet` and `01-worker-kubelet`.

. To check the current value of max pods per node, run:
+
[source,terminal]
----
# oc describe node <node-ip> | grep Allocatable -A6
----
+
Look for `value: pods: <value>`.
+
For example:
+
[source,terminal]
----
# oc describe node ip-172-31-128-158.us-east-2.compute.internal | grep Allocatable -A6
----
+
.Example output
[source,terminal]
----
Allocatable:
 attachable-volumes-aws-ebs:  25
 cpu:                         3500m
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      15341844Ki
 pods:                        250
----

. To set the max pods per node on the worker nodes, create a custom resource file that contains the kubelet configuration. For example, `change-maxPods-cr.yaml`:
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: large-pods
  kubeletConfig:
    maxPods: 500
----
+
The rate at which the kubelet talks to the API server depends on queries per second (QPS) and burst values. The default values, `50` for `kubeAPIQPS` and `100` for `kubeAPIBurst`, are good enough if there are limited pods running on each node. Updating the kubelet QPS and burst rates is recommended if there are enough CPU and memory resources on the node:
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: large-pods
  kubeletConfig:
    maxPods: <pod_count>
    kubeAPIBurst: <burst_rate>
    kubeAPIQPS: <QPS>
----

.. Update the machine config pool for workers with the label:
+
[source,terminal]
----
$ oc label machineconfigpool worker custom-kubelet=large-pods
----

.. Create the `KubeletConfig` object:
+
[source,terminal]
----
$ oc create -f change-maxPods-cr.yaml
----

.. Verify that the `KubeletConfig` object is created:
+
[source,terminal]
----
$ oc get kubeletconfig
----
+
This should return `set-max-pods`.
+
Depending on the number of worker nodes in the cluster, wait for the worker nodes to be rebooted one by one. For a cluster with 3 worker nodes, this could take about 10 to 15 minutes.

. Check for `maxPods` changing for the worker nodes:
+
[source,terminal]
----
$ oc describe node
----

.. Verify the change by running:
+
[source,terminal]
----
$ oc get kubeletconfigs set-max-pods -o yaml
----
+
This should show a status of `True` and `type:Success`

.Procedure

By default, only one machine is allowed to be unavailable when applying the
kubelet-related configuration to the available worker nodes. For a large
cluster, it can take a long time for the configuration change to be reflected.
At any time, you can adjust the number of machines that are updating to speed up
the process.

. Edit the `worker` machine config pool:
+
[source,terminal]
----
$ oc edit machineconfigpool worker
----

. Set `maxUnavailable` to the desired value.
+
[source,yaml]
----
spec:
  maxUnavailable: <node_count>
----
+
[IMPORTANT]
====
When setting the value, consider the number of worker nodes that can be
unavailable without affecting the applications running on the cluster.
====
