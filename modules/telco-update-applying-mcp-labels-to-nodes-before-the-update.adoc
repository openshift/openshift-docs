// Module included in the following assemblies:
//
// * edge_computing/day_2_core_cnf_clusters/updating/telco-update-ocp-update-prep.adoc

:_mod-docs-content-type: PROCEDURE
[id="telco-update-applying-mcp-labels-to-nodes-before-the-update_{context}"]
= Applying MachineConfigPool labels to nodes before the update

Prepare `MachineConfigPool` (`mcp`) node labels to group nodes together in groups of roughly 8 to 10 nodes.
With `mcp` groups, you can reboot groups of nodes independently from the rest of the cluster.

You use the `mcp` node labels to pause and unpause the set of nodes during the update process so that you can do the update and reboot at a time of your choosing.

[id="telco-update-staggering-the-cluster-update_{context}"]
== Staggering the cluster update

Sometimes there are problems during the update.
Often the problem is related to hardware failure or nodes needing to be reset.
Using `mcp` node labels, you can update nodes in stages by pausing the update at critical moments, tracking paused and unpaused nodes as you proceed.
When a problem occurs, you use the nodes that are in an unpaused state to ensure that there are enough nodes running to keep all applications pods running.

[id="telco-update-dividing-worker-nodes-into-mcp-groups_{context}"]
== Dividing worker nodes into MachineConfigPool groups

How you divide worker nodes into `mcp` groups can vary depending on how many nodes are in the cluster or how many nodes you assign to a node role.
By default the 2 roles in a cluster are control plane and worker.

In clusters that run telco workloads, you can further split the worker nodes between CNF control plane and CNF data plane roles.
Add `mcp` role labels that split the worker nodes into each of these two groups.

[NOTE]
====
Larger clusters can have as many as 100 worker nodes in the CNF control plane role.
No matter how many nodes there are in the cluster, keep each `MachineConfigPool` group to around 10 nodes.
This allows you to control how many nodes are taken down at a time.
With multiple `MachineConfigPool` groups, you can unpause several groups at a time to accelerate the update, or separate the update over 2 or more maintenance windows.
====

Example cluster with 15 worker nodes::
Consider a cluster with 15 worker nodes:

* 10 worker nodes are CNF control plane nodes.
* 5 worker nodes are CNF data plane nodes.

+
Split the CNF control plane and data plane worker node roles into at least 2 `mcp` groups each.
Having 2 `mcp` groups per role means that you can have one set of nodes that are not affected by the update.

Example cluster with 6 worker nodes::
Consider a cluster with 6 worker nodes:

* Split the worker nodes into 3 `mcp` groups of 2 nodes each.

+
Upgrade one of the `mcp` groups.
Allow the updated nodes to sit through a day to allow for verification of CNF compatibility before completing the update on the other 4 nodes.

[IMPORTANT]
====
The process and pace at which you unpause the `mcp` groups is determined by your CNF applications and configuration.

If your CNF pod can handle being scheduled across nodes in a cluster, you can unpause several `mcp` groups at a time and set the `MaxUnavailable` in the `mcp` custom resource (CR) to as high as 50%. This allows up to half of the nodes in an `mcp` group to restart and get updated.
====
