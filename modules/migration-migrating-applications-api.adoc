// Module included in the following assemblies:
//
// * migration/migrating_3_4/migrating-applications-with-cam-3-4.adoc
// * migration/migrating_4_1_4/migrating-applications-with-cam-4-1-4.adoc
// * migration/migrating_4_2_4/migrating-applications-with-cam-4-2-4.adoc

[id='migration-migrating-applications-api_{context}']
= Migrating your applications with the {mtc-full} API

You can migrate your applications on the command line with the {mtc-full} ({mtc-short}) API.

You can migrate applications from a local cluster to a remote cluster, from a remote cluster to a local cluster, and between remote clusters.

This procedure describes how to perform indirect migration and direct migration:

* Indirect migration: Images, volumes, and Kubernetes objects are copied from the source cluster to the replication repository and then from the replication repository to the destination cluster.
* Direct migration: Images or volumes are copied directly from the source cluster to the destination cluster. Direct image migration and direct volume migration have significant performance benefits.

You create the following custom resources (CRs) to perform a migration:

* `MigCluster` CR: Defines a `host`, local, or remote cluster
+
The `migration-controller` pod runs on the `host` cluster.

* `Secret` CR: Contains credentials for a remote cluster or storage
* `MigStorage` CR: Defines a replication repository
+
Different storage providers require different parameters in the `MigStorage` CR manifest.

* `MigPlan` CR: Defines a migration plan
* `MigMigration` CR: Performs a migration defined in an associated `MigPlan`
+
You can create multiple `MigMigration` CRs for a single `MigPlan` CR for the following purposes:
+
* To perform stage migrations, which copy most of the data without stopping the application, before running a migration. Stage migrations improve the performance of the migration.
* To cancel a migration in progress
* To roll back a completed migration

.Prerequisites

* You must have `cluster-admin` privileges for all clusters.
* You must install the {product-title} CLI (`oc`).
* You must install the {mtc-full} Operator on all clusters.
* The _version_ of the installed {mtc-full} Operator must be the same on all clusters.
* You must configure an object storage as a replication repository.
* If you are using direct image migration, you must expose a secure registry route on all remote clusters.
* If you are using direct volume migration, the source cluster must not have an HTTP proxy configured.

.Procedure

. Create a `MigCluster` CR manifest for the `host` cluster called `host-cluster.yaml`:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigCluster
metadata:
  name: host
  namespace: openshift-migration
spec:
  isHostCluster: true
----

. Create a `MigCluster` CR for the `host` cluster:
+
[source,terminal]
----
$ oc create -f host-cluster.yaml -n openshift-migration
----

. Create a `Secret` CR manifest for each remote cluster called `cluster-secret.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: <cluster_secret>
  namespace: openshift-config
type: Opaque
data:
  saToken: <sa_token> <1>
----
<1> Specify the base64-encoded `migration-controller` service account (SA) token of the remote cluster.
+
You can obtain the SA token by running the following command:
+
[source,terminal]
----
$ oc sa get-token migration-controller -n openshift-migration | base64 -w 0
----

. Create a `Secret` CR for each remote cluster:
+
[source,terminal]
----
$ oc create -f cluster-secret.yaml
----

. Create a `MigCluster` CR manifest for each remote cluster called `remote-cluster.yaml`:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigCluster
metadata:
  name: <remote_cluster>
  namespace: openshift-migration
spec:
  exposedRegistryPath: <exposed_registry_route> <1>
  insecure: false <2>
  isHostCluster: false
  serviceAccountSecretRef:
    name: <remote_cluster_secret> <3>
    namespace: openshift-config
  url: <remote_cluster_url> <4>
----
<1> Optional: Specify the exposed registry route, for example, `docker-registry-default.apps.example.com` if you are using direct image migration.
<2> SSL verification is enabled if `false`. CA certificates are not required or checked if `true`.
<3> Specify the `Secret` CR of the remote cluster.
<4> Specify the URL of the remote cluster.

. Create a `MigCluster` CR for each remote cluster:
+
[source,terminal]
----
$ oc create -f remote-cluster.yaml -n openshift-migration
----

. Verify that all clusters are in a `Ready` state:
+
[source,terminal]
----
$ oc describe cluster <cluster_name>
----

. Create a `Secret` CR manifest for the replication repository called `storage-secret.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  namespace: openshift-config
  name: <migstorage_creds>
type: Opaque
data:
  aws-access-key-id: <key_id_base64> <1>
  aws-secret-access-key: <secret_key_base64> <2>
----
<1> Specify the key ID in base64 format.
<2> Specify the secret key in base64 format.
+
AWS credentials are base64-encoded by default. If you are using another storage provider, you must encode your credentials by running the following command with each key:
+
[source,terminal]
----
$ echo -n "<key>" | base64 -w 0 <1>
----
<1> Specify the key ID or the secret key. Both keys must be base64-encoded.

. Create the `Secret` CR for the replication repository:
+
[source,terminal]
----
$ oc create -f storage-secret.yaml
----

. Create a `MigStorage` CR manifest for the replication repository called `migstorage.yaml`:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigStorage
metadata:
  name: <storage_name>
  namespace: openshift-migration
spec:
  backupStorageConfig:
    awsBucketName: <bucket_name> <1>
    credsSecretRef:
      name: <storage_secret_ref> <2>
      namespace: openshift-config
  backupStorageProvider: <storage_provider_name> <3>
  volumeSnapshotConfig:
    credsSecretRef:
      name: <storage_secret_ref> <4>
      namespace: openshift-config
  volumeSnapshotProvider: <storage_provider_name> <5>
----
<1> Specify the bucket name.
<2> Specify the `Secrets` CR of the object storage. You must ensure that the credentials stored in the `Secrets` CR of the object storage are correct.
<3> Specify the storage provider.
<4> Optional: If you are copying data by using snapshots, specify the `Secrets` CR of the object storage. You must ensure that the credentials stored in the `Secrets` CR of the object storage are correct.
<5> Optional: If you are copying data by using snapshots, specify the storage provider.

. Create the `MigStorage` CR:
+
[source,terminal]
----
$ oc create -f migstorage.yaml -n openshift-migration
----

. Verify that the `MigStorage` CR is in a `Ready` state:
+
[source,terminal]
----
$ oc describe migstorage <migstorage_name>
----

. Create a `MigPlan` CR manifest called `migplan.yaml`:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: <migration_plan>
  namespace: openshift-migration
spec:
  destMigClusterRef:
    name: host
    namespace: openshift-migration
  indirectImageMigration: true <1>
  indirectVolumeMigration: true <2>
  migStorageRef:
    name: <migstorage_ref> <3>
    namespace: openshift-migration
  namespaces:
    - <application_namespace> <4>
  srcMigClusterRef:
    name: <remote_cluster_ref> <5>
    namespace: openshift-migration
----
<1> Direct image migration is enabled if `false`.
<2> Direct volume migration is enabled if `false`.
<3> Specify the name of the `MigStorage` CR instance.
<4> Specify one or more namespaces to be migrated.
<5> Specify the name of the source cluster `MigCluster` instance.

. Create the `MigPlan` CR:
+
[source,terminal]
----
$ oc create -f migplan.yaml -n openshift-migration
----

. View the `MigPlan` instance to verify that it is in a `Ready` state:
+
[source,terminal]
----
$ oc describe migplan <migplan_name> -n openshift-migration
----

. Create a `MigMigration` CR manifest called `migmigration.yaml`:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigMigration
metadata:
  name: <migmigration_name>
  namespace: openshift-migration
spec:
  migPlanRef:
    name: <migplan_name> <1>
    namespace: openshift-migration
  quiescePods: true <2>
  stage: false <3>
  rollback: false <4>
----
<1> Specify the `MigPlan` CR name.
<2> The pods on the source cluster are stopped before migration if `true`.
<3> A stage migration, which copies most of the data without stopping the application, is performed if `true`.
<4> A completed migration is rolled back if `true`.

. Create the `MigMigration` CR to start the migration defined in the `MigPlan` CR:
+
[source,terminal]
----
$ oc create -f migmigration.yaml -n openshift-migration
----

. Verify the progress of the migration by watching the `MigMigration` CR:
+
[source,terminal]
----
$ oc watch migmigration <migmigration_name> -n openshift-migration
----
+
The output resembles the following:
+
.Example output
+
[source,yaml]
----
Name:         c8b034c0-6567-11eb-9a4f-0bc004db0fbc
Namespace:    openshift-migration
Labels:       migration.openshift.io/migplan-name=django
Annotations:  openshift.io/touch: e99f9083-6567-11eb-8420-0a580a81020c
API Version:  migration.openshift.io/v1alpha1
Kind:         MigMigration
...
Spec:
  Mig Plan Ref:
    Name:       my_application
    Namespace:  openshift-migration
  Stage:        false
Status:
  Conditions:
    Category:              Advisory
    Last Transition Time:  2021-02-02T15:04:09Z
    Message:               Step: 19/47
    Reason:                InitialBackupCreated
    Status:                True
    Type:                  Running
    Category:              Required
    Last Transition Time:  2021-02-02T15:03:19Z
    Message:               The migration is ready.
    Status:                True
    Type:                  Ready
    Category:              Required
    Durable:               true
    Last Transition Time:  2021-02-02T15:04:05Z
    Message:               The migration registries are healthy.
    Status:                True
    Type:                  RegistriesHealthy
  Itinerary:               Final
  Observed Digest:         7fae9d21f15979c71ddc7dd075cb97061895caac5b936d92fae967019ab616d5
  Phase:                   InitialBackupCreated
  Pipeline:
    Completed:  2021-02-02T15:04:07Z
    Message:    Completed
    Name:       Prepare
    Started:    2021-02-02T15:03:18Z
    Message:    Waiting for initial Velero backup to complete.
    Name:       Backup
    Phase:      InitialBackupCreated
    Progress:
      Backup openshift-migration/c8b034c0-6567-11eb-9a4f-0bc004db0fbc-wpc44: 0 out of estimated total of 0 objects backed up (5s)
    Started:        2021-02-02T15:04:07Z
    Message:        Not started
    Name:           StageBackup
    Message:        Not started
    Name:           StageRestore
    Message:        Not started
    Name:           DirectImage
    Message:        Not started
    Name:           DirectVolume
    Message:        Not started
    Name:           Restore
    Message:        Not started
    Name:           Cleanup
  Start Timestamp:  2021-02-02T15:03:18Z
Events:
  Type    Reason   Age                 From                     Message
  ----    ------   ----                ----                     -------
  Normal  Running  57s                 migmigration_controller  Step: 2/47
  Normal  Running  57s                 migmigration_controller  Step: 3/47
  Normal  Running  57s (x3 over 57s)   migmigration_controller  Step: 4/47
  Normal  Running  54s                 migmigration_controller  Step: 5/47
  Normal  Running  54s                 migmigration_controller  Step: 6/47
  Normal  Running  52s (x2 over 53s)   migmigration_controller  Step: 7/47
  Normal  Running  51s (x2 over 51s)   migmigration_controller  Step: 8/47
  Normal  Ready    50s (x12 over 57s)  migmigration_controller  The migration is ready.
  Normal  Running  50s                 migmigration_controller  Step: 9/47
  Normal  Running  50s                 migmigration_controller  Step: 10/47
----
