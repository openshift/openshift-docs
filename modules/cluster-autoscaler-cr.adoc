// Module included in the following assemblies:
//
// * machine_management/compute_machine_management/applying-autoscaling.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: REFERENCE
[id="cluster-autoscaler-cr_{context}"]
= Cluster autoscaler resource definition

This `ClusterAutoscaler` resource definition shows the parameters and sample values for the cluster autoscaler.

[source,yaml]
----
apiVersion: "autoscaling.openshift.io/v1"
kind: "ClusterAutoscaler"
metadata:
  name: "default"
spec:
  podPriorityThreshold: -10 # <1>
  resourceLimits:
    maxNodesTotal: 24 # <2>
    cores:
      min: 8 # <3>
      max: 128 # <4>
    memory:
      min: 4 # <5>
      max: 256 # <6>
    gpus:
      - type: nvidia.com/gpu # <7>
        min: 0 # <8>
        max: 16 # <9>
      - type: amd.com/gpu
        min: 0
        max: 4
  logVerbosity: 4 # <10>
  scaleDown: # <11>
    enabled: true # <12>
    delayAfterAdd: 10m # <13>
    delayAfterDelete: 5m # <14>
    delayAfterFailure: 30s # <15>
    unneededTime: 5m # <16>
    utilizationThreshold: "0.4" # <17>
  expanders: ["Random"] # <18>
----
<1> Specify the priority that a pod must exceed to cause the cluster autoscaler to deploy additional nodes. Enter a 32-bit integer value. The `podPriorityThreshold` value is compared to the value of the `PriorityClass` that you assign to each pod.
<2> Specify the maximum number of nodes to deploy. This value is the total number of machines that are deployed in your cluster, not just the ones that the autoscaler controls. Ensure that this value is large enough to account for all of your control plane and compute machines and the total number of replicas that you specify in your `MachineAutoscaler` resources.
<3> Specify the minimum number of cores to deploy in the cluster.
<4> Specify the maximum number of cores to deploy in the cluster.
<5> Specify the minimum amount of memory, in GiB, in the cluster.
<6> Specify the maximum amount of memory, in GiB, in the cluster.
<7> Optional: Specify the type of GPU node to deploy. Only `nvidia.com/gpu` and `amd.com/gpu` are valid types.
<8> Specify the minimum number of GPUs to deploy in the cluster.
<9> Specify the maximum number of GPUs to deploy in the cluster.
<10> Specify the logging verbosity level between `0` and `10`. The following log level thresholds are provided for guidance:
+
--
* `1`: (Default) Basic information about changes.
* `4`: Debug-level verbosity for troubleshooting typical issues.
* `9`: Extensive, protocol-level debugging information.
--
+
If you do not specify a value, the default value of `1` is used.
<11> In this section, you can specify the period to wait for each action by using any valid link:https://golang.org/pkg/time/#ParseDuration[ParseDuration] interval, including `ns`, `us`, `ms`, `s`, `m`, and `h`.
<12> Specify whether the cluster autoscaler can remove unnecessary nodes.
<13> Optional: Specify the period to wait before deleting a node after a node has recently been _added_. If you do not specify a value, the default value of `10m` is used.
<14> Optional: Specify the period to wait before deleting a node after a node has recently been _deleted_. If you do not specify a value, the default value of `0s` is used.
<15> Optional: Specify the period to wait before deleting a node after a scale down failure occurred. If you do not specify a value, the default value of `3m` is used.
<16> Optional: Specify a period of time before an unnecessary node is eligible for deletion. If you do not specify a value, the default value of `10m` is used.
<17> Optional:  Specify the _node utilization level_. Nodes below this utilization level are eligible for deletion.
+
The node utilization level is the sum of the requested resources divided by the allocated resources for the node, and must be a value greater than `"0"` but less than `"1"`. If you do not specify a value, the cluster autoscaler uses a default value of `"0.5"`, which corresponds to 50% utilization. You must express this value as a string.
<18> Optional: Specify any expanders that you want the cluster autoscaler to use.
The following values are valid:
+
--
* `LeastWaste`: Selects the machine set that minimizes the idle CPU after scaling.
If multiple machine sets would yield the same amount of idle CPU, the selection minimizes unused memory.
* `Priority`: Selects the machine set with the highest user-assigned priority.
To use this expander, you must create a config map that defines the priority of your machine sets.
For more information, see "Configuring a priority expander for the cluster autoscaler."
* `Random`: (Default) Selects the machine set randomly.
--
+
If you do not specify a value, the default value of `Random` is used.
+
You can specify multiple expanders by using the `[LeastWaste, Priority]` format.
The cluster autoscaler applies each expander according to the specified order.
+
In the `[LeastWaste, Priority]` example, the cluster autoscaler first evaluates according to the `LeastWaste` criteria.
If more than one machine set satisfies the `LeastWaste` criteria equally well, the cluster autoscaler then evaluates according to the `Priority` criteria.
If more than one machine set satisfies all of the specified expanders equally well, the cluster autoscaler selects one to use at random.

[NOTE]
====
When performing a scaling operation, the cluster autoscaler remains within the ranges set in the `ClusterAutoscaler` resource definition, such as the minimum and maximum number of cores to deploy or the amount of memory in the cluster. However, the cluster autoscaler does not correct the current values in your cluster to be within those ranges.

The minimum and maximum CPUs, memory, and GPU values are determined by calculating those resources on all nodes in the cluster, even if the cluster autoscaler does not manage the nodes. For example, the control plane nodes are considered in the total memory in the cluster, even though the cluster autoscaler does not manage the control plane nodes.
====
