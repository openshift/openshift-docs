// Module included in the following assemblies:
//
// * machine_management/applying-autoscaling.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_content-type: REFERENCE
[id="cluster-autoscaler-cr_{context}"]
= ClusterAutoscaler resource definition

This `ClusterAutoscaler` resource definition shows the parameters and sample values for the cluster autoscaler.


[source,yaml]
----
apiVersion: "autoscaling.openshift.io/v1"
kind: "ClusterAutoscaler"
metadata:
  name: "default"
spec:
  podPriorityThreshold: -10 <1>
  resourceLimits:
    maxNodesTotal: 24 <2>
    cores:
      min: 8 <3>
      max: 128 <4>
    memory:
      min: 4 <5>
      max: 256 <6>
    gpus:
      - type: nvidia.com/gpu <7>
        min: 0 <8>
        max: 16 <9>
      - type: amd.com/gpu
        min: 0
        max: 4
  scaleDown: <10>
    enabled: true <11>
    delayAfterAdd: 10m <12>
    delayAfterDelete: 5m <13>
    delayAfterFailure: 30s <14>
    unneededTime: 5m <15>
    utilizationThreshold: "0.4" <16>
----
<1> Specify the priority that a pod must exceed to cause the cluster autoscaler to deploy additional nodes. Enter a 32-bit integer value. The `podPriorityThreshold` value is compared to the value of the `PriorityClass` that you assign to each pod.
<2> Specify the maximum number of nodes to deploy. This value is the total number of machines that are deployed in your cluster, not just the ones that the autoscaler controls. Ensure that this value is large enough to account for all of your control plane and compute machines and the total number of replicas that you specify in your `MachineAutoscaler` resources.
<3> Specify the minimum number of cores to deploy in the cluster.
<4> Specify the maximum number of cores to deploy in the cluster.
<5> Specify the minimum amount of memory, in GiB, in the cluster.
<6> Specify the maximum amount of memory, in GiB, in the cluster.
<7> Optional: Specify the type of GPU node to deploy. Only `nvidia.com/gpu` and `amd.com/gpu` are valid types.
<8> Specify the minimum number of GPUs to deploy in the cluster.
<9> Specify the maximum number of GPUs to deploy in the cluster.
<10> In this section, you can specify the period to wait for each action by using any valid link:https://golang.org/pkg/time/#ParseDuration[ParseDuration] interval, including `ns`, `us`, `ms`, `s`, `m`, and `h`.
<11> Specify whether the cluster autoscaler can remove unnecessary nodes.
<12> Optional: Specify the period to wait before deleting a node after a node has recently been _added_. If you do not specify a value, the default value of `10m` is used.
<13> Optional: Specify the period to wait before deleting a node after a node has recently been _deleted_. If you do not specify a value, the default value of `0s` is used.
<14> Optional: Specify the period to wait before deleting a node after a scale down failure occurred. If you do not specify a value, the default value of `3m` is used.
<15> Optional: Specify the period before an unnecessary node is eligible for deletion. If you do not specify a value, the default value of `10m` is used.
<16> Optional: Specify the _node utilization level_ below which an unnecessary node is eligible for deletion. The node utilization level is the sum of the requested resources divided by the allocated resources for the node, and must be a value greater than `"0"` but less than `"1"`. If you do not specify a value, the cluster autoscaler uses a default value of `"0.5"`, which corresponds to 50% utilization. This value must be expressed as a string.
// Might be able to add a formula to show this visually, but need to look into asciidoc math formatting and what our tooling supports.

[NOTE]
====
When performing a scaling operation, the cluster autoscaler remains within the ranges set in the `ClusterAutoscaler` resource definition, such as the minimum and maximum number of cores to deploy or the amount of memory in the cluster. However, the cluster autoscaler does not correct the current values in your cluster to be within those ranges.

The minimum and maximum CPUs, memory, and GPU values are determined by calculating those resources on all nodes in the cluster, even if the cluster autoscaler does not manage the nodes. For example, the control plane nodes are considered in the total memory in the cluster, even though the cluster autoscaler does not manage the control plane nodes.
====
