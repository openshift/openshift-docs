// Module included in the following assemblies:
//
// * observability/otel/otel-troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="exposing-metrics_{context}"]
= Exposing the metrics

The OpenTelemetry Collector exposes the following metrics about the data volumes it has processed:

`otelcol_receiver_accepted_spans`:: The number of spans successfully pushed into the pipeline.
`otelcol_receiver_refused_spans`:: The number of spans that could not be pushed into the pipeline.
`otelcol_exporter_sent_spans`:: The number of spans successfully sent to the destination.
`otelcol_exporter_enqueue_failed_spans`:: The number of spans failed to be added to the sending queue.
`otelcol_receiver_accepted_logs`:: The number of logs successfully pushed into the pipeline.
`otelcol_receiver_refused_logs`:: The number of logs that could not be pushed into the pipeline.
`otelcol_exporter_sent_logs`:: The number of logs successfully sent to the destination.
`otelcol_exporter_enqueue_failed_logs`:: The number of logs failed to be added to the sending queue.
`otelcol_receiver_accepted_metrics`:: The number of metrics successfully pushed into the pipeline.
`otelcol_receiver_refused_metrics`:: The number of metrics that could not be pushed into the pipeline.
`otelcol_exporter_sent_metrics`:: The number of metrics successfully sent to the destination.
`otelcol_exporter_enqueue_failed_metrics`:: The number of metrics failed to be added to the sending queue.

You can use these metrics to troubleshoot issues with your Collector. For example, if the `otelcol_receiver_refused_spans` metric has a high value, it indicates that the Collector is not able to process incoming spans.

The Operator creates a `<cr_name>-collector-monitoring` telemetry service that you can use to scrape the metrics endpoint.

.Procedure

. Enable the telemetry service by adding the following lines in the `OpenTelemetryCollector` custom resource (CR):

+
[source,yaml]
----
# ...
  config:
    service:
      telemetry:
        metrics:
          readers:
          - pull:
              exporter:
                prometheus:
                  host: 0.0.0.0
                  port: 8888 # <1>
# ...
----
<1> The port at which the internal collector metrics are exposed. Defaults to `:8888`.

. Retrieve the metrics by running the following command, which uses the port-forwarding Collector pod:
+
[source,terminal]
----
$ oc port-forward <collector_pod>
----

. In the `OpenTelemetryCollector` CR, set the `enableMetrics` field to `true` to scrape internal metrics:
+
[source,yaml]
----
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
spec:
# ...
  mode: deployment
  observability:
    metrics:
      enableMetrics: true
# ...
----
+
Depending on the deployment mode of the OpenTelemetry Collector, the internal metrics are scraped by using `PodMonitors` or `ServiceMonitors`.
+
[NOTE]
====
Alternatively, if you do not set the `enableMetrics` field to `true`, you can access the metrics endpoint at `+http://localhost:8888/metrics+`.
====

. Optional: If the *User Workload Monitoring* feature is enabled in the web console, go to *Observe* -> *Dashboards* in the web console, and then select the *OpenTelemetry Collector* dashboard from the drop-down list to view it. For more information about the *User Workload Monitoring* feature, see "Enabling monitoring for user-defined projects" in _Monitoring_.
+
[TIP]
====
You can filter the visualized data such as spans or metrics by the Collector instance, namespace, or OpenTelemetry components such as processors, receivers, or exporters.
====
