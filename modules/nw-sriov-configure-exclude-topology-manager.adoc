// Module included in the following assemblies:
//
// * networking/hardware_networks/configuring-sriov-device.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-sriov-configure-exclude-topology-manager_{context}"]
= Excluding the SR-IOV network topology for NUMA-aware scheduling

To exclude advertising the SR-IOV network resource's Non-Uniform Memory Access (NUMA) node to the Topology Manager, you can configure the `excludeTopology` specification in the `SriovNetworkNodePolicy` custom resource. Use this configuration for more flexible SR-IOV network deployments during NUMA-aware pod scheduling.

.Prerequisites

* You have installed the OpenShift CLI (`oc`).
* You have configured the CPU Manager policy to `static`. For more information about CPU Manager, see the _Additional resources_ section.
* You have configured the Topology Manager policy to `single-numa-node`.
* You have installed the SR-IOV Network Operator.

.Procedure

. Create the `SriovNetworkNodePolicy` CR:

.. Save the following YAML in the `sriov-network-node-policy.yaml` file, replacing values in the YAML to match your environment:
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: <policy_name>
  namespace: openshift-sriov-network-operator
spec:
  resourceName: sriovnuma0 <1>
  nodeSelector:
    kubernetes.io/hostname: <node_name>
  numVfs: <number_of_Vfs>
  nicSelector: <2>
    vendor: "<vendor_ID>"
    deviceID: "<device_ID>"
  deviceType: netdevice
  excludeTopology: true <3>
----
<1> The resource name of the SR-IOV network device plugin. This YAML uses a sample `resourceName` value.
<2> Identify the device for the Operator to configure by using the NIC selector.
<3> To exclude advertising the NUMA node for the SR-IOV network resource to the Topology Manager, set the value to `true`. The default value is `false`.
+
[NOTE]
====
If multiple `SriovNetworkNodePolicy` resources target the same SR-IOV network resource, the `SriovNetworkNodePolicy` resources must have the same value as the `excludeTopology` specification. Otherwise, the conflicting policy is rejected.
====

.. Create the `SriovNetworkNodePolicy` resource by running the following command:
+
[source,terminal]
----
$ oc create -f sriov-network-node-policy.yaml
----
+
.Example output
[source,terminal]
----
sriovnetworknodepolicy.sriovnetwork.openshift.io/policy-for-numa-0 created
----

. Create the `SriovNetwork` CR:

.. Save the following YAML in the `sriov-network.yaml` file, replacing values in the YAML to match your environment:
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: sriov-numa-0-network <1>
  namespace: openshift-sriov-network-operator
spec:
  resourceName: sriovnuma0 <2>
  networkNamespace: <namespace> <3>
  ipam: |- <4>
    {
      "type": "<ipam_type>",
    }
----
<1> Replace `sriov-numa-0-network` with the name for the SR-IOV network resource.
<2> Specify the resource name for the `SriovNetworkNodePolicy` CR from the previous step. This YAML uses a sample `resourceName` value.
<3> Enter the namespace for your SR-IOV network resource.
<4> Enter the IP address management configuration for the SR-IOV network.

.. Create the `SriovNetwork` resource by running the following command:
+
[source,terminal]
----
$ oc create -f sriov-network.yaml
----
+
.Example output
[source,terminal]
----
sriovnetwork.sriovnetwork.openshift.io/sriov-numa-0-network created
----

. Create a pod and assign the SR-IOV network resource from the previous step:

.. Save the following YAML in the `sriov-network-pod.yaml` file, replacing values in the YAML to match your environment:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: <pod_name>
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "sriov-numa-0-network", <1>
        }
      ]
spec:
  containers:
  - name: <container_name>
    image: <image>
    imagePullPolicy: IfNotPresent
    command: ["sleep", "infinity"]
----
<1> This is the name of the `SriovNetwork` resource that uses the `SriovNetworkNodePolicy` resource.

.. Create the `Pod` resource by running the following command:
+
[source,terminal]
----
$ oc create -f sriov-network-pod.yaml
----
+
.Example output
[source,terminal]
----
pod/example-pod created
----

.Verification

. Verify the status of the pod by running the following command, replacing `<pod_name>` with the name of the pod:
+
[source,terminal]
----
$ oc get pod <pod_name>
----
+
.Example output
[source,terminal]
----
NAME                                     READY   STATUS    RESTARTS   AGE
test-deployment-sriov-76cbbf4756-k9v72   1/1     Running   0          45h
----

. Open a debug session with the target pod to verify that the SR-IOV network resources are deployed to a different node than the memory and CPU resources.

.. Open a debug session with the pod by running the following command, replacing <pod_name> with the target pod name.
+
[source,terminal]
----
$ oc debug pod/<pod_name>
----

..  Set `/host` as the root directory within the debug shell. The debug pod mounts the root file system from the host in `/host` within the pod. By changing the root directory to `/host`, you can run binaries from the host file system:
+
[source,terminal]
----
$ chroot /host
----

.. View information about the CPU allocation by running the following commands:
+
[source,terminal]
----
$ lscpu | grep NUMA
----
+
.Example output
[source,terminal]
----
NUMA node(s):                    2
NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,...
NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,...
----
+
[source,terminal]
----
$ cat /proc/self/status | grep Cpus
----
+
.Example output
[source,terminal]
----
Cpus_allowed:	aa
Cpus_allowed_list:	1,3,5,7
----
+
[source,terminal]
----
$ cat  /sys/class/net/net1/device/numa_node
----
+
.Example output
[source,terminal]
----
0
----
+
In this example, CPUs 1,3,5, and 7 are allocated to `NUMA node1` but the SR-IOV network resource can use the NIC in `NUMA node0`.

[NOTE]
====
If the `excludeTopology` specification is set to `True`, it is possible that the required resources exist in the same NUMA node.
====
