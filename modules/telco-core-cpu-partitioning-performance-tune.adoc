// Module included in the following assemblies:
//
// * scalability_and_performance/telco_ref_design_specs/core/telco-core-ref-design-components.adoc

:_mod-docs-content-type: REFERENCE
[id="telco-core-cpu-partitioning-performance-tune_{context}"]
= CPU partitioning and performance tuning

New in this release::
* No reference design updates in this release

Description::
CPU partitioning allows for the separation of sensitive workloads from generic purposes, auxiliary processes, interrupts, and driver work queues to achieve improved performance and latency.

Limits and requirements::
* The operating system needs a certain amount of CPU to perform all the support tasks including kernel networking.
** A system with just user plane networking applications (DPDK) needs at least one Core (2 hyperthreads when enabled) reserved for the operating system and the infrastructure components.
* A system with Hyper-Threading enabled must always put all core sibling threads to the same pool of CPUs.
* The set of reserved and isolated cores must include all CPU cores.
* Core 0 of each NUMA node must be included in the reserved CPU set.
* Isolated cores might be impacted by interrupts. The following annotations must be attached to the pod if guaranteed QoS pods require full use of the CPU:
+
----
cpu-load-balancing.crio.io: "disable"
cpu-quota.crio.io: "disable"
irq-load-balancing.crio.io: "disable"
----
* When per-pod power management is enabled with `PerformanceProfile.workloadHints.perPodPowerManagement` the following annotations must also be attached to the pod if guaranteed QoS pods require full use of the CPU:
+
----
cpu-c-states.crio.io: "disable"
cpu-freq-governor.crio.io: "performance"
----

Engineering considerations::
* The minimum reserved capacity (`systemReserved`) required can be found by following the guidance in  link:https://access.redhat.com/solutions/5843241["Which amount of CPU and memory are recommended to reserve for the system in OpenShift 4 nodes?"]
* The actual required reserved CPU capacity depends on the cluster configuration and workload attributes.
* This reserved CPU value must be rounded up to a full core (2 hyper-thread) alignment.
* Changes to the CPU partitioning will drain and reboot the nodes in the MCP.
* The reserved CPUs reduce the pod density, as the reserved CPUs are removed from the allocatable capacity of the OpenShift node.
* The real-time workload hint should be enabled if the workload is real-time capable.
* Hardware without Interrupt Request (IRQ) affinity support will impact isolated CPUs. To ensure that pods with guaranteed CPU QoS have full use of allocated CPU, all hardware in the server must support IRQ affinity.
* OVS dynamically manages its `cpuset` configuration to adapt to network traffic needs.
You do not need to reserve additional CPUs for handling high network throughput on the primary CNI.
* If workloads running on the cluster require cgroups v1, you can configure nodes to use cgroups v1 as part of the initial cluster deployment.
For more information, see "Enabling Linux cgroup v1 during installation".
