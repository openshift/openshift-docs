// Module included in the following assemblies:
//
// * virt/logging_events_monitoring/virt-events.html/virt-virtualization-alerts.adoc
:_content-type: REFERENCE
[id="virt-cnv-virt-alerts_{context}"]
= Virt alerts

Virt alerts provide information about problems for the {VirtProductName} Virt Operator.

//KubeVirtComponentExceedsRequestedCPU Alert
[id="KubeVirtComponentExceedsRequestedCPU_{context}"]
== KubeVirtComponentExceedsRequestedCPU alert

.Description

This alert occurs when a container uses more CPU than it requested.

.Reason

If this alert consistently occurs, then the usage of the node’s CPU resources is not optimal, resulting in potential overload.

.Troubleshoot

. Set the environment variable `NAMESPACE`.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----

. Check to see what the CPU resource limit is.
+
[source,terminal]
----
$ oc -n $NAMESPACE get deployment <name-of-resource-firing-alert> -o yaml | grep requests: -A 2
----

. Check actual resource usage using promQL.
+
[source,terminal]
----
node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate{namespace="$NAMESPACE",container="<name-of-resource-firing-alert>"}
----

.Resolution

After checking the actual resource usage, determine what a better resource request is for the resource and update it using the `customizeComponents` option on the custom resource (CR).
[source,yaml]
----
  spec:
  customizeComponents:
    patches:
    - type:
      resourceName: <name-of-resource-firing-alert>
      resourceType: <Deployment|DaemonSet>
      type: strategic
      patch: '{"spec":{"template":{"spec":{"containers":[{"name":"< name-of-resource-firing-alert >","resources":{"requests":{"cpu":" < new-CPU-request > "}}}]}}}}'
----

//KubeVirtComponentExceedsRequestedMemory Alert
[id="KubeVirtComponentExceedsRequestedMemory_{context}"]
== KubeVirtComponentExceedsRequestedMemory alert

.Description

This alert occurs when a container uses more memory than it requested.

.Reason

If this alert consistently occurs, the usage of the node’s memory resources is not optimal, resulting in potential overload.

.Troubleshoot

. Set the environment variable `NAMESPACE`.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----

. Check to see what the CPU resource limit is.
+
[source,terminal]
----
$ oc -n $NAMESPACE get deployment <name-of-resource-firing-alert> -o yaml | grep requests: -A 2
----

. Check actual resource usage using promQL.
+
[source,terminal]
----
container_memory_usage_bytes{namespace="$NAMESPACE",container="<name-of-resource-firing-alert>"}
----

.Resolution

After checking the actual resource usage, determine what a better resource request is for the resource and update it using the `customizeComponents` option on the CR.
[source,yaml]
----
  spec:
  customizeComponents:
    patches:
    - type:
      resourceName: < name-of-resource-firing-alert >
      resourceType: < Deployment|DaemonSet >
      type: strategic
      patch: '{"spec":{"template":{"spec":{"containers":[{"name":"< name-of-resource-firing-alert >","resources":{"requests":{"memory":" < new-memory-request > "}}}]}}}}'
----

//KubevirtVmHighMemoryUsageBasedOnRequests
[id="KubevirtVmHighMemoryUsageBasedOnRequests_{context}"]
== KubevirtVmHighMemoryUsageBasedOnRequests alert

.Description

The container hosting a virtual machine has less than 20 MB free memory and is close to its requested memory.

.Reason

The scheduler considers the memory request when scheduling a container to a node. Then, the schedules allocates the requested memory on the chosen node for use by the container.

If exhaustion of a node’s memory occurs, then the system prioritizes evicting containers whose memory usage most exceeds their memory request. In serious cases of memory exhaustion, the node Out of Memory (OOM) killer selects and stops the virtual machine in the container, based on a similar metric.

.Troubleshoot

. Check the `compute` container memory resource request and limit.
+
[source,terminal]
----
$ oc get pod <virt launcher pod name> -o yaml
----
Look for the container name `compute`.

. Identify processes with high memory usage.
+
[source,terminal]
----
$ oc exec -it <virt launcher pod name> -c compute -- top
----

.Resolution

Consider changing the container memory limit.

Memory resource requests and limits are set in the virtual machine's manifest. For example:

[source,yaml]
----
spec:
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm-name
    spec:
      domain:
        resources:
          limits:
            memory: 200Mi
          requests:
            memory: 128Mi
...
----

//KubevirtVmHighMemoryUsage Alert
[id="KubevirtVmHighMemoryUsage_{context}"]
== KubevirtVmHighMemoryUsage alert

.Description

The container hosting a virtual machine has less than 20 MB of free memory and is close to its memory limit.

.Reason

When the container memory usage exceeds the memory limit, the runtime stops the virtual machine.

.Troubleshoot

. Check the `compute` container memory resource request and limit.
+
[source,terminal]
----
$ oc get pod <virt launcher pod name> -o yaml
----
Look for the container name `compute`.

. Identify processes with high memory usage.
+
[source,terminal]
----
$ oc exec -it <virt launcher pod name> -c compute -- top
----

.Resolution

Consider changing container memory limit.

Memory resource requests and limits are set in the virtual machine's manifest. For example:

[source,yaml]
----
spec:
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm-name
    spec:
      domain:
        resources:
          limits:
            memory: 200Mi
          requests:
            memory: 128Mi
...
----

//LowKVMNodesCount Alert
[id="LowKVMNodesCount_{context}"]
== LowKVMNodesCount alert

.Description

Virtual machine migration requires at least two nodes with a Kernel-based virtual machine (KVM) resource. This alert occurs if there are less than two nodes with a KVM resource available.

.Reason

You cannot schedule and run a VM if there are no nodes with a KVM resource available.

You cannot migrate a VM if less than two nodes in the cluster have a KVM resource available.

.Troubleshoot

. Verify that nodes have a KVM resource available.
+
[source,terminal]
----
$ oc get nodes -o jsonpath='{.items[*].status.allocatable}' | grep devices.kubevirt.io/kvm
----

.Resolution

. Validate hardware virtualization support using `virt-host-validate` to ensure that your hosts are capable of running virtualization workloads:
+
[source,terminal]
----
virt-host-validate qemu
----

//LowReadyVirtControllersCount Alert
[id="LowReadyVirtControllersCount_{context}"]
== LowReadyVirtControllersCount alert

.Description

Virt-controller is responsible for monitoring virtual machine instances (VMIs) and managing the associated pods by creating and managing the lifecycle of the pods associated with the VMI objects.

A VMI object is always associated with a pod during its lifetime. However, the pod instance might change over time because of VMI migration.

.Reason

If some virt-controllers are running but not ready in the past five minutes, then the virt-controller becomes a single point of failure.

.Troubleshoot

. Modify the environment variable `NAMESPACE`.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----

. Run this command:
+
[source,terminal]
----
$ oc get deployment -n $NAMESPACE virt-controller -o jsonpath='{.status.readyReplicas}'
----

. Check the status of the virt-controller deployment to find out more information.
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-controller -o yaml
----
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deploy virt-controller
----

. Check if there are issues with the nodes, such as if the nodes are in a `NotReady` state.
+
[source,terminal]
----
$ oc get nodes
----

.Resolution

Open a support issue and provide the information gathered in the troubleshooting process.

//LowReadyVirtOperatorsCount Alert
[id="LowReadyVirtOperatorsCount_{context}"]
== LowReadyVirtOperatorsCount alert

.Description

Some virt Operators are running but not in the `Ready` state in the past 10 minutes. The virt-operator deployment has a default replica of two pods.

.Reason

The virt-operator is the first Kubernetes Operator active in an OpenShift cluster. Its primary responsibilities are:

* Installation
* Live-update
* Live-upgrade of a cluster
* Monitoring the lifecycle of top-level controllers such as virt-controller, virt-handler, and virt-launcher
* Managing the reconciliation of top-level controllers

In addition, the virt-operator is responsible for cluster-wide tasks such as certificate rotation and some infrastructure management.

[NOTE]
====
Virt-operator is not directly responsible for virtual machines in the cluster. Virt-operator's unavailability does not affect the custom workloads.
====

If this alert occurs and the NoReadyVirtOperator alert does not occur, then the virt-operator becomes a single point of failure.

.Troubleshoot

. Check the status of the virt-operator deployment to learn more information. These commands provide the associated events and show if there are any specific issues:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-operator -o yaml
----
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deploy virt-operator
----

. Check if there are issues with the nodes for control-plane and masters such as if they are in a `NotReady` state.
+
[source,terminal]
----
$ oc get nodes
----

.Resolution

There are several reasons for a low number of virt-operator pods in a `Ready` state. Identify the root cause and take appropriate action.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//LowVirtAPICount Alert
[id="LowVirtAPICount_{context}"]
== LowVirtAPICount alert

.Description

This alert occurs if only one virt-api pod is available in a 60 minute period, despite at least two worker nodes available for scheduling.

.Reason

The virt-api pod becomes a single point of failure that can lead to an API calls outage if the pod fails.

.Troubleshoot

. Modify the environment variable `NAMESPACE`.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----

. Run this command:
+
[source,terminal]
----
$ oc get deployment -n $NAMESPACE virt-api -o jsonpath='{.status.readyReplicas}'
----

. Check the status of the virt-api deployment. Use these commands to learn about related events and show if there are any issues with pulling an image, crashing pod, or other similar problems.
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-api -o yaml
----
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deploy virt-api
----

. Check if there are issues with the nodes, such as if the nodes are in a `NotReady` state.
+
[source,terminal]
----
$ oc get nodes
----

.Resolution

Open a support issue and provide the information gathered in the troubleshooting process.

//LowVirtControllersCount Alert
[id="LowVirtControllersCount_{context}"]
== LowVirtControllersCount alert

.Description

More than one virt-controller pod must be ready to ensure high availability. The current default number of replicas is two.

.Reason

If the virt-controller fails, then VM lifecycle management, such as launching a new VM instance or shutting down an existing VM instance, completely fails.

.Troubleshoot

. Modify the environment variable `NAMESPACE`.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----

. Check the status of the virt-controller deployment.
+
[source,terminal]
----
# oc get deployment -n $NAMESPACE virt-controller -o yaml
----

. Check if there are any running virt-controller pods.
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-controller
----

. Check the virt-controller pods that are not ready or crashing.
+
[source,terminal]
----
$ oc -n $NAMESPACE logs virt-launcher-<unique id>
----
+
[source,terminal]
----
oc -n $NAMESPACE describe pod/virt-launcher-<unique id>
----

.Resolution

There are several reasons for a low number of virt-controller pods. Identify the root cause and take appropriate action.

* Not enough memory on the cluster
* Nodes are down
* The API server overloads, such as when the scheduler is not 100% available
* Networking issues

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//LowVirtOperatorCount Alert
[id="LowVirtOperatorCount_{context}"]
== LowVirtOperatorCount alert

.Description

There is only one virt-operator pod running in `Ready` state in the past 60 minutes.

.Reason

The virt-operator is the first Kubernetes Operator active in an OpenShift cluster. Its primary responsibilities are:

* Installation
* Live-update
* Live-upgrade of a cluster
* Monitoring the lifecycle of top-level controllers such as virt-controller, virt-handler, and virt-launcher
* Managing the reconciliation of top-level controllers

In addition, the virt-operator is responsible for cluster-wide tasks such as certificate rotation and some infrastructure management.

[NOTE]
====
Virt-operator is not directly responsible for virtual machines in the cluster. Virt-operator's unavailability does not affect the custom workloads.
====

.Troubleshoot

. Check the states of virt-operator pods.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-operator
----

. Check in-depth virt-operator pods that are in trouble.
+
[source,terminal]
----
$ oc -n $NAMESPACE logs <pod-name>
----
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pod <pod-name>
----

.Resolution

There can be several reasons for a low virt-operator count. Identify the root cause and take appropriate action.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//NoLeadingVirtOperator Alert
[id="NoLeadingVirtOperator_{context}"]
== NoLeadingVirtOperator alert

.Description

In the past 10 minutes, no virt-operator pod holds the leader lease, despite one or more virt-operator pods being in `Ready` state. The alert suggests no operating virt-operator pod exists.

.Reason

The virt-operator is the first Kubernetes Operator active in a {product-title} cluster. Its primary responsibilities are:

* Installation
* Live-update
* Live-upgrade of a cluster
* Monitoring the lifecycle of top-level controllers such as virt-controller, virt-handler, and virt-launcher
* Managing the reconciliation of top-level controllers

In addition, the virt-operator is responsible for cluster-wide tasks such as certificate rotation and some infrastructure management.

The virt-operator deployment has a default replica of two pods with one leader pod holding a leader lease, indicating an operating virt-operator pod.

This alert indicates a failure at the cluster level. Critical cluster-wide management functionalities such as certification rotation, upgrade, and reconciliation of controllers may be temporarily unavailable.

.Troubleshoot

Determine a virt-operator pod's leader status from the pod logs. The log messages containing `Started leading` and `acquire leader` indicate the leader status of a given virt-operator pod.

Additionally, always check if there are any running virt-operator pods and the pods' statuses with these commands:
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-operator
----
[source,terminal]
----
$ oc -n $NAMESPACE logs <pod-name>
----
[source,terminal]
----
$ oc -n $NAMESPACE describe pod <pod-name>
----

*Leader pod example:*
[source,terminal]
----
$ oc -n $NAMESPACE logs <pod-name> |grep lead
----
.Example output
[source,terminal]
----
{"component":"virt-operator","level":"info","msg":"Attempting to acquire leader status","pos":"application.go:400","timestamp":"2021-11-30T12:15:18.635387Z"}
I1130 12:15:18.635452       1 leaderelection.go:243] attempting to acquire leader lease <namespace>/virt-operator...
I1130 12:15:19.216582       1 leaderelection.go:253] successfully acquired lease <namespace>/virt-operator
----
[source,terminal]
----
{"component":"virt-operator","level":"info","msg":"Started leading","pos":"application.go:385","timestamp":"2021-11-30T12:15:19.216836Z"}
----

*Non-leader pod example:*
[source,terminal]
----
$ oc -n $NAMESPACE logs <pod-name> |grep lead
----
.Example output
[source,terminal]
----
{"component":"virt-operator","level":"info","msg":"Attempting to acquire leader status","pos":"application.go:400","timestamp":"2021-11-30T12:15:20.533696Z"}
I1130 12:15:20.533792       1 leaderelection.go:243] attempting to acquire leader lease <namespace>/virt-operator...
----

.Resolution

There are several reasons for no virt-operator pod holding the leader lease, despite one or more virt-operator pods being in `Ready` state. Identify the root cause and take appropriate action.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//NoReadyVirtController Alert
[id="NoReadyVirtController_{context}"]
== NoReadyVirtController alert

.Description

The virt-controller monitors virtual machine instances (VMIs). The virt-controller also manages the associated pods by creating and managing the lifecycle of the pods associated with the VMI objects.

A VMI object always associates with a pod during its lifetime. However, the pod instance can change over time because of VMI migration.

This alert occurs when detection of no ready virt-controllers occurs for five minutes.

.Reason

If the virt-controller fails, then VM lifecycle management completely fails. Lifecycle management tasks include launching a new VMI or shutting down an existing VMI.

.Troubleshoot

. Check the vdeployment status of the virt-controller for available replicas and conditions.
+
[source,terminal]
----
$ oc -n $NAMESPACE get deployment virt-controller -o yaml
----

. Check if the virt-controller pods exist and check their statuses.
+
[source,terminal]
----
$ get pods -n $NAMESPACE |grep virt-controller
----

. Check the virt-controller pods' events.
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pods <virt-controller pod>
----

. Check the virt-controller pods' logs.
+
[source,terminal]
----
$ oc -n $NAMESPACE logs <virt-controller pod>
----

. Check if there are issues with the nodes, such as if the nodes are in a `NotReady` state.
+
[source,terminal]
----
$ oc get nodes
----

.Resolution

There are several reasons for no virt-controller pods being in a `Ready` state. Identify the root cause and take appropriate action.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//NoReadyVirtOperator Alert
[id="NoReadyVirtOperator_{context}"]
== NoReadyVirtOperator alert

.Description

No detection of a virt-operator pod in the `Ready` state occurs in the past 10 minutes. The virt-operator deployment has a default replica of two pods.

.Reason

The virt-operator is the first Kubernetes Operator active in an {product-title} cluster. Its primary responsibilities are:

* Installation
* Live-update
* Live-upgrade of a cluster
* Monitoring the lifecycle of top-level controllers such as virt-controller, virt-handler, and virt-launcher
* Managing the reconciliation of top-level controllers

In addition, the virt-operator is responsible for cluster-wide tasks such as certificate rotation and some infrastructure management.

[NOTE]
====
Virt-operator is not directly responsible for virtual machines in the cluster. Virt-operator's unavailability does not affect the custom workloads.
====

This alert indicates a failure at the cluster level. Critical cluster-wide management functionalities such as certification rotation, upgrade, and reconciliation of controllers are temporarily unavailable.

.Troubleshoot

. Check the deployment status of the virt-operator for available replicas and conditions.
+
[source,terminal]
----
$ oc -n $NAMESPACE get deployment virt-operator -o yaml
----

. Check the virt-controller pods' events.
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pods <virt-operator pod>
----

. Check the virt-operator pods' logs.
+
[source,terminal]
----
$ oc -n $NAMESPACE logs <virt-operator pod>
----

. Check if there are issues with the nodes for the control plane and masters, such as if they are in a `NotReady` state.
+
[source,terminal]
----
$ oc get nodes
----

.Resolution

There are several reasons for no virt-operator pods being in a `Ready` state. Identify the root cause and take appropriate action.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//OrphanedVirtualMachineInstances Alert
[id="OrphanedVirtualMachineInstances_{context}"]
== OrphanedVirtualMachineInstances alert

.Description

A virtual machine instance (VMI) such as a `virt-launcher` pod is running on a node that does not have a running `virt-handler` pod.

.Reason

If a node does not have a running `virt-handler`, then any VMI running on that node is an orphan and no longer manageable.

.Troubleshoot

. Confirm the alert by finding which nodes your virt-handler pods are running on using this command:
+
[source,terminal]
----
$ oc get pods --all-namespaces -o wide -l kubevirt.io=virt-handler
----
+
Output:
+
[source,terminal]
----
NAME                 READY   STATUS    RESTARTS   AGE  IP               NODE     NOMINATED NODE   READINESS GATES
virt-handler-vhqsp   1/1     Running   0          4h   10.244.140.80    node02   <none>           <none>
virt-handler-xd8jc   1/1     Running   0          4h   10.244.196.168   node01   <none>           <none>
----

. Check to see on which nodes the VMIs are running. Any VMI running on a node that a `virt-handler` pod does not exist on is an orphan. Use this command:
+
[source,terminal]
----
$ oc get vmis --all-namespaces
----
+
Output:
+
[source,terminal]
----
NAMESPACE   NAME            AGE   PHASE       IP    NODENAME
default     vmi-ephemeral   4s    Scheduled         node02
----

. Check to see if the DaemonSet that controls the `virt-handler` pods is healthy using this command:
+
[source,terminal]
----
$ oc get daemonset virt-handler --all-namespaces
----
+
Output:
+
[source,terminal]
----
NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
virt-handler            2         2         2       2            2           kubernetes.io/os=linux   4h
----

The DaemonSet is healthy if the Desired, Ready and Available columns contain the same number.

*Unhealthy virt-handler DaemonSet*

. Check the DaemonSet's status to determine what issues occur when deploying the pods.
+
[source,terminal]
----
$ oc describe daemonset virt-handler --all-namespaces
----
+
. Check the status by determining the object and reading through its status object.
+
[source,terminal]
----
$ oc get daemonset virt-handler --all-namespaces -o yaml | jq .status
----
+
. Check the health of the cluster nodes.
+
[source,terminal]
----
$ oc get nodes
----

*Healthy virt-handler DaemonSet*

. Verify if there is a `workloads` placement policy on the resource in the `spec.workloads` field.
+
[source,terminal]
----
$ oc get kubevirt kubevirt --all-namespaces -o yaml
----

.Resolution

If there is a placement policy, then you can make adjustments so that the node that is running the VMI is included in the placement policy.

There may also be a change to a node’s taints and tolerations or a pod’s scheduling rules.

//OutdatedVirtualMachineInstanceWorkloads Alert
[id="OutdatedVirtualMachineInstanceWorkloads_{context}"]
== OutdatedVirtualMachineInstanceWorkloads alert

.Description

There are VMIs running in outdated virt-launcher pods 24 hours after the OpenShift control plane update finishes.

.Reason

Non-updated VMIs trying to run in the most recent virt-launcher pod do not have access to new features and do not have any security fixes associated with the virt-launcher pod update.

.Troubleshoot

You can identify the outdated VMIs by using the `kubevirt.io/outdatedLauncherImage` label as a label selector when listing VMIs. An example of a command that lists all out-of-date VMIs across all namespaces within the cluster is:
[source,terminal]
----
$ oc get vmi -l kubevirt.io/outdatedLauncherImage --all-namespaces
----

.Resolution

*Check for enabling automatic workload updates*

Check the CR used to install KubeVirt to see if the configuration of the CR spec's `workloadUpdateStrategy` attribute is correct.

If you use automatic workload updates, then workloads that are able to be live migrated can always migrate. If workloads are not able to be live migrated, eviction of those workloads occurs, causing a restart when using `RunStrategy: Always` on the corresponding VM definition. An example is:
[source,yaml]
----
apiVersion: kubevirt.io/v1
kind: KubeVirt
metadata:
  name: kubevirt
  namespace: kubevirt
spec:
  imagePullPolicy: IfNotPresent
  workloadUpdateStrategy:
    workloadUpdateMethods:
      - LiveMigrate
      - Evict
    batchEvictSize: 10
    batchEvictInterval: "1m"
----

If you do not enable automatic workload updates, consider enabling them. Enabling automatic workload updates causes the OpenShift control plane to automatically update the VMI workloads using the methods defined in the `workloadUpdateMethods` field.

*Enabled automatic workload updates, but VMIs are still out-of-date*

Identify the VMIs that are out of date. Check each affected VMI to see if the VMIs are able to be live migrated or not by looking for the `LiveMigratable` condition within the VMI’s status.

If a VMI is not able to be live migrated and the eviction method is not chosen as a `workloadUpdateMethods` value on the CR, then you must stop the VMI. If a corresponding VM controls the VMI with `RunStrategy: Always` set, then a new VMI immediately starts in an updated virt-launcher pod to replace the stopped VMI.

If the VMI is able to be live migrated and the migration fails, then you can still stop the VMI. However, the failure interrupts the workload.

To stop a VM named `my-vm` in namespace `my-namespace` using `virtctl`, use this command:
[source,terminal]
----
$ virtctl stop --namespace my-namespace my-vm
----

*Manually updating VMIs*

To manually update VMIs, either manually create migration objects for VMIs (non-destructive) that can be live migrated or manually stop VMIs (destructive) that cannot be live migrated.

With live migration, the VMIs migrate into an updated virt-launcher container.

If you stop (and potentially restart when a VM controls the VMI), any replacement VMIs start in updated virt-launcher containers.

You can manually execute a live migration by posting a `VirtualMachineInstanceMigration` object to the cluster that targets a specific running VM. For example, you can create a VM called `my-vm` that runs in the namespace `my-namespace`.
[source,yaml]
----
cat << EOF > migration.yaml
apiVersion: kubevirt.io/v1
kind: VirtualMachineInstanceMigration
metadata:
  name: my-vm-migration-job
  namespace: my-namespace
spec:
  vmiName: my-vm
EOF

$ oc create -f migration.yaml
----

//VMCannotBeEvicted Alert
[id="VMCannotBeEvicted_{context}"]
== VMCannotBeEvicted alert

.Description

This alert occurs when a VM's eviction strategy is set to `LiveMigration` but the VM cannot be live migrated.

.Reason

VMs that cannot be live migrated block node eviction and affect operations such as node drain and updates.

.Troubleshoot

. Check the eviction strategy and the `Migratable` status of the VMI.

. Use the `oc get vmis -o yaml` command. Search for the `evictionStrategy` field. For example, `evictionStrategy: LiveMigrate`.

. Use the `oc get vmis -o wide` command. Examine the `LIVE-MIGRATABLE` column of the output. In case the status is `False`, you can inspect the VMI to understand why you cannot migrate the VM.
+
. Use the `oc get vmis -o yaml` command. Inspect the conditions section under the VMI status. For example:
+
[source,yaml]
----
  status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: null
    message: cannot migrate VMI which does not use masquerade to connect to the pod network
    reason: InterfaceNotLiveMigratable
    status: "False"
    type: LiveMigratable
----

.Resolution

To resolve this alert, you can either:

. Set the `evictionStrategy` to Shutdown.

. Determine why a VM cannot be live migrated and if changing disk type or network configuration alters the migration status.

//VirtAPIDown Alert
[id="VirtAPIDown_{context}"]
== VirtAPIDown alert

.Description

All {product-title} API servers are down.

.Reason

If all {product-title} API servers are down, then no API calls for {product-title} entities occur.

.Troubleshoot

. Modify the environment variable `NAMESPACE`.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----

. Verify if there are any running virt-api pods.
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-api
----

. View the pods' logs using `oc logs` and the pods' statuses using `oc describe`.

. Check the status of the virt-api deployment. Use these commands to learn about related events and show if there are any issues with pulling an image, a crashing pod, or other similar problems.
+
[source,terminal]
----
$ oc -n $NAMESPACE get deployment virt-api -o yaml
----
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deployment virt-api
----

. Check if there are issues with the nodes, such as if the nodes are in a `NotReady` state.
+
[source,terminal]
----
$ oc get nodes
----

.Resolution

Virt-api pods can be down for several reasons. Identify the root cause and take appropriate action.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//VirtApiRESTErrorsBurst Alert
[id="VirtApiRESTErrorsBurst_{context}"]
== VirtApiRESTErrorsBurst alert

.Description

More than 80% of the REST calls fail in virt-api in the last five minutes.

.Reason

A very high rate of failed REST calls to virt-api causes slow response, slow execution of API calls, or even complete dismissal of API calls.

.Troubleshoot

. Modify the environment variable `NAMESPACE`.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----

. Check to see how many running virt-api pods exist.
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-api
----

. View the pods' logs using `oc logs` and the pods' statuses using `oc describe`.

. Check the status of the virt-api deployment to find out more information. These commands provide the associated events and show if there are any issues with pulling an image or a crashing pod.
+
[source,terminal]
----
$ oc -n $NAMESPACE get deployment virt-api -o yaml
----
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deployment virt-api
----

. Check if there are issues with the nodes, such as if the nodes are overloaded or not in a `NotReady` state.
+
[source,terminal]
----
$ oc get nodes
----

.Resolution

There are several reasons for a high rate of failed REST calls. Identify the root cause and take appropriate action.

* Node resource exhaustion
* Not enough memory on the cluster
* Nodes are down
* The API server overloads, such as when the scheduler is not 100% available)
* Networking issues

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//VirtApiRESTErrorsHigh Alert
[id="VirtApiRESTErrorsHigh_{context}"]
== VirtApiRESTErrorsHigh alert

.Description

More than 5% of the REST calls failed in virt-api for the last 60 minutes.

.Reason

A high rate of failed REST calls to virt-api causes slow response and slow execution of API calls.

.Troubleshoot

. Modify the environment variable `NAMESPACE`.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----

. Check to see how many running virt-api pods exist.
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-api
----

. View the pods' logs using `oc logs` and pod status using `oc describe`.

. Check the status of the virt-api deployment to find out more information. These commands provide the associated events and show if there are any issues with pulling an image or a crashing pod.
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-api -o yaml
----
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deploy virt-api
----

. Check if there are issues with the nodes, such as if the nodes are in a `NotReady` state.
+
[source,terminal]
----
$ oc get nodes
----

.Resolution

Virt-api pods fail for several reasons. Identify the root cause and take appropriate action.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//VirtControllerDown Alert
[id="VirtControllerDown_{context}"]
== VirtControllerDown alert

.Description

If no detection of virt-controllers occurs in the past five minutes, then virt-controller deployment has a default replica of two pods.

.Reason

If the virt-controller fails, then VM lifecycle management tasks, such as launching a new VMI or shutting down an existing VMI, completely fail.

.Troubleshoot

. Modify the environment variable `NAMESPACE`.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----

. Check the status of the virt-controller deployment.
+
[source,terminal]
----
$ oc get deployment -n $NAMESPACE virt-controller -o yaml
----

. Check the virt-controller pods' events.
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pods <virt-controller pod>
----

. Check the virt-controller pods' logs.
+
[source,terminal]
----
$ oc -n $NAMESPACE logs <virt-controller pod>
----

. Check the manager pod's logs to determine why creating the virt-controller pods fails.
+
[source,terminal]
----
$ oc get logs <virt-controller-pod>
----

An example of a virt-controller pod name in the logs is `virt-controller-7888c64d66-dzc9p`. However, there may be several pods that run virt-controller.

.Resolution

There are several known reasons why the detection of no running virt-controller occurs. Identify the root cause from the list of possible reasons and take appropriate action.

* Node resource exhaustion
* Not enough memory on the cluster
* Nodes are down
* The API server overloads, such as when the scheduler is not 100% available)
* Networking issues

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//VirtControllerRESTErrorsBurst Alert
[id="VirtControllerRESTErrorsBurst_{context}"]
== VirtControllerRESTErrorsBurst alert

.Description

More than 80% of the REST calls failed in virt-controller in the last five minutes.

.Reason

Virt-controller has potentially fully lost connectivity to the API server. This loss does not affect running workloads, but propagation of status updates and actions like migrations cannot occur.

.Troubleshoot

There are two common error types associated with virt-controller REST call failure:

* The API server overloads, causing timeouts. Check the API server metrics and details like response times and overall calls.

* The virt-controller pod cannot reach the API server. Common causes are:
** DNS issues on the node
** Networking connectivity issues

.Resolution

Check the virt-controller logs to determine if the virt-controller pod cannot connect to the API server at all. If so, delete the pod to force a restart.

Additionally, verify if node resource exhaustion or not having enough memory on the cluster is causing the connection failure.

The issue normally relates to DNS or CNI issues outside of the scope of this alert.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//VirtControllerRESTErrorsHigh Alert
[id="VirtControllerRESTErrorsHigh_{context}"]
== VirtControllerRESTErrorsHigh alert

.Description

More than 5% of the REST calls failed in virt-controller for the last 60 minutes.

.Reason

Virt-controller partially loses the connection to the API server. Delay of cluster-level related actions such as starting, migrating, and scheduling VMs occurs. This delay does not affect running workloads, but delay of reporting the workloads' current status occurs.

.Troubleshoot

There are two common error types associated with virt-controller REST call failure:

* The API server overloads, causing timeouts. Check the API server metrics and details like response times and overall calls.

* The virt-controller pod cannot reach the API server. Common causes are:
** DNS issues on the node
** Networking connectivity issues

.Resolution

Check the virt-controller logs to determine if it cannot connect to the API server at all. If so, delete the pod to force a restart. The issue normally relates to DNS or CNI issues outside of the scope of this alert.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//VirtHandlerDaemonSetRolloutFailing Alert
[id="VirtHandlerDaemonSetRolloutFailing_{context}"]
== VirtHandlerDaemonSetRolloutFailing alert

.Description

Some virt-handler DaemonSets fail to roll out after 15 minutes. This alert suggests that, in the cluster, at least one worker node does not have the virt-handler DaemonSet pod successfully rolls out in the given time.

.Reason

This alert does not indicate the failure of rollouts of all virt-handler DaemonSet. The normal VM lifecycle has no problems if no overload of the cluster.

.Troubleshoot

You can identify the nodes associated with the failed rollouts to show that at least one worker node does not have a virt-handler pod running:

. List all the pods in the virt-handler DaemonSet.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----
+
[source,terminal]
----
$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-handler
----

. Determine the name of the worker node that the pod deploys on for each virt-handler pod.
+
[source,terminal]
----
$ oc -n $NAMESPACE get pod <virt-handler-pod-name> -o jsonpath='{.spec.nodeName}'
----

.Resolution

A common reason for this alert is that the nodes associated with the failed rollouts run out of resources. For example, you can delete some non-DaemonSet pods from the affected nodes.

//VirtHandlerRESTErrorsBurst Alert
[id="VirtHandlerRESTErrorsBurst_{context}"]
== VirtHandlerRESTErrorsBurst alert

.Description

More than 80% of the REST calls failed in virt-handler in the last five minutes.

.Reason

Virt-handler lost the connection to the API server. Running workloads on the affected node still run, but status updates cannot propagate and actions such as migrations cannot occur.

.Troubleshoot

There are two common error types associated with virt-operator REST call failure:

* The API server overloads, causing timeouts. Check the API server metrics and details like response times and overall calls.

* The virt-operator pod cannot reach the API server. Common causes are:
** DNS issues on the node
** Networking connectivity issues

.Resolution

If the virt-handler cannot connect to the API server, delete the pod to force a restart. The issue normally relates to DNS or CNI issues outside of the scope of this alert. Identify the root cause and take appropriate action.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//VirtOperatorRESTErrorsHigh Alert
[id="VirtHandlerRESTErrorsHigh_{context}"]
== VirtHandlerRESTErrorsHigh alert

.Description

More than 5% of the REST calls failed in virt-handler for the last 60 minutes.

.Reason

Virt-handler has partially lost the connection to the API server. Delay of node-related actions such as starting and migrating workloads occurs. This delay does not affect running workloads, but delay of reporting the workloads' current status occurs.

.Troubleshoot

There are two common error types associated with virt-handler REST call failure:

* The API server overloads, causing timeouts. Check the API server metrics and details like response times and overall calls. If you do not have cluster privileges, then you can fetch OpenShift API server pod logs with the `oc logs` command.

* The virt-handler pod cannot reach the API server. Common causes are:
** DNS issues on the node
** Networking connectivity issues

.Resolution

If there is an indication that the virt-operator cannot connect to the API server, delete the pod to force a restart. The issue normally relates to DNS or CNI issues outside of the scope of this alert.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//VirtOperatorDown Alert
[id="VirtOperatorDown_{context}"]
== VirtOperatorDown alert

.Description

This alert occurs when no virt-operator pod is in the `Running` state in the past 10 minutes. The virt-operator deployment has a default replica of two pods.

.Reason

The virt-operator is the first Kubernetes Operator active in an {product-title} cluster. Its primary responsibilities are:

* Installation
* Live-update
* Live-upgrade of a cluster
* Monitoring the lifecycle of top-level controllers such as virt-controller, virt-handler, and virt-launcher
* Managing the reconciliation of top-level controllers

In addition, the virt-operator is responsible for cluster-wide tasks such as certificate rotation and some infrastructure management.

[NOTE]
====
The virt-operator is not directly responsible for virtual machines in the cluster. The virt-operator's unavailability does not affect the custom workloads.
====

This alert indicates a failure at the cluster level. Critical cluster-wide management functionalities such as certification rotation, upgrade, and reconciliation of controllers are temporarily unavailable.

.Troubleshoot

. Modify the environment variable `NAMESPACE`.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----

. Check the status of the virt-operator deployment.
+
[source,terminal]
----
$ oc get deployment -n $NAMESPACE virt-operator -o yaml
----

. Check the virt-operator pods' events.
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pods <virt-operator pod>
----

. Check the virt-operator pods' logs.
+
[source,terminal]
----
$ oc -n $NAMESPACE logs <virt-operator pod>
----

. Check the manager pod's logs to determine why creating the virt-operator pods fails.
+
[source,terminal]
----
$ oc get logs <virt-operator-pod>
----

An example of a virt-operator pod name in the logs is `virt-operator-7888c64d66-dzc9p`. However, there may be several pods that run virt-operator.

.Resolution

There are several known reasons why the detection of no running virt-operator occurs. Identify the root cause from the list of possible reasons and take appropriate action.

* Node resource exhaustion
* Not enough memory on the cluster
* Nodes are down
* The API server overloads, such as when the scheduler is not 100% available)
* Networking issues

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//VirtOperatorRESTErrorsBurst Alert
[id="VirtOperatorRESTErrorsBurst_{context}"]
== VirtOperatorRESTErrorsBurst alert

.Description

More than 80% of the REST calls failed in virt-operator in the last five minutes.

.Reason

Virt-operator lost the connection to the API server. Cluster-level actions such as upgrading and controller reconciliation do not function. There is no effect to customer workloads such as VMs and VMIs.

.Troubleshoot

There are two common error types associated with virt-operator REST call failure:

* The API server overloads, causing timeouts. Check the API server metrics and details, such as response times and overall calls.

* The virt-operator pod cannot reach the API server. Common causes are network connectivity problems and DNS issues on the node. Check the virt-operator logs to verify that the pod can connect to the API server at all.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-operator
----
+
[source,terminal]
----
$ oc -n $NAMESPACE logs <pod-name>
----
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pod <pod-name>
----

.Resolution

If the virt-operator cannot connect to the API server, delete the pod to force a restart. The issue normally relates to DNS or CNI issues outside of the scope of this alert. Identify the root cause and take appropriate action.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.

//VirtOperatorRESTErrorsHigh Alert
[id="VirtOperatorRESTErrorsHigh_{context}"]
== VirtOperatorRESTErrorsHigh alert

.Description

More than 5% of the REST calls failed in virt-operator for the last 60 minutes.

.Reason

Virt-operator has partially lost the connection to the API server. Delay of cluster-level actions such as upgrading and controller reconciliation occurs. There is no effect to customer workloads such as VMs and VMIs.

.Troubleshoot

There are two common error types associated with virt-operator REST call failure:

* The API server overloads, causing timeouts. Check the API server metrics and details like response times and overall calls. If you do not have cluster privileges, then you can fetch OpenShift API server pod logs with the `oc logs` command.

* The virt-operator pod cannot reach the API server. A common cause is a network connectivity problem such as DNS issues on the node. Check virt-operator logs to verify whether it can connect to the API server at all.
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o custom-columns="":.metadata.namespace)"
----
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-operator
----
+
[source,terminal]
----
$ oc -n $NAMESPACE logs <pod-name>
----
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pod <pod-name>
----

.Resolution

If there is an indication that the virt-operator cannot connect to the API server, delete the pod to force a restart. The issue normally relates to DNS or CNI issues outside of the scope of this alert.

Otherwise, open a support issue and provide the information gathered in the troubleshooting process.
