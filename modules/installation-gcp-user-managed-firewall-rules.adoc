// Module included in the following assembly:
//
// * installing/installing_gcp/installing-gcp-account.adoc

:_mod-docs-content-type: CONCEPT
[id="installation-gcp-user-managed-firewall-rules_{context}"]
= Managing your own firewall rules

[role="_abstract"]
You can manage your own firewall rules when installing a cluster on {gcp-short} into an existing VPC by enabling the `firewallRulesManagement` parameter in the `install-config.yaml` file. You can limit the permissions that you grant to the installation program by managing your own firewall rules.

[IMPORTANT]
====
If you manage your own firewall rules, you must continue to manage them through the lifetime of the cluster. If you create a new service, create a new firewall rule that permits traffic on the service port, from the desired source addresses to the compute nodes. An example rule is described in the table below.
====

If you want to manage your own firewall rules, you must create the following rules before installation:

[cols="1,1,1,1"]
|====
|Rule Name |Protocol:Port |Source |Destination

|bootstrap-in-ssh
|`tcp:22`
|`<allowed_external_cidr>`
|`<control_plane_node_tags>`

|api
|`tcp:6443`
|`<allowed_external_cidr>`
|`<control_plane_node_tags>`

|health-checks
|`tcp:6080,6443,22624`
|`35.191.0.0/16`, `130.211.0.0/22`, `209.85.152.0/22`, `209.85.204.0/22`
|`<control_plane_node_tags>`

|etcd
|`tcp:2379,2380`
|`<control_plane_node_tags>`
|`<control_plane_node_tags>`

|control-plane
|`tcp:10257,10259,22623`
|`<control_plane_node_tags>`, `<compute_node_tags>`
|`<control_plane_node_tags>`

|internal-network
|icmp,`tcp:22`
|`<internal_network_cidr>`
|`<control_plane_node_tags>`, `<compute_node_tags>`

|internal-cluster
|`udp:500,4500,4789,6081`, `udp:9000-9999,30000-32767`, `esp`, `tcp:9000-9999,10250`, `tcp:30000-32767`
|`<control_plane_node_tags>`, `<compute_node_tags>`
|`<control_plane_node_tags>`, `<compute_node_tags>`

|ingress-k8s-fw
|`tcp:80,443`
|`<allowed_external_cidr>`
|`<control_plane_node_tags>`, `<compute_node_tags>`

|ingress-k8s-http-hc
|`tcp:30000-32767`
|`35.191.0.0/16`, `130.211.0.0/22`, `209.85.152.0/22`, `209.85.204.0/22`
|`<control_plane_node_tags>`, `<compute_node_tags>`

|`<sample_rule_name>`
|`<service_port>`
|`<allowed_external_cidr>`
|`<compute_node_tags>`
|====
where:

`<allowed_external_cidr>`:: Specifies a network CIDR of the machines that you want to grant access to your cluster. For a public cluster, this would typically be `0.0.0.0/0`. For a private cluster, access might be restricted to the cluster machine network.
`<control_plane_node_tags>`:: Specifies the network tags that apply to the control plane machines in your cluster. These tags must be specified in the `install-config.yaml` file you use to deploy the cluster.
`<compute_node_tags>`:: Specifies the network tags that apply to the compute machines in your cluster. These tags must be specified in the `install-config.yaml` file you use to deploy the cluster.
`<internal_network_cidr>`:: Specifies the network CIDR of the machine network that contains all the machines in your cluster.
`<sample_rule_name>`:: Specifies the name of a custom rule added after installation, for example if you create a new service in your cluster. You can add multiple custom rules as needed.
`<service_port>`:: Specifies the port or port range of the service you created, and the network protocol, such as TCP or UDP. If you create a service using `oc expose`, you can find the service port and protocol by running the command `oc get service <service_name> -o jsonpath='{.spec.ports[0].port}'`, where `<service_name>` is the name of the service you created.

After installation, you can reduce the port range of the `ingress-k8s-http-hc` and `internal-cluster` rules from `tcp:30000-32767` to the individual port that the ingress load balancer service uses, which is not known before installation. You can determine the service port by running the following command after installation:

[source,terminal]
----
$ oc get svc router-default -n openshift-ingress -o jsonpath='{.spec.ports[*].nodePort}'
----