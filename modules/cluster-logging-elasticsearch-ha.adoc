// Module included in the following assemblies:
//
// * logging/cluster-logging-elasticsearch.adoc

[id="cluster-logging-elasticsearch-ha_{context}"]
= Configuring replication policy for the log store 

You can define how Elasticsearch shards are replicated across data nodes in the cluster.

.Prerequisites

* Cluster logging and Elasticsearch must be installed.

.Procedure

. Edit the Cluster Logging Custom Resource (CR) in the `openshift-logging` project:
+
[source,terminal]
----
$ oc edit clusterlogging instance
----
+
[source,yaml]
----
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "instance"

....

spec:
  logStore:
    type: "elasticsearch"
    elasticsearch:
      redundancyPolicy: "SingleRedundancy" <1>
----
<1> Specify a redundancy policy for the shards. The change is applied upon saving the changes.
+
* *FullRedundancy*. Elasticsearch fully replicates the primary shards for each index
to every data node. This provides the highest safety, but at the cost of the highest amount of disk required and the poorest performance.
* *MultipleRedundancy*. Elasticsearch fully replicates the primary shards for each index to half of the data nodes.
This provides a good tradeoff between safety and performance.
* *SingleRedundancy*. Elasticsearch makes one copy of the primary shards for each index.
Logs are always available and recoverable as long as at least two data nodes exist.
Better performance than MultipleRedundancy, when using 5 or more nodes.  You cannot
apply this policy on deployments of single Elasticsearch node.
* *ZeroRedundancy*. Elasticsearch does not make copies of the primary shards.
Logs might be unavailable or lost in the event a node is down or fails.
Use this mode when you are more concerned with performance than safety, or have
implemented your own disk/PVC backup/restore strategy.

[NOTE]
====
The number of primary shards for the index templates is equal to the number of Elasticsearch data nodes.
====

