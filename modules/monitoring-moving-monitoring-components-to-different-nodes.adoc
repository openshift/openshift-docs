// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

[id="moving-monitoring-components-to-different-nodes_{context}"]
= Moving monitoring components to different nodes

You can move any of the monitoring stack components to specific nodes.

.Prerequisites

* *If you are configuring core {product-title} monitoring components*:
** You have access to the cluster as a user with the `cluster-admin` role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
* *If you are configuring components that monitor user-defined projects*:
** You have access to the cluster as a user with the `cluster-admin` role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the ConfigMap:
** *To move a component that monitors core {product-title} projects*:
.. Edit the `cluster-monitoring-config` ConfigMap in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Specify the `nodeSelector` constraint for the component under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    <component>:
      nodeSelector:
        <node_key>: <node_value>
        <node_key>: <node_value>
        <...>
----
+
Substitute `<component>` accordingly and substitute `<node_key>: <node_value>` with the map of key-value pairs that specifies a group of destination nodes. Often, only a single key-value pair is used.
+
The component can only run on nodes that have each of the specified key-value pairs as labels. The nodes can have additional labels as well.
+
[IMPORTANT]
====
Many of the monitoring components are deployed by using multiple pods across different nodes in the cluster to maintain high availability. When moving monitoring components to labeled nodes, ensure that enough matching nodes are available to maintain resilience for the component. If only one label is specified, ensure that enough nodes contain that label to distribute all of the pods for the component across separate nodes. Alternatively you can specify multiple labels each relating to individual nodes.
====
+
[NOTE]
====
If monitoring components remain in a `Pending` state after configuring the `nodeSelector` contstraint, check the pod logs for errors relating to taints and tolerations.
====
+
For example, to move monitoring components for core {product-title} projects to specific nodes that are labeled `nodename: controlplane1`, `nodename: worker1`, `nodename: worker2`, and `nodename: worker2`, use:
+
[source,yaml,subs=quotes]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusOperator:
      nodeSelector:
        nodename: controlplane1
    prometheusK8s:
      nodeSelector:
        nodename: worker1
        nodename: worker2
    alertmanagerMain:
      nodeSelector:
        nodename: worker1
        nodename: worker2
    kubeStateMetrics:
      nodeSelector:
        nodename: worker1
    grafana:
      nodeSelector:
        nodename: worker1
    telemeterClient:
      nodeSelector:
        nodename: worker1
    k8sPrometheusAdapter:
      nodeSelector:
        nodename: worker1
        nodename: worker2
    openshiftStateMetrics:
      nodeSelector:
        nodename: worker1
    thanosQuerier:
      nodeSelector:
        nodename: worker1
        nodename: worker2
----

** *To move a component that monitors user-defined projects*:
.. Edit the `user-workload-monitoring-config` ConfigMap in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Specify the `nodeSelector` constraint for the component under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    <component>:
      nodeSelector:
        <node_key>: <node_value>
        <node_key>: <node_value>
        <...>
----
+
Substitute `<component>` accordingly and substitute `<node_key>: <node_value>` with the map of key-value pairs that specifies the destination nodes. Often, only a single key-value pair is used.
+
The component can only run on nodes that have each of the specified key-value pairs as labels. The nodes can have additional labels as well.
+
[IMPORTANT]
====
Many of the monitoring components are deployed by using multiple pods across different nodes in the cluster to maintain high availability. When moving monitoring components to labeled nodes, ensure that enough matching nodes are available to maintain resilience for the component. If only one label is specified, ensure that enough nodes contain that label to distribute all of the pods for the component across separate nodes. Alternatively you can specify multiple labels each relating to individual nodes.
====
+
[NOTE]
====
If monitoring components remain in a `Pending` state after configuring the `nodeSelector` contstraint, check the pod logs for errors relating to taints and tolerations.
====
+
For example, to move monitoring components for user-defined projects to specific worker nodes labeled `nodename: worker1`, `nodename: worker2`, and `nodename: worker2`, use:
+
[source,yaml,subs=quotes]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheusOperator:
      nodeSelector:
        nodename: worker1
    prometheus:
      nodeSelector:
        nodename: worker1
        nodename: worker2
    thanosRuler:
      nodeSelector:
        nodename: worker1
        nodename: worker2
----

. Save the file to apply the changes. The components affected by the new configuration are moved to the new nodes automatically.
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` ConfigMap are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====
+
[WARNING]
====
When changes are saved to a monitoring ConfigMap, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.
====
