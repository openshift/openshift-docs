// Module included in the following assemblies:
//
// * scalability_and_performance/low_latency_tuning/cnf-tuning-low-latency-nodes-with-perf-profile.adoc

:_mod-docs-content-type: PROCEDURE
[id="running-the-performance-profile-profile-hosted-cluster-using-podman_{context}"]
= Running the Performance Profile Creator on a hosted cluster using Podman

As a cluster administrator, you can use Podman with the Performance Profile Creator (PPC) tool to create a performance profile.

For more information about PPC arguments, see "Performance Profile Creator arguments".

The PPC tool is designed to be hosted-cluster aware. When it detects a hosted cluster from the `must-gather` data it automatically takes the following actions:

* Recognizes that there is no machine config pool (MCP).
* Uses node pools as the source of truth for compute node configurations instead of MCPs.
* Does not require you to specify the `node-pool-name` value explicitly unless you want to target a specific pool.

[IMPORTANT]
====
The PPC uses the `must-gather` data from your hosted cluster to create the performance profile. If you make any changes to your cluster, such as relabeling a node targeted for performance configuration, you must re-create the `must-gather` data before running PPC again.
====

.Prerequisites

* Access to the cluster as a user with the `cluster-admin` role.
* A hosted cluster is installed.
* Installation of Podman and the {oc-first}.
* Access to the Node Tuning Operator image.
* Access to the `must-gather` data for your cluster.

.Procedure

. On the hosted cluster, use Podman to authenticate to `registry.redhat.io` by running the following command:
+
[source,terminal]
----
$ podman login registry.redhat.io
----
+
[source,bash]
----
Username: <user_name>
Password: <password>
----

. Create a performance profile on the hosted cluster, by running the following command. The example uses sample PPC arguments and values:
+
[source,terminal,subs="attributes+"]
----
$ podman run --entrypoint performance-profile-creator \
    -v /path/to/must-gather:/must-gather:z \// <1>
    registry.redhat.io/openshift4/ose-cluster-node-tuning-rhel9-operator:v{product-version} \
    --must-gather-dir-path /must-gather \
    --reserved-cpu-count=2 \// <2>
    --rt-kernel=false \// <3>
    --split-reserved-cpus-across-numa=false \ <4>
    --topology-manager-policy=single-numa-node \// <5>
    --node-pool-name=democluster-us-east-1a \ 
    --power-consumption-mode=ultra-low-latency \// <6>
    --offlined-cpu-count=1 \// <7>
    > my-hosted-cp-performance-profile.yaml
----
+
<1> Mounts the local directory where the output of an `oc adm must-gather` was created into the container. 
<2> Specifies two reserved CPUs.
<3> Disables the real-time kernel.
<4> Disables reserved CPUs splitting across NUMA nodes.
<5> Specifies the NUMA topology policy. If installing the NUMA Resources Operator, this must be set to `single-numa-node`.
<6> Specifies minimal latency at the cost of increased power consumption.
<7> Specifies one offlined CPU.
+
.Example output
[source,terminal]
----
level=info msg="Nodes names targeted by democluster-us-east-1a pool are: ip-10-0-129-110.ec2.internal "
level=info msg="NUMA cell(s): 1"
level=info msg="NUMA cell 0 : [0 2 1 3]"
level=info msg="CPU(s): 4"
level=info msg="2 reserved CPUs allocated: 0,2 "
level=info msg="1 isolated CPUs allocated: 1"
level=info msg="Additional Kernel Args based on configuration: []
----

. Review the created YAML file by running the following command:
+
[source,terminal]
----
$ cat my-hosted-cp-performance-profile
----
.Example output
+
[source,yaml]
----
---
apiVersion: v1
data:
  tuning: |
    apiVersion: performance.openshift.io/v2
    kind: PerformanceProfile
    metadata:
      creationTimestamp: null
      name: performance
    spec:
      cpu:
        isolated: "1"
        offlined: "3"
        reserved: 0,2
      net:
        userLevelNetworking: false
      nodeSelector:
        node-role.kubernetes.io/worker: ""
      numa:
        topologyPolicy: single-numa-node
      realTimeKernel:
        enabled: false
      workloadHints:
        highPowerConsumption: true
        perPodPowerManagement: false
        realTime: true
    status: {}
kind: ConfigMap
metadata:
  name: performance
  namespace: clusters
----