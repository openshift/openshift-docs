// Module included in the following assemblies:
//
// <List assemblies here, each on a new line>

// This module can be included from assemblies using the following include statement:
// include::<path>/proc_installing-openshift-to-z-vm.adoc[leveloffset=+1]

// The file name and the ID are based on the module title. For example:
// * file name: proc_doing-procedure-a.adoc
// * ID: [id='proc_doing-procedure-a_{context}']
// * Title: = Doing procedure A
//
// The ID is used as an anchor for linking to the module. Avoid changing
// it after the module has been published to ensure existing links are not
// broken.
//
// The `context` attribute enables module reuse. Every module's ID includes
// {context}, which ensures that the module has a unique ID even if it is
// reused multiple times in a guide.
//
// Start the title with a verb, such as Creating or Create. See also
// _Wording of headings_ in _The IBM Style Guide_.
[id="installing-openshift-to-z-vm_{context}"]
= Installing OpenShift to z-VM

Now that we have created VMs for all of our nodes, we can begin installing OpenShift 4 itself. By the end of this section, we should have a working installation that we can use to develop and test.


[discrete]
== Prerequisites

All z/VM guests in the environment should be uniformly configured with the same virtual device IDs for disk and network.

* 1 zVM guest with RHEL 8 installed (dns/haproxy/bastion workstation)

* 3+ z/VM guests with master node specs

* 4+ vcpus

* 16GB+ RAM

* 1+ z/VM guest(s) with worker specs

* 2+ vcpus

* 16GB+ RAM

* 1 z/VM guest with bootstrap specs

* 2+ vcpus

* 16GB+ RAM


[discrete]
== Procedure

This section describes the instructions for installing OpenShift on the z/VMs specified above. Many of the following steps expect to be run in a Linux terminal or 3270 console. The host it that terminal or console should be connected to is in *bold*.

. SSH into the bastion from your *localhost*.
....
$ ssh root@bastion_fqdn_or_ip
....
+
. Download openshift-upi-playbooks from partners-ftp to the *bastion*.
....
$ curl -O ftp://partners.redhat.com/1c5d859a/IBM-917ca288c24859a81f35b894e0b77589/Openshift-ibm/OCP_4.2.z_for_Z/70178/openshift-upi-playbooks-20191108-s390x.tar.gz
....
+
. On the *bastion*, un-tar the deployment playbooks.
....
$ tar -xf openshift-upi-playbooks-20191108-s390x.tar.gz; mv openshift-upi-playbooks-20191108-s390x openshift-upi-playbooks
....
+
. On the *bastion*, change your working directory to the deployment playbooks directory.
....
$ cd openshift-upi-playbooks
....
+
. Optional: update the “cluster_name” variable in group_vars/all.yml with desired OpenShift cluster name, as shown in step 5a below.
+
.. `group_vars/all.yml`
....
---
# Configure the internal domain name of the cluster here.
# The default 'redhat.com' address will work because
# the automation runs its own DNS, but it may be changed
# to something else (eg. ibm.com).
cluster_base_domain: "redhat.com"
cluster_domain_name: "{{ cluster_name }}.{{ cluster_base_domain }}"
cluster_name: desired_openshift_cluster_name
---
....
+
. Configure lab-assigned IP addresses of nodes, network nameserver, and network gateway in `group_vars/all.yml`.
+
. Ensure unique mac addresses for each node in `group_vars/all.yml`.

NOTE: The z/VM install does not make use of dhcp, so correct mac addresses are not technically required.  The mac address config parameter is still used in some places in the automation for key-value look-ups based on the node.  This will be simplified in future versions of the automation to prevent unnecessary confusion.
+
. Ensure correct device IDs in group_vars/all.yml for zvm_rd_znet and zvm_rd_dasd.

WARNING: Using the wrong device IDs for `rd.dasd` could lead to data loss by overwriting the wrong disk.  The example value in `group_vars/all.yml` shows the kernel parameter format, and will not necessarily work in your environment.

NOTE: A list of device IDs for each z/VM guest can generally be found via the 3270 console via the command:
....
#cp q v
....

+
. Store the pull secret you’ve obtained from the multi-arch team in ~/.ocp4_pull_secret on the *bastion*.
+
. *Manual step required*: Power off the bootstrap node.  This is not strictly necessary, but the automation includes a step to wait for ssh accessibility while the user manually IPLs the RHCOS nodes.  If the bootstrap node is running linux from a previous installation, the automation will incorrectly detect that coreos has been freshly installed on the bootstrap, and the user will have less time to manually IPL the RHCOS nodes.
+
. Run the main installation playbook on the bastion.
....
$ ansible-playbook -i inventory playbooks/main.yml
....
+
. While the automation is running: *In the 3270 console on each z/VM guest*, clear the virtual punch card reader.
....
#cp pur rdr all
....
+
. Once the automation reaches the point of “Waiting for bootstrap node accessibility,” the user must IPL the OCP nodes with the coreos images.  The parm files will be generated automatically in `/var/lib/tftpboot` along with the downloaded CoreOS images.
+
.. Ensure the reader devices are available on the bastion
....
cio_ignore -r c-e
chccwdev -e c-e
....
+
.. For each OCP node, from the bastion
....
cd /var/lib/tftpboot
vmur pun -r -u <z/vm guest id> -N kernel.img \
rhcos-<version>-installer-kernel
vmur pun -r -u <z/vm guest id> -N generic.parm <ocp-node>.parm
vmur pun -r -u <z/vm guest id> N initrd.img rhcos-<version>.img
....
+
In the 3270 console *on each z/VM guest* (except for the bastion)
....
#cp ipl 00c
....
+
. Monitor the playbook to ensure that the cluster installs successfully. Congratulations, you should now have a working OpenShift 4 cluster!

.. To start accessing the environment on your localhost, follow the instructions above.
