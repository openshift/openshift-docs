// Module included in the following assemblies:
//
// <List assemblies here, each on a new line>

// This module can be included from assemblies using the following include statement:
// include::<path>/con_installing-ocp-to-z-vm-guests.adoc[leveloffset=+1]

// The file name and the ID are based on the module title. For example:
// * file name: con_my-concept-module-a.adoc
// * ID: [id='con_my-concept-module-a_{context}']
// * Title: = My concept module A
//
// The ID is used as an anchor for linking to the module. Avoid changing
// it after the module has been published to ensure existing links are not
// broken.
//
// The `context` attribute enables module reuse. Every module's ID includes
// {context}, which ensures that the module has a unique ID even if it is
// reused multiple times in a guide.
//
// In the title, include nouns that are used in the body text. This helps
// readers and search engines find information quickly.
// Do not start the title with a verb. See also _Wording of headings_
// in _The IBM Style Guide_.
[id="installing-ocp-to-z-vm-guests_{context}"]
= Installing OCP to z-VM guests

Now that we have provided a code drop for installation on a z/VM setup, some teams will want to work on deploying to their our z/VM environments. These instructions walk you through how you can deploy your own OpenShift cluster using the automation provided by the Red Hat Multi-Arch team.

== Differences From the zKVM Setup

If you are familiar with the zKVM deployment instructions, the essence of the install steps remain the same. However, there are a few important differences to keep in mind as you go through this installation.

* Instead of using DHCP we are passing static IP addresses and the local DNS server address via the kernel command line.

** In a production environment, it will be up to the system administrator to determine how they want to set up networking in their clusters.  Most lab environments have some existing method of assigning IP addresses to guests, and passing static IP addresses is more portable than having a dependency on running our own dhcp.

* We need to punch the kernel and init-rd files manually to boot our nodes.

** In our zKVM setup, we could rely on libvirt for the purpose of booting our nodes with Red Hat CoreOS. For our z/VM guests, we need to punch the kernel and initrd into the reader before we can boot them.  We will use vmur for this in our example environment.

* Correct network configuration is essential.

** While this is true in the zKVM deployment, it’s even more important to double check your network setup in the deployment scripts before you try to deploy on z/VM guests. This will likely be the cause of most errors in this deployment strategy.

* z/VM disk and network device IDs must be checked and updated in the automation before running. Failure to do so could result in loss of data.

* There isn’t a stand-alone hypervisor guest.
