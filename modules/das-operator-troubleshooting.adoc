// Module included in the following assemblies:
//
// * operators/user/das-operator-using.adoc

:_mod-docs-content-type: PROCEDURE
[id="das-operator-troubleshooting_{context}"]
= Troubleshooting the DAS Operator

If you experience issues with the DAS Operator, you can use the following troubleshooting steps to diagnose and resolve problems.

.Prerequisites

* You have installed the DAS Operator.
* You have access to the {product-title} cluster with appropriate permissions.

== Debugging DAS Operator components

. Check the status of all DAS Operator components:
+
[source,terminal]
----
$ oc get pods -n das-operator
----
+
.Example output
[source,terminal]
----
NAME                                    READY   STATUS    RESTARTS   AGE
das-daemonset-6rsfd                     1/1     Running   0          5m16s
das-daemonset-8qzgf                     1/1     Running   0          5m16s
das-operator-5946478b47-cjfcp           1/1     Running   0          5m18s
das-operator-5946478b47-npwmn           1/1     Running   0          5m18s
das-operator-webhook-59949d4f85-5n9qt   1/1     Running   0          68s
das-operator-webhook-59949d4f85-nbtdl   1/1     Running   0          68s
das-scheduler-6cc59dbf96-4r85f          1/1     Running   0          68s
das-scheduler-6cc59dbf96-bf6ml          1/1     Running   0          68s
----

. Inspect the logs of the DAS Operator controller:
+
[source,terminal]
----
$ oc logs -n das-operator deployment/das-operator
----

. Check the logs of the webhook server:
+
[source,terminal]
----
$ oc logs -n das-operator deployment/das-operator-webhook
----

. Check the logs of the scheduler plugin:
+
[source,terminal]
----
$ oc logs -n das-operator deployment/das-scheduler
----

. Check the logs of the device plugin daemonset:
+
[source,terminal]
----
$ oc logs -n das-operator daemonset/das-daemonset
----

== Monitoring AllocationClaims

. Inspect active `AllocationClaim` resources:
+
[source,terminal]
----
$ oc get allocationclaims -n das-operator
----

. Check for claims in different states:
+
[source,terminal]
----
$ oc get allocationclaims -n das-operator -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.state}{"\n"}{end}'
----
+
.Example output
[source,terminal]
----
13950288-57df-4ab5-82bc-6138f646633e-harpatil000034jma-qh5fm-worker-f-57md9-cuda-vectoradd-0	inUse
ce997b60-a0b8-4ea4-9107-cf59b425d049-harpatil000034jma-qh5fm-worker-f-fl4wg-cuda-vectoradd-0	inUse
----

. View events related to `AllocationClaim` resources:
+
[source,terminal]
----
$ oc get events -n das-operator --field-selector involvedObject.kind=AllocationClaim
----

. Check `NodeAccelerator` resources to verify GPU hardware detection:
+
[source,terminal]
----
$ oc get nodeaccelerator -n das-operator
----
+
.Example output
[source,terminal]
----
NAME                                     AGE
harpatil000034jma-qh5fm-worker-f-57md9   96m
harpatil000034jma-qh5fm-worker-f-fl4wg   96m
----
+
The `NodeAccelerator` resources represent the GPU-capable nodes detected by the DAS Operator.

== Verifying GPU device availability

. On a node with GPU hardware, verify that CDI devices were created:
+
[source,terminal]
----
$ oc debug node/<node-name>
----
+
[source,terminal]
----
sh-4.4# chroot /host
sh-4.4# ls -l /var/run/cdi/
----

. Check the NVIDIA GPU Operator status:
+
[source,terminal]
----
$ oc get clusterpolicies.nvidia.com -o jsonpath='{.items[0].status.state}'
----
+
The output should show `ready`.

== Increasing log verbosity

To get more detailed debugging information:

. Edit the `DASOperator` resource to increase log verbosity:
+
[source,terminal]
----
$ oc edit dasoperator -n das-operator
----

. Set the `operatorLogLevel` field to `Debug` or `Trace`:
+
[source,yaml]
----
spec:
  operatorLogLevel: Debug
----

. Save the changes and verify that the operator pods restart with increased verbosity.

== Common issues and solutions

.Pods stuck in UnexpectedAdmissionError state
[NOTE]
====
Due to link:https://github.com/kubernetes/kubernetes/issues/128043[kubernetes/kubernetes#128043], pods may enter an `UnexpectedAdmissionError` state if admission fails. Pods managed by higher level controllers such as Deployments will be recreated automatically. Naked pods, however, must be cleaned up manually with `oc delete pod`. Using controllers is recommended until the upstream issue is resolved.
====

.Prerequisites not met
If the DAS Operator fails to start or function properly, verify that all prerequisites are installed:

* Cert-manager
* Node Feature Discovery (NFD) Operator
* NVIDIA GPU Operator

.GPU resources not available
If GPU resources are not being advertised:

. Verify that the NVIDIA GPU Operator is installed and operational.
. Check that GPU hardware is available and properly configured.
. Ensure that the NFD Operator has labeled nodes with GPU features.

== Additional resources

* link:https://github.com/kubernetes/kubernetes/issues/128043[Kubernetes issue #128043]
* xref:../../hardware_enablement/psap-node-feature-discovery-operator.adoc#psap-node-feature-discovery-operator[Node Feature Discovery Operator]
* link:https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/troubleshooting.html[NVIDIA GPU Operator troubleshooting]