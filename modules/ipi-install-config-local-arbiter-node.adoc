// Module included in the following assemblies:
//
//  *installing/installing_bare_metal/ipi/ipi-install-installation-workflow.adoc

:_mod-docs-content-type: PROCEDURE
[id="ipi-install-config-local-arbiter-node_{context}"]
= Configuring a local arbiter node

You can configure an {product-title} cluster with two control plane nodes and one local arbiter node so to retain high availability (HA) while reducing infrastructure costs for your cluster. This configuration is supported only for bare-metal installations.

:FeatureName: Configuring a local arbiter node
include::snippets/technology-preview.adoc[]

A local arbiter node is a lower-cost, co-located machine that participates in control plane quorum decisions. Unlike a standard control plane node, the arbiter node does not run the full set of control plane services. You can use this configuration to maintain HA in your cluster with only two fully provisioned control plane nodes instead of three.

[IMPORTANT]
====
You can configure a local arbiter node only. Remote arbiter nodes are not supported.
====

To deploy a cluster with two control plane nodes and one local arbiter node, you must define the following nodes in the `install-config.yaml` file:

* 2 control plane nodes
* 1 arbiter node

You must enable the `TechPreviewNoUpgrade` feature set in the `FeatureGate` custom resource (CR) to enable the arbiter node feature. 
For more information about feature gates, see "Understanding feature gates".

The arbiter node must meet the following minimum system requirements:

* 2 threads
* 8 GB of RAM
* 120 GB of SSD or equivalent storage

The arbiter node must be located in a network environment with an end-to-end latency of less than 500 milliseconds, including disk I/O. In high-latency environments, you might need to apply the `etcd` slow profile.

The control plane nodes must meet the following minimum system requirements:

* 4 threads
* 16 GB of RAM
* 120 GB of SSD or equivalent storage

Additionally, the control plane nodes must also have enough storage for the workload.

.Prerequisites

* You have downloaded {oc-first} and the installation program.
* You have logged into the {oc-first}.

.Procedure

. Edit the `install-config.yaml` file to define the arbiter node alongside control plane nodes.
+
.Example `install-config.yaml` configuration for deploying an arbiter node
[source,yaml]
----
apiVersion: v1
baseDomain: devcluster.openshift.com
compute:
  - architecture: amd64
    hyperthreading: Enabled
    name: worker
    platform: {}
    replicas: 0
arbiter: <1>
  architecture: amd64
  hyperthreading: Enabled
  replicas: 1 <2>
  name: arbiter <3>
  platform:
    baremetal: {}
controlPlane: <4>
  architecture: amd64
  hyperthreading: Enabled
  name: master
  platform:
    baremetal: {}
  replicas: 2 <5>
featureSet: TechPreviewNoUpgrade
platform:
  baremetal:
# ...
    hosts:
      - name: cluster-master-0
        role: master
# ...
      - name: cluster-master-1
        role: master
        ...
      - name: cluster-arbiter-0
        role: arbiter
# ...
----
<1> Defines the arbiter machine pool. You must configure this field to deploy a cluster with an arbiter node.
<2> Set the `replicas` field to `1` for the arbiter pool. You cannot set this field to a value that is greater than 1.
<3> Specifies a name for the arbiter machine pool.
<4> Defines the control plane machine pool.
<5> When an arbiter pool is defined, two control plane replicas are valid.

. Save the modified `install-config.yaml` file.