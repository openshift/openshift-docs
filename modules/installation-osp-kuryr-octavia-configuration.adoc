// Module included in the following assemblies:
//
// * installing/installing_openstack/installing-openstack-installer-kuryr.adoc

[id="installation-osp-kuryr-octavia-configuration_{context}"]
= Configuring Octavia

Kuryr SDN uses {rh-openstack-first}'s Octavia LBaaS to implement {product-title} Services. Thus,
you must install and configure Octavia components in {rh-openstack}
to use Kuryr SDN.

To enable Octavia, you must include the Octavia Service during the installation
of the {rh-openstack} Overcloud, or upgrade the Octavia Service if the Overcloud
already exists. The following steps for enabling Octavia apply to both a clean
install of the Overcloud or an Overcloud update.

[NOTE]
====
The following steps only capture the key pieces required during the
https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/13/html/director_installation_and_usage/[deployment of {rh-openstack}]
when dealing with Octavia. It is also important to note that
https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/13/html/director_installation_and_usage/configuring-a-container-image-source#registry-methods[registry methods]
vary.

This example uses the local registry method.
====

.Procedure

. If you are using the local registry, create a template to upload the images to
the registry. For example:
+
----
(undercloud) $ openstack overcloud container image prepare \
-e /usr/share/openstack-tripleo-heat-templates/environments/services-docker/octavia.yaml \
--namespace=registry.access.redhat.com/rhosp13 \
--push-destination=<local-ip-from-undercloud.conf>:8787 \
--prefix=openstack- \
--tag-from-label {version}-{release} \
--output-env-file=/home/stack/templates/overcloud_images.yaml \
--output-images-file /home/stack/local_registry_images.yaml
----

. Verify that the `local_registry_images.yaml` file contains the Octavia images.
For example:
+
----
...
- imagename: registry.access.redhat.com/rhosp13/openstack-octavia-api:13.0-43
  push_destination: <local-ip-from-undercloud.conf>:8787
- imagename: registry.access.redhat.com/rhosp13/openstack-octavia-health-manager:13.0-45
  push_destination: <local-ip-from-undercloud.conf>:8787
- imagename: registry.access.redhat.com/rhosp13/openstack-octavia-housekeeping:13.0-45
  push_destination: <local-ip-from-undercloud.conf>:8787
- imagename: registry.access.redhat.com/rhosp13/openstack-octavia-worker:13.0-44
  push_destination: <local-ip-from-undercloud.conf>:8787
----
+
[NOTE]
====
The Octavia container versions vary depending upon the specific
{rh-openstack} release installed.
====

. Pull the container images from registry.redhat.io to the Undercloud node:
+
----
(undercloud) $ sudo openstack overcloud container image upload \
  --config-file  /home/stack/local_registry_images.yaml \
  --verbose
----
+
This may take some time depending on the speed of your network and Undercloud
disk.

. Since an Octavia load balancer is used to access the OpenShift API, you must
increase their listeners' default timeouts for the connections. The default
timeout is 50 seconds. Increase the timeout to 20 minutes by passing the
following file to the Overcloud deploy command:
+
----
(undercloud) $ cat octavia_timeouts.yaml
parameter_defaults:
  OctaviaTimeoutClientData: 1200000
  OctaviaTimeoutMemberData: 1200000
----
+
[NOTE]
====
This is not needed for {rh-openstack} 14+.
====

. Install or update your Overcloud environment with Octavia:
+
----
openstack overcloud deploy --templates \
  -e /usr/share/openstack-tripleo-heat-templates/environments/services-docker/octavia.yaml \
  -e octavia_timeouts.yaml
----
+
[NOTE]
====
This command only includes the files associated with Octavia; it varies based on
your specific installation of {rh-openstack}. See the {rh-openstack}
documentation for further information. For more information on customizing your
Octavia installation, see
https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/13/html-single/networking_guide/#planning_your_octavia_deployment[installation
of Octavia using Director].
====
+
[NOTE]
====
When leveraging Kuryr SDN, the Overcloud installation requires the Neutron `trunk` extension. This is available by default on Director deployments.
Use the `openvswitch` firewall instead of the default `ovs-hybrid` when the Neutron
backend is ML2/OVS. There is no need for modifications if the backend is
ML2/OVN.
====

. In {rh-openstack} versions 13 and 15, add the project ID
to the `octavia.conf` configuration file after you create the project.
* To enforce
network policies across Services, like when traffic goes through
the Octavia load balancer, you must ensure Octavia creates the Amphora VM
security groups on the user project.
+
This change ensures that required LoadBalancer security groups belong to that project,
and that they can be updated to enforce Services isolation.
+
[NOTE]
====
This task is unnecessary in {rh-openstack} version 16 or later.

Octavia implements a new ACL API that restricts access to the Load
Balancers VIP.
====

.. Get the project ID
+
----
$ openstack project show <project>
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description |                                  |
| domain_id   | default                          |
| enabled     | True                             |
| id          | PROJECT_ID                       |
| is_domain   | False                            |
| name        | *<project>*                      |
| parent_id   | default                          |
| tags        | []                               |
+-------------+----------------------------------+
----

.. Add the project ID to `octavia.conf` for the controllers.

... List the Overcloud controllers.
+
----
$ source stackrc  # Undercloud credentials
$ openstack server list
+--------------------------------------+--------------+--------+-----------------------+----------------+------------+
│
| ID                                   | Name         | Status | Networks
| Image          | Flavor     |
│
+--------------------------------------+--------------+--------+-----------------------+----------------+------------+
│
| 6bef8e73-2ba5-4860-a0b1-3937f8ca7e01 | controller-0 | ACTIVE |
ctlplane=192.168.24.8 | overcloud-full | controller |
│
| dda3173a-ab26-47f8-a2dc-8473b4a67ab9 | compute-0    | ACTIVE |
ctlplane=192.168.24.6 | overcloud-full | compute    |
│
+--------------------------------------+--------------+--------+-----------------------+----------------+------------+
----

... SSH into the controller(s).
+
----
$ ssh heat-admin@192.168.24.8
----

... Edit the `octavia.conf` to add the project into the list of projects where
Amphora security groups are on the user's account.
+
----
# List of project IDs that are allowed to have Load balancer security groups
# belonging to them.
amp_secgroup_allowed_projects = PROJECT_ID
----

.. Restart the Octavia worker so the new configuration loads.
+
----
controller-0$ sudo docker restart octavia_worker
----

[NOTE]
====
Depending on your {rh-openstack} environment, Octavia might not support UDP
listeners. If you use Kuryr SDN on {rh-openstack} version 15 or earlier, UDP services are not supported. 
{rh-openstack} version 16 or later support UDP.
====

[id="installation-osp-kuryr-octavia-driver_{context}"]
== The Octavia OVN Driver

Octavia supports multiple provider drivers through the Octavia API.

To see all available Octavia provider drivers, on a command line, enter:
----
$ openstack loadbalancer provider list
----

The result is a list of drivers:
----
+---------+-------------------------------------------------+
| name    | description                                     |
+---------+-------------------------------------------------+
| amphora | The Octavia Amphora driver.                     |
| octavia | Deprecated alias of the Octavia Amphora driver. |
| ovn     | Octavia OVN driver.                             |
+---------+-------------------------------------------------+
----

Beginning with {rh-openstack} version 16, the Octavia OVN provider driver (`ovn`) is supported on
{product-title} on {rh-openstack} deployments.

`ovn` is an integration driver for the load balancing
that Octavia and OVN provide. It supports basic load balancing capabilities,
and is based on OpenFlow rules. The driver is automatically enabled
in Octavia by Director on deployments that use OVN Neutron ML2.

The Amphora provider driver is the default driver. If `ovn` is enabled, however, Kuryr uses it.

If Kuryr uses `ovn` instead of Amphora, it offers the following benefits:

* Decreased resource requirements. Kuryr does not require a load balancer VM for each Service.
* Reduced network latency.
* Increased service creation speed by using OpenFlow rules instead of a VM for each Service.
* Distributed load balancing actions across all nodes instead of centralized on Amphora VMs.


[id="installation-osp-kuryr-octavia-driver_upgrade_{context}"]
== Upgrading Octavia provider drivers after installation

You can upgrade a cluster that is deployed on {rh-openstack-first} from the Octavia Amphora
provider driver to the Octavia OVN provider driver.

[IMPORTANT]
====
{rh-openstack} replaces existing load balancers after you change provider drivers. This process
results in some downtime.
====

.Prerequisites

* Install the {rh-openstack} CLI, `openstack`.

* Install the {product-title} CLI, `oc`.

* Enable the Octavia OVN driver on {rh-openstack}.
+
[TIP]
====
To view a list of available Octavia drivers, on a command line, enter `openstack loadbalancer provider list`.
====

.Procedure

To change from the Octavia Amphora provider driver to Octavia OVN:

. Open the `kuryr-config` ConfigMap. On a command line, enter:
+
----
$ oc -n openshift-kuryr edit cm kuryr-config
----

. In the ConfigMap, delete the line that contains `kuryr-octavia-provider`. For example:
+
----
...
kind: ConfigMap
metadata:
  annotations:
    networkoperator.openshift.io/kuryr-octavia-provider: default <1>
...
----
<1> Delete this line. The cluster will regenerate it with `ovn` as the value.
+
After making this modification, wait for the `kuryr-controller` and `kuryr-cni` pods to redeploy. This process may take several minutes.

. Verify that the `kuryr-config` ConfigMap annotation is present with `ovn` as its value. On a command line, enter:
+
----
$ oc -n openshift-kuryr edit cm kuryr-config
----
+
The `ovn` provider value should be present in the output:
+
----
...
kind: ConfigMap
metadata:
  annotations:
    networkoperator.openshift.io/kuryr-octavia-provider: ovn
...
----

. Verify that {rh-openstack} recreated its load balancers.

.. On a command line, type:
+
----
$ openstack loadbalancer list | grep amphora
----
+
You should see a single Amphora load balancer. For example:
+
----
a4db683b-2b7b-4988-a582-c39daaad7981 | ostest-7mbj6-kuryr-api-loadbalancer  | 84c99c906edd475ba19478a9a6690efd | 172.30.0.1     | ACTIVE              | amphora
----

.. Then, type:
+
----
$ openstack loadbalancer list | grep ovn
----
+
The remaining load balancers should be the `ovn` type. For example:
+
----
2dffe783-98ae-4048-98d0-32aa684664cc | openshift-apiserver-operator/metrics | 84c99c906edd475ba19478a9a6690efd | 172.30.167.119 | ACTIVE              | ovn
0b1b2193-251f-4243-af39-2f99b29d18c5 | openshift-etcd/etcd                  | 84c99c906edd475ba19478a9a6690efd | 172.30.143.226 | ACTIVE              | ovn
f05b07fc-01b7-4673-bd4d-adaa4391458e | openshift-dns-operator/metrics       | 84c99c906edd475ba19478a9a6690efd | 172.30.152.27  | ACTIVE              | ovn
----
