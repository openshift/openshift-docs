// Module included in the following assemblies:
//
// * scalability_and_performance/ztp-factory-install-clusters.adoc
:_content-type: PROCEDURE
[id="run-the-spoke-cluster_factory_install_pipeline_{context}"]
= Run the spoke cluster factory install pipeline

The spoke factory install pipeline is as follows.

.Prerequisites

* An {product-title} hub cluster
* Log in as a user with `cluster-admin` privileges.

.Procedure

. Edit the `spoke.yaml` with sample details as shown. A sample configuration file is present in ``examples/config.yaml``. 
+
[NOTE]
====
At this stage you are building out the `spokes` section. 
====
+
[source,yaml]
----
config:
  clusterimageset: openshift-v4.9.0 
  OC_OCP_VERSION: "4.9" 
  OC_OCP_TAG: "4.9.0-x86_64" 
  OC_RHCOS_RELEASE: "49.84.202110081407-0" # TODO automate it to get it automated using binary <4>
  OC_ACM_VERSION: "2.4" 
  OC_OCS_VERSION: "4.8"

spokes:
  - spoke1-name:
      master0:
        nic_ext_dhcp: eno4 <1> 
        nic_int_static: eno5 <2> 
        mac_ext_dhcp: "aa:ss:dd:ee:b0:10" <3>
        mac_int_static: "aa:ss:dd:ee:b1:10" <4>
        bmc_url: "<url bmc>" <5>
        bmc_user: "user-bmc" <6>
        bmc_pass: "user-pass" <7>
        storage_disk: <8>
          - sdb
          - sdc
          - sde
          - sdd
      master1:
        nic_ext_dhcp: eno4
        nic_int_static: eno5
        mac_ext_dhcp: "aa:ss:dd:ee:b0:11"
        mac_int_static: "aa:ss:dd:ee:b1:11"
        bmc_url: "<url bmc>"
        bmc_user: "user-bmc"
        bmc_pass: "user-pass"
        storage_disk:
          - sdb
          - sdc
          - sde
          - sdd
      master2:
        nic_ext_dhcp: eno4
        nic_int_static: eno5
        mac_ext_dhcp: "aa:ss:dd:ee:b0:12"
        mac_int_static: "aa:ss:dd:ee:b1:12"
        bmc_url: "<url bmc>"
        bmc_user: "user-bmc"
        bmc_pass: "user-pass"
        storage_disk:
          - sdb
          - sdc
          - sde
          - sdd
      worker0:
        nic_ext_dhcp: eno4
        nic_int_static: eno5
        mac_ext_dhcp: "aa:ss:dd:ee:b0:19"
        mac_int_static: "aa:ss:dd:ee:b1:19"
        bmc_url: "<url bmc>"
        bmc_user: "user-bmc"
        bmc_pass: "user-pass"
  - spoke2-name:
      master0:
        nic_ext_dhcp: eno4
        nic_int_static: eno5
        mac_ext_dhcp: "aa:ss:dd:ee:b0:20"
        mac_int_static: "aa:ss:dd:ee:b1:20"
        bmc_url: "<url bmc>"
        bmc_user: "user-bmc"
        bmc_pass: "user-pass"
        storage_disk:
          - sdb
          - sdc
          - sde
          - sdd
      master1:
        nic_ext_dhcp: eno4
        nic_int_static: eno5
        mac_ext_dhcp: "aa:ss:dd:ee:b0:21"
        mac_int_static: "aa:ss:dd:ee:b1:21"
        bmc_url: "<url bmc>"
        bmc_user: "user-bmc"
        bmc_pass: "user-pass"
        storage_disk:
          - sdb
          - sdc
          - sde
          - sdd
      master2:
        nic_ext_dhcp: eno4
        nic_int_static: eno5
        mac_ext_dhcp: "aa:ss:dd:ee:b0:22"
        mac_int_static: "aa:ss:dd:ee:b1:22"
        bmc_url: "<url bmc>"
        bmc_user: "user-bmc"
        bmc_pass: "user-pass"
        storage_disk:
          - sdb
          - sdc
          - sde
          - sdd
      worker0:
        nic_ext_dhcp: eno4
        nic_int_static: eno5
        mac_ext_dhcp: "aa:ss:dd:ee:b0:29"
        mac_int_static: "aa:ss:dd:ee:b1:29"
        bmc_url: "<url bmc>"
        bmc_user: "user-bmc"
        bmc_pass: "user-pass"
----
<1> External interface name connected to the factory
<2> Internal interface name which is connected to the cluster router
<3> External MAC address for the factory 
<4> Internal MAC address for the internal router 
<5> The address field for each ``bmc`` entry is a URL for connecting to the {product-title} cluster nodes, including the type of controller in the URL scheme and its location on the network.
<6> The BMC username
<7> The BMC password
<8> The disks used for the storage cluster under {product-title} Data Foundation

. Set the following environment variable:
+
[source,terminal]
----
$ export KUBECONFIG=<path_to_kubeconfig>/kubeconfig-file
----

. Start the spoke cluster pipeline from the command line: 
+
[source,terminal]
----
$ tkn pipeline start -n spoke-deployer -p spokes-config="$(cat /path-to-spoke-yaml/spoke.yaml)" -p kubeconfig=${KUBECONFIG} -w=ztp,claimName=ztp-pvc --timeout 5h --use-param-defaults deploy-ztp-spokes
----
+
[NOTE]
====
This command starts the pipeline in the namespace `spoke-deployer` with the defined spokes configuration and the kube configuration in the workspace ztp with the previously configured persistent storage claim ztp-pvc. A timeout of 5 hours is set for the execution of the `deploy-ztp-hub` with all other parameters set to the default.
====
+
.Expected output
+
[source,terminal]
----
PipelineRun started: deploy-ztp-spoke-run-2rklt

In order to track the PipelineRun progress run: 
tkn pipeline logs deploy-ztp-spoke-run-2rklt -f -n spoke-deployer
----

.Monitoring the deployment

You can monitor the progress of the pipelines using the {product-version} web console and using the deployment log file. 

. Check the logs to monitor the progress of the `deploy-ztp-spokes`. 
+
[source,terminal]
----
tkn pipeline logs deploy-ztp-spoke-run-2rklt -f -n spoke-deployer
----
. Log in to the {product-title} web console.
. Navigate to *Pipelines* -> *Pipleines* and select the Project *spoke-deployer*. 
[NOTE]
====
The spoke-deployer pipeline stores all the artefacts for {product-version} Pipelines. 
====
. Select **PipelineRuns** to drill down into the details of the pipeline runs. 

. The stages of the pipeline are clearly shown and you can select each in turn to view the logs associated with that stage of the deployment. 