// Module included in the following assemblies:
//
// * storage/container_storage_interface/persistent-storage-csi-gcp-pd.adoc

:_mod-docs-content-type: PROCEDURE
[id="persistent-storage-csi-gcp-hyperdisk-storage-pools-procedure_{context}"]
= Setting up hyperdisk-balanced disks

.Prerequisites
* Access to the cluster with administrative privileges

.Procedure
Complete the following steps to set up hyperdisk-balanced disks:

ifdef::openshift-dedicated[]
. Create an {product-title} cluster on {GCP} with attached disks provisioned with hyperdisk-balanced disks. This can be achieved by provisioning the cluster with compute node types that support hyperdisk-balanced disks, such as the C3 and N4 machine series from {GCP}.
. Once the OSD cluster is ready, navigate to the **OpenShift console** for Storage Class creation.
Within the console, navigate to the **Storage** section to create a Storage Class specifying the hyperdisk-balanced disk:
+
.Example StorageClass YAML file
[source, yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
 name: hyperdisk-sc <1>
 annotations:
   storageclass.kubernetes.io/is-default-class: 'true'
provisioner: pd.csi.storage.gke.io <2>
parameters:
 replication-type: none
 storage-pools: projects/myproject/zones/us-east1-c/storagePools/hyperdisk-storagepool <3>
 type: hyperdisk-balanced <4>
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate
----
<1> Specify the name for your storage class. In this example, the name is `hyperdisk-sc`.
<2> Specify the GCP CSI provisioner as `pd.csi.storage.gke.io`.
<3> If using storage pools, specify a list of specific storage pools that you want to use in the following format: `projects/PROJECT_ID/zones/ZONE/storagePools/STORAGE_POOL_NAME`.
<4> Specify the disk type as `hyperdisk-balanced`.
+
[NOTE]
====
If you use storage pools, you must first create a Hyperdisk Storage Pool of the type "Hyperdisk Balanced" in the Google Cloud console prior to referencing it in the OpenShift Storage Class. The Hyperdisk Storage Pool must be created in the same zone as the compute node supporting Hyperdisk is installed in the cluster configuration. For more information about creating a Hyperdisk Storage Pool, see link:https://cloud.google.com/compute/docs/disks/create-storage-pools#create-pool[Create a Hyperdisk Storage Pool] in the Google Cloud documentation.
====

endif::openshift-dedicated[]

ifndef::openshift-dedicated[]
. Create a GCP cluster with attached disks provisioned with hyperdisk-balanced disks.
endif::openshift-dedicated[]

ifndef::openshift-dedicated[]
. Create a storage class specifying the hyperdisk-balanced disks during installation:
endif::openshift-dedicated[]

ifndef::openshift-dedicated[]
.. Follow the procedure in the _Installing a cluster on GCP with customizations_ section.
+
For your install-config.yaml file, use the following example file:
+
.Example install-config YAML file
[source, yaml]
----
apiVersion: v1
metadata:
  name: ci-op-9976b7t2-8aa6b

sshKey: |
  XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
baseDomain: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
platform:
  gcp:
    projectID: XXXXXXXXXXXXXXXXXXXXXX
    region: us-central1
controlPlane:
  architecture: amd64
  name: master
  platform:
    gcp:
      type: n4-standard-4 <1>
      osDisk:
        diskType: hyperdisk-balanced <2>
        diskSizeGB: 200
  replicas: 3
compute:
- architecture: amd64
  name: worker
  replicas: 3
  platform:
    gcp:
      type: n4-standard-4 <1>
      osDisk:
        diskType: hyperdisk-balanced <2>
----
<1> Specifies the node type as n4-standard-4.
<2> Specifies the node has the root disk backed by hyperdisk-balanced disk type. All nodes in the cluster should use the same disk type, either hyperdisks-balanced or pd-*.
+
[NOTE]
====
All nodes in the cluster must support hyperdisk-balanced volumes. Clusters with mixed nodes are not supported, for example N2 and N3 using hyperdisk-balanced disks.
====
endif::openshift-dedicated[]

ifndef::openshift-dedicated[]
.. After step 3 in _Incorporating the Cloud Credential Operator utility manifests_ section, copy the following manifests into the manifests directory created by the installation program:
+
* cluster_csi_driver.yaml - specifies opting out of the default storage class creation
* storageclass.yaml - creates a hyperdisk-specific storage class
+
--
.Example cluster CSI driver YAML file
[source, yaml]
----
apiVersion: operator.openshift.io/v1
kind: "ClusterCSIDriver"
metadata:
  name: "pd.csi.storage.gke.io"
spec:
  logLevel: Normal
  managementState: Managed
  operatorLogLevel: Normal
  storageClassState: Unmanaged <1>
----
<1> Specifies disabling creation of the default {product-title} storage classes.
--
+
--
.Example storage class YAML file
[source, yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hyperdisk-sc <1>
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: pd.csi.storage.gke.io <2>
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete
parameters:
  type: hyperdisk-balanced <3>
  replication-type: none
  provisioned-throughput-on-create: "140Mi" <4>
  provisioned-iops-on-create: "3000" <5>
  storage-pools: projects/my-project/zones/us-east4-c/storagePools/pool-us-east4-c <6>
allowedTopologies: <7>
- matchLabelExpressions:
  - key: topology.kubernetes.io/zone
    values:
    - us-east4-c
...
----
<1> Specify the name for your storage class. In this example, it is `hyperdisk-sc`.
<2> `pd.csi.storage.gke.io` specifies GCP CSI provisioner.
<3> Specifies using hyperdisk-balanced disks.
<4> Specifies the throughput value in MiBps using the "Mi" qualifier. For example, if your required throughput is 250 MiBps, specify "250Mi". If you do not specify a value, the capacity is based upon the disk type default.
<5> Specifies the IOPS value without any qualifiers. For example, if you require 7,000 IOPS, specify "7000". If you do not specify a value, the capacity is based upon the disk type default.
<6> If using storage pools, specify a list of specific storage pools that you want to use in the format: projects/PROJECT_ID/zones/ZONE/storagePools/STORAGE_POOL_NAME.
<7> If using storage pools, set `allowedTopologies` to restrict the topology of provisioned volumes to where the storage pool exists. In this example, `us-east4-c`.
--
endif::openshift-dedicated[]

. Create a persistent volume claim (PVC) that uses the hyperdisk-specific storage class using the following example YAML file:
+
.Example PVC YAML file
[source, yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  storageClassName: hyperdisk-sc <1>
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2048Gi <2>
----
<1> PVC references the storage pool-specific storage class. In this example, `hyperdisk-sc`.
<2> Target storage capacity of the hyperdisk-balanced volume. In this example, `2048Gi`.

. Create a deployment that uses the PVC that you just created. Using a deployment helps ensure that your application has access to the persistent storage even after the pod restarts and rescheduling:

.. Ensure a node pool with the specified machine series is up and running before creating the deployment. Otherwise, the pod fails to schedule.

.. Use the following example YAML file to create the deployment:
+
.Example deployment YAML file
[source, yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
spec:
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      nodeSelector:
        cloud.google.com/machine-family: n4 <1>
      containers:
      - name: postgres
        image: postgres:14-alpine
        args: [ "sleep", "3600" ]
        volumeMounts:
        - name: sdk-volume
          mountPath: /usr/share/data/
      volumes:
      - name: sdk-volume
        persistentVolumeClaim:
          claimName: my-pvc <2>
----
<1> Specifies the machine family. In this example, it is `n4`.
<2> Specifies the name of the PVC created in the preceding step. In this example, it is `my-pfc`.

.. Confirm that the deployment was successfully created by running the following command:
+
[source, terminal]
----
$ oc get deployment
----
+
.Example output
[source, terminal]
----
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
postgres   0/1     1            0           42s
----
+
It might take a few minutes for hyperdisk instances to complete provisioning and display a READY status.

.. Confirm that PVC `my-pvc` has been successfully bound to a persistent volume (PV) by running the following command:
+
[source, terminal]
----
$ oc get pvc my-pvc
----
+
.Example output
+
[source, terminal]
----
NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       VOLUMEATTRIBUTESCLASS  AGE
my-pvc        Bound    pvc-1ff52479-4c81-4481-aa1d-b21c8f8860c6   2Ti        RWO            hyperdisk-sc       <unset>                2m24s
----

.. Confirm the expected configuration of your hyperdisk-balanced disk:
+
[source, terminal]
----
$ gcloud compute disks list
----
+
.Example output
+
[source, terminal]
----
NAME                                        LOCATION        LOCATION_SCOPE  SIZE_GB  TYPE                STATUS
instance-20240914-173145-boot               us-central1-a   zone            150      pd-standard         READY
instance-20240914-173145-data-workspace     us-central1-a   zone            100      pd-balanced         READY
c4a-rhel-vm                                 us-central1-a   zone            50       hyperdisk-balanced  READY <1>
----
<1> Hyperdisk-balanced disk.

.. If using storage pools, check that the volume is provisioned as specified in your storage class and PVC by running the following command:
+
[source, terminal]
----
$ gcloud compute storage-pools list-disks pool-us-east4-c --zone=us-east4-c
----
+
.Example output
+
[source, terminal]
----
NAME                                      STATUS  PROVISIONED_IOPS  PROVISIONED_THROUGHPUT  SIZE_GB
pvc-1ff52479-4c81-4481-aa1d-b21c8f8860c6  READY   3000              140                     2048
----
