// Module included in the following assemblies:
//
// * logging/efk-logging-elasticsearch.adoc

[id='efk-logging-elasticsearch-exposing_{context}']
= Exposing Elasticsearch as a route

By default, Elasticsearch deployed with OpenShift aggregated logging is not
accessible from outside the logging cluster. You can enable a route for external
access to Elasticsearch for those tools that want to access its data.

You have access to Elasticsearch using your OpenShift token, and
you can provide the external Elasticsearch and Elasticsearch Ops
hostnames when creating the server certificate (similar to Kibana).

.Procedure

. To access Elasticsearch as a reencrypt route, define the following variables:
+
----
openshift_logging_es_allow_external=True
openshift_logging_es_hostname=elasticsearch.example.com
----

. Change to the playbook directory and run the following Ansible playbook:
+
----
$ cd /usr/share/ansible/openshift-ansible
$ ansible-playbook [-i </path/to/inventory>] \
    playbooks/openshift-logging/config.yml
----

. To log in to Elasticsearch remotely, the request must contain three HTTP headers:
+
----
Authorization: Bearer $token
X-Proxy-Remote-User: $username
X-Forwarded-For: $ip_address
----

. You must have access to the project in order to be able to access to the
logs. For example:
+
----
$ oc login <user1>
$ oc new-project <user1project>
$ oc new-app <httpd-example>
----

. You need to get the token of this ServiceAccount to be used in the request:
+
----
$ token=$(oc whoami -t)
----

. Using the token previously configured, you should be able access Elasticsearch
through the exposed route:
+
----
$ curl -k -H "Authorization: Bearer $token" -H "X-Proxy-Remote-User: $(oc whoami)" -H "X-Forwarded-For: 127.0.0.1" https://es.example.test/project.my-project.*/_search?q=level:err | python -mjson.tool
----
