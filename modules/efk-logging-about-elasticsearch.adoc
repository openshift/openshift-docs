// Module included in the following assemblies:
//
// * logging/efk-logging.adoc

[id='efk-logging-about-elasticsearch_{context}']
= About Elasticsearch in {product-title} 

{product-title} uses link:https://www.elastic.co/products/elasticsearch[Elasticsearch (ES)] to organize the log data from Fleutnd into datastores, or _indices_. 

Elasticsearch subdivides each index into multiple pieces called _shards_, which it spreads across a set of Elasticsearch nodes in your cluster.
You can configure Elasticsearch to make copies of the shards, called _replicas_. Elasticsearch also spreads these replicas across
the Elactisearch nodes. The *ClusterLogging* object allows  you to specify the replication policy in the custom resource definition (CRD) to provide data redundancy and resilience to failure.

The data redundancy policy defines how shards are replicated across data nodes in the cluster.

* *FullRedundancy.* The shards for each index are fully replicated to every data node.

* *MultipleRedundancy.*|The shards for each index are spread over half of the data nodes.

* *SingleRedundancy.* A single copy of each shard. Logs are always available and recoverable as long as at least two data nodes exist.

* *ZeroRedundancy.* No copies of any shards.  Logs may be unavailable (or lost) in the event a node is down or fails|

The {product-title} logging installer ensures each Elasticsearch node is deployed using a unique deployment configuration that includes its own storage volume.
During installation, you can use the `openshift_logging_es_cluster_size` Ansible variable to specify the number of Elasticsearch nodes.
You can scale Elasticsearch by creating an additional deployment configuration for each Elasticsearch node you add to the logging system.
Or, you can scale up your existing cluster by modifying the
`openshift_logging_es_cluster_size` in the inventory file and re-running the
logging playbook. Additional clustering parameters can be modified and are
described in *Specifying Logging Ansible Variables*.

You can scale the Elastisearch cluster by adjusting the `nodeCount` parameter in the *ClusterLogging* CRD.

The following example specifies a 3 node cluster where each data node is bound to a `PersistentVolumeClaim` 
that requests `200G` of AWS general purpose storage:

----
spec:
  logStore:
    type: "elasticsearch"
    elasticsearch:
      nodeCount: 3 <1>
      storage:
        storageClass: <2>
          name: "gp2"
          size: "200G"
----

<1> Specify the number of nodes in the Elasticsearch cluster
<2> Specify a PVC for the node storage. 

Refer to
link:https://www.elastic.co/guide/en/elasticsearch/guide/current/hardware.html[Elastic's
documentation] for considerations involved in choosing storage and
network location as directed below.

//tag::elasticsearch-ha[]
[NOTE]
====
A highly-available Elasticsearch environment requires at least three Elasticsearch nodes,
each on a different host, and setting the `openshift_logging_es_number_of_replicas` Ansible variable
to a value of `1` or higher to create replicas.
====
//end::elasticsearch-ha[]

For more information, see https://www.elastic.co/products/elasticsearch[Elasticsearch (ES)].
