// Module included in the following assemblies:
//
// * operators/user/das-operator-installing.adoc

:_mod-docs-content-type: PROCEDURE
[id="das-operator-installing-web-console_{context}"]
= Installing the DAS Operator using the web console

As a cluster administrator, you can install the Dynamic Accelerator Slicer (DAS) Operator using the {product-title} web console.

.Prerequisites

* You have access to an {product-title} cluster using an account with `cluster-admin` permissions.
* You have installed the required prerequisites:
** xref:../../security/cert_manager_operator/cert-manager-operator-install.adoc#cert-manager-operator-install[{cert-manager-operator}]
** xref:../../hardware_enablement/psap-node-feature-discovery-operator.adoc#psap-node-feature-discovery-operator[Node Feature Discovery (NFD) Operator]
** link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator]
* You have configured the NVIDIA GPU Operator for MIG support as described in the following procedure.

.Procedure

. Configure the NVIDIA GPU Operator for MIG support:

.. After installing the NVIDIA GPU Operator, navigate to *Operators* -> *Installed Operators* in the {product-title} web console.

.. Select the *NVIDIA GPU Operator* from the list of installed operators.

.. Click the *ClusterPolicy* tab and then click *Create ClusterPolicy*.

.. In the YAML editor, replace the default content with the following cluster policy configuration to disable the default NVIDIA device plugin and enable MIG support:
+
[source,yaml]
----
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  daemonsets:
    rollingUpdate:
      maxUnavailable: "1"
    updateStrategy: RollingUpdate
  dcgm:
    enabled: true
  dcgmExporter:
    config:
      name: ""
    enabled: true
    serviceMonitor:
      enabled: true
  devicePlugin:
    config:
      default: ""
      name: ""
    enabled: false
    mps:
      root: /run/nvidia/mps
  driver:
    certConfig:
      name: ""
    enabled: true
    kernelModuleConfig:
      name: ""
    licensingConfig:
      configMapName: ""
      nlsEnabled: true
    repoConfig:
      configMapName: ""
    upgradePolicy:
      autoUpgrade: true
      drain:
        deleteEmptyDir: false
        enable: false
        force: false
        timeoutSeconds: 300
      maxParallelUpgrades: 1
      maxUnavailable: 25%
      podDeletion:
        deleteEmptyDir: false
        force: false
        timeoutSeconds: 300
      waitForCompletion:
        timeoutSeconds: 0
    useNvidiaDriverCRD: false
    useOpenKernelModules: false
    virtualTopology:
      config: ""
  gdrcopy:
    enabled: false
  gds:
    enabled: false
  gfd:
    enabled: true
  mig:
    strategy: mixed
  migManager:
    config:
      default: ""
      name: default-mig-parted-config
    enabled: true
    env:
      - name: WITH_REBOOT
        value: 'true'
      - name: MIG_PARTED_MODE_CHANGE_ONLY
        value: 'true'    
  nodeStatusExporter:
    enabled: true
  operator:
    defaultRuntime: crio
    initContainer: {}
    runtimeClass: nvidia
    use_ocp_driver_toolkit: true
  sandboxDevicePlugin:
    enabled: true
  sandboxWorkloads:
    defaultWorkload: container
    enabled: false
  toolkit:
    enabled: true
    installDir: /usr/local/nvidia
  validator:
    plugin:
      env:
      - name: WITH_WORKLOAD
        value: "false"
    cuda:
      env:
      - name: WITH_WORKLOAD
        value: "false"
  vfioManager:
    enabled: true
  vgpuDeviceManager:
    enabled: true
  vgpuManager:
    enabled: false
----

.. Click *Create* to apply the cluster policy.

.. Navigate to *Workloads* -> *Pods* and select the `nvidia-gpu-operator` namespace to monitor the cluster policy deployment.

.. Wait for the NVIDIA GPU Operator cluster policy to reach the `Ready` state. You can monitor this by:
+
... Navigating to *Operators* -> *Installed Operators* -> *NVIDIA GPU Operator*.
... Clicking the *ClusterPolicy* tab and checking that the status shows `ready`.

.. Verify that all pods in the NVIDIA GPU Operator namespace are running by checking the *Workloads* -> *Pods* page with the `nvidia-gpu-operator` namespace selected.

.. Label nodes with MIG-capable GPUs to enable MIG mode:
+
... Navigate to *Compute* -> *Nodes*.
... Select a node that has MIG-capable GPUs.
... Click *Actions* -> *Edit Labels*.
... Add the label `nvidia.com/mig.config=all-enabled`.
... Click *Save*.
... Repeat for each node with MIG-capable GPUs.
+
[IMPORTANT]
====
After applying the MIG label, the labeled nodes will reboot to enable MIG mode. Wait for the nodes to come back online before proceeding.
====

.. Verify that the nodes have successfully enabled MIG mode by checking that the nodes show the `nvidia.com/mig.config=all-enabled` label in the *Compute* -> *Nodes* page.

.

. In the {product-title} web console, click *Operators* -> *OperatorHub*.

. Search for *Dynamic Accelerator Slicer* or *DAS* in the filter box to locate the DAS Operator.

. Select the *Dynamic Accelerator Slicer (DAS) Operator* and click *Install*.

. On the *Install Operator* page:
.. Select *A specific namespace on the cluster* for the installation mode.
.. For *Installed Namespace*, select *Create new namespace* or select an existing namespace.
.. If creating a new namespace, enter `das-operator` as the namespace name.
.. Select an update channel.
.. Select *Automatic* or *Manual* for the approval strategy.

. Click *Install*.

.Verification

To verify that the DAS Operator installed successfully:

. Navigate to the *Operators* -> *Installed Operators* page.
. Ensure that *Dynamic Accelerator Slicer (DAS) Operator* is listed in the `das-operator` namespace with a *Status* of *InstallSucceeded*.

[NOTE]
====
During installation an Operator might display a *Failed* status. If the installation later succeeds with an *InstallSucceeded* message, you can ignore the *Failed* message.
====

You can also verify the installation by checking the pods:

. Navigate to the *Workloads* -> *Pods* page and select the `das-operator` namespace.
. Verify that all DAS Operator component pods are running:
** `das-operator` pods (main operator controllers)
** `das-operator-webhook` pods (webhook servers)
** `das-scheduler` pods (scheduler plugins)
** `das-daemonset` pods (only on nodes with MIG-compatible GPUs)

[NOTE]
====
The `das-daemonset` pods will only appear on nodes that have MIG-compatible GPU hardware. If you do not see any daemonset pods, verify that your cluster has nodes with supported GPU hardware and that the NVIDIA GPU Operator is properly configured.
====

If the Operator does not appear as installed, troubleshoot further:

. Navigate to the *Operators* -> *Installed Operators* page and inspect the *Operator Subscriptions* and *Install Plans* tabs for any failure or errors under *Status*.
. Navigate to the *Workloads* -> *Pods* page and check the logs for pods in the `das-operator` namespace.