// Module included in the following assemblies:
//
// * logging/cluster-logging-elasticsearch.adoc

[id="cluster-logging-elasticsearch-limits_{context}"]
= Configuring Elasticsearch CPU and memory requests

Each component specification allows for adjustments to both the CPU and memory requests.
You should not have to manually adjust these values as the Elasticsearch
Operator sets values sufficient for your environment.

Each Elasticsearch node can operate with a lower memory setting though this is *not* recommended for production deployments. 
For production use, you should have no less than the default 16Gi allocated to each pod. Preferably you should allocate as much as possible, up to 64Gi per pod.

.Prerequisites

* Cluster logging and Elasticsearch must be installed.

.Procedure

. Edit the `ClusterLogging` custom resource (CR) in the `openshift-logging` project:
+
----
$ oc edit ClusterLogging instance
----
+
[source,yaml]
----
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "instance"
....
spec:
    logStore:
      type: "elasticsearch"
      elasticsearch:
        resources: <1>
          limits:
            memory: 16Gi
          requests:
            cpu: 500m
            memory: 16Gi
----
<1> Specify the CPU and memory requests as needed. If you leave these values blank,
the Elasticsearch Operator sets default values that should be sufficient for most deployments.
+
If you adjust the amount of Elasticsearch memory, you must change both the request value and the limit value. 
+
For example:
+
[source,yaml]
----
      resources:
        limits:
          memory: "32Gi"
        requests:
          cpu: "8"
          memory: "32Gi"
----
+
Kubernetes generally adheres the node configuration and does not allow Elasticsearch to use the specified limits. 
Setting the same value for the `requests` and `limits` ensures that Elasticsearch can use the memory you want, assuming the node has the CPU and memory available. 

