// Module included in the following assemblies:
//
// * logging/cluster-logging-elasticsearch.adoc

[id="cluster-logging-elasticsearch-limits_{context}"]
= Configuring Elasticsearch CPU and memory limits

Each component specification allows for adjustments to both the CPU and memory limits.
You should not have to manually adjust these values as the Elasticsearch
Operator sets values sufficient for your environment.

.Prerequisites

* Cluster logging and Elasticsearch must be installed.

.Procedure

. Edit the Cluster Logging Custom Resource (CR) in the `openshift-logging` project:
+
----
$ oc edit ClusterLogging instance
----
+
[source,yaml]
----
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "instance"
....
spec:
    logStore:
      type: "elasticsearch"
      elasticsearch:
        resources: <1>
          limits:
            cpu: "4000m"
            memory: "4Gi"
          requests:
            cpu: "4000m"
            memory: "1Gi"
----
<1> Specify the CPU and memory limits as needed. If you leave these values blank,
the Elasticsearch Operator sets default values that should be sufficient for most deployments.
+
If you adjust the amount of Elasticsearch CPU and memory, you must change both the request value and the limit value. 
+
For example:
+
[source,yaml]
----
      resources:
        limits:
          cpu: "8"
          memory: "32Gi"
        requests:
          cpu: "8"
          memory: "32Gi"
----
+
Kubernetes generally adheres the node CPU configuration and DOES not allow Elasticsearch to use the specified limits. 
Setting the same value for the `requests` and `limits` ensures that Elasticseach can use the CPU and memory you want, assuming the node has the CPU and memory available.
