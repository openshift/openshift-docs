:_mod-docs-content-type: PROCEDURE
[id="cluster-logging-collector-log-forward-project_{context}"]
= Forwarding application logs from specific projects

You can use the Cluster Log Forwarder to send a copy of the application logs from specific projects to an external log aggregator. You can do this in addition to, or instead of, using the default Elasticsearch log store. You must also configure the external log aggregator to receive log data from {product-title}.

To configure forwarding application logs from a project, you must create a `ClusterLogForwarder` custom resource (CR) with at least one input from a project, optional outputs for other log aggregators, and pipelines that use those inputs and outputs.

.Prerequisites

* You must have a logging server that is configured to receive the logging data using the specified protocol or format.

.Procedure

. Create or edit a YAML file that defines the `ClusterLogForwarder` CR object:
+
[source,yaml]
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance <1>
  namespace: openshift-logging <2>
spec:
  outputs:
   - name: fluentd-server-secure <3>
     type: fluentdForward <4>
     url: 'tls://fluentdserver.security.example.com:24224' <5>
     secret: <6>
        name: fluentd-secret
   - name: fluentd-server-insecure
     type: fluentdForward
     url: 'tcp://fluentdserver.home.example.com:24224'
  inputs: <7>
   - name: my-app-logs
     application:
        namespaces:
        - my-project
  pipelines:
   - name: forward-to-fluentd-insecure <8>
     inputRefs: <9>
     - my-app-logs
     outputRefs: <10>
     - fluentd-server-insecure
     labels:
       project: "my-project" <11>
   - name: forward-to-fluentd-secure <12>
     inputRefs:
     - application
     - audit
     - infrastructure
     outputRefs:
     - fluentd-server-secure
     - default
     labels:
       clusterId: "C1234"
----
<1> The name of the `ClusterLogForwarder` CR must be `instance`.
<2> The namespace for the `ClusterLogForwarder` CR must be `openshift-logging`.
<3> Specify a name for the output.
<4> Specify the output type: `elasticsearch`, `fluentdForward`, `syslog`, or `kafka`.
<5> Specify the URL and port of the external log aggregator as a valid absolute URL. If the cluster-wide proxy using the CIDR annotation is enabled, the output must be a server name or FQDN, not an IP address.
<6> If using a `tls` prefix, you must specify the name of the secret required by the endpoint for TLS communication. The secret must exist in the `openshift-logging` project and have *tls.crt*, *tls.key*, and *ca-bundle.crt* keys that each point to the certificates they represent.
<7> Configuration for an input to filter application logs from the specified projects.
<8> Configuration for a pipeline to use the input to send project application logs to an external Fluentd instance.
<9> The `my-app-logs` input.
<10> The name of the output to use.
<11> Optional: String. One or more labels to add to the logs.
<12> Configuration for a pipeline to send logs to other log aggregators.
** Optional: Specify a name for the pipeline.
** Specify which log types to forward by using the pipeline: `application,` `infrastructure`, or `audit`.
** Specify the name of the output to use when forwarding logs with this pipeline.
** Optional: Specify the `default` output to forward logs to the internal Elasticsearch instance.
** Optional: String. One or more labels to add to the logs.

. Create the CR object:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
