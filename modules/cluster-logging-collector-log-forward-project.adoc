// Module included in the following assemblies:
//
// * logging/cluster-logging-external.adoc

[id="cluster-logging-collector-log-forward-project_{context}"]
= Forwarding application logs from specific projects

You can use the Cluster Log Forwarder to send a copy of the application logs from specific projects to an external log aggregator. You can do this in addition to, or instead of, using the default Elasticsearch log store. You must also configure the external log aggregator to receive log data from {product-title}.

To configure forwarding application logs from a project, create a `ClusterLogForwarder` custom resource (CR) with at least one input from a project, optional outputs for other log aggregators, and pipelines that use those inputs and outputs.

.Prerequisites

* You must have a logging server that is configured to receive the logging data using the specified protocol or format.

.Procedure

. Create a `ClusterLogForwarder` CR YAML file similar to the following:
+
[source,yaml]
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance <1>
  namespace: openshift-logging <2>
spec:
  outputs:
   - name: fluentd-server-secure <3>
     type: fluentdForward <4>
     url: 'tls://fluentdserver.security.example.com:24224' <5>
     secret: <6>
        name: fluentd-secret
   - name: fluentd-server-insecure
     type: fluentdForward
     url: 'tcp://fluentdserver.home.example.com:24224'
  inputs: <7>
   - name: my-app-logs
     application:
        namespaces:
        - my-project
  pipelines:
   - name: forward-to-fluentd-insecure <8>
     inputRefs: <9>
     - my-app-logs
     outputRefs: <10>
     - fluentd-server-insecure
     parse: json <11>
     labels:
       project: "my-project" <12>
   - name: forward-to-fluentd-secure <13>
     inputRefs:
     - application
     - audit
     - infrastructure
     outputRefs:
     - fluentd-server-secure
     - default
     labels:
       clusterId: "C1234"
----
<1> The name of the `ClusterLogForwarder` CR must be `instance`.
<2> The namespace for the `ClusterLogForwarder` CR must be `openshift-logging`.
<3> Specify a name for the output.
<4> Specify the output type: `elasticsearch`, `fluentdForward`, `syslog`, or `kafka`.
<5> Specify the URL and port of the external log aggregator as a valid absolute URL. If the cluster-wide proxy using the CIDR annotation is enabled, the output must be a server name or FQDN, not an IP address.
<6> If using a `tls` prefix, you must specify the name of the secret required by the endpoint for TLS communication. The secret must exist in the `openshift-logging` project and have *tls.crt*, *tls.key*, and *ca-bundle.crt* keys that each point to the certificates they represent.
<7> Configuration for an input to filter application logs from the specified projects.
<8> Configuration for a pipeline to use the input to send project application logs to an external Fluentd instance.
<9> The `my-app-logs` input.
<10> The name of the output to use.
<11> Optional: Forward structured JSON log entries as JSON objects in the `structured` field. The log entry must contain valid structured JSON; otherwise, OpenShift Logging removes the `structured` field and instead sends the log entry to the default index, `app-00000x`.
<12> Optional: String. One or more labels to add to the logs.
<13> Configuration for a pipeline to send logs to other log aggregators.
** Optional: Specify a name for the pipeline.
** Specify which log types should be forwarded using that pipeline: `application,` `infrastructure`, or `audit`.
** Specify the output to use with that pipeline for forwarding the logs.
** Optional: Specify the `default` output to forward logs to the internal Elasticsearch instance.
** Optional: String. One or more labels to add to the logs.

. Create the CR object:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
