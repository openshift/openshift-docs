// Module included in the following assemblies:
//
// * logging/cluster-logging-external.adoc

[id="cluster-logging-collector-log-forward-project_{context}"]
= Forwarding application logs from specific projects

You can use the Log forwarding API to send a copy of the application logs from specific projects to an external log aggregator instead of, or in addition to, the default Elasticsearch log store. You are responsible for configuring the external log aggregator to receive the logs from {product-title}.

To configure forwarding application logs from a project, create a `ClusterLogForwarder` custom resource (CR) with at least one input from a project, optional outputs for other log aggregators, and pipelines that use those inputs and outputs. 

.Procedure

. Create a `ClusterLogForwarder` CR YAML file similar to the following:
+
[source,yaml]
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance <1>
  namespace: openshift-logging <2>
spec:
  outputs:
   - name: fluentd-server-secure <3>
     type: fluentdForward <4>
     url: 'tls://fluentdserver.security.example.com:24224' <5>
     secret: <6>
        name: fluentd-secret
   - name: fluentd-server-inecure
     type: fluentdForward
     url: 'tcp://fluentdserver.home.example.com:24224'
  inputs: <7>
   - name: my-app-logs
     application:
        namespaces:
        - my-project
  pipelines:
   - name: forward-to-fluentd-insecure <8>
     inputRefs: <9>
     - my-app-logs
     outputRefs: <10>
     - fluentd-server-insecure
     labels: <11>
       project: my-project
   - name: forward-to-fluentd-secure <12>
     inputRefs:
     - application
     - audit
     - infrastructure
     outputRefs:
     - fluentd-server-secure
     - default
     labels:
       clusterId: C1234
----
<1> The name of the `ClusterLogForwarder` CR must be `instance`.
<2> The namespace for the log forwarding CR must be `openshift-logging`.
<3> Specify a name for the output.
<4> Specify the output type: `elasticsearch`, `fluentdForward`, `syslog`, or `kafka`.
<5> Specify the URL and port of the external log aggregator as a valid absolute URL. If the cluster-wide proxy using the CIDR annotation is enabled, the output must be a server name or FQDN, not an IP address.
<6> If using a `tls` prefix, you must specify the name of the secret required by the endpoint for TLS communication. The secret must exist in the `openshift-logging` project and must have keys of: *tls.crt*, *tls.key*, and *ca-bundle.crt* that point to the respective certificates that they represent.
<7> Configuration for an input to filter application logs from the specified projects.
<8> Configuration for a pipeline to use the input to send project application logs to an external Fluentd instance.
<9> The `my-app-logs` input.
<10 The name of the output to use.
<11> Optional: A label to add to the logs.
<12> Configuration for a pipeline to send logs to other log aggregators.
** Optional: Specify a name for the pipeline.
** Specify which log types should be forwarded using that pipeline: `application,` `infrastructure`, or `audit`.
** Specify the output to use with that pipeline for forwarding the logs.
** Optional: Specify the `default` output to forward logs to the internal Elasticsearch instance.
** Optional: One or more labels to add to the logs.

. Create the CR object:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
