// Module included in the following assemblies:
// hosted_control_planes/hcp-manage/hcp-manage-aws.adoc

:_mod-docs-content-type: PROCEDURE
[id="hcp-np-capacity-blocks_{context}"]
= Configuring node pool capacity blocks on {aws-short}

After creating a hosted cluster, you can configure node pool capacity blocks for graphics processing unit (GPU) reservations on {aws-first}.

.Procedure

. Create GPU reservations on {aws-short} by running the following command:
+
[IMPORTANT]
====
The zone of the GPU reservation must match your hosted cluster zone.
====
+
[source,terminal]
----
$ aws ec2 describe-capacity-block-offerings \
      --instance-type "p4d.24xlarge"\ #<1>
      --instance-count  "1" \ #<2>
      --start-date-range "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"  \ #<3>
      --end-date-range "$(date -u -d "2 day" +"%Y-%m-%dT%H:%M:%SZ")" \ #<4>
      --capacity-duration-hours 24 \ #<5>
      --output json
----
<1> Defines the type of your {aws-short} instance, for example, `p4d.24xlarge`.
<2> Defines your instance purchase quantity, for example, `1`. Valid values are integers ranging from `1` to `64`.
<3> Defines the start date range, for example, `2025-07-21T10:14:39Z`.
<4> Defines the end date range, for example, `2025-07-22T10:16:36Z`.
<5> Defines the duration of capacity blocks in hours, for example, `24`.

. Purchase the minimum fee capacity block by running the following command:
+
[source,terminal]
----
$ aws ec2 purchase-capacity-block \
      --capacity-block-offering-id "${MIN_FEE_ID}" \ #<1>
      --instance-platform "Linux/UNIX"\ #<2>
      --tag-specifications 'ResourceType=capacity-reservation,Tags=[{Key=usage-cluster-type,Value=hypershift-hosted}]' \ #<3>
      --output json   > "${CR_OUTPUT_FILE}"
----
<1> Defines the ID of the capacity block offering.
<2> Defines the platform of your instance.
<3> Defines the tag for your instance.

. Create an environment variable to set the capacity reservation ID by running the following command:
+
[source,terminal]
----
$ CB_RESERVATION_ID=$(jq -r '.CapacityReservation.CapacityReservationId' "${CR_OUTPUT_FILE}")
----
+
Wait for a couple of minutes for the GPU reservation to become available.

. Add a node pool to use the GPU reservation by running the following command:
+
[source,terminal]
----
$ hcp create nodepool aws \
  --cluster-name <hosted_cluster_name> \ #<1>
  --name <node_pool_name> \ #<2>
  --node-count 1 \ #<3>
  --instance-type p4d.24xlarge \ #<4>
  --arch amd64 \ #<5>
  --release-image <release_image> \ #<6>
  --render > /tmp/np.yaml
----
<1> Replace `<hosted_cluster_name>` with the name of your hosted cluster.
<2> Replace `<node_pool_name>` with the name of your node pool.
<3> Defines the node pool count, for example, `1`.
<4> Defines the instance type, for example, `p4d.24xlarge`.
<5> Defines an architecture type, for example, `amd64`.
<6> Replace `<release_image>` with the release image you want to use.

. Add the `capacityReservation` setting in your `NodePool` resource by using the following example configuration:
+
[source,yaml]
----
# ...
spec:
  arch: amd64
  clusterName: cb-np-hcp
  management:
    autoRepair: false
    upgradeType: Replace
  platform:
    aws:
      instanceProfile: cb-np-hcp-dqppw-worker
      instanceType: p4d.24xlarge
      rootVolume:
        size: 120
        type: gp3
      subnet:
        id: subnet-00000
      placement:
        capacityReservation:
          id: ${CB_RESERVATION_ID}
          marketType: CapacityBlocks
    type: AWS
# ...
----

. Apply the node pool configuration by running the following command:
+
[source,terminal]
----
$ oc apply -f /tmp/np.yaml
----

.Verification

. Verify that your new node pool is created successfully by running the following command:
+
[source,terminal]
----
$ oc get np -n clusters
----
+
.Example output
[source,terminal]
----
NAMESPACE   NAME    CLUSTER     DESIRED NODES   CURRENT  NODES   AUTOSCALING     AUTOREPAIR   VERSION                               UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
clusters    cb-np   cb-np-hcp   1               1                False           False        4.20.0-0.nightly-2025-06-05-224220    False             False
----

. Verify that your new compute nodes are created in the hosted cluster by running the following command:
+
[source,terminal]
----
$ oc get nodes
----
+
.Example output
[source, terminal]
----
NAME                           STATUS   ROLES    AGE    VERSION
ip-10-0-132-74.ec2.internal    Ready    worker   17m    v1.33.4
ip-10-0-134-183.ec2.internal   Ready    worker   4h5m   v1.33.4
----
