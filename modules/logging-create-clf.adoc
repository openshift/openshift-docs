// Module included in the following assemblies:
//
// * logging/log_collection_forwarding/log-forwarding.adoc

:_mod-docs-content-type: REFERENCE
[id="logging-create-clf_{context}"]
= Creating a log forwarder

To create a log forwarder, you must create a `ClusterLogForwarder` CR that specifies the log input types that the service account can collect. You can also specify which outputs the logs can be forwarded to. If you are using the multi log forwarder feature, you must also reference the service account in the `ClusterLogForwarder` CR.

If you are using the multi log forwarder feature on your cluster, you can create `ClusterLogForwarder` custom resources (CRs) in any namespace, using any name.
If you are using a legacy implementation, the `ClusterLogForwarder` CR must be named `instance`, and must be created in the `openshift-logging` namespace.

[IMPORTANT]
====
You need administrator permissions for the namespace where you create the `ClusterLogForwarder` CR.
====

.ClusterLogForwarder resource example
[source,yaml]
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: <log_forwarder_name> <1>
  namespace: <log_forwarder_namespace> <2>
spec:
  serviceAccountName: <service_account_name> <3>
  pipelines:
   - inputRefs:
     - <log_type> <4>
     outputRefs:
     - <output_name> <5>
  outputs:
  - name: <output_name> <6>
    type: <output_type> <5>
    url: <log_output_url> <7>
# ...
----
<1> In legacy implementations, the CR name must be `instance`. In multi log forwarder implementations, you can use any name.
<2> In legacy implementations, the CR namespace must be `openshift-logging`. In multi log forwarder implementations, you can use any namespace.
<3> The name of your service account. The service account is only required in multi log forwarder implementations if the log forwarder is not deployed in the `openshift-logging` namespace.
<4> The log types that are collected. The value for this field can be `audit` for audit logs, `application` for application logs, `infrastructure` for infrastructure logs, or a named input that has been defined for your application.
<5> The type of output that you want to forward logs to. The value of this field can be `default`, `loki`, `kafka`, `elasticsearch`, `fluentdForward`, `syslog`, or `cloudwatch`.
+
[NOTE]
====
The `default` output type is not supported in mutli log forwarder implementations.
====
<6> A name for the output that you want to forward logs to.
<7> The URL of the output that you want to forward logs to.

// To be followed up on by adding input examples / docs:
////
spec:
  inputs:
  - name: chatty-app
    type: application
    selector:
        matchLabels:
          load: heavy
  pipelines:
  - inputRefs:
    - chatty-app
    - infrastructure
  - outputRefs:
    - default
////

The following example forwards the audit logs to a secure external Elasticsearch instance, the infrastructure logs to an insecure external Elasticsearch instance, the application logs to a Kafka broker, and the application logs from the `my-apps-logs` project to the internal Elasticsearch instance:

.Sample log forwarding outputs and pipelines
[source,yaml]
----
apiVersion: "logging.openshift.io/v1"
kind: ClusterLogForwarder
metadata:
  name: <log_forwarder_name> <1>
  namespace: <log_forwarder_namespace> <2>
spec:
  serviceAccountName: <service_account_name> <3>
  outputs:
   - name: elasticsearch-secure <4>
     type: "elasticsearch"
     url: https://elasticsearch.secure.com:9200
     secret:
        name: elasticsearch
   - name: elasticsearch-insecure <5>
     type: "elasticsearch"
     url: http://elasticsearch.insecure.com:9200
   - name: kafka-app <6>
     type: "kafka"
     url: tls://kafka.secure.com:9093/app-topic
  inputs: <7>
   - name: my-app-logs
     application:
        namespaces:
        - my-project
  pipelines:
   - name: audit-logs <8>
     inputRefs:
      - audit
     outputRefs:
      - elasticsearch-secure
      - default
     labels:
       secure: "true" <9>
       datacenter: "east"
   - name: infrastructure-logs <10>
     inputRefs:
      - infrastructure
     outputRefs:
      - elasticsearch-insecure
     labels:
       datacenter: "west"
   - name: my-app <11>
     inputRefs:
      - my-app-logs
     outputRefs:
      - default
   - inputRefs: <12>
      - application
     outputRefs:
      - kafka-app
     labels:
       datacenter: "south"
----
<1> In legacy implementations, the CR name must be `instance`. In multi log forwarder implementations, you can use any name.
<2> In legacy implementations, the CR namespace must be `openshift-logging`. In multi log forwarder implementations, you can use any namespace.
<3> The name of your service account. The service account is only required in multi log forwarder implementations if the log forwarder is not deployed in the `openshift-logging` namespace.
<4> Configuration for an secure Elasticsearch output using a secret with a secure URL.
** A name to describe the output.
** The type of output: `elasticsearch`.
** The secure URL and port of the Elasticsearch instance as a valid absolute URL, including the prefix.
** The secret required by the endpoint for TLS communication. The secret must exist in the `openshift-logging` project.
<5> Configuration for an insecure Elasticsearch output:
** A name to describe the output.
** The type of output: `elasticsearch`.
** The insecure URL and port of the Elasticsearch instance as a valid absolute URL, including the prefix.
<6> Configuration for a Kafka output using a client-authenticated TLS communication over a secure URL:
** A name to describe the output.
** The type of output: `kafka`.
** Specify the URL and port of the Kafka broker as a valid absolute URL, including the prefix.
<7> Configuration for an input to filter application logs from the `my-project` namespace.
<8> Configuration for a pipeline to send audit logs to the secure external Elasticsearch instance:
** A name to describe the pipeline.
** The `inputRefs` is the log type, in this example `audit`.
** The `outputRefs` is the name of the output to use, in this example `elasticsearch-secure` to forward to the secure Elasticsearch instance and `default` to forward to the internal Elasticsearch instance.
** Optional: Labels to add to the logs.
<9> Optional: String. One or more labels to add to the logs. Quote values like "true" so they are recognized as string values, not as a boolean.
<10> Configuration for a pipeline to send infrastructure logs to the insecure external Elasticsearch instance.
<11> Configuration for a pipeline to send logs from the `my-project` project to the internal Elasticsearch instance.
** A name to describe the pipeline.
** The `inputRefs` is a specific input: `my-app-logs`.
** The `outputRefs` is `default`.
** Optional: String. One or more labels to add to the logs.
<12> Configuration for a pipeline to send logs to the Kafka broker, with no pipeline name:
** The `inputRefs` is the log type, in this example `application`.
** The `outputRefs` is the name of the output to use.
** Optional: String. One or more labels to add to the logs.
