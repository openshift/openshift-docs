:_mod-docs-content-type: REFERENCE
[id="rn-ocp-release-notes-new-features_{context}"]
= New features and enhancements

This release adds improvements related to the following components and concepts:

// [id="ocp-release-notes-api_{context}"]
// == API server

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-auth_{context}"]
== Authentication and authorization

Using the Azure `kubelogin` plugin for direct authentication with Microsoft Entra ID::
Red{nbsp}Hat tested authenticating to {product-title} by using the Azure `kubelogin` plugin. This validation covers environments where Microsoft Entra ID is configured as the external OIDC provider for direct authentication. The following login modes for `kubelogin` were tested:
+
--
* Device code grant
* Service principal authentication
* Interactive web browser flow
--
+
For more information, see xref:../authentication/external-auth.adoc#external-auth[Enabling direct authentication with an external OIDC identity provider].

[id="ocp-release-notes-auto_{context}"]
== Autoscaling

Network policy support for Autoscaling Operators::
+
The following Operators now have multiple network policies that control network traffic to and from the Operator and operand pods. These policies restrict traffic to only traffic that is explicitly allowed or required.

* Cluster Resource Override Operator
* Cluster Autoscaler
* Vertical Pod Autoscaler
* Horizontal Pod Autoscaler

Applying VPA recommendations without pod re-creation::
+
You can now configure a Vertical Pod Autoscaler Operator (VPA) in the `InPlaceOrRecreate` mode. In this mode, the VPA attempts to apply the recommended updates without re-creating pods. If the VPA is unable to update the pods in place, the VPA falls back to re-creating the pods. For more information, see xref:../nodes/pods/nodes-pods-vertical-autoscaler.adoc#nodes-pods-vertical-autoscaler-modes_nodes-pods-vertical-autoscaler[About the Vertical Pod Autoscaler Operator modes].

Cluster Autoscaler Operator can now cordon nodes before removing the node::
+
By default, when the Cluster Autoscaler Operator removes a node, it does not cordon the node when draining the pods from the node. You can configure the Operator to cordon the node before draining and moving the pods. For more information, see xref:../machine_management/applying-autoscaling.adoc#cluster-autoscaler-about_applying-autoscaling[About the cluster autoscaler].

[id="ocp-release-notes-edge-computing_{context}"]
== Edge computing

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

ClusterInstance CR replaces SiteConfig CR for {ztp} deployments::

In earlier releases, the `SiteConfig` custom resource (CR) was deprecated. This release removes support for the `SiteConfig` CR. You must now use the `ClusterInstance` CR to deploy managed clusters with {ztp}. For more information, see xref:../edge_computing/ztp-deploying-far-edge-sites.adoc#ztp-deploying-far-edge-sites[Deploying a managed cluster with ClusterInstance and {ztp}].

Support for removing devices and device classes from a volume group::

You can remove the device paths in the `deviceSelector.paths` field and `deviceClass` object from the `LVMCluster` resource. For more information, see xref:../storage/persistent_storage_local/persistent-storage-using-lvms.adoc#about-removing-devices-deviceclasses-from-a-vg_logical-volume-manager-storage[About removing devices and device classes from a volume group]

[id="ocp-release-notes-etcd_{context}"]
== etcd

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

Manage etcd size by limiting the time-to-live (TTL) duration for Kubernetes events (Technology Preview)::
+
With this release, you can manage etcd size by setting the `eventTTLMinutes` property. Having too many stale Kubernetes events in an etcd database can degrade performance. By setting the `eventTTLMinutes` property, you can specify how long an event can stay in the database before it is purged. For more information, see xref:../etcd/etcd-performance.adoc#etcd-customize-ttl_etcd-performance[Managing etcd size by limiting the duration of Kubernetes events].

[id="ocp-release-notes-extensions_{context}"]
== Extensions ({olmv1})

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////
Cluster extension support for webhooks in bundles::
+
With this update, {olmv1} supports Operators that use webhooks for validation, mutation, or conversion. For more information, see  xref:../extensions/ce/olmv1-supported-extensions.adoc#olmv1-webhook-support_olmv1-supported-extensions[Webhook support].

Support for `SingleNamespace` and `OwnNamespace` install modes by using the configuration API (Technology Preview)::
+
If an Operator supports the `SingleNamespace` or `OwnNamespace` install modes, you can configure the Operator to watch a specified namespace. For more information, see xref:../extensions/ce/olmv1-configuring-extensions.adoc#olmv1-config-api_olmv1-configuring-extensions[Extension configuration].

{olmv1} software catalog in the web console (Technology Preview)::
+
With this update, you can preview the {olmv1} software catalog in the web console. Select *Ecosystem* -> *Software Catalog* -> *Operators* to preview this feature. To see the {olmv0} software catalog, click the *Enable {olmv1} (Tech Preview)* toggle.

// [id="ocp-release-notes-hcp_{context}"]
// == Hosted control planes

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-ibm-power_{context}"]
== IBM Power

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////
The {ibm-power-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components::
+
This release introduces support for the following features on {ibm-power-title}:
+
* Enable Installer-Provisioned Infrastructure (IPI) support for PowerVC [Technology Preview]
* Enable Spyre Accelerator on {ibm-power-name}

The {ibm-power-name} release for {product-title} {product-version} adds support for the following operator::
+
* CIFS/SMB CSI Driver Operator
* Kernel Module Management Operator (KMMO)
* Red Hat build of Kueue

[IMPORTANT]
====
When using `kdump` on {ibm-power-name}, the following limitations apply:

* Firmware-assisted dump (`fadump`) is not supported.
* Persistent memory dump is not supported.
====

[id="ocp-release-notes-ibm-z-linux-one_{context}"]
== IBM Z and IBM LinuxONE

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////
The {ibm-z-name} and {ibm-linuxone-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components::
+
This release introduces support for the following features on {ibm-z-name} and {ibm-linuxone-name}:
+
* Enable Spyre Accelerator on {ibm-z-name}

The {ibm-z-name} release for {product-title} {product-version} adds support for the following operator::
+
* Kernel Module Management Operator (KMMO)
* Red Hat build of Kueue

// [id="ocp-release-notes-ibm-power-linuxone_{context}"]
// == IBM Power, IBM Z, and IBM LinuxOne support matrix

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

// [id="ocp-release-notes-insights-operator_{context}"]
// == Insights Operator

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-install-update_{context}"]
== Installation and update

Configuring {image-mode-os-lower} during installation is now supported::
+
You can now apply a custom layered image to your nodes during {product-title} installation. For more information, see xref:../machine_configuration/mco-coreos-layering.adoc#coreos-layering-install-time_mco-coreos-layering[Applying a custom layered image during OpenShift Container Platform installation].

Installing a cluster on {gcp-short} with a user-provisioned DNS is generally available::
+
You can enable a user-provisioned domain name server (DNS) instead of the default cluster-provisioned DNS solution. For example, your organization’s security policies might not allow the use of public DNS services such as {gcp-full} DNS. You can manage your DNS only for the IP addresses of the API and Ingress servers. If you use this feature, you must provide your own DNS solution that includes records for `api.<cluster_name>.<base_domain>.` and `*.apps.<cluster_name>.<base_domain>.`.
+
Installing a cluster on {gcp-short} with a user-provisioned DNS was introduced in {product-title} 4.19 with Technology Preview status. In {product-title} {product-version}, it is now generally available.
+
For more information, see xref:../installing/installing_gcp/installing-restricted-networks-gcp-installer-provisioned.adoc#installation-gcp-enabling-user-managed-DNS_installing-restricted-networks-gcp-installer-provisioned[Enabling a user-managed DNS] and xref:../installing/installing_gcp/installing-restricted-networks-gcp-installer-provisioned.adoc#installation-gcp-provisioning-own-dns-records_installing-restricted-networks-gcp-installer-provisioned[Provisioning your own DNS records].

Installing a cluster on {azure-full} uses Marketplace images by default::
+
As of this update, the {product-title} installation program uses Marketplace images by default when installing a cluster on {azure-short}. This speeds up the installation by removing the need to upload a virtual hard disk to {azure-short} and create an image during installation. This feature is not supported on Azure Stack Hub, or for {azure-short} installations that use Confidential VMs.

The ccoctl utility supports preserving custom {azure-full} role assignments::
+
The Cloud Credential Operator utility (`ccoctl`) can now preserve custom role assignments by using the `--preserve-existing-roles` flag. Previously, the tool removed role assignments that were not defined in the CredentialsRequest, including those manually added by administrators.

Managing your own firewall rules when installing a cluster on {gcp-short} into an existing VPC::
+
As of this update, you can manage your own firewall rules when installing a cluster on {gcp-short} into an existing VPC by enabling the `firewallRulesManagement` parameter in the `install-config.yaml` file. You can limit the permissions that you grant to the installation program by managing your own firewall rules.
+
For more information, see xref:../installing/installing_gcp/installing-gcp-account.adoc#installation-gcp-user-managed-firewall-rules_installing-gcp-account[Managing your own firewall rules].

The `ccoctl` utility supports {aws-full} permissions boundaries::
+
The Cloud Credential Operator utility (`ccoctl`) now supports attaching an {aws-short} permissions boundary to the IAM roles that it creates. You can use this feature to meet organizational security requirements that restrict the maximum permissions of created roles.

Throughput customization for {aws-full} gp3 drives::
+
With this update, you can now customize the maximum throughput for gp3 `rootVolume` drives when installing a cluster on {aws-full}. This customization is set by modifying the `compute.platform.aws.rootVolume.throughput` or `controlPlane.platform.aws.rootVolume.throughput` parameters in the `install-config.yaml` file.
+
For more information, see xref:../installing/installing_aws/installation-config-parameters-aws.adoc#installation-configuration-parameters-optional-aws_installation-config-parameters-aws[Optional AWS configuration parameters].

Support for {vmw-full} Foundation 9 and VMware Cloud Foundation 9::
+
You can now install {product-title} on {vmw-full} Foundation (VVF) 9 and VMware Cloud Foundation (VCF) 9.
+
[NOTE]
====
The following additional VCF and VVF components are outside the scope of Red Hat support:

* Management: VCF Operations, VCF Automation, VCF Fleet Management, and VCF Identity Broker.
* Networking: VMware NSX Container Plugin (NCP).
* Migration: VMware HCX.
====

Support for installing {product-title} on {oda-first}::
+
With this update, you can install a cluster on {oda} using the {ai-full}.
+
For more information, see xref:../installing/installing_oda/installing-oda-assisted.adoc#installing-oda-assisted-installer[Installing a cluster on {oda} by using the {ai-full}].

Installing a cluster on {aws-short} with a user-provisioned DNS (Technology Preview)::
+
You can enable a user-provisioned domain name server (DNS) instead of the default cluster-provisioned DNS solution. For example, your organization's security policies might not allow the use of public DNS services such as {aws-first} DNS. As a result, you can manage the API and Ingress DNS records in your own system rather than adding the records to the DNS of the cloud. If you use this feature, you must provide your own DNS solution that includes records for `api.<cluster_name>.<base_domain>.` and `*.apps.<cluster_name>.<base_domain>.`. Enabling a user-provisioned DNS is available as a Technology Preview feature.
+
For more information, see xref:../installing/installing_aws/ipi/installing-aws-customizations.adoc#installation-aws-enabling-user-managed-DNS_installing-aws-customizations[Enabling a user-managed DNS] and xref:../installing/installing_aws/ipi/installing-aws-customizations.adoc#installation-aws-provisioning-own-dns-records_installing-aws-customizations[Provisioning your own DNS records].

Installing a cluster on {azure-first} using NAT Gateways::
+
With this update, you can install a cluster on {azure-short} using NAT Gateways as your outbound routing strategy. NAT Gateways can minimize the risk of SNAT port exhaustion that can occur with other outbound routing strategies. You can configure NAT Gateways using the `platform.azure.outboundType` parameter in the `install-config.yaml` file.
+
For more information, see xref:../installing/installing_azure/installation-config-parameters-azure.adoc#installation-configuration-parameters-additional-azure_installation-config-parameters-azure[Additional Azure configuration parameters].

Installing a cluster using {gcp-first} private and restricted API endpoints::
+
With this release, you can use {gcp-first} Private Service Connect (PSC) endpoints when installing your {product-title} cluster so that your installation meets your organization's strict regulatory policies.
+
For more information, see xref:../installing/installing_gcp/installation-config-parameters-gcp.adoc#installation-configuration-parameters-optional_installation-config-parameters-gcp[Optional configuration parameters].

Dell iDRAC10 supported for bare metal installation using Redfish virtual media::
+
Dell iDRAC10 versions 1.20.25.00, 1.20.60.50, and 1.20.70.50 have been tested and verified to work for installer-provisioned {product-title} clusters deployed by using Redfish virtual media. iDRAC10 has not been tested with installations that use a provisioning network. For more information, see xref:../installing/installing_bare_metal/ipi/ipi-install-prerequisites.adoc#ipi-install-firmware-requirements-for-installing-with-virtual-media_ipi-install-prerequisites[Firmware requirements for installing with virtual media].

Providing a local or self-signed CA certificate for Baseboard Management Controllers (BMCs) when installing a cluster on bare metal::
+
With this update, you can provide your own local or self-signed CA certificate to secure communication with BMCs when installing a cluster on bare metal. You can configure this certificate using the `platform.baremetal.bmcCACert` parameter in the install-config.yaml file. If you do not use a trusted CA certificate, you can secure BMC communication by providing your own CA certificate. You can also configure a local or self-signed CA certificate after installation, whether the cluster was installed with a different BMC CA certificate or with no BMC CA certificate.
+
For more information, see xref:../installing/installing_bare_metal/ipi/ipi-install-installation-workflow.adoc#additional-install-config-parameters_ipi-install-installation-workflow[Additional installation configuration parameters] and xref:../installing/installing_bare_metal/bare-metal-postinstallation-configuration.adoc#bare-metal-self-signed-cert-post-install_bare-metal-postinstallation-configuration[Configuring a local or self-signed Baseboard Management Controller CA certificate].
Running firmware upgrades for hosts in deployed bare metal clusters (Generally Available)::
+
For hosts in deployed bare metal clusters, you can update firmware attributes and the firmware image. As a result, you can run firmware upgrades and update BIOS settings for hosts that are already provisioned without fully deprovisioning them. Performing a live update to the `HostFirmwareComponents`, `HostFirmareSettings`, or  `HostUpdatePolicy` resource can be a destructive and destabilizing action. Perform these updates only after careful consideration.
+
This feature was introduced in {product-title} 4.18 with Technology Preview status. This feature is now supported as generally available in {product-title} {product-version}.
+
For more information, see xref:../installing/installing_bare_metal/bare-metal-postinstallation-configuration.adoc#bmo-performing-a-live-update-to-the-hostfirmwaresettings-resource_bare-metal-postinstallation-configuration[Performing a live update to the HostFirmwareSettings resource],
xref:../installing/installing_bare_metal/bare-metal-postinstallation-configuration.adoc#bmo-performing-a-live-update-to-the-hostfirmwarecomponents-resource_bare-metal-postinstallation-configuration[Performing a live update to the HostFirmwareComponents resource], and
xref:../installing/installing_bare_metal/bare-metal-postinstallation-configuration.adoc#bmo-setting-the-hostupdatepolicy-resource_bare-metal-postinstallation-configuration[Setting the HostUpdatePolicy resource].

Testing for {aws-full} m7 instance type::
+
As of {product-title} 4.21, m7 instance types have been tested for installations on {aws-full}.
For more information about tested instance types, see xref:../installing/installing_aws/ipi/installing-aws-customizations.adoc#installation-aws-tested-machine-types_installing-aws-customizations[Tested instance types for {aws-short}].

Installing a cluster on {azure-full} with a user-provisioned DNS (Technology Preview)::
+
You can enable a user-provisioned domain name server (DNS) instead of the default cluster-provisioned DNS solution. For example, your organization's security policies might not allow the use of public DNS services such as {azure-full} DNS. You can manage your DNS only for the IP addresses of the API and Ingress servers. If you use this feature, you must provide your own DNS solution that includes records for `api.<cluster_name>.<base_domain>.` and `*.apps.<cluster_name>.<base_domain>.`. Enabling a user-provisioned DNS is available as a Technology Preview feature.
+
For more information, see
xref:../installing/installing_azure/ipi/installing-restricted-networks-azure-installer-provisioned.adoc#installation-azure-enabling-user-managed-DNS_installing-restricted-networks-azure-installer-provisioned[Enabling a user-managed DNS] and
xref:../installing/installing_azure/ipi/installing-azure-customizations.adoc#installation-azure-provisioning-own-dns-records_installing-azure-customizations[Provisioning your own DNS records].

[id="ocp-release-notes-machine-config-operator_{context}"]
== Machine Config Operator

Boot image management for {azure-short} and {vmw-short} clusters promoted to GA::
+
Updating boot images has been promoted to GA for {azure-full} and {vmw-full} clusters. For more information, see xref:../machine_configuration/mco-update-boot-images.adoc#mco-update-boot-images[Boot image management].

Configuring {image-mode-os-lower} during installation is now supported::
+
You can now apply a custom layered image to your nodes during {product-title} installation. For more information, see xref:../machine_configuration/mco-coreos-layering.adoc#coreos-layering-install-time_mco-coreos-layering[Applying a custom layered image during OpenShift Container Platform installation].

{image-mode-os-caps} status reporting improvements::
+
The output of the `oc describe machineconfignodes <mcp_name>` now contains an `ImageBuildDegraded` error that indicates if an {image-mode-os-lower} failed. For more information, see xref:../machine_configuration/index.adoc#checking-mco-node-status_machine-config-overview[About node status during updates].

{image-mode-os-caps} status reporting improvements (Technology preview)::
+
The `oc describe machineconfigpool <mcp_name>` output,  as a Technology Preview feature, now includes the following fields that report the status of machine config updates when {image-mode-os-lower} is enabled:
+
* `Spec.ConfigImage.DesiredImage`. This is the desired image for that node.
* `Status.ConfigImage.CurrentImage`. This is the current image on that node.
* `Status.Conditions.ImagePulledFromRegistry`. This reports whether an image is able to pull correctly in an image mode update.

For more information, see xref:../machine_configuration/index.adoc#checking-mco-node-status_machine-config-overview[About node status during updates].

Boot image management for control plane nodes is now supported (Technology Preview)::
+
Updating boot images is now supported as a Technology Preview feature for VMware vSphere clusters. This feature allows you to configure your cluster to update the node boot image whenever you update your cluster. Previously, updating boot images was supported for worker nodes. For more information, see xref:../machine_configuration/mco-update-boot-images.adoc#mco-update-boot-images[Boot image management].

Overriding storage or partition setup (Technology preview)::
+
You can now use a `MachineConfig` object to change the installed disk partition schema, file systems, and RAID configurations for new nodes. Previously, for security reasons, you were blocked from changing these configurations from what was established during the cluster installation. For more information, see "Overriding storage and partition setup".

[id="ocp-release-notes-machine-management_{context}"]
== Machine management

Creating {gcp-short} Spot VMs by using compute machine sets::
With this release, {product-title} supports deploying Machine API compute machines on Spot VMs in {gcp-short} clusters.
{gcp-short} recommends using Spot VMs over their predecessor, preemptible VMs, because they include new features that preemptible VMs do not support.
+
For more information, see xref:../machine_management/creating_machinesets/creating-machineset-gcp.adoc#machineset-non-guaranteed-instance_creating-machineset-gcp[Machine sets that deploy machines as Spot VMs].

Additional control plane machine set failure domain options for {azure-short}::
This release includes additional configuration options for control plane machine set failure domains on {azure-full}.
+
For more information, see xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-azure.adoc#cpmso-yaml-failure-domain-azure_cpmso-config-options-azure[Sample {azure-short} failure domain configuration].

Configuration of throughput for {aws-full} gp3 volumes on EBS devices::
+
This release includes support for configuring the maximum throughput for gp3 drives on EBS devices for clusters installed on {aws-first}.
+
For more information, see xref:../machine_management/creating_machinesets/creating-machineset-aws.adoc#machine-feature-aws-throughput_creating-machineset-aws[Configuring storage throughput for gp3 drives (Machine API)] and xref:../machine_management/cluster_api_machine_management/cluster_api_provider_configurations/cluster-api-config-options-aws.adoc#machine-feature-aws-throughput-capi_cluster-api-config-options-aws[Configuring storage throughput for gp3 drives (Cluster API)].

Bare-metal nodes on {vmw-full} clusters (Technology Preview)::
+
You can now add bare-metal compute machines to an existing {product-title} cluster on {vmw-short}. This capability enables you to migrate workloads to physical hardware without reinstalling the cluster. For instructions on adding these machines, see xref:../machine_management/user_infra/adding-bare-metal-compute-vsphere-user-infra.adoc#adding-bare-metal-compute-vsphere-user-infra[Adding bare-metal compute machines to a vSphere cluster].

[id="ocp-release-notes-monitoring_{context}"]
== Monitoring

// This should be left for a little while to get users used to the fact that they will find release notes at a new place.
[NOTE]
====
The monitoring stack documentation is now available as a separate documentation set. The {product-version} monitoring release notes are available at link:https://docs.redhat.com/en/documentation/monitoring_stack_for_red_hat_openshift/{product-version}/html-single/release_notes_for_openshift_monitoring[Release notes for OpenShift monitoring].
====

[id="ocp-release-notes-networking_{context}"]
== Networking

MetalLB Operator status reporting::
+
You can now use enhanced MetalLB Operator reporting features to view real-time operational data for IP address allocation and Border Gateway Protocol (BGP) connectivity. Previously, viewing this information required manual log inspection across multiple controllers. With this release, you can monitor your network health and resolve connectivity issues directly through the following custom resources:
+
* `IPAddressPool`: Monitor cluster-wide IP address allocation through the `status` field to track usage and prevent address exhaustion.
* `ServiceBGPStatus`: Verify which service IP addresses are announced to specific BGP peers to ensure correct route advertisements.
* `BGPSessionStatus`: Check the real-time state of BGP and Bidirectional Forwarding Detection sessions to quickly identify connectivity drops.
+
For more information, see xref:../networking/ingress_load_balancing/metallb/monitoring-metallb-status.adoc[Monitoring MetalLB configuration status].

Applying unassisted holdover for boundary clocks and time synchronous clocks::
+
{product-title} 4.20 introduced unassisted holdover for boundary clocks and time synchronous clocks as a Technology Preview feature. This feature is now Generally Available (GA).
+
For more information, see xref:../networking/advanced_networking/ptp/configuring-ptp.adoc[Applying unassisted holdover for boundary clocks and time slave clocks].

SR-IOV Operator supports ARM architecture::
+
The Single Root I/O Virtualization (SR-IOV) Operator can now communicate with ARM hardware. You can now complete tasks such as configure network cards that are already plugged into an ARM server and use these cards in your applications. For instructions on how to search for ARM hardware that the SR-IOV Operator supports, see xref:../networking/hardware_networks/about-sriov.adoc[About Single Root I/O Virtualization (SR-IOV) hardware networks].

Support for {SMProductName} version 3.2::
+
{product-title} {product-version} updates {SMProductShortName} to version 3.2. This version update incorporates essential CVE fixes and ensures that your {product-title} instances receive the latest fixes, features, and enhancements. See the link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_mesh/3.2/html/release_notes/ossm-release-notes[{SMProductShortName} 3.2 release notes] for more information.

PTP Operator introduces GNSS-to-NTP failover for high-precision timing::
+
With this release, the PTP Operator introduces an active GNSS-to-NTP failover configuration to ensure time synchronization continuity in environments requiring extremely high time accuracy.
+
When the primary Global Navigation Satellite System (GNSS) signal is lost or compromised, for example because of satellite jamming, the system automatically fails over to Network Time Protocol (NTP) to maintain time accuracy. When the GNSS signal is restored, the system automatically recovers back to using GNSS as the primary time source.
+
This feature is particularly important in telco environments that require high precision time synchronization with built-in redundancy. To enable GNSS to NTP failover, you configure the `PtpConfig` resource with the `ntpfailover` plugin enabled and configure both `chronyd` and `ts2phc` settings.
+
For more information, see xref:../networking/advanced_networking/ptp/configuring-ptp.adoc#cnf-configuring-gnss-ntp-failover_configuring-ptp[Configuring GNSS failover to NTP for time synchronization continuity].

Network policies for additional namespaces::
With this release, OpenShift Container Platform continues to deploy Kubernetes network policies to additional system namespaces to control ingress and egress traffic. It is anticipated that future releases might include network policies for additional system namespaces and Red Hat Operators.


Ingress network flow analysis with the commatrix plugin::

With this release, you can use the `commatrix` plugin to generate ingress network flow data from your cluster. You can also use the plugin to identify any differences between open ports on the host and expected ingress flows for your environment.

For more information, see xref:../installing/install_config/configuring-firewall.adoc#network-commatrix-plugin-intro_configuring-firewall[Ingress network flow analysis with the commatrix plugin]

[id="ocp-release-notes-nodes_{context}"]
== Nodes

Allocating specific GPUs to pods (DRA) is now generally available::
+
Attribute-Based GPU Allocation, which allows pods to request GPUs based on specific device attributes by using a Dynamic Resource Allocation (DRA) driver, is now generally available. For more information, see xref:../nodes/pods/nodes-pods-allocate-dra.adoc#nodes-pods-allocate-dra[Allocating GPUs to Pods].

The default `openshift` cluster image policy is now generally available::
+
The default `openshift` cluster image policy is now generally available and active by default. For more information, see xref:../nodes/nodes-sigstore-using.adoc#nodes-sigstore-using[Manage secure signatures with sigstore].
+
If your {product-title} 4.20 or earlier cluster has a cluster image policy named `openshift`, the upgrade to {product-title} marks the cluster as not updatable (`Upgradeable=False`) because of this default `openshift` cluster image policy. You must remove your `openshift` cluster image policy to clear the `Upgradeable=False` condition and proceed with the update. You can optionally create your own cluster image policy with a different name before removing your `openshift` cluster image policy.

Support for sigstore BYOPKI is now generally available::
+
Support for using a certificate from your own public key infrastructure as a Sigstore root of trust is now generally available. For more information, see xref:../nodes/nodes-sigstore-using.adoc#nodes-sigstore-using[Manage secure signatures with sigstore].

Automatically calculate and apply CPU and memory resources for system components::
+
{product-title} now automatically calculates and reserves a portion of the CPU and memory resources for use by the underlying node and system components. Previously, you needed to enable the feature by creating a `KubeletConfig` custom resource (CR) with the `autoSizingReserved: true` parameter. For clusters updated to {product-title} 4.21, you can enable the feature by deleting the `50-worker-auto-sizing-disabled` machine config. After you delete the machine config, the nodes reboot with the new resource settings. If you manually configured system reserved CPU or memory resources, these settings remain upon update and do not change. For more information on this new feature, see xref:../nodes/nodes/nodes-nodes-resources-configuring.adoc#nodes-nodes-resources-configuring-auto_nodes-nodes-resources-configuring[Automatically allocating resources for nodes].

Linux PSI monitoring is now enabled by default::
+
Linux Pressure Stall Information (PSI) monitoring, which makes PSI metrics for CPU, memory, and I/O available for your cluster, is now enabled by default. Before this update, you needed to manually enable PSI. For more information, see xref:../nodes/nodes/nodes-nodes-managing.adoc#nodes-nodes-psi-enable_nodes-nodes-managing[Enabling Pressure Stall Information (PSI) monitoring].

[id="ocp-release-notes-ocp-cli_{context}"]
== OpenShift CLI (oc)

Signature mirroring enabled by default for oc-mirror v2::
With this update, the oc-mirror v2 plugin mirrors image signatures by default. This enhancement ensures that image integrity is automatically preserved during the mirroring process without requiring additional configuration. If your environment does not require signature validation, you can manually disable this feature by using the `--remove-signatures` command-line flag. For more information, see xref:../disconnected/about-installing-oc-mirror-v2.adoc#oc-mirror-signature-mirroring_about-installing-oc-mirror-v2[Disabling signature mirroring for oc-mirror plugin v2].
////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-operator-development_{context}"]
== Operator development

////
This section applies to the removed Operator SDK and the Operator base images that are still supported.
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////
Supported Operator base images::
+
--
include::snippets/osdk-release-notes-operator-images.adoc[]
--

[id="ocp-release-notes-post-install-configuration_{context}"]
== Postinstallation configuration

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

Enabling hardware metrics monitoring on bare-metal clusters (Technology Preview)::
+
With this update, you can enable your cluster to collect hardware metrics from the Redfish-compatible baseboard management controllers of your bare-metal nodes.
Metrics include temperature, power consumption, fan status, and drive health.
You enable this Technology Preview feature by enabling the Ironic Prometheus Exporter in your cluster as a postinstallation task.
+
For more information, see xref:../installing/installing_bare_metal/bare-metal-postinstallation-configuration.adoc#bm-about-ipe_bare-metal-postinstallation-configuration[Hardware metrics in the Monitoring stack].

// [id="ocp-release-notes-rhcos_{context}"]
// == Red Hat Enterprise Linux CoreOS (RHCOS)

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-scale-and-perform_{context}"]
== Scalability and performance

Pod-level IRQ affinity introduces housekeeping mode::
+
For latency-sensitive workloads, you can now configure the `irq-load-balancing.crio.io` pod annotation to use `housekeeping` mode. This mode enables a subset of pinned CPUs to handle system interrupts while isolating the remaining pinned CPUs for latency-sensitive workloads. This reduces the overall CPU footprint by eliminating the need for dedicated housekeeping CPUs for IRQ handling. When you configure `housekeeping` mode, the first pinned CPU and its thread siblings handle interrupts for the system.
+
For more information, see xref:../scalability_and_performance/cnf-provisioning-low-latency-workloads.adoc#cnf-disabling-interrupt-processing-for-individual-pods_cnf-provisioning-low-latency[Configuring interrupt processing for individual pods].

[id="ocp-release-notes-storage_{context}"]
== Storage

Volume Attributes Classes is generally available::
+
Volume Attributes Classes provide a way for administrators to describe "classes" of storage they offer. Different classes might correspond to different quality-of-service levels. Volume Attributes Classes was introduced in {product-title} 4.19, and is now generally available in 4.21.
+
Volume Attributes Classes is available only with AWS Elastic Block Storage (EBS) and Google Cloud Platform (GCP) persistent disk (PD) Container Storage Interface (CSI).
+
You can apply a Volume Attributes Classes to a persistent volume claim (PVC). If a new Volume Attributes Class becomes available in the cluster, you can update the PVC with the new Volume Attributes Classes if needed.
+
Volume Attributes Classes have parameters that describe volumes belonging to them. If a parameter is omitted, the default is used at volume provisioning. If a user applies the PVC with a different Volume Attributes Class with omitted parameters, the default value of the parameters might be used depending on the CSI driver implementation. For more information, see the related CSI driver documentation.
+
For more information, see xref:../storage/understanding-persistent-storage.adoc#storage-persistent-storage-pvc-volumeattributesclass_understanding-persistent-storage[Volume Attributes Classes].

Azure File CSI supporting snapshots feature is generally available::
+
A snapshot represents the state of the storage volume in a cluster at a particular point in time. Volume snapshots can be used to provision a new volume.
+
{product-title} 4.17 introduced volume snapshot support for the Microsoft Azure File Container Storage Interface (CSI) Driver Operator as a Technology Preview feature. In 4.21, this feature is generally available. Also, Azure File snapshots now supports Network File System (NFS) in addition to Server Message Block (SMB).
+
For more information, see xref:../storage/container_storage_interface/persistent-storage-csi.adoc#csi-drivers-supported_persistent-storage-csi[CSI drivers supported by OpenShift Container Platform] and xref:../storage/container_storage_interface/persistent-storage-csi-snapshots.adoc[CSI volume snapshots].

Azure File CSI supporting volume cloning feature is generally available::
+
Volume cloning duplicates an existing persistent volume (PV) to help protect against data loss in {product-title}. You can also use a volume clone just as you would use any standard volume.
+
{product-title} 4.16 introduced volume cloning for the Microsoft Azure File Container Storage Interface (CSI) Driver Operator as a Technology Preview feature. In 4.21, this feature is generally available. Also, Azure File cloning now supports Network File System (NFS) in addition to Server Message Block (SMB).
+
For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-azure-file.adoc[Azure File CSI Driver Operator] and xref:../storage/container_storage_interface/persistent-storage-csi-cloning.adoc[CSI volume cloning].

oVirt CSI Driver Operator is removed from {product-title} 4.21::
+
Red Hat Virtualization (RHV) as a host platform for {product-title} was deprecated in version 4.14 and is no longer supported. In {product-title} 4.21, the oVirt CSI Driver Operator is removed.

CIFS/SMB CSI Driver Operator supports IBM Power::
+
In {product-title} 4.21, the CIFS/SMB CSI Driver Operator supports IBM Power (ppc64le).
+
For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-smb-cifs.adoc[CIFS/SMB CSI Driver Operator].

Introduction of new field to track the status of volume resize attempts::
+
{product-title} 4.19 introduced resizing recovery that stops the expansion controller from indefinitely attempting to expand a volume to an unsupported size request. This feature allows you to recover and provide another smaller resize value for the persistent volume claim (PVC). The new value must be larger than the original volume size.
+
{product-title} 4.21 introduces the `pvc.Status.AllocatedResourceStatus` field, which shows the status of volume resize attempts. If a user changes the size of their PVCs, this new field allows resource quota to be tracked accurately.
+
For more information about resizing volumes, see xref:../storage/expanding-persistent-volumes.adoc[Expanding persistent volumes].
+
For more information about recovering when resizing volumes, see xref:../storage/expanding-persistent-volumes.adoc#expanding-recovering-from-failure_expanding-persistent-volumes[Recovering from failure when expanding volumes].

Mutable CSI node allocatable property (Technical Preview)::
+
This feature allows for dynamically updating the maximum number of storage volumes a node can handle. Without this feature, volume limits are essentially immutable when a node first joins the cluster. If the environment changes—for example, if you attach a new network interface (ENI) that shares a hardware "slot" with your storage—{product-title} does not recognize it has fewer slots available for disks, leading to pods becoming stuck.
+
This feature is only supported on AWS Elastic Block Storage (EBS).
+
Mutable CSI node allocatable property is supported in {product-title} 4.21 as a Technical Preview feature. To enable this feature, you need to enable Feature Gates.
+
For more information about enabling Technical Preview features, see xref:../nodes/clusters/nodes-cluster-enabling-features.adoc[Feature Gates].

Reducing permissions while using the GCP PD CSI Driver Operator is generally available::
+
The default installation allows the Google Cloud Platform (GCP) persistent disk (PD) Container Storage Interface (CSI) Driver to impersonate any service account in the Google Cloud project. You can reduce the scope of permissions granted to the GCP PD CSI Driver service account in your Google Cloud project to only the required node service accounts.
+
For more information about this feature, see xref:../storage/container_storage_interface/persistent-storage-csi-gcp-pd.adoc#persistent-storage-csi-gcp-pd-reduce-permissions_persistent-storage-csi-gcp-pd[Reducing permissions while using the GCP PD CSI Driver Operator].

Volume group snapshots API updated (Technical Preview)::
+
The API for the Container Storage Interface (CSI) volume group snapshot feature is updated from `v1beta1` to `v1beta2`.
+
This feature is supported at the Technical Preview level.
+
For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-group-snapshots.adoc[CSI volume group snapshots].

Updated release of the {secrets-store-operator}::
+
The {secrets-store-operator} version v4.21 is now based on the upstream version v1.5.4 release of secrets-store-csi-driver.

// [id="ocp-release-notes-support-sigstore_{context}"]
// == Support for sigstore bring your own PKI (BYOPKI) image validation

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-web-console_{context}"]
== Web console

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

{olmv1} software catalog in the web console (Technology Preview)::
+
With this update, you can preview the {olmv1-first} software catalog in the web console. Select *Ecosystem* -> *Software Catalog* -> *Operators* to preview this feature. To see the {olmv0} software catalog, click the *Enable {olmv1} (Tech Preview)* toggle.
