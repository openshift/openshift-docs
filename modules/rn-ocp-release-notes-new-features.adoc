:_mod-docs-content-type: REFERENCE
[id="rn-ocp-release-notes-new-features_{context}"]
= New features and enhancements

This release adds improvements related to the following components and concepts:

[id="ocp-release-notes-api_{context}"]
== API server

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-auth_{context}"]
== Authentication and authorization

Using the Azure `kubelogin` plugin for direct authentication with Microsoft Entra ID::
Red{nbsp}Hat tested authenticating to {product-title} by using the Azure `kubelogin` plugin. This validation covers environments where Microsoft Entra ID is configured as the external OIDC provider for direct authentication. The following login modes for `kubelogin` were tested:
+
--
* Device code grant
* Service principal authentication
* Interactive web browser flow
--
+
For more information, see xref:../authentication/external-auth.adoc#external-auth[Enabling direct authentication with an external OIDC identity provider].

[id="ocp-release-notes-auto_{context}"]
== Autoscaling

Network policy support for Autoscaling Operators::
+
The following Operators now have multiple network policies that control network traffic to and from the Operator and operand pods. These policies restrict traffic to only traffic that is explicitly allowed or required.

* Cluster Resource Override Operator
* Cluster Autoscaler
* Vertical Pod Autoscaler
* Horizontal Pod Autoscaler

Applying VPA recommendations without pod re-creation::
+
You can now configure a Vertical Pod Autoscaler Operator (VPA) in the `InPlaceOrRecreate` mode. In this mode, the VPA attempts to apply the recommended updates without re-creating pods. If the VPA is unable to update the pods in place, the VPA falls back to re-creating the pods. For more information, see xref:../nodes/pods/nodes-pods-vertical-autoscaler.adoc#nodes-pods-vertical-autoscaler-modes_nodes-pods-vertical-autoscaler[About the Vertical Pod Autoscaler Operator modes].

Cluster Autoscaler Operator can now cordon nodes before removing the node::
+
By default, when the Cluster Autoscaler Operator removes a node, it does not cordon the node when draining the pods from the node. You can configure the Operator to cordon the node before draining and moving the pods. For more information, see xref:../machine_management/applying-autoscaling.adoc#cluster-autoscaler-about_applying-autoscaling[About the cluster autoscaler].

[id="ocp-release-notes-edge-computing_{context}"]
== Edge computing

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-etcd_{context}"]
== etcd

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

Manage etcd size by limiting the time-to-live (TTL) duration for Kubernetes events (Technology Preview)::
+
With this release, you can manage etcd size by setting the `eventTTLMinutes` property. Having too many stale Kubernetes events in an etcd database can degrade performance. By setting the `eventTTLMinutes` property, you can specify how long an event can stay in the database before it is purged. For more information, see xref:../etcd/etcd-performance.adoc#etcd-customize-ttl_etcd-performance[Managing etcd size by limiting the duration of Kubernetes events].

[id="ocp-release-notes-extensions_{context}"]
== Extensions ({olmv1})

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////
Cluster extension support for webhooks in bundles::
+
With this update, {olmv1} supports Operators that use webhooks for validation, mutation, or conversion. For more information, see  xref:../extensions/ce/olmv1-supported-extensions.adoc#olmv1-webhook-support_olmv1-supported-extensions[Webhook support].

Support for `SingleNamespace` and `OwnNamespace` install modes by using the configuration API (Technology Preview)::
+
If an Operator supports the `SingleNamespace` or `OwnNamespace` install modes, you can configure the Operator to watch a specified namespace. For more information, see xref:../extensions/ce/olmv1-configuring-extensions.adoc#olmv1-config-api_olmv1-configuring-extensions[Extension configuration].

{olmv1} software catalog in the web console (Technology Preview)::
+
With this update, you can preview the {olmv1} software catalog in the web console. Select *Ecosystem* -> *Software Catalog* -> *Operators* to preview this feature. To see the {olmv0} software catalog, click the *Enable {olmv1} (Tech Preview)* toggle.

[id="ocp-release-notes-hcp_{context}"]
== Hosted control planes

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-ibm-power_{context}"]
== IBM Power

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////
The {ibm-power-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components::
+
This release introduces support for the following features on {ibm-power-title}:
+
* Enable Installer-Provisioned Infrastructure (IPI) support for PowerVC [Technology Preview]
* Enable Spyre Accelerator on {ibm-power-name}

The {ibm-power-name} release for {product-title} {product-version} adds support for the following operator::
+
* CIFS/SMB CSI Driver Operator
* Kernel Module Management Operator (KMMO)
* Red Hat build of Kueue

[IMPORTANT]
====
When using `kdump` on {ibm-power-name}, the following limitations apply:

* Firmware-assisted dump (`fadump`) is not supported.
* Persistent memory dump is not supported.
====

[id="ocp-release-notes-ibm-z-linux-one_{context}"]
== IBM Z and IBM LinuxONE

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////
The {ibm-z-name} and {ibm-linuxone-name} release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components::
+
This release introduces support for the following features on {ibm-z-name} and {ibm-linuxone-name}:
+
* Enable Spyre Accelerator on {ibm-z-name}

The {ibm-z-name} release for {product-title} {product-version} adds support for the following operator::
+
* Kernel Module Management Operator (KMMO)
* Red Hat build of Kueue

[id="ocp-release-notes-ibm-power-linuxone_{context}"]
== IBM Power, IBM Z, and IBM LinuxOne support matrix

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-insights-operator_{context}"]
== Insights Operator

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-install-update_{context}"]
== Installation and update

Configuring {image-mode-os-lower} during installation is now supported::
+
You can now apply a custom layered image to your nodes during {product-title} installation. For more information, see xref:../machine_configuration/mco-coreos-layering.adoc#coreos-layering-install-time_mco-coreos-layering[Applying a custom layered image during OpenShift Container Platform installation].

Installing a cluster on {gcp-short} with a user-provisioned DNS is generally available::
+
You can enable a user-provisioned domain name server (DNS) instead of the default cluster-provisioned DNS solution. For example, your organizationâ€™s security policies might not allow the use of public DNS services such as {gcp-full} DNS. You can manage your DNS only for the IP addresses of the API and Ingress servers. If you use this feature, you must provide your own DNS solution that includes records for `api.<cluster_name>.<base_domain>.` and `*.apps.<cluster_name>.<base_domain>.`.
+
Installing a cluster on {gcp-short} with a user-provisioned DNS was introduced in {product-title} 4.19 with Technology Preview status. In {product-title} {product-version}, it is now generally available.
+
For more information, see xref:../installing/installing_gcp/installing-restricted-networks-gcp-installer-provisioned.adoc#installation-gcp-enabling-user-managed-DNS_installing-restricted-networks-gcp-installer-provisioned[Enabling a user-managed DNS] and xref:../installing/installing_gcp/installing-restricted-networks-gcp-installer-provisioned.adoc#installation-gcp-provisioning-own-dns-records_installing-restricted-networks-gcp-installer-provisioned[Provisioning your own DNS records].

Installing a cluster on {azure-full} uses Marketplace images by default::
+
As of this update, the {product-title} installation program uses Marketplace images by default when installing a cluster on {azure-short}. This speeds up the installation by removing the need to upload a virtual hard disk to {azure-short} and create an image during installation. This feature is not supported on Azure Stack Hub, or for {azure-short} installations that use Confidential VMs.

Managing your own firewall rules when installing a cluster on {gcp-short} into an existing VPC::
+
As of this update, you can manage your own firewall rules when installing a cluster on {gcp-short} into an existing VPC by enabling the `firewallRulesManagement` parameter in the `install-config.yaml` file. You can limit the permissions that you grant to the installation program by managing your own firewall rules.
+
For more information, see xref:../installing/installing_gcp/installing-gcp-account.adoc#installation-gcp-user-managed-firewall-rules_installing-gcp-account[Managing your own firewall rules].

Throughput customization for {aws-full} gp3 drives::
+
With this update, you can now customize the maximum throughput for gp3 `rootVolume` drives when installing a cluster on {aws-full}. This customization is set by modifying the `compute.platform.aws.rootVolume.throughput` or `controlPlane.platform.aws.rootVolume.throughput` parameters in the `install-config.yaml` file.
+
For more information, see xref:../installing/installing_aws/installation-config-parameters-aws.adoc#installation-configuration-parameters-optional-aws_installation-config-parameters-aws[Optional AWS configuration parameters].

Support for {vmw-full} Foundation 9 and VMware Cloud Foundation 9::
+
You can now install {product-title} on {vmw-full} Foundation (VVF) 9 and VMware Cloud Foundation (VCF) 9.
+
[NOTE]
====
The following additional VCF and VVF components are outside the scope of Red Hat support:

* Management: VCF Operations, VCF Automation, VCF Fleet Management, and VCF Identity Broker.
* Networking: VMware NSX Container Plugin (NCP).
* Migration: VMware HCX.
====

Support for installing {product-title} on {oda-first}::
+
With this update, you can install a cluster on {oda} using the {ai-full}.
+
For more information, see xref:../installing/installing_oda/installing-oda-assisted.adoc#installing-oda-assisted-installer[Installing a cluster on {oda} by using the {ai-full}].


Installing a cluster on {azure-first} using NAT Gateways::
+
With this update, you can install a cluster on {azure-short} using NAT Gateways as your outbound routing strategy. NAT Gateways can minimize the risk of SNAT port exhaustion that can occur with other outbound routing strategies. You can configure NAT Gateways using the `platform.azure.outboundType` parameter in the `install-config.yaml` file.
+
For more information, see xref:../installing/installing_azure/installation-config-parameters-azure.adoc#installation-configuration-parameters-additional-azure_installation-config-parameters-azure[Additional Azure configuration parameters].

Installing a cluster using {gcp-first} private and restricted API endpoints::
+
With this release, you can use {gcp-first} Private Service Connect (PSC) endpoints when installing your {product-title} cluster so that your installation meets your organization's strict regulatory policies.
+
For more information, see xref:../installing/installing_gcp/installation-config-parameters-gcp.adoc#installation-configuration-parameters-optional_installation-config-parameters-gcp[Optional configuration parameters].

Dell iDRAC10 supported for bare metal installation using Redfish virtual media::
+
Dell iDRAC10 versions 1.20.25.00, 1.20.60.50, and 1.20.70.50 have been tested and verified to work for installer-provisioned {product-title} clusters deployed by using Redfish virtual media. iDRAC10 has not been tested with installations that use a provisioning network. For more information, see xref:../installing/installing_bare_metal/ipi/ipi-install-prerequisites.adoc#ipi-install-firmware-requirements-for-installing-with-virtual-media_ipi-install-prerequisites[Firmware requirements for installing with virtual media].

Providing a local or self-signed CA certificate for Baseboard Management Controllers (BMCs) when installing a cluster on bare metal::
+
With this update, you can provide your own local or self-signed CA certificate to secure communication with BMCs when installing a cluster on bare metal. You can configure this certificate using the `platform.baremetal.bmcCACert` parameter in the install-config.yaml file. If you do not use a trusted CA certificate, you can secure BMC communication by providing your own CA certificate. You can also configure a local or self-signed CA certificate after installation, whether the cluster was installed with a different BMC CA certificate or with no BMC CA certificate.
+
For more information, see xref:../installing/installing_bare_metal/ipi/ipi-install-installation-workflow.adoc#additional-install-config-parameters_ipi-install-installation-workflow[Additional installation configuration parameters] and xref:../installing/installing_bare_metal/bare-metal-postinstallation-configuration.adoc#bare-metal-self-signed-cert-post-install_bare-metal-postinstallation-configuration[Configuring a local or self-signed Baseboard Management Controller CA certificate].
Running firmware upgrades for hosts in deployed bare metal clusters (Generally Available)::
+
For hosts in deployed bare metal clusters, you can update firmware attributes and the firmware image. As a result, you can run firmware upgrades and update BIOS settings for hosts that are already provisioned without fully deprovisioning them. Performing a live update to the `HostFirmwareComponents`, `HostFirmareSettings`, or  `HostUpdatePolicy` resource can be a destructive and destabilizing action. Perform these updates only after careful consideration.
+
This feature was introduced in {product-title} 4.18 with Technology Preview status. This feature is now supported as generally available in {product-title} {product-version}.
+
For more information, see xref:../installing/installing_bare_metal/bare-metal-postinstallation-configuration.adoc#bmo-performing-a-live-update-to-the-hostfirmwaresettings-resource_bare-metal-postinstallation-configuration[Performing a live update to the HostFirmwareSettings resource],
xref:../installing/installing_bare_metal/bare-metal-postinstallation-configuration.adoc#bmo-performing-a-live-update-to-the-hostfirmwarecomponents-resource_bare-metal-postinstallation-configuration[Performing a live update to the HostFirmwareComponents resource], and
xref:../installing/installing_bare_metal/bare-metal-postinstallation-configuration.adoc#bmo-setting-the-hostupdatepolicy-resource_bare-metal-postinstallation-configuration[Setting the HostUpdatePolicy resource].

[id="ocp-release-notes-machine-config-operator_{context}"]
== Machine Config Operator

Boot image management for {azure-short} and {vmw-short} clusters promoted to GA::
+
Updating boot images has been promoted to GA for {azure-full} and {vmw-full} clusters. For more information, see xref:../machine_configuration/mco-update-boot-images.adoc#mco-update-boot-images[Boot image management].

Configuring {image-mode-os-lower} during installation is now supported::
+
You can now apply a custom layered image to your nodes during {product-title} installation. For more information, see xref:../machine_configuration/mco-coreos-layering.adoc#coreos-layering-install-time_mco-coreos-layering[Applying a custom layered image during OpenShift Container Platform installation].

{image-mode-os-caps} status reporting improvements::
+
The output of the `oc describe machineconfignodes <mcp_name>` now contains an `ImageBuildDegraded` error that indicates if an {image-mode-os-lower} failed. For more information, see xref:../machine_configuration/index.adoc#checking-mco-node-status_machine-config-overview[About node status during updates].

{image-mode-os-caps} status reporting improvements (Technology preview)::
+
The `oc describe machineconfigpool <mcp_name>` output,  as a Technology Preview feature, now includes the following fields that report the status of machine config updates when {image-mode-os-lower} is enabled:
+
* `Spec.ConfigImage.DesiredImage`. This is the desired image for that node.
* `Status.ConfigImage.CurrentImage`. This is the current image on that node.
* `Status.Conditions.ImagePulledFromRegistry`. This reports whether an image is able to pull correctly in an image mode update.

For more information, see xref:../machine_configuration/index.adoc#checking-mco-node-status_machine-config-overview[About node status during updates].

Boot image management for control plane nodes is now supported (Technology Preview)::
+
Updating boot images is now supported as a Technology Preview feature for VMware vSphere clusters. This feature allows you to configure your cluster to update the node boot image whenever you update your cluster. Previously, updating boot images was supported for worker nodes. For more information, see xref:../machine_configuration/mco-update-boot-images.adoc#mco-update-boot-images[Boot image management].

Overriding storage or partition setup (Technology preview)::
+
You can now use a `MachineConfig` object to change the installed disk partition schema, file systems, and RAID configurations for new nodes. Previously, for security reasons, you were blocked from changing these configurations from what was established during the cluster installation. For more information, see "Overriding storage and partition setup".

[id="ocp-release-notes-machine-management_{context}"]
== Machine management

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

Additional control plane machine set failure domain options for {azure-short}::
This release includes additional configuration options for control plane machine set failure domains on {azure-full}.
+
For more information, see xref:../machine_management/control_plane_machine_management/cpmso_provider_configurations/cpmso-config-options-azure.adoc#cpmso-yaml-failure-domain-azure_cpmso-config-options-azure[Sample {azure-short} failure domain configuration].

[id="ocp-release-notes-monitoring_{context}"]
== Monitoring

// This should be left for a little while to get users used to the fact that they will find release notes at a new place.
[NOTE]
====
The monitoring stack documentation is now available as a separate documentation set. The {product-version} monitoring release notes are available at link:https://docs.redhat.com/en/documentation/monitoring_stack_for_red_hat_openshift/{product-version}/html-single/release_notes_for_openshift_monitoring[Release notes for OpenShift monitoring].
====

[id="ocp-release-notes-networking_{context}"]
== Networking

Applying unassisted holdover for boundary clocks and time synchronous clocks::
+
{product-title} 4.20 introduced unassisted holdover for boundary clocks and time synchronous clocks as a Technology Preview feature. This feature is now Generally Available (GA).
+
For more information, see xref:../networking/advanced_networking/ptp/configuring-ptp.adoc[Applying unassisted holdover for boundary clocks and time slave clocks].

SR-IOV Operator supports ARM architecture::
+
The Single Root I/O Virtualization (SR-IOV) Operator can now communicate with ARM hardware. You can now complete tasks such as configure network cards that are already plugged into an ARM server and use these cards in your applications. For instructions on how to search for ARM hardware that the SR-IOV Operator supports, see xref:../networking/hardware_networks/about-sriov.adoc[About Single Root I/O Virtualization (SR-IOV) hardware networks].

Support for {SMProductName} version 3.2::
+
{product-title} {product-version} updates {SMProductShortName} to version 3.2. This version update incorporates essential CVE fixes and ensures that your {product-title} instances receive the latest fixes, features, and enhancements. See the link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_mesh/3.2/html/release_notes/ossm-release-notes[{SMProductShortName} 3.2 release notes] for more information.

[id="ocp-release-notes-nodes_{context}"]
== Nodes

Allocating specific GPUs to pods (DRA) is now generally available::
+
Attribute-Based GPU Allocation, which allows pods to request GPUs based on specific device attributes by using a Dynamic Resource Allocation (DRA) driver, is now generally available. For more information, see xref:../nodes/pods/nodes-pods-allocate-dra.adoc#nodes-pods-allocate-dra[Allocating GPUs to Pods].

The default `openshift` cluster image policy is now generally available::
+
The default `openshift` cluster image policy is now generally available and active by default. For more information, see xref:../nodes/nodes-sigstore-using.adoc#nodes-sigstore-using[Manage secure signatures with sigstore].
+
If your {product-title} 4.20 or earlier cluster has a cluster image policy named `openshift`, the upgrade to {product-title} marks the cluster as not updatable (`Upgradeable=False`) because of this default `openshift` cluster image policy. You must remove your `openshift` cluster image policy to clear the `Upgradeable=False` condition and proceed with the update. You can optionally create your own cluster image policy with a different name before removing your `openshift` cluster image policy.

Support for sigstore BYOPKI is now generally available::
+
Support for using a certificate from your own public key infrastructure as a Sigstore root of trust is now generally available. For more information, see xref:../nodes/nodes-sigstore-using.adoc#nodes-sigstore-using[Manage secure signatures with sigstore].

Automatically calculate and apply CPU and memory resources for system components::
+
{product-title} now automatically calculates and reserves a portion of the CPU and memory resources for use by the underlying node and system components. Previously, you needed to enable the feature by creating a `KubeletConfig` custom resource (CR) with the `autoSizingReserved: true` parameter. For clusters updated to {product-title} 4.21, you can enable the feature by deleting the `50-worker-auto-sizing-disabled` machine config. After you delete the machine config, the nodes reboot with the new resource settings. If you manually configured system reserved CPU or memory resources, these settings remain upon update and do not change. For more information on this new feature, see xref:../nodes/nodes/nodes-nodes-resources-configuring.adoc#nodes-nodes-resources-configuring-auto_nodes-nodes-resources-configuring[Automatically allocating resources for nodes].

[id="ocp-release-notes-ocp-cli_{context}"]
== OpenShift CLI (oc)

Signature mirroring enabled by default for oc-mirror v2::
With this update, the oc-mirror v2 plugin mirrors image signatures by default. This enhancement ensures that image integrity is automatically preserved during the mirroring process without requiring additional configuration. If your environment does not require signature validation, you can manually disable this feature by using the `--remove-signatures` command-line flag. For more information, see xref:../disconnected/about-installing-oc-mirror-v2.adoc#oc-mirror-signature-mirroring_about-installing-oc-mirror-v2[Disabling signature mirroring for oc-mirror plugin v2].
////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-operator-development_{context}"]
== Operator development

////
This section applies to the removed Operator SDK and the Operator base images that are still supported.
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////
Supported Operator base images::
+
--
include::snippets/osdk-release-notes-operator-images.adoc[]
--

[id="ocp-release-notes-post-install-configuration_{context}"]
== Postinstallation configuration

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

Enabling hardware metrics monitoring on bare-metal clusters (Technology Preview)::
+
With this update, you can enable your cluster to collect hardware metrics from the Redfish-compatible baseboard management controllers of your bare-metal nodes.
Metrics include temperature, power consumption, fan status, and drive health.
You enable this Technology Preview feature by enabling the Ironic Prometheus Exporter in your cluster as a postinstallation task.
+
For more information, see xref:../installing/installing_bare_metal/bare-metal-postinstallation-configuration.adoc#bm-about-ipe_bare-metal-postinstallation-configuration[Hardware metrics in the Monitoring stack].

[id="ocp-release-notes-rhcos_{context}"]
== Red Hat Enterprise Linux CoreOS (RHCOS)

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-scale-and-perform_{context}"]
== Scalability and performance

Pod-level IRQ affinity introduces housekeeping mode::
+
For latency-sensitive workloads, you can now configure the `irq-load-balancing.crio.io` pod annotation to use `housekeeping` mode. This mode enables a subset of pinned CPUs to handle system interrupts while isolating the remaining pinned CPUs for latency-sensitive workloads. This reduces the overall CPU footprint by eliminating the need for dedicated housekeeping CPUs for IRQ handling. When you configure `housekeeping` mode, the first pinned CPU and its thread siblings handle interrupts for the system.
+
For more information, see xref:../scalability_and_performance/cnf-provisioning-low-latency-workloads.adoc#cnf-disabling-interrupt-processing-for-individual-pods_cnf-provisioning-low-latency[Configuring interrupt processing for individual pods].

[id="ocp-release-notes-storage_{context}"]
== Storage

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-support-sigstore_{context}"]
== Support for sigstore bring your own PKI (BYOPKI) image validation

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

[id="ocp-release-notes-web-console_{context}"]
== Web console

////
Instructions: Add entries in the following format:

Item description::
+
Detailed information.
////

{olmv1} software catalog in the web console (Technology Preview)::
+
With this update, you can preview the {olmv1-first} software catalog in the web console. Select *Ecosystem* -> *Software Catalog* -> *Operators* to preview this feature. To see the {olmv0} software catalog, click the *Enable {olmv1} (Tech Preview)* toggle.
