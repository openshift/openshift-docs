// Module included in the following assemblies:
//
// * installing/installing_ibm_z/installing-ibm-z.adoc

[id="installation-user-infra-machines-iso-ibm-z_{context}"]
= Creating {op-system-first} machines

Before you install a cluster on IBM Z infrastructure that you provision, you must install {op-system} on z/VM guest virtual machines for the cluster to use. Complete the following steps to create the machines.

.Prerequisites

* An HTTP or HTTPS server running on your provisioning machine that is accessible to the machines you create.

.Procedure

. Log in to Linux on your provisioning machine.

. Obtain the {op-system-first} kernel, initramfs, and rootfs files from the link:https://mirror.openshift.com/pub/openshift-v4/s390x/dependencies/rhcos/latest/[{op-system} image mirror].
+
[IMPORTANT]
====
The {op-system} images might not change with every release of {product-title}.
You must download images with the highest version that is less than or equal
to the {product-title} version that you install. Only use the appropriate kernel, initramfs, and rootfs artifacts described in the following procedure.
====
+
The file names contain the {product-title} version number. They resemble the following examples:

* kernel: `rhcos-<version>-live-kernel-<architecture>`
* initramfs: `rhcos-<version>-live-initramfs.<architecture>.img`
* rootfs: `rhcos-<version>-live-rootfs.<architecture>.img`
+
[NOTE]
====
The rootfs image is the same for FCP and DASD.
====
+
. Create parameter files. The following parameters are specific for a particular virtual machine:
** For `ip=`, specify the following seven entries:
... The IP address for the machine.
... An empty string.
... The gateway.
... The netmask.
... The machine host and domain name in the form `hostname.domainname`. Omit this value to let {op-system} decide.
... The network interface name. Omit this value to let {op-system} decide.
... If you use static IP addresses, specify `none`.
** For `coreos.inst.ignition_url=`, specify the Ignition file for the machine role. Use `bootstrap.ign`, `master.ign`, or `worker.ign`. Only HTTP and HTTPS protocols are supported.
** For `coreos.live.rootfs_url=`, specify the matching rootfs artifact for the kernel and initramfs you are booting. Only HTTP and HTTPS protocols are supported.

** For installations on DASD-type disks, complete the following tasks:
... For `coreos.inst.install_dev=`, specify `dasda`.
... Use `rd.dasd=` to specify the DASD where {op-system} is to be installed.
... Leave all other parameters unchanged.
+
Example parameter file, `bootstrap-0.parm`, for the bootstrap machine:
+
[source,terminal]
----
rd.neednet=1 \
console=ttysclp0 \
coreos.inst.install_dev=dasda \
coreos.live.rootfs_url=http://cl1.provide.example.com:8080/assets/rhcos-live-rootfs.s390x.img \
coreos.inst.ignition_url=http://cl1.provide.example.com:8080/ignition/bootstrap.ign \
ip=172.18.78.2::172.18.78.1:255.255.255.0:::none nameserver=172.18.78.1 \
rd.znet=qeth,0.0.bdf0,0.0.bdf1,0.0.bdf2,layer2=1,portno=0 \
zfcp.allow_lun_scan=0 \
rd.dasd=0.0.3490
----
+
Write all options in the parameter file as a single line and make sure you have no newline characters.

** For installations on FCP-type disks, complete the following tasks:
... Use `rd.zfcp=<adapter>,<wwpn>,<lun>` to specify the FCP disk where {op-system} is to be installed. For multipathing repeat this step for each additional path.
... For multipathing, set the following parameter: `rd.multipath=default`.
... For multipathing, set the install device as: `coreos.inst.install_dev=/dev/mapper/mpatha`.
... For single-path installation, set the install device as: `coreos.inst.install_dev=sda`.
+
[NOTE]
====
If additional LUNs are configured with NPIV, FCP requires `zfcp.allow_lun_scan=0`. If you must enable `zfcp.allow_lun_scan=1` because you use a CSI driver, for example, you must configure your NPIV so that each node cannot access the boot partition of another node.
====
... Leave all other parameters unchanged.
+
[IMPORTANT]
====
Additional post-installation steps are required to fully enable multipathing. For more information, see â€œEnabling multipathing with kernel arguments on {op-system}" in _Post-installation machine configuration tasks_.
====
// Add xref once it's allowed.
+
The following is an example parameter file `worker-1.parm` for a worker node with multipathing:
+
[source,terminal]
----
rd.neednet=1 \
console=ttysclp0 \
coreos.inst.install_dev=sda \
coreos.live.rootfs_url=http://cl1.provide.example.com:8080/assets/rhcos-live-rootfs.s390x.img \
coreos.inst.ignition_url=http://cl1.provide.example.com:8080/ignition/worker.ign \
ip=172.18.78.2::172.18.78.1:255.255.255.0:::none nameserver=172.18.78.1 \
rd.znet=qeth,0.0.bdf0,0.0.bdf1,0.0.bdf2,layer2=1,portno=0 \
zfcp.allow_lun_scan=0 \
rd.zfcp=0.0.1987,0x50050763070bc5e3,0x4008400B00000000 \
rd.zfcp=0.0.19C7,0x50050763070bc5e3,0x4008400B00000000 \
rd.zfcp=0.0.1987,0x50050763071bc5e3,0x4008400B00000000 \
rd.zfcp=0.0.19C7,0x50050763071bc5e3,0x4008400B00000000
----
+
Write all options in the parameter file as a single line and make sure you have no newline characters.

. Transfer the initramfs, kernel, parameter files, and {op-system} images to z/VM, for example with FTP. For details about how to transfer the files with FTP and boot from the virtual reader, see link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/installation_guide/sect-installing-zvm-s390[Installing under Z/VM].
. Punch the files to the virtual reader of the z/VM guest virtual machine that is to become your bootstrap node.
+
See link:https://www.ibm.com/docs/en/zvm/7.1?topic=commands-punch[PUNCH] in IBM Documentation.
+
[TIP]
====
You can use the CP PUNCH command or, if you use Linux, the **vmur** command to transfer files between two z/VM guest virtual machines.
====
+
. Log in to CMS on the bootstrap machine.
. IPL the bootstrap machine from the reader:
+
----
$ ipl c
----
+
See link:https://www.ibm.com/docs/en/zvm/7.1?topic=commands-ipl[IPL] in IBM Documentation.
+
. Repeat this procedure for the other machines in the cluster.
