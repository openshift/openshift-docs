// Module included in the following assemblies:
//
// * installing/installing_ibm_z/installing-ibm-z.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc
// * installing/installing_ibm_z/installing-ibm-z-lpar.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z-lpar.adoc


ifeval::["{context}" == "installing-ibm-z"]
:ibm-z:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z"]
:ibm-z:
endif::[]
ifeval::["{context}" == "installing-ibm-z-lpar"]
:ibm-z-lpar:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z-lpar"]
:ibm-z-lpar:
endif::[]

:_mod-docs-content-type: PROCEDURE
[id="installation-user-infra-machines-iso-ibm-z_{context}"]
= Installing {op-system} and starting the {product-title} bootstrap process

ifdef::ibm-z[]
To install {product-title} on {ibm-z-name} infrastructure that you provision, you must install {op-system-first} on z/VM guest virtual machines. When you install {op-system}, you must provide the Ignition config file that was generated by the {product-title} installation program for the type of machine you are installing. If you have configured suitable networking, DNS, and load balancing infrastructure, the {product-title} bootstrap process begins automatically after the {op-system} z/VM guest virtual machines have rebooted.
endif::ibm-z[]
ifdef::ibm-z-lpar[]
To install {product-title} on {ibm-z-name} infrastructure that you provision, you must install {op-system-first} in an LPAR. When you install {op-system}, you must provide the Ignition config file that was generated by the {product-title} installation program for the type of machine you are installing. If you have configured suitable networking, DNS, and load balancing infrastructure, the {product-title} bootstrap process begins automatically after the {op-system} guest machines have rebooted.
endif::ibm-z-lpar[]

Complete the following steps to create the machines.

.Prerequisites

* An HTTP or HTTPS server running on your provisioning machine that is accessible to the machines you create.
* If you want to enable secure boot, you have obtained the appropriate Red Hat Product Signing Key and read link:https://www.ibm.com/docs/en/linux-on-systems?topic=security-secure-boot-linux-onibm-z-linuxone[Secure boot on IBM Z and IBM LinuxONE] in IBM documentation.

.Procedure

. Log in to Linux on your provisioning machine.

. Obtain the {op-system-first} kernel, initramfs, and rootfs files from the link:https://mirror.openshift.com/pub/openshift-v4/s390x/dependencies/rhcos/latest/[{op-system} image mirror].
+
[IMPORTANT]
====
The {op-system} images might not change with every release of {product-title}.
You must download images with the highest version that is less than or equal
to the {product-title} version that you install. Only use the appropriate kernel, initramfs, and rootfs artifacts described in the following procedure.
====
+
The file names contain the {product-title} version number. They resemble the following examples:

* kernel: `rhcos-<version>-live-kernel-<architecture>`
* initramfs: `rhcos-<version>-live-initramfs.<architecture>.img`
* rootfs: `rhcos-<version>-live-rootfs.<architecture>.img`
+
[NOTE]
====
The rootfs image is the same for FCP and DASD.
====
+
. Create parameter files. The following parameters are specific for a particular virtual machine:

** For `ip=`, specify the following seven entries:
... The IP address for the machine.
... An empty string.
... The gateway.
... The netmask.
... The machine host and domain name in the form `hostname.domainname`. Omit this value to let {op-system} decide.
... The network interface name. Omit this value to let {op-system} decide.
... If you use static IP addresses, specify `none`.
** For `coreos.inst.ignition_url=`, specify the Ignition file for the machine role. Use `bootstrap.ign`, `master.ign`, or `worker.ign`. Only HTTP and HTTPS protocols are supported.
** For `coreos.live.rootfs_url=`, specify the matching rootfs artifact for the kernel and initramfs you are booting. Only HTTP and HTTPS protocols are supported.
** Optional: To enable secure boot, add `coreos.inst.secure_ipl`

** For installations on DASD-type disks, complete the following tasks:
... For `coreos.inst.install_dev=`, specify `/dev/dasda`.
... Use `rd.dasd=` to specify the DASD where {op-system} is to be installed.
... Leave all other parameters unchanged.
+
Example parameter file, `bootstrap-0.parm`, for the bootstrap machine:
+
[source,terminal]
----
cio_ignore=all,!condev rd.neednet=1 \
console=ttysclp0 \
coreos.inst.install_dev=/dev/<block_device> \// <1>
coreos.inst.ignition_url=http://<http_server>/bootstrap.ign \// <2>
coreos.live.rootfs_url=http://<http_server>/rhcos-<version>-live-rootfs.<architecture>.img \// <3>
coreos.inst.secure_ipl \// <4>
ip=<ip>::<gateway>:<netmask>:<hostname>::none nameserver=<dns> \
rd.znet=qeth,0.0.bdf0,0.0.bdf1,0.0.bdf2,layer2=1,portno=0 \
rd.dasd=0.0.3490 \
zfcp.allow_lun_scan=0
----
ifdef::ibm-z[]
<1> Specify the block device type. For installations on DASD-type disks, specify `/dev/dasda`. For installations on FCP-type disks, specify `/dev/sda`.
endif::ibm-z[]
ifdef::ibm-z-lpar[]
<1> Specify the block device type. For installations on DASD-type disks, specify `/dev/dasda`. For installations on FCP-type disks, specify `/dev/sda`. For installations on NVMe-type disks, specify `/dev/nvme0n1`.
endif::ibm-z-lpar[]
<2> Specify the location of the Ignition config file. Use `bootstrap.ign`, `master.ign`, or `worker.ign`. Only HTTP and HTTPS protocols are supported.
<3> Specify the location of the `rootfs` artifact for the `kernel` and `initramfs` you are booting. Only HTTP and HTTPS protocols are supported.
<4> Optional: To enable secure boot, add `coreos.inst.secure_ipl`.
+
Write all options in the parameter file as a single line and make sure you have no newline characters.

** For installations on FCP-type disks, complete the following tasks:
... Use `rd.zfcp=<adapter>,<wwpn>,<lun>` to specify the FCP disk where {op-system} is to be installed. For multipathing repeat this step for each additional path.
+
[NOTE]
====
When you install with multiple paths, you must enable multipathing directly after the installation, not at a later point in time, as this can cause problems.
====
... Set the install device as: `coreos.inst.install_dev=/dev/disk/by-id/scsi-<serial_number>`.
+
[NOTE]
====
If additional LUNs are configured with NPIV, FCP requires `zfcp.allow_lun_scan=0`. If you must enable `zfcp.allow_lun_scan=1` because you use a CSI driver, for example, you must configure your NPIV so that each node cannot access the boot partition of another node.
====
... Leave all other parameters unchanged.
+
[IMPORTANT]
====
Additional postinstallation steps are required to fully enable multipathing. For more information, see â€œEnabling multipathing with kernel arguments on {op-system}" in _Postinstallation machine configuration tasks_.
====
// Add xref once it's allowed.
+
The following is an example parameter file `worker-1.parm` for a compute node with multipathing:
+
[source,terminal]
----
cio_ignore=all,!condev rd.neednet=1 \
console=ttysclp0 \
coreos.inst.install_dev=/dev/disk/by-id/scsi-<serial_number> \
coreos.live.rootfs_url=http://<http_server>/rhcos-<version>-live-rootfs.<architecture>.img \
coreos.inst.ignition_url=http://<http_server>/worker.ign \
ip=<ip>::<gateway>:<netmask>:<hostname>::none nameserver=<dns> \
rd.znet=qeth,0.0.bdf0,0.0.bdf1,0.0.bdf2,layer2=1,portno=0 \
rd.zfcp=0.0.1987,0x50050763070bc5e3,0x4008400B00000000 \
rd.zfcp=0.0.19C7,0x50050763070bc5e3,0x4008400B00000000 \
rd.zfcp=0.0.1987,0x50050763071bc5e3,0x4008400B00000000 \
rd.zfcp=0.0.19C7,0x50050763071bc5e3,0x4008400B00000000 \
zfcp.allow_lun_scan=0
----
+
Write all options in the parameter file as a single line and make sure you have no newline characters.

ifdef::ibm-z[]
. Transfer the initramfs, kernel, parameter files, and {op-system} images to z/VM, for example with FTP. For details about how to transfer the files with FTP and boot from the virtual reader, see link:https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/interactively_installing_rhel_over_the_network/index#installing-under-z-vm_booting-the-installation-media[Booting the installation on {ibm-z-name} to install {op-system-base} in z/VM].
. Punch the files to the virtual reader of the z/VM guest virtual machine that is to become your bootstrap node.
+
See link:https://www.ibm.com/docs/en/zvm/latest?topic=commands-punch[PUNCH] in IBM Documentation.
+
[TIP]
====
You can use the CP PUNCH command or, if you use Linux, the **vmur** command to transfer files between two z/VM guest virtual machines.
====
+
. Log in to CMS on the bootstrap machine.
. IPL the bootstrap machine from the reader:
+
----
$ ipl c
----
+
See link:https://www.ibm.com/docs/en/zvm/latest?topic=commands-ipl[IPL] in IBM Documentation.
+
endif::ibm-z[]
ifdef::ibm-z-lpar[]
. Transfer the initramfs, kernel, parameter files, and {op-system} images to the LPAR, for example with FTP. For details about how to transfer the files with FTP and boot, see link:https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/interactively_installing_rhel_over_the_network/index#installing-in-an-lpar_booting-the-installation-media[Booting the installation on {ibm-z-name} to install {op-system-base} in an LPAR].

. Boot the machine
endif::ibm-z-lpar[]

. Repeat this procedure for the other machines in the cluster.

ifeval::["{context}" == "installing-ibm-z"]
:!ibm-z:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z"]
:!ibm-z:
endif::[]
ifeval::["{context}" == "installing-ibm-z-lpar"]
:!ibm-z-lpar:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z-lpar"]
:!ibm-z-lpar:
endif::[]