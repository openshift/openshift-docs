// Module included in the following assemblies:
//
// * hosted_control_planes/hcp-deploy-virt.adoc

:_mod-docs-content-type: CONCEPT
[id="hcp-virt-reqs_{context}"]
= Requirements to deploy {hcp} on {VirtProductName}

As you prepare to deploy {hcp} on {VirtProductName}, consider the following information:

* Run the hub cluster and workers on the same platform for {hcp}.
* Each hosted cluster must have a cluster-wide unique name. A hosted cluster name cannot be the same as any existing managed cluster in order for {mce-short} to manage it.
* Do not use `clusters` as a hosted cluster name.
* A hosted cluster cannot be created in the namespace of a {mce-short} managed cluster.
* When you configure storage for {hcp}, consider the recommended etcd practices. To ensure that you meet the latency requirements, dedicate a fast storage device to all hosted control plane etcd instances that run on each control-plane node. You can use LVM storage to configure a local storage class for hosted etcd pods. For more information, see _Recommended etcd practices_ and _Persistent storage using logical volume manager storage_.

[id="hcp-virt-prereqs_{context}"]
== Prerequisites

You must meet the following prerequisites to create an {product-title} cluster on {VirtProductName}:

* You need administrator access to an {product-title} cluster, version 4.14 or later, specified by the `KUBECONFIG` environment variable.
* The {product-title} hosting cluster must have wildcard DNS routes enabled, as shown in the following DNS:
+
[source,terminal]
----
$ oc patch ingresscontroller -n openshift-ingress-operator default --type=json -p '[{ "op": "add", "path": "/spec/routeAdmission", "value": {wildcardPolicy: "WildcardsAllowed"}}]'
----
* The {product-title} hosting cluster must have {VirtProductName}, version 4.14 or later, installed on it. For more information, see xref:../../virt/install/installing-virt.adoc#installing-virt-web[Installing OpenShift Virtualization using the web console].
* The {product-title} hosting cluster must be configured with OVNKubernetes as the default pod network CNI.
* The {product-title} hosting cluster must have a default storage class. For more information, see xref:../../post_installation_configuration/post-install-storage-configuration.adoc#post-install-storage-configuration[Postinstallation storage configuration]. The following example shows how to set a default storage class:
+
[source,terminal]
----
$ oc patch storageclass ocs-storagecluster-ceph-rbd -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
----

* You need a valid pull secret file for the `quay.io/openshift-release-dev` repository. For more information, see link:https://console.redhat.com/openshift/install/platform-agnostic/user-provisioned[Install OpenShift on any x86_64 platform with user-provisioned infrastructure].
* You need to install the hosted control plane command line interface.
* Before you can provision your cluster, you need to configure a load balancer. For more information, see _Optional: Configuring MetalLB_.
* For optimal network performance, use a network maximum transmission unit (MTU) of 9000 or greater on the {product-title} cluster that hosts the KubeVirt virtual machines. If you use a lower MTU setting, network latency and the throughput of the hosted pods are affected. Enable multiqueue on node pools only when the MTU is 9000 or greater.

* The {mce-short} must have at least one managed {product-title} cluster. The `local-cluster` is automatically imported. For more information about the `local-cluster`, see link:https://docs.redhat.com/en/documentation/red_hat_advanced_cluster_management_for_kubernetes/2.12/html/clusters/cluster_mce_overview#advanced-config-engine[Advanced configuration] in the {mce-short} documentation. You can check the status of your hub cluster by running the following command:
+
[source,terminal]
----
$ oc get managedclusters local-cluster
----

[id="hcp-virt-firewall-port_{context}"]
== Firewall and port requirements

Ensure that you meet the firewall and port requirements so that ports can communicate between the management cluster, the control plane, and hosted clusters:

* The `kube-apiserver` service runs on port 6443 by default and requires ingress access for communication between the control plane components.

** If you use the `NodePort` publishing strategy, ensure that the node port that is assigned to the `kube-apiserver` service is exposed.
** If you use MetalLB load balancing, allow ingress access to the IP range that is used for load balancer IP addresses.

* If you use the `NodePort` publishing strategy, use a firewall rule for the `ignition-server` and `Oauth-server` settings.

* The `konnectivity` agent, which establishes a reverse tunnel to allow bi-directional communication on the hosted cluster, requires egress access to the cluster API server address on port 6443. With that egress access, the agent can reach the `kube-apiserver` service.

** If the cluster API server address is an internal IP address, allow access from the workload subnets to the IP address on port 6443.
** If the address is an external IP address, allow egress on port 6443 to that external IP address from the nodes.

* If you change the default port of 6443, adjust the rules to reflect that change.
* Ensure that you open any ports that are required by the workloads that run in the clusters.
* Use firewall rules, security groups, or other access controls to restrict access to only required sources. Avoid exposing ports publicly unless necessary.
* For production deployments, use a load balancer to simplify access through a single IP address.