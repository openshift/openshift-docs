// Module included in the following assemblies:
//
// * logging/cluster-logging-external.adoc

[id="cluster-logging-collector-log-forwarding-about_{context}"]
= About forwarding logs to third-party systems

Forwarding cluster logs to external third-party systems requires a combination of _outputs_ and _pipelines_ specified in a `ClusterLogForwarder` custom resource (CR) to send logs to specific endpoints inside and outside of your {product-title} cluster. You can also use _inputs_ to forward the application logs associated with a specific project to a an endpoint. 

* An _output_ is the destination for log data that you define and associate with the pipeline. An output can be one of the following types:
+
--
** `elasticsearch` to forward logs to an external Elasticsearch v5.x or v6.x instance. The `elasticsearch` output can use a TLS connection.

** `fluentdForward` to forward logs to an external log aggregation solution that supports Fluentd. This option uses the Fluentd *forward* protocols.  The `fluentForward` output can use a TLS connection. A fluentForward output can the Fluentd shared-key authentication by providing a *shared_key* field in the secret. Shared-key authentication can be used with or without TLS.

** `syslog` to forward logs to an external log aggregation solution that supports the syslog link:https://tools.ietf.org/html/rfc3164[RFC3164] or link:https://tools.ietf.org/html/rfc5424[RFC5424] protocols. The `syslog` output can use a UDP, TCP, or TLS connection.

** `kafka` to forward logs to a Kafka broker. The `kafka` output can use a TLS connection.
--
+
If the output URL scheme requires TLS (https, tls, or udps), then TLS server-side authentication is enabled. To also enable client authentication, the output must name a secret in the `openshift-logging` project. The secret must have keys of: *tls.crt*, *tls.key*, and *ca-bundler.crt* that point to the respective certificates that they represent.

* A _pipeline_ defines simple routing from one log type to one or more outputs. The log types are one of the following:
+
--
* `application` - Container logs generated by user applications running in the cluster, except infrastructure container applications.

* `infrastructure` - Container logs from pods that run in the `openshift*`, `kube*`, or `default` projects and journal logs sourced from node filesystem.

* `audit` - Logs generated by the node audit system (auditd) and the audit logs from the Kubernetes API server and the OpenShift API server. 
--
+
You can add labels to log messages using `key:value` pairs in the pipeline that are affixed to outbound log messages. For example, you could add a label to messages forwarded to others data center or label the logs by type. Labels added to objects are also forwarded with the log message.

* An _input_ forwards the application logs associated with a specific project to a pipeline.

Note the following:

* You can also forward logs to the  internal {product-title} Elasticsearch instance using `default` in the pipeline. You do not need to configure the default output. If you do configure a `default` output, you will receive an error message because the `default` output is reserved for the Cluster Logging Operator.

* If you want to forward all logs to only the internal OpenShift Container Platform Elasticsearch instance, do not configure the Log Forwarding API.

* If a `ClusterLogForwarder` object exists, logs are not forwarded to the default Elasticsearch instance, unless there is a pipeline with the `default` output.

* If a `ClusterLogForwarder` object does not exist, logs are forwarded to the default Elasticsearch instance.

* If you do not define a pipeline for a log type, the logs of the undefined types are dropped. For example, if you specify a pipeline for the `application` and `audit` types, but do not specify a pipeline for the `infrastructure` type, `infrastructure` logs are dropped.

* You can use multiple types of outputs in the `ClusterLogForwarder` custom resource (CR) to send logs to servers that support different protocols. 

* The internal {product-title} Elasticsearch instance does not provide secure storage for audit logs. We recommend you ensure that the system to which you forward audit logs is compliant with your organizational and governmental regulations and is properly secured. {product-title} cluster logging does not comply with those regulations.

* You are responsible for creating and maintaining any additional configurations that external destinations might require, such as keys and secrets, service accounts, port openings, or global proxy configuration.

The following example forwards the infrasturcture logs to a secure external Elasticsearch instance, the audit logs to an insecure external Elasticsearch instance, the application logs to a Kafka broker, and the application logs from the `my-apps-logs` project to the internal Elasticsearch instance. 

.Sample log forwarding outputs and pipelines
[source,yaml]
----
apiVersion: "logging.openshift.io/v1"
kind: ClusterLogForwarder
metadata:
  name: instance <1>
spec:
  outputs:
   - name: elasticsearch-insecure <2>
     type: "elasticsearch"
     url: http://elasticsearch.insecure.com:9200
   - name: elasticsearch-secure <3>
     type: "elasticsearch"
     url: https://elasticsearch.secure.com:9200
     secret:
        name: elasticsearch
   - name: kafka-app <4>
     type: "kafka"
     url: tls://kafka.secure.com:9093/app-topic
  inputs: <5>
   - name: my-app-logs 
     application:
        namespaces:
        - my-project
  pipelines:
   - name: audit-logs <6>
     inputRefs:
      - audit
     outputRefs:
      - elasticsearch-insecure
     labels:
       datacenter: east
   - name: infrastructure-logs <7>
     inputRefs:
      - infrastructure
     outputRefs:
      - elasticsearch-secure
     labels:
       datacenter: west
   - name: my-app <8>
     inputRefs:
      - my-app-logs
     outputRefs:
      - default
   - inputRefs: <9>
      - application   
     outputRefs:
      - kafka-app
     labels:
       datacenter: south
----
<1> The name of the `ClusterLogForward` CR must be `instance`.
<2> Configuration for an insecure Elasticsearch output:
** A name to describe the output.
** The type of output: `elasticsearch`.
** The insecure URL and port of the Elasticseach instance as a valid absolute URL, including the prefix.
<3> Configuration for an secure Elasticsearch output using a secret with a secure URL.
** A name to describe the output.
** The type of output: `elasticsearch`.
** The secure URL and port of the Elasticseach instance as a valid absolute URL, including the prefix.
** The secret required by the endpoint for TLS communication. The secret must exist in the `openshift-logging` project.
<4> Configuration for a Kafka output using a client-authenticated TLS communication over a secure URL
** A name to describe the output.
** The type of output: `kafka`.
** Specify the URL and port of the Kafka broker as a valid absolute URL, including the prefix.
<5> Configuration for an input to filter application logs from the `my-namespace` projects.
<6> Configuration for a pipeline to send audit logs to the insecure external Elasticsearch instance:
** Optional. A name to describe the pipeline.
** The `inputRefs` is the log type, in this example `audit`.
** The `outputRefs` is the name of the output to use.
** Optional. A label to add to the logs.
<7> Configuration for a pipeline to send infrastructure logs to  the secure external Elasticsearch instance:
<8> Configuration for a pipeline to send logs from the `my-project` project to the internal Elasticsearch instance.
** The `inputRefs` is a specific input: `my-app-logs`.
** The `outputRefs` is `default`.
** Optional. A label to add to the logs.
<9> Configuration for a pipeline to send logs to the Kafka broker, with no pipeline name:
** The `inputRefs` is the log type, in this example `application`.
** The `outputRefs` is the name of the output to use.
** Optional. A label to add to the logs.
