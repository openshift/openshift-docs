include::modules/cnv_document_attributes.adoc[]
:context: cnv-release-notes

= {RN_BookName}

[[RN_intro]]
== Product overview

include::modules/cnv_introduction_to_cnv.adoc[leveloffset=+2]

[[RN_new_features]]
== New and changed features

=== Introducing Operators
* Operators power the installation of KubeVirt, Containerized Data Importer, and
the web-ui in {ProductName} 1.4.

=== Updated KubeVirt API
* A new version of the KubeVirt API is included in {ProductName} 1.4. Several
important changes are reflected in the latest configuration file templates.
** The `apiVersion` has been updated from `kubevirt.io/v1alpha2` to
`kubevirt.io/v1alpha3`.
** The `volumeName` attribute no longer exists. Ensure that each disk name
matches the corresponding volume name in all configuration files.
** All instances of `registryDisk` must be updated to `containerDisk` in
configuration files.


[[RN_resolved_issues]]
== Resolved issues

* The `runc` package version `runc-1.0.0-54` contained a bug that caused the
virt-launcher to crash if FIPS was disabled. The version of `runc` containing
the fix is now shipped with Red Hat Enterprise Linux 7 Extras.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1650512[*BZ1650512*])

* In the Create Virtual Machine Wizard, using the *PXE* source option with the
*Start virtual machine on creation* option resulted in the boot order not
changing after stopping and starting the virtual machine. This issue has been
resolved. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1648245[*BZ#1648245*])
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1647447[*BZ#1647447*])


[[RN_known_issues]]
== Known issues

* CPU Manager, a feature that provides CPU pinning in {product-title}, is
currently disabled in {ProductName} due to performance regressions. CPU Manager
does not consider the physical CPU topology, resulting in sub-optimal pinning
when hyper-threading is enabled.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1667854[*BZ#1667854*])

* Red Hat OpenShift Container Storage versions before 3.11.1 are not
compatible with {ProductName}. In the incompatible versions, Gluster nodes do
not deploy with CRI-O as the container runtime.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1651270[*BZ#1651270*])

* When installing {ProductName} 1.4, the `ansible-playbook` command fails
if the `multus` image and its underlying layers are not pulled within the
timeout period. As a workaround, wait a few minutes and try the command again.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1664274[*BZ#1664274*])

* The `virtctl image-upload` command fails if the `--uploadproxy-url` value
ends with a trailing slash. If you use a custom URL, ensure that it does not end
with a trailing slash before running the command.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1660888[*BZ#1660888*])

* The limit for compute node devices is currently 110. This limit cannot be
configured, but scaling up to more than 110 devices will be supported in a
future release.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1673438[*BZ#1673438*])

* When uploading or importing a disk image to a PVC, the space allocated for the
PVC must be at least `2 * actual image size + virtual image size`. Otherwise,
the virtual machine does not boot successfully.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1676824[*BZ#1676824*])
+
[NOTE]
====
The command `$ ls -sh <image_file>` reports the actual size. For uncompressed raw
images, the command `$ ls -lh <image_file>` reports the virtual size. For QCOW2
images, use `$ qemu-img info <image_file>` to get both values.
====

* If you create a new DataVolume while a PVC already exists with the same name,
the DataVolume enters an unrecoverable error state. If this DataVolume is
associated with a virtual machine, or if it was created with the
`dataVolumeTemplates` section of a virtual machine configuration file, then the
virtual machine will fail to start. In these cases, the underlying DataVolume
error will not be propagated to the virtual machine.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1669163[*BZ#1669163*])

* If a CDI import into a PVC fails, a request to delete the PVC might not work
immediately. Instead, the importer pod gets stuck in a `CrashLoopBackOff` state,
causing the PVC to enter a `Terminating` phase.  To resolve this issue, find the
importer pod associated with the PVC and delete it. The PVC will then be
deleted. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1673683[*BZ#1673683*])

* If you use `virtctl image-upload` to upload a QCOW2 image to a PVC, the
operation might fail with the error `Unexpected return value 500`, resulting in
an unusable PVC. This can be caused by a bug where conversion of certain QCOW2
images during an upload operation exceeds predefined process limits.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1679134[*BZ#1679134*])
+
To confirm
that the failure was caused by this bug, check the associated `uploadserver`
pod logs for a message like this:
+
----
1 uploadserver.go:203] Saving stream failed: data stream copy failed: Local qcow to raw conversion failed: could not convert local qcow2 image to raw: qemu-img execution failed: signal: killed
----
+
As a workaround, locally convert the file to compressed raw format and then
upload the result:
+
----
$ qemu-img convert -f qcow2 -O raw <failing-image.qcow2> image.raw
$ gzip image.raw
$ virtctl image-upload ... image.raw.gz
----

* When a virtual machine provisioned from a `URL` source is started for the first time, the virtual machine will be in the *Importing* state while {ProductName} imports the container from the endpoint URL. +
Restarting a virtual machine while it is in the *Importing* state results in an error.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1673921[*BZ#1673921*])

* If the kubelet on a node crashes or restarts, this causes the kubelet to
incorrectly report 0 KVM devices. Virtual machines are not properly scheduled
on affected nodes.
+
Verify the number of devices that the kubelet reports by running:
+
----
$ oc get node $NODE | grep devices.kubevirt
----
+
The output on an affected node shows `devices.kubevirt.io/kvm: 0`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1681175[*BZ#1681175*])
+
[NOTE]
====
As a workaround, kill the relevant `virt-handler` pod on the affected node. The
pod automatically restarts, and the kubelet reports the correct number of
available KVM devices.
====

* If a virtual machine is connected to the pod network by using `bridge` mode,
the virtual machine might stop if the kubelet gets restarted.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1685118[*BZ#1685118*])





