[[install-running-installation-playbooks]]
= Running Installation Playbooks
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:
ifdef::openshift-enterprise[]
:pb-prefix: /usr/share/ansible/openshift-ansible/
endif::[]
ifdef::openshift-origin[]
:pb-prefix: ~/openshift-ansible/
endif::[]
include::_snippets/glusterfs.adoc[]

toc::[]

[[install-before-initiating-installation]]
== Before Initiating Installation

Before installing {product-title}, you must first:

- See the
xref:prerequisites.adoc#install-config-install-prerequisites[Prerequisites]
and xref:host_preparation.adoc#install-config-install-host-preparation[Host Preparation] 
topics to prepare your hosts. This includes verifying system and environment
requirements per component type and properly installing and configuring the
*docker* service. It also includes installing Ansible version 2.4 or later, as
the installation method is based on Ansible playbooks and as such requires
directly invoking Ansible.

- See the
xref:configuring_inventory_file.adoc#install-config-configuring-inventory-file[Configuring
Your Inventory File] 
topic to define your environment and desired {product-title} cluster
configuration. This inventory file will be used to initiate the installation,
and should be saved and maintained for future cluster upgrades as well.

If you are interested in installing {product-title} using the system container
method (required for RHEL Atomic Host systems), see
xref:index.adoc#planning-installation-types[RPM Versus System Container Considerations] 
to ensure that you understand the differences between these methods, then return
to this topic to continue.

For large-scale installs, including suggestions for optimizing install time,
see the xref:../scaling_performance/install_practices.adoc#scaling-performance-install-best-practices[Scaling and Performance Guide].

[NOTE]
====
To alternatively install {product-title} solely as a stand-alone registry, see
xref:stand_alone_registry.adoc#install-config-installing-stand-alone-registry[Installing a Stand-alone Registry].
====

ifdef::openshift-origin[]
[[advanced-cloud-providers]]
=== Cloud installation

{product-title} VMs can be provisioned in a cloud environment. You can use
Ansible playbooks to automate defining of your cloud hosted infrastructure and
applying post-provision configuration for the supported cloud providers.

==== OpenStack provider

As an alternate, you can install {product-title} using the OpenStack CLI.
For more information, See the
link:https://access.redhat.com/documentation/en-us/reference_architectures/2017/html-single/deploying_and_managing_red_hat_openshift_container_platform_3.6_on_red_hat_openstack_platform_10[reference architecture] for {product-title} 3.6 and Red Hat OpenStack Platform 10.

As a prerequisite to using the OpenStack CLI, first provision VMs and configure the cloud
infrastructure, such as networking, storage, firewall, and security groups.
For information on these configuration tasks using the reference architecture,
see the
xref:prerequisites#prereq-cloud-provider-considerations[cloud provider considerations]
and link:https://github.com/openshift/openshift-ansible/tree/master/playbooks/openstack[Ansible playbooks]
to automate it. See also
xref:../install_config/configuring_openstack#install-config-configuring-openstack[Configuring for OpenStack]
and
xref:configuring_inventory_file.adoc#configuring-ansible[Configuring Your Inventory File].

////
*The following IMPORTANT admonition to be included in openshift-enterprise distro after this "advanced-cloud-providers" section is no longer solely included in openshift-origin distro. Commenting out for now

[IMPORTANT]
====
The reference architecture for automated installations based on
link:https://docs.openstack.org/heat/latest[OpenStack Heat] templates for
link:https://access.redhat.com/documentation/en-us/reference_architectures/2017/html/deploying_red_hat_openshift_container_platform_3.4_on_red_hat_openstack_platform_10[{product-title} 3.4 on Red Hat OpenStack Platform 10]
is no longer supported. For the Red Hat OpenStack 13 release, this process is replaced with the
link:https://github.com/openshift/openshift-ansible/tree/master/playbooks/openstack[Ansible driven deployment solution].
For automated installations, follow that guide.
====
////
endif::[]


[[running-the-installation-playbooks]]
== Running the Installation Playbooks

The installer uses modularized playbooks allowing administrators to install
specific components as needed. By breaking up the roles and playbooks, there is
better targeting of ad hoc administration tasks. This results in an increased
level of control during installations and results in time savings. The playbooks
and their ordering are detailed below in
xref:running-the-advanced-installation-individual-components[Running Individual Component Playbooks].

[IMPORTANT]
====
While RHEL Atomic Host is supported for running {product-title} services as
system container, the installation method utilizes Ansible, which is not
available in RHEL Atomic Host. The RPM-based installer must therefore be run
from
ifdef::openshift-enterprise[]
a RHEL 7 system.
endif::[]
ifdef::openshift-origin[]
a supported version of Fedora, CentOS, or RHEL.
endif::[]
The host initiating the installation does not need to be intended for inclusion
in the {product-title} cluster, but it can be. Alternatively, a
xref:running-the-advanced-installation-system-container[containerized version of the installer] 
is available as a system container, which can be run from a RHEL
Atomic Host system.
====

After you have
xref:configuring_inventory_file.adoc#configuring-ansible[configured Ansible] by
defining an inventory file in *_/etc/ansible/hosts_*, run the installation
playbook via Ansible using either the RPM-based or containerized installer.

[[running-the-advanced-installation-rpm]]
=== Running the RPM-based Installer

The RPM-based installer uses Ansible installed via RPM packages to run playbooks
and configuration files available on the local host.

[IMPORTANT]
====
Do not run OpenShift Ansible playbooks under `nohup`. Using `nohup` with the
playbooks causes file descriptors to be created and not closed. Therefore, the
system can run out of files to open and the playbook will fail.
====

To run the RPM-based installer:

. Run the *_prequisites.yml_* playbook. This must be run only once before
deploying a new cluster. Use the following command, specifying `-i` if your
inventory file located somewhere other than *_/etc/ansible/hosts_*:
+
----
ifdef::openshift-enterprise[]
# ansible-playbook [-i /path/to/inventory] \
    /usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook [-i /path/to/inventory] \
    ~/openshift-ansible/playbooks/prerequisites.yml
endif::[]
----

. Run the *_deploy_cluster.yml_* playbook to initiate the cluster installation:
+
----
ifdef::openshift-enterprise[]
# ansible-playbook [-i /path/to/inventory] \
    /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook [-i /path/to/inventory] \
    ~/openshift-ansible/playbooks/deploy_cluster.yml
endif::[]
----

If for any reason the installation fails, before re-running the installer, see
xref:installer-known-issues[Known Issues] to check for any specific
instructions or workarounds.

[WARNING]
====
The installer caches playbook configuration values for 10 minutes, by default. If you change any system, network, or inventory configuration, 
and then re-run the installer within that 10 minute period, the new values are not used, and the previous values are used instead. 
You can delete the contents of the cache, which is defined
by the `fact_caching_connection` value in the *_/etc/ansible/ansible.cfg_* file. An example of this file is
shown in xref:../scaling_performance/install_practices.adoc#scaling-performance-install-optimization[Recommended Installation Practices].  
====

[[running-the-advanced-installation-containerized]]
=== Running the Containerized Installer

The
ifdef::openshift-enterprise[]
*openshift3/ose-ansible*
endif::[]
ifdef::openshift-origin[]
*openshift/origin-ansible*
endif::[]
image is a containerized version of the {product-title} installer.
This installer image provides the same functionality as the RPM-based
installer, but it runs in a containerized environment that provides all
of its dependencies rather than being installed directly on the host.
The only requirement to use it is the ability to run a container.

[[running-the-advanced-installation-system-container]]
==== Running the Installer as a System Container

The installer image can be used as a
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/managing_containers/running_system_containers[system container].
System containers are stored and run outside of the traditional *docker* service.
This enables running the installer image from one of the target hosts without
concern for the install restarting *docker* on the host.

To use the Atomic CLI to run the installer as a run-once system container, perform the following steps as the *root* user:

. Run the *_prerequisites.yml_* playbook:
+
----
# atomic install --system \
    --storage=ostree \
    --set INVENTORY_FILE=/path/to/inventory \ <1>
    --set PLAYBOOK_FILE=/usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml \
    --set OPTS="-v" \
ifdef::openshift-enterprise[]
    registry.access.redhat.com/openshift3/ose-ansible:v3.9
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.9
endif::[]
----
<1> Specify the location on the local host for your inventory file.
+
This command runs a set of prerequiste tasks by using the inventory file
specified and the `root` user's SSH configuration.

. Run the *_deploy_cluster.yml_* playbook:
+
----
# atomic install --system \
    --storage=ostree \
    --set INVENTORY_FILE=/path/to/inventory \ <1>
    --set PLAYBOOK_FILE=/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml \
    --set OPTS="-v" \
ifdef::openshift-enterprise[]
    registry.access.redhat.com/openshift3/ose-ansible:v3.9
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.9
endif::[]
----
<1> Specify the location on the local host for your inventory file.
+
This command initiates the cluster installation by using the inventory file
specified and the `root` user's SSH configuration. It logs the output on the
terminal and also saves it in the *_/var/log/ansible.log_* file. The first time
this command is run, the image is imported into
link:https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/content_management_guide/managing_ostree_content[OSTree]
storage (system containers use this rather than *docker* daemon storage). On
subsequent runs, it reuses the stored image.
+
If for any reason the installation fails, before re-running the installer, see
xref:installer-known-issues[Known Issues] to check for any specific instructions
or workarounds.

[[running-the-advanced-installation-system-container-other-playbooks]]
==== Running Other Playbooks

You can use the `PLAYBOOK_FILE` environment variable to specify other playbooks
you want to run by using the containerized installer. The default value of the `PLAYBOOK_FILE` is
*_/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml_*, which is the
main cluster installation playbook, but you can set it to the path of another
playbook inside the container.

For example, to run the
xref:configuring-cluster-pre-install-checks[pre-install checks] playbook before
installation, use the following command:

----
# atomic install --system \
    --storage=ostree \
    --set INVENTORY_FILE=/path/to/inventory \
    --set PLAYBOOK_FILE=/usr/share/ansible/openshift-ansible/playbooks/openshift-checks/pre-install.yml \ <1>
    --set OPTS="-v" \ <2>
ifdef::openshift-enterprise[]
    registry.access.redhat.com/openshift3/ose-ansible:v3.9
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.9
endif::[]
----
<1> Set `PLAYBOOK_FILE` to the full path of the playbook starting at the
*_playbooks/_* directory. Playbooks are located in the same locations as with
the RPM-based installer.
<2> Set `OPTS` to add command line options to `ansible-playbook`.

[[running-the-advanced-installation-docker]]
==== Running the Installer as a Docker Container

The installer image can also run as a *docker* container anywhere that *docker* can run.

[WARNING]
====
This method must not be used to run the installer on one of the hosts being configured,
as the install may restart *docker* on the host, disrupting the installer container execution.
====

[NOTE]
====
Although this method and the system container method above use the same image, they
run with different entry points and contexts, so runtime parameters are not the same.
====

At a minimum, when running the installer as a *docker* container you must provide:

* SSH key(s), so that Ansible can reach your hosts.
* An Ansible inventory file.
* The location of the Ansible playbook to run against that inventory.

Here is an example of how to run an install via `docker`, which must be run by a
non-*root* user with access to `docker`:

. First, run the *_prerequisites.yml_* playbook:
+
----
$ docker run -t -u `id -u` \ <1>
    -v $HOME/.ssh/id_rsa:/opt/app-root/src/.ssh/id_rsa:Z \ <2>
    -v $HOME/ansible/hosts:/tmp/inventory:Z \ <3>
    -e INVENTORY_FILE=/tmp/inventory \ <3>
    -e PLAYBOOK_FILE=playbooks/prerequisites.yml \ <4>
    -e OPTS="-v" \ <5>
ifdef::openshift-enterprise[]
    registry.access.redhat.com/openshift3/ose-ansible:v3.9
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.9
endif::[]
----
<1> `-u `id -u`` makes the container run with the same UID as the current
user, which allows that user to use the SSH key inside the container (SSH
private keys are expected to be readable only by their owner).
<2> `-v $HOME/.ssh/id_rsa:/opt/app-root/src/.ssh/id_rsa:Z` mounts your SSH key
(`$HOME/.ssh/id_rsa`) under the container user's `$HOME/.ssh`
(*_/opt/app-root/src_* is the `$HOME` of the user in the container). If you
mount the SSH key into a non-standard location you can add an environment
variable with `-e ANSIBLE_PRIVATE_KEY_FILE=/the/mount/point` or set
`ansible_ssh_private_key_file=/the/mount/point` as a variable in the inventory
to point Ansible at it. Note that the SSH key is mounted with the `:Z` flag.
This is required so that the container can read the SSH key under its restricted
SELinux context. This also means that your original SSH key file will be
re-labeled to something like `system_u:object_r:container_file_t:s0:c113,c247`.
For more details about `:Z`, check the `docker-run(1)` man page. Keep this
in mind when providing these volume mount specifications because this might have
unexpected consequences: for example, if you mount (and therefore re-label) your
whole `$HOME/.ssh` directory it will block the host's *sshd* from accessing your
public keys to login. For this reason you may want to use a separate copy of the
SSH key (or directory), so that the original file labels remain untouched.
<3> `-v $HOME/ansible/hosts:/tmp/inventory:Z` and `-e INVENTORY_FILE=/tmp/inventory`
mount a static Ansible inventory file into the container as *_/tmp/inventory_*
and set the corresponding environment variable to point at it. As with the SSH
key, the inventory file SELinux labels may need to be relabeled by using the
`:Z` flag to allow reading in the container, depending on the existing label
(for files in a user `$HOME` directory this is likely to be needed). So again
you may prefer to copy the inventory to a dedicated location before mounting it.
The inventory file can also be downloaded from a web server if you specify the
`INVENTORY_URL` environment variable, or generated dynamically using
`DYNAMIC_SCRIPT_URL` to specify an executable script that provides a dynamic
inventory.
<4> `-e PLAYBOOK_FILE=playbooks/prerequisites.yml` specifies the playbook to run (in
this example, the prereqsuites playbook) as a relative path from the top level
directory of *openshift-ansible* content. The full path from the RPM can also be
used, as well as the path to any other playbook file in the container.
<5> `-e OPTS="-v"` supplies arbitrary command line options (in this case,
`-v` to increase verbosity) to the `ansible-playbook` command that runs
inside the container.

. Next, run the *_deploy_cluster.yml_* playbook to initiate the cluster
installation:
+
----
$ docker run -t -u `id -u` \
    -v $HOME/.ssh/id_rsa:/opt/app-root/src/.ssh/id_rsa:Z \
    -v $HOME/ansible/hosts:/tmp/inventory:Z \
    -e INVENTORY_FILE=/tmp/inventory \
    -e PLAYBOOK_FILE=playbooks/deploy_cluster.yml \
    -e OPTS="-v" \
ifdef::openshift-enterprise[]
    registry.access.redhat.com/openshift3/ose-ansible:v3.9
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.9
endif::[]
----

[[running-the-advanced-installation-individual-components]]
=== Running Individual Component Playbooks

The main installation playbook *_{pb-prefix}playbooks/deploy_cluster.yml_* runs a
set of individual component playbooks in a specific order, and the installer
reports back at the end what phases you have gone through. If the installation
fails during a phase, you are notified on the screen along with the errors from
the Ansible run.

After you resolve the issue, rather than run the entire installation over again,
you can pick up from the failed phase. You must then run each of the remaining
playbooks in order:

----
# ansible-playbook [-i /path/to/inventory] <playbook_file_location>
----

The following table is sorted in order of when each individual component
playbook is run:

.Individual Component Playbook Run Order
[options="header",cols="1,3"]
|===
|Playbook Name |File Location

|Health Check
|*_{pb-prefix}playbooks/openshift-checks/pre-install.yml_*

|etcd Install
|*_{pb-prefix}playbooks/openshift-etcd/config.yml_*

|NFS Install
|*_{pb-prefix}playbooks/openshift-nfs/config.yml_*

|Load Balancer Install
|*_{pb-prefix}playbooks/openshift-loadbalancer/config.yml_*

|Master Install
|*_{pb-prefix}playbooks/openshift-master/config.yml_*

|Master Additional Install
|*_{pb-prefix}playbooks/openshift-master/additional_config.yml_*

|Node Install
|*_{pb-prefix}playbooks/openshift-node/config.yml_*

|GlusterFS Install
|*_{pb-prefix}playbooks/openshift-glusterfs/config.yml_*

|Hosted Install
|*_{pb-prefix}playbooks/openshift-hosted/config.yml_*

|Web Console Install
|*_{pb-prefix}playbooks/openshift-web-console/config.yml_*

|Metrics Install
|*_{pb-prefix}playbooks/openshift-metrics/config.yml_*

|Logging Install
|*_{pb-prefix}playbooks/openshift-logging/config.yml_*

|Prometheus Install
|*_{pb-prefix}playbooks/openshift-prometheus/config.yml_*

|Service Catalog Install
|*_{pb-prefix}playbooks/openshift-service-catalog/config.yml_*

|Management Install
|*_{pb-prefix}playbooks/openshift-management/config.yml_*
|===

[[advanced-verifying-the-installation]]
== Verifying the Installation

// tag::verifying-the-installation[]
After the installation completes:

. Verify that the master is started and nodes
are registered and reporting in *Ready* status. _On the master host_, run the
following as root:
+
----
# oc get nodes
NAME                   STATUS    ROLES     AGE       VERSION
master.example.com     Ready     master    7h        v1.9.1+a0ce1bc657
node1.example.com      Ready     compute   7h        v1.9.1+a0ce1bc657
node2.example.com      Ready     compute   7h        v1.9.1+a0ce1bc657
----

. To verify that the web console is installed correctly, use the master host name
and the web console port number to access the web console with a web browser.
+
For example, for a master host with a host name of `master.openshift.com` and
using the default port of `8443`, the web console would be found at `\https://master.openshift.com:8443/console`.

// end::verifying-the-installation[]

[discrete]
[[verifying-multiple-etcd-hosts]]
==== Verifying Multiple etcd Hosts

If you installed multiple etcd hosts:

. First, verify that the *etcd* package, which provides the `etcdctl`
command, is installed:
+
----
# yum install etcd
----

. On a master host, verify the etcd cluster health, substituting for the FQDNs
of your etcd hosts in the following:
+
----
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/origin/master/master.etcd-ca.crt \
    --cert-file=/etc/origin/master/master.etcd-client.crt \
    --key-file=/etc/origin/master/master.etcd-client.key cluster-health
----

. Also verify the member list is correct:
+
----
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/origin/master/master.etcd-ca.crt \
    --cert-file=/etc/origin/master/master.etcd-client.crt \
    --key-file=/etc/origin/master/master.etcd-client.key member list
----

[discrete]
[[verifying-multiple-masters-haproxy]]
==== Verifying Multiple Masters Using HAProxy

If you installed multiple masters using HAProxy as a load balancer, browse to
the following URL according to your *[lb]* section definition and check
HAProxy's status:

----
http://<lb_hostname>:9000
----

You can verify your installation by consulting the
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Load_Balancer_Administration/ch-haproxy-setup-VSA.html[HAProxy
Configuration documentation].

[[optionally-securing-builds]]
== Optionally Securing Builds

Running `docker build` is a privileged process, so the container has more access
to the node than might be considered acceptable in some multi-tenant
environments. If you do not trust your users, you can use a more secure option
at the time of installation. Disable Docker builds on the cluster and require
that users build images outside of the cluster. See
xref:../admin_guide/securing_builds.adoc#admin-guide-securing-builds[Securing
Builds by Strategy] for more information on this optional process.

[[uninstalling-advanced]]
== Uninstalling {product-title}

You can uninstall {product-title} hosts in your cluster by running the
*_uninstall.yml_* playbook. This playbook deletes {product-title} content
installed by Ansible, including:

- Configuration
- Containers
- Default templates and image streams
- Images
- RPM packages

The playbook will delete content for any hosts defined in the inventory file
that you specify when running the playbook. If you want to uninstall
{product-title} across all hosts in your cluster, run the playbook using the
inventory file you used when installing {product-title} initially or ran most
recently:

----
ifdef::openshift-enterprise[]
# ansible-playbook [-i /path/to/file] \
    /usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook [-i /path/to/file] \
    ~/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
----

[[uninstalling-nodes-advanced]]
=== Uninstalling Nodes

You can also uninstall node components from specific hosts using the
*_uninstall.yml_* playbook while leaving the remaining hosts and cluster alone:

[WARNING]
====
This method should only be used when attempting to uninstall specific node hosts
and not for specific masters or etcd hosts, which would require further
configuration changes within the cluster.
====

. First follow the steps in
xref:../admin_guide/manage_nodes.adoc#deleting-nodes[Deleting Nodes] to
remove the node object from the cluster, then continue with the remaining steps
in this procedure.

. Create a different inventory file that only references those hosts. For
example, to only delete content from one node:
+
----
[OSEv3:children]
nodes <1>

[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

[nodes]
node3.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}" <2>
----
<1> Only include the sections that pertain to the hosts you are interested in
uninstalling.
<2> Only include hosts that you want to uninstall.

. Specify that new inventory file using the `-i` option when running the
*_uninstall.yml_* playbook:
+
----
ifdef::openshift-enterprise[]
# ansible-playbook -i /path/to/new/file \
    /usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook -i /path/to/new/file \
    ~/openshift-ansible/playbooks/adhoc/uninstall.yml
endif::[]
----

When the playbook completes, all {product-title} content should be removed from
any specified hosts.

[[installer-known-issues]]
== Known Issues

- On failover in multiple master clusters, it is possible for the controller
manager to overcorrect, which causes the system to run more pods than what was
intended. However, this is a transient event and the system does correct itself
over time. See https://github.com/kubernetes/kubernetes/issues/10030 for
details.

- On failure of the Ansible installer, you must start from a clean operating
system installation. If you are using virtual machines, start from a fresh
image. If you are using bare metal machines, see
xref:uninstalling-advanced[Uninstalling {product-title}] for instructions.

include::release_notes/ocp_3_9_release_notes.adoc[tag=BZ1558672]

== What's Next?

Now that you have a working {product-title} instance, you can:

- Deploy an xref:../install_config/registry/index.adoc#install-config-registry-overview[integrated Docker registry].
- Deploy a xref:../install_config/router/index.adoc#install-config-router-overview[router].
ifdef::openshift-origin[]
- xref:../install_config/imagestreams_templates.adoc#install-config-imagestreams-templates[Populate your {product-title} installation]
with a useful set of Red Hat-provided image streams and templates.
endif::[]
