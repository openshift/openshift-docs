[[install-running-installation-playbooks]]
= Installing {product-title}
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:
ifdef::openshift-enterprise[]
:pb-prefix: /usr/share/ansible/openshift-ansible/
endif::[]
ifdef::openshift-origin[]
:pb-prefix: ~/openshift-ansible/
endif::[]
include::_snippets/glusterfs.adoc[]

toc::[]

To install a {product-title} cluster, you run a series of Ansible playbooks.

[IMPORTANT]
====
Running Ansible playbooks with the `--tags` or `--check` options is not supported by Red Hat.
====

[NOTE]
====
To install {product-title} as a stand-alone registry, see
xref:stand_alone_registry.adoc#install-config-installing-stand-alone-registry[Installing a Stand-alone Registry].
====

[[install-before-initiating-installation]]
== Prerequisites

Before installing {product-title}, prepare your cluster hosts:

- Review the
xref:prerequisites.adoc#install-config-install-prerequisites[System and environment requirements].

- If you will have a large cluster, review the
xref:../scaling_performance/install_practices.adoc#scaling-performance-install-best-practices[Scaling and Performance Guide]
for suggestions for optimizing installation time.

- xref:host_preparation.adoc#install-config-install-host-preparation[Prepare your hosts].
This process includes verifying system and environment
requirements per component type, installing and configuring the
*docker* service, and installing Ansible version 2.6 or later. You must install
Ansible to run the installation playbooks.

- xref:configuring_inventory_file.adoc#install-config-configuring-inventory-file[Configure
your inventory file] to define your environment and {product-title} cluster
configuration. Both your initial installation and future cluster upgrades
are based on this inventory file.

- If you are installing {product-title} on Red Hat Enterprise Linux, decide if
you want to use the
xref:index.adoc#planning-installation-types[RPM or system container]
installation method. The system container method is required for RHEL Atomic
Host systems.

////
[IMPORTANT]
====
Starting in {product-title} 3.10, setting `openshift_node_group_name` per host
to a node group is required for all cluster installations whether you are using
the default node group definitions and ConfigMaps or are customizing your own.
See
xref:configuring_inventory_file.adoc#configuring-inventory-defining-node-group-and-host-mappings[Defining Node Groups and Host Mappings]
for more details if you have not set them yet.
====
////

ifdef::openshift-origin[]
[[advanced-cloud-providers]]
=== Cloud installation

You can provision {product-title} VMs in a cloud environment. You can use
Ansible playbooks to automate the definition of your cloud hosted infrastructure and
application of post-provision configuration for the supported cloud providers.

==== OpenStack provider

You can install {product-title} by using the OpenStack CLI.
For more information, See the
link:https://access.redhat.com/documentation/en-us/reference_architectures/2017/html-single/deploying_and_managing_red_hat_openshift_container_platform_3.6_on_red_hat_openstack_platform_10[reference architecture] for {product-title} 3.6 and Red Hat OpenStack Platform 10.

As a prerequisite to using the OpenStack CLI, first provision VMs and configure the cloud
infrastructure, such as networking, storage, firewall, and security groups.
For information about these configuration tasks, see the
xref:prerequisites#prereq-cloud-provider-considerations[cloud provider considerations]
and link:https://github.com/openshift/openshift-ansible/tree/master/playbooks/openstack[Ansible playbooks]
to automate them. See also
xref:../install_config/configuring_openstack#install-config-configuring-openstack[Configuring for OpenStack]
and
xref:configuring_inventory_file.adoc#configuring-ansible[Configuring Your Inventory File].

////
*The following IMPORTANT admonition to be included in openshift-enterprise distro after this "advanced-cloud-providers" section is no longer solely included in openshift-origin distro. Commenting out for now

[IMPORTANT]
====
The reference architecture for automated installations based on
link:https://docs.openstack.org/heat/latest[OpenStack Heat] templates for
link:https://access.redhat.com/documentation/en-us/reference_architectures/2017/html/deploying_red_hat_openshift_container_platform_3.4_on_red_hat_openstack_platform_10[{product-title} 3.4 on Red Hat OpenStack Platform 10]
is no longer supported. For the Red Hat OpenStack 13 release, this process is replaced with the
link:https://github.com/openshift/openshift-ansible/tree/master/playbooks/openstack[Ansible driven deployment solution].
For automated installations, follow that guide.
====
////
endif::[]

[[running-the-advanced-installation-rpm]]
=== Running the RPM-based installer

The RPM-based installer uses Ansible installed via RPM packages to run playbooks
and configuration files available on the local host.

[IMPORTANT]
====
Do not run OpenShift Ansible playbooks under `nohup`. Using `nohup` with the
playbooks causes file descriptors to be created but not closed. Therefore, the
system can run out of files to open and the playbook fails.
====

To run the RPM-based installer:

. Change to the playbook directory and run the *_prerequisites.yml_* playbook. This playbook installs required software
packages, if any, and modifies the container runtimes. Unless you need to
configure the container runtimes, run this playbook only once, before you
deploy a cluster the first time:
+
----
$ cd /usr/share/ansible/openshift-ansible
$ ansible-playbook [-i /path/to/inventory] \ <1>
  playbooks/prerequisites.yml
----
<1> If your inventory file is not in the *_/etc/ansible/hosts_* directory,
specify `-i` and the path to the inventory file.

. Change to the playbook directory and run the *_deploy_cluster.yml_* playbook to initiate the cluster installation:
+
----
$ cd /usr/share/ansible/openshift-ansible
$ ansible-playbook [-i /path/to/inventory] \
    playbooks/deploy_cluster.yml
----
<1> If your inventory file is not in the *_/etc/ansible/hosts_* directory,
specify `-i` and the path to the inventory file.

.
** If your installation succeeded,
xref:advanced-verifying-the-installation[verify the installation].
** If your installation failed, xref:advanced-retrying-installation[retry the installation].

[[running-the-advanced-installation-containerized]]
=== Running the containerized installer

The
ifdef::openshift-enterprise[]
*openshift3/ose-ansible*
endif::[]
ifdef::openshift-origin[]
*openshift/origin-ansible*
endif::[]
image is a containerized version of the {product-title} installer.
This installer image provides the same functionality as the RPM-based
installer, but it runs in a containerized environment that provides all
of its dependencies rather than being installed directly on the host.
The only requirement to use it is the ability to run a container.

[[running-the-advanced-installation-system-container]]
==== Running the installer as a system container

The installer image can be used as a
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/managing_containers/running_system_containers[system container].
System containers are stored and run outside of the traditional *docker* service.
This enables running the installer image from one of the target hosts without
concern for the install restarting *docker* on the host.

To use the Atomic CLI to run the installer as a run-once system container, perform the following steps as the *root* user:

. Run the *_prerequisites.yml_* playbook:
+
----
# atomic install --system \
    --storage=ostree \
    --set INVENTORY_FILE=/path/to/inventory \ <1>
    --set PLAYBOOK_FILE=/usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml \
    --set OPTS="-v" \
ifdef::openshift-enterprise[]
    registry.redhat.io/openshift3/ose-ansible:v3.11
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.11
endif::[]
----
<1> Specify the location on the local host for your inventory file.
+
This command runs a set of prerequiste tasks by using the inventory file
specified and the `root` user's SSH configuration.

. Run the *_deploy_cluster.yml_* playbook:
+
----
# atomic install --system \
    --storage=ostree \
    --set INVENTORY_FILE=/path/to/inventory \ <1>
    --set PLAYBOOK_FILE=/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml \
    --set OPTS="-v" \
ifdef::openshift-enterprise[]
    registry.redhat.io/openshift3/ose-ansible:v3.11
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.11
endif::[]
----
<1> Specify the location on the local host for your inventory file.
+
This command initiates the cluster installation by using the inventory file
specified and the `root` user's SSH configuration. It logs the output on the
terminal and also saves it in the *_/var/log/ansible.log_* file. The first time
this command is run, the image is imported into
link:https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/content_management_guide/managing_ostree_content[OSTree]
storage (system containers use this rather than *docker* daemon storage). On
subsequent runs, it reuses the stored image.
+
If for any reason the installation fails, before re-running the installer, see
xref:installer-known-issues[Known Issues] to check for any specific instructions
or workarounds.

[[running-the-advanced-installation-system-container-other-playbooks]]
==== Running other playbooks

You can use the `PLAYBOOK_FILE` environment variable to specify other playbooks
you want to run by using the containerized installer. The default value of the `PLAYBOOK_FILE` is
*_/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml_*, which is the
main cluster installation playbook, but you can set it to the path of another
playbook inside the container.

For example, to run the
xref:configuring-cluster-pre-install-checks[pre-install checks] playbook before
installation, use the following command:

----
# atomic install --system \
    --storage=ostree \
    --set INVENTORY_FILE=/path/to/inventory \
    --set PLAYBOOK_FILE=/usr/share/ansible/openshift-ansible/playbooks/openshift-checks/pre-install.yml \ <1>
    --set OPTS="-v" \ <2>
ifdef::openshift-enterprise[]
    registry.redhat.io/openshift3/ose-ansible:v3.11
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.11
endif::[]
----
<1> Set `PLAYBOOK_FILE` to the full path of the playbook starting at the
*_playbooks/_* directory. Playbooks are located in the same locations as with
the RPM-based installer.
<2> Set `OPTS` to add command line options to `ansible-playbook`.

[[running-the-advanced-installation-docker]]
==== Running the installer as a container

The installer image can also run as a *docker* container anywhere that *docker* can run.

[WARNING]
====
This method must not be used to run the installer on one of the hosts being configured,
because the installer might restart *docker* on the host and disrupt the installation.
====

[NOTE]
====
Although this method and the system container method above use the same image, they
run with different entry points and contexts, so runtime parameters are not the same.
====

At a minimum, when running the installer as a *docker* container you must provide:

* SSH key(s), so that Ansible can reach your hosts.
* An Ansible inventory file.
* The location of the Ansible playbook to run against that inventory.

Here is an example of how to run an install via `docker`, which must be run by a
non-*root* user with access to `docker`:

. First, run the *_prerequisites.yml_* playbook:
+
----
$ docker run -t -u `id -u` \ <1>
    -v $HOME/.ssh/id_rsa:/opt/app-root/src/.ssh/id_rsa:Z \ <2>
    -v $HOME/ansible/hosts:/tmp/inventory:Z \ <3>
    -e INVENTORY_FILE=/tmp/inventory \ <3>
    -e PLAYBOOK_FILE=playbooks/prerequisites.yml \ <4>
    -e OPTS="-v" \ <5>
ifdef::openshift-enterprise[]
    registry.redhat.io/openshift3/ose-ansible:v3.11
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.11
endif::[]
----
<1> `-u `id -u`` makes the container run with the same UID as the current
user, which allows that user to use the SSH key inside the container. SSH
private keys are expected to be readable only by their owner.
<2> `-v $HOME/.ssh/id_rsa:/opt/app-root/src/.ssh/id_rsa:Z` mounts your SSH key,
`$HOME/.ssh/id_rsa`, under the container user's `$HOME/.ssh` directory.
*_/opt/app-root/src_* is the `$HOME` of the user in the container. If you
mount the SSH key into a different location, add an environment
variable with `-e ANSIBLE_PRIVATE_KEY_FILE=/the/mount/point` or set
`ansible_ssh_private_key_file=/the/mount/point` as a variable in the inventory
to point Ansible to it. Note that the SSH key is mounted with the `:Z` flag.
This flag is required so that the container can read the SSH key under its restricted
SELinux context. This also means that your original SSH key file will be
re-labeled to something like `system_u:object_r:container_file_t:s0:c113,c247`.
For more details about `:Z`, review the `docker-run(1)` man page. Keep this
in mind when providing these volume mount specifications because this might have
unexpected consequences, For example, if you mount (and therefore re-label) your
whole `$HOME/.ssh` directory, it will block the host's *sshd* from accessing your
public keys to log in. For this reason, you might want to use a separate copy of the
SSH key or directory so that the original file labels remain untouched.
<3> `-v $HOME/ansible/hosts:/tmp/inventory:Z` and `-e INVENTORY_FILE=/tmp/inventory`
mount a static Ansible inventory file into the container as *_/tmp/inventory_*
and set the corresponding environment variable to point at it. As with the SSH
key, the inventory file SELinux labels might need to be relabeled by using the
`:Z` flag to allow reading in the container, depending on the existing label.
For files in a user `$HOME` directory, this is likely to be needed.
You might prefer to copy the inventory to a dedicated location before you mount it.
You can also download the inventory file from a web server if you specify the
`INVENTORY_URL` environment variable or generate it dynamically by using the
`DYNAMIC_SCRIPT_URL` parameter to specify an executable script that provides a dynamic
inventory.
<4> `-e PLAYBOOK_FILE=playbooks/prerequisites.yml` specifies the playbook to run
as a relative path from the top level directory of *openshift-ansible* content.
In this example, you specify the prerequisites playbook.
You can also specify the full path from the RPM or
the path to any other playbook file in the container.
<5> `-e OPTS="-v"` supplies arbitrary command line options
to the `ansible-playbook` command that runs inside the container. In this example,
specify `-v` to increase verbosity.

. Next, run the *_deploy_cluster.yml_* playbook to initiate the cluster
installation:
+
----
$ docker run -t -u `id -u` \
    -v $HOME/.ssh/id_rsa:/opt/app-root/src/.ssh/id_rsa:Z \
    -v $HOME/ansible/hosts:/tmp/inventory:Z \
    -e INVENTORY_FILE=/tmp/inventory \
    -e PLAYBOOK_FILE=playbooks/deploy_cluster.yml \
    -e OPTS="-v" \
ifdef::openshift-enterprise[]
    registry.redhat.io/openshift3/ose-ansible:v3.11
endif::[]
ifdef::openshift-origin[]
    docker.io/openshift/origin-ansible:v3.11
endif::[]
----

[[running-the-advanced-installation-openstack]]
==== Running the Installation Playbook for OpenStack

To install {product-title} on an existing OpenStack installation, use the
OpenStack playbook. For more information about the playbook, including detailed
prerequisites, see
link:https://github.com/openshift/openshift-ansible/tree/master/playbooks/openstack#installation[the OpenStack Provisioning readme file].

To run the playbook, run the following command:

----
$ ansible-playbook --user openshift \
  -i openshift-ansible/playbooks/openstack/inventory.py \
  -i inventory \
  openshift-ansible/playbooks/openstack/openshift-cluster/provision_install.yml
----

[[running-the-installation-playbooks]]
=== About the installation playbooks

The installer uses modularized playbooks so that administrators can install
specific components as needed. By breaking up the roles and playbooks, there is
better targeting of ad hoc administration tasks. This results in an increased
level of control during installations and results in time savings.

The main installation playbook *_{pb-prefix}playbooks/deploy_cluster.yml_* runs a
set of individual component playbooks in a specific order, and the installer
reports back at the end what phases you have gone through. If the installation
fails, you are notified which phase failed along with the errors from
the Ansible run.

[IMPORTANT]
====
While RHEL Atomic Host is supported for running {product-title} services as
system container, the installation method uses Ansible, which is not
available in RHEL Atomic Host. The RPM-based installer must therefore be run
from
ifdef::openshift-enterprise[]
a RHEL 7 system.
endif::[]
ifdef::openshift-origin[]
a supported version of Fedora, CentOS, or RHEL.
endif::[]
The host initiating the installation does not need to be intended for inclusion
in the {product-title} cluster, but it can be. Alternatively, a
xref:running-the-advanced-installation-system-container[containerized version of the installer]
is available as a system container, which can be run from a RHEL
Atomic Host system.
====

[[advanced-retrying-installation]]
== Retrying the installation

If the Ansible installer fails, you can still install {product-title}:

. Review the xref:installer-known-issues[Known Issues] to check for any specific
instructions or workarounds.

. Address the errors from your installation.

. Determine if you need to uninstall and reinstall or retry the installation:
** If you did not modify the SDN configuration or generate new certificates,
retry the installation.
** If you modified the SDN configuration, generated new certificates, or the
installer fails again, you must either start over with a clean operating system
installation or xref:../install/uninstalling_cluster.adoc#uninstalling[uninstall] and
install again.
** If you use virtual machines, start from a new image or
xref:../install/uninstalling_cluster.adoc#uninstalling[uninstall] and install again.
** If you use bare metal machines,
xref:../install/uninstalling_cluster.adoc#uninstalling[uninstall] and install again.


. Retry the installation:
** You can run the *_deploy_cluster.yml_* playbook again.
** You can run the remaining individual installation playbooks.
+
If you want to run only the remaining playbooks, start by running the playbook
for the phase that failed and then run each of the remaining playbooks in
order. Run each playbook with the following command:
+
----
# ansible-playbook [-i /path/to/inventory] <playbook_file_location>
----
+
The following table lists the playbooks in the order that they must run:
+
.Individual Component Playbook Run Order
[options="header",cols="1,3"]
|===
|Playbook Name |File Location

|Health Check
|*_{pb-prefix}playbooks/openshift-checks/pre-install.yml_*

|Node Bootstrap
|*_{pb-prefix}playbooks/openshift-node/bootstrap.yml_*

|etcd Install
|*_{pb-prefix}playbooks/openshift-etcd/config.yml_*

|NFS Install
|*_{pb-prefix}playbooks/openshift-nfs/config.yml_*

|Load Balancer Install
|*_{pb-prefix}playbooks/openshift-loadbalancer/config.yml_*

|Master Install
|*_{pb-prefix}playbooks/openshift-master/config.yml_*

|Master Additional Install
|*_{pb-prefix}playbooks/openshift-master/additional_config.yml_*

|Node Join
|*_{pb-prefix}playbooks/openshift-node/join.yml_*

|GlusterFS Install
|*_{pb-prefix}playbooks/openshift-glusterfs/config.yml_*

|Hosted Install
|*_{pb-prefix}playbooks/openshift-hosted/config.yml_*

|Monitoring Install
|*_{pb-prefix}playbooks/openshift-monitoring/config.yml_*

|Web Console Install
|*_{pb-prefix}playbooks/openshift-web-console/config.yml_*

|Admin Console Install
|*_{pb-prefix}playbooks/openshift-console/config.yml_*

|Metrics Install
|*_{pb-prefix}playbooks/openshift-metrics/config.yml_*

|Logging Install
|*_{pb-prefix}playbooks/openshift-logging/config.yml_*

|Availability Monitoring Install
|*_{pb-prefix}playbooks/openshift-monitor-availability/config.yml_*

|Service Catalog Install
|*_{pb-prefix}playbooks/openshift-service-catalog/config.yml_*

|Management Install
|*_{pb-prefix}playbooks/openshift-management/config.yml_*

|Descheduler Install
|*_{pb-prefix}playbooks/openshift-descheduler/config.yml_*

|Node Problem Detector Install
|*_{pb-prefix}playbooks/openshift-node-problem-detector/config.yml_*

|Autoheal Install
|*_{pb-prefix}playbooks/openshift-autoheal/config.yml_*

|Operator Lifecycle Manager (OLM) Install (Technology Preview)
|*_{pb-prefix}playbooks/olm/config.yml_*
|===

[[advanced-verifying-the-installation]]
== Verifying the Installation

// tag::verifying-the-installation[]
After the installation completes:

. Verify that the master is started and nodes
are registered and reporting in *Ready* status. _On the master host_, run the
following command as root:
+
----
# oc get nodes
NAME                   STATUS    ROLES     AGE       VERSION
master.example.com     Ready     master    7h        v1.9.1+a0ce1bc657
node1.example.com      Ready     compute   7h        v1.9.1+a0ce1bc657
node2.example.com      Ready     compute   7h        v1.9.1+a0ce1bc657
----

. To verify that the web console is installed correctly, use the master host name
and the web console port number to access the web console with a web browser.
+
For example, for a master host with a host name of `master.openshift.com` and
using the default port of `8443`, the web console URL is `\https://master.openshift.com:8443/console`.

// end::verifying-the-installation[]

[discrete]
[[verifying-multiple-etcd-hosts]]
==== Verifying Multiple etcd Hosts

If you installed multiple etcd hosts:

. First, verify that the *etcd* package, which provides the `etcdctl`
command, is installed:
+
----
# yum install etcd
----

. On a master host, verify the etcd cluster health, substituting for the FQDNs
of your etcd hosts in the following:
+
----
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/origin/master/master.etcd-ca.crt \
    --cert-file=/etc/origin/master/master.etcd-client.crt \
    --key-file=/etc/origin/master/master.etcd-client.key cluster-health
----

. Also verify the member list is correct:
+
----
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/origin/master/master.etcd-ca.crt \
    --cert-file=/etc/origin/master/master.etcd-client.crt \
    --key-file=/etc/origin/master/master.etcd-client.key member list
----

[discrete]
[[verifying-multiple-masters-haproxy]]
==== Verifying Multiple Masters Using HAProxy

If you installed multiple masters using HAProxy as a load balancer, open
the following URL and check HAProxy's status:

----
http://<lb_hostname>:9000 <1>
----
<1> Provide the load balancer host name listed in the `[lb]` section of your
inventory file.

You can verify your installation by consulting the
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Load_Balancer_Administration/ch-haproxy-setup-VSA.html[HAProxy
Configuration documentation].

[[optionally-securing-builds]]
== Optionally securing builds

Running `docker build` is a privileged process, so the container has more access
to the node than might be considered acceptable in some multi-tenant
environments. If you do not trust your users, you can configure a more secure option
after installation. Disable Docker builds on the cluster and require
that users build images outside of the cluster. See
xref:../admin_guide/securing_builds.adoc#admin-guide-securing-builds[Securing
Builds by Strategy] for more information about this optional process.

[[installer-known-issues]]
== Known Issues

- On failover in multiple master clusters, it is possible for the controller
manager to overcorrect, which causes the system to run more pods than what was
intended. However, this is a transient event and the system does correct itself
over time. See https://github.com/kubernetes/kubernetes/issues/10030 for
details.

== What's Next?

Now that you have a working {product-title} instance, you can:

- Deploy an xref:../install_config/registry/index.adoc#install-config-registry-overview[integrated container image registry].
- Deploy a xref:../install_config/router/index.adoc#install-config-router-overview[router].
ifdef::openshift-origin[]
- xref:../install_config/imagestreams_templates.adoc#install-config-imagestreams-templates[Populate your {product-title} installation]
with a useful set of Red Hat-provided image streams and templates.
endif::[]
