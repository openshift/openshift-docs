[[install-config-installing-stand-alone-registry]]
= Installing a Stand-alone Deployment of OpenShift Container Registry
{product-author}
{product-version]
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

[[install-config-about-ocr]]
== About OpenShift Container Registry

{product-title} is a fully-featured enterprise solution that includes an
integrated container registry called
xref:../architecture/infrastructure_components/image_registry.adoc#integrated-openshift-registry[OpenShift
Container Registry] (OCR). Alternatively, instead of deploying {product-title}
as a full PaaS environment for developers, you can install OCR as a stand-alone
container registry to run on-premise or in the cloud.

When installing a stand-alone deployment of OCR, a cluster of masters and nodes
is still installed, similar to a typical {product-title} installation. Then, the
container registry is deployed to run on the cluster. This stand-alone
deployment option is useful for administrators that want a container registry,
but do not require the full {product-title} environment that includes the
developer-focused web console and application build and deployment tools.

OCR provides the following capabilities:

- A user-focused xref:../install/configuring_inventory_file.adoc#advanced-install-configuring-registry-console[registry web console], Cockpit.
- xref:../install_config/registry/securing_and_exposing_registry.adoc#securing-the-registry[Secured traffic] by default, served via TLS.
- Global xref:../install_config/configuring_authentication.adoc#install-config-configuring-authentication[identity provider authentication].
- A
xref:../architecture/core_concepts/projects_and_users.adoc#architecture-core-concepts-projects-and-users[project namespace] model to enable teams to collaborate through
xref:../architecture/additional_concepts/authorization.adoc#architecture-additional-concepts-authorization[role-based access control (RBAC)] authorization.
- A xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#architecture-infrastructure-components-kubernetes-infrastructure[Kubernetes-based cluster] to manage services.
- An image abstraction called xref:../architecture/core_concepts/builds_and_image_streams.adoc#image-streams[image streams] to enhance image management.

Administrators may want to deploy a stand-alone OCR to manage a registry
separately that supports multiple {product-title} clusters. A stand-alone OCR
also enables administrators to separate their registry to satisfy their own
security or compliance requirements.

[[registry-minimum-hardware-requirements]]
== Minimum Hardware Requirements

Installing a stand-alone OCR has the following hardware requirements:

- Physical or virtual system, or an instance running on a public or private IaaS.
- Base OS:
ifdef::openshift-origin[]
Fedora 21, CentOS 7.4, or
endif::[]
RHEL 7.3, 7.4, or 7.5 with the "Minimal" installation option and the latest packages from the
RHEL 7 Extras channel, or RHEL Atomic Host 7.4.5 or later.
- NetworkManager 1.0 or later
- 2 vCPU.
- Minimum 16 GB RAM.
- Minimum 15 GB hard disk space for the file system containing *_/var/_*.
- An additional minimum 15 GB unallocated space to be used for Docker's storage
back end; see xref:host_preparation.adoc#configuring-docker-storage[Configuring Docker Storage] for details.

[IMPORTANT]
====
{product-title} only supports servers with x86_64 architecture.
====

[NOTE]
====
Meeting the *_/var/_* file system sizing requirements in RHEL Atomic Host
requires making changes to the default configuration. See
https://access.redhat.com/documentation/en/red-hat-enterprise-linux-atomic-host/version-7/getting-started-with-containers/#managing_storage_in_red_hat_enterprise_linux_atomic_host[Managing
Storage in Red Hat Enterprise Linux Atomic Host] for instructions on configuring
this during or after installation.
====

[[registry-supported-system-topologies]]
== Supported System Topologies

The following system topologies are supported for stand-alone OCR:

[horizontal]
All-in-one::
A single host that includes the master, node, etcd, and registry components.
Multiple Masters (Highly-Available)::
Three hosts with all components included on each (master, node, etcd, and
registry), with the masters configured for native high-availability.

[[registry-host-preparation]]
== Host Preparation

Before installing stand-alone OCR, all of the same steps detailed in the
xref:host_preparation.adoc#install-config-install-host-preparation[Host Preparation] topic for installing a full {product-title} PaaS must be performed.
This includes registering and subscribing the host(s) to the proper
repositories, installing or updating certain packages, and setting up Docker and
its storage requirements.

Follow the steps in the
xref:host_preparation.adoc#install-config-install-host-preparation[Host Preparation] topic, then continue to
xref:registry-installation-methods[Stand-alone Registry Installation Methods].

[[registry-installation-methods]]
== Installing Using Ansible

When installing stand-alone OCR, the steps are mostly the same as installing a
full {product-title} cluster using Ansible, as described in the full
xref:index.adoc#install-planning[cluster installation] process. The main
difference is that you must set `deployment_subtype=registry` in the inventory
file within the `[OSEv3:vars]` section for the playbooks to follow the registry
installation path.

See the following example inventory files for the different supported system
topologies:

.All-in-one Stand-alone OpenShift Container Registry Inventory File
----
# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
etcd

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=root

openshift_master_default_subdomain=apps.test.example.com

# If ansible_ssh_user is not root, ansible_become must be set to true
#ansible_become=true

openshift_deployment_type=openshift-enterprise
deployment_subtype=registry <1>
openshift_hosted_infra_selector="" <2>

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# host group for masters
[masters]
registry.example.com

# host group for etcd
[etcd]
registry.example.com

# host group for nodes
[nodes]
registry.example.com
----
<1> Set `deployment_subtype=registry` to ensure installation of stand-alone OCR and
not a full {product-title} environment.
<2> Allows the registry and its web console to be scheduled on the single host.

.Multiple Masters (Highly-Available) Stand-alone OpenShift Container Registry Inventory File
----
# Create an OSEv3 group that contains the master, nodes, etcd, and lb groups.
# The lb group lets Ansible configure HAProxy as the load balancing solution.
# Comment lb out if your load balancer is pre-configured.
[OSEv3:children]
masters
nodes
etcd
lb

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
openshift_deployment_type=openshift-enterprise
deployment_subtype=registry <1>

openshift_master_default_subdomain=apps.test.example.com

# Uncomment the following to enable htpasswd authentication; defaults to
# DenyAllPasswordIdentityProvider.
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# Native high availability cluster method with optional load balancer.
# If no lb group is defined installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=openshift-internal.example.com
openshift_master_cluster_public_hostname=openshift-cluster.example.com

# apply updated node defaults
openshift_node_kubelet_args={'pods-per-core': ['10'], 'max-pods': ['250'], 'image-gc-high-threshold': ['90'], 'image-gc-low-threshold': ['80']}

# enable ntp on masters to ensure proper failover
openshift_clock_enabled=true

# host group for masters
[masters]
master1.example.com
master2.example.com
master3.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# Specify load balancer host
[lb]
lb.example.com

# host group for nodes, includes region info
[nodes]
master[1:3].example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
----
<1> Set `deployment_subtype=registry` to ensure installation of stand-alone OCR and
not a full {product-title} environment.

After you have configured Ansible by defining an inventory file in *_/etc/ansible/hosts_*:

. Run the *_prequisites.yml_* playbook to configure base packages and Docker.
This must be run only once before deploying a new cluster. Use the following command, specifying `-i` if your
inventory file located somewhere other than *_/etc/ansible/hosts_*:
+
[IMPORTANT]
====
The host that you run the Ansible playbook on must have at least 75MiB of free
memory per host in the inventory.
====
+
----
ifdef::openshift-enterprise[]
# ansible-playbook  [-i /path/to/inventory] \
    /usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook [-i /path/to/inventory] \
    ~/openshift-ansible/playbooks/prerequisites.yml
endif::[]
----

. Run the *_deploy_cluster.yml_* playbook to initiate the installation:
+
----
ifdef::openshift-enterprise[]
# ansible-playbook  [-i /path/to/inventory] \
    /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml
endif::[]
ifdef::openshift-origin[]
# ansible-playbook [-i /path/to/inventory] \
    ~/openshift-ansible/playbooks/deploy_cluster.yml
endif::[]
----

[NOTE]
====
For more detailed usage information on the cluster installation process,
including a comprehensive list of available Ansible variables, see the full
documentation starting with xref:index.adoc#install-planning[Planning Your Installation].
====
