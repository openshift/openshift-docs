[[install-config-example-inventories]]
= Example Inventory Files
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:
ifdef::openshift-enterprise[]
:pb-prefix: /usr/share/ansible/openshift-ansible/
endif::[]
ifdef::openshift-origin[]
:pb-prefix: ~/openshift-ansible/
endif::[]
include::_snippets/glusterfs.adoc[]

toc::[]

== Overview

After getting to know the basics of
xref:configuring_inventory_file.adoc#install-config-configuring-inventory-file[configuring your own inventory file],
you can review the following example inventories which describe various
environment topographies, including xref:multiple-masters[using multiple
masters] for high availability. You can choose an example that matches your
requirements, modify it to match your own environment, and use it as your
inventory file when
xref:running_install.adoc#running-the-installation-playbooks[running the installation].

[IMPORTANT]
====
The following example inventories use the default set of node groups when
setting `openshift_node_group_name` per host in the `[nodes]` group. To define
and use your own custom node group definitions, set the `openshift_node_groups`
variable in the inventory file. See
xref:configuring_inventory_file.adoc#configuring-inventory-defining-node-group-and-host-mappings[Defining Node Groups and Host Mappings]
for details.
====

[[single-master]]
== Single Master Examples

You can configure an environment with a single master and multiple nodes, and
either a single or multiple number of etcd hosts.

[NOTE]
====
Moving from a single master cluster to multiple masters after installation is
not supported.
====

[[single-master-multi-node-ai]]
=== Single Master, Single etcd, and Multiple Nodes

The following table describes an example environment for a single
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[master]
(with a single etcd instance running as a static pod on the same host), two
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
for hosting user applications, and two nodes with the `node-role.kubernetes.io/infra=true` label for hosting
xref:configuring_inventory_file.adoc#configuring-dedicated-infrastructure-nodes[dedicated infrastructure]:

[options="header"]
|===

|Host Name |Component/Role(s) to Install

|*master.example.com*
|Master, etcd, and node

|*node1.example.com*
.2+.^|Compute node

|*node2.example.com*

|*infra-node1.example.com*
.2+.^|Infrastructure node

|*infra-node2.example.com*
|===

You can see these example hosts present in the *[masters]*, *[etcd]*, and
*[nodes]* sections of the following example inventory file:

.Single Master, Single etcd, and Multiple Nodes Inventory File
----
# Create an OSEv3 group that contains the masters, nodes, and etcd groups
[OSEv3:children]
masters
nodes
etcd

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=root

# If ansible_ssh_user is not root, ansible_become must be set to true
#ansible_become=true

ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
oreg_url=example.com/openshift3/ose-${component}:${version}
oreg_auth_user=${user_name}
oreg_auth_password=${password}
openshift_examples_modify_imagestreams=true
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

ifdef::openshift-origin[]
# uncomment the following to enable htpasswd authentication; defaults to AllowAllPasswordIdentityProvider
endif::[]
ifdef::openshift-enterprise[]
# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
endif::[]
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# host group for masters
[masters]
master.example.com

# host group for etcd
[etcd]
master.example.com

# host group for nodes, includes region info
[nodes]
master.example.com openshift_node_group_name='node-config-master'
node1.example.com openshift_node_group_name='node-config-compute'
node2.example.com openshift_node_group_name='node-config-compute'
infra-node1.example.com openshift_node_group_name='node-config-infra'
infra-node2.example.com openshift_node_group_name='node-config-infra'
----

// tag::exampleinventorynodelabel[]
[IMPORTANT]
====
See xref:configuring_inventory_file.adoc#configuring-node-host-labels[Configuring Node Host Labels] to ensure
you understand the default node selector requirements and node label
considerations beginning in {product-title} 3.9.
====
// end::exampleinventorynodelabel[]

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[[single-master-multi-etcd-multi-node-ai]]
=== Single Master, Multiple etcd, and Multiple Nodes

The following table describes an example environment for a single
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[master],
three
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[etcd]
hosts, two
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
for hosting user applications, and two nodes with the `node-role.kubernetes.io/infra=true` label for hosting
xref:configuring_inventory_file.adoc#configuring-dedicated-infrastructure-nodes[dedicated infrastructure]:

[options="header"]
|===

|Host Name |Component/Role(s) to Install

|*master.example.com*
|Master and node

|*etcd1.example.com*
.3+.^|etcd

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Compute node

|*node2.example.com*

|*infra-node1.example.com*
.2+.^|Dedicated infrastructure node

|*infra-node2.example.com*
|===

You can see these example hosts present in the *[masters]*, *[nodes]*, and
*[etcd]* sections of the following example inventory file:

.Single Master, Multiple etcd, and Multiple Nodes Inventory File
----
# Create an OSEv3 group that contains the masters, nodes, and etcd groups
[OSEv3:children]
masters
nodes
etcd

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
oreg_url=example.com/openshift3/ose-${component}:${version}
oreg_auth_user=${user_name}
oreg_auth_password=${password}
openshift_examples_modify_imagestreams=true
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

ifdef::openshift-origin[]
# uncomment the following to enable htpasswd authentication; defaults to AllowAllPasswordIdentityProvider
endif::[]
ifdef::openshift-enterprise[]
# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
endif::[]
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# host group for masters
[masters]
master.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# host group for nodes, includes region info
[nodes]
master.example.com openshift_node_group_name='node-config-master'
node1.example.com openshift_node_group_name='node-config-compute'
node2.example.com openshift_node_group_name='node-config-compute'
infra-node1.example.com openshift_node_group_name='node-config-infra'
infra-node2.example.com openshift_node_group_name='node-config-infra'
----

include::install/example_inventories.adoc[tag=exampleinventorynodelabel]

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[[multiple-masters]]
== Multiple Masters Examples

You can configure an environment with multiple masters, multiple etcd hosts,
and multiple nodes. Configuring
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#high-availability-masters[multiple
masters for high availability] (HA) ensures that the cluster has no single point
of failure.

[NOTE]
====
Moving from a single master cluster to multiple masters after installation is
not supported.
====

When configuring multiple masters, the cluster installation process supports the
`native` high availability (HA) method. This method leverages the native HA
master capabilities built into {product-title} and can be combined with any load
balancing solution.

If a host is defined in the *[lb]* section of the inventory file, Ansible
installs and configures HAProxy automatically as the load balancing solution. If
no host is defined, it is assumed you have pre-configured an external load
balancing solution of your choice to balance the master API (port 8443) on all
master hosts.

[NOTE]
====
This HAProxy load balancer is intended to demonstrate the API server's HA mode
and is not recommended for production environments. If you are deploying to a
cloud provider, Red Hat recommends deploying a cloud-native TCP-based load
balancer or take other steps to provide a highly available load balancer.
====

For an external load balancing solution, you must have:

* A pre-created load balancer virtual IP (VIP) configured for SSL passthrough.
* A VIP listening on the port specified by the xref:advanced-master-ports[`openshift_master_api_port`]
value (8443 by default) and proxying back to all master hosts on that port.
* A domain name for VIP registered in DNS.
** The domain name will become the value of both
`openshift_master_cluster_public_hostname` and
`openshift_master_cluster_hostname` in the {product-title} installer.

See
link:https://github.com/redhat-cop/openshift-playbooks/blob/master/playbooks/installation/load_balancing.adoc[the External Load Balancer Integrations example in Github] for more information. For more on the high
availability master architecture, see
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[Kubernetes Infrastructure].

[NOTE]
====
The cluster installation process does not currently support multiple HAProxy
load balancers in an active-passive setup. See the
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Load_Balancer_Administration/ch-lvs-overview-VSA.html[Load Balancer Administration documentation] for post-installation amendments.
====

To configure multiple masters, refer to
xref:multi-masters-using-native-ha-ai[Multiple Masters with Multiple etcd]

[[multi-masters-using-native-ha-ai]]
=== Multiple Masters Using Native HA with External Clustered etcd

The following describes an example environment for three
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[masters]
using the `native` HA method:, one HAProxy load balancer, three
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[etcd]
hosts, two
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
for hosting user applications, and two nodes with the `node-role.kubernetes.io/infra=true` label for hosting
xref:configuring_inventory_file.adoc#configuring-dedicated-infrastructure-nodes[dedicated infrastructure]:

[options="header"]
|===

|Host Name |Component/Role(s) to Install

|*master1.example.com*
.3+.^|Master (clustered using native HA) and node

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|HAProxy to load balance API master endpoints

|*etcd1.example.com*
.3+.^|etcd

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Compute node

|*node2.example.com*

|*infra-node1.example.com*
.2+.^|Dedicated infrastructure node

|*infra-node2.example.com*
|===

You can see these example hosts present in the *[masters]*, *[etcd]*, *[lb]*,
and *[nodes]* sections of the following example inventory file:

.Multiple Masters Using HAProxy Inventory File
----
# Create an OSEv3 group that contains the master, nodes, etcd, and lb groups.
# The lb group lets Ansible configure HAProxy as the load balancing solution.
# Comment lb out if your load balancer is pre-configured.
[OSEv3:children]
masters
nodes
etcd
lb

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
oreg_url=example.com/openshift3/ose-${component}:${version}
oreg_auth_user=${user_name}
oreg_auth_password=${password}
openshift_examples_modify_imagestreams=true
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

ifdef::openshift-origin[]
# uncomment the following to enable htpasswd authentication; defaults to AllowAllPasswordIdentityProvider
endif::[]
ifdef::openshift-enterprise[]
# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
endif::[]
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# Native high availbility cluster method with optional load balancer.
# If no lb group is defined installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=openshift-internal.example.com
openshift_master_cluster_public_hostname=openshift-cluster.example.com

# apply updated node defaults
openshift_node_groups=[{'name': 'node-config-all-in-one', 'labels': ['node-role.kubernetes.io/master=true', 'node-role.kubernetes.io/infra=true', 'node-role.kubernetes.io/compute=true'], 'edits': [{ 'key': 'kubeletArguments.pods-per-core','value': ['20']}]}]

# host group for masters
[masters]
master1.example.com
master2.example.com
master3.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# Specify load balancer host
[lb]
lb.example.com

# host group for nodes, includes region info
[nodes]
master[1:3].example.com openshift_node_group_name='node-config-master'
node1.example.com openshift_node_group_name='node-config-compute'
node2.example.com openshift_node_group_name='node-config-compute'
infra-node1.example.com openshift_node_group_name='node-config-infra'
infra-node2.example.com openshift_node_group_name='node-config-infra'
----

include::install/example_inventories.adoc[tag=exampleinventorynodelabel]

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.

[[multi-masters-single-etcd-using-native-ha]]
=== Multiple Masters Using Native HA with Co-located Clustered etcd

The following describes an example environment for three
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[masters]
using the `native` HA method (with
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[etcd]
running as a static pod on each host), one HAProxy load balancer, two
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
for hosting user applications, and two nodes with the `node-role.kubernetes.io/infra=true` label for hosting
xref:configuring_inventory_file.adoc#configuring-dedicated-infrastructure-nodes[dedicated infrastructure]:

[options="header"]
|===

|Host Name |Component/Role(s) to Install

|*master1.example.com*
.3+.^|Master (clustered using native HA) and node with etcd running as a static
pod on each host

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|HAProxy to load balance API master endpoints

|*node1.example.com*
.2+.^|Compute node

|*node2.example.com*

|*infra-node1.example.com*
.2+.^|Dedicated infrastructure node

|*infra-node2.example.com*
|===

You can see these example hosts present in the *[masters]*, *[etcd]*, *[lb]*,
and *[nodes]* sections of the following example inventory file:

----
# Create an OSEv3 group that contains the master, nodes, etcd, and lb groups.
# The lb group lets Ansible configure HAProxy as the load balancing solution.
# Comment lb out if your load balancer is pre-configured.
[OSEv3:children]
masters
nodes
etcd
lb

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
ifdef::openshift-enterprise[]
openshift_deployment_type=openshift-enterprise
oreg_url=example.com/openshift3/ose-${component}:${version}
oreg_auth_user=${user_name}
oreg_auth_password=${password}
openshift_examples_modify_imagestreams=true
endif::[]
ifdef::openshift-origin[]
openshift_deployment_type=origin
endif::[]

ifdef::openshift-origin[]
# uncomment the following to enable htpasswd authentication; defaults to AllowAllPasswordIdentityProvider
endif::[]
ifdef::openshift-enterprise[]
# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
endif::[]
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# Native high availability cluster method with optional load balancer.
# If no lb group is defined installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=openshift-internal.example.com
openshift_master_cluster_public_hostname=openshift-cluster.example.com

# host group for masters
[masters]
master1.example.com
master2.example.com
master3.example.com

# host group for etcd
[etcd]
master1.example.com
master2.example.com
master3.example.com

# Specify load balancer host
[lb]
lb.example.com

# host group for nodes, includes region info
[nodes]
master[1:3].example.com openshift_node_group_name='node-config-master'
node1.example.com openshift_node_group_name='node-config-compute'
node2.example.com openshift_node_group_name='node-config-compute'
infra-node1.example.com openshift_node_group_name='node-config-infra'
infra-node2.example.com openshift_node_group_name='node-config-infra'
----

include::install/example_inventories.adoc[tag=exampleinventorynodelabel]

To use this example, modify the file to match your environment and
specifications, and save it as *_/etc/ansible/hosts_*.
