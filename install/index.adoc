[[install-planning]]
= Planning Your Installation
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

[[planning-about-the-installer]]
== Introduction to Installing Clusters

To install clusters in production environments, {product-title} provides an
installation method (the installer) implemented using Ansible playbooks.
Familiarity with Ansible is assumed, however this installation guide will
provide you with the information to help you create an inventory file that
represents your environment and desired {product-title} cluster configuration,
then run the installation using the Ansible CLI tooling.

[NOTE]
====
You can read more about Ansible and its basic usage in the
link:http://docs.ansible.com/ansible/[official documentation].
====

[[inital-planning]]
== Initial Planning

When installing your {product-title} cluster for a production environment,
several factors influence installation. Consider the following questions as you
read through this guide:

ifdef::openshift-origin[]
* _Do you install on-premises or in public/private clouds?_ The xref:planning-cloud-providers[Installation Methods]
section provides more information about the cloud providers options available.
endif::[]

* _How many pods are required in your cluster?_ The xref:sizing[Sizing Considerations]
section provides limits for nodes and pods so you can calculate how large your
environment needs to be.

* _How many hosts do you require in the cluster?_ The xref:environment-scenarios[Environment Scenarios]
section provides multiple examples of Single Master and Multiple Master
configurations.

* _Is xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#high-availability-masters[high availability] for the cluster required?_ High availability is recommended for fault tolerance. In this situation, you might aim to use the
xref:multi-masters-using-native-ha[Multiple Masters Using Native HA] example as
a basis for your environment.

* _Which installation type do you want to use: xref:planning-installation-types[RPM or system containers]?_
Both installations provide a working {product-title} environment, but you might
have a preference for a particular method of installing, managing, and updating
your services.
 
* _Which identity provider do you use for xref:../install_config/configuring_authentication.adoc#install-config-configuring-authentication[authentication]?_
If you already use a supported identity provider, it is a best practice to
configure {product-title} to use that identity provider during installation. 
 
ifdef::openshift-enterprise[]
* _Is my installation supported if integrating with other technologies?_
See the link:https://access.redhat.com/articles/2176281[OpenShift Container Platform Tested Integrations]
for a list of tested integrations.
endif::[]

ifdef::openshift-origin[]
[[planning-cloud-providers]]
=== On-premises Versus Cloud Providers

{product-title} can be installed on-premises or hosted on public or private
clouds. Ansible playbooks can help you with automating
the provisioning and installation processes. For information, see
xref:running_install.adoc#advanced-cloud-providers[Running Installation Playbooks].
endif::[]

[[sizing]]
== Sizing Considerations

Determine how many nodes and pods you require for your {product-title} cluster.
Cluster scalability correlates to the number of pods in a cluster environment.
That number influences the other numbers in your setup. See
xref:../scaling_performance/cluster_limits.adoc#scaling-performance-cluster-limits[Cluster
Limits] for the latest limits for objects in {product-title}.

[[environment-scenarios]]
== Environment Scenarios

This section outlines different examples of scenarios for your {product-title}
environment. Use these scenarios as a basis for planning your own
{product-title} cluster, based on your xref:sizing[sizing] needs.

[NOTE]
====
Moving from a single master cluster to multiple masters after installation is
not supported.
====

For information on updating labels, see
xref:../admin_guide/manage_nodes.adoc#updating-labels-on-nodes[Updating Labels
on Nodes].

[[single-master-single-box]]
=== Single Master and Node on One System

{product-title} can be installed on a single system
for a development environment only.
An _all-in-one environment_ is not considered a production environment.

[[single-master-multi-node]]
=== Single Master and Multiple Nodes

The following table describes an example environment for a single
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[master] (with etcd installed on the same host)
and two
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master.example.com*
|Master, etcd, and node

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[[single-master-multi-etcd-multi-node]]
=== Single Master, Multiple etcd, and Multiple Nodes

The following table describes an example environment for a single
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[master],
three
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[etcd]
hosts, and two
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master.example.com*
|Master and node

|*etcd1.example.com*
.3+.^|etcd

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[[multi-masters-using-native-ha-colocated]]
=== Multiple Masters Using Native HA with Co-located Clustered etcd

The following describes an example environment for three
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[masters] with co-located clustered etcd,
one HAProxy load balancer, and two
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
using the `native` HA method:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master1.example.com*
.3+.^|Master (clustered using native HA) and node and clustered etcd

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|HAProxy to load balance API master endpoints

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[[multi-masters-using-native-ha]]
=== Multiple Masters Using Native HA with External Clustered etcd

The following describes an example environment for three
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[masters],
one HAProxy load balancer, three external clustered xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[etcd]
hosts, and two
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
using the `native` HA method:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master1.example.com*
.3+.^|Master (clustered using native HA) and node

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|HAProxy to load balance API master endpoints

|*etcd1.example.com*
.3+.^|Clustered etcd

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[[planning-stand-alone-registry]]
=== Stand-alone Registry

You can also install {product-title} to act as a stand-alone registry using the
{product-title}'s integrated registry. See
xref:stand_alone_registry.adoc#install-config-installing-stand-alone-registry[Installing a Stand-alone Registry] for details on this scenario.

[[planning-installation-types]]
== Installation Types

An RPM installation installs all services through package management and
configures services to run within the same user space, while a system container
installation installs services using system container images and runs separate
services in individual containers.

Starting in {product-title} 3.10, if you use Red Hat Enterprise Linux (RHEL)
Server as the underlying OS for a host, the RPM method is used to install
{product-title} components on that host. If you use RHEL Atomic Host, the system
container method is used on that host. Either installation type provides the
same functionality for the cluster, but the choice comes from the operating
system and how you choose to update your hosts.

When using RPMs, all services are installed and updated by package management
from an outside source. These modify a host's existing configuration within the
same user space. Alternatively, with system container installs, each component of
{product-title} is shipped as a container (in a self-contained package) and
leverages the host's kernel to start and run. Any updated, newer containers
replace any existing ones on your host.

The following table outlines further differences between the installation types:

.Differences Between Installation Types
[cols="h,2*",options="header"]
|===
| |RPM-based Installations  |System Container Installations

|Delivery Mechanism |RPM packages using `yum` |System container images using `docker`
|Service Management |`systemd` |`docker` and `systemd` units
|Operating System |Red Hat Enterprise Linux (RHEL) |RHEL Atomic Host
|===

[[containerized-required-images]]
=== Required Images for System Containers

The system container installation type makes use of the following images:

ifdef::openshift-origin[]
- *openshift/origin*
- *openshift/node* (*node* + *openshift-sdn* + *openvswitch* RPM for client tools)
- *openshift/openvswitch* (CentOS 7 + *openvswitch* RPM, runs *ovsdb* and *ovsctl* processes)
- *registry.access.redhat.com/rhel7/etcd*
endif::[]
ifdef::openshift-enterprise[]
- *openshift3/ose*
- *openshift3/node*
- *openshift3/openvswitch*
- *registry.access.redhat.com/rhel7/etcd*

By default, all of the above images are pulled from the Red Hat Registry at
https://registry.access.redhat.com[registry.access.redhat.com].
endif::[]

If you need to use a private registry to pull these images during the
installation, you can specify the registry information ahead of time. Set the
following Ansible variables in your inventory file, as required:

----
ifdef::openshift-origin[]
oreg_url='<registry_hostname>/openshift/origin-${component}:${version}'
endif::[]
ifdef::openshift-enterprise[]
oreg_url='<registry_hostname>/openshift3/ose-${component}:${version}'
endif::[]
openshift_docker_insecure_registries=<registry_hostname>
openshift_docker_blocked_registries=<registry_hostname>
----

The configuration of additional, insecure, and blocked Docker registries occurs
at the beginning of the installation process to ensure that these settings are
applied before attempting to pull any of the required images.

[[containerized-starting-and-stopping-containers]]
=== Starting and Stopping System Containers

The installation process creates relevant *systemd* units which can be used to
start, stop, and poll services using normal *systemctl* commands. For
containerized installations, these unit names match those of an RPM
installation, with the exception of the *etcd* service which is named
*etcd_container*.

This change is necessary as currently RHEL Atomic Host ships with the *etcd*
package installed as part of the operating system, so a containerized version is
used for the {product-title} installation instead. The installation process
disables the default *etcd* service.

[NOTE]
====
The *etcd* package is slated to be removed from RHEL Atomic Host in the future.
====

[[containerized-file-paths]]
=== File Path Locations

All {product-title} configuration files are placed in the same locations during
containerized installation as RPM based installations and will survive *os-tree*
upgrades.

However,
xref:../install_config/imagestreams_templates.adoc#install-config-imagestreams-templates[the default image stream and template files]
are installed at *_/etc/origin/examples/_* for
containerized installations rather than the standard
*_/usr/share/openshift/examples/_*, because that directory is read-only on RHEL
Atomic Host.

[[containerized-storage-requirements]]
=== Storage Requirements

RHEL Atomic Host installations normally have a very small root file system.
However, the etcd, master, and node containers persist data in the *_/var/lib/_*
directory. Ensure that you have enough space on the root file system before
installing {product-title}. See the
xref:prerequisites.adoc#system-requirements[System
Requirements] section for details.

[[containerized-openvswitch-sdn-initialization]]
=== Open vSwitch SDN Initialization

OpenShift SDN initialization requires that the Docker bridge be
reconfigured and that Docker is restarted. This complicates the situation when
the node is running within a container. When using the Open vSwitch (OVS) SDN,
you will see the node start, reconfigure Docker, restart Docker (which restarts
all containers), and finally start successfully.

In this case, the node service may fail to start and be restarted a few times,
because the master services are also restarted along with Docker. The current
implementation uses a workaround which relies on setting the `*Restart=always*`
parameter in the Docker based *systemd* units.
