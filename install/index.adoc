[[install-planning]]
= Planning your installation
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

You install {product-title} by running a series of Ansible playbooks. As you
prepare to install your cluster, you create an inventory file that
represents your environment and {product-title} cluster configuration. While
familiarity with Ansible might make this process easier, it is not required.

You can read more about Ansible and its basic usage in the
link:http://docs.ansible.com/ansible/[official documentation].

[[inital-planning]]
== Initial planning

Before you install your production {product-title} cluster, you need answers to
the following questions:

ifdef::openshift-origin[]
* _Do you install on-premise or in a public or private cloud?_ The xref:planning-cloud-providers[Installation Methods]
section provides more information about the cloud providers options available.
endif::[]

ifdef::openshift-enterprise[]
* _Do your on-premise servers use IBM POWER or x86_64 processors?_
You can install {product-title} on servers that use either type of processor. If
you use POWER servers, review the xref:install_power[Limitations and Considerations for Installations on IBM POWER].
endif::[]

* _How many pods are required in your cluster?_ The xref:sizing[Sizing Considerations]
section provides limits for nodes and pods so you can calculate how large your
environment needs to be.

* _How many hosts do you require in the cluster?_ The xref:environment-scenarios[Environment Scenarios]
section provides multiple examples of Single Master and Multiple Master
configurations.

* _Do you need a xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#high-availability-masters[high availability] cluster?_
High availability configurations improve fault tolerance. In this situation, you
might use the xref:multi-masters-using-native-ha-colocated[Multiple Masters Using Native HA]
example to set up your environment.

* _Is xref:../install_config/prometheus_cluster_monitoring.adoc#prometheus-cluster-monitoring[cluster monitoring] required?_
The monitoring stack requires additional
xref:../scaling_performance/scaling_cluster_monitoring.adoc#scaling-performance-cluster-monitoring[system resources].
Note that the monitoring stack is installed by default.
See the
xref:../install_config/prometheus_cluster_monitoring.adoc#prometheus-cluster-monitoring[cluster monitoring documentation] for more information.

* _Do you want to use Red Hat Enterprise Linux (RHEL) or RHEL Atomic Host as the operating system for your cluster nodes?_
If you install {product-title} on RHEL, you use an RPM-based installation. On
RHEL Atomic Host, you use a system container.
xref:planning-installation-types[Both installation types] provide a working
{product-title} environment.

* _Which identity provider do you use for xref:../install_config/configuring_authentication.adoc#install-config-configuring-authentication[authentication]?_
If you already use a supported identity provider, configure {product-title} to
use that identity provider during installation.

ifdef::openshift-enterprise[]
* _Is my installation supported if I integrate it with other technologies?_
See the link:https://access.redhat.com/articles/2176281[OpenShift Container Platform Tested Integrations]
for a list of tested integrations.
endif::[]

ifdef::openshift-origin[]
[[planning-cloud-providers]]
=== On-premise versus cloud providers

You can install {product-title} on-premise or host it on public or private
clouds. You can use the provided Ansible playbooks to help you automate
the provisioning and installation processes. For information, see
xref:running_install.adoc#advanced-cloud-providers[Running Installation Playbooks].
endif::[]

[[install_power]]
=== Limitations and Considerations for Installations on IBM POWER

As of version 3.10.45, you can install {product-title} on IBM POWER servers.

* Your cluster must use only Power nodes and masters. Because of the way that images are
tagged, {product-title} cannot differentiate between x86 images and Power images.
* Image streams and templates are not installed by default or updated when you
upgrade. You can manually install and update the image streams.
* You can install only on on-premise Power servers. You cannot install {product-title}
on nodes in any cloud provider.
* Not all storage providers are supported. You can use only the following
storage providers:
** GlusterFS
** NFS
** Local storage

[[sizing]]
== Sizing considerations

Determine how many nodes and pods you require for your {product-title} cluster.
Cluster scalability correlates to the number of pods in a cluster environment.
That number influences the other numbers in your setup. See
xref:../scaling_performance/cluster_maximums.adoc#scaling-performance-cluster-maximums[Cluster
Limits] for the latest limits for objects in {product-title}.

[[environment-scenarios]]
== Environment scenarios

Use these environment scenarios to help plan your {product-title} cluster
based on your sizing needs.

[NOTE]
====
Moving from a single master cluster to multiple masters after installation is
not supported.
====

In all environments, if your etcd hosts are co-located with master hosts, etcd
runs as a static pod on the host. If your etcd hosts are not co-located with
master hosts, they run etcd as standalone processes.

[NOTE]
====
If you use RHEL Atomic Host, you can configure etcd on only master hosts.
====

[[single-master-single-box]]
=== Single master and node on one system

You can install {product-title} on a single system for only a development
environment. You cannot use an _all-in-one environment_ as a production
environment.

[[single-master-multi-node]]
=== Single master and multiple nodes

The following table describes an example environment for a single
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[master] (with etcd installed on the same host)
and two
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master.example.com*
|Master, etcd, and node

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

////
[[single-master-multi-etcd-multi-node]]
=== Single Master, Multiple etcd, and Multiple Nodes

The following table describes an example environment for a single
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[master],
three separate
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[etcd]
hosts, and two
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master.example.com*
|Master and node

|*etcd1.example.com*
.3+.^|etcd

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

////

[[multi-masters-using-native-ha-colocated]]
=== Multiple masters using native HA

The following describes an example environment for three
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[masters],
one HAProxy load balancer, and two
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
using the `native` HA method. etcd runs as static pods on the master nodes:

[IMPORTANT]
====
Routers and master nodes must be load balanced to have a highly available and
fault-tolerant environment. Red Hat recommends the use of an enterprise-grade
external load balancer for production environments. This load balancing applies
to the masters and nodes, hosts running the {product-title} routers.
Transmission Control Protocol (TCP) layer 4 load balancing, in which the load is
spread across IP addresses, is recommended. See
link:http://v1.uncontained.io/playbooks/installation/load_balancing.html[External
Load Balancer Integrations with OpenShift Enterprise 3] for a reference design,
which is not recommended for production use cases.
====

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master1.example.com*
.3+.^|Master (clustered using native HA) and node and clustered etcd

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|HAProxy to load balance API master endpoints

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[[multi-masters-using-native-ha]]
=== Multiple Masters Using Native HA with External Clustered etcd

The following describes an example environment for three
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[masters],
one HAProxy load balancer, three external clustered xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[etcd]
hosts, and two
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[nodes]
using the `native` HA method:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master1.example.com*
.3+.^|Master (clustered using native HA) and node

|*master2.example.com*

|*master3.example.com*

|*lb.example.com*
|HAProxy to load balance API master endpoints

|*etcd1.example.com*
.3+.^|Clustered etcd

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[[planning-stand-alone-registry]]
=== Stand-alone registry

You can also install {product-title} to act as a stand-alone registry using the
{product-title}'s integrated registry. See
xref:stand_alone_registry.adoc#install-config-installing-stand-alone-registry[Installing a Stand-alone Registry] for details on this scenario.

[[planning-installation-types]]
== Installation types for supported operating systems

Starting in {product-title} 3.10, if you use RHEL
as the underlying OS for a host, the RPM method is used to install
{product-title} components on that host. If you use RHEL Atomic Host, the system
container method is used on that host. Either installation type provides the
same functionality for the cluster, but the operating system you use determines
how you manage services and host updates.

An RPM installation installs all services through package management and
configures services to run in the same user space, while a system container
installation installs services using system container images and runs separate
services in individual containers.

When using RPMs on RHEL, all services are installed and updated by package management
from an outside source. These packages modify a host's existing configuration in the
same user space. With system container installations on RHEL Atomic Host, each component of
{product-title} is shipped as a container, in a self-contained package, that
uses the host's kernel to run. Updated, newer containers
replace any existing ones on your host.

The following table and sections outline further differences between the
installation types:

.Differences between installation types
[cols="h,2*",options="header"]
|===
| |Red Hat Enterprise Linux | RHEL Atomic Host

|Installation Type |RPM-based |System container
|Delivery Mechanism |RPM packages using `yum` |System container images using `docker`
|Service Management |*systemd* |`docker` and *systemd* units
|===

[[containerized-required-images]]
=== Required images for system containers

The system container installation type makes use of the following images:

ifdef::openshift-origin[]
- *openshift/origin-node*
endif::[]
ifdef::openshift-enterprise[]
- *openshift3/ose-node*

By default, all of the above images are pulled from the Red Hat Registry at
https://registry.redhat.io[registry.redhat.io].
endif::[]

If you need to use a private registry to pull these images during the
installation, you can specify the registry information ahead of time. Set the
following Ansible variables in your inventory file, as required:

----
ifdef::openshift-origin[]
oreg_url='<registry_hostname>/openshift/origin-${component}:${version}'
endif::[]
ifdef::openshift-enterprise[]
oreg_url='<registry_hostname>/openshift3/ose-${component}:${version}'
endif::[]
openshift_docker_insecure_registries=<registry_hostname>
openshift_docker_blocked_registries=<registry_hostname>
----

[NOTE]
====
You can also set the `openshift_docker_insecure_registries` variable to the IP
address of the host. `0.0.0.0/0` is not a valid setting.
====

The default component inherits the image prefix and version from the `oreg_url`
value.

The configuration of additional, insecure, and blocked container registries occurs
at the beginning of the installation process to ensure that these settings are
applied before attempting to pull any of the required images.

[[planning-installation-types-service-names]]
=== systemd service names

The installation process creates relevant *systemd* units which can be used to
start, stop, and poll services using normal *systemctl* commands. For system
container installations, these unit names match those of an RPM installation.

[[containerized-file-paths]]
=== File path locations

All {product-title} configuration files are placed in the same locations during
containerized installation as RPM based installations and will survive *os-tree*
upgrades.

However,
xref:../install_config/imagestreams_templates.adoc#install-config-imagestreams-templates[the default image stream and template files]
are installed at *_/etc/origin/examples/_* for
Atomic Host installations rather than the standard
*_/usr/share/openshift/examples/_* because that directory is read-only on RHEL
Atomic Host.

[[containerized-storage-requirements]]
=== Storage requirements

RHEL Atomic Host installations normally have a very small root file system.
However, the etcd, master, and node containers persist data in the *_/var/lib/_*
directory. Ensure that you have enough space on the root file system before
installing {product-title}. See the
xref:prerequisites.adoc#system-requirements[System
Requirements] section for details.
