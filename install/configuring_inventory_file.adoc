[[install-config-configuring-inventory-file]]
= Configuring Your Inventory File
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:
ifdef::openshift-enterprise[]
:pb-prefix: /usr/share/ansible/openshift-ansible/
endif::[]
ifdef::openshift-origin[]
:pb-prefix: ~/openshift-ansible/
endif::[]
include::_snippets/glusterfs.adoc[]

toc::[]

[[configuring-ansible]]
== Customizing Inventory Files for Your Cluster

Ansible inventory files describe the details about the hosts in your cluster and
the cluster configuration details for your {product-title}
installation. The {product-title} installation playbooks read your inventory
file to know where and how to install {product-title} across your set of hosts.

[NOTE]
====
See link:https://docs.ansible.com/ansible/2.9/user_guide/intro_inventory.html[Ansible documentation]
for details about the format of an inventory file, including basic details about
link:https://docs.ansible.com/ansible/2.9/reference_appendices/YAMLSyntax.html[YAML syntax].
====

When you install the *openshift-ansible* RPM package as described in
xref:../install/host_preparation.adoc#installing-base-packages[Host preparation], Ansible
dependencies create a file at the default location of *_/etc/ansible/hosts_*.
However, the file is simply the default Ansible example and has no variables
related specifically to {product-title} configuration. To successfully install
{product-title}, you _must_ replace the default contents of the file with your
own configuration based on your cluster topography and requirements.

The following sections describe commonly-used variables to set in your inventory
file during cluster installation. Many of the Ansible variables described
are optional. For development environments, you can accept the default values
for the required parameters, but you must select appropriate values
for them in production environments.

You can review
xref:example_inventories.adoc#install-config-example-inventories[Example Inventory Files]
for various examples to use as a starting point for your cluster installation.

[NOTE]
====
Images require a version number policy in order to maintain updates. See
the xref:../architecture/core_concepts/containers_and_images.adoc#architecture-images-tag-policy[Image
Version Tag Policy] section in the Architecture Guide for more information.
====

[[configuring-cluster-variables]]
== Configuring Cluster Variables

To assign global cluster environment variables during the Ansible installation, add
them to the *[OSEv3:vars]* section of the *_/etc/ansible/hosts_* file. You must
place each parameter value on a on separate line. For example:

----
[OSEv3:vars]

openshift_master_identity_providers=[{'name': 'htpasswd_auth',
'login': 'true', 'challenge': 'true',
'kind': 'HTPasswdPasswordIdentityProvider',}]

openshift_master_default_subdomain=apps.test.example.com
----

include::install/topics/escaping_special_characters.adoc[]

The following tables describe global cluster variables for use with the Ansible
installer:

[[cluster-variables-table]]
.General Cluster Variables
[cols=".^5,.^5a",options="header"]
|===

|Variable |Purpose

|`ansible_ssh_user`
|This variable sets the SSH user for the installer to use and defaults to
`root`. This user must allow SSH-based authentication
xref:host_preparation.adoc#ensuring-host-access[without requiring a password]. If
using SSH key-based authentication, then the key must be managed by an SSH
agent.

|`ansible_become`
|If `ansible_ssh_user` is not `root`, this variable must be set to `true` and
the user must be configured for passwordless `sudo`.

|`debug_level`
a|This variable sets which INFO messages are logged to the `systemd-journald.service`. Set one of the following:

* `0` to log errors and warnings only
* `2` to log normal information (This is the default level.)
* `4` to log debugging-level information
* `6` to log API-level debugging information (request / response)
* `8` to log body-level API debugging information

For more information on debug log levels, see xref:../install_config/master_node_configuration.adoc#master-node-config-logging-levels[Configuring Logging Levels].

|`openshift_clock_enabled`
a| Whether to enable Network Time Protocol (NTP) on cluster nodes. The default
value is `true`.

If the `chrony` package is installed, it is configured to provide NTP service.
If the `chrony` package is not installed, the installation playbooks install and
configure the `ntp` package to provide NTP service.

[IMPORTANT]
====
To prevent masters and nodes in the
cluster from going out of sync, do not change the default value of this parameter.
====

|`openshift_master_admission_plugin_config`
a|This variable sets the parameter and arbitrary JSON values as per the requirement in your inventory hosts file. For example:

----
openshift_master_admission_plugin_config={"ClusterResourceOverride":{"configuration":{"apiVersion":"v1","kind":"ClusterResourceOverrideConfig","memoryRequestToLimitPercent":"25","cpuRequestToLimitPercent":"25","limitCPUToMemoryPercent":"200"}}}
----
In this value, `openshift_master_admission_plugin_config={"openshift.io/ImagePolicy":{"configuration":{"apiVersion":"v1","executionRules":[{"matchImageAnnotations":[{"key":"images.openshift.io/deny-execution","value":"true"}],"name":"execution-denied","onResources":[{"resource":"pods"},{"resource":"builds"}],"reject":true,"skipOnResolutionFailure":true}],"kind":"ImagePolicyConfig"}}}` is the default parameter value.

[IMPORTANT]
====
You must include the default `openshift_master_admission_plugin_config` value even if you need to add a custom setting.
====

|`openshift_master_audit_config`
|This variable enables API service auditing. See
xref:../install_config/master_node_configuration.adoc#master-node-config-audit-config[Audit
Configuration] for more information.

|`openshift_master_audit_policyfile`
|Provide the location of a audit policy file. See
xref:../install_config/master_node_configuration.adoc#master-node-config-advanced-audit[Audit Policy
Configuration] for more information.

|`openshift_master_cluster_hostname`
|This variable overrides the host name for the cluster, which defaults to the
host name of the master.

|`openshift_master_cluster_public_hostname`
|This variable overrides the public host name for the cluster, which defaults to
the host name of the master. If you use an external load balancer, specify the address of the external load balancer.

For example:

----
openshift_master_cluster_public_hostname=openshift-ansible.public.example.com
----

|`openshift_master_cluster_method`
|Optional. This variable defines the HA method when deploying multiple masters.
Supports the `native` method. See xref:example_inventories.adoc#multiple-masters[Multiple Masters] for
more information.

|`openshift_rolling_restart_mode`
|This variable enables rolling restarts of HA masters (i.e., masters are taken
down one at a time) when
xref:../upgrading/automated_upgrades.adoc#install-config-upgrading-automated-upgrades[running
the upgrade playbook directly]. It defaults to `services`, which allows rolling
restarts of services on the masters. It can instead be set to `system`, which
enables rolling, full restarts of the master nodes.

A rolling restart of the masters can be necessary to apply additional changes using the
supplied xref:../upgrading/automated_upgrades.adoc#upgrade-hooks[Ansible hooks] during the upgrade.
Depending on the tasks you choose to perform you might want to reboot the host to restart your services.


|`openshift_master_identity_providers`
|This variable sets the
xref:../install_config/configuring_authentication.adoc#install-config-configuring-authentication[identity provider].
The default value is
xref:../install_config/configuring_authentication.adoc#DenyAllPasswordIdentityProvider[Deny
All]. If you use a supported identity provider, configure {product-title} to
use it. You can configure multiple identity providers.

|`openshift_master_named_certificates`
.2+.^|These variables are used to configure xref:../install_config/certificate_customization.adoc#install-config-certificate-customization[custom certificates] which are deployed as part of the installation. See xref:advanced-install-custom-certificates[Configuring Custom Certificates] for more information.
|`openshift_master_overwrite_named_certificates`

|`openshift_hosted_router_certificate`
|Provide the location of the
xref:../install_config/certificate_customization.adoc#install-config-certificate-customization[custom certificates]
for the hosted router.

|`openshift_master_ca_certificate`
| Provide the
xref:../install_config/redeploying_certificates.adoc#redeploying-new-custom-ca[single certificate]
and key that signs the {product-title} certificates.
See xref:../install_config/redeploying_certificates.adoc#redeploying-new-custom-ca[Redeploying a New or Custom {product-title} CA]

|`openshift_additional_ca`
| If the certificate for your `openshift_master_ca_certificate` parameter is
signed by an intermediate certificate, provide the bundled certificate that
contains the full chain of intermediate and root certificates for the CA.
See xref:../install_config/redeploying_certificates.adoc#redeploying-new-custom-ca[Redeploying a New or Custom {product-title} CA]

|`openshift_redeploy_service_signer`
| If the parameter is set to `false`, the service signer is not redeployed when you run the `openshift-master/redeploy-certificates.yml` playbook.
The default value is `true`.

|`openshift_hosted_registry_cert_expire_days`
|Validity of the auto-generated registry certificate in days. Defaults to `730` (2 years).

|`openshift_ca_cert_expire_days`
|Validity of the auto-generated CA certificate in days. Defaults to `1825` (5 years).

|`openshift_master_cert_expire_days`
|Validity of the auto-generated master certificate in days. Defaults to `730` (2 years).

|`etcd_ca_default_days`
|Validity of the auto-generated external etcd certificates in days. Controls
validity for etcd CA, peer, server and client certificates. Defaults to `1825`
(5 years).

|`openshift_certificate_expiry_warning_days`
|Halt upgrades to clusters that have certificates expiring in this many days or fewer. Defaults to `365` (1 year).

|`openshift_certificate_expiry_fail_on_warn`
|Whether upgrade fails if the auto-generated certificates are not valid for the
period specified by the `openshift_certificate_expiry_warning_days` parameter.
Defaults to `True`.

|`os_firewall_use_firewalld`
|Set to `true` to use firewalld instead of the default iptables. Not available on RHEL Atomic Host. See the xref:advanced-install-configuring-firewalls[Configuring the Firewall] section for more information.

|`openshift_master_session_name`
.4+.^|These variables override defaults for
xref:../install_config/configuring_authentication.adoc#session-options[session
options] in the OAuth configuration. See xref:advanced-install-session-options[Configuring Session Options] for more information.

|`openshift_master_session_max_seconds`

|`openshift_master_session_auth_secrets`

|`openshift_master_session_encryption_secrets`

|`openshift_master_image_policy_config`
|Sets `imagePolicyConfig` in the master configuration. See xref:../install_config/master_node_configuration.adoc#master-config-image-config[Image Configuration] for details.

|`openshift_router_selector`
|Default node selector for automatically deploying router pods. See
xref:configuring-node-host-labels[Configuring Node Host Labels] for details.

|`openshift_registry_selector`
|Default node selector for automatically deploying registry pods. See
xref:configuring-node-host-labels[Configuring Node Host Labels] for details.

|`openshift_template_service_broker_namespaces`
|This variable enables the template service broker by specifying one or more
namespaces whose templates will be served by the broker.

|`openshift_master_bootstrap_auto_approve`
|This variable enables TLS bootstrapping auto approval, which allows nodes to
automatically join the cluster when provided a bootstrap credential. Set to
`true` if you will enable the
xref:../admin_guide/cluster-autoscaler.adoc#configuring-cluster-auto-scaler-AWS[cluster auto-scaler]
on an Amazon Web Services (AWS) cluster. The default value is `false`.

|`ansible_service_broker_node_selector`
|Default node selector for automatically deploying Ansible service broker pods,
defaults `{"node-role.kubernetes.io/infra":"true"}`. See
xref:configuring-node-host-labels[Configuring Node Host Labels] for details.

|`osm_default_node_selector`
|This variable overrides the node selector that projects will use by default when
placing pods, which is defined by the `projectConfig.defaultNodeSelector` field
in the master configuration file. This defaults
to `node-role.kubernetes.io/compute=true` if undefined.

|`openshift_docker_additional_registries`
a|{product-title} adds the specified additional registry or registries to the
*docker* configuration. These are the registries to search.
If the registry requires access to a port other than `80`, include the port number required in the form of `<address>:<port>`.

For example:

----
openshift_docker_additional_registries=example.com:443
----

[NOTE]
====
If you need to configure your cluster to use an alternate registry, set
`oreg_url` rather than rely on `openshift_docker_additional_registries`.
====

|`openshift_docker_insecure_registries`
|{product-title} adds the specified additional insecure registry or registries to
the *docker* configuration. For any of these registries, secure sockets layer
(SSL) is not verified. Can be set to the host name or IP address
of the host. `0.0.0.0/0` is not a valid setting for the IP address.

|`openshift_docker_blocked_registries`
|{product-title} adds the specified blocked registry or registries to the
*docker* configuration. Block the listed registries. Setting this to `all`
blocks everything not in the other variables.

|`openshift_docker_ent_reg`
|An additional registry that is trusted by the container runtime, when `openshift_deployment_type` is set to `openshift-enterprise`. The default is `registry.redhat.io`. If you set `openshift_docker_ent_reg=''`, then `registry.redhat.io` will not be added to the *docker* configuration.

|`openshift_metrics_hawkular_hostname`
|This variable sets the host name for integration with the metrics console by
overriding `metricsPublicURL` in the master configuration for cluster metrics.
If you alter this variable, ensure the host name is accessible via your router.

|`openshift_clusterid`
|This variable is a cluster identifier unique to the AWS Availability Zone. Using this avoids potential issues in Amazon Web Services
(AWS) with multiple zones or multiple clusters. See xref:../install_config/configuring_aws.adoc#aws-cluster-labeling[Labeling Clusters for AWS] for details.

|`openshift_encryption_config`
|Use this variable to configure datastore-layer encryption.

|`openshift_image_tag`
|Use this variable to specify a container image tag to install or configure.

|`openshift_pkg_version`
|Use this variable to specify an RPM version to install or configure.

|===

[WARNING]
====
If you modify the `openshift_image_tag` or the `openshift_pkg_version` variables
after the cluster is set up, then an upgrade can be triggered, resulting in
downtime.

* If `openshift_image_tag` is set, its value is used for all hosts in
system container environments, even those that have another version installed. If
* `openshift_pkg_version` is set, its value is used for all hosts in RPM-based
environments, even those that have another version installed.
====

[[advanced-install-networking-variables-table]]
.Networking Variables
[options="header"]
|===

|Variable |Purpose

|`openshift_master_default_subdomain`
|This variable overrides the default subdomain to use for exposed
xref:../architecture/networking/routes.adoc#architecture-core-concepts-routes[routes]. The value for this variable must consist of lower case alphanumeric characters or dashes (`-`). It must start with an alphabetic character, and end with an alphanumeric character.

|`os_sdn_network_plugin_name`
|This variable configures which
xref:../architecture/networking/sdn.adoc#architecture-additional-concepts-sdn[OpenShift SDN plug-in] to
use for the pod network, which defaults to `redhat/openshift-ovs-subnet` for the
standard SDN plug-in. Set the variable to `redhat/openshift-ovs-multitenant` to
use the multitenant SDN plug-in.

|`osm_cluster_network_cidr`
|This variable overrides the SDN cluster network CIDR block. This is the network
from which pod IPs are assigned. Specify a private block
that does not conflict with existing network blocks in your infrastructure to
which pods, nodes, or the master might require access. Defaults to `10.128.0.0/14`
and cannot be arbitrarily re-configured after deployment, although certain
changes to it can be made in the
xref:../install_config/configuring_sdn.adoc#configuring-the-pod-network-on-masters[SDN
master configuration].

|`openshift_portal_net`
|This variable configures the subnet in which
xref:../architecture/core_concepts/pods_and_services.adoc#services[services]
will be created in the
xref:../architecture/networking/sdn.adoc#architecture-additional-concepts-sdn[{product-title}
SDN]. Specify a private block that does not conflict with any
existing network blocks in your infrastructure to which pods, nodes, or the
master might require access to, or the installation will fail. Defaults to
`172.30.0.0/16`, and cannot be re-configured after deployment. If changing from the default, avoid `172.17.0.0/16`, which the *docker0* network bridge uses by default, or modify the *docker0* network.

|`osm_host_subnet_length`
|This variable specifies the size of the per host subnet allocated for pod IPs
by
xref:../architecture/networking/sdn.adoc#sdn-design-on-masters[{product-title}
SDN]. Defaults to `9` which means that a subnet of size /23 is allocated to each
host; for example, given the default 10.128.0.0/14 cluster network, this will
allocate 10.128.0.0/23, 10.128.2.0/23, 10.128.4.0/23, and so on. This cannot be
re-configured after deployment.

|`openshift_node_proxy_mode`
|This variable specifies the
xref:../architecture/core_concepts/pods_and_services.adoc#service-proxy-mode[service
proxy mode] to use: either `iptables` for the default, pure-`iptables`
implementation, or `userspace` for the user space proxy.

|`openshift_use_flannel`
|This variable enables *flannel* as an alternative networking layer instead of
the default SDN. If enabling *flannel*, disable the default SDN with the
`openshift_use_openshift_sdn` variable. For more information, see xref:../install_config/configuring_sdn.adoc#using-flannel[Using Flannel].

|`openshift_use_openshift_sdn`
|Set to `false` to disable the OpenShift SDN plug-in.

|`openshift_sdn_vxlan_port`
|This variable sets the `vxlan port` number for `cluster network`. Defaults to `4789`. See xref:../install_config/configuring_sdn.html#config-changing-vxlan-port-for-cluster-network[Changing the VXLAN PORT for the cluster network] for more information.

|`openshift_node_sdn_mtu`
|This variable specifies the MTU size to use for OpenShift SDN. The value must be `50` bytes less than the MTU of the primary network interface of the node. For example, if the primary network interface has an MTU of `1500`, this value will be `1450`. The default value is `1450`.

|===

[[advanced-install-deployment-types]]
== Configuring Deployment Type

Various defaults used throughout the playbooks and roles used by the installer
are based on the deployment type configuration (usually defined in an Ansible
inventory file).

ifdef::openshift-enterprise[]
Ensure the `openshift_deployment_type` parameter in your inventory file's `[OSEv3:vars]`
section is set to `openshift-enterprise` to install the {product-title} variant:

----
[OSEv3:vars]
openshift_deployment_type=openshift-enterprise
----
endif::[]
ifdef::openshift-origin[]
Ensure the `openshift_deployment_type` parameter in your inventory file's `[OSEv3:vars]`
section is set to `origin` to install the {product-title} variant:

----
[OSEv3:vars]
openshift_deployment_type=origin
----
endif::[]


[[configuring-host-variables]]
== Configuring Host Variables

To assign environment variables to hosts during the Ansible installation, set
them in the *_/etc/ansible/hosts_* file after the host entry in
the *[masters]* or *[nodes]* sections. For example:

----
[masters]
ec2-52-6-179-239.compute-1.amazonaws.com openshift_public_hostname=ose3-master.public.example.com
----

The following table describes variables for use with the Ansible installer that
can be assigned to individual host entries:

[[advanced-host-variables]]
.Host Variables
[options="header"]
|===

|Variable |Purpose

|`openshift_public_hostname`
|This variable overrides the system's public host name. Use this for cloud
installations, or for hosts on networks using a network address translation
(NAT).

|`openshift_public_ip`
|This variable overrides the system's public IP address. Use this for cloud
installations, or for hosts on networks using a network address translation
(NAT).

|`openshift_node_labels`
|This variable is deprecated; see
xref:configuring-inventory-defining-node-group-and-host-mappings[Defining Node
Groups and Host Mappings] for the current method of setting node labels.

|`openshift_docker_options`
a|This variable configures additional `docker` options in
*_/etc/sysconfig/docker_*, such as options used in
xref:host_preparation.adoc#managing-docker-container-logs[Managing Container Logs].
It is recommended to use `json-file`.

The following example shows the configuration of Docker to use the `json-file` log
driver, where Docker rotates between three 1 MB log files and signature verification is not required.
When supplying additional options, ensure that you maintain the single quotation mark formatting:
----
OPTIONS=' --selinux-enabled --log-opt  max-size=1M --log-opt max-file=3 --insecure-registry 172.30.0.0/16 --log-driver=json-file --signature-verification=false'
----

|`openshift_schedulable`
|This variable configures whether the host is marked as a schedulable node,
meaning that it is available for placement of new pods. See
xref:marking-masters-as-unschedulable-nodes[Configuring Schedulability on Masters].

|`openshift_node_problem_detector_install`
|This variable is used to activate the xref:../admin_guide/node_problem_detector.adoc#admin-guide-node-problem-detector[Node Problem Detector].
If set to `false, the default`, the Node Problem Detector is not installed or started.

|===

[[configuring-inventory-defining-node-group-and-host-mappings]]
== Defining Node Groups and Host Mappings

Node configurations are
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node-bootstrapping[bootstrapped]
from the master. When the node boots and services are started, the node checks if
a *_kubeconfig_* and other node configuration files exist before joining the
cluster. If they do not, the node pulls the configuration from the master, then
joins the cluster.

This process replaces administrators having to manually maintain the node
configuration uniquely on each node host. Instead, the contents of a node host's
*_/etc/origin/node/node-config.yaml_* file are now provided by ConfigMaps
sourced from the master.

[[configuring-inventory--node-group-configmaps]]
=== Node ConfigMaps

The Configmaps for defining the node configurations must be available in the
*openshift-node* project. ConfigMaps are also now the authoritative definition
for node labels; the old `openshift_node_labels` value is effectively ignored.

By default during a cluster installation, the installer creates the following
default ConfigMaps:

- `node-config-master`
- `node-config-infra`
- `node-config-compute`

The following ConfigMaps are also created, which label nodes into multiple roles:

- `node-config-all-in-one`
- `node-config-master-infra`

The following ConfigMaps are *CRI-O* variants for each of the existing default node groups:

- `node-config-master-crio`
- `node-config-infra-crio`
- `node-config-compute-crio`
- `node-config-all-in-one-crio`
- `node-config-master-infra-crio`

[IMPORTANT]
====
You must not modify a node host's
*_/etc/origin/node/node-config.yaml_* file. Any changes are overwritten by the
configuration that is defined in the ConfigMap the node uses.
====

[[configuring-inventory-node-group-definitions]]
=== Node Group Definitions

After installing the latest *openshift-ansible* package, you can view the
default set of node group definitions in YAML format in the
*_{pb-prefix}roles/openshift_facts/defaults/main.yml_* file:

----
openshift_node_groups:
  - name: node-config-master <1>
    labels:
      - 'node-role.kubernetes.io/master=true' <2>
    edits: [] <3>
  - name: node-config-infra
    labels:
      - 'node-role.kubernetes.io/infra=true'
    edits: []
  - name: node-config-compute
    labels:
      - 'node-role.kubernetes.io/compute=true'
    edits: []
  - name: node-config-master-infra
    labels:
      - 'node-role.kubernetes.io/infra=true,node-role.kubernetes.io/master=true'
    edits: []
  - name: node-config-all-in-one
    labels:
      - 'node-role.kubernetes.io/infra=true,node-role.kubernetes.io/master=true,node-role.kubernetes.io/compute=true'
    edits: []
----
<1> Node group name.
<2> List of node labels associated with the node group. See
xref:configuring-node-host-labels[Node Host Labels] for details.
<3> Any edits to the node group's configuration.

If you do not set the `openshift_node_groups` variable in your inventory file's
`[OSEv3:vars]` group, these defaults values are used. However, if you
want to set custom node groups, you must define the entire
`openshift_node_groups` structure, including all planned node groups, in your
inventory file.

The `openshift_node_groups` value is not merged with the default values, and you
must translate the YAML definitions into a Python dictionary. You can then use
the `edits` field to modify any node configuration variables by specifying
key-value pairs.

[NOTE]
====
See
xref:../install_config/master_node_configuration.adoc#node-configuration-files[Master
and Node Configuration Files] for reference on configurable node variables.
====

For example, the following entry in an inventory file defines groups named
`node-config-master`, `node-config-infra`, and `node-config-compute`.

----
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true']}, {'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true']}]
----

You can also define new node group names with other labels, the following entry in an inventory file defines groups named
`node-config-master`, `node-config-infra`, `node-config-compute` and `node-config-compute-storage`.

----
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true']}, {'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true']}, {'name': 'node-config-compute-storage', 'labels': ['node-role.kubernetes.io/compute-storage=true']}]
----

When you set an entry in the inventory file, you can also edit the ConfigMap for
a node group:

* You can use a list, such as modifying the `node-config-compute` to set
`kubeletArguments.pods-per-core` to `20`:

----
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true']}, {'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true'], 'edits': [{ 'key': 'kubeletArguments.pods-per-core','value': ['20']}]}]
----

* You can use a list to modify multiple key value pairs, such as modifying the
`node-config-compute` group to add two parameters to the `kubelet`:

----
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true']}, {'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true'], 'edits': [{ 'key': 'kubeletArguments.experimental-allocatable-ignore-eviction','value': ['true']}, {'key': 'kubeletArguments.eviction-hard', 'value': ['memory.available<1Ki']}]}]
----

* You can use also use a dictionary as value, such as modifying the
`node-config-compute` group to set `perFSGroup` to `512Mi`:

----
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true']}, {'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true'], 'edits': [{ 'key': 'volumeConfig.localQuota','value': {'perFSGroup':'512Mi'}}]}]
----

Whenever the *_openshift_node_group.yml_* playbook is run, the changes defined
in the `edits` field will update the related ConfigMap (`node-config-compute` in
this example), which will ultimately affect the node's configuration file on the
host.

[NOTE]
====
Running the *_openshift_node_group.yaml_* playbook only updates new nodes. It cannot be run to update existing nodes in a cluster.
====

[[configuring-inventory-mapping-hosts-to-node-groups]]
=== Mapping Hosts to Node Groups

To map which ConfigMap to use for which node host, all hosts defined in the
`[nodes]` group of your inventory must be assigned to a _node group_ using the
`openshift_node_group_name` variable.

[IMPORTANT]
====
Setting `openshift_node_group_name` per host to a node group is required for all
cluster installations whether you use the default node group definitions
and ConfigMaps or are customizing your own.
====

The value of `openshift_node_group_name` is used to select the ConfigMap that
configures each node. For example:

----
[nodes]
master[1:3].example.com openshift_node_group_name='node-config-master'
infra-node1.example.com openshift_node_group_name='node-config-infra'
infra-node2.example.com openshift_node_group_name='node-config-infra'
node1.example.com openshift_node_group_name='node-config-compute'
node2.example.com openshift_node_group_name='node-config-compute'
----

If other custom ConfigMaps have been defined in `openshift_node_groups` they can also be used. For exmaple:

----
[nodes]
master[1:3].example.com openshift_node_group_name='node-config-master'
infra-node1.example.com openshift_node_group_name='node-config-infra'
infra-node2.example.com openshift_node_group_name='node-config-infra'
node1.example.com openshift_node_group_name='node-config-compute'
node2.example.com openshift_node_group_name='node-config-compute'
gluster[1:6].example.com openshift_node_group_name='node-config-compute-storage'
----

[[configuring-node-host-labels]]
=== Node Host Labels

You can assign xref:../architecture/core_concepts/pods_and_services.adoc#labels[Labels]
to node hosts during cluster installation. You can use these labels to
determine the placement of pods onto nodes using the
xref:../admin_guide/scheduling/scheduler.adoc#configurable-predicates[scheduler].

You must create your own
custom node groups if you want to modify the default labels that are assigned to
node hosts. You can no longer set the `openshift_node_labels` variable to change
labels. See xref:configuring-inventory-node-group-definitions[Node Group Definitions]
to modify the default node groups.

Other than `node-role.kubernetes.io/infra=true` (hosts using this group are also referred to as
_dedicated infrastructure nodes_ and discussed further in
xref:configuring-dedicated-infrastructure-nodes[Configuring Dedicated
Infrastructure Nodes]), the actual label names and values are arbitrary and can
be assigned however you see fit per your cluster's requirements.

[[marking-masters-as-unschedulable-nodes]]
==== Pod Schedulability on Masters

Configure all hosts that you designate as masters during the installation process
as nodes. By doing so, the masters are configured as part of the
xref:../architecture/networking/network_plugins.adoc#openshift-sdn[OpenShift SDN].
You must add entries for the master hosts to the `[nodes]` section:

----
[nodes]
master[1:3].example.com openshift_node_group_name='node-config-master'
----

If you want to change the schedulability of a host post-installation, see
xref:../admin_guide/manage_nodes.adoc#marking-nodes-as-unschedulable-or-schedulable[Marking Nodes as Unschedulable or Schedulable].


[[configuring-node-host-labels-pod-schedulability-nodes]]
==== Pod Schedulability on Nodes

Masters are marked as schedulable nodes by default, so the default node selector
is set by default during cluster installations. The default node selector is
defined in the master configuration file's `projectConfig.defaultNodeSelector`
field to determine which node projects will use by default when placing pods. It
is set to `node-role.kubernetes.io/compute=true` unless overridden using the
`osm_default_node_selector` variable.

[IMPORTANT]
====
If you accept the default node selector of
`node-role.kubernetes.io/compute=true` during installation, ensure that you do
not only have dedicated infrastructure nodes as the non-master nodes defined in
your cluster. In that scenario, application pods fail to deploy because no
nodes with the `node-role.kubernetes.io/compute=true` label are available
to match the default node selector when scheduling pods for projects.
====

See
xref:../admin_guide/managing_projects.adoc#setting-the-cluster-wide-default-node-selector[Setting the Cluster-wide Default Node Selector]
for steps on adjusting this setting post-installation if needed.

[[configuring-dedicated-infrastructure-nodes]]
==== Configuring Dedicated Infrastructure Nodes

It is recommended for production environments that you maintain dedicated
infrastructure nodes where the registry and router pods can run separately from
pods used for user applications.

The `openshift_router_selector` and `openshift_registry_selector` Ansible
settings determine the label selectors used when placing registry and router
pods. They are set to `node-role.kubernetes.io/infra=true` by default:

----
# default selectors for router and registry services
# openshift_router_selector='node-role.kubernetes.io/infra=true'
# openshift_registry_selector='node-role.kubernetes.io/infra=true'
----

The registry and router are only able to run on node hosts with the
`node-role.kubernetes.io/infra=true` label, which are then considered dedicated
infrastructure nodes. Ensure that at least one node host in your {product-title}
environment has the `node-role.kubernetes.io/infra=true` label; you can use the
default *node-config-infra*, which sets this label:

----
[nodes]
infra-node1.example.com openshift_node_group_name='node-config-infra'
----

[IMPORTANT]
====
If there is not a node in the `[nodes]` section that matches the selector
settings, the default router and registry will be deployed as failed with
`Pending` status.
====

If you do not intend to use {product-title} to manage the registry and router,
configure the following Ansible settings:

----
openshift_hosted_manage_registry=false
openshift_hosted_manage_router=false
----

If you use an image registry other than the default
`registry.redhat.io`, you must
xref:advanced-install-configuring-registry-location[specify the registry]
in the  *_/etc/ansible/hosts_* file.

As described in xref:marking-masters-as-unschedulable-nodes[Configuring Schedulability on Masters],
master hosts are marked schedulable by default. If
you label a master host with `node-role.kubernetes.io/infra=true` and have no other dedicated
infrastructure nodes, the master hosts must also be marked as schedulable.
Otherwise, the registry and router pods cannot be placed anywhere.

You can use the default *node-config-master-infra* node group to achieve this:

----
[nodes]
master.example.com openshift_node_group_name='node-config-master-infra'
----

[[configuring-project]]
== Configuring Project Parameters

To configure the default project settings, configure the following variables in
the  *_/etc/ansible/hosts_* file:

[[advanced-project-parameters]]
.Project Parameters
[options="header"]
|===

|Parameter |Description |Type |Default Value

|`osm_project_request_message`
|The string presented to a user if they are unable to request a project via the
*projectrequest* API endpoint.
|String
| null

|`osm_project_request_template`
|The template to use for creating projects in response to a *projectrequest*. If you
do not specify a value, the default template is used.
|String with the format `<namespace>/<template>`
|null

|`osm_mcs_allocator_range`
|Defines the range of MCS categories to assign to namespaces. If this value is
changed after startup, new projects might receive labels that are already
allocated to other projects. The prefix can be any valid
SELinux set of terms, including user, role, and type. However, leaving the
prefix at its default allows the server to set them automatically. For example,
`s0:/2` allocates labels from s0:c0,c0 to s0:c511,c511 whereas `s0:/2,512`
allocates labels from s0:c0,c0,c0 to s0:c511,c511,511.
|String with the format `<prefix>/<numberOfLabels>[,<maxCategory>]`
|`s0:/2`

|`osm_mcs_labels_per_project`
|Defines the number of labels to reserve per project.
|Integer
|`5`

|`osm_uid_allocator_range`
|Defines the total set of Unix user IDs (UIDs) automatically allocated to
projects and the size of the block that each namespace gets. For example,
`1000-1999/10` allocates ten UIDs per namespace and can
allocate up to 100 blocks before running out of space. The default value is the
expected size of the ranges for container images when user namespaces are
started.
|String in the format `<block_range>/<number_of_UIDs>`
|`1000000000-1999999999/10000`

|===

[[configuring-host-port]]
== Configuring Master API Port

To configure the default ports used by the master API, configure the following
variables in the *_/etc/ansible/hosts_* file:

[[advanced-master-ports]]
.Master API Port
[options="header"]
|===

|Variable |Purpose
|`openshift_master_api_port`
|This variable sets the port number to access the {product-title} API.
|===

For example:

----
openshift_master_api_port=3443
----

The web console port setting (`openshift_master_console_port`) must match the
API server port (`openshift_master_api_port`).

[[configuring-cluster-pre-install-checks]]
== Configuring Cluster Pre-install Checks

Pre-install checks are a set of diagnostic tasks that run as part of the
*openshift_health_checker* Ansible role. They run prior to an Ansible
installation of {product-title}, ensure that required inventory values are set,
and identify potential issues on a host that can prevent or interfere with a
successful installation.

The following table describes available pre-install checks that will run before
every Ansible installation of {product-title}:

[[configuring-cluster-pre-install-checks-pre-install-checks]]
.Pre-install Checks
[options="header"]
|===

|Check Name |Purpose

|`memory_availability`
|This check ensures that a host has the recommended amount of memory for the
specific deployment of {product-title}. Default values have been derived from
the
xref:prerequisites.adoc#system-requirements[latest
installation documentation]. A user-defined value for minimum memory
requirements might be set by setting the `openshift_check_min_host_memory_gb`
cluster variable in your inventory file.

|`disk_availability`
|This check only runs on etcd, master, and node hosts. It ensures that the mount
path for an {product-title} installation has sufficient disk space remaining.
Recommended disk values are taken from the
xref:prerequisites.adoc#system-requirements[latest
installation documentation]. A user-defined value for minimum disk space
requirements might be set by setting `openshift_check_min_host_disk_gb` cluster
variable in your inventory file.

|`docker_storage`
|Only runs on hosts that depend on the *docker* daemon (nodes and system
container installations). Checks that *docker*'s total usage does not exceed a
user-defined limit. If no user-defined limit is set, *docker*'s maximum usage
threshold defaults to 90% of the total size available. The threshold limit for
total percent usage can be set with a variable in your inventory file:
`max_thinpool_data_usage_percent=90`. A user-defined limit for maximum thinpool
usage might be set by setting the `max_thinpool_data_usage_percent` cluster
variable in your inventory file.

|`docker_storage_driver`
|Ensures that the *docker* daemon is using a storage driver supported by
{product-title}. If the `devicemapper` storage driver is being used, the check
additionally ensures that a loopback device is not being used. For more
information, see
link:https://docs.docker.com/storage/storagedriver/device-mapper-driver/[Docker's
Use the Device Mapper Storage Driver guide].

|`docker_image_availability`
|Attempts to ensure that images required by an {product-title} installation are
available either locally or in at least one of the configured container image
registries on the host machine.

|`package_version`
|Runs on `yum`-based systems determining if multiple releases of a required
{product-title} package are available. Having multiple releases of a package
available during an `enterprise` installation of OpenShift suggests that there
are multiple `yum` repositories enabled for different releases, which might lead
to installation problems.

|`package_availability`
|Runs prior to RPM installations of {product-title}. Ensures that
RPM packages required for the current installation are available.

|`package_update`
|Checks whether a `yum` update or package installation will succeed, without
actually performing it or running `yum` on the host.
|===

To disable specific pre-install checks, include the variable
`openshift_disable_check` with a comma-delimited list of check names in your
inventory file. For example:

----
openshift_disable_check=memory_availability,disk_availability
----

[NOTE]
====
A similar set of health checks meant to run for diagnostics on existing clusters
can be found in
xref:../admin_guide/diagnostics_tool.adoc#admin-guide-health-checks-via-ansible-playbook[Ansible-based Health Checks]. Another set of checks for checking certificate expiration can be
found in
xref:../install_config/redeploying_certificates.adoc#install-config-redeploying-certificates[Redeploying Certificates].
====

[[advanced-install-configuring-registry-location]]
== Configuring a Registry Location

If you use the default registry at `registry.redhat.io`, you must set the following
variables:

----
oreg_url=registry.redhat.io/openshift3/ose-${component}:${version}
oreg_auth_user="<user>"
oreg_auth_password="<password>"
----

For more information about setting up the registry access token, see
link:https://access.redhat.com/RegistryAuthentication[Red Hat Container Registry
Authentication].

If you use an image registry other than the default at `registry.redhat.io`,
specify the registry in the *_/etc/ansible/hosts_* file.

----
oreg_url=example.com/openshift3/ose-${component}:${version}
openshift_examples_modify_imagestreams=true
----

.Registry Variables
[options="header"]
|===

|Variable |Purpose
|`oreg_url`
|Set to the alternate image location. Necessary if you are not using the default
registry at `registry.redhat.io`. The default component inherits the image
prefix and version from the `oreg_url` value. For the default registry, and registries
that require authentication, you need to specify `oreg_auth_user` and
`oreg_auth_password`.

|`openshift_examples_modify_imagestreams`
|Set to `true` if pointing to a registry other than the default. Modifies the
image stream location to the value of `oreg_url`.

|`oreg_auth_user`
|If `oreg_url` points to a registry requiring authentication, use the `oreg_auth_user`
variable to provide your user name. You must also provide your password as the
`oreg_auth_password` parameter value. If you use the default registry, specify a
user that can access `registry.redhat.io`.

|`oreg_auth_password`
|If `oreg_url` points to a registry requiring authentication, use the `oreg_auth_password`
variable to provide your password. You must also provide your user name as the
`oreg_auth_user` parameter value. If you use the default registry, specify the
password or token for that user.
|===

[NOTE]
====
The default registry requires an authentication token. For more information,
see xref:../install_config/configuring_red_hat_registry.adoc#install-config-configuring-red-hat-registry[Accessing and Configuring the Red Hat Registry]
====

For example:
----
oreg_url=example.com/openshift3/ose-${component}:${version}
oreg_auth_user=${user_name}
oreg_auth_password=${password}
openshift_examples_modify_imagestreams=true
----

[[advanced-install-configuring-docker-route]]
== Configuring a Registry Route

To allow users to push and pull images to the internal container image registry from
outside of the {product-title} cluster, configure the registry route in the
*_/etc/ansible/hosts_* file. By default, the registry route is
*_docker-registry-default.router.default.svc.cluster.local_*.


.Registry Route Variables
[options="header"]
|===

|Variable |Purpose
|`openshift_hosted_registry_routehost`
|Set to the value of the desired registry route. The route contains either
a name that resolves to an infrastructure node where a router manages
communication or the subdomain that you set as the default application subdomain
wildcard value. For example, if you set the `openshift_master_default_subdomain`
parameter to `apps.example.com` and `.apps.example.com` resolves to
infrastructure nodes or a load balancer, you might use
`registry.apps.example.com` as the registry route.

|`openshift_hosted_registry_routecertificates`
a|Set the paths to the registry certificates. If you do not provide values for
the certificate locations, certificates are generated. You can define locations
for the following certificates:

* `certfile`
* `keyfile`
* `cafile`

|`openshift_hosted_registry_routetermination`
a| Set to one of the following values:

* Set to `reencrypt` to terminate encryption at the edge
router and re-encrypt it with a new certificate supplied by the destination.
* Set to `passthrough` to terminate encryption at
the destination. The destination is responsible for decrypting traffic.
|===

For example:
----
openshift_hosted_registry_routehost=<path>
openshift_hosted_registry_routetermination=reencrypt
openshift_hosted_registry_routecertificates= "{'certfile': '<path>/org-cert.pem', 'keyfile': '<path>/org-privkey.pem', 'cafile': '<path>/org-chain.pem'}"

----

[[install-configuring-router-sharding]]
== Configuring Router Sharding

xref:../architecture/networking/routes.adoc#router-sharding[Router sharding]
support is enabled by supplying the correct data to the inventory. The variable
`openshift_hosted_routers` holds the data, which is in the form of a list. If no
data is passed, then a default router is created. There are multiple
combinations of router sharding. The following example supports routers on
separate nodes:

----
openshift_hosted_routers=[{'name': 'router1', 'certificate': {'certfile': '/path/to/certificate/abc.crt',
'keyfile': '/path/to/certificate/abc.key', 'cafile':
'/path/to/certificate/ca.crt'}, 'replicas': 1, 'serviceaccount': 'router',
'namespace': 'default', 'stats_port': 1936, 'edits': [], 'images':
'openshift3/ose-${component}:${version}', 'selector': 'type=router1', 'ports':
['80:80', '443:443']},

{'name': 'router2', 'certificate': {'certfile': '/path/to/certificate/xyz.crt',
'keyfile': '/path/to/certificate/xyz.key', 'cafile':
'/path/to/certificate/ca.crt'}, 'replicas': 1, 'serviceaccount': 'router',
'namespace': 'default', 'stats_port': 1936, 'edits': [{'action': 'append',
'key': 'spec.template.spec.containers[0].env', 'value': {'name': 'ROUTE_LABELS',
'value': 'route=external'}}], 'images':
'openshift3/ose-${component}:${version}', 'selector': 'type=router2', 'ports':
['80:80', '443:443']}]
----

[[advanced-install-glusterfs-persistent-storage]]
== Configuring {gluster} Persistent Storage

include::install/topics/glusterfs_intro.adoc[]

You configure {gluster} clusters using variables, which interact with the
{product-title} clusters. The variables, which you define in the `[OSEv3:vars]`
group, include host variables, role variables, and image name and version tag
variables.

You use the `glusterfs_devices` host variable to define the list of block devices
to manage the {gluster} cluster. Each host in your configuration must
have at least one `glusterfs_devices` variable, and for every configuration,
there must be at least one bare device with no partitions or LVM PVs.

Role variables control the integration of a {gluster} cluster into a new or
existing {product-title} cluster. You can define a number of role variables,
each of which also has a corresponding variable to optionally configure a separate
{gluster} cluster for use as storage for an integrated Docker registry.

You can define image name and version tag variables to prevent {product-title}
pods from upgrading after an outage, which could lead to a cluster with different
{product-title} versions. You can also define these variables to specify the
image name and version tags for all containerized components.

Additional information and examples, including the ones below, can be found at
xref:../install_config/persistent_storage/persistent_storage_glusterfs.adoc#install-config-persistent-storage-persistent-storage-glusterfs[Persistent Storage Using {gluster}].

[[advanced-install-containerized-glusterfs-persistent-storage]]
=== Configuring {gluster-native}

[IMPORTANT]
====
See
xref:host_preparation.adoc#prereq-glusterfs[{gluster-native} Considerations]
for specific host preparations and prerequisites.
====

include::install_config/persistent_storage/topics/glusterfs_example_basic.adoc[]

// List the specific images in the registry for OSE only
ifdef::openshift-enterprise[]
include::install_config/persistent_storage/topics/glusterfs_images.adoc[]
endif::[]

[[advanced-install-external-glusterfs-persistent-storage]]
=== Configuring {gluster-external}

include::install_config/persistent_storage/topics/glusterfs_example_basic_external.adoc[]

[[advanced-install-registry]]
== Configuring an OpenShift Container Registry

An integrated
xref:../architecture/infrastructure_components/image_registry.adoc#integrated-openshift-registry[OpenShift Container Registry]
can be deployed using the installer.

[[advanced-install-registry-storage]]
=== Configuring Registry Storage

If no registry storage options are used, the default OpenShift Container
Registry is ephemeral and all data will be lost when the pod no longer exists.

[IMPORTANT]
====
Testing shows issues with using the RHEL NFS server as a storage backend
for the container image registry. This includes the OpenShift Container Registry and Quay.
Therefore, using the RHEL NFS server to back PVs used by core services is not recommended.

Other NFS implementations on the marketplace might not have these issues. Contact
the individual NFS implementation vendor for more information on any
testing that was possibly completed against these OpenShift core components.
====

There are several options for enabling registry storage when using the advanced
installer:

[discrete]
[[advanced-install-registry-storage-nfs-host-group]]
==== Option A: NFS Host Group

When the following variables are set, an NFS volume is created during cluster
installation with the path *_<nfs_directory>/<volume_name>_* on the host in
the `[nfs]` host group. For example, the volume path using these options is
be *_/exports/registry_*:

----
[OSEv3:vars]

# nfs_directory must conform to DNS-1123 subdomain must consist of lower case
# alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character

openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=10Gi
----

[discrete]
[[advanced-install-registry-storage-external-nfs]]
==== Option B: External NFS Host

To use an external NFS volume, one must already exist with a path of
*_<nfs_directory>/<volume_name>_* on the storage host. The remote volume path
using the following options is *_nfs.example.com:/exports/registry_*.

----
[OSEv3:vars]

# nfs_directory must conform to DNS-1123 subdomain must consist of lower case
# alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character

openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_host=nfs.example.com
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=10Gi
----

[discrete]
[[advanced-install-cluster-registry-storage-upgrade-nfs]]
==== Upgrading or Installing {product-title} with NFS

[discrete]
[[advanced-install-registry-storage-openstack]]
==== Option C: OpenStack Platform

An OpenStack storage configuration must already exist.

----
[OSEv3:vars]

openshift_hosted_registry_storage_kind=openstack
openshift_hosted_registry_storage_access_modes=['ReadWriteOnce']
openshift_hosted_registry_storage_openstack_filesystem=ext4
openshift_hosted_registry_storage_openstack_volumeID=3a650b4f-c8c5-4e0a-8ca5-eaee11f16c57
openshift_hosted_registry_storage_volume_size=10Gi
----

[discrete]
[[advanced-install-registry-storage-aws]]
==== Option D: AWS or Another S3 Storage Solution

The simple storage solution (S3) bucket must already exist.

----
[OSEv3:vars]

#openshift_hosted_registry_storage_kind=object
#openshift_hosted_registry_storage_provider=s3
#openshift_hosted_registry_storage_s3_accesskey=access_key_id
#openshift_hosted_registry_storage_s3_secretkey=secret_access_key
#openshift_hosted_registry_storage_s3_bucket=bucket_name
#openshift_hosted_registry_storage_s3_region=bucket_region
#openshift_hosted_registry_storage_s3_chunksize=26214400
#openshift_hosted_registry_storage_s3_rootdirectory=/registry
#openshift_hosted_registry_pullthrough=true
#openshift_hosted_registry_acceptschema2=true
#openshift_hosted_registry_enforcequota=true
----

If you use a different S3 service, such as Minio or ExoScale, also add the
region endpoint parameter:

----
openshift_hosted_registry_storage_s3_regionendpoint=https://myendpoint.example.com/
----

[discrete]
[[advanced-install-registry-storage-glusterfs]]
==== Option E: {gluster-native}

Similar to
xref:advanced-install-containerized-glusterfs-persistent-storage[configuring {gluster-native}],
{gluster} can be configured to provide storage for an OpenShift Container
Registry during the initial installation of the cluster to offer redundant and
reliable storage for the registry.

[IMPORTANT]
====
See
xref:host_preparation.adoc#prereq-glusterfs[{gluster-native} Considerations]
for specific host preparations and prerequisites.
====

include::install_config/persistent_storage/topics/glusterfs_example_registry.adoc[]

[discrete]
[[advanced-install-registry-storage-gce]]
==== Option F: Google Cloud Storage (GCS) bucket on Google Compute Engine (GCE)

A GCS bucket must already exist.

----
[OSEv3:vars]

openshift_hosted_registry_storage_provider=gcs
openshift_hosted_registry_storage_gcs_bucket=bucket01
openshift_hosted_registry_storage_gcs_keyfile=test.key
openshift_hosted_registry_storage_gcs_rootdirectory=/registry
----

[discrete]
[[advanced-install-registry-storage-vsphere]]
==== Option G: vSphere Volume with vSphere Cloud Provider (VCP)

The vSphere Cloud Provider must be configured with a datastore accessible by the
{product-title} nodes.

When using vSphere volume for the registry, you must set the storage access mode
to `ReadWriteOnce` and the replica count to `1`:

----
[OSEv3:vars]

openshift_hosted_registry_storage_kind=vsphere
openshift_hosted_registry_storage_access_modes=['ReadWriteOnce']
openshift_hosted_registry_storage_annotations=['volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/vsphere-volume']
openshift_hosted_registry_replicas=1
----

[[advanced-install-configuring-global-proxy]]
== Configuring Global Proxy Options

If your hosts require use of a HTTP or HTTPS proxy in order to connect to
external hosts, there are many components that must be configured to use the
proxy, including masters, Docker, and builds. Node services only connect to the
master API requiring no external access and therefore do not need to be
configured to use a proxy.

In order to simplify this configuration, the following Ansible variables can be
specified at a cluster or host level to apply these settings uniformly across
your environment.

[NOTE]
====
See xref:../install_config/build_defaults_overrides.adoc#install-config-build-defaults-overrides[Configuring
Global Build Defaults and Overrides] for more information on how the proxy
environment is defined for builds.
====

.Cluster Proxy Variables
[options="header"]
|===

|Variable |Purpose

|`openshift_http_proxy`
|This variable specifies the `HTTP_PROXY` environment variable for masters and
the Docker daemon.

|`openshift_https_proxy`
|This variable specifices the `HTTPS_PROXY` environment variable for masters
and the Docker daemon.

|`openshift_no_proxy`
a|This variable is used to set the `NO_PROXY` environment variable for masters
and the Docker daemon. Provide a comma-separated list of host names, domain
names, or wildcard host names that do not use the defined proxy. By default,
this list is augmented with the list of all defined {product-title} host names.

The host names that do not use the defined proxy include:

* Master and node host names. You must include the domain suffix.
* Other internal host names. You must include the domain suffix.
* etcd IP addresses. You must provide the IP address because etcd access is managed by IP address.
* The container image registry IP address.
* The Kubernetes IP address. This value is `172.30.0.1` by default and the
`openshift_portal_net` parameter value if you provided one.
* The `cluster.local` Kubernetes internal domain suffix.
* The `svc` Kubernetes internal domain suffix.

|`openshift_generate_no_proxy_hosts`
|This boolean variable specifies whether or not the names of all defined
OpenShift hosts and `pass:[*.cluster.local]` are automatically appended to
the `NO_PROXY` list. Defaults to `true`; set it to `false` to override this
option.

|`openshift_builddefaults_http_proxy`
|This variable defines the `HTTP_PROXY` environment variable inserted into
builds using the `BuildDefaults` admission controller. If you do not define
this parameter but define the `openshift_http_proxy` parameter, the
`openshift_http_proxy` value is used. Set the
`openshift_builddefaults_http_proxy` value to `False` to disable default
http proxy for builds regardless of the `openshift_http_proxy` value.

|`openshift_builddefaults_https_proxy`
|This variable defines the `HTTPS_PROXY` environment variable inserted into
builds using the `BuildDefaults` admission controller. If you do not define
this parameter but define the `openshift_http_proxy` parameter, the
`openshift_https_proxy` value is used. Set the
`openshift_builddefaults_https_proxy` value to `False` to disable default
https proxy for builds regardless of the `openshift_https_proxy` value.

|`openshift_builddefaults_no_proxy`
|This variable defines the `NO_PROXY` environment variable inserted into
builds using the `BuildDefaults` admission controller. Set the
`openshift_builddefaults_no_proxy` value to `False` to disable default no proxy
settings for builds regardless of the `openshift_no_proxy` value.

|`openshift_builddefaults_git_http_proxy`
|This variable defines the HTTP proxy used by `git clone` operations during a
build, defined using the `BuildDefaults` admission controller. Set the
`openshift_builddefaults_git_http_proxy` value to `False` to disable default
http proxy for `git clone` operations during a build regardless of the
`openshift_http_proxy` value.

|`openshift_builddefaults_git_https_proxy`
|This variable defines the HTTPS proxy used by `git clone` operations during a
build, defined using the `BuildDefaults` admission controller. Set the
`openshift_builddefaults_git_https_proxy` value to `False` to disable default
https proxy for `git clone` operations during a build regardless of the
`openshift_https_proxy` value.
|===


ifdef::openshift-enterprise,openshift-origin[]
[[advanced-install-configuring-firewalls]]
== Configuring the Firewall

[IMPORTANT]
====
* If you are changing the default firewall, ensure that each host in your cluster
is using the same firewall type to prevent inconsistencies.
* Do not use firewalld with the {product-title} installed on Atomic Host. firewalld is not supported on Atomic host.
====

[NOTE]
====
While iptables is the default firewall, firewalld is recommended for new
installations.
====

{product-title} uses iptables as the default firewall, but you can configure
your cluster to use firewalld during the install process.

Because iptables is the default firewall, {product-title} is designed to have it
configured automatically. However, iptables rules can break {product-title} if
not configured correctly. The advantages of firewalld include allowing multiple
objects to safely share the firewall rules.

To use firewalld as the firewall for an {product-title} installation, add the
`os_firewall_use_firewalld` variable to the list of configuration variables in
the Ansible host file at install:

----
[OSEv3:vars]
os_firewall_use_firewalld=True <1>
----
<1> Setting this variable to `true` opens the required ports and adds rules to
the default zone, ensuring that firewalld is configured correctly.

[NOTE]
====
Using the firewalld default configuration comes with limited configuration
options, and cannot be overridden. For example, while you can set up a storage
network with interfaces in multiple zones, the interface that nodes communicate
on must be in the default zone.
====

endif::[]

[[advanced-install-session-options]]
== Configuring Session Options

xref:../install_config/configuring_authentication.adoc#session-options[Session
options] in the OAuth configuration are configurable in the inventory file. By
default, Ansible populates a `sessionSecretsFile` with generated
authentication and encryption secrets so that sessions generated by one master
can be decoded by the others. The default location is
*_/etc/origin/master/session-secrets.yaml_*, and this file will only be
re-created if deleted on all masters.

You can set the session name and maximum number of seconds with
`openshift_master_session_name` and `openshift_master_session_max_seconds`:

----
openshift_master_session_name=ssn
openshift_master_session_max_seconds=3600
----

If provided, `openshift_master_session_auth_secrets` and
`openshift_master_encryption_secrets` must be equal length.

For `openshift_master_session_auth_secrets`, used to authenticate sessions
using HMAC, it is recommended to use secrets with 32 or 64 bytes:

----
openshift_master_session_auth_secrets=['DONT+USE+THIS+SECRET+b4NV+pmZNSO']
----

For `openshift_master_encryption_secrets`, used to encrypt sessions, secrets
must be 16, 24, or 32 characters long, to select AES-128, AES-192, or AES-256:

----
openshift_master_session_encryption_secrets=['DONT+USE+THIS+SECRET+b4NV+pmZNSO']
----

[[advanced-install-custom-certificates]]
== Configuring Custom Certificates

xref:../install_config/certificate_customization.adoc#install-config-certificate-customization[Custom serving
certificates] for the public host names of the {product-title} API and
xref:../architecture/infrastructure_components/web_console.adoc#architecture-infrastructure-components-web-console[web console]
can be deployed during cluster installation and are configurable in the
inventory file.

[NOTE]
====
Configure custom certificates for the host name associated with
the `*publicMasterURL*`, which you set as the
`*openshift_master_cluster_public_hostname*` parameter value. Using a custom serving certificate
for the host name associated with the `*masterURL*`
(`openshift_master_cluster_hostname`) results in TLS errors because
infrastructure components attempt to contact the master API using the
internal `*masterURL*` host.
====

Certificate and key file paths can be configured using the
`openshift_master_named_certificates` cluster variable:

----
openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key", "cafile": "/path/to/custom-ca1.crt"}]
----

File paths must be local to the system where Ansible will be run. Certificates
are copied to master hosts and are deployed in the
*_/etc/origin/master/named_certificates/_* directory.

Ansible detects a certificate's `Common Name` and `Subject Alternative Names`.
Detected names can be overridden by providing the `"names"` key when setting
`openshift_master_named_certificates`:

----
openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key", "names": ["public-master-host.com"], "cafile": "/path/to/custom-ca1.crt"}]
----

Certificates configured using `openshift_master_named_certificates` are cached
on masters, meaning that each additional Ansible run with a different set of
certificates results in all previously deployed certificates remaining in place
on master hosts and in the master configuration file.

If you want to overwrite `*openshift_master_named_certificates*` with
the provided value (or no value), specify the
`openshift_master_overwrite_named_certificates` cluster variable:

----
openshift_master_overwrite_named_certificates=true
----

For a more complete example, consider the following cluster variables in an
inventory file:

----
openshift_master_cluster_method=native
openshift_master_cluster_hostname=lb-internal.openshift.com
openshift_master_cluster_public_hostname=custom.openshift.com
----

To overwrite the certificates on a subsequent Ansible run, set the
following parameter values:

----
openshift_master_named_certificates=[{"certfile": "/root/STAR.openshift.com.crt", "keyfile": "/root/STAR.openshift.com.key", "names": ["custom.openshift.com"], "cafile": "/root/ca-file.crt"}]
openshift_master_overwrite_named_certificates=true
----

[IMPORTANT]
====
The `cafile` certificate is imported to the `ca-bundle.crt` file on the masters during installation or during redeployment of certificates. The `ca-bundle.crt` file is mounted to every pod that runs in {product-title}. Several {product-title} components automatically trust the named certificates by default when they access the `masterPublicURL` endpoint. If you omit the `cafile` option from the certificates parameter, the functionality of Web Console and several other components is reduced.
====

[[advanced-install-config-certificate-validity]]
== Configuring Certificate Validity

By default, the certificates used to govern the etcd, master, and kubelet expire
after two to five years. The validity (length in days until they expire) for the
auto-generated registry, CA, node, and master certificates can be configured
during installation using the following variables (default values shown):

----
[OSEv3:vars]

openshift_hosted_registry_cert_expire_days=730
openshift_ca_cert_expire_days=1825
openshift_master_cert_expire_days=730
etcd_ca_default_days=1825
----

These values are also used when
xref:../install_config/redeploying_certificates.adoc#install-config-redeploying-certificates[redeploying certificates] via Ansible post-installation.

[[advanced-install-monitoring]]
== Configuring Cluster Monitoring

Prometheus Cluster Monitoring is set to automatically deploy. To prevent its
automatic deployment, set the following:

----
[OSEv3:vars]

openshift_cluster_monitoring_operator_install=false
----

For more information on Prometheus Cluster Monitoring and its configuration, see
xref:../install_config/prometheus_cluster_monitoring.adoc#prometheus-cluster-monitoring[Prometheus
Cluster Monitoring documentation].

[[advanced-install-cluster-metrics]]
== Configuring Cluster Metrics

Cluster metrics are not set to automatically deploy. Set the following to enable
cluster metrics during cluster installation:

----
[OSEv3:vars]

openshift_metrics_install_metrics=true
----

The metrics public URL can be set during cluster
installation using the `openshift_metrics_hawkular_hostname` Ansible variable,
which defaults to:

`\https://hawkular-metrics.{{openshift_master_default_subdomain}}/hawkular/metrics`

If you alter this variable, ensure the host name is accessible via your router.

`openshift_metrics_hawkular_hostname=hawkular-metrics.{{openshift_master_default_subdomain}}`

[IMPORTANT]
====
In accordance with upstream Kubernetes rules, metrics can be collected only on the default interface of `eth0`.
====

[NOTE]
====
You must set an `openshift_master_default_subdomain` value to deploy metrics.
====

[[advanced-install-cluster-metrics-storage]]
=== Configuring Metrics Storage

The `openshift_metrics_cassandra_storage_type` variable must be set in order to
use persistent storage for metrics. If
`openshift_metrics_cassandra_storage_type` is not set, then cluster metrics data
is stored in an `emptyDir` volume, which will be deleted when the Cassandra pod
terminates.

[IMPORTANT]
====
Testing shows issues with using the RHEL NFS server as a storage backend
for the container image registry. This includes Cassandra for metrics storage.
Therefore, using the RHEL NFS server to back PVs used by core services is not recommended.

Cassandra is designed to provide redundancy via multiple independent, instances.
For this reason, using NFS or a SAN for data directories is an antipattern and
is not recommended.

However, NFS/SAN implementations on the marketplace might not have issues backing
or providing storage to this component. Contact the individual NFS/SAN
implementation vendor for more information on any testing that was possibly completed
against these OpenShift core components.
====

There are three options for enabling cluster metrics storage during cluster
installation:

[discrete]
[[advanced-install-cluster-metrics-storage-dynamic]]
==== Option A: Dynamic

If your {product-title} environment supports
xref:../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[dynamic volume provisioning]
for your cloud provider, use the following variable:

----
[OSEv3:vars]

openshift_metrics_cassandra_storage_type=dynamic
----

If there are multiple default dynamically provisioned volume types, such as
gluster-storage and glusterfs-storage-block, you can specify the
provisioned volume type by variable. Use the following variables:

----
[OSEv3:vars]

openshift_metrics_cassandra_storage_type=pv
openshift_metrics_cassandra_pvc_storage_class_name=glusterfs-storage-block
----

Check
xref:../install_config/master_node_configuration.adoc#master-node-config-volume-config[Volume
Configuration] for more information on using `DynamicProvisioningEnabled` to
enable or disable dynamic provisioning.

[discrete]
[[advanced-install-cluster-metrics-storage-nfs-host-group]]
==== Option B: NFS Host Group

When the following variables are set, an NFS volume is created during cluster
installation with path *_<nfs_directory>/<volume_name>_* on the host in the
`[nfs]` host group. For example, the volume path using these options is
*_/exports/metrics_*:

----
[OSEv3:vars]

# nfs_directory must conform to DNS-1123 subdomain must consist of lower case
# alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character

openshift_metrics_storage_kind=nfs
openshift_metrics_storage_access_modes=['ReadWriteOnce']
openshift_metrics_storage_nfs_directory=/exports
openshift_metrics_storage_nfs_options='*(rw,root_squash)'
openshift_metrics_storage_volume_name=metrics
openshift_metrics_storage_volume_size=10Gi
----

[discrete]
[[advanced-install-cluster-metrics-storage-external-nfs]]
==== Option C: External NFS Host

To use an external NFS volume, one must already exist with a path of
*_<nfs_directory>/<volume_name>_* on the storage host.

----
[OSEv3:vars]

# nfs_directory must conform to DNS-1123 subdomain must consist of lower case
# alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character

openshift_metrics_storage_kind=nfs
openshift_metrics_storage_access_modes=['ReadWriteOnce']
openshift_metrics_storage_host=nfs.example.com
openshift_metrics_storage_nfs_directory=/exports
openshift_metrics_storage_volume_name=metrics
openshift_metrics_storage_volume_size=10Gi
----

The remote volume path using the following options is
*_nfs.example.com:/exports/metrics_*.

[discrete]
[[advanced-install-cluster-metrics-storage-upgrade-nfs]]
==== Upgrading or Installing {product-title} with NFS

The use of NFS for the core {product-title} components is not recommended, as NFS
(and the NFS Protocol) does not provide the proper consistency needed for the
applications that make up the {product-title} infrastructure.

As a result, the installer and update playbooks require an option to enable the use
of NFS with core infrastructure components.

----
# Enable unsupported configurations, things that will yield a partially
# functioning cluster but would not be supported for production use
#openshift_enable_unsupported_configurations=false
----

If you see the following messages when upgrading or installing your cluster,
then an additional step is required.

----
TASK [Run variable sanity checks] **********************************************
fatal: [host.example.com]: FAILED! => {"failed": true, "msg": "last_checked_host: host.example.com, last_checked_var: openshift_hosted_registry_storage_kind;nfs is an unsupported type for openshift_hosted_registry_storage_kind. openshift_enable_unsupported_configurations=True mustbe specified to continue with this configuration."}
----

In your Ansible inventory file, specify the following parameter:
----
[OSEv3:vars]
openshift_enable_unsupported_configurations=True
----

[[advanced-install-cluster-logging]]
== Configuring Cluster Logging

Cluster logging is not set to automatically deploy by default. Set the following
to enable cluster logging during cluster installation:

----
[OSEv3:vars]

openshift_logging_install_logging=true
----

[NOTE]
====
When installing cluster logging, you must also specify a node selector,
such as `openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra": "true"}`
in the Ansible inventory file.
====

For more information on the available cluster logging variables, see
xref:../install_config/aggregate_logging.html#aggregate-logging-ansible-variables[Specifying Logging Ansible Variables].

[[advanced-installation-logging-storage]]
=== Configuring Logging Storage

The `openshift_logging_es_pvc_dynamic` variable must be set in order to use
persistent storage for logging. If `openshift_logging_es_pvc_dynamic` is
not set, then cluster logging data is stored in an `emptyDir` volume, which will
be deleted when the Elasticsearch pod terminates.

[IMPORTANT]
====
Testing shows issues with using the RHEL NFS server as a storage backend
for the container image registry. This includes ElasticSearch for logging storage.
Therefore, using the RHEL NFS server to back PVs used by core services is not recommended.

Due to ElasticSearch not implementing a custom deletionPolicy, the use of NFS
storage as a volume or a persistent volume is not supported for Elasticsearch
storage, as Lucene and the default deletionPolicy, relies on file system behavior
that NFS does not supply. Data corruption and other problems can occur.

NFS implementations on the marketplace might not have these issues. Contact
the individual NFS implementation vendor for more information on any testing they
might have performed against these OpenShift core components.
====

There are three options for enabling cluster logging storage during cluster
installation:

[discrete]
[[advanced-installation-logging-storage-dynamic]]
==== Option A: Dynamic

If your {product-title} environment has dynamic volume provisioning, it could be configured
either via the cloud provider or by an independent storage provider. For instance, the cloud provider
could have a StorageClass with provisioner `kubernetes.io/gce-pd` on GCE, and an
independent storage provider such as GlusterFS could have a `StorageClass` with provisioner
`kubernetes.io/glusterfs`. In either case, use the following variable:

----
[OSEv3:vars]

openshift_logging_es_pvc_dynamic=true
----

For additional information on dynamic provisioning, see
xref:../install_config/persistent_storage/dynamically_provisioning_pvs.adoc#install-config-persistent-storage-dynamically-provisioning-pvs[Dynamic provisioning and creating storage classes].


If there are multiple default dynamically provisioned volume types, such as
gluster-storage and glusterfs-storage-block, you can specify the
provisioned volume type by variable. Use the following variables:

----
[OSEv3:vars]

openshift_logging_elasticsearch_storage_type=pvc
openshift_logging_es_pvc_storage_class_name=glusterfs-storage-block
----

Check
xref:../install_config/master_node_configuration.adoc#master-node-config-volume-config[Volume
Configuration] for more information on using `DynamicProvisioningEnabled` to
enable or disable dynamic provisioning.

[discrete]
[[advanced-installation-logging-storage-nfs-host-group]]
==== Option B: NFS Host Group

When the following variables are set, an NFS volume is created during cluster
installation with path *_<nfs_directory>/<volume_name>_* on the host in the
`[nfs]` host group. For example, the volume path using these options is
*_/exports/logging_*:

----
[OSEv3:vars]

# nfs_directory must conform to DNS-1123 subdomain must consist of lower case
# alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character

openshift_logging_storage_kind=nfs
openshift_logging_storage_access_modes=['ReadWriteOnce']
openshift_logging_storage_nfs_directory=/exports <1>
openshift_logging_storage_nfs_options='*(rw,root_squash)' <1>
openshift_logging_storage_volume_name=logging <2>
openshift_logging_storage_volume_size=10Gi
openshift_enable_unsupported_configurations=true
openshift_logging_elasticsearch_storage_type=pvc
openshift_logging_es_pvc_size=10Gi
openshift_logging_es_pvc_storage_class_name=''
openshift_logging_es_pvc_dynamic=true
openshift_logging_es_pvc_prefix=logging
----

<1> These parameters work only with the `/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml`
installation playbook. The parameters will not work with the `/usr/share/ansible/openshift-ansible/playbooks/openshift-logging/config.yml`
playbook.
<2> The NFS volume name must be `logging`.

[discrete]
[[advanced-installation-logging-storage-external-nfs]]
==== Option C: External NFS Host

To use an external NFS volume, one must already exist with a path of
*_<nfs_directory>/<volume_name>_* on the storage host.

----
[OSEv3:vars]

# nfs_directory must conform to DNS-1123 subdomain must consist of lower case
# alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character

openshift_logging_storage_kind=nfs
openshift_logging_storage_access_modes=['ReadWriteOnce']
openshift_logging_storage_host=nfs.example.com <1>
openshift_logging_storage_nfs_directory=/exports <1>
openshift_logging_storage_volume_name=logging <2>
openshift_logging_storage_volume_size=10Gi
openshift_enable_unsupported_configurations=true
openshift_logging_elasticsearch_storage_type=pvc
openshift_logging_es_pvc_size=10Gi
openshift_logging_es_pvc_storage_class_name=''
openshift_logging_es_pvc_dynamic=true
openshift_logging_es_pvc_prefix=logging
----

<1> These parameters work only with the `/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml`
installation playbook. The parameters will not work with the `/usr/share/ansible/openshift-ansible/playbooks/openshift-logging/config.yml`
playbook.
<2> The NFS volume name must be `logging`.

The remote volume path using the following options is
*_nfs.example.com:/exports/logging_*.

[discrete]
[[advanced-install-cluster-logging-storage-upgrade-nfs]]
==== Upgrading or Installing {product-title} with NFS

The use of NFS for the core {product-title} components is not recommended, as NFS
(and the NFS Protocol) does not provide the proper consistency needed for the
applications that make up the {product-title} infrastructure.

As a result, the installer and update playbooks require an option to enable the use
of NFS with core infrastructure components.

----
# Enable unsupported configurations, things that will yield a partially
# functioning cluster but would not be supported for production use
#openshift_enable_unsupported_configurations=false
----

If you see the following messages when upgrading or installing your cluster,
then an additional step is required.

----
TASK [Run variable sanity checks] **********************************************
fatal: [host.example.com]: FAILED! => {"failed": true, "msg": "last_checked_host: host.example.com, last_checked_var: openshift_hosted_registry_storage_kind;nfs is an unsupported type for openshift_hosted_registry_storage_kind. openshift_enable_unsupported_configurations=True mustbe specified to continue with this configuration."}
----

In your Ansible inventory file, specify the following parameter:
----
[OSEv3:vars]
openshift_enable_unsupported_configurations=True
----

[[enabling-service-catalog]]
== Customizing Service Catalog Options

The
xref:../architecture/service_catalog/index.adoc#architecture-additional-concepts-service-catalog[service catalog]
is enabled by default during installation. Enabling the service broker allows
you to register service brokers with the catalog. When the service catalog is
enabled, the OpenShift Ansible broker and template service broker are both
installed as well; see
xref:configuring-openshift-ansible-broker[Configuring the OpenShift Ansible Broker]
and xref:configuring-template-service-broker[Configuring the Template Service Broker]
for more information.
If you disable the service catalog, the OpenShift Ansible broker and template
service broker are not installed.

To disable automatic deployment of the service catalog, set the following
cluster variable in your inventory file:

----
openshift_enable_service_catalog=false
----

If you use your own registry, you must add:

* `openshift_service_catalog_image_prefix`: When pulling the service catalog
image, force the use of a specific prefix (for example, `registry`). You must
provide the full registry name up to the image name.

* `openshift_service_catalog_image_version`: When pulling the service catalog
image, force the use of a specific image version.

For example:

----
openshift_service_catalog_image="docker-registry.default.example.com/openshift/ose-service-catalog:${version}"
openshift_service_catalog_image_prefix="docker-registry-default.example.com/openshift/ose-"
openshift_service_catalog_image_version="v3.9.30"
----

[[configuring-openshift-ansible-broker]]
=== Configuring the OpenShift Ansible Broker

The
xref:../architecture/service_catalog/ansible_service_broker.adoc#arch-ansible-service-broker[OpenShift Ansible broker]
(OAB) is enabled by default during installation.

If you do not want to install the OAB, set the `ansible_service_broker_install`
parameter value to `false` in the inventory file:

----
ansible_service_broker_install=false
----

.Service broker customization variables
[options="header"]
|===
|Variable |Purpose

|`openshift_service_catalog_image_prefix`
|Specify the prefix for the service catalog component image.

|===

[[configuring-oab-storage]]
==== Configuring Persistent Storage for the OpenShift Ansible Broker

The OAB deploys its own etcd instance separate from the etcd used by the rest of
the {product-title} cluster. The OAB's etcd instance requires separate storage
using persistent volumes (PVs) to function. If no PV is available, etcd will
wait until the PV can be satisfied. The OAB application will enter a `CrashLoop`
state until its etcd instance is available.

Some Ansible playbook bundles (APBs) also require a PV for their own usage in
order to deploy. For example, each of the database APBs have two plans: the
Development plan uses ephemeral storage and does not require a PV, while the
Production plan is persisted and does require a PV.

[options="header"]
|===
|APB |PV Required?

|*postgresql-apb*
|Yes, but only for the Production plan

|*mysql-apb*
|Yes, but only for the Production plan

|*mariadb-apb*
|Yes, but only for the Production plan

|*mediawiki-apb*
|Yes

|===

To configure persistent storage for the OAB:

[NOTE]
====
The following example shows usage of an NFS host to provide the required PVs,
but
xref:../install_config/persistent_storage/index.adoc#install-config-persistent-storage-index[other persistent storage providers] can be used instead.
====

. In your inventory file, add `nfs` to the `[OSEv3:children]` section to enable
the `[nfs]` group:
+
----
[OSEv3:children]
masters
nodes
nfs
----

. Add a `[nfs]` group section and add the host name for the system that will
be the NFS host:
+
----
[nfs]
master1.example.com
----

. Add the following in the `[OSEv3:vars]` section:
+
----
# nfs_directory must conform to DNS-1123 subdomain must consist of lower case
# alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character

openshift_hosted_etcd_storage_kind=nfs
openshift_hosted_etcd_storage_nfs_options="*(rw,root_squash,sync,no_wdelay)"
openshift_hosted_etcd_storage_nfs_directory=/opt/osev3-etcd <1>
openshift_hosted_etcd_storage_volume_name=etcd-vol2 <1>
openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
openshift_hosted_etcd_storage_volume_size=1G
openshift_hosted_etcd_storage_labels={'storage': 'etcd'}

ifdef::openshift-origin[]
ansible_service_broker_registry_url=registry.redhat.io
ansible_service_broker_registry_user=<user_name> <2>
ansible_service_broker_registry_password=<password> <2>
ansible_service_broker_registry_organization=<organization> <2>
endif::[]
----
<1> An NFS volume will be created with path `<nfs_directory>/<volume_name>` on the
host in the `[nfs]` group. For example, the volume path using these options
is *_/opt/osev3-etcd/etcd-vol2_*.
ifdef::openshift-origin[]
<2> Only required if `ansible_service_broker_registry_url` is set to a registry that
requires authentication for pulling APBs.
endif::[]
+
These settings create a persistent volume that is attached to the OAB's etcd
instance during cluster installation.

[[configuring-oab-local-apb-devel]]
==== Configuring the OpenShift Ansible Broker for Local APB Development

In order to do xref:../apb_devel/index.adoc#apb-devel-intro[APB development]
with the OpenShift Container Registry in conjunction with the OAB, a whitelist
of images the OAB can access must be defined. If a whitelist is not defined, the
broker will ignore APBs and users will not see any APBs available.

By default, the whitelist is empty so that a user cannot add APB images to the
broker without a cluster administrator configuring the broker. To whitelist all
images that end in `-apb`:

. In your inventory file, add the following to the `[OSEv3:vars]` section:
+
----
ansible_service_broker_local_registry_whitelist=['.*-apb$']
----

[[configuring-template-service-broker]]
=== Configuring the Template Service Broker

The
xref:../architecture/service_catalog/template_service_broker.adoc#arch-template-service-broker[template service broker]
(TSB) is enabled by default during installation.

If you do not want to install the TSB, set the `template_service_broker_install`
parameter value to `false`:
----
template_service_broker_install=false
----

To configure the TSB, one or more projects must be defined as the broker's
source namespace(s) for loading templates and image streams into the service
catalog. Set the source projects by modifying the following in your inventory
file's `[OSEv3:vars]` section:

----
openshift_template_service_broker_namespaces=['openshift','myproject']
----

.Template service broker customization variables
[options="header"]
|===
|Variable |Purpose

|`template_service_broker_prefix`
|Specify the prefix for the template service broker component image.

|`ansible_service_broker_image_prefix`
|Specify the prefix for the ansible service broker component image.

|===

[[configuring-web-console-customization]]
== Configuring Web Console Customization

The following Ansible variables set master configuration options for customizing
the web console. See
xref:../install_config/web_console_customization.adoc#install-config-web-console-customization[Customizing the Web Console] for more details on these customization options.

.Web Console Customization Variables
[options="header"]
|===

|Variable |Purpose

|`openshift_web_console_install`
|Determines whether to install the web console. Can be set to `true` or `false`. Defaults to `true`.

|`openshift_web_console_prefix`
|Specify the prefix for the web console images.

|`openshift_master_logout_url`
|Sets `clusterInfo.logoutPublicURL` in the web console configuration. See xref:../install_config/web_console_customization.adoc#changing-the-logout-url[Changing the Logout URL] for details. Example value: `\https://example.com/logout`

|`openshift_web_console_extension_script_urls`
|Sets `extensions.scriptURLs` in the web console configuration. See xref:../install_config/web_console_customization.adoc#loading-custom-scripts-and-stylesheets[Loading Extension Scripts and Stylesheets] for details. Example value: `['https://example.com/scripts/menu-customization.js','https://example.com/scripts/nav-customization.js']`

|`openshift_web_console_extension_stylesheet_urls`
|Sets `extensions.stylesheetURLs` in the web console configuration. See xref:../install_config/web_console_customization.adoc#loading-custom-scripts-and-stylesheets[Loading Extension Scripts and Stylesheets] for details. Example value: `['https://example.com/styles/logo.css','https://example.com/styles/custom-styles.css']`

|`openshift_master_oauth_template`
|Sets the OAuth template in the master configuration. See xref:../install_config/web_console_customization.adoc#customizing-the-login-page[Customizing the Login Page] for details. Example value: `['/path/to/login-template.html']`

|`openshift_master_metrics_public_url`
|Sets `metricsPublicURL` in the master configuration. See xref:../install_config/cluster_metrics.adoc#install-setting-the-metrics-public-url[Setting the Metrics Public URL] for details. Example value: `\https://hawkular-metrics.example.com/hawkular/metrics`

|`openshift_master_logging_public_url`
|Sets `loggingPublicURL` in the master configuration. See xref:../install_config/aggregate_logging.adoc#aggregate-logging-kibana[Kibana] for details. Example value: `\https://kibana.example.com`

|`openshift_web_console_inactivity_timeout_minutes`
|Configurate the web console to log the user out automatically after a period of inactivity. Must be a whole number greater than or equal to 5, or 0 to disable the feature. Defaults to 0 (disabled).

|`openshift_web_console_cluster_resource_overrides_enabled`
|Boolean value indicating if the cluster is configured for overcommit. When `true`, the web console will hide fields for CPU request, CPU limit, and memory request when editing resource limits because you must set these values with the cluster resource override configuration.

|`openshift_web_console_enable_context_selector`
|Enable the context selector in the web console and admin console mastheads for quickly switching between the two consoles. Defaults to `true` when both consoles are installed.

|===

[[configuring-the-admin-console]]
== Configuring the Cluster Console

The cluster console is an additional web interface like the web console, but
focused on admin tasks. The cluster console supports many of the same common
{product-title} resources as the web console, but it also allows you to view
metrics about the cluster and manage cluster-scoped resources such as nodes,
persistent volumes, cluster roles, and custom resource definitions. The
following variables can be used to customize the cluster console.

.Cluster Console Customization Variables
[options="header"]
|===

|Variable |Purpose

|`openshift_console_install`
|Determines whether to install the cluster console. Can be set to `true` or `false`. Defaults to `true`.

|`openshift_console_hostname`
|Sets the host name of the cluster console. Defaults to `console.<openshift_master_default_subdomain>`. If you alter this variable, ensure the host name is accessible via your router.

|`openshift_console_cert`
|Optional certificate to use for the cluster console route. This is only needed if using a custom host name.

|`openshift_console_key`
|Optional key to use for the cluster console route. This is only needed if using a custom host name.

|`openshift_console_ca`
|Optional CA to use for the cluster console route. This is only needed if using a custom host name.

|`openshift_base_path`
|Optional base path for the cluster console. If set, it should begin and end with a slash like `/console/`. Defaults to `/` (no base path).

|`openshift_console_auth_ca_file`
|Optional CA file to use to connect to the OAuth server. Defaults to
`/var/run/secrets/kubernetes.io/serviceaccount/ca.crt`. Typically, this value
does not need to be changed. You must create a ConfigMap that contains the CA
file and mount it into the `console` Deployment in the `openshift-console`
namespace at the same location as the location that you specify.

|===

[[configuring-the-operator-lifecycle-manager]]
== Configuring the Operator Lifecycle Manager

ifdef::openshift-enterprise[]
[IMPORTANT]
====
The Operator Framework is a Technology Preview feature.
Technology Preview features are not supported with Red Hat production service
level agreements (SLAs), might not be functionally complete, and Red Hat does
not recommend to use them for production. These features provide early access to
upcoming product features, enabling customers to test functionality and provide
feedback during the development process.

For more information on Red Hat Technology Preview features support scope, see
https://access.redhat.com/support/offerings/techpreview/.
====
endif::[]

The Technology Preview
link:https://coreos.com/blog/introducing-operator-framework[Operator Framework]
includes the Operator Lifecycle Manager (OLM). You can optionally install the
OLM during cluster installation by setting the following variables in your
inventory file:

[NOTE]
====
Alternatively, the Technology Preview Operator Framework can be installed after
cluster installation. See
xref:../install_config/installing-operator-framework.adoc#installing-olm-using-ansible_installing-operator-framework[Installing Operator Lifecycle Manager using Ansible] for separate
instructions.
====

. Add the `openshift_enable_olm` variable in the `[OSEv3:vars]` section,
setting it to `true`:
+
----
openshift_enable_olm=true
----

. Add the `openshift_additional_registry_credentials` variable in the
`[OSEv3:vars]` section, setting credentials required to pull the Operator
containers:
+
----
openshift_additional_registry_credentials=[{'host':'registry.connect.redhat.com','user':'<your_user_name>','password':'<your_password>','test_image':'mongodb/enterprise-operator:0.3.2'}]
----
+
Set `user` and `password` to the credentials that you use to log in to the Red
Hat Customer Portal at link:https://access.redhat.com[].
+
The `test_image` represents an image that will be used to test the credentials
you provided.

After your cluster installation has completed successful, see
xref:../install_config/installing-operator-framework.adoc#launching-your-first-operator_installing-operator-framework[Launching your first Operator] for further steps on using the OLM as a cluster administrator during this Technology Preview phase.
