[[install-config-install-prerequisites]]
= System and environment requirements
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:
ifdef::openshift-enterprise[]
:pb-prefix: /usr/share/ansible/openshift-ansible/
endif::[]
ifdef::openshift-origin[]
:pb-prefix: ~/openshift-ansible/
endif::[]
include::_snippets/glusterfs.adoc[]

toc::[]

[[system-requirements]]
== System requirements

The hosts in your {product-title} environment must meet the following hardware
specifications and system-level requirements.

ifdef::openshift-enterprise[]
[[red-hat-subscription]]
=== Red Hat subscriptions
You must have an active {product-title} subscription on your Red Hat
account. If you do not, contact your sales representative for more information.

endif::[]

[[hardware]]
=== Minimum hardware requirements

The system requirements vary per host type:

[cols="1,7"]
|===

|xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#master[Masters]
a|- Physical or virtual system or an instance running on a public or private IaaS.
ifdef::openshift-origin[]
- Base OS: Fedora 21, CentOS 7.4,
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/installation_guide/index[Red Hat Enterprise Linux (RHEL) 7.4 or later]
with the "Minimal" installation option and the latest packages from the Extras
channel, or
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/installation_and_configuration_guide/[RHEL Atomic Host]
7.4.5 or later.
endif::[]
ifdef::openshift-enterprise[]
- Base OS:
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/installation_guide/index[Red Hat Enterprise Linux (RHEL) 7.4 or later]
with the "Minimal" installation option and the latest packages from the Extras
channel, or
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/installation_and_configuration_guide/[RHEL Atomic Host]
7.4.5 or later.
endif::[]
- Minimum 4 vCPU (additional are strongly recommended).
- Minimum 16 GB RAM (additional memory is strongly recommended, especially if etcd is co-located on masters).
- Minimum 40 GB hard disk space for the file system containing *_/var/_*. image:redcircle-1.png[]
- Minimum 1 GB hard disk space for the file system containing *_/usr/local/bin/_*.
- Minimum 1 GB hard disk space for the file system containing the system's
temporary directory. image:redcircle-2.png[]
- Masters with a co-located etcd require a minimum of 4 cores. Two-core systems do not work.

|xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#node[Nodes]
a| * Physical or virtual system, or an instance running on a public or private IaaS.
ifdef::openshift-origin[]
* Base OS: Fedora 21, CentOS 7.4,
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/installation_guide/index[RHEL 7.4 or later]
with "Minimal" installation option, or
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/installation_and_configuration_guide/[RHEL Atomic Host]
7.4.5 or later.
endif::[]
ifdef::openshift-enterprise[]
* Base OS:
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/installation_guide/index[RHEL 7.4 or later]
with "Minimal" installation option, or
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/installation_and_configuration_guide/[RHEL Atomic Host]
7.4.5 or later.
endif::[]
* NetworkManager 1.0 or later.
* 1 vCPU.
* Minimum 8 GB RAM.
* Minimum 15 GB hard disk space for the file system containing *_/var/_*. image:redcircle-1.png[]
* Minimum 1 GB hard disk space for the file system containing *_/usr/local/bin/_*.
* Minimum 1 GB hard disk space for the file system containing the system's temporary directory. image:redcircle-2.png[]
* An additional minimum 15 GB unallocated space per system running containers for Docker's storage
back end; see xref:host_preparation.adoc#configuring-docker-storage[Configuring
Docker Storage]. Additional space might be required, depending on the size and number of containers that run on the node.

|External etcd Nodes
a|* Minimum 20 GB hard disk space for etcd data.
* See link:https://github.com/coreos/etcd/blob/master/Documentation/op-guide/hardware.md#hardware-recommendations[the Hardware Recommendations section of the CoreOS etcd documentation] for information how to properly size your etcd nodes.
* Currently, {product-title} stores image, build, and deployment metadata in
etcd. You must periodically xref:../admin_guide/pruning_resources.adoc#admin-guide-pruning-resources[prune old resources].
If you are planning to leverage a large number of these resources, place etcd on
machines with large amounts of memory and fast SSD drives.

|Ansible controller
|The host that you run the Ansible playbook on must have at least 75MiB of free
memory per host in the inventory.
|===
image:redcircle-1.png[] Meeting the *_/var/_* file system sizing requirements in
RHEL Atomic Host requires making changes to the default configuration. See
https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/managing_containers/managing_storage_with_docker_formatted_containers[Managing Storage with Docker-formatted Containers] for instructions on configuring
this during or after installation.

image:redcircle-2.png[] The system's temporary directory is determined according
to the rules defined in the
https://docs.python.org/2/library/tempfile.html#tempfile.tempdir[`*tempfile*`]
module in Python's standard library.

[IMPORTANT]
====
{product-title} only supports servers with x86_64 architecture.
====

You must configure storage for each system that runs a container daemon. For
containerized installations, you need storage on masters. Also, by default, the
web console runs in containers on masters, and masters need storage to
run the web console. Containers run on nodes, so nodes always require storage.
The size of storage depends on workload, the number of containers, the
size of the running containers, and the containers' storage requirements.
You must also configure storage to run containerized etcd.

=== Production level hardware requirements

Test or sample environments function with the minimum requirements. For
production environments, the following recommendations apply:

Master hosts::
In a highly available {product-title} cluster with external etcd, a master host
needs to meet the minimum requirements and have 1 CPU
core and 1.5 GB of memory for each 1000 pods. Therefore, the recommended size of
a master host in an {product-title} cluster of 2000 pods is the minimum
requirements of 2 CPU cores and 16 GB of RAM, plus 2 CPU cores and 3 GB of RAM,
totaling 4 CPU cores and 19 GB of RAM.

See
xref:../scaling_performance/host_practices.adoc#scaling-performance-capacity-host-practices-master[Recommended
Practices for {product-title} Master Hosts] for performance guidance.

Node hosts::
The size of a node host depends on the expected size of its workload. As an
{product-title} cluster administrator, you need to calculate the expected
workload and add about 10 percent for overhead. For production environments,
allocate enough resources so that a node host failure does not affect your
maximum capacity.

For more information, see
xref:index.adoc#sizing[Sizing Considerations] and
xref:../scaling_performance/cluster_limits.adoc#scaling-performance-cluster-limits[Cluster
Limits].

[IMPORTANT]
====
Oversubscribing the physical resources on a node affects resource guarantees the
Kubernetes scheduler makes during pod placement. Learn what measures you can
take to xref:../admin_guide/overcommit.adoc#disabling-swap-memory[avoid memory swapping].
====

[[prerequisites-storage-management]]
=== Storage management

.The main directories to which {product-title} components write data

[cols="h,3*",options="header"]
|===
|Directory|Notes|Sizing|Expected Growth

|*_/var/lib/openshift_*
|Used for etcd storage only when in single master mode and etcd is embedded in the *atomic-openshift-master* process.
|Less than 10GB.
|Will grow slowly with the environment. Only storing metadata.

|*_/var/lib/etcd_*
|Used for etcd storage when in Multi-Master mode or when etcd is made standalone by an administrator.
|Less than 20 GB.
|Will grow slowly with the environment. Only storing metadata.

|*_/var/lib/docker_*
|When the run time is docker, this is the mount point. Storage used for active container runtimes (including pods) and storage of local images (not used for registry storage). Mount point should be managed by docker-storage rather than manually.
|50 GB for a Node with 16 GB memory.

Additional 20-25 GB for every additional 8 GB of memory.
|Growth is limited by the capacity for running containers.

|*_/var/lib/containers_*
|When the run time is CRI-O, this is the mount point. Storage used for active container runtimes (including pods) and storage of local images (not used for registry storage).
|50 GB for a Node with 16 GB memory.

Additional 20-25 GB for every additional 8 GB of memory.
|Growth limited by capacity for running containers

|*_/var/lib/origin/openshift.local.volumes_*
|Ephemeral volume storage for pods. This includes anything external that is mounted into a container at runtime. Includes environment variables, kube secrets, and data volumes not backed by persistent storage PVs.
|Varies
|Minimal if pods requiring storage are using persistent volumes. If using ephemeral storage, this can grow quickly.

|*_/var/log_*
|Log files for all components.
|10 to 30 GB.
|Log files can grow quickly; size can be managed by growing disks or managed using log rotate.

|===

[[hardware-glusterfs]]
=== {gluster} hardware requirements

include::install_config/persistent_storage/topics/glusterfs_prereqs_hardware.adoc[]

[[hardware-monitoring]]
=== Monitoring hardware requirements

The monitoring stack imposes additional system resource requirements and is
installed by default. See the
xref:../scaling_performance/scaling_cluster_monitoring.adoc#cluster-monitoring-recommendations-for-OCP[
computing resources recommendations] and the
xref:../install_config/prometheus_cluster_monitoring.adoc#overview[cluster monitoring
documentation].

[[prereq-selinux]]
=== SELinux requirements

Security-Enhanced Linux (SELinux) must be enabled on all of the servers before
installing {product-title} or the installer will fail. Also, configure
`SELINUX=enforcing` and `SELINUXTYPE=targeted` in the
*_/etc/selinux/config_* file:

----
# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=enforcing
# SELINUXTYPE= can take one of these three values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected.
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted
----

[[configuring-core-usage]]
=== Optional: Configuring Core Usage

By default, {product-title} masters and nodes use all available cores in the
system they run on. You can choose the number of cores you want {product-title}
to use by setting the `*GOMAXPROCS*` environment variable. See the
link:https://golang.org/pkg/runtime/#GOMAXPROCS[Go Language documentation] for
more information, including how the `*GOMAXPROCS*` environment variable works.

For example, run the following before starting the server to make
{product-title} only run on one core:

----
# export GOMAXPROCS=1
----

ifdef::openshift-origin[]
Alternatively, if you plan to
getting_started/administrators.adoc#running-in-a-docker-container[run
OpenShift in a container], add `-e GOMAXPROCS=1` to the `docker run`
command when launching the server.
endif::[]

[[install-prerequisites-overlayfs]]
=== Optional: Using OverlayFS

OverlayFS is a union file system that allows you to overlay one file system on
top of another.

As of Red Hat Enterprise Linux 7.4, you have the option to configure your
{product-title} environment to use OverlayFS. The `overlay2` graph driver is
fully supported in addition to the older `overlay` driver. However, Red Hat
recommends using `overlay2` instead of `overlay`, because of its speed and
simple implementation.

See the
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/managing_containers/#using_the_overlay_graph_driver[Overlay Graph Driver] section of the Atomic Host documentation for instructions on how
to enable the `overlay2` graph driver for the Docker service.

[[security-warning]]
=== Security Warning

// tag::container-image-security-warning[]
{product-title} runs containers on hosts in the cluster, and in some cases, such
as build operations and the registry service, it does so using privileged
containers. Furthermore, those containers access the hosts' Docker daemon and
perform `docker build` and `docker push` operations. As such, cluster
administrators must be aware of the inherent security risks associated with
performing `docker run` operations on arbitrary images as they effectively have
root access. This is particularly relevant for `docker build` operations.

Exposure to harmful containers can be limited by assigning specific builds to
nodes so that any exposure is limited to those nodes. To do this, see the
xref:../dev_guide/builds/advanced_build_operations.adoc#dev-guide-assigning-builds-to-nodes[Assigning Builds to Specific Nodes] section of the Developer Guide. For cluster
administrators, see the
xref:../install_config/build_defaults_overrides.adoc#overview[Configuring Global Build Defaults and Overrides] topic.

You can also use
xref:../architecture/additional_concepts/authorization.adoc#security-context-constraints[security
context constraints] to control the actions that a pod can perform and what it
has the ability to access. For instructions on how to enable images to run with
*USER* in the Dockerfile, see
xref:../admin_guide/manage_scc.adoc#how-do-i[Managing Security Context
Constraints] (requires a user with *cluster-admin* privileges).

For more information, see these articles:

- http://opensource.com/business/14/7/docker-security-selinux
- https://docs.docker.com/engine/security/security/
// end::container-image-security-warning[]

[[envirornment-requirements]]
== Environment Requirements

The following section defines the requirements of the environment containing
your {product-title} configuration. This includes networking considerations
and access to external services, such as Git repository access, storage, and
cloud infrastructure providers.

[[prereq-dns]]
=== DNS Requirements

{product-title} requires a fully functional DNS server in the environment. This
is ideally a separate host running DNS software and can provide name resolution
to hosts and containers running on the platform.

[IMPORTANT]
Adding entries into the *_/etc/hosts_* file on each host is not enough. This
file is not copied into containers running on the platform.

Key components of {product-title} run themselves inside of containers and use
the following process for name resolution:

. By default, containers receive their DNS configuration
file (*_/etc/resolv.conf_*) from their host.

. {product-title} then sets the pod's first nameserver to the IP address of
the node.

As of {product-title}
ifdef::openshift-enterprise[]
3.2,
endif::[]
ifdef::openshift-origin[]
1.2,
endif::[]
*dnsmasq* is automatically configured on all masters and nodes. The pods use the
nodes as their DNS, and the nodes forward the requests. By default, *dnsmasq*
is configured on the nodes to listen on port 53, therefore the nodes cannot run
any other type of DNS application.

[NOTE]
====
include::install/prerequisites.adoc[tag=networkmanager]

Similarly, if the `PEERDNS` parameter is set to `no` in the network script, for example, *_/etc/sysconfig/network-scripts/ifcfg-em1_*, then the dnsmasq files are not generated, and the Ansible install will fail. Ensure the `PEERDNS` setting is set to `yes`.
====

The following is an example set of DNS records:

----
master1    A   10.64.33.100
master2    A   10.64.33.103
node1      A   10.64.33.101
node2      A   10.64.33.102
----

If you do not have a properly functioning DNS environment, you might experience
failure with:

- Product installation via the reference Ansible-based scripts
- Deployment of the infrastructure containers (registry, routers)
- Access to the {product-title} web console, because it is not accessible via
IP address alone


[[dns-config-prereq]]
==== Configuring Hosts to Use DNS

Make sure each host in your environment is configured to resolve hostnames from
your DNS server. The configuration for hosts' DNS resolution depend on whether
DHCP is enabled. If DHCP is:

- Disabled, then configure your network interface to be static, and add DNS
nameservers to NetworkManager.

- Enabled, then the NetworkManager dispatch script automatically configures DNS
based on the DHCP configuration.

To verify that hosts can be resolved by your DNS server:

. Check the contents of *_/etc/resolv.conf_*:
+
----
$ cat /etc/resolv.conf
# Generated by NetworkManager
search example.com
nameserver 10.64.33.1
# nameserver updated by /etc/NetworkManager/dispatcher.d/99-origin-dns.sh
----
+
In this example, 10.64.33.1 is the address of our DNS server.

. Test that the DNS servers listed in *_/etc/resolv.conf_* are able to resolve
host names to the IP addresses of all masters and nodes in your {product-title}
environment:
+
----
$ dig <node_hostname> @<IP_address> +short
----
+
For example:
+
----
$ dig master.example.com @10.64.33.1 +short
10.64.33.100
$ dig node1.example.com @10.64.33.1 +short
10.64.33.101
----

[[wildcard-dns-prereq]]
==== Configuring a DNS Wildcard

Optionally, configure a wildcard for the router to use, so that you do not need
to update your DNS configuration when new routes are added.

A wildcard for a DNS zone must ultimately resolve to the IP address of the
{product-title} xref:../architecture/networking/routes.adoc#routers[router].

For example, create a wildcard DNS entry for *cloudapps* that has a low
time-to-live value (TTL) and points to the public IP address of the host where
the router will be deployed:

----
*.cloudapps.example.com. 300 IN  A 192.168.133.2
----

[WARNING]
====
In your *_/etc/resolv.conf_* file on each node host, ensure that the DNS server
that has the wildcard entry is not listed as a nameserver or that the wildcard
domain is not listed in the search list. Otherwise, containers managed by
{product-title} might fail to resolve host names properly.
====

[[node-hostnames-prereq]]
==== Configuring Node Host Names

When you set up a cluster that is not integrated with a cloud provider, you must
correctly set your nodes' host names. Each node's host name must be resolvable,
and each node must be able to reach each other node.

To confirm that a node can reach another node:

. On one node, obtain the host name:
+
----
$ hostname

master-1.example.com
----

. On that same node, obtain the fully qualified domain name of the host:
+
----
$ hostname -f

master-1.example.com
----

. From a different node, confirm that you can reach the first node:
+
----
$ ping master-1.example.com -c 1

PING master-1.example.com (172.16.122.9) 56(84) bytes of data.
64 bytes from master-1.example.com (172.16.122.9): icmp_seq=1 ttl=64 time=0.319 ms

--- master-1.example.com ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.319/0.319/0.319/0.000 ms
----

[[prereq-network-access]]
=== Network Access Requirements

A shared network must exist between the master and node hosts. If you plan to
configure
xref:../architecture/infrastructure_components/kubernetes_infrastructure.adoc#high-availability-masters[multiple masters for high-availability] using standard cluster installation
 process, you must also select an IP to be configured as your
 xref:../admin_guide/high_availability.adoc#virtual-ips[virtual IP] (VIP)
 during the installation process. The IP that you select must be routable
 between all of your nodes, and if you configure using a FQDN it must resolve
 on all nodes.

[[prereq-networkmanager]]
==== NetworkManager

//tag::networkmanager[]
*NetworkManager*, a program for providing detection and configuration for
systems to automatically connect to the network, is required on the nodes in
order to populate *dnsmasq* with the DNS IP addresses.

`NM_CONTROLLED` is set to `yes` by default. If `NM_CONTROLLED` is set to `no`,
then the NetworkManager dispatch script does not create the relevant
*_origin-upstream-dns.conf_* dnsmasq file, and you must configure
dnsmasq manually.
//end::networkmanager[]

[[install-config-network-using-firewalld]]
==== Configuring firewalld as the firewall

While iptables is the default firewall, firewalld is recommended for new
installations. You can enable firewalld by setting
`os_firewall_use_firewalld=true` in
xref:configuring_inventory_file.adoc#advanced-install-configuring-firewalls[the
Ansible inventory file].

----
[OSEv3:vars]
os_firewall_use_firewalld=True
----

Setting this variable to `true` opens the required ports and adds rules to the
default zone, which ensure that firewalld is configured correctly.

[NOTE]
====
Using the firewalld default configuration comes with limited configuration
options, and cannot be overridden. For example, while you can set up a storage
network with interfaces in multiple zones, the interface that nodes communicate
on must be in the default zone.
====


[[required-ports]]
==== Required Ports

The {product-title} installation automatically creates a set of internal
firewall rules on each host using
xref:../admin_guide/iptables.adoc#overview[iptables]. However, if your
network configuration uses an external firewall, such as a hardware-based
firewall, you must ensure infrastructure components can communicate with each
other through specific ports that act as communication endpoints for certain
processes or services.

Ensure the following ports required by {product-title} are open on your network
and configured to allow access between hosts. Some ports are optional depending
on your configuration and usage.

.Node to Node
[cols='2,1,8']
|===
| *4789*
|UDP
|Required for SDN communication between pods on separate hosts.
|===

.Nodes to Master
[cols='2,1,8']
|===
| *53* or *8053*
|TCP/UDP
|Required for DNS resolution of cluster services (SkyDNS).
ifdef::openshift-origin[]
Installations prior to 1.2 or environments upgraded to 1.2 use port 53.
endif::[]
ifdef::openshift-enterprise[]
Installations prior to 3.2 or environments upgraded to 3.2 use port 53.
endif::[]
New installations will use 8053 by default so that *dnsmasq* might be configured.

| *4789*
|UDP
|Required for SDN communication between pods on separate hosts.

| *443* or *8443*
|TCP
|Required for node hosts to communicate to the master API, for the node hosts to
post back status, to receive tasks, and so on.
|===

.Master to Node
[cols='2,1,8']
|===
| *4789*
|UDP
|Required for SDN communication between pods on separate hosts.

| *10250*
|TCP
|The master proxies to node hosts via the Kubelet for `oc` commands.

| *10010*
|TCP
|If using CRI-O, open this port to allow `oc exec` and `oc rsh` operations.
|===

.Master to Master
[cols='2,1,8']
|===
| *53* or *8053*
|TCP/UDP
|Required for DNS resolution of cluster services (SkyDNS).
ifdef::openshift-origin[]
Installations prior to 1.2 or environments upgraded to 1.2 use port 53.
endif::[]
ifdef::openshift-enterprise[]
Installations prior to 3.2 or environments upgraded to 3.2 use port 53.
endif::[]
New installations will use 8053 by default so that *dnsmasq* might be configured.

| *2049*
|TCP/UDP
|Required when provisioning an NFS host as part of the installer.

| *2379*
|TCP
|Used for standalone etcd (clustered) to accept changes in state.

| *2380*
|TCP
|etcd requires this port be open between masters for leader election and peering
connections when using standalone etcd (clustered).

| *4789*
|UDP
|Required for SDN communication between pods on separate hosts.

|===

.External to Load Balancer
[cols='2,1,8']
|===
| *9000*
|TCP
|If you choose the `*native*` HA method, optional to allow access to the HAProxy statistics page.

|===


.External to Master
[cols='2,1,8']
|===
| *443* or *8443*
|TCP
|Required for node hosts to communicate to the master API, for node hosts to
post back status, to receive tasks, and so on.

|*8444*
|TCP
|Port that the controller service listens on. Required to be open for the `/metrics` and `/healthz` endpoints.
|===

.IaaS Deployments
[cols='2,1,8']
|===
| *22*
|TCP
| Required for SSH by the installer or system administrator.

| *53* or *8053*
|TCP/UDP
|Required for DNS resolution of cluster services (SkyDNS).
ifdef::openshift-origin[]
Installations prior to 1.2 or environments upgraded to 1.2 use port 53.
endif::[]
ifdef::openshift-enterprise[]
Installations prior to 3.2 or environments upgraded to 3.2 use port 53.
endif::[]
New installations will use 8053 by default so that *dnsmasq* might be configured.
Only required to be internally open on master hosts.

| *80* or *443*
|TCP
| For HTTP/HTTPS use for the router. Required to be externally open on node hosts, especially on nodes running the router.

| *1936*
|TCP
| (*Optional*) Required to be open when running the template router to access
statistics. Can be open externally or internally to connections depending on if
you want the statistics to be expressed publicly. Can require extra
configuration to open. See the Notes section below for more information.

| *2379* and *2380*
|TCP
| For standalone etcd use. Only required to be internally open on the master host.
*2379* is for server-client connections. *2380* is for server-server
connections, and is only required if you have clustered etcd.

| *4789*
|UDP
| For VxLAN use (OpenShift SDN). Required only internally on node hosts.

| *8443*
|TCP
| For use by the {product-title} web console, shared with the API server.

| *10250*
|TCP
| For use by the Kubelet. Required to be externally open on nodes.
|===

*Notes*

* In the above examples, port *4789* is used for User Datagram Protocol (UDP).
* When deployments are using the SDN, the pod network is accessed via a service
proxy, unless it is accessing the registry from the same node the registry is
deployed on.
* {product-title} internal DNS cannot be received over SDN. For non-cloud
deployments, this will default to the IP address associated with the default
route on the master host. For cloud deployments, it will default to the IP
address associated with the first internal interface as defined by the cloud
metadata.
* The master host uses port *10250* to reach the nodes and does not go over SDN.
It depends on the target host of the deployment and uses the computed value of
`*openshift_public_hostname*`.
* Port *1936* can still be inaccessible due to your iptables rules. Use the following to configure iptables to open port *1936*:
+
----
# iptables -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp \
    --dport 1936 -j ACCEPT
----

.Aggregated Logging
[cols='2,1,8']
|===
| *9200*
|TCP
|For Elasticsearch API use. Required to be internally open on any infrastructure
nodes so Kibana is able to retrieve logs for display. It can be externally
opened for direct access to Elasticsearch by means of a route. The route can be
created using `oc expose`.

| *9300*
|TCP
|For Elasticsearch inter-cluster use. Required to be internally open on any
infrastructure node so the members of the Elasticsearch cluster might communicate
with each other.
|===

[[prereq-persistent-storage]]
=== Persistent Storage

The Kubernetes
xref:../architecture/additional_concepts/storage.adoc#architecture-additional-concepts-storage[persistent volume]
framework allows you to provision an {product-title} cluster with persistent storage
using networked storage available in your environment. This can be done after
completing the initial {product-title} installation depending on your application
needs, giving users a way to request those resources without having any
knowledge of the underlying infrastructure.

The Configuring Clusters guide  provides instructions for cluster administrators
on provisioning an {product-title} cluster with persistent storage using
xref:../install_config/persistent_storage/persistent_storage_nfs.adoc#install-config-persistent-storage-persistent-storage-nfs[NFS],
xref:../install_config/persistent_storage/persistent_storage_glusterfs.adoc#install-config-persistent-storage-persistent-storage-glusterfs[GlusterFS],
xref:../install_config/persistent_storage/persistent_storage_ceph_rbd.adoc#install-config-persistent-storage-persistent-storage-ceph-rbd[Ceph
RBD],
xref:../install_config/persistent_storage/persistent_storage_cinder.adoc#install-config-persistent-storage-persistent-storage-cinder[OpenStack
Cinder],
xref:../install_config/persistent_storage/persistent_storage_aws.adoc#install-config-persistent-storage-persistent-storage-aws[AWS Elastic Block Store (EBS)],
xref:../install_config/persistent_storage/persistent_storage_gce.adoc#install-config-persistent-storage-persistent-storage-gce[GCE
Persistent Disks], and
xref:../install_config/persistent_storage/persistent_storage_iscsi.adoc#install-config-persistent-storage-persistent-storage-iscsi[iSCSI].

[[prereq-cloud-provider-considerations]]
=== Cloud Provider Considerations

There are certain aspects to take into consideration if installing {product-title}
on a cloud provider.

* For Amazon Web Services, see the xref:../install_config/configuring_aws.adoc#configuring-aws-permissions[Permissions] and the
xref:../install_config/configuring_aws.adoc#configuring-a-security-group-aws[Configuring a Security Group] sections.
* For OpenStack, see the  xref:../install_config/configuring_openstack.adoc#configuring-openstack-permissions[Permissions] and the
xref:../install_config/configuring_openstack.adoc#configuring-a-security-group-openstack[Configuring a Security Group] sections.

[[overriding-detected-ip-addresses-host-names]]
==== Overriding Detected IP Addresses and Host Names

Some deployments require that the user override the detected host names and IP
addresses for the hosts. To see the default values, change to the playbook directory and run the `*openshift_facts*`
playbook:

----
$ cd /usr/share/ansible/openshift-ansible
$ ansible-playbook  [-i /path/to/inventory] \
  playbooks/byo/openshift_facts.yml
----

[IMPORTANT]
====
For Amazon Web Services, see the xref:../install_config/configuring_aws.adoc#overriding-detected-ip-addresses-host-names-aws[Overriding Detected IP Addresses and Host Names] section.
====


Now, verify the detected common settings. If they are not what you expect them
to be, you can override them.

The
xref:../install/configuring_inventory_file.adoc#configuring-ansible[Configuring Your Inventory File]
topic discusses the available Ansible variables in greater detail.

[cols="1,2",options="header"]
|===
|Variable |Usage

|`*hostname*`
a| - Resolves to the internal IP address from the instances themselves.

|`*ip*`
a| - The internal IP address of the instance.

|`*public_hostname*`
a| - Resolves to the external IP from hosts outside of the cloud.
- Provider `*openshift_public_hostname*` overrides.

|`*public_ip*`
a| - The externally accessible IP address associated with the instance.
- `*openshift_public_ip*` overrides.

|`*use_openshift_sdn*`
a| - For all clouds but GCE, set to `true`.
- `*openshift_use_openshift_sdn*` overrides.

|===

==== Post-Installation Configuration for Cloud Providers

Following the installation process, you can configure {product-title} for
xref:../install_config/configuring_aws.adoc#install-config-configuring-aws[AWS],
xref:../install_config/configuring_openstack.adoc#install-config-configuring-openstack[OpenStack], or
xref:../install_config/configuring_gce.adoc#install-config-configuring-gce[GCE].
